## ESPnet3: Data Preparation and Parallel Processing Guide

This guide explains how to prepare data for training with ESPnet3, focusing on efficient and scalable processing using Python-based tools. The content is organized into two main sections:

1. Data Processing
2. Data Sharing

> Note: This guide assumes that users have already downloaded raw datasets (e.g., audio/text). The process of downloading raw data is not covered here.

---

### 1. Data Processing (with Dask)

#### ðŸ”„ Converting HuggingFace Datasets to Lhotse CutSet
You can convert a HuggingFace dataset into a Lhotse CutSet using `espnet3.data.huggingface_utils.cutset_from_huggingface`, which supports parallel execution via Dask:

```python
from espnet3.data.huggingface_utils import cutset_from_huggingface
from espnet3.parallel import get_client
from lhotse import validate
import datasets

# 1. Load the HuggingFace dataset
dataset_id = "your_dataset_name"
split = "train"
dataset = datasets.load_dataset(dataset_id, split=split)

# 2. Define how to extract audio and text from each sample
data_info = {
    "audio": lambda x: x["audio"]["array"],
    "text": lambda x: x["text"]
}

# 3. Launch Dask client with parallel config
from omegaconf import OmegaConf
config = OmegaConf.load("your_parallel_config.yaml")

with get_client(config.parallel) as client:
    cuts = cutset_from_huggingface(
        data_info=data_info,
        dataset_length=len(dataset),
        dataset_id=dataset_id,
        split=split,
        client=client
    )

# 4. Optionally validate and save the CutSet
validate(cuts)
cuts.to_file("cuts_train.jsonl.gz")
```

Large-scale datasets can take a significant amount of time to preprocess. While ESPnet2 relied on shell scripts and `cmd.sh` for distributing jobs across clusters, ESPnet3 replaces this with a more Pythonic and scalable solution: **Dask**.

Dask allows for flexible, parallel job execution both locally and on cluster environments like SLURM.

#### âœ… Example Parallel Config (SLURM)
```yaml
parallel:
  env: slurm
  n_workers: 8
  options:
    queue: gpu
    cores: 8
    processes: 1
    memory: 16GB
    walltime: 30:00
    job_extra_directives:
      - "--gres=gpu:1"
```

#### âœ… Example Python Usage
```python
from espnet3.parallel import get_client
from espnet3.data.huggingface_utils import convert_to_dataset

# Launch 8 workers with 8 cores and 1 GPU each
with get_client(config.parallel) as client:
    # Distribute map tasks over workers
    result = client.map(convert_to_dataset, data_list)
```

- `get_client` dynamically constructs a Dask cluster from the config
- Parallel workers are launched when entering the `with` block and terminated on exit
- `convert_to_dataset` is a user-defined or ESPnet3 utility function for feature extraction, formatting, etc.

#### ðŸ§  Tip: Resource Planning
Communication between the main (parent) process and workers can become a bottleneck if the parent has insufficient resources.

Example Pitfall:
> Launching 32 workers with 2 CPUs each while the parent only has 2 CPUs can drastically limit throughput.

Recommendation:
> Allocate **sufficient CPUs and memory** to the parent process (e.g., 8â€“16 cores) to fully utilize parallelism.

---

### 2. Data Sharing

After data preparation, we recommend sharing datasets in HuggingFace Datasets format when possible. ESPnet3 supports both:
- Preprocessed datasets stored locally (e.g., JSON, SCP, HDF5)
- Online repositories via HuggingFace

By wrapping these datasets with `ESPnet3Dataset`, you ensure compatibility with ESPnet3 models and pipelines.

```python
from datasets import load_dataset
from espnet3.data.dataset import ESPnet3Dataset

hf_dataset = load_dataset("your_dataset_name")
dataset = ESPnet3Dataset(hf_dataset)
```

If you are training a custom model (not based on ESPnet), you may use HuggingFace datasets, Lhotse CutSets, or even plain Python lists directly without wrapping.
However, if you plan to use ESPnet-based models or want to take advantage of ESPnet3 training pipelines, wrapping the dataset with `ESPnet3Dataset` is recommended to ensure compatibility.

---

ESPnet3 offers a fully Python-native, cluster-friendly way to prepare, preprocess, and share large-scale datasets efficiently.
