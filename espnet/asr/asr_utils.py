#!/usr/bin/env python

# Copyright 2017 Johns Hopkins University (Shinji Watanabe)
#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)

import json
import logging

# chainer related
import chainer

from chainer import training
from chainer.training import extensions

import torch

# io related
import kaldi_io_py

import numpy as np

from espnet.utils.pytorch_utils import torch_load
from espnet.utils.pytorch_utils import torch_resume
from espnet.utils.pytorch_utils import torch_save

from espnet.utils.training.train_utils import add_attention_report
from espnet.utils.training.train_utils import add_early_stop
from espnet.utils.training.train_utils import add_tensorboard
from espnet.utils.training.train_utils import REPORT_INTERVAL

# matplotlib related
import matplotlib

matplotlib.use('Agg')


# * -------------------- training iterator related -------------------- *

def make_args_batchset(data, args):
    """Make batch set from json dictionary

    :param data: dictionary loaded from data.json
    :param args: the program arguments
    :return: list of batches
    """
    return make_batchset(data, args.batch_size, args.maxlen_in, args.maxlen_out, args.num_batches,
                         args.ngpu if args.ngpu > 1 else 1)


def make_batchset(data, batch_size, max_length_in, max_length_out,
                  num_batches=0, min_batch_size=1):
    """Make batch set from json dictionary

    :param dict data: dictionary loaded from data.json
    :param int batch_size: batch size
    :param int max_length_in: maximum length of input to decide adaptive batch size
    :param int max_length_out: maximum length of output to decide adaptive batch size
    :param int num_batches: # number of batches to use (for debug)
    :param int min_batch_size: mininum batch size (for multi-gpu)
    :return: list of batches
    """
    # sort it by input lengths (long to short)
    sorted_data = sorted(data.items(), key=lambda data: int(
        data[1]['input'][0]['shape'][0]), reverse=True)
    logging.info('# utts: ' + str(len(sorted_data)))

    # check #utts is more than min_batch_size
    if len(sorted_data) < min_batch_size:
        raise ValueError("#utts is less than min_batch_size.")

    # make list of minibatches
    minibatches = []
    start = 0
    while True:
        ilen = int(sorted_data[start][1]['input'][0]['shape'][0])
        olen = int(sorted_data[start][1]['output'][0]['shape'][0])
        factor = max(int(ilen / max_length_in), int(olen / max_length_out))
        # change batchsize depending on the input and output length
        # if ilen = 1000 and max_length_in = 800
        # then b = batchsize / 2
        # and max(min_batches, .) avoids batchsize = 0
        bs = max(min_batch_size, int(batch_size / (1 + factor)))
        end = min(len(sorted_data), start + bs)
        minibatch = sorted_data[start:end]

        # check each batch is more than minimum batchsize
        if len(minibatch) < min_batch_size:
            mod = min_batch_size - len(minibatch) % min_batch_size
            additional_minibatch = [sorted_data[i] for i in np.random.randint(0, start, mod)]
            minibatch.extend(additional_minibatch)
        minibatches.append(minibatch)

        if end == len(sorted_data):
            break
        start = end

    # for debugging
    if num_batches > 0:
        minibatches = minibatches[:num_batches]
    logging.info('# minibatches: ' + str(len(minibatches)))

    return minibatches


def load_inputs_and_targets(batch):
    """Function to load inputs and targets from list of dicts

    :param list batch: list of dict which is subset of loaded data.json
    :return: list of input feature sequences [(T_1, D), (T_2, D), ..., (T_B, D)]
    :rtype: list of float ndarray
    :return: list of target token id sequences [(L_1), (L_2), ..., (L_B)]
    :rtype: list of int ndarray
    """
    # load acoustic features and target sequence of token ids
    xs = [kaldi_io_py.read_mat(b[1]['input'][0]['feat']) for b in batch]
    ys = [b[1]['output'][0]['tokenid'].split() for b in batch]

    # get index of non-zero length samples
    nonzero_idx = filter(lambda i: len(ys[i]) > 0, range(len(xs)))
    # sort in input lengths
    nonzero_sorted_idx = sorted(nonzero_idx, key=lambda i: -len(xs[i]))
    if len(nonzero_sorted_idx) != len(xs):
        logging.warning('Target sequences include empty tokenid (batch %d -> %d).' % (
            len(xs), len(nonzero_sorted_idx)))

    # remove zero-length samples
    xs = [xs[i] for i in nonzero_sorted_idx]
    ys = [np.fromiter(map(int, ys[i]), dtype=np.int64) for i in nonzero_sorted_idx]

    return xs, ys


# * -------------------- chainer extension related -------------------- *
class CompareValueTrigger(object):
    """Trigger invoked when key value getting bigger or lower than before

    :param str key : Key of value
    :param function compare_fn : Function to compare the values
    :param (int, str) trigger : Trigger that decide the comparison interval
    """

    def __init__(self, key, compare_fn, trigger=(1, 'epoch')):
        self._key = key
        self._best_value = None
        self._interval_trigger = training.util.get_trigger(trigger)
        self._init_summary()
        self._compare_fn = compare_fn

    def __call__(self, trainer):
        observation = trainer.observation
        summary = self._summary
        key = self._key
        if key in observation:
            summary.add({key: observation[key]})

        if not self._interval_trigger(trainer):
            return False

        stats = summary.compute_mean()
        value = float(stats[key])  # copy to CPU
        self._init_summary()

        if self._best_value is None:
            # initialize best value
            self._best_value = value
            return False
        elif self._compare_fn(self._best_value, value):
            return True
        else:
            self._best_value = value
            return False

    def _init_summary(self):
        self._summary = chainer.reporter.DictSummary()


def restore_snapshot(model, snapshot, load_fn=chainer.serializers.load_npz):
    """Extension to restore snapshot"""

    @training.make_extension(trigger=(1, 'epoch'))
    def restore_snapshot(trainer):
        _restore_snapshot(model, snapshot, load_fn)

    return restore_snapshot


def _restore_snapshot(model, snapshot, load_fn=chainer.serializers.load_npz):
    load_fn(snapshot, model)
    logging.info('restored from ' + str(snapshot))


def adadelta_eps_decay(eps_decay):
    """Extension to perform adadelta eps decay"""

    @training.make_extension(trigger=(1, 'epoch'))
    def adadelta_eps_decay(trainer):
        _adadelta_eps_decay(trainer, eps_decay)

    return adadelta_eps_decay


def _adadelta_eps_decay(trainer, eps_decay):
    optimizer = trainer.updater.get_optimizer('main')
    # for chainer
    if hasattr(optimizer, 'eps'):
        current_eps = optimizer.eps
        setattr(optimizer, 'eps', current_eps * eps_decay)
        logging.info('adadelta eps decayed to ' + str(optimizer.eps))
    # pytorch
    else:
        for p in optimizer.param_groups:
            p["eps"] *= eps_decay
            logging.info('adadelta eps decayed to ' + str(p["eps"]))


# * ------------------ recognition related ------------------ *
def parse_hypothesis(hyp, char_list):
    """Function to parse hypothesis

    :param list hyp: recognition hypothesis
    :param list char_list: list of characters
    :return: recognition text string
    :return: recognition token string
    :return: recognition tokenid string
    """
    # remove sos and get results
    tokenid_as_list = list(map(int, hyp['yseq'][1:]))
    token_as_list = [char_list[idx] for idx in tokenid_as_list]
    score = float(hyp['score'])

    # convert to string
    tokenid = " ".join([str(idx) for idx in tokenid_as_list])
    token = " ".join(token_as_list)
    text = "".join(token_as_list).replace('<space>', ' ')

    return text, token, tokenid, score


def add_results_to_json(js, nbest_hyps, char_list):
    """Function to add N-best results to json

    :param dict js: groundtruth utterance dict
    :param list nbest_hyps: list of hypothesis
    :param list char_list: list of characters
    :return: N-best results added utterance dict
    """
    # copy old json info
    new_js = dict()
    new_js['utt2spk'] = js['utt2spk']
    new_js['output'] = []

    for n, hyp in enumerate(nbest_hyps, 1):
        # parse hypothesis
        rec_text, rec_token, rec_tokenid, score = parse_hypothesis(hyp, char_list)

        # copy ground-truth
        out_dic = dict(js['output'][0].items())

        # update name
        out_dic['name'] += '[%d]' % n

        # add recognition results
        out_dic['rec_text'] = rec_text
        out_dic['rec_token'] = rec_token
        out_dic['rec_tokenid'] = rec_tokenid
        out_dic['score'] = score

        # add to list of N-best result dicts
        new_js['output'].append(out_dic)

        # show 1-best result
        if n == 1:
            logging.info('groundtruth: %s' % out_dic['text'])
            logging.info('prediction : %s' % out_dic['rec_text'])

    return new_js


def add_epsilon_decay(trainer, model, args):
    """Adds the extension managing the epsilon decay

    :param trainer: The trainer to add the extension to
    :param model: The model to train
    :param args: The program arguments
    """
    mtl_mode = get_mtl_mode(args.mtlalpha)
    load_fn = chainer.serializers.load_npz if args.backend == 'chainer' else torch_load
    if args.opt == 'adadelta':
        if args.criterion == 'acc' and mtl_mode is not 'ctc':
            trainer.extend(restore_snapshot(model, args.outdir + '/model.acc.best', load_fn=load_fn),
                           trigger=CompareValueTrigger(
                               'validation/main/acc',
                               lambda best_value, current_value: best_value > current_value))
            trainer.extend(adadelta_eps_decay(args.eps_decay),
                           trigger=CompareValueTrigger(
                               'validation/main/acc',
                               lambda best_value, current_value: best_value > current_value))
        elif args.criterion == 'loss':
            trainer.extend(restore_snapshot(model, args.outdir + '/model.loss.best', load_fn=load_fn),
                           trigger=CompareValueTrigger(
                               'validation/main/loss',
                               lambda best_value, current_value: best_value < current_value))
            trainer.extend(adadelta_eps_decay(args.eps_decay),
                           trigger=CompareValueTrigger(
                               'validation/main/loss',
                               lambda best_value, current_value: best_value < current_value))


def add_snapshot(trainer, model, mtl_mode, savefun):
    """Adds the model snapshot extension

    :param trainer: The trainer to add the extension to
    :param model: The model to save
    :param mtl_mode: The mtl mode
    :param savefun: The save function to use
    """
    trainer.extend(extensions.snapshot_object(model, 'model.loss.best', savefun=savefun),
                   trigger=training.triggers.MinValueTrigger('validation/main/loss'))
    if mtl_mode is not 'ctc':
        trainer.extend(extensions.snapshot_object(model, 'model.acc.best', savefun=savefun),
                       trigger=training.triggers.MaxValueTrigger('validation/main/acc'))


def add_plot_report(trainer):
    """Adds the plot report extension

    :param trainer: The trainer to add the extension to
    """
    # Make a plot for training and validation values
    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss',
                                          'main/loss_ctc', 'validation/main/loss_ctc',
                                          'main/loss_att', 'validation/main/loss_att'],
                                         'epoch', file_name='loss.png'))
    trainer.extend(extensions.PlotReport(['main/acc', 'validation/main/acc'],
                                         'epoch', file_name='acc.png'))


def get_dimensions(json_file):
    """Returns the input and output dimensions given a json filepath

    :param json_file: The json file containing the utterances data
    :return: input dim, output dim
    """
    with open(json_file, 'rb') as f:
        parsed_json = json.load(f)['utts']
    utts = list(parsed_json.keys())
    idim = int(parsed_json[utts[0]]['input'][0]['shape'][1])
    odim = int(parsed_json[utts[0]]['output'][0]['shape'][1])
    logging.info('#input dims : ' + str(idim))
    logging.info('#output dims: ' + str(odim))
    return idim, odim


def get_mtl_mode(mtlalpha):
    """Returns the mtl mode given the mtlalpha value

    :param float mtlalpha: the multitask learning mode coefficient
    :return: a string representing the mtlmode
    """
    # specify attention, CTC, hybrid mode
    if mtlalpha == 1.0:
        mtl_mode = 'ctc'
        logging.info('Pure CTC mode')
    elif mtlalpha == 0.0:
        mtl_mode = 'att'
        logging.info('Pure attention mode')
    else:
        mtl_mode = 'mtl'
        logging.info('Multitask learning mode')
    return mtl_mode


def _get_trainer_lambda(trainer, is_pytorch):
    return trainer.updater.get_optimizer('main').param_groups[0][
        'eps'] if is_pytorch else trainer.updater.get_optimizer('main').eps


def add_progress_report(trainer, args):
    """Adds the logging and progress report extensions

    :param trainer: The trainer to add the extensions to
    :param args: The program arguments
    """
    is_pytorch = args.backend == 'pytorch'
    trainer.extend(extensions.LogReport(trigger=(REPORT_INTERVAL, 'iteration')))
    report_keys = ['epoch', 'iteration', 'main/loss', 'main/loss_ctc', 'main/loss_att',
                   'validation/main/loss', 'validation/main/loss_ctc', 'validation/main/loss_att',
                   'main/acc', 'validation/main/acc', 'elapsed_time']
    if args.opt == 'adadelta':
        trainer.extend(extensions.observe_value('eps', lambda trainer: _get_trainer_lambda(trainer, is_pytorch)),
                       trigger=(REPORT_INTERVAL, 'iteration'))
        report_keys.append('eps')
    if is_pytorch:
        if args.report_cer:
            report_keys.append('validation/main/cer')
        if args.report_wer:
            report_keys.append('validation/main/wer')
    trainer.extend(extensions.PrintReport(
        report_keys), trigger=(REPORT_INTERVAL, 'iteration'))

    trainer.extend(extensions.ProgressBar(update_interval=REPORT_INTERVAL))


def prepare_trainer(updater, evaluator, converter, model, valid_json, args, device):
    """Instantiates and adds common extensions to the trainer

    :param updater: The training updater
    :param evaluator: The training evaluator
    :param converter: The batch converter
    :param model: The model to train
    :param valid_json: The validation json
    :param args: The program arguments
    :param device: The device to use
    :return: the trainer
    """
    is_chainer = args.backend == 'chainer'
    mtl_mode = get_mtl_mode(args.mtlalpha)
    savefun = chainer.serializers.save_npz if is_chainer else torch_save
    resume_fun = chainer.serializers.load_npz if is_chainer else torch_resume

    trainer = training.Trainer(
        updater, (args.epochs, 'epoch'), out=args.outdir)

    if args.resume:
        logging.info('resumed from %s' % args.resume)
        resume_fun(args.resume, trainer)

    trainer.extend(evaluator)
    add_progress_report(trainer, args)
    add_plot_report(trainer)
    add_epsilon_decay(trainer, model, args)
    add_snapshot(trainer, model, mtl_mode, savefun)
    att_reporter = add_attention_report(trainer, model, args, valid_json, converter, device)
    add_early_stop(trainer, args)
    add_tensorboard(trainer, args.tensorboard_dir, att_reporter)
    return trainer


def single_beam_search(model, js, args, train_args, rnnlm):
    """Executes single (no batch) beam search decoding

    :param model: The model used for recognition
    :param js: The input features as a json
    :param args: The decoding arguments
    :param train_args: The model training arguments
    :param rnnlm: The RNNLM used for Shallow Fusion
    :return: The json with the predictions
    """
    is_pytorch = args.backend == 'pytorch'
    func = torch.no_grad if is_pytorch else chainer.no_backprop_mode
    new_js = {}
    with func():
        for idx, name in enumerate(js.keys(), 1):
            logging.info('(%d/%d) decoding ' + name, idx, len(js.keys()))
            feat = kaldi_io_py.read_mat(js[name]['input'][0]['feat'])
            nbest_hyps = model.recognize(feat, args, train_args.char_list, rnnlm)
            new_js[name] = add_results_to_json(js[name], nbest_hyps, train_args.char_list)
    return new_js


def write_results(js, result_label):
    """Writes the json dictionary to a file

    :param js: the json object
    :param str result_label: the file to write to
    """
    # TODO(watanabe) fix character coding problems when saving it
    with open(result_label, 'wb') as f:
        f.write(json.dumps({'utts': js}, indent=4, sort_keys=True).encode('utf_8'))
