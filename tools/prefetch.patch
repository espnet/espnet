--- venv/lib/python2.7/site-packages/chainer/iterators/multiprocess_iterator.py	2018-11-10 01:04:46.653012000 +0900
+++ multiprocess_iterator.py	2018-11-10 01:12:04.552889065 +0900
@@ -89,6 +89,10 @@
             This should return the next order. The size of the order
             should remain constant.
             This option cannot be used when ``shuffle`` is not ``None``.
+        maxtasksperchild (int): Number of tasks a worker of prefetch process
+            can complete before it will exit and be replaced with a fresh
+            worker process, to enable unused resources to be freed. If
+            ``None``, worker processes will live as long as the pool.
 
     """
 
@@ -102,7 +106,8 @@
 
     def __init__(self, dataset, batch_size, repeat=True, shuffle=None,
                  n_processes=None, n_prefetch=1, shared_mem=None,
-                 order_sampler=None, dataset_timeout=30.0):
+                 order_sampler=None, dataset_timeout=30.0,
+                 maxtasksperchild=None):
         self.dataset = dataset
         self.batch_size = batch_size
         self.repeat = repeat
@@ -111,6 +116,7 @@
         self.n_prefetch = max(n_prefetch, 1)
         self.shared_mem = shared_mem
         self.dataset_timeout = dataset_timeout
+        self._maxtasksperchild = maxtasksperchild
 
         if self.shuffle is not None:
             if order_sampler is not None:
@@ -135,7 +141,7 @@
             self.dataset, self.batch_size, self.repeat,
             self.n_processes, self.n_prefetch, self.shared_mem,
             self._comm, self.order_sampler,
-            self._interruption_testing)
+            self._interruption_testing, self._maxtasksperchild)
         # defer launching prefetch thread until creating the worker pool,
         # not to leave a background thread in forked processes.
 
@@ -350,7 +356,7 @@
     def __init__(self, dataset, batch_size, repeat,
                  n_processes, n_prefetch, mem_size, comm,
                  order_sampler,
-                 _interruption_testing):
+                 _interruption_testing, maxtasksperchild):
         self.dataset = dataset
         self.batch_size = batch_size
         self.repeat = repeat
@@ -358,6 +364,7 @@
         self.mem_size = mem_size
         self._comm = comm
         self.order_sampler = order_sampler
+        self.maxtasksperchild = maxtasksperchild
 
         self._allocate_shared_memory()
 
@@ -430,7 +437,8 @@
         self._pool = multiprocessing.Pool(
             processes=self.n_processes,
             initializer=_fetch_setup,
-            initargs=(self.dataset, self.mem_size, self.mem_bulk))
+            initargs=(self.dataset, self.mem_size, self.mem_bulk),
+            maxtasksperchild=self.maxtasksperchild)
         if self._interruption_testing:
             pids = self._pool.map(_report_pid, range(self.n_processes))
             print(' '.join(map(str, pids)))
