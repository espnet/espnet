corelm: ar_delay
corelm_conf:
    share_emb: false
    qk_norm: true
    att_unit: 1024
    head: 16
    layer: 24
    n_ctx: 3000
    prefix_lm: true

# Dataloader
batch_type: numel
batch_bins: 8000
accum_grad: 1
grad_clip: 5
max_epoch: 20
num_workers: 8
drop_last_iter: true
log_interval: 100
num_att_plot: 0

# Criterion & Optimization
optim: adamw
optim_conf:
    lr: 0.001
    betas:
        - 0.9
        - 0.95
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 10000

best_model_criterion:
-   - valid
    - acc
    - max
-   - valid
    - total_count
    - max
keep_nbest_models: 5
nbest_averaging_interval: 5


# Global settings
use_amp: true
use_tf32: false
use_fsdp: false
encoder_decoder_format: false
codec_token_per_frame: 8
codec_token_in_use: 8

# Use the tokens used in the pre-training
init_param: [../../librispeech/speechlm1/exp/speechlm_espnet_speechlm_pretrained_tts/60epoch.pth]
