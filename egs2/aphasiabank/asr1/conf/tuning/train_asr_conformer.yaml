# Based on librispeech_100 conformer config, but layer sizes are adjusted to match
# E-Branchformer's number of trainable parameters.
encoder: conformer
encoder_conf:
  output_size: 256
  attention_heads: 4
  linear_units: 2048
  num_blocks: 12
  dropout_rate: 0.1
  positional_dropout_rate: 0.1
  attention_dropout_rate: 0.1
  input_layer: conv2d2 # subsampling rate = 2
  normalize_before: true
  macaron_style: true
  pos_enc_layer_type: "rel_pos"
  selfattention_layer_type: "rel_selfattn"
  activation_type: "swish"
  use_cnn_module:  true
  cnn_module_kernel: 31

decoder: transformer
decoder_conf:
  attention_heads: 4
  linear_units: 2048
  num_blocks: 6
  dropout_rate: 0.1
  positional_dropout_rate: 0.1
  self_attention_dropout_rate: 0.1
  src_attention_dropout_rate: 0.1

model_conf:
  ctc_weight: 0.3
  lsm_weight: 0.1
  length_normalized_loss: false

frontend_conf:
  n_fft: 512
  win_length: 400
  hop_length: 160

seed: 2022
log_interval: 200
num_att_plot: 0
num_workers: 4
sort_in_batch: descending
sort_batch: descending
batch_type: numel
batch_bins: 4000000
accum_grad: 12
grad_clip: 5
max_epoch: 50
patience: none
init: none
best_model_criterion:
  - - valid
    - acc
    - max
keep_nbest_models: 10

use_amp: true
cudnn_deterministic: false
cudnn_benchmark: false

optim: adam
optim_conf:
  lr: 0.001
  weight_decay: 0.000001
scheduler: warmuplr
scheduler_conf:
  warmup_steps: 2500

specaug: specaug
specaug_conf:
  apply_time_warp: true
  time_warp_window: 5
  time_warp_mode: bicubic
  apply_freq_mask: true
  freq_mask_width_range:
    - 0
    - 27
  num_freq_mask: 2
  apply_time_mask: true
  time_mask_width_ratio_range:
    - 0.
    - 0.05
  num_time_mask: 5
