# AISHELL3 RECIPE

This is the recipe of Mandrain multi-speaker TTS2 model with [aishell3](https://www.openslr.org/93/) corpus.

See the following pages for running on clusters. They can help you to set the environment and get familiar with ESPNet's repo structure.
- [PSC usage tutorial](https://www.wavlab.org/activities/2022/psc-usage/)
- [Espnet recipe tutorial](https://github.com/espnet/notebook/blob/master/ESPnet2/Course/CMU_SpeechRecognition_Fall2022/recipe_tutorial.ipynb)


## Brief on TTS2

- In terms of features

  ``tts2`` uses discrete acoustic features instead of continuous features in ``tts1``. Current TEMPLATE supports the discrete FastSpeech2 model training.
- In terms of data

  ``tts2`` additionally requires duration information, which can be obtained from **Speech-Text Alignment Tools** (tacotron teacher model or mfa). According to the [FastSpeech2](https://arxiv.org/pdf/2006.04558) paper, mfa has a higher quality.


## Run the Recipe

üåü Please notice that most of the ``bash files`` are symbolic linked from the TEMPLATE. It might be updated by later commits using other corpus, so please double check and customize the parameters before your run.

Here is the basic order for running scripts, followed by more details.

1. ``./local/run_mfa.sh``
2. ``./run_train_teacher.sh`` (to stage 8, must use teacher forcing in decoding)
3. ``./run_train_teacher.sh`` (stage 6 only, to extract energy and pitch)
4. ``./run.sh`` (to stage 8, use custom cn_hubert, layer17 for large)
5. Train a vocoder at [PWG](https://github.com/kan-bayashi/ParallelWaveGAN/tree/master/egs) (We use discrete hifigan here)
6. ``./run.sh`` (stage 9 only)
7. Evaluate the generated wav using [scripts here](https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1#evaluation)


### 1. Data Preparation

* Download aishell-3 dataset(trainset & testset)
* Trim slience to improve the efficiency and potentially improve the generated wave quality by cutting off noise.
* Get the initial ``{dset}_phn`` dictionary.
* Split 250 samples from the trainset to be the devset.

```
//{dset}/text sample
SSB00050353 Ê∑±‰∫§ÊâÄÂâØÊÄªÁªèÁêÜÂë®ÊòéÊåáÂá∫
//{dset}_phn/text sample
SSB00050353 shen1 jiao1 suo3 fu4 zong3 jing1 li3 zhou1 ming2 zhi3 chu1
```

  NOTE: The parameters like ``fs``, ``n_fft``, in ``trim_slience.sh`` don't have to be the same as what in ``run.sh``, since they only determine the precision of slience trimming, where the outcome of different sets of parameters will be roughly the same (corpus w/ minimum slience sound).

### 2. Train the teacher model
Following ``tts1``, we train a Tacotron2 model to be the teacher model for FastSpeech2 in ``tts2``.

Set ``audio_format=wav`` is recommended, as it can be directly processed if you want to use x-vector. Or you can use ``flac``, but take ``egs2/libirspeech/asr1/local/data.sh`` as a reference for ``uttid path-to-utt``

Remember to keep the frame shift(fs), hop_size for the teacher model and the student model to be the same, only by which the soft targets generated by teacher Tacotron2 can align with the Fastspeech2 input.

More specifically, the script can be executed by:

```
# Train the teacher model. Total steps >= 100k is recommended.
./run_train_teacher.sh --stage 2 --stop_stage 7
```

Notice that ``test_set`` doesn't need all the processing here since only the pseudo labels from ``train_set`` and ``valid_set`` are required. Skipping some steps e.g. mfa, teacher forcing decoding on ``test_set`` is feasible.

However, it is better to specify ``--test_sets`` in stage 1-3. Since test set phoneme is converted from grapheme to phoneme after new g2p model trained in mfa, the ``wav.scp`` from stage 2 can be used in vocoder part, and the ``spk_emb`` extracted from stage 3 can be used in the overall decoding test.

Then generate the pseudo labels from ``train_set`` and ``valid_set``.

```
# use teacher forcing in decoding
./run_train_teacher.sh --stage 8 --stop_stage 8 \
    --tts_exp exp/tts_train_teacher_raw_phn_none \
    --test_sets "train_no_dev_phn dev_phn" \
    --inference_args "--use_teacher_forcing true" \
    --inference_model 50epoch.pth
```

### 3. Extract additional features

Calculate pitch and energy (still following ``tts1``), for fastspeech2.
```
./run_train_teacher.sh --stage 6 --stop_stage 6 \
    --train_config conf/train_fastspeech2.yaml \
    --teacher_dumpdir exp/tts_train_teacher_raw_phn_none/decode_teacher_use_teacher_forcingtrue_50epoch \
    --tts_stats_dir exp/tts_train_teacher_raw_phn_none/decode_teacher_use_teacher_forcingtrue_50epoch/stats \
    --write_collected_feats true
```

### 4. Train discrete fastspeech2
The datasets include text, durations, speech, discrete speech, pitch, energy, and spkembs. We use cn_hubert (pretrained on mandarin) here for discrete tts feature extraction.

```
# Process test_set for stage 6. The discrete unit will be used in the vocoder part. Modify the bash file to avoid reprocessing train_set

./local/data.sh --stage 1 --stop_stage 1
./run.sh --stage 2 --stop_stage 2
```

```
# It is recommended to modify tts2.sh, switching the English hubert to Chinese hubert, for aishell3 customization.
./run.sh --stage 5 --stop_stage 6 --s3prl_upstream_name hf_hubert_custom --feature_layer 17

./run.sh --stage 8 --stop_stage 8 --s3prl_upstream_name hf_hubert_custom --feature_layer 17 \
    --teacher_dumpdir exp/tts_train_teacher_raw_phn_none/decode_teacher_use_teacher_forcingtrue_50epoch \
    --tts2_stats_dir exp/tts_train_teacher_raw_phn_none/decode_teacher_use_teacher_forcingtrue_50epoch/stats \
    --tts2_exp exp/tts_fastspeech2_raw_phn_none_cn_hubert

```

### 5. Train a vocoder
A customized vocoder for aishell3 discrete features is necessary for the purpose of generating ``wav`` from discrete hubert features.

The vocoder for tts2 are not exactly mel2text, so our goal here is not to train a rule-based vocoder like ``tts1``, but another unique vocoder that maps discrete features to waves.

We use [PWG repo](https://github.com/kan-bayashi/ParallelWaveGAN/tree/master/egs), and here are the detailed steps:


* git clone https://github.com/kan-bayashi/ParallelWaveGAN.git , ``cd ParallelWaveGAN/egs/aishell3/hubert_voc1``.

* Collect hubert text to a single file

  ```shell
  cat path/to/train_hubert.txt path/to/dev_hubert.txt path/to/test_hubert.txt > path/to/newfile_all.txt
  ```
* Modify the ``hubert_text`` in ./run.sh. Follow instructions in stage 0 to symlink the data(silence trimmed). ``wav`` format is better supported in kaldiio than ``flac``. Notice that aishell3 has unknown speakers, so we don't use sid.

* Modify ``num_embs``(equals to the number of k-means clusters), ``batch_max_steps``(as the comment suggested) and custom parameters in the config file ``conf/hifigan_hubert_24k.v1.yaml``.

* Start feature extraction and training from stage 1.


### 6. Inference
Run the inference stage in espnet2 recipe with your trained vocoder. Waveform will be directly generated this time.

```
./run.sh --stage 9 --stop_stage 9 --tts2_exp exp/tts_fastspeech2_raw_phn_none_cn_hubert
```

### 7. Evaluate model performance
Please follow [scripts here](https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1#evaluation).


## Other references

**Speech-Text Alignment Tools**

The token duration is predicted using Speech-Text alignment tools, which can be either force-aligner or attention-based auto-regressive model (e.g., Tacotron2). Please refer to [Alignment from Tacotron2](https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1#fastspeech-training) and [Montreal Forced Aligner(MFA)](https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1#new-mfa-aligments-generation) for details.


**MFA**

Firstly make sure ``mfa`` has been prepared in the environment.
```
cd ../../../tools
make mfa.done
cd -
```

Originally, ``Stage 1`` in ``run.sh`` calls ``local/data.sh``, but here we won't run ``Stage 1``, instead, we use

```
./local/run_mfa.sh
```

which is an entry point that will call ``scripts/mfa.sh`` and further call ``local/data.sh``. If ``--train false``, this script will download pretrained g2p and acoustic models, else if ``--train true``, this script will generate the alignments. The generated results will be stored in the ``<split_sets>_phn`` lexicon.

For aishell-3, we train a new G2P model on ``mandarin_china_mfa`` dictionary, and generate the lexicon. Then train the speech-text alignment MFA.

If you want to use the duration extracted by mfa, then you can continue the training on the main script from ``Stage 2``:

```
./run_.sh --stage 2 --stop_stage 2 --teacher_dumpdir "data"
```

### Multi-Speakers tts2

In multi-spk scenario, adding speaker id or speaker embedding can help better tell speakers apart, specified using ``--use_spk_embed`` or ``--use_sid``. But since aishell-3 is not a fixed speaker corpus, i.e. exists speakers with unknown id, here we use speaker embeddings.

**Speaker Embeddings**

ESPnet supports several types of speaker embeddings (kaldi: x-vector, speechbrain, espnet_spk). The recently proposed espnet_spk shows SOTA performance among many tasks, thus we use it here.


### Discrete Speech Challenge Baseline

<table class="table">
  <thread>
    <tr>
      <th scope="col">Model</th>
      <th scope="col">MCD ‚¨áÔ∏è</th>
      <th scope="col">Log F0 RMSE ‚¨áÔ∏è</th>
      <th scope="col">CER ‚¨áÔ∏è</th>
      <th scope="col">UTMOS ‚¨ÜÔ∏è</th>
    </tr>
  </thread>
  <tbody>
    <tr>
      <th scope="col">cn_hubert-large-layer17</th>
      <th scope="col">8.5473 ¬± 0.9407</th>
      <th scope="col">0.3032 ¬± 0.1354</th>
      <th scope="col">42.3</th>
      <th scope="col">1.7565 ¬± 0.3628</th>
    </tr>
  </tbody>
</table>

* CER is calculated using openai-whisper-large and Chinese characters.
