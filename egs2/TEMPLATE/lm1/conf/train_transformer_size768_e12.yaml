num_workers: 8
use_amp: true
sharded_ddp: false
num_att_plot: 0

lm: transformer
lm_conf:
    pos_enc: null
    embed_unit: 768
    att_unit: 768
    head: 12
    unit: 3072
    layer: 12
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.1

model_conf:
    lsm_weight: 0.1
    length_normalized_loss: false
    sos_syms:   # multiple sos symbols are used
    - "<generatetext>"
    - "<generatespeech>"
    eos_sym: "<sos/eos>"

# optimization related
grad_clip: 5.0
batch_type: length
batch_bins: 20000
accum_grad: 10
# num_iters_per_epoch: 10000
max_epoch: 70

optim: adam
optim_conf:
   lr: 0.001
   weight_decay: 1.0e-06
scheduler: warmuplr
scheduler_conf:
   warmup_steps: 10000

best_model_criterion:
-   - valid
    - acc
    - max
keep_nbest_models: 10
