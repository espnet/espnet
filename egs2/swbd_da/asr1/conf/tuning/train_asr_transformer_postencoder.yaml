# network architecture
# encoder related
encoder: transformer
encoder_conf:
    output_size: 256    # dimension of attention
    attention_heads: 4
    linear_units: 2048  # the number of units of position-wise feed forward
    num_blocks: 12      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: conv2d # encoder architecture type
    normalize_before: true

# postencoder
postencoder: hugging_face_transformers
postencoder_conf:
    # pick up a model from https://huggingface.co/models?filter=transformers
    # most of models should work, but maybe some don't
    # known to work: bert, gpt2, xlnet, roberta, mpnet, t5, bart
    # xlnet currently works for single gpu only
    model_name_or_path: "bert-base-cased"

# decoder related
decoder: transformer
decoder_conf:
    attention_heads: 4
    linear_units: 2048
    num_blocks: 6
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    self_attention_dropout_rate: 0.0
    src_attention_dropout_rate: 0.0

model_conf:
    ctc_weight: 0.0

max_epoch: 10
