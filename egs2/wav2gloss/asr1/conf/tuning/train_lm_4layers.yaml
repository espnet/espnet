# https://github.com/espnet/espnet/blob/75db853/egs2/librispeech/asr1/conf/tuning/train_lm_adam.yaml
lm_conf:
    nlayers: 4
    unit: 2048
optim: adam
optim_conf:
    lr: 0.001
batch_type: folded
batch_size: 64   # batch size in LM training
max_epoch: 15     # if the data size is large, we can reduce this
patience: 3

init: xavier_uniform
best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 1
