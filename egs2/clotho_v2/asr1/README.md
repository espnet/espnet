# Clotho Audio Captioning RECIPE

This recipe implements the DCASE 2023 Automated Audio Captioning (AAC) task with a BEATs encoder BART decoder model on the Clotho_v2 dataset, very close to what is described in [this paper](https://arxiv.org/abs/2309.17352) and reusing part of code from the [original implementation](https://github.com/slSeanWU/beats-conformer-bart-audio-captioner?tab=readme-ov-file).
More specifically, we provide the pre-training and fine-tuning code for second last row in Table 2 (without Instructor embedding) from the paper.
We also reuse the code from the [BEATs repository](https://github.com/microsoft/unilm/tree/master/beats) for this implementation.

# Training Details and Requirements
The training is divided into an AAC pre-training and a fine-tuning stage.
We do the AAC pre-training on [AudioCaps](https://aclanthology.org/N19-1011/) and [Clotho ChatGPT mixup](https://huggingface.co/datasets/slseanwu/clotho-chatgpt-mixup-50K) data.
Fine-tuning is performed only on the standard development set of Clotho_v2 dataset.


### Steps to run

1. Download AudioCaps and set the path to its root directory in db.sh (This recipe downloads clotho for you). Also download clotho mixup data (wav ids and captions) from [this repo](https://huggingface.co/datasets/slseanwu/clotho-chatgpt-mixup-50K) and set `CLOTHO_CHATGPT_MIXUP` in db.sh. The code will take care of mixing and creating the audio files.
2. Download the BEATs checkpoint: [BEATs_iter3+](https://onedrive.live.com/?authkey=%21AGXnEG4l3mlIzfA&id=6B83B49411CA81A7%2125960&cid=6B83B49411CA81A7&parId=root&parQt=sharedby&o=OneUp) and change the `beats_ckpt_path` path in `conf/beats_bart_pt.yaml`
3. Launch with `run.sh`


## Trained checkpoints
AAC Pre-trained model: https://huggingface.co/espnet/DCASE23.AudioCaptioning.PreTrained

Fine-tuned model: https://huggingface.co/espnet/DCASE23.AudioCaptioning.FineTuned


### GPU Time
AAC pre-training takes around ~4 hours on two A6000 gpus.
Fine-tuning, decoding and evalution takes ~1 hour on two A6000.
All the scripts above are setup with 2 GPUs but that can be changed with the `ngpu` argument. Please make sure to change the batch size accordingly (the provided setup requires 42.6GB GPU memory).


<!-- Generated by scripts/utils/show_asr_result.sh -->
# RESULTS
## Environments
- date: `Fri Nov 29 20:06:53 EST 2024`
- python version: `3.9.20 (main, Oct  3 2024, 07:27:41)  [GCC 11.2.0]`
- espnet version: `espnet 202409`
- pytorch version: `pytorch 2.4.0`
- Git hash: `65ea259e8effab5a43cdff87161a301dc0f20930`
  - Commit date: `Fri Nov 29 10:54:44 2024 -0500`

<!-- Copied from the output produced by local/evaluation.py -->
## exp/asr_ft
```
=====================================================
 Split: evaluation Evaluation over 1045 predictions.
=====================================================
 cider_d             : 0.46045061153488653
 spice               : 0.1345877073651595
 spider              : 0.297519159450023
 sbert_sim           : 0.5147198918907185
 fer                 : 0.019138755980861243
 fense               : 0.5048554784476391
 meteor              : 0.18487611036251259
 rouge_l             : 0.39408182293804006
 fer.add_tail_prob   : 0.046671394258737564
 fer.repeat_event_prob: 0.07274453341960907
 fer.repeat_adv_prob : 0.0019690715707838535
 fer.remove_conj_prob: 0.12462866306304932
 fer.remove_verb_prob: 0.22472403943538666
 fer.error_prob      : 0.34762266278266907
 spider_fl           : 0.2923702923078383
=====================================================
```
