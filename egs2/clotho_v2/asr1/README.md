# Clotho Audio Captioning RECIPE

This recipe implements the DCASE 2023 Automated Audio Captioning (AAC) task with a BEATs encoder BART decoder model on the Clotho_v2 dataset, very close to what is described in [this paper](https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf) and reusing part of code from the [original implementation](https://github.com/slSeanWU/beats-conformer-bart-audio-captioner?tab=readme-ov-file). 
We also reuse the code from the [BEATs repository](https://github.com/microsoft/unilm/tree/master/beats) for this implementation.

# Training Details and Requirements
The training is divided into an AAC pre-training and a fine-tuning stage.
We do the AAC pre-training on [AudioCaps](https://aclanthology.org/N19-1011/) and [Clotho ChatGPT mixup](https://huggingface.co/datasets/slseanwu/clotho-chatgpt-mixup-50K) data.
Fine-tuning is performed only on the standard development set of Clotho_v2 dataset.


### Steps to run
For running AAC pre-training
1. Download AudioCaps and set the path to its root directory in db.sh (This recipe downloads clotho for you). 
2. Download the BEATs checkpoint: [BEATs_iter3+](https://onedrive.live.com/?authkey=%21AGXnEG4l3mlIzfA&id=6B83B49411CA81A7%2125960&cid=6B83B49411CA81A7&parId=root&parQt=sharedby&o=OneUp) and change the `beats_ckpt_path` path in `conf/beats_bart_pt.yaml`
3. Launch with `run_pt.sh`

For running fine-tuning: 
1. Set the value of `pt_tag` in `run_ft.sh` and
2. Launch with `run_ft.sh`

For running inference and scoring on all checkpoints `run_inference.sh` can be used by just changing the `name_tag`.

## Trained checkpoints
AAC Pre-trained model: https://huggingface.co/shikhar7ssu/dcase23.aac.pt

Fine-tuned model: https://huggingface.co/shikhar7ssu/dcase23.aac


### GPU Time
AAC pre-training takes around ~12 hours on a single A40
Fine-tuning, decoding and evalution takes ~2.5 hours on a single A40.
All the scripts above are setup with 2 GPUs but that can be changed with the `ngpu` argument.


<!-- Generated by scripts/utils/show_asr_result.sh -->
# RESULTS
## Environments
- date: `Tue Nov  5 15:18:41 CST 2024`
- python version: `3.9.18 | packaged by conda-forge | (main, Dec 23 2023, 16:33:10)  [GCC 12.3.0]`
- espnet version: `espnet 202409`
- pytorch version: `pytorch 2.4.0`
- Git hash: `ac484495e6e3cbf58bd3d1175c607c3f05bf6898`
  - Commit date: `Tue Nov 5 09:24:48 2024 -0600`

<!-- Copied from the output produced by local/evaluation.py -->
## exp/asr_ft_lr5e-5.initfix.bigbatch512.lr2e-4.weighted12layers.20241103.145125
```
=====================================================
 Split: evaluation Evaluation over 1045 predictions.
=====================================================
 cider_d             : 0.40163786973038024
 spice               : 0.12328679455316437
 spider              : 0.2624623321417723
 sbert_sim           : 0.5094392538748004
 fer                 : 0.03827751196172249
 fense               : 0.4915462823319092
 meteor              : 0.17194480402701573
 rouge_l             : 0.3550182495603068
 fer.add_tail_prob   : 0.04409123957157135
 fer.repeat_event_prob: 0.07279406487941742
 fer.repeat_adv_prob : 0.0016611652681604028
 fer.remove_conj_prob: 0.10949967056512833
 fer.remove_verb_prob: 0.20523187518119812
 fer.error_prob      : 0.319917231798172
 spider_fl           : 0.2532289450419614
=====================================================
```