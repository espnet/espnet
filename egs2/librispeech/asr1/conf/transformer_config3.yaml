config: conf/transformer_config.yaml
print_config: false
log_level: INFO
max_epoch: 100
grad_clip: 5.0
accum_grad: 8
batch_size: 15
batch_type: seq
optim: adam
optim_conf:
    lr: 5.0
scheduler: noamlr
scheduler_conf:
    model_size: 512
    warmup_steps: 50000
e2e_conf:
    ctc_weight: 0.3
    ignore_id: -1
    lsm_weight: 0.1
    length_normalized_loss: false
    report_cer: false
    report_wer: false
    sym_space: <space>
    sym_blank: <blank>
use_preprocessor: true
token_type: bpe
bpemodel: data/token_list/bpe_unigram5000/model.model
non_linguistic_symbols: null
normalize: global_mvn
normalize_conf:
    stats_file: exp/asr_stats/train/feats_stats.npz
encoder: transformer
encoder_conf:
    output_size: 512
    attention_heads: 8
    num_blocks: 12
    linear_units: 2048
    dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: conv2d
decoder: transformer
decoder_conf:
    attention_heads: 8
    num_blocks: 6
    linear_units: 2048
    dropout_rate: 0.1
init: xavier_uniform
