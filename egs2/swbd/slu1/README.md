# Turn taking prediction model

- Training config: [conf/train_asr_whisper_turn_taking.yaml](conf/train_asr_whisper_turn_taking.yaml)
- Inference config: [conf/decode_asr_chunk.yaml](conf/decode_asr_chunk.yaml)
- Model link: [https://huggingface.co/espnet/Turn_taking_prediction_SWBD](https://huggingface.co/espnet/Turn_taking_prediction_SWBD)

## ROC_AUC

|dataset|Continuation|Backchannel|Turn change|Interruption|Silence|Overall|
|---|---|---|---|---|---|---|
|decode_asr_chunk_asr_model_valid.loss.ave/test|93.3|89.4|90.8|91.3|95.1|92.0|

# Guidance for training judge turn-taking model

- Complete data preparation setup with turn taking labels using the label annotation sequence defined in \cite{arora2025talking}
- Get turn taking label for each 40msec and downsample chunks from training and validation set such that there are roughly similar numbers of samples for each label class.
- Scripts for preparing data for switchboard provided in this recipe.
- Run the following command ``./run.sh --stop_stage 11``

# Guidance for using judge turn-taking model for inference
- Run the following command to run inference on switchboard: ``./run.sh --stage 12 --stop_stage 12``
- Evaluate judge turn taking model on switchboard using ROC_AUC values and Macro F1 values : ``./run.sh --stage 13 --stop_stage 13``
- To apply the model to your own test set, format your human-AI conversation dataset in the same structure as the Switchboard test set, then run: ``./run.sh --stage 12 --stop_stage 12 --test_sets ${own_test_set}`

# Guidance to benchmark audio FM using our proposed metric
- Collect human-audio FM conversation data
- Follow the previously described steps to obtain judge model's inference. The output will be structured as follows:
```
Each line contain likelihood predictions
generated by the
turn-taking model.
Each line begins with the name of an audio file,
followed by space-separated
likelihood predictions for each "chunk".
The likelihood prediction for each chunk
is an array storing the likelihood
of Label "L" at LabelIndex."L".value.
Example entry:
  moshi_audio_1.wav 0.52,0.25,0.03,0.04,0.16 0.20,0.62,0.02,0.02,0.14
```
- Extract turn-taking decisions from human and AI interactions using a speaker diarization model. The turn-taking decisions file should be in a **similar format** to `Test_Two_Channel_Label_Mono.csv`, created using Switchboard data preparation.i.e.:
```
Each line contain turn-taking decisions in Human AI conversation.
Each entry corresponds to 40msec
and is structured as:
  file_id,[start time],[end time],[Turn Taking Event],[Speaker Turn]
Example entry:
  moshi_audio_1,0.96,1.0,C,A
```
- Use the likelihood predictions from the **Judge Turn-Taking Model** along with the **actual turn-taking decisions** to compute the proposed evaluation metrics. An example of how metrics can be computed is shown in [../../TEMPLATE/asr1/pyscripts/utils/test_compute_turn_take_metrics.py](../../TEMPLATE/asr1/pyscripts/utils/test_compute_turn_take_metrics.py)

# Citing ESPnet

```BibTex

@inproceedings{
arora2025talking,
title={Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics},
author={Siddhant Arora and Zhiyun Lu and Chung-Cheng Chiu and Ruoming Pang and Shinji Watanabe},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=2e4ECh0ikn}
}

@inproceedings{watanabe2018espnet,
  author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson Yalta and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai},
  title={{ESPnet}: End-to-End Speech Processing Toolkit},
  year={2018},
  booktitle={Proceedings of Interspeech},
  pages={2207--2211},
  doi={10.21437/Interspeech.2018-1456},
  url={http://dx.doi.org/10.21437/Interspeech.2018-1456}
}

```
