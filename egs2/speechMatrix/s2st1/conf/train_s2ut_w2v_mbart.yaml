# =====================
# Network Architecture
# https://arxiv.org/pdf/2204.02967.pdf
# =====================

s2st_type: discrete_unit

# wav2vec2 takes in raw waveform
frontend: None
specaug: None

# Tgt feats extract
tgt_feats_extract: None

# Encoder
input_size: 1  # doesn't matter, we just need to force raw features
encoder: wav2vec2
encoder_conf:
  w2v_url: https://huggingface.co/tjysdsg/speechmatrix_w2v_mbart/resolve/main/wav2vec2_transformer_base.es.pth?download=true
  w2v_dir_path: exp/wav2vec2
  output_size: 1024
unused_parameters: true

# ASR CTC
asr_ctc: false

# ASR Decoder
# Predicts source discrete units
asr_decoder: transformer
asr_decoder_conf:
  input_layer: "embed"
  num_blocks: 2
  linear_units: 2048
  attention_heads: 4

# ST CTC
st_ctc: false

# Synthesizer
# Predicts target discrete units
synthesizer: unit_bart
synthesizer_conf:
  bart_url: https://huggingface.co/tjysdsg/speechmatrix_w2v_mbart/resolve/main/unit_mbart.pth?download=true
  dict_url: https://huggingface.co/tjysdsg/speechmatrix_w2v_mbart/resolve/main/dict.txt?download=true
  bart_dir_path: exp/unit_mbart

# =====================
# Training Related
# =====================

# Loss-related
# TODO(Jiyang): weights?
losses:
  - name: src_attn
    type: attention
    conf:
      weight: 1.0
      smoothing: 0.2
      padding_idx: -1
  - name: synthesis
    type: attention
    conf:
      weight: 1.0
      smoothing: 0.2
      padding_idx: -1

# Optimization
optim: adamw
accum_grad: 12
grad_clip: 1.0
grad_noise: false
optim_conf:
  lr: 0.0005
  eps: 1.0e-06
scheduler: warmuplr
scheduler_conf:
  warmup_steps: 30000

# Batching
batch_type: numel
batch_bins: 12000000
sort_in_batch: descending   # how to sort data in making batch
sort_batch: descending      # how to sort created batches

# Other Training Setting
use_amp: true
num_iters_per_epoch: 5000
max_epoch: 120
num_workers: 4
log_interval: null
seed: 0
num_att_plot: 0
best_model_criterion:
  - - valid
    - loss
    - min
  - - train
    - loss
    - min
keep_nbest_models: 10
