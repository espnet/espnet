lm_conf:
    nlayers: 2
    unit: 650
optim: sgd        # or adam
batch_type: folded
batch_size: 64   # batch size in LM training
max_epoch: 20      # if the data size is large, we can reduce this
patience: 3

best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 1