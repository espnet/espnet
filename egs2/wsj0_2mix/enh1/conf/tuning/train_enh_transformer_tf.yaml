optim: adam
init: xavier_uniform
max_epoch: 5
batch_type: folded
batch_size:  8
num_workers: 4
optim_conf:
    lr: 1.0e-03
    eps: 1.0e-08
    weight_decay: 1.0e-7
patience: 10
val_scheduler_criterion:
- valid
- loss
best_model_criterion:
-   - valid
    - si_snr
    - max
-   - valid
    - loss
    - min
keep_nbest_models: 1
scheduler: reducelronplateau
scheduler_conf:
    mode: min
    factor: 0.7
    patience: 1

encoder: stft
encoder_conf:
    n_fft: 512
    hop_length: 128
decoder: stft
decoder_conf:
    n_fft: 512
    hop_length: 128
separator: transformer
separator_conf:
    num_spk: 2
    adim: 384
    aheads: 4
    layers: 6
    linear_units: 1536
    nonlinear: relu
    positionwise_layer_type: linear
    positionwise_conv_kernel_size: 1
    normalize_before: False
    concat_after: False
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.1



# A list for criterions
# The overlall loss in the multi-task learning will be:
# loss = weight_1 * loss_1 + ... + weight_N * loss_N
# The default `weight` for each sub-loss is 1.0
criterions: 
  # The first criterion
  - name: mse 
    conf:
      compute_on_mask: True
      mask_type: PSM
    # the wrapper for the current criterion
    # PIT is widely used in the speech separation task
    wrapper: pit
    wrapper_conf:
      weight: 1.0
  # The second criterion
  - name: l1 
    conf:
      compute_on_mask: False
    wrapper: pit
    wrapper_conf:
      weight: 1.0
      # whether the permutation is computed independently
      # If false, it will use the permutation order from 
      # the pervious criterion (if it exists)
      independent_perm: False  
  # The third criterion
  - name: si_snr 
    conf:
      eps: 1.0e-7
    wrapper: pit
    wrapper_conf:
      weight: 5.0
      independent_perm: False