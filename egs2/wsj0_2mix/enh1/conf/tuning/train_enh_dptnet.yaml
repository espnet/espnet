optim: adam
init: none
max_epoch: 150
batch_type: folded
batch_size: 4 # batch_size 4 can be trained on 4 RTX 2080ti GPUs
iterator_type: chunk
chunk_length: 32000
num_workers: 4
unused_parameters: true
grad_clip: 5
optim_conf:
    lr: 4.0e-04
    eps: 1.0e-08
    weight_decay: 1.0e-05
patience: 10
val_scheduler_criterion:
- valid
- loss
best_model_criterion:
-   - valid
    - si_snr
    - max
-   - valid
    - loss
    - min
keep_nbest_models: 1
scheduler: warmupsteplr
scheduler_conf:
   # for WarmupLR
   warmup_steps: 4000
   # for StepLR
   steps_per_epoch: 6180
   step_size: 2
   gamma: 0.98

encoder: conv
encoder_conf:
    channel: 64
    kernel_size: 2
    stride: 1
decoder: conv
decoder_conf:
    channel: 64
    kernel_size: 2
    stride: 1
separator: dptnet
separator_conf:
    num_spk: 2
    layer: 6
    rnn_type: lstm
    bidirectional: True  # this is for the inter-block transformer
    feature_dim: 64
    unit: 128
    att_heads: 4
    dropout: 0.0
    activation: relu
    norm_type: gLN
    segment_size: 250
    nonlinear: relu

# A list for criterions
# The overlall loss in the multi-task learning will be:
# loss = weight_1 * loss_1 + ... + weight_N * loss_N
# The default `weight` for each sub-loss is 1.0
criterions: 
  # The first criterion
  - name: si_snr 
    conf:
      eps: 1.0e-7
    wrapper: pit
    wrapper_conf:
      weight: 1.0
      independent_perm: True
