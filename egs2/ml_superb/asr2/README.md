<!-- Generated by scripts/utils/show_asr_result.sh -->
# ML_SUPERB data download
This can be referred to the ML-SUPERB [recipe](https://github.com/espnet/espnet/blob/master/egs2/ml_superb/asr1)

[Download link](https://drive.google.com/file/d/1QYjl-7vflle__3AfuosAC5VJGiBDvEqz/view?usp=drive_link)

After download the dataset, please set the `MLSUPERB` in `db.sh`. The preparation will be automatically done in scripts for each tasks.

# Bitrate calculation

We show how to calculate the bitrate in the baseline system (16.49). For each eval set, it follows three steps:
1. Convert the input text to index (`token_int.rm.wavlm_large_21_km2000`). This is because the baseline uses subword modeling (e.g BPE), and it is represented in CJK characters. **Note: as long as the input text is in index, this step can be skipped.**
   ```
   1272-135031-0014        784 0 1867 1102 866 2042 1209 184 187 1209 768 3 17 111 940 202 69 1518 217 337 1984 121 2383 916 276 2470 1287 217 833 508 690 161 211 161 211 206 174 30 96 3 729
   ```
2. # Bitrate calculationpyscripts/utils/convert_token2json.py` generates json file for `vocab`, `tokens`, `ref_len`.
3. `pyscripts/utils/calculate_bitrate.py` computes the bitrate

Finally, the overall bitrate is computed by aggregating the results of all eval sets.

```bash
$ token_suffix="wavlm_large_21_km2000"
$ bpe_folder="data/token_list/src_bpe_unigram3000_rm_wavlm_large_21_km2000"
$ bitrate_dir="./bitrate"
$ for dset in dev_clean dev_other test_clean test_other test_1h; do
$   paste \
$     <(<dump/raw/${dset}/text.rm.${token_suffix} cut -d" " -f1) \
$     <(<dump/raw/${dset}/text.rm.${token_suffix} spm_encode --model=${bpe_folder}/bpe.model --output_format=id) \
$     > dump/raw/${dset}/token_int.rm.${token_suffix}

$   python pyscripts/utils/convert_token2json.py \
$     --vocab data/token_list/src_bpe_unigram3000_rm_${token_suffix}/tokens.txt \
$     --token dump/raw/${dset}/token_int.rm.${token_suffix} \
$     --ref_scp data/${dset}/wav.scp \
$     --result_dir "${bitrate_dir}/${dset}"

$   python pyscripts/utils/calculate_bitrate.py \
$     --vocab "${bitrate_dir}/${dset}"/vocab.json \
$     --tokens "${bitrate_dir}/${dset}"/tokens.json \
$     --reference_len "${bitrate_dir}/${dset}"/ref_len.scp \
$     --bitrate_details "${bitrate_dir}/${dset}"/details.txt
$ done

$ python - <<EOF
import numpy as np
bitrates=[]
bitrate_dir="${bitrate_dir}"
for dset in ["dev_clean", "dev_other", "test_clean", "test_other", "test_1h"]:
    with open(f"{bitrate_dir}/{dset}/details.txt", "r") as f:
        for line in f.readlines():
            lst = line.strip().split()
            bitrates.append(float(lst[1]))
print(np.round(np.mean(bitrates), 2))

EOF

$ 16.49
```
# Training Configuration

You can run `bash run.sh` to start experiment.

For duration, you can set 10min/1h to decide the amount of training data for each languages.
## Multilingual ASR
For multilingual experiment, you can set `multilingual` in `run.sh` to true.
## Monolingual ASR
For single language experiment, you can set `multilingual` in `run.sh` to false and provide the language code to `single_lang`.

# RESULTS
## Environments
- date: `Sat Aug 10 18:29:07 Asia 2024`
- python version: `3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0]`
- espnet version: `espnet 202402`
- pytorch version: `pytorch 1.12.1+cu113`
- Git hash: `e2276f2ab3f0239d198d37967983cbc620239456`
  - Commit date: `Wed Aug 7 06:09:43 2024 +0800`

## exp/asr_train_discrete_asr_e_branchformer1_1gpu_lr5e-4_warmup5k_multilingual_10min
### WER

|dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err|
|---|---|---|---|---|---|---|---|---|
|decode_ctc0.3_asr_model_valid.acc.ave/test_10min|24256|254316|2.7|94.1|3.2|70.1|167.4|100.0|

### CER

|dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err|
|---|---|---|---|---|---|---|---|---|
|decode_ctc0.3_asr_model_valid.acc.ave/test_10min|24256|1600215|30.4|56.3|13.4|18.5|88.1|100.0|
