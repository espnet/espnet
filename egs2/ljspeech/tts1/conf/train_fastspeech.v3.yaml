# This configuration is for ESPnet2 to train Fastspeech TTS. 
# Use 2 gpus.

##########################################################
#                 E2E-TTS MODEL SETTING                  #
##########################################################
tts: feedforwardtransformer
tts_conf:
  adim: 384
  aheads: 2
  elayers: 6
  eunits: 1536
  dlayers: 6
  dunits: 1536
  duration_predictor_layers: 2
  duration_predictor_chans: 256
  duration_predictor_kernel_size: 3
  positionwise_layer_type: "conv1d"
  positionwise_conv_kernel_size: 3
  postnet_layers: 5
  postnet_filts: 5
  postnet_chans: 256
  use_batch_norm: True
  use_scaled_pos_enc: True
  encoder_normalize_before: False
  decoder_normalize_before: False
  encoder_concat_after: False
  decoder_concat_after: False
  reduction_factor: 1
  spk_embed_dim: null                  # speaker embedding dimension
  spk_embed_integration_type: "add"
  initial_encoder_alpha: 1.0
  initial_decoder_alpha: 1.0
  transformer_enc_dropout_rate: 0.1
  transformer_enc_positional_dropout_rate: 0.1
  transformer_enc_attn_dropout_rate: 0.1
  transformer_dec_dropout_rate: 0.1
  transformer_dec_positional_dropout_rate: 0.1
  transformer_dec_attn_dropout_rate: 0.1
  transformer_enc_dec_attn_dropout_rate: 0.1
  postnet_dropout_rate: 0.5
  duration_predictor_dropout_rate: 0.1
  transfer_encoder_from_teacher: False
  transferred_encoder_module: "embed"
  use_masking: True

##########################################################
#                  OPTIMIZER SETTING                     #
##########################################################
optim: adam           # optimizer type
optim_conf:           # keyword arguments for selected optimizer
  lr: 1.0             # learning rate org: 1.0
  betas: [0.9, 0.98]  # adam betas
  eps: 1.0e-09        # epsilon
  weight_decay: 0.0   # weight decay coefficient
scheduler: noamlr     # The lr scheduler type
scheduler_conf:
  model_size: 384     # should equal to adim
  warmup_steps: 4000

##########################################################
#                OTHER TRAINING SETTING                  #
##########################################################
max_epoch: 10000               # number of epochs
grad_clip: 1.0              # gradient clipping norm
grad_noise: false           # whether to use gradient noise injection
accum_grad: 2               # gradient accumulation
batch_size: 128              # batch size, 64 per GPU
batch_type: seq             # how to make batch
sort_in_batch: descending   # how to sort data in making batch
sort_batch: descending      # how to sort created batches
num_workers: 1              # number of workers of data loader
train_dtype: float32        # dtype in training
log_interval: null          # log interval in iterations
keep_n_best_checkpoints: 10 # number of checkpoints to keep
seed: 0                     # random seed number
