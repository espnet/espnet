##########################################################
#                  CODEC MODEL SETTING                   #
##########################################################
codec: dac
codec_conf:
    sampling_rate: 24000

    # generator related
    generator_params:
        hidden_dim: 512
        codebook_dim: 512
        encdec_channels: 1
        encdec_n_filters: 32
        encdec_n_residual_layers: 3
        encdec_ratios: [8, 5, 4, 2]
        encdec_activation: Snake
        # encdec_activation_params:
        encdec_norm: weight_norm
        encdec_kernel_size: 7
        encdec_residual_kernel_size: 7
        encdec_last_kernel_size: 7
        encdec_dilation_base: 2
        encdec_causal: false
        encdec_pad_mode: reflect
        encdec_true_skip: false
        encdec_compress: 2
        encdec_lstm: 2
        decoder_trim_right_ratio: 1.0
        decoder_final_activation: null
        decoder_final_activation_params: null
        quantizer_n_q: 32
        quantizer_bins: 1024
        quantizer_decay: 0.99
        quantizer_kmeans_init: true
        quantizer_kmeans_iters: 50
        quantizer_threshold_ema_dead_code: 2
        quantizer_target_bandwidth: [2, 4, 8, 16, 32]
        quantizer_dropout: true

    # discriminator related
    discriminator_params:
        # multi-scale discriminator related
        scales: 3
        scale_downsample_pooling: AvgPool1d
        scale_downsample_pooling_params:
            kernel_size: 4
            stride: 2
            padding: 2
        scale_discriminator_params:
            in_channels: 1
            out_channels: 1
            kernel_sizes: [15, 41, 5, 3]
            channels: 128
            max_downsample_channels: 1024
            max_groups: 16
            bias: true
            downsample_scales: [2, 2, 4, 4, 1]
            nonlinear_activation: LeakyReLU
            nonlinear_activation_params:
                negative_slope: 0.1
        scale_follow_official_norm: false

        # multi scale multi band multi resolution discriminator related
        msmpmb_discriminator_params:
            rates: []
            sample_rate: 24000
            fft_sizes: [2048, 1024, 512]
            periods: [2, 3, 5, 7, 11]
            period_discriminator_params:
                in_channels: 1
                out_channels: 1
                kernel_sizes: [5, 3]
                channels: 32
                downsample_scales: [3, 3, 3, 3, 1]
                max_downsample_channels: 1024
                bias: True
                nonlinear_activation: "LeakyReLU"
                nonlinear_activation_params:
                    negative_slope: 0.1
                use_weight_norm: True
                use_spectral_norm: False
            band_discriminator_params:
                hop_factor: 0.25
                sample_rate: 24000
                bands:
                    - [0.0, 0.1]
                    - [0.1, 0.25]
                    - [0.25, 0.5]
                    - [0.5, 0.75]
                    - [0.75, 1.0]
                channel: 32

    # loss function related
    generator_adv_loss_params:
        average_by_discriminators: false # whether to average loss value by #discriminators
        loss_type: mse                   # loss type, "mse" or "hinge"
    discriminator_adv_loss_params:
        average_by_discriminators: false # whether to average loss value by #discriminators
        loss_type: mse                   # loss type, "mse" or "hinge"
    use_feat_match_loss: true            # whether to use feat match loss
    feat_match_loss_params:
        average_by_discriminators: false # whether to average loss value by #discriminators
        average_by_layers: false         # whether to average loss value by #layers of each discriminator
        include_final_outputs: true      # whether to include final outputs for loss calculation
    use_mel_loss: true     # whether to use mel-spectrogram loss
    mel_loss_params:
        range_start: 6
        range_end: 11
        window: hann        # window type
        n_mels: 80          # number of Mel basis
        fmin: 0             # minimum frequency for Mel basis
        fmax: null          # maximum frequency for Mel basis
        log_base: null      # null represent natural log
    lambda_quantization: 0.25       # loss scaling coefficient for codec quantization loss
    lambda_commit: 1.0      # loss scaling coefficient for codec commitment loss
    lambda_reconstruct: 1.0 # loss scaling coefficient for speech reconstruction loss
    lambda_adv: 1.0         # loss scaling coefficient for adversarial loss
    lambda_mel: 45.0        # loss scaling coefficient for Mel loss
    lambda_feat_match: 2.0 # loss scaling coefficient for feat match loss

    # others
    cache_generator_outputs: true # whether to cache generator outputs in the training

##########################################################
#            OPTIMIZER & SCHEDULER SETTING               #
##########################################################
# optimizer setting for generator
optim: adam # [CHANGED]
optim_conf:
    lr: 2.0e-4
    betas: [0.5, 0.9]
    eps: 1.0e-9
    weight_decay: 0.0
scheduler: exponentiallr
scheduler_conf:
    gamma: 0.999875
# optimizer setting for discriminator
optim2: adam
optim2_conf:
    lr: 2.0e-4
    betas: [0.5, 0.9]
    eps: 1.0e-9
    weight_decay: 0.0
scheduler2: exponentiallr
scheduler2_conf:
    gamma: 0.999875
generator_first: true # whether to start updating generator first


##########################################################
#                OTHER TRAINING SETTING                  #
##########################################################
num_iters_per_epoch: 5000 # number of iterations per epoch
max_epoch: 120              # number of epochs
accum_grad: 1             # gradient accumulation
batch_size: 8             # CHANGED
batch_type: unsorted      # how to make batch


iterator_type: chunk
chunk_length: 24000 # CHANGED
num_cache_chunks: 64

grad_clip: -1             # gradient clipping norm
grad_noise: false         # whether to use gradient noise injection
sort_in_batch: descending # how to sort data in making batch
sort_batch: descending    # how to sort created batches
num_workers: 1            # number of workers of data loader
use_amp: false            # whether to use pytorch amp
log_interval: 50          # log interval in iterations
keep_nbest_models: 5      # number of models to keep
num_att_plot: 0           # number of attention figures to be saved in every check
seed: 777                 # random seed number
patience: null            # patience for early stopping
unused_parameters: true   # needed for multi gpu case
best_model_criterion:     # criterion to save the best models
-   - valid
    - mel_loss
    - min
-   - train
    - mel_loss
    - min
-   - train
    - total_count
    - max
cudnn_deterministic: false # setting to false accelerates the training speed but makes it non-deterministic
                           # in the case of GAN-TTS training, we strongly recommend setting to false
cudnn_benchmark: false     # setting to true might acdelerate the training speed but sometimes decrease it
                           # therefore, we set to false as a default (recommend trying both cases)
