##########################################################
#                  TTS MODEL SETTING                     #
##########################################################
tts: vits
tts_conf:
    # generator related
    generator_type: vits_generator
    generator_params:
        hidden_channels: 2
        spks: -1
        global_channels: -1
        segment_size: 4
        text_encoder_attention_heads: 2
        text_encoder_ffn_expand: 1
        text_encoder_blocks: 1
        text_encoder_positionwise_layer_type: "conv1d"
        text_encoder_positionwise_conv_kernel_size: 3
        text_encoder_positional_encoding_layer_type: "rel_pos"
        text_encoder_self_attention_layer_type: "rel_selfattn"
        text_encoder_activation_type: "swish"
        text_encoder_normalize_before: true
        text_encoder_dropout_rate: 0.1
        text_encoder_positional_dropout_rate: 0.0
        text_encoder_attention_dropout_rate: 0.1
        use_macaron_style_in_text_encoder: true
        # NOTE(kan-bayashi): Conformer conv requires BatchNorm1d which causes
        #   errors when multiple GPUs in pytorch 1.7.1. Therefore, we disable
        #   it as a default. We need to consider the alternative normalization
        #   or different version pytorch may solve this issue.
        use_conformer_conv_in_text_encoder: false
        text_encoder_conformer_kernel_size: -1
        decoder_kernel_size: 3
        decoder_channels: 2
        decoder_upsample_scales: [8, 8, 2, 2]
        decoder_upsample_kernel_sizes: [16, 16, 4, 4]
        decoder_resblock_kernel_sizes: [3, 3, 3]
        decoder_resblock_dilations: [[1, 1, 1], [1, 1, 1], [1, 1, 1]]
        use_weight_norm_in_decoder: true
        posterior_encoder_kernel_size: 3
        posterior_encoder_layers: 2
        posterior_encoder_stacks: 1
        posterior_encoder_base_dilation: 1
        posterior_encoder_dropout_rate: 0.0
        use_weight_norm_in_posterior_encoder: true
        flow_flows: 2
        flow_kernel_size: 3
        flow_base_dilation: 1
        flow_layers: 1
        flow_dropout_rate: 0.0
        use_weight_norm_in_flow: true
        use_only_mean_in_flow: true
        stochastic_duration_predictor_kernel_size: 3
        stochastic_duration_predictor_dropout_rate: 0.5
        stochastic_duration_predictor_flows: 1
        stochastic_duration_predictor_dds_conv_layers: 1
    # discriminator related
    discriminator_type: hifigan_multi_scale_multi_period_discriminator
    discriminator_params:
        scales: 1
        scale_downsample_pooling: "AvgPool1d"
        scale_downsample_pooling_params:
            kernel_size: 4
            stride: 2
            padding: 2
        scale_discriminator_params:
            in_channels: 1
            out_channels: 1
            kernel_sizes: [3, 3]
            channels: 2
            max_downsample_channels: 4
            max_groups: 16
            bias: True
            downsample_scales: [2, 2, 1]
            nonlinear_activation: "LeakyReLU"
            nonlinear_activation_params:
                negative_slope: 0.1
            use_weight_norm: True
            use_spectral_norm: False
        follow_official_norm: False
        periods: [2, 3]
        period_discriminator_params:
            in_channels: 1
            out_channels: 1
            kernel_sizes: [5, 3]
            channels: 2
            downsample_scales: [3, 1]
            max_downsample_channels: 4
            bias: True
            nonlinear_activation: "LeakyReLU"
            nonlinear_activation_params:
                negative_slope: 0.1
            use_weight_norm: True
            use_spectral_norm: False

##########################################################
#                OTHER TRAINING SETTING                  #
##########################################################
num_iters_per_epoch: 1
max_epoch: 1
batch_type: folded
batch_size: 2
