import torch
import torch.nn as nn
from fairseq.models.wav2vec.wav2vec2 import TransformerSentenceEncoderLayer

from espnet2.asr.frontend.adapter_utils.adapters.adapter import Adapter


class AdapterTransformerSentenceEncoderLayer(TransformerSentenceEncoderLayer):
    """
    Modified fairseq's TransformerSentenceEncoderLayer for wav2vec2 with adapters.
    https://github.com/facebookresearch/fairseq/blob/06c65c8297396959c025bf61aaeaa6f4ace9940e/fairseq/models/wav2vec/wav2vec2.py#L1175
    """

    def __init__(
        self,
        embedding_dim: float = 768,
        ffn_embedding_dim: float = 3072,
        num_attention_heads: int = 8,
        dropout: float = 0.1,
        attention_dropout: float = 0.1,
        activation_dropout: float = 0.1,
        activation_fn: str = "relu",
        layer_norm_first: bool = False,
        adapter_down_dim: int = 192,
    ) -> None:
        super().__init__(
            embedding_dim,
            ffn_embedding_dim,
            num_attention_heads,
            dropout,
            attention_dropout,
            activation_dropout,
            activation_fn,
            layer_norm_first,
        )
        self.adapter1 = Adapter(embedding_dim, adapter_down_dim)
        self.adapter2 = Adapter(embedding_dim, adapter_down_dim)

    def forward(
        self,
        x: torch.Tensor,
        self_attn_mask: torch.Tensor = None,
        self_attn_padding_mask: torch.Tensor = None,
        need_weights: bool = False,
        att_args=None,
    ):
        residual = x

        if self.layer_norm_first:
            x = self.self_attn_layer_norm(x)
            x, attn = self.self_attn(
                query=x,
                key=x,
                value=x,
                key_padding_mask=self_attn_padding_mask,
                attn_mask=self_attn_mask,
                need_weights=False,
            )
            x = self.adapter1(x)

            x = self.dropout1(x)
            x = residual + x

            residual = x
            x = self.final_layer_norm(x)
            x = self.activation_fn(self.fc1(x))
            x = self.dropout2(x)
            x = self.fc2(x)

            x = self.adapter2(x)

            layer_result = x

            x = self.dropout3(x)
            x = residual + x
        else:
            x, attn = self.self_attn(
                query=x,
                key=x,
                value=x,
                key_padding_mask=self_attn_padding_mask,
                need_weights=False,
            )
            x = self.adapter1(x)

            x = self.dropout1(x)
            x = residual + x

            x = self.self_attn_layer_norm(x)

            residual = x
            x = self.activation_fn(self.fc1(x))
            x = self.dropout2(x)
            x = self.fc2(x)

            x = self.adapter2(x)

            layer_result = x

            x = self.dropout3(x)
            x = residual + x
            x = self.final_layer_norm(x)

        return x, (attn, layer_result)
