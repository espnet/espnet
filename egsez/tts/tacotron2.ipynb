{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS demo for ESPnet-Easy!\n",
    "In this notebook, we will demonstrate how to train an Text to Speech (TTS) model using the LJSpeech dataset. Basic flow of data preparation and training is the same with ASR.\n",
    "\n",
    "Before proceeding, please ensure that you have already downloaded the LJSpeech dataset from [here](https://keithito.com/LJ-Speech-Dataset/) and have placed the data in a directory of your choice. In this notebook, we assume that you have stored the dataset in the `/hdd/dataset/` directory. If your dataset is located in a different directory, please make sure to replace `/hdd/dataset/` with the actual path to your dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "First, let's create dump files!  \n",
    "The format of the dump files is the same as the ASR dump files.\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"data_name\": [\"dump_file_name\", \"dump_format\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import espnetez as ez\n",
    "\n",
    "\n",
    "DUMP_DIR = \"./dump/ljspeech\"\n",
    "LJS_DIRS = \"/hdd/database/LJSpeech-1.1\"\n",
    "data_info = {\n",
    "    \"speech\": [\"wav.scp\", \"sound\"],\n",
    "    \"text\": [\"text\", \"text\"],\n",
    "}\n",
    "\n",
    "# prepare dataset\n",
    "train_dataset = {}\n",
    "test_dataset = {}\n",
    "with open(os.path.join(LJS_DIRS, \"metadata.csv\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for t in lines[:-50]:\n",
    "    k, _, text = t.strip().split(\"|\", 2)\n",
    "    train_dataset[k] = dict(text=text)\n",
    "\n",
    "for t in lines[-50:]:\n",
    "    k, _, text = t.strip().split(\"|\", 2)\n",
    "    test_dataset[k] = dict(text=text)\n",
    "\n",
    "\n",
    "# set speech path\n",
    "for k in train_dataset.keys():\n",
    "    train_dataset[k]['speech'] = os.path.join(LJS_DIRS, \"wavs\", f\"{k}.wav\")\n",
    "\n",
    "for k in test_dataset.keys():\n",
    "    test_dataset[k]['speech'] = os.path.join(LJS_DIRS, \"wavs\", f\"{k}.wav\")\n",
    "\n",
    "\n",
    "train_dir = os.path.join(DUMP_DIR, \"train\")\n",
    "test_dir = os.path.join(DUMP_DIR, \"test\")\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    os.makedirs(train_dir)\n",
    "\n",
    "if not os.path.exists(test_dir):\n",
    "    os.makedirs(test_dir)\n",
    "\n",
    "ez.data.create_dump_file(train_dir, train_dataset, data_info)\n",
    "ez.data.create_dump_file(test_dir, test_dataset, data_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate token list\n",
    "\n",
    "To generate a token list, we need to run `espnet2.bin.tokenize_text` script.\n",
    "ESPnet-Easy has a wrapper function for this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training texts from the training data\n",
    "# you can select several datasets to train sentencepiece.\n",
    "ez.preprocess.prepare_sentences([\"dump/ljspeech/train/text\"], \"data/\")\n",
    "ez.preprocess.tokenize(\n",
    "    input=\"data/train.txt\",\n",
    "    output=\"data/tokenized.txt\",\n",
    "    token_type=\"phn\",\n",
    "    cleaner=\"tacotron\",\n",
    "    g2p=\"g2p_en\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To prepare the stats file before training, you can execute the `collect_stats` method. This step is required before the training process and ensuring accurate statistics for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_DIR = \"exp/train_tts\"\n",
    "STATS_DIR = \"exp/stats\"\n",
    "\n",
    "# load config\n",
    "training_config = ez.config.from_yaml(\n",
    "    \"tts\",\n",
    "    \"tacotron2.yaml\",\n",
    ")\n",
    "with open(\"data/tokenized.txt\", \"r\") as f:\n",
    "    training_config[\"token_list\"] = [t.replace(\"\\n\", \"\") for t in f.readlines()]\n",
    "\n",
    "# Define the Trainer class\n",
    "trainer = ez.Trainer(\n",
    "    task='tts',\n",
    "    train_config=training_config,\n",
    "    train_dump_dir=\"dump/ljspeech/train\",\n",
    "    valid_dump_dir=\"dump/ljspeech/test\",\n",
    "    data_info=data_info,\n",
    "    output_dir=EXP_DIR,\n",
    "    stats_dir=STATS_DIR,\n",
    "    ngpu=1,\n",
    ")\n",
    "trainer.collect_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to begin the training process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "You can just use the inference API of the ESPnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from espnet2.bin.tts_inference import Text2Speech\n",
    "\n",
    "m = Text2Speech(\n",
    "    \"./exp/finetune/config.yaml\",\n",
    "\t\"./exp/finetune/valid.loss.ave.pth\",\n",
    ")\n",
    "\n",
    "text = \"hello world\"\n",
    "output = m(text)['wav']\n",
    "sf.write(\"output.wav\", output, 16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
