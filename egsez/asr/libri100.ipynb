{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample demo for using ESPnet-Easy!\n",
    "In this notebook, we will train an ASR model on Librispeech-100 dataset.\n",
    "\n",
    "This notebook assumes that you have already downloaded the Librispeech-100 dataset from [OpenSLR](https://www.openslr.org/12), and placed the data in `/hdd/dataset/` directory.\n",
    "Please replace the `/hdd/dataset/` directory with your own path."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "This notebook follows an data preparation steps written in `asr.sh`. First, we will create a dump file to store the data id, audio path, and the transcription.\n",
    "\n",
    "ESPnet-Easy accepts several types of datasets, including:\n",
    "- Dictionary-based dataset with the following structure:\n",
    "  ```python\n",
    "  {\n",
    "    \"data_id\": {\n",
    "        \"speech\": path_to_speech_file,\n",
    "        \"text\": transcription\n",
    "    },\n",
    "  }\n",
    "  ```\n",
    "- List of datasets with the following structure:\n",
    "  ```python\n",
    "  [\n",
    "    {\n",
    "        \"speech\": path_to_speech_file,\n",
    "        \"text\": transcription\n",
    "    },\n",
    "  ]\n",
    "  ```\n",
    "\n",
    "If you want to use a dictionary-based dataset, each `data_id` must be unique.\n",
    "ESPnet-Easy also accepts a dump file already created by `asr.sh`. But in this notebook, we will craete dump file from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to install espnet if you don't have it\n",
    "!pwd\n",
    "!pip install -U ../../\n",
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we define a function to create a dataset in dictionary format.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def create_dataset(data_dir):\n",
    "    dataset = {}\n",
    "    for chapter in glob.glob(os.path.join(data_dir, \"*/*\")):\n",
    "        text_file = glob.glob(os.path.join(chapter, \"*.txt\"))[0]\n",
    "\n",
    "        with open(text_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        ids_text = {\n",
    "            line.split(\" \")[0]: line.split(\" \", maxsplit=1)[1].replace(\"\\n\", \"\")\n",
    "            for line in lines\n",
    "        }\n",
    "        audio_files = glob.glob(os.path.join(chapter, \"*.flac\"))\n",
    "        for audio_file in audio_files:\n",
    "            audio_id = os.path.basename(audio_file)[: -len(\".flac\")]\n",
    "            dataset[audio_id] = {\"speech\": audio_file, \"text\": ids_text[audio_id]}\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you create dump files, you need to specify the data types for each inputs. In this notebook, we use `speech` as the sound data and `text` as the text data. So we define the `data_inputs` dictionary to store these information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = {\n",
    "    \"speech\": {\"file\": \"wav.scp\", \"type\": \"sound\"},\n",
    "    \"text\": {\"file\": \"text\", \"type\": \"text\"},\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `file` key denotes the file name for the dump file, and the `type` key denotes the type of the inputs.\n",
    "The `type` must be one of the data type listed in the [DATA_TYPES](https://github.com/espnet/espnet/blob/1409d89d1ca33417a7f57e4cfa77925a4f00cc3f/espnet2/train/dataset.py#L208).\n",
    "\n",
    "Then we generate the dump files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import espnetez as ez\n",
    "\n",
    "# Then create the dump files\n",
    "DUMP_DIR = \"./dump/libri100\"\n",
    "LIBRI_100_DIRS = [\n",
    "    [\"/hdd/database/librispeech-100/LibriSpeech/train-clean-100\", \"train\"],\n",
    "    [\"/hdd/database/librispeech-100/LibriSpeech/dev-clean\", \"dev-clean\"],\n",
    "    [\"/hdd/database/librispeech-100/LibriSpeech/dev-other\", \"dev-other\"],\n",
    "]\n",
    "\n",
    "for d, n in LIBRI_100_DIRS:\n",
    "    dump_dir = os.path.join(DUMP_DIR, n)\n",
    "    if not os.path.exists(dump_dir):\n",
    "        os.makedirs(dump_dir)\n",
    "\n",
    "    dataset = create_dataset(d)\n",
    "    ez.data.create_dump_file(dump_dir, dataset, data_inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dev files, we have `dev-clean` and `dev-other` directories.\n",
    "We can join them to get one dev dataset, by using `ez.data.join_dumps` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ez.data.join_dumps(\n",
    "    [\"dump/libri100/dev-clean\", \"dump/libri100/dev-other\"], \"dump/libri100/dev\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have dataset files in the `dump` directory.\n",
    "It looks like this:\n",
    "\n",
    "wav.scp\n",
    "```\n",
    "1255-138279-0008 /hdd/database/librispeech-100/LibriSpeech/dev-other/1255/138279/1255-138279-0008.flac\n",
    "1255-138279-0022 /hdd/database/librispeech-100/LibriSpeech/dev-other/1255/138279/1255-138279-0022.flac\n",
    "```\n",
    "\n",
    "text\n",
    "```\n",
    "1255-138279-0008 TWO THREE\n",
    "1255-138279-0022 IF I SAID SO OF COURSE I WILL\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sentencepiece model\n",
    "\n",
    "Next, we will train a sentencepiece model. We need text file for training, so let's create a training file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training texts from the training data\n",
    "# you can select several datasets to train sentencepiece.\n",
    "ez.preprocess.prepare_sentences([\"dump/libri100/train/text\"], \"dump/spm\")\n",
    "\n",
    "ez.preprocess.train_sentencepiece(\n",
    "    \"dump/spm/train.txt\",\n",
    "    \"data/bpemodel\",\n",
    "    vocab_size=5000,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we have finished the data preparation. Now we will configure training process. We can use the configuration files already created by the ESPnet contributers.\n",
    "\n",
    "To use the configuration file, we need to create the yaml file on your local machine. For example, I will use this [e-branchformer config](train_asr_e_branchformer_size256_mlp1024_linear1024_e12_mactrue_edrop0.0_ddrop0.0.yaml).\n",
    "\n",
    "I changed the `batch_bins` parameter from `16000000` to `1600000`, to train on my GPU (RTX2080ti)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing I changed from the original ESPnet in this notebook is the way we define the token list.\n",
    "\n",
    "The original ESPnet configuration defines the token list by giving all tokens in the yaml file, but in the ESPnet-Easy, we just give the path to the vocab file.\n",
    "So the configuration file for preprocessing looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "use_preprocessor: true\n",
    "\n",
    "token_type: bpe\n",
    "bpemodel: data/bpemodel/bpe.model\n",
    "rir_scp: null\n",
    "rir_apply_prob: 1.0\n",
    "noise_scp: null\n",
    "noise_apply_prob: 1.0\n",
    "noise_db_range: '13_15'\n",
    "speech_volume_normalize: null\n",
    "non_linguistic_symbols: null\n",
    "\n",
    "cleaner: null\n",
    "g2p: null\n",
    "preprocessor: default\n",
    "preprocessor_conf:\n",
    "  speech_name: speech\n",
    "  text_name: text\n",
    "\n",
    "token_list: data/bpemodel/tokens.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can start training the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_DIR = \"exp/train_asr_branchformer_e24_amp\"\n",
    "STATS_DIR = \"exp/stats_all\"\n",
    "\n",
    "training_config = ez.config.from_yaml(\n",
    "    \"asr\",\n",
    "    \"train_asr_e_branchformer_size256_mlp1024_linear1024_e12_mactrue_edrop0.0_ddrop0.0.yaml\",\n",
    ")\n",
    "# Need to update the configuration\n",
    "preprocessor_config = ez.utils.load_yaml(\"preprocess.yaml\")\n",
    "training_config.update(preprocessor_config)\n",
    "\n",
    "# replace token list\n",
    "with open(preprocessor_config[\"token_list\"], \"r\") as f:\n",
    "    training_config[\"token_list\"] = [t.replace(\"\\n\", \"\") for t in f.readlines()]\n",
    "\n",
    "trainer = ez.trainer.Trainer(\n",
    "    \"asr\",\n",
    "    os.path.join(DUMP_DIR, \"train\"),\n",
    "    os.path.join(DUMP_DIR, \"dev\"),\n",
    "    EXP_DIR,\n",
    "    data_inputs,\n",
    "    STATS_DIR,\n",
    "    training_config,\n",
    "    ngpu=1,  # you can also update configuration here\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
