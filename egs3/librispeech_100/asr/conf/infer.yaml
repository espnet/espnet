# Inference/decoding configuration for LibriSpeech 100h.
# Keeps only inference pieces and mirrors the training paths.

# Minimal runtime scaffold; inference uses Speech2Text, so no task needed.
num_device: 1
num_nodes: 1

# Path scaffold relative to the recipe root.
recipe_dir: .
data_dir: ${recipe_dir}/data
exp_tag: librispeech100_asr_eval
exp_dir: ${recipe_dir}/exp/${exp_tag}
stats_dir: ${recipe_dir}/exp/stats
decode_dir: ${exp_dir}/decode

# Dataset definition for test splits only.
dataset:
  dataset_module: src/dataset.py
  test:
    - name: test-clean
      dataset:
        split: test-clean
    - name: test-other
      dataset:
        split: test-other

# Parallel execution used during data loading / scoring (defaults to local).
parallel:
  env: local
  n_workers: 1

# DataLoader defaults; small batch/numel settings keep decoding light.
dataloader:
  collate_fn:
    _target_: espnet2.train.collate_fn.CommonCollateFn
    int_pad_value: -1
  train:
    multiple_iterator: false
    num_shards: 1
    iter_factory:
      _target_: espnet2.iterators.sequence_iter_factory.SequenceIterFactory
      shuffle: true
      collate_fn: ${dataloader.collate_fn}
      batches:
        type: numel
        shape_files:
          - ${stats_dir}/train/feats_shape
        batch_size: 4
        batch_bins: 4000000
  valid:
    multiple_iterator: false
    num_shards: 1
    iter_factory:
      _target_: espnet2.iterators.sequence_iter_factory.SequenceIterFactory
      shuffle: false
      collate_fn: ${dataloader.collate_fn}
      batches:
        type: ${dataloader.train.iter_factory.batches.type}
        shape_files:
          - ${stats_dir}/valid/feats_shape
        batch_size: ${dataloader.train.iter_factory.batches.batch_size}
        batch_bins: ${dataloader.train.iter_factory.batches.batch_bins}

# Inference model: load the trained config/checkpoint and beam search params.
model:
  _target_: espnet2.bin.asr_inference.Speech2Text
  asr_train_config: ${exp_dir}/config.yaml
  asr_model_file: ${exp_dir}/last.ckpt
  beam_size: 1
  ctc_weight: 0.3

# Tokenizer config mirrors the training side for BPE-5k.
tokenizer:
  vocab_size: 5000
  character_coverage: 1.0
  model_type: bpe
  save_path: ${data_dir}/${tokenizer.model_type}_${tokenizer.vocab_size}
