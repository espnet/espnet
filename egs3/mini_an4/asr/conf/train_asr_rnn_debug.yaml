# This is a debug config for CI
# Mini AN4 ASR training config for ESPnet3 integration tests.

num_device: 1
num_nodes: 1
task: espnet3.systems.asr.task.ASRTask

recipe_dir: .
data_dir: ${recipe_dir}/data
exp_tag: train_asr_rnn_debug
exp_dir: ${recipe_dir}/exp/${exp_tag}
stats_dir: ${recipe_dir}/exp/stats
decode_dir: ${exp_dir}/decode
dataset_dir: ${data_dir}/mini_an4

create_dataset:
  func: src.create_dataset.create_dataset
  dataset_dir: ${dataset_dir}
  archive_path: ${recipe_dir}/../../egs2/mini_an4/asr1/downloads.tar.gz

dataset:
  _target_: espnet3.components.data.data_organizer.DataOrganizer
  train:
    - name: train_nodev
      dataset:
        _target_: src.dataset.MiniAN4Dataset
        manifest_path: ${dataset_dir}/manifest/train_nodev.tsv
  valid:
    - name: train_dev
      dataset:
        _target_: src.dataset.MiniAN4Dataset
        manifest_path: ${dataset_dir}/manifest/train_dev.tsv
  preprocessor:
    _target_: espnet2.train.preprocessor.CommonPreprocessor
    _convert_: all
    fs: 16000
    train: true
    data_aug_effects:   # no need to set the "sample_rate" argument for each effect here
        - [0.1, "contrast", {"enhancement_amount": 75.0}]
        - [0.1, "highpass", {"cutoff_freq": 5000, "Q": 0.707}]
        - [0.1, "equalization", {"center_freq": 1000, "gain": 0, "Q": 0.707}]
        - - 0.1
          - - [0.3, "speed_perturb", {"factor": 0.9}]
            - [0.3, "speed_perturb", {"factor": 1.1}]
            - [0.3, "speed_perturb", {"factor": 1.3}]
    data_aug_num: [1, 4]
    data_aug_prob: 1.0
    token_type: bpe
    token_list: ${tokenizer.save_path}/tokens.txt
    bpemodel: ${tokenizer.save_path}/bpe.model


parallel:
  env: local
  n_workers: 1

dataloader:
  collate_fn:
    _target_: espnet2.train.collate_fn.CommonCollateFn
    int_pad_value: -1
  train:
    multiple_iterator: false
    num_shards: 1
    iter_factory:
      _target_: espnet2.iterators.sequence_iter_factory.SequenceIterFactory
      shuffle: true
      collate_fn: ${dataloader.collate_fn}
      num_workers: 0
      batches:
        type: sorted
        shape_files:
          - ${stats_dir}/train/feats_shape
        batch_size: 2
        batch_bins: 200000
  valid:
    multiple_iterator: false
    num_shards: 1
    iter_factory:
      _target_: espnet2.iterators.sequence_iter_factory.SequenceIterFactory
      shuffle: false
      collate_fn: ${dataloader.collate_fn}
      batches:
        type: ${dataloader.train.iter_factory.batches.type}
        shape_files:
          - ${stats_dir}/valid/feats_shape
        batch_size: ${dataloader.train.iter_factory.batches.batch_size}
        batch_bins: ${dataloader.train.iter_factory.batches.batch_bins}



optim:
  _target_: torch.optim.Adam
  lr: 0.001
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: min
  factor: 0.5
  patience: 1

val_scheduler_criterion: valid/loss

best_model_criterion:
  - - valid/loss
    - 1
    - min

trainer:
  accelerator: auto
  devices: ${num_device}
  num_nodes: ${num_nodes}
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  gradient_clip_val: 1.0
  log_every_n_steps: 1
  max_epochs: 1
  limit_train_batches: 1
  limit_val_batches: 1
  precision: 32

  logger:
    - _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ${exp_dir}/tensorboard
      name: tb_logger

  strategy: auto


tokenizer:
  vocab_size: 30
  character_coverage: 1.0
  model_type: bpe
  save_path: ${data_dir}/${tokenizer.model_type}_${tokenizer.vocab_size}
  text_builder:
    func: src.tokenizer.gather_training_text
    manifest_path: ${dataset_dir}/manifest/train_nodev.tsv


model:
  vocab_size: ${tokenizer.vocab_size}
  token_list: ${tokenizer.save_path}/tokens.txt

  encoder: vgg_rnn
  encoder_conf:
    num_layers: 1
    hidden_size: 2
    output_size: 2

  decoder: rnn
  decoder_conf:
    hidden_size: 2

  normalize: utterance_mvn
  normalize_conf: {}

  model_conf:
    ctc_weight: 0.3
    lsm_weight: 0.1
    length_normalized_loss: false

  frontend: default
  frontend_conf:
    n_fft: 512
    win_length: 400
    hop_length: 160
