# This is a debug config for CI
# Mini AN4 ASR training config for ESPnet3 integration tests.

init: xavier_uniform


num_device: 1
num_nodes: 1
task: espnet3.systems.asr.task.ASRTask

recipe_dir: .
data_dir: ${recipe_dir}/data
exp_tag: train_asr_transducer_debug
exp_dir: ${recipe_dir}/exp/${exp_tag}
stats_dir: ${recipe_dir}/exp/stats
decode_dir: ${exp_dir}/decode
dataset_dir: ${data_dir}/mini_an4

create_dataset:
  func: src.create_dataset.create_dataset
  dataset_dir: ${dataset_dir}
  archive_path: ${recipe_dir}/../../egs2/mini_an4/asr1/downloads.tar.gz

dataset:
  _target_: espnet3.components.data.data_organizer.DataOrganizer
  train:
    - name: train_nodev
      dataset:
        _target_: src.dataset.MiniAN4Dataset
        manifest_path: ${dataset_dir}/manifest/train_nodev.tsv
  valid:
    - name: train_dev
      dataset:
        _target_: src.dataset.MiniAN4Dataset
        manifest_path: ${dataset_dir}/manifest/train_dev.tsv
  preprocessor:
    _target_: espnet2.train.preprocessor.CommonPreprocessor
    train: true
    token_type: bpe
    token_list: ${tokenizer.save_path}/tokens.txt
    bpemodel: ${tokenizer.save_path}/bpe.model
    text_cleaner:

parallel:
  env: local
  n_workers: 1

dataloader:
  collate_fn:
    _target_: espnet2.train.collate_fn.CommonCollateFn
    int_pad_value: -1
  train:
    multiple_iterator: false
    num_shards: 1
    iter_factory:
      _target_: espnet2.iterators.sequence_iter_factory.SequenceIterFactory
      shuffle: true
      collate_fn: ${dataloader.collate_fn}
      num_workers: 0
      batches:
        type: unsorted
        shape_files:
          - ${stats_dir}/train/feats_shape
        batch_size: 2
        batch_bins: 200000
  valid:
    multiple_iterator: false
    num_shards: 1
    iter_factory:
      _target_: espnet2.iterators.sequence_iter_factory.SequenceIterFactory
      shuffle: false
      collate_fn: ${dataloader.collate_fn}
      batches:
        type: ${dataloader.train.iter_factory.batches.type}
        shape_files:
          - ${stats_dir}/valid/feats_shape
        batch_size: ${dataloader.train.iter_factory.batches.batch_size}
        batch_bins: ${dataloader.train.iter_factory.batches.batch_bins}

optim:
  _target_: torch.optim.Adam
  lr: 0.005

scheduler:
  _target_: espnet2.schedulers.warmup_lr.WarmupLR
  warmup_steps: 100

best_model_criterion:
  - - valid/loss
    - 1
    - min

trainer:
  accelerator: auto
  devices: ${num_device}
  num_nodes: ${num_nodes}
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  gradient_clip_val: 1.0
  log_every_n_steps: 1
  max_epochs: 1
  limit_train_batches: 1
  limit_val_batches: 1
  precision: 32

  logger:
    - _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ${exp_dir}/tensorboard
      name: tb_logger

  strategy: auto


tokenizer:
  vocab_size: 30
  character_coverage: 1.0
  model_type: bpe
  save_path: ${data_dir}/${tokenizer.model_type}_${tokenizer.vocab_size}
  text_builder:
    func: src.tokenizer.gather_training_text
    manifest_path: ${dataset_dir}/manifest/train_nodev.tsv


model:
  vocab_size: ${tokenizer.vocab_size}
  token_list: ${tokenizer.save_path}/tokens.txt

  encoder: transformer
  encoder_conf:
    output_size: 2
    attention_heads: 2
    linear_units: 2
    num_blocks: 2
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: conv1d2
    normalize_before: true

  decoder: transducer
  decoder_conf:
    rnn_type: lstm
    num_layers: 1  # Decoder Layers
    hidden_size: 4  # Decoder dim
    dropout: 0.1
    dropout_embed: 0.2

  joint_net_conf:
    joint_space_size: 4

  normalize: utterance_mvn
  normalize_conf: {}

  model: espnet
  model_conf:
    ctc_weight: 0.3
    lsm_weight: 0.1
    length_normalized_loss: false

  frontend: default
  frontend_conf:
    n_fft: 512
    win_length: 400
    hop_length: 160
