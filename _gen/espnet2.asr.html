<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>espnet2.asr package &mdash; ESPnet 202211 documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="espnet2.torch_utils package" href="espnet2.torch_utils.html" />
    <link rel="prev" title="espnet.nets package" href="espnet.nets.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> ESPnet
          </a>
              <div class="version">
                202211
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p><span class="caption-text">Tutorial:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallelization.html">Using Job scheduling system</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docker.html">Docker</a></li>
</ul>
<p><span class="caption-text">ESPnet2:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_tutorial.html">ESPnet2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_tutorial.html#instruction-for-run-sh">Instruction for run.sh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_training_option.html">Change the configuration for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_task.html">Task class and data input system for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_distributed.html">Distributed training</a></li>
</ul>
<p><span class="caption-text">Notebook:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notebook/asr_cli.html">Speech Recognition (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/asr_library.html">Speech Recognition (Library)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_2pass_slu_demo.html">ESPNET 2 pass SLU Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_realtime_demo.html">ESPnet2-ASR realtime demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_transfer_learning_demo.html"><strong>Use transfer learning for ASR in ESPnet2</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_transfer_learning_demo.html#Abstract">Abstract</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_transfer_learning_demo.html#ESPnet-installation-(about-10-minutes-in-total)">ESPnet installation (about 10 minutes in total)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_transfer_learning_demo.html#mini_an4-recipe-as-a-transfer-learning-example">mini_an4 recipe as a transfer learning example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.html">CMU 11751/18781 Fall 2022: ESPnet Tutorial2 (New task)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.html#Install-ESPnet-(Almost-same-procedure-as-your-first-tutorial)">Install ESPnet (Almost same procedure as your first tutorial)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.html#What-we-provide-you-and-what-you-need-to-proceed">What we provide you and what you need to proceed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html">CMU 11751/18781 Fall 2022: ESPnet Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html#Install-ESPnet">Install ESPnet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html#Run-an-existing-recipe">Run an existing recipe</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html#Make-a-new-recipe">Make a new recipe</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html#Additional-resources">Additional resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_streaming_asr_demo.html">ESPnet2 real streaming Transformer demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tts_realtime_demo.html">ESPnet2-TTS realtime demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tutorial_2021_CMU_11751_18781.html">CMU 11751/18781 2021: ESPnet Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tutorial_2021_CMU_11751_18781.html#Run-an-inference-example">Run an inference example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tutorial_2021_CMU_11751_18781.html#Full-installation">Full installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tutorial_2021_CMU_11751_18781.html#Run-a-recipe-example">Run a recipe example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet_se_demonstration_for_waspaa_2021.html">ESPnet Speech Enhancement Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet_se_demonstration_for_waspaa_2021.html#Contents">Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet_se_demonstration_for_waspaa_2021.html#(1)-Tutorials-on-the-Basic-Usage">(1) Tutorials on the Basic Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet_se_demonstration_for_waspaa_2021.html#(2)-Tutorials-on-Contributing-to-ESPNet-SE-Project">(2) Tutorials on Contributing to ESPNet-SE Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/onnx_conversion_demo.html">espnet_onnx demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/onnx_conversion_demo.html#Install-Dependency">Install Dependency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/onnx_conversion_demo.html#Export-your-model">Export your model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/onnx_conversion_demo.html#Inference-with-onnx">Inference with onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/onnx_conversion_demo.html#Using-streaming-model">Using streaming model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/pretrained.html">Pretrained Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/se_demo.html">ESPnet Speech Enhancement Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/st_demo.html">ESPnet Speech Translation Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/tts_cli.html">Text-to-Speech (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/tts_realtime_demo.html">ESPnet real time E2E-TTS demonstration</a></li>
</ul>
<p><span class="caption-text">Package Reference:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="espnet.asr.html">espnet.asr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.scheduler.html">espnet.scheduler package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.optimizer.html">espnet.optimizer package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.transform.html">espnet.transform package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.tts.html">espnet.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.utils.html">espnet.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.lm.html">espnet.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.vc.html">espnet.vc package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.bin.html">espnet.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.st.html">espnet.st package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.mt.html">espnet.mt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.distributed.html">espnet.distributed package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.nets.html">espnet.nets package</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">espnet2.asr package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-maskctc-model">espnet2.asr.maskctc_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-init">espnet2.asr.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-pit-espnet-model">espnet2.asr.pit_espnet_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-espnet-model">espnet2.asr.espnet_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-ctc">espnet2.asr.ctc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-fused">espnet2.asr.frontend.fused</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-default">espnet2.asr.frontend.default</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-windowing">espnet2.asr.frontend.windowing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-abs-frontend">espnet2.asr.frontend.abs_frontend</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-init">espnet2.asr.frontend.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-s3prl">espnet2.asr.frontend.s3prl</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-whisper">espnet2.asr.frontend.whisper</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-transducer-beam-search-transducer">espnet2.asr.transducer.beam_search_transducer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-transducer-error-calculator">espnet2.asr.transducer.error_calculator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-transducer-init">espnet2.asr.transducer.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-cauchy">espnet2.asr.state_spaces.cauchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-registry">espnet2.asr.state_spaces.registry</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-attention">espnet2.asr.state_spaces.attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-utils">espnet2.asr.state_spaces.utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-components">espnet2.asr.state_spaces.components</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-block">espnet2.asr.state_spaces.block</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-pool">espnet2.asr.state_spaces.pool</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-base">espnet2.asr.state_spaces.base</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-init">espnet2.asr.state_spaces.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-model">espnet2.asr.state_spaces.model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-ff">espnet2.asr.state_spaces.ff</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-s4">espnet2.asr.state_spaces.s4</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-state-spaces-residual">espnet2.asr.state_spaces.residual</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-abs-encoder">espnet2.asr.encoder.abs_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-vgg-rnn-encoder">espnet2.asr.encoder.vgg_rnn_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-rnn-encoder">espnet2.asr.encoder.rnn_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-whisper-encoder">espnet2.asr.encoder.whisper_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-conformer-encoder">espnet2.asr.encoder.conformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-transformer-encoder">espnet2.asr.encoder.transformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-transformer-encoder-multispkr">espnet2.asr.encoder.transformer_encoder_multispkr</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-longformer-encoder">espnet2.asr.encoder.longformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-contextual-block-conformer-encoder">espnet2.asr.encoder.contextual_block_conformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-e-branchformer-encoder">espnet2.asr.encoder.e_branchformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-init">espnet2.asr.encoder.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-hubert-encoder">espnet2.asr.encoder.hubert_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-contextual-block-transformer-encoder">espnet2.asr.encoder.contextual_block_transformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-branchformer-encoder">espnet2.asr.encoder.branchformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-wav2vec2-encoder">espnet2.asr.encoder.wav2vec2_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-specaug-specaug">espnet2.asr.specaug.specaug</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-specaug-abs-specaug">espnet2.asr.specaug.abs_specaug</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-specaug-init">espnet2.asr.specaug.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-postencoder-init">espnet2.asr.postencoder.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-postencoder-abs-postencoder">espnet2.asr.postencoder.abs_postencoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-postencoder-hugging-face-transformers-postencoder">espnet2.asr.postencoder.hugging_face_transformers_postencoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-preencoder-sinc">espnet2.asr.preencoder.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-preencoder-abs-preencoder">espnet2.asr.preencoder.abs_preencoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-preencoder-linear">espnet2.asr.preencoder.linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-preencoder-init">espnet2.asr.preencoder.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-layers-cgmlp">espnet2.asr.layers.cgmlp</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-layers-init">espnet2.asr.layers.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-layers-fastformer">espnet2.asr.layers.fastformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-transducer-decoder">espnet2.asr.decoder.transducer_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-rnn-decoder">espnet2.asr.decoder.rnn_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-mlm-decoder">espnet2.asr.decoder.mlm_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-s4-decoder">espnet2.asr.decoder.s4_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-hugging-face-transformers-decoder">espnet2.asr.decoder.hugging_face_transformers_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-init">espnet2.asr.decoder.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-transformer-decoder">espnet2.asr.decoder.transformer_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-abs-decoder">espnet2.asr.decoder.abs_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-whisper-decoder">espnet2.asr.decoder.whisper_decoder</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.torch_utils.html">espnet2.torch_utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.main_funcs.html">espnet2.main_funcs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.diar.html">espnet2.diar package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.tts.html">espnet2.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.fileio.html">espnet2.fileio package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.hubert.html">espnet2.hubert package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.train.html">espnet2.train package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.text.html">espnet2.text package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.fst.html">espnet2.fst package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.optimizers.html">espnet2.optimizers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.tasks.html">espnet2.tasks package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.iterators.html">espnet2.iterators package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.slu.html">espnet2.slu package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.schedulers.html">espnet2.schedulers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.utils.html">espnet2.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.uasr.html">espnet2.uasr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.lm.html">espnet2.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.layers.html">espnet2.layers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.gan_svs.html">espnet2.gan_svs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.bin.html">espnet2.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.samplers.html">espnet2.samplers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.st.html">espnet2.st package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.mt.html">espnet2.mt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.gan_tts.html">espnet2.gan_tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.svs.html">espnet2.svs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.enh.html">espnet2.enh package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.asr_transducer.html">espnet2.asr_transducer package</a></li>
</ul>
<p><span class="caption-text">Tool Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../apis/espnet_bin.html">core tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/espnet2_bin.html">core tools (espnet2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/utils_py.html">python utility tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/utils_sh.html">bash utility tools</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ESPnet</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">espnet2.asr package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/_gen/espnet2.asr.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="espnet2-asr-package">
<h1>espnet2.asr package<a class="headerlink" href="#espnet2-asr-package" title="Permalink to this headline">¶</a></h1>
<section id="espnet2-asr-maskctc-model">
<span id="id1"></span><h2>espnet2.asr.maskctc_model<a class="headerlink" href="#espnet2-asr-maskctc-model" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.maskctc_model"></span><dl class="class">
<dt id="espnet2.asr.maskctc_model.MaskCTCInference">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.maskctc_model.</code><code class="sig-name descname">MaskCTCInference</code><span class="sig-paren">(</span><em class="sig-param">asr_model: espnet2.asr.maskctc_model.MaskCTCModel</em>, <em class="sig-param">n_iterations: int</em>, <em class="sig-param">threshold_probability: float</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCInference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCInference" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Mask-CTC-based non-autoregressive inference</p>
<p>Initialize Mask-CTC inference</p>
<dl class="method">
<dt id="espnet2.asr.maskctc_model.MaskCTCInference.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet.nets.beam_search.Hypothesis]<a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCInference.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCInference.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Mask-CTC inference</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.maskctc_model.MaskCTCInference.ids2text">
<code class="sig-name descname">ids2text</code><span class="sig-paren">(</span><em class="sig-param">ids: List[int]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCInference.ids2text"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCInference.ids2text" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.maskctc_model.MaskCTCModel">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.maskctc_model.</code><code class="sig-name descname">MaskCTCModel</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int, token_list: Union[Tuple[str, ...], List[str]], frontend: Optional[espnet2.asr.frontend.abs_frontend.AbsFrontend], specaug: Optional[espnet2.asr.specaug.abs_specaug.AbsSpecAug], normalize: Optional[espnet2.layers.abs_normalize.AbsNormalize], preencoder: Optional[espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder], encoder: espnet2.asr.encoder.abs_encoder.AbsEncoder, postencoder: Optional[espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder], decoder: espnet2.asr.decoder.mlm_decoder.MLMDecoder, ctc: espnet2.asr.ctc.CTC, joint_network: Optional[torch.nn.modules.module.Module] = None, ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '&lt;space&gt;', sym_blank: str = '&lt;blank&gt;', sym_mask: str = '&lt;mask&gt;', extract_feats_in_collect_stats: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.espnet_model.ESPnetASRModel" title="espnet2.asr.espnet_model.ESPnetASRModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.espnet_model.ESPnetASRModel</span></code></a></p>
<p>Hybrid CTC/Masked LM Encoder-Decoder model (Mask-CTC)</p>
<dl class="method">
<dt id="espnet2.asr.maskctc_model.MaskCTCModel.batchify_nll">
<code class="sig-name descname">batchify_nll</code><span class="sig-paren">(</span><em class="sig-param">encoder_out: torch.Tensor</em>, <em class="sig-param">encoder_out_lens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor</em>, <em class="sig-param">ys_pad_lens: torch.Tensor</em>, <em class="sig-param">batch_size: int = 100</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCModel.batchify_nll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCModel.batchify_nll" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute negative log likelihood(nll) from transformer-decoder</p>
<p>To avoid OOM, this fuction seperate the input into batches.
Then call nll for each batch and combine and return results.
:param encoder_out: (Batch, Length, Dim)
:param encoder_out_lens: (Batch,)
:param ys_pad: (Batch, Length)
:param ys_pad_lens: (Batch,)
:param batch_size: int, samples each batch contain when computing nll,</p>
<blockquote>
<div><p>you may change this to avoid OOM or increase
GPU memory usage</p>
</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.maskctc_model.MaskCTCModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">text: torch.Tensor</em>, <em class="sig-param">text_lengths: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Frontend + Encoder + Decoder + Calc loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>speech</strong> – (Batch, Length, …)</p></li>
<li><p><strong>speech_lengths</strong> – (Batch, )</p></li>
<li><p><strong>text</strong> – (Batch, Length)</p></li>
<li><p><strong>text_lengths</strong> – (Batch,)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.maskctc_model.MaskCTCModel.nll">
<code class="sig-name descname">nll</code><span class="sig-paren">(</span><em class="sig-param">encoder_out: torch.Tensor</em>, <em class="sig-param">encoder_out_lens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor</em>, <em class="sig-param">ys_pad_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCModel.nll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCModel.nll" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute negative log likelihood(nll) from transformer-decoder</p>
<p>Normally, this function is called in batchify_nll.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_out</strong> – (Batch, Length, Dim)</p></li>
<li><p><strong>encoder_out_lens</strong> – (Batch,)</p></li>
<li><p><strong>ys_pad</strong> – (Batch, Length)</p></li>
<li><p><strong>ys_pad_lens</strong> – (Batch,)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-init">
<span id="id2"></span><h2>espnet2.asr.__init__<a class="headerlink" href="#espnet2-asr-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.__init__"></span></section>
<section id="espnet2-asr-pit-espnet-model">
<span id="id3"></span><h2>espnet2.asr.pit_espnet_model<a class="headerlink" href="#espnet2-asr-pit-espnet-model" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.pit_espnet_model"></span><dl class="class">
<dt id="espnet2.asr.pit_espnet_model.ESPnetASRModel">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.pit_espnet_model.</code><code class="sig-name descname">ESPnetASRModel</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int, token_list: Union[Tuple[str, ...], List[str]], frontend: Optional[espnet2.asr.frontend.abs_frontend.AbsFrontend], specaug: Optional[espnet2.asr.specaug.abs_specaug.AbsSpecAug], normalize: Optional[espnet2.layers.abs_normalize.AbsNormalize], preencoder: Optional[espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder], encoder: espnet2.asr.encoder.abs_encoder.AbsEncoder, postencoder: Optional[espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder], decoder: espnet2.asr.decoder.abs_decoder.AbsDecoder, ctc: espnet2.asr.ctc.CTC, joint_network: Optional[torch.nn.modules.module.Module], ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '&lt;space&gt;', sym_blank: str = '&lt;blank&gt;', sym_sos: str = '&lt;sos/eos&gt;', sym_eos: str = '&lt;sos/eos&gt;', extract_feats_in_collect_stats: bool = True, lang_token_id: int = -1, num_inf: int = 1, num_ref: int = 1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/pit_espnet_model.html#ESPnetASRModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.pit_espnet_model.ESPnetASRModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.espnet_model.ESPnetASRModel" title="espnet2.asr.espnet_model.ESPnetASRModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.espnet_model.ESPnetASRModel</span></code></a></p>
<p>CTC-attention hybrid Encoder-Decoder model</p>
<dl class="method">
<dt id="espnet2.asr.pit_espnet_model.ESPnetASRModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">text: torch.Tensor</em>, <em class="sig-param">text_lengths: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/pit_espnet_model.html#ESPnetASRModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.pit_espnet_model.ESPnetASRModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Frontend + Encoder + Decoder + Calc loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>speech</strong> – (Batch, Length, …)</p></li>
<li><p><strong>speech_lengths</strong> – (Batch, )</p></li>
<li><p><strong>text</strong> – (Batch, Length)</p></li>
<li><p><strong>text_lengths</strong> – (Batch,)</p></li>
<li><p><strong>kwargs</strong> – “utt_id” is among the input.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.pit_espnet_model.PITLossWrapper">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.pit_espnet_model.</code><code class="sig-name descname">PITLossWrapper</code><span class="sig-paren">(</span><em class="sig-param">criterion_fn: Callable</em>, <em class="sig-param">num_ref: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/pit_espnet_model.html#PITLossWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.pit_espnet_model.PITLossWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="espnet2.enh.html#espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper" title="espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.pit_espnet_model.PITLossWrapper.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inf: torch.Tensor</em>, <em class="sig-param">inf_lens: torch.Tensor</em>, <em class="sig-param">ref: torch.Tensor</em>, <em class="sig-param">ref_lens: torch.Tensor</em>, <em class="sig-param">others: Dict = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/pit_espnet_model.html#PITLossWrapper.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.pit_espnet_model.PITLossWrapper.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>PITLoss Wrapper function. Similar to espnet2/enh/loss/wrapper/pit_solver.py</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inf</strong> – Iterable[torch.Tensor], (batch, num_inf, …)</p></li>
<li><p><strong>inf_lens</strong> – Iterable[torch.Tensor], (batch, num_inf, …)</p></li>
<li><p><strong>ref</strong> – Iterable[torch.Tensor], (batch, num_ref, …)</p></li>
<li><p><strong>ref_lens</strong> – Iterable[torch.Tensor], (batch, num_ref, …)</p></li>
<li><p><strong>permute_inf</strong> – If true, permute the inference and inference_lens according to
the optimal permutation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.pit_espnet_model.PITLossWrapper.permutate">
<em class="property">classmethod </em><code class="sig-name descname">permutate</code><span class="sig-paren">(</span><em class="sig-param">perm</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/pit_espnet_model.html#PITLossWrapper.permutate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.pit_espnet_model.PITLossWrapper.permutate" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-espnet-model">
<span id="id4"></span><h2>espnet2.asr.espnet_model<a class="headerlink" href="#espnet2-asr-espnet-model" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.espnet_model"></span><dl class="class">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.espnet_model.</code><code class="sig-name descname">ESPnetASRModel</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int, token_list: Union[Tuple[str, ...], List[str]], frontend: Optional[espnet2.asr.frontend.abs_frontend.AbsFrontend], specaug: Optional[espnet2.asr.specaug.abs_specaug.AbsSpecAug], normalize: Optional[espnet2.layers.abs_normalize.AbsNormalize], preencoder: Optional[espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder], encoder: espnet2.asr.encoder.abs_encoder.AbsEncoder, postencoder: Optional[espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder], decoder: Optional[espnet2.asr.decoder.abs_decoder.AbsDecoder], ctc: espnet2.asr.ctc.CTC, joint_network: Optional[torch.nn.modules.module.Module], aux_ctc: dict = None, ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '&lt;space&gt;', sym_blank: str = '&lt;blank&gt;', sym_sos: str = '&lt;sos/eos&gt;', sym_eos: str = '&lt;sos/eos&gt;', extract_feats_in_collect_stats: bool = True, lang_token_id: int = -1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="espnet2.train.html#espnet2.train.abs_espnet_model.AbsESPnetModel" title="espnet2.train.abs_espnet_model.AbsESPnetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.train.abs_espnet_model.AbsESPnetModel</span></code></a></p>
<p>CTC-attention hybrid Encoder-Decoder model</p>
<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.batchify_nll">
<code class="sig-name descname">batchify_nll</code><span class="sig-paren">(</span><em class="sig-param">encoder_out: torch.Tensor</em>, <em class="sig-param">encoder_out_lens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor</em>, <em class="sig-param">ys_pad_lens: torch.Tensor</em>, <em class="sig-param">batch_size: int = 100</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.batchify_nll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.batchify_nll" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute negative log likelihood(nll) from transformer-decoder</p>
<p>To avoid OOM, this fuction seperate the input into batches.
Then call nll for each batch and combine and return results.
:param encoder_out: (Batch, Length, Dim)
:param encoder_out_lens: (Batch,)
:param ys_pad: (Batch, Length)
:param ys_pad_lens: (Batch,)
:param batch_size: int, samples each batch contain when computing nll,</p>
<blockquote>
<div><p>you may change this to avoid OOM or increase
GPU memory usage</p>
</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.collect_feats">
<code class="sig-name descname">collect_feats</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">text: torch.Tensor</em>, <em class="sig-param">text_lengths: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.collect_feats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.collect_feats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.encode">
<code class="sig-name descname">encode</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Frontend + Encoder. Note that this method is used by asr_inference.py</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>speech</strong> – (Batch, Length, …)</p></li>
<li><p><strong>speech_lengths</strong> – (Batch, )</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">text: torch.Tensor</em>, <em class="sig-param">text_lengths: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Frontend + Encoder + Decoder + Calc loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>speech</strong> – (Batch, Length, …)</p></li>
<li><p><strong>speech_lengths</strong> – (Batch, )</p></li>
<li><p><strong>text</strong> – (Batch, Length)</p></li>
<li><p><strong>text_lengths</strong> – (Batch,)</p></li>
<li><p><strong>kwargs</strong> – “utt_id” is among the input.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.nll">
<code class="sig-name descname">nll</code><span class="sig-paren">(</span><em class="sig-param">encoder_out: torch.Tensor</em>, <em class="sig-param">encoder_out_lens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor</em>, <em class="sig-param">ys_pad_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.nll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.nll" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute negative log likelihood(nll) from transformer-decoder</p>
<p>Normally, this function is called in batchify_nll.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_out</strong> – (Batch, Length, Dim)</p></li>
<li><p><strong>encoder_out_lens</strong> – (Batch,)</p></li>
<li><p><strong>ys_pad</strong> – (Batch, Length)</p></li>
<li><p><strong>ys_pad_lens</strong> – (Batch,)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-ctc">
<span id="id5"></span><h2>espnet2.asr.ctc<a class="headerlink" href="#espnet2-asr-ctc" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.ctc"></span><dl class="class">
<dt id="espnet2.asr.ctc.CTC">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.ctc.</code><code class="sig-name descname">CTC</code><span class="sig-paren">(</span><em class="sig-param">odim: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">dropout_rate: float = 0.0</em>, <em class="sig-param">ctc_type: str = 'builtin'</em>, <em class="sig-param">reduce: bool = True</em>, <em class="sig-param">ignore_nan_grad: bool = None</em>, <em class="sig-param">zero_infinity: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>CTC module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>odim</strong> – dimension of outputs</p></li>
<li><p><strong>encoder_output_size</strong> – number of encoder projection units</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate (0.0 ~ 1.0)</p></li>
<li><p><strong>ctc_type</strong> – builtin or gtnctc</p></li>
<li><p><strong>reduce</strong> – reduce the CTC loss into a scalar</p></li>
<li><p><strong>ignore_nan_grad</strong> – Same as zero_infinity (keeping for backward compatiblity)</p></li>
<li><p><strong>zero_infinity</strong> – Whether to zero infinite losses and the associated gradients.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.ctc.CTC.argmax">
<code class="sig-name descname">argmax</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.argmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.argmax" title="Permalink to this definition">¶</a></dt>
<dd><p>argmax of frame activations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hs_pad</strong> (<em>torch.Tensor</em>) – 3d tensor (B, Tmax, eprojs)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>argmax applied 2d tensor (B, Tmax)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.ctc.CTC.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em>, <em class="sig-param">hlens</em>, <em class="sig-param">ys_pad</em>, <em class="sig-param">ys_lens</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate CTC loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hs_pad</strong> – batch of padded hidden state sequences (B, Tmax, D)</p></li>
<li><p><strong>hlens</strong> – batch of lengths of hidden state sequences (B)</p></li>
<li><p><strong>ys_pad</strong> – batch of padded character id sequence tensor (B, Lmax)</p></li>
<li><p><strong>ys_lens</strong> – batch of lengths of character sequence (B)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.ctc.CTC.log_softmax">
<code class="sig-name descname">log_softmax</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.log_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>log_softmax of frame activations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hs_pad</strong> (<em>Tensor</em>) – 3d tensor (B, Tmax, eprojs)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>log softmax applied 3d tensor (B, Tmax, odim)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.ctc.CTC.loss_fn">
<code class="sig-name descname">loss_fn</code><span class="sig-paren">(</span><em class="sig-param">th_pred</em>, <em class="sig-param">th_target</em>, <em class="sig-param">th_ilen</em>, <em class="sig-param">th_olen</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.loss_fn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.loss_fn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.ctc.CTC.softmax">
<code class="sig-name descname">softmax</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>softmax of frame activations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hs_pad</strong> (<em>Tensor</em>) – 3d tensor (B, Tmax, eprojs)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>softmax applied 3d tensor (B, Tmax, odim)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-frontend-fused">
<span id="id6"></span><h2>espnet2.asr.frontend.fused<a class="headerlink" href="#espnet2-asr-frontend-fused" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.fused"></span><dl class="class">
<dt id="espnet2.asr.frontend.fused.FusedFrontends">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.fused.</code><code class="sig-name descname">FusedFrontends</code><span class="sig-paren">(</span><em class="sig-param">frontends=None</em>, <em class="sig-param">align_method='linear_projection'</em>, <em class="sig-param">proj_dim=100</em>, <em class="sig-param">fs=16000</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/fused.html#FusedFrontends"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.fused.FusedFrontends" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="espnet2.asr.frontend.abs_frontend.AbsFrontend"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.frontend.abs_frontend.AbsFrontend</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.frontend.fused.FusedFrontends.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/fused.html#FusedFrontends.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.fused.FusedFrontends.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.fused.FusedFrontends.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/fused.html#FusedFrontends.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.fused.FusedFrontends.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-frontend-default">
<span id="id7"></span><h2>espnet2.asr.frontend.default<a class="headerlink" href="#espnet2-asr-frontend-default" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.default"></span><dl class="class">
<dt id="espnet2.asr.frontend.default.DefaultFrontend">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.default.</code><code class="sig-name descname">DefaultFrontend</code><span class="sig-paren">(</span><em class="sig-param">fs: Union[int</em>, <em class="sig-param">str] = 16000</em>, <em class="sig-param">n_fft: int = 512</em>, <em class="sig-param">win_length: int = None</em>, <em class="sig-param">hop_length: int = 128</em>, <em class="sig-param">window: Optional[str] = 'hann'</em>, <em class="sig-param">center: bool = True</em>, <em class="sig-param">normalized: bool = False</em>, <em class="sig-param">onesided: bool = True</em>, <em class="sig-param">n_mels: int = 80</em>, <em class="sig-param">fmin: int = None</em>, <em class="sig-param">fmax: int = None</em>, <em class="sig-param">htk: bool = False</em>, <em class="sig-param">frontend_conf: Optional[dict] = {'badim': 320</em>, <em class="sig-param">'bdropout_rate': 0.0</em>, <em class="sig-param">'blayers': 3</em>, <em class="sig-param">'bnmask': 2</em>, <em class="sig-param">'bprojs': 320</em>, <em class="sig-param">'btype': 'blstmp'</em>, <em class="sig-param">'bunits': 300</em>, <em class="sig-param">'delay': 3</em>, <em class="sig-param">'ref_channel': -1</em>, <em class="sig-param">'taps': 5</em>, <em class="sig-param">'use_beamformer': False</em>, <em class="sig-param">'use_dnn_mask_for_wpe': True</em>, <em class="sig-param">'use_wpe': False</em>, <em class="sig-param">'wdropout_rate': 0.0</em>, <em class="sig-param">'wlayers': 3</em>, <em class="sig-param">'wprojs': 320</em>, <em class="sig-param">'wtype': 'blstmp'</em>, <em class="sig-param">'wunits': 300}</em>, <em class="sig-param">apply_stft: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/default.html#DefaultFrontend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.default.DefaultFrontend" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="espnet2.asr.frontend.abs_frontend.AbsFrontend"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.frontend.abs_frontend.AbsFrontend</span></code></a></p>
<p>Conventional frontend structure for ASR.</p>
<p>Stft -&gt; WPE -&gt; MVDR-Beamformer -&gt; Power-spec -&gt; Mel-Fbank -&gt; CMVN</p>
<dl class="method">
<dt id="espnet2.asr.frontend.default.DefaultFrontend.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/default.html#DefaultFrontend.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.default.DefaultFrontend.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.default.DefaultFrontend.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/default.html#DefaultFrontend.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.default.DefaultFrontend.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-frontend-windowing">
<span id="id8"></span><h2>espnet2.asr.frontend.windowing<a class="headerlink" href="#espnet2-asr-frontend-windowing" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.windowing"></span><p>Sliding Window for raw audio input data.</p>
<dl class="class">
<dt id="espnet2.asr.frontend.windowing.SlidingWindow">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.windowing.</code><code class="sig-name descname">SlidingWindow</code><span class="sig-paren">(</span><em class="sig-param">win_length: int = 400</em>, <em class="sig-param">hop_length: int = 160</em>, <em class="sig-param">channels: int = 1</em>, <em class="sig-param">padding: int = None</em>, <em class="sig-param">fs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/windowing.html#SlidingWindow"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.windowing.SlidingWindow" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="espnet2.asr.frontend.abs_frontend.AbsFrontend"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.frontend.abs_frontend.AbsFrontend</span></code></a></p>
<p>Sliding Window.</p>
<p>Provides a sliding window over a batched continuous raw audio tensor.
Optionally, provides padding (Currently not implemented).
Combine this module with a pre-encoder compatible with raw audio data,
for example Sinc convolutions.</p>
<p>Known issues:
Output length is calculated incorrectly if audio shorter than win_length.
WARNING: trailing values are discarded - padding not implemented yet.
There is currently no additional window function applied to input values.</p>
<p>Initialize.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>win_length</strong> – Length of frame.</p></li>
<li><p><strong>hop_length</strong> – Relative starting point of next frame.</p></li>
<li><p><strong>channels</strong> – Number of input channels.</p></li>
<li><p><strong>padding</strong> – Padding (placeholder, currently not implemented).</p></li>
<li><p><strong>fs</strong> – Sampling rate (placeholder for compatibility, not used).</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.frontend.windowing.SlidingWindow.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/windowing.html#SlidingWindow.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.windowing.SlidingWindow.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply a sliding window on the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – Input (B, T, C*D) or (B, T*C*D), with D=C=1.</p></li>
<li><p><strong>input_lengths</strong> – Input lengths within batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output with dimensions (B, T, C, D), with D=win_length.
Tensor: Output lengths within batch.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.windowing.SlidingWindow.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/windowing.html#SlidingWindow.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.windowing.SlidingWindow.output_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Return output length of feature dimension D, i.e. the window length.</p>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-frontend-abs-frontend">
<span id="id9"></span><h2>espnet2.asr.frontend.abs_frontend<a class="headerlink" href="#espnet2-asr-frontend-abs-frontend" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.abs_frontend"></span><dl class="class">
<dt id="espnet2.asr.frontend.abs_frontend.AbsFrontend">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.abs_frontend.</code><code class="sig-name descname">AbsFrontend</code><a class="reference internal" href="../_modules/espnet2/asr/frontend/abs_frontend.html#AbsFrontend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.frontend.abs_frontend.AbsFrontend.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/abs_frontend.html#AbsFrontend.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.abs_frontend.AbsFrontend.output_size">
<em class="property">abstract </em><code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/abs_frontend.html#AbsFrontend.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-frontend-init">
<span id="id10"></span><h2>espnet2.asr.frontend.__init__<a class="headerlink" href="#espnet2-asr-frontend-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.__init__"></span></section>
<section id="espnet2-asr-frontend-s3prl">
<span id="id11"></span><h2>espnet2.asr.frontend.s3prl<a class="headerlink" href="#espnet2-asr-frontend-s3prl" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.s3prl"></span><dl class="class">
<dt id="espnet2.asr.frontend.s3prl.S3prlFrontend">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.s3prl.</code><code class="sig-name descname">S3prlFrontend</code><span class="sig-paren">(</span><em class="sig-param">fs: Union[int</em>, <em class="sig-param">str] = 16000</em>, <em class="sig-param">frontend_conf: Optional[dict] = {'badim': 320</em>, <em class="sig-param">'bdropout_rate': 0.0</em>, <em class="sig-param">'blayers': 3</em>, <em class="sig-param">'bnmask': 2</em>, <em class="sig-param">'bprojs': 320</em>, <em class="sig-param">'btype': 'blstmp'</em>, <em class="sig-param">'bunits': 300</em>, <em class="sig-param">'delay': 3</em>, <em class="sig-param">'ref_channel': -1</em>, <em class="sig-param">'taps': 5</em>, <em class="sig-param">'use_beamformer': False</em>, <em class="sig-param">'use_dnn_mask_for_wpe': True</em>, <em class="sig-param">'use_wpe': False</em>, <em class="sig-param">'wdropout_rate': 0.0</em>, <em class="sig-param">'wlayers': 3</em>, <em class="sig-param">'wprojs': 320</em>, <em class="sig-param">'wtype': 'blstmp'</em>, <em class="sig-param">'wunits': 300}</em>, <em class="sig-param">download_dir: str = None</em>, <em class="sig-param">multilayer_feature: bool = False</em>, <em class="sig-param">layer: int = -1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/s3prl.html#S3prlFrontend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.s3prl.S3prlFrontend" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="espnet2.asr.frontend.abs_frontend.AbsFrontend"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.frontend.abs_frontend.AbsFrontend</span></code></a></p>
<p>Speech Pretrained Representation frontend structure for ASR.</p>
<dl class="method">
<dt id="espnet2.asr.frontend.s3prl.S3prlFrontend.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/s3prl.html#S3prlFrontend.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.s3prl.S3prlFrontend.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.s3prl.S3prlFrontend.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/s3prl.html#S3prlFrontend.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.s3prl.S3prlFrontend.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.s3prl.S3prlFrontend.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/s3prl.html#S3prlFrontend.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.s3prl.S3prlFrontend.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-frontend-whisper">
<span id="id12"></span><h2>espnet2.asr.frontend.whisper<a class="headerlink" href="#espnet2-asr-frontend-whisper" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.whisper"></span><dl class="class">
<dt id="espnet2.asr.frontend.whisper.WhisperFrontend">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.whisper.</code><code class="sig-name descname">WhisperFrontend</code><span class="sig-paren">(</span><em class="sig-param">whisper_model: str = 'small'</em>, <em class="sig-param">freeze_weights: bool = True</em>, <em class="sig-param">download_dir: str = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/whisper.html#WhisperFrontend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.whisper.WhisperFrontend" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="espnet2.asr.frontend.abs_frontend.AbsFrontend"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.frontend.abs_frontend.AbsFrontend</span></code></a></p>
<p>Speech Representation Using Encoder Outputs from OpenAI’s Whisper Model:</p>
<p>URL: <a class="reference external" href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></p>
<dl class="method">
<dt id="espnet2.asr.frontend.whisper.WhisperFrontend.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/whisper.html#WhisperFrontend.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.whisper.WhisperFrontend.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.whisper.WhisperFrontend.log_mel_spectrogram">
<code class="sig-name descname">log_mel_spectrogram</code><span class="sig-paren">(</span><em class="sig-param">audio: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/frontend/whisper.html#WhisperFrontend.log_mel_spectrogram"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.whisper.WhisperFrontend.log_mel_spectrogram" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.whisper.WhisperFrontend.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/whisper.html#WhisperFrontend.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.whisper.WhisperFrontend.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.whisper.WhisperFrontend.whisper_encode">
<code class="sig-name descname">whisper_encode</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/frontend/whisper.html#WhisperFrontend.whisper_encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.whisper.WhisperFrontend.whisper_encode" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-transducer-beam-search-transducer">
<span id="id13"></span><h2>espnet2.asr.transducer.beam_search_transducer<a class="headerlink" href="#espnet2-asr-transducer-beam-search-transducer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.transducer.beam_search_transducer"></span><p>Search algorithms for Transducer models.</p>
<dl class="class">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.transducer.beam_search_transducer.</code><code class="sig-name descname">BeamSearchTransducer</code><span class="sig-paren">(</span><em class="sig-param">decoder: espnet2.asr.decoder.abs_decoder.AbsDecoder</em>, <em class="sig-param">joint_network: espnet2.asr_transducer.joint_network.JointNetwork</em>, <em class="sig-param">beam_size: int</em>, <em class="sig-param">lm: torch.nn.modules.module.Module = None</em>, <em class="sig-param">lm_weight: float = 0.1</em>, <em class="sig-param">search_type: str = 'default'</em>, <em class="sig-param">max_sym_exp: int = 2</em>, <em class="sig-param">u_max: int = 50</em>, <em class="sig-param">nstep: int = 1</em>, <em class="sig-param">prefix_alpha: int = 1</em>, <em class="sig-param">expansion_gamma: int = 2.3</em>, <em class="sig-param">expansion_beta: int = 2</em>, <em class="sig-param">score_norm: bool = True</em>, <em class="sig-param">nbest: int = 1</em>, <em class="sig-param">token_list: Optional[List[str]] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Beam search implementation for Transducer.</p>
<p>Initialize Transducer search module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decoder</strong> – Decoder module.</p></li>
<li><p><strong>joint_network</strong> – Joint network module.</p></li>
<li><p><strong>beam_size</strong> – Beam size.</p></li>
<li><p><strong>lm</strong> – LM class.</p></li>
<li><p><strong>lm_weight</strong> – LM weight for soft fusion.</p></li>
<li><p><strong>search_type</strong> – Search algorithm to use during inference.</p></li>
<li><p><strong>max_sym_exp</strong> – Number of maximum symbol expansions at each time step. (TSD)</p></li>
<li><p><strong>u_max</strong> – Maximum output sequence length. (ALSD)</p></li>
<li><p><strong>nstep</strong> – Number of maximum expansion steps at each time step. (NSC/mAES)</p></li>
<li><p><strong>prefix_alpha</strong> – Maximum prefix length in prefix search. (NSC/mAES)</p></li>
<li><p><strong>expansion_beta</strong> – Number of additional candidates for expanded hypotheses selection. (mAES)</p></li>
<li><p><strong>expansion_gamma</strong> – Allowed logp difference for prune-by-value method. (mAES)</p></li>
<li><p><strong>score_norm</strong> – Normalize final scores by length. (“default”)</p></li>
<li><p><strong>nbest</strong> – Number of final hypothesis.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.align_length_sync_decoding">
<code class="sig-name descname">align_length_sync_decoding</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.Hypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.align_length_sync_decoding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.align_length_sync_decoding" title="Permalink to this definition">¶</a></dt>
<dd><p>Alignment-length synchronous beam search implementation.</p>
<p>Based on <a class="reference external" href="https://ieeexplore.ieee.org/document/9053040">https://ieeexplore.ieee.org/document/9053040</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>h</strong> – Encoder output sequences. (T, D)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best hypothesis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nbest_hyps</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.default_beam_search">
<code class="sig-name descname">default_beam_search</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.Hypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.default_beam_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.default_beam_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Beam search implementation.</p>
<p>Modified from <a class="reference external" href="https://arxiv.org/pdf/1211.3711.pdf">https://arxiv.org/pdf/1211.3711.pdf</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enc_out</strong> – Encoder output sequence. (T, D)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best hypothesis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nbest_hyps</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.greedy_search">
<code class="sig-name descname">greedy_search</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.Hypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.greedy_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.greedy_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Greedy search implementation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enc_out</strong> – Encoder output sequence. (T, D_enc)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>1-best hypotheses.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>hyp</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.modified_adaptive_expansion_search">
<code class="sig-name descname">modified_adaptive_expansion_search</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.modified_adaptive_expansion_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.modified_adaptive_expansion_search" title="Permalink to this definition">¶</a></dt>
<dd><p>It’s the modified Adaptive Expansion Search (mAES) implementation.</p>
<p>Based on/modified from <a class="reference external" href="https://ieeexplore.ieee.org/document/9250505">https://ieeexplore.ieee.org/document/9250505</a> and NSC.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enc_out</strong> – Encoder output sequence. (T, D_enc)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best hypothesis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nbest_hyps</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.nsc_beam_search">
<code class="sig-name descname">nsc_beam_search</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.nsc_beam_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.nsc_beam_search" title="Permalink to this definition">¶</a></dt>
<dd><p>N-step constrained beam search implementation.</p>
<p>Based on/Modified from <a class="reference external" href="https://arxiv.org/pdf/2002.03577.pdf">https://arxiv.org/pdf/2002.03577.pdf</a>.
Please reference ESPnet (b-flo, PR #2444) for any usage outside ESPnet
until further modifications.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enc_out</strong> – Encoder output sequence. (T, D_enc)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best hypothesis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nbest_hyps</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.prefix_search">
<code class="sig-name descname">prefix_search</code><span class="sig-paren">(</span><em class="sig-param">hyps: List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis], enc_out_t: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.prefix_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.prefix_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Prefix search for NSC and mAES strategies.</p>
<p>Based on <a class="reference external" href="https://arxiv.org/pdf/1211.3711.pdf">https://arxiv.org/pdf/1211.3711.pdf</a></p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.sort_nbest">
<code class="sig-name descname">sort_nbest</code><span class="sig-paren">(</span><em class="sig-param">hyps: Union[List[espnet2.asr.transducer.beam_search_transducer.Hypothesis], List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis]]</em><span class="sig-paren">)</span> &#x2192; Union[List[espnet2.asr.transducer.beam_search_transducer.Hypothesis], List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis]]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.sort_nbest"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.sort_nbest" title="Permalink to this definition">¶</a></dt>
<dd><p>Sort hypotheses by score or score given sequence length.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyps</strong> – Hypothesis.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Sorted hypothesis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>hyps</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.time_sync_decoding">
<code class="sig-name descname">time_sync_decoding</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.Hypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.time_sync_decoding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.time_sync_decoding" title="Permalink to this definition">¶</a></dt>
<dd><p>Time synchronous beam search implementation.</p>
<p>Based on <a class="reference external" href="https://ieeexplore.ieee.org/document/9053040">https://ieeexplore.ieee.org/document/9053040</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enc_out</strong> – Encoder output sequence. (T, D)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best hypothesis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nbest_hyps</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.transducer.beam_search_transducer.</code><code class="sig-name descname">ExtendedHypothesis</code><span class="sig-paren">(</span><em class="sig-param">score: float, yseq: List[int], dec_state: Union[Tuple[torch.Tensor, Optional[torch.Tensor]], List[Optional[torch.Tensor]], torch.Tensor], lm_state: Union[Dict[str, Any], List[Any]] = None, dec_out: List[torch.Tensor] = None, lm_scores: torch.Tensor = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#ExtendedHypothesis"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.transducer.beam_search_transducer.Hypothesis" title="espnet2.asr.transducer.beam_search_transducer.Hypothesis"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.transducer.beam_search_transducer.Hypothesis</span></code></a></p>
<p>Extended hypothesis definition for NSC beam search and mAES.</p>
<dl class="attribute">
<dt id="espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis.dec_out">
<code class="sig-name descname">dec_out</code><em class="property"> = None</em><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis.dec_out" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis.lm_scores">
<code class="sig-name descname">lm_scores</code><em class="property"> = None</em><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis.lm_scores" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.transducer.beam_search_transducer.Hypothesis">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.transducer.beam_search_transducer.</code><code class="sig-name descname">Hypothesis</code><span class="sig-paren">(</span><em class="sig-param">score: float, yseq: List[int], dec_state: Union[Tuple[torch.Tensor, Optional[torch.Tensor]], List[Optional[torch.Tensor]], torch.Tensor], lm_state: Union[Dict[str, Any], List[Any]] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#Hypothesis"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.Hypothesis" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Default hypothesis definition for Transducer search algorithms.</p>
<dl class="attribute">
<dt id="espnet2.asr.transducer.beam_search_transducer.Hypothesis.lm_state">
<code class="sig-name descname">lm_state</code><em class="property"> = None</em><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.Hypothesis.lm_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-transducer-error-calculator">
<span id="id14"></span><h2>espnet2.asr.transducer.error_calculator<a class="headerlink" href="#espnet2-asr-transducer-error-calculator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.transducer.error_calculator"></span><p>Error Calculator module for Transducer.</p>
<dl class="class">
<dt id="espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.transducer.error_calculator.</code><code class="sig-name descname">ErrorCalculatorTransducer</code><span class="sig-paren">(</span><em class="sig-param">decoder: espnet2.asr.decoder.abs_decoder.AbsDecoder, joint_network: torch.nn.modules.module.Module, token_list: List[int], sym_space: str, sym_blank: str, report_cer: bool = False, report_wer: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/transducer/error_calculator.html#ErrorCalculatorTransducer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Calculate CER and WER for transducer models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decoder</strong> – Decoder module.</p></li>
<li><p><strong>token_list</strong> – List of tokens.</p></li>
<li><p><strong>sym_space</strong> – Space symbol.</p></li>
<li><p><strong>sym_blank</strong> – Blank symbol.</p></li>
<li><p><strong>report_cer</strong> – Whether to compute CER.</p></li>
<li><p><strong>report_wer</strong> – Whether to compute WER.</p></li>
</ul>
</dd>
</dl>
<p>Construct an ErrorCalculatorTransducer.</p>
<dl class="method">
<dt id="espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer.calculate_cer">
<code class="sig-name descname">calculate_cer</code><span class="sig-paren">(</span><em class="sig-param">char_pred: torch.Tensor</em>, <em class="sig-param">char_target: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; float<a class="reference internal" href="../_modules/espnet2/asr/transducer/error_calculator.html#ErrorCalculatorTransducer.calculate_cer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer.calculate_cer" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate sentence-level CER score.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>char_pred</strong> – Prediction character sequences. (B, ?)</p></li>
<li><p><strong>char_target</strong> – Target character sequences. (B, ?)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Average sentence-level CER score.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer.calculate_wer">
<code class="sig-name descname">calculate_wer</code><span class="sig-paren">(</span><em class="sig-param">char_pred: torch.Tensor</em>, <em class="sig-param">char_target: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; float<a class="reference internal" href="../_modules/espnet2/asr/transducer/error_calculator.html#ErrorCalculatorTransducer.calculate_wer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer.calculate_wer" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate sentence-level WER score.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>char_pred</strong> – Prediction character sequences. (B, ?)</p></li>
<li><p><strong>char_target</strong> – Target character sequences. (B, ?)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Average sentence-level WER score</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer.convert_to_char">
<code class="sig-name descname">convert_to_char</code><span class="sig-paren">(</span><em class="sig-param">pred: torch.Tensor</em>, <em class="sig-param">target: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[List, List]<a class="reference internal" href="../_modules/espnet2/asr/transducer/error_calculator.html#ErrorCalculatorTransducer.convert_to_char"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer.convert_to_char" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert label ID sequences to character sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> – Prediction label ID sequences. (B, U)</p></li>
<li><p><strong>target</strong> – Target label ID sequences. (B, L)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Prediction character sequences. (B, ?)
char_target: Target character sequences. (B, ?)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>char_pred</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-transducer-init">
<span id="id15"></span><h2>espnet2.asr.transducer.__init__<a class="headerlink" href="#espnet2-asr-transducer-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.transducer.__init__"></span></section>
<section id="espnet2-asr-state-spaces-cauchy">
<span id="id16"></span><h2>espnet2.asr.state_spaces.cauchy<a class="headerlink" href="#espnet2-asr-state-spaces-cauchy" title="Permalink to this headline">¶</a></h2>
</section>
<section id="espnet2-asr-state-spaces-registry">
<span id="id17"></span><h2>espnet2.asr.state_spaces.registry<a class="headerlink" href="#espnet2-asr-state-spaces-registry" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.state_spaces.registry"></span></section>
<section id="espnet2-asr-state-spaces-attention">
<span id="id18"></span><h2>espnet2.asr.state_spaces.attention<a class="headerlink" href="#espnet2-asr-state-spaces-attention" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.state_spaces.attention"></span><p>Multi-Head Attention layer definition.</p>
<dl class="class">
<dt id="espnet2.asr.state_spaces.attention.MultiHeadedAttention">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.attention.</code><code class="sig-name descname">MultiHeadedAttention</code><span class="sig-paren">(</span><em class="sig-param">n_feat</em>, <em class="sig-param">n_head</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">transposed=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/attention.html#MultiHeadedAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.attention.MultiHeadedAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.base.SequenceModule" title="espnet2.asr.state_spaces.base.SequenceModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.base.SequenceModule</span></code></a></p>
<p>Multi-Head Attention layer inheriting SequenceModule.</p>
<p>Comparing default MHA module in ESPnet, this module returns additional dummy state
and has step function for autoregressive inference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_head</strong> (<em>int</em>) – The number of heads.</p></li>
<li><p><strong>n_feat</strong> (<em>int</em>) – The number of features.</p></li>
<li><p><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</p></li>
</ul>
</dd>
</dl>
<p>Construct an MultiHeadedAttention object.</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.attention.MultiHeadedAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">query</em>, <em class="sig-param">memory=None</em>, <em class="sig-param">mask=None</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/attention.html#MultiHeadedAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.attention.MultiHeadedAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute scaled dot product attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor (#batch, time1, size).</p></li>
<li><p><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor (#batch, time2, size).</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor (#batch, time2, size).</p></li>
<li><p><strong>mask</strong> (<em>torch.Tensor</em>) – Mask tensor (#batch, 1, time2) or
(#batch, time1, time2).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor (#batch, time1, d_model).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.attention.MultiHeadedAttention.forward_attention">
<code class="sig-name descname">forward_attention</code><span class="sig-paren">(</span><em class="sig-param">value</em>, <em class="sig-param">scores</em>, <em class="sig-param">mask</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/attention.html#MultiHeadedAttention.forward_attention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.attention.MultiHeadedAttention.forward_attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute attention context vector.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – Transformed value (#batch, n_head, time2, d_k).</p></li>
<li><p><strong>scores</strong> (<em>torch.Tensor</em>) – Attention score (#batch, n_head, time1, time2).</p></li>
<li><p><strong>mask</strong> (<em>torch.Tensor</em>) – Mask (#batch, 1, time2) or (#batch, time1, time2).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Transformed value (#batch, time1, d_model)</dt><dd><p>weighted by the attention score (#batch, time1, time2).</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.attention.MultiHeadedAttention.forward_qkv">
<code class="sig-name descname">forward_qkv</code><span class="sig-paren">(</span><em class="sig-param">query</em>, <em class="sig-param">key</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/attention.html#MultiHeadedAttention.forward_qkv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.attention.MultiHeadedAttention.forward_qkv" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform query, key and value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor (#batch, time1, size).</p></li>
<li><p><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor (#batch, time2, size).</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor (#batch, time2, size).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Transformed query tensor (#batch, n_head, time1, d_k).
torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k).
torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.attention.MultiHeadedAttention.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">query</em>, <em class="sig-param">state</em>, <em class="sig-param">memory=None</em>, <em class="sig-param">mask=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/attention.html#MultiHeadedAttention.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.attention.MultiHeadedAttention.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step the model recurrently for one step of the input sequence.</p>
<p>For example, this should correspond to unrolling an RNN for one step.
If the forward pass has signature (B, L, H1) -&gt; (B, L, H2),
this method should generally have signature
(B, H1) -&gt; (B, H2) with an optional recurrent state.</p>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-state-spaces-utils">
<span id="id19"></span><h2>espnet2.asr.state_spaces.utils<a class="headerlink" href="#espnet2-asr-state-spaces-utils" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.state_spaces.utils"></span><p>Utilities for dealing with collection objects (lists, dicts) and configs.</p>
<dl class="function">
<dt id="espnet2.asr.state_spaces.utils.extract_attrs_from_obj">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.utils.</code><code class="sig-name descname">extract_attrs_from_obj</code><span class="sig-paren">(</span><em class="sig-param">obj</em>, <em class="sig-param">*attrs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/utils.html#extract_attrs_from_obj"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.utils.extract_attrs_from_obj" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.utils.get_class">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.utils.</code><code class="sig-name descname">get_class</code><span class="sig-paren">(</span><em class="sig-param">registry</em>, <em class="sig-param">_name_</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/utils.html#get_class"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.utils.get_class" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.utils.instantiate">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.utils.</code><code class="sig-name descname">instantiate</code><span class="sig-paren">(</span><em class="sig-param">registry</em>, <em class="sig-param">config</em>, <em class="sig-param">*args</em>, <em class="sig-param">partial=False</em>, <em class="sig-param">wrap=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/utils.html#instantiate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.utils.instantiate" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiate registered module.</p>
<dl class="simple">
<dt>registry: Dictionary mapping names to functions or target paths</dt><dd><p>(e.g. {‘model’: ‘models.SequenceModel’})</p>
</dd>
<dt>config: Dictionary with a ‘_name_’ key indicating which element of the registry</dt><dd><p>to grab, and kwargs to be passed into the target constructor</p>
</dd>
</dl>
<p>wrap: wrap the target class (e.g. ema optimizer or tasks.wrap)
<a href="#id20"><span class="problematic" id="id21">*</span></a>args, <a href="#id22"><span class="problematic" id="id23">**</span></a>kwargs: additional arguments</p>
<blockquote>
<div><p>to override the config to pass into the target constructor</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.utils.is_dict">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.utils.</code><code class="sig-name descname">is_dict</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/utils.html#is_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.utils.is_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.utils.is_list">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.utils.</code><code class="sig-name descname">is_list</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/utils.html#is_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.utils.is_list" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.utils.omegaconf_filter_keys">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.utils.</code><code class="sig-name descname">omegaconf_filter_keys</code><span class="sig-paren">(</span><em class="sig-param">d</em>, <em class="sig-param">fn=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/utils.html#omegaconf_filter_keys"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.utils.omegaconf_filter_keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Only keep keys where fn(key) is True. Support nested DictConfig.</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.utils.to_dict">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.utils.</code><code class="sig-name descname">to_dict</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">recursive=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/utils.html#to_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.utils.to_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert Sequence or Mapping object to dict.</p>
<p>lists get converted to {0: x[0], 1: x[1], …}</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.utils.to_list">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.utils.</code><code class="sig-name descname">to_list</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">recursive=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/utils.html#to_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.utils.to_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert an object to list.</p>
<p>If Sequence (e.g. list, tuple, Listconfig): just return it</p>
<p>Special case: If non-recursive and not a list, wrap in list</p>
</dd></dl>

</section>
<section id="espnet2-asr-state-spaces-components">
<span id="id24"></span><h2>espnet2.asr.state_spaces.components<a class="headerlink" href="#espnet2-asr-state-spaces-components" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.state_spaces.components"></span><dl class="function">
<dt id="espnet2.asr.state_spaces.components.Activation">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">Activation</code><span class="sig-paren">(</span><em class="sig-param">activation=None</em>, <em class="sig-param">size=None</em>, <em class="sig-param">dim=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#Activation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.Activation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.components.DropoutNd">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">DropoutNd</code><span class="sig-paren">(</span><em class="sig-param">p: float = 0.5</em>, <em class="sig-param">tie=True</em>, <em class="sig-param">transposed=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#DropoutNd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.DropoutNd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Initialize dropout module.</p>
<p>tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.components.DropoutNd.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#DropoutNd.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.DropoutNd.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>X: (batch, dim, lengths…)</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.components.LinearActivation">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">LinearActivation</code><span class="sig-paren">(</span><em class="sig-param">d_input</em>, <em class="sig-param">d_output</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">zero_bias_init=False</em>, <em class="sig-param">transposed=False</em>, <em class="sig-param">initializer=None</em>, <em class="sig-param">activation=None</em>, <em class="sig-param">activate=False</em>, <em class="sig-param">weight_norm=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#LinearActivation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.LinearActivation" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a linear module, initialization, and activation.</p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.components.Normalization">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">Normalization</code><span class="sig-paren">(</span><em class="sig-param">d</em>, <em class="sig-param">transposed=False</em>, <em class="sig-param">_name_='layer'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#Normalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.Normalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.components.Normalization.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#Normalization.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.Normalization.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.components.Normalization.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#Normalization.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.Normalization.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dInput">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">ReversibleInstanceNorm1dInput</code><span class="sig-paren">(</span><em class="sig-param">d</em>, <em class="sig-param">transposed=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#ReversibleInstanceNorm1dInput"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dInput" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dInput.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#ReversibleInstanceNorm1dInput.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dInput.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dOutput">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">ReversibleInstanceNorm1dOutput</code><span class="sig-paren">(</span><em class="sig-param">norm_input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#ReversibleInstanceNorm1dOutput"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dOutput" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dOutput.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#ReversibleInstanceNorm1dOutput.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dOutput.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.components.SquaredReLU">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">SquaredReLU</code><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#SquaredReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.SquaredReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.components.SquaredReLU.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#SquaredReLU.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.SquaredReLU.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.components.StochasticDepth">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">StochasticDepth</code><span class="sig-paren">(</span><em class="sig-param">p: float</em>, <em class="sig-param">mode: str</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#StochasticDepth"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.StochasticDepth" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Stochastic depth module.</p>
<p>See <a class="reference internal" href="#espnet2.asr.state_spaces.components.stochastic_depth" title="espnet2.asr.state_spaces.components.stochastic_depth"><code class="xref py py-func docutils literal notranslate"><span class="pre">stochastic_depth()</span></code></a>.</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.components.StochasticDepth.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#StochasticDepth.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.StochasticDepth.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.components.TSInverseNormalization">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">TSInverseNormalization</code><span class="sig-paren">(</span><em class="sig-param">method</em>, <em class="sig-param">normalizer</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#TSInverseNormalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.TSInverseNormalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.components.TSInverseNormalization.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#TSInverseNormalization.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.TSInverseNormalization.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.components.TSNormalization">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">TSNormalization</code><span class="sig-paren">(</span><em class="sig-param">method</em>, <em class="sig-param">horizon</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#TSNormalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.TSNormalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.components.TSNormalization.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#TSNormalization.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.TSNormalization.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.components.TransposedLN">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">TransposedLN</code><span class="sig-paren">(</span><em class="sig-param">d</em>, <em class="sig-param">scalar=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#TransposedLN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.TransposedLN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Transposed LayerNorm module.</p>
<p>LayerNorm module over second dimension
Assumes shape (B, D, L), where L can be 1 or more axis</p>
<p>This is slow and a dedicated CUDA/Triton implementation
shuld provide substantial end-to-end speedup</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.components.TransposedLN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#TransposedLN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.TransposedLN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.components.TransposedLinear">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">TransposedLinear</code><span class="sig-paren">(</span><em class="sig-param">d_input</em>, <em class="sig-param">d_output</em>, <em class="sig-param">bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#TransposedLinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.TransposedLinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Transposed linear module.</p>
<p>Linear module on the second-to-last dimension
Assumes shape (B, D, L), where L can be 1 or more axis</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.components.TransposedLinear.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#TransposedLinear.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.TransposedLinear.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.components.get_initializer">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">get_initializer</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">activation=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#get_initializer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.get_initializer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.components.stochastic_depth">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.components.</code><code class="sig-name descname">stochastic_depth</code><span class="sig-paren">(</span><em class="sig-param">input: torch._VariableFunctionsClass.tensor</em>, <em class="sig-param">p: float</em>, <em class="sig-param">mode: str</em>, <em class="sig-param">training: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/components.html#stochastic_depth"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.components.stochastic_depth" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply stochastic depth.</p>
<p>Implements the Stochastic Depth from <a class="reference external" href="https://arxiv.org/abs/1603.09382">“Deep Networks with Stochastic Depth”</a> used for randomly dropping residual
branches of residual architectures.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em><em>[</em><em>N</em><em>, </em><em>..</em><em>]</em>) – The input tensor or arbitrary dimensions with the first
one being its batch i.e. a batch with <code class="docutils literal notranslate"><span class="pre">N</span></code> rows.</p></li>
<li><p><strong>p</strong> (<em>float</em>) – probability of the input to be zeroed.</p></li>
<li><p><strong>mode</strong> (<em>str</em>) – <code class="docutils literal notranslate"><span class="pre">&quot;batch&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;row&quot;</span></code>.
<code class="docutils literal notranslate"><span class="pre">&quot;batch&quot;</span></code> randomly zeroes the entire input, <code class="docutils literal notranslate"><span class="pre">&quot;row&quot;</span></code> zeroes
randomly selected rows from the batch.</p></li>
<li><p><strong>training</strong> – apply stochastic depth if is <code class="docutils literal notranslate"><span class="pre">True</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The randomly zeroed tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor[N, ..]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="espnet2-asr-state-spaces-block">
<span id="id25"></span><h2>espnet2.asr.state_spaces.block<a class="headerlink" href="#espnet2-asr-state-spaces-block" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.state_spaces.block"></span><p>Implements a full residual block around a black box layer.</p>
<p>Configurable options include:
normalization position: prenorm or postnorm
normalization type: batchnorm, layernorm etc.
subsampling/pooling
residual options: feedforward, residual, affine scalars, depth-dependent scaling, etc.</p>
<dl class="class">
<dt id="espnet2.asr.state_spaces.block.SequenceResidualBlock">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.block.</code><code class="sig-name descname">SequenceResidualBlock</code><span class="sig-paren">(</span><em class="sig-param">d_input</em>, <em class="sig-param">i_layer=None</em>, <em class="sig-param">prenorm=True</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">tie_dropout=False</em>, <em class="sig-param">transposed=False</em>, <em class="sig-param">layer=None</em>, <em class="sig-param">residual=None</em>, <em class="sig-param">norm=None</em>, <em class="sig-param">pool=None</em>, <em class="sig-param">drop_path=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/block.html#SequenceResidualBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.block.SequenceResidualBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.base.SequenceModule" title="espnet2.asr.state_spaces.base.SequenceModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.base.SequenceModule</span></code></a></p>
<p>Residual block wrapper for black box layer.</p>
<p>The SequenceResidualBlock class implements a generic
(batch, length, d_input) -&gt; (batch, length, d_input) transformation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_input</strong> – Input feature dimension</p></li>
<li><p><strong>i_layer</strong> – Layer index, only needs to be passed into certain residuals like Decay</p></li>
<li><p><strong>dropout</strong> – Dropout for black box module</p></li>
<li><p><strong>tie_dropout</strong> – Tie dropout mask across sequence like nn.Dropout1d/nn.Dropout2d</p></li>
<li><p><strong>transposed</strong> – Transpose inputs so each layer receives (batch, dim, length)</p></li>
<li><p><strong>layer</strong> – Config for black box module</p></li>
<li><p><strong>residual</strong> – Config for residual function</p></li>
<li><p><strong>norm</strong> – Config for normalization layer</p></li>
<li><p><strong>pool</strong> – Config for pooling layer per stage</p></li>
<li><p><strong>drop_path</strong> – Drop ratio for stochastic depth</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.state_spaces.block.SequenceResidualBlock.d_output">
<em class="property">property </em><code class="sig-name descname">d_output</code><a class="headerlink" href="#espnet2.asr.state_spaces.block.SequenceResidualBlock.d_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Output dimension of model.</p>
<p>This attribute is required for all SequenceModule instantiations.
It is used by the rest of the pipeline
(e.g. model backbone, decoder) to track the internal shapes of the full model.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.block.SequenceResidualBlock.d_state">
<em class="property">property </em><code class="sig-name descname">d_state</code><a class="headerlink" href="#espnet2.asr.state_spaces.block.SequenceResidualBlock.d_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Return dimension of output of self.state_to_tensor.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.block.SequenceResidualBlock.default_state">
<code class="sig-name descname">default_state</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/block.html#SequenceResidualBlock.default_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.block.SequenceResidualBlock.default_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Create initial state for a batch of inputs.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.block.SequenceResidualBlock.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/block.html#SequenceResidualBlock.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.block.SequenceResidualBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>A sequence-to-sequence transformation with an optional state.</p>
<p>Generally, this should map a tensor of shape
(batch, length, self.d_model) to (batch, length, self.d_output)</p>
<p>Additionally, it returns a “state” which can be any additional information
For example, RNN and SSM layers may return their hidden state,
while some types of transformer layers
(e.g. Transformer-XL) may want to pass a state as well</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.block.SequenceResidualBlock.state_to_tensor">
<em class="property">property </em><code class="sig-name descname">state_to_tensor</code><a class="headerlink" href="#espnet2.asr.state_spaces.block.SequenceResidualBlock.state_to_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a function mapping a state to a single tensor.</p>
<p>This method should be implemented if one wants to use
the hidden state insteadof the output sequence for final prediction.
Currently only used with the StateDecoder.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.block.SequenceResidualBlock.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/block.html#SequenceResidualBlock.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.block.SequenceResidualBlock.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step the model recurrently for one step of the input sequence.</p>
<p>For example, this should correspond to unrolling an RNN for one step.
If the forward pass has signature (B, L, H1) -&gt; (B, L, H2),
this method should generally have signature
(B, H1) -&gt; (B, H2) with an optional recurrent state.</p>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-state-spaces-pool">
<span id="id26"></span><h2>espnet2.asr.state_spaces.pool<a class="headerlink" href="#espnet2-asr-state-spaces-pool" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.state_spaces.pool"></span><p>Implements downsampling and upsampling on sequences.</p>
<dl class="class">
<dt id="espnet2.asr.state_spaces.pool.DownAvgPool">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.pool.</code><code class="sig-name descname">DownAvgPool</code><span class="sig-paren">(</span><em class="sig-param">d_input</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">expand=1</em>, <em class="sig-param">transposed=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownAvgPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownAvgPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.base.SequenceModule" title="espnet2.asr.state_spaces.base.SequenceModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.base.SequenceModule</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownAvgPool.d_output">
<em class="property">property </em><code class="sig-name descname">d_output</code><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownAvgPool.d_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Output dimension of model.</p>
<p>This attribute is required for all SequenceModule instantiations.
It is used by the rest of the pipeline
(e.g. model backbone, decoder) to track the internal shapes of the full model.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownAvgPool.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownAvgPool.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownAvgPool.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>A sequence-to-sequence transformation with an optional state.</p>
<p>Generally, this should map a tensor of shape
(batch, length, self.d_model) to (batch, length, self.d_output)</p>
<p>Additionally, it returns a “state” which can be any additional information
For example, RNN and SSM layers may return their hidden state,
while some types of transformer layers
(e.g. Transformer-XL) may want to pass a state as well</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownAvgPool.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownAvgPool.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownAvgPool.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step the model recurrently for one step of the input sequence.</p>
<p>For example, this should correspond to unrolling an RNN for one step.
If the forward pass has signature (B, L, H1) -&gt; (B, L, H2),
this method should generally have signature
(B, H1) -&gt; (B, H2) with an optional recurrent state.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.pool.DownLinearPool">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.pool.</code><code class="sig-name descname">DownLinearPool</code><span class="sig-paren">(</span><em class="sig-param">d_input</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">expand=1</em>, <em class="sig-param">transposed=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownLinearPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownLinearPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.base.SequenceModule" title="espnet2.asr.state_spaces.base.SequenceModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.base.SequenceModule</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownLinearPool.d_output">
<em class="property">property </em><code class="sig-name descname">d_output</code><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownLinearPool.d_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Output dimension of model.</p>
<p>This attribute is required for all SequenceModule instantiations.
It is used by the rest of the pipeline
(e.g. model backbone, decoder) to track the internal shapes of the full model.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownLinearPool.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownLinearPool.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownLinearPool.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>A sequence-to-sequence transformation with an optional state.</p>
<p>Generally, this should map a tensor of shape
(batch, length, self.d_model) to (batch, length, self.d_output)</p>
<p>Additionally, it returns a “state” which can be any additional information
For example, RNN and SSM layers may return their hidden state,
while some types of transformer layers
(e.g. Transformer-XL) may want to pass a state as well</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownLinearPool.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownLinearPool.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownLinearPool.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step the model recurrently for one step of the input sequence.</p>
<p>For example, this should correspond to unrolling an RNN for one step.
If the forward pass has signature (B, L, H1) -&gt; (B, L, H2),
this method should generally have signature
(B, H1) -&gt; (B, H2) with an optional recurrent state.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.pool.DownPool">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.pool.</code><code class="sig-name descname">DownPool</code><span class="sig-paren">(</span><em class="sig-param">d_input</em>, <em class="sig-param">d_output=None</em>, <em class="sig-param">expand=None</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">transposed=True</em>, <em class="sig-param">weight_norm=True</em>, <em class="sig-param">initializer=None</em>, <em class="sig-param">activation=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.base.SequenceModule" title="espnet2.asr.state_spaces.base.SequenceModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.base.SequenceModule</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownPool.default_state">
<code class="sig-name descname">default_state</code><span class="sig-paren">(</span><em class="sig-param">*batch_shape</em>, <em class="sig-param">device=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownPool.default_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownPool.default_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Create initial state for a batch of inputs.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownPool.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownPool.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownPool.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>A sequence-to-sequence transformation with an optional state.</p>
<p>Generally, this should map a tensor of shape
(batch, length, self.d_model) to (batch, length, self.d_output)</p>
<p>Additionally, it returns a “state” which can be any additional information
For example, RNN and SSM layers may return their hidden state,
while some types of transformer layers
(e.g. Transformer-XL) may want to pass a state as well</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownPool.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownPool.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownPool.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step one time step as a recurrent model.</p>
<p>x: (…, H)</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.pool.DownPool2d">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.pool.</code><code class="sig-name descname">DownPool2d</code><span class="sig-paren">(</span><em class="sig-param">d_input</em>, <em class="sig-param">d_output</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">transposed=True</em>, <em class="sig-param">weight_norm=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.base.SequenceModule" title="espnet2.asr.state_spaces.base.SequenceModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.base.SequenceModule</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownPool2d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownPool2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownPool2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>A sequence-to-sequence transformation with an optional state.</p>
<p>Generally, this should map a tensor of shape
(batch, length, self.d_model) to (batch, length, self.d_output)</p>
<p>Additionally, it returns a “state” which can be any additional information
For example, RNN and SSM layers may return their hidden state,
while some types of transformer layers
(e.g. Transformer-XL) may want to pass a state as well</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.pool.DownSample">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.pool.</code><code class="sig-name descname">DownSample</code><span class="sig-paren">(</span><em class="sig-param">d_input</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">expand=1</em>, <em class="sig-param">transposed=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownSample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownSample" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.base.SequenceModule" title="espnet2.asr.state_spaces.base.SequenceModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.base.SequenceModule</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownSample.d_output">
<em class="property">property </em><code class="sig-name descname">d_output</code><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownSample.d_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Output dimension of model.</p>
<p>This attribute is required for all SequenceModule instantiations.
It is used by the rest of the pipeline
(e.g. model backbone, decoder) to track the internal shapes of the full model.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownSample.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownSample.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownSample.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>A sequence-to-sequence transformation with an optional state.</p>
<p>Generally, this should map a tensor of shape
(batch, length, self.d_model) to (batch, length, self.d_output)</p>
<p>Additionally, it returns a “state” which can be any additional information
For example, RNN and SSM layers may return their hidden state,
while some types of transformer layers
(e.g. Transformer-XL) may want to pass a state as well</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownSample.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownSample.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownSample.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step the model recurrently for one step of the input sequence.</p>
<p>For example, this should correspond to unrolling an RNN for one step.
If the forward pass has signature (B, L, H1) -&gt; (B, L, H2),
this method should generally have signature
(B, H1) -&gt; (B, H2) with an optional recurrent state.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.pool.DownSpectralPool">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.pool.</code><code class="sig-name descname">DownSpectralPool</code><span class="sig-paren">(</span><em class="sig-param">d_input</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">expand=1</em>, <em class="sig-param">transposed=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownSpectralPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownSpectralPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.base.SequenceModule" title="espnet2.asr.state_spaces.base.SequenceModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.base.SequenceModule</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownSpectralPool.d_output">
<em class="property">property </em><code class="sig-name descname">d_output</code><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownSpectralPool.d_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Output dimension of model.</p>
<p>This attribute is required for all SequenceModule instantiations.
It is used by the rest of the pipeline
(e.g. model backbone, decoder) to track the internal shapes of the full model.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownSpectralPool.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownSpectralPool.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownSpectralPool.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>x: (B, L…, D)</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.DownSpectralPool.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#DownSpectralPool.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.DownSpectralPool.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step the model recurrently for one step of the input sequence.</p>
<p>For example, this should correspond to unrolling an RNN for one step.
If the forward pass has signature (B, L, H1) -&gt; (B, L, H2),
this method should generally have signature
(B, H1) -&gt; (B, H2) with an optional recurrent state.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.pool.UpPool">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.pool.</code><code class="sig-name descname">UpPool</code><span class="sig-paren">(</span><em class="sig-param">d_input</em>, <em class="sig-param">d_output</em>, <em class="sig-param">stride</em>, <em class="sig-param">transposed=True</em>, <em class="sig-param">weight_norm=True</em>, <em class="sig-param">initializer=None</em>, <em class="sig-param">activation=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#UpPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.UpPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.base.SequenceModule" title="espnet2.asr.state_spaces.base.SequenceModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.base.SequenceModule</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.UpPool.d_output">
<em class="property">property </em><code class="sig-name descname">d_output</code><a class="headerlink" href="#espnet2.asr.state_spaces.pool.UpPool.d_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Output dimension of model.</p>
<p>This attribute is required for all SequenceModule instantiations.
It is used by the rest of the pipeline
(e.g. model backbone, decoder) to track the internal shapes of the full model.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.UpPool.default_state">
<code class="sig-name descname">default_state</code><span class="sig-paren">(</span><em class="sig-param">*batch_shape</em>, <em class="sig-param">device=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#UpPool.default_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.UpPool.default_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Create initial state for a batch of inputs.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.UpPool.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">skip=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#UpPool.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.UpPool.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>A sequence-to-sequence transformation with an optional state.</p>
<p>Generally, this should map a tensor of shape
(batch, length, self.d_model) to (batch, length, self.d_output)</p>
<p>Additionally, it returns a “state” which can be any additional information
For example, RNN and SSM layers may return their hidden state,
while some types of transformer layers
(e.g. Transformer-XL) may want to pass a state as well</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.UpPool.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#UpPool.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.UpPool.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step one time step as a recurrent model.</p>
<p>x: (…, H)</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.pool.UpSample">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.pool.</code><code class="sig-name descname">UpSample</code><span class="sig-paren">(</span><em class="sig-param">d_input</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">expand=1</em>, <em class="sig-param">transposed=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#UpSample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.UpSample" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.UpSample.d_output">
<em class="property">property </em><code class="sig-name descname">d_output</code><a class="headerlink" href="#espnet2.asr.state_spaces.pool.UpSample.d_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.UpSample.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#UpSample.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.UpSample.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.pool.UpSample.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#UpSample.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.UpSample.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.pool.downsample">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.pool.</code><code class="sig-name descname">downsample</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">expand=1</em>, <em class="sig-param">transposed=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#downsample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.downsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.pool.upsample">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.pool.</code><code class="sig-name descname">upsample</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">expand=1</em>, <em class="sig-param">transposed=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/pool.html#upsample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.pool.upsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-asr-state-spaces-base">
<span id="id27"></span><h2>espnet2.asr.state_spaces.base<a class="headerlink" href="#espnet2-asr-state-spaces-base" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.state_spaces.base"></span><dl class="class">
<dt id="espnet2.asr.state_spaces.base.SequenceIdentity">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.base.</code><code class="sig-name descname">SequenceIdentity</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">transposed=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/base.html#SequenceIdentity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.base.SequenceIdentity" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.base.SequenceIdentity" title="espnet2.asr.state_spaces.base.SequenceIdentity"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.base.SequenceIdentity</span></code></a></p>
<p>Simple SequenceModule for testing purposes.</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.base.SequenceIdentity.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/base.html#SequenceIdentity.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.base.SequenceIdentity.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.base.SequenceModule">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.base.</code><code class="sig-name descname">SequenceModule</code><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/base.html#SequenceModule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.base.SequenceModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Abstract sequence model class.</p>
<p>All models must adhere to this interface</p>
<p>A SequenceModule is generally a model that transforms an input of shape
(n_batch, l_sequence, d_model) to (n_batch, l_sequence, d_output)</p>
<p>REQUIRED methods and attributes
forward, d_model, d_output: controls standard forward pass,
a sequence-to-sequence transformation
__init__ should also satisfy the following interface;
see SequenceIdentity for an example</p>
<blockquote>
<div><p>def __init__(self, d_model, transposed=False, <a href="#id28"><span class="problematic" id="id29">**</span></a>kwargs)</p>
</div></blockquote>
<p>OPTIONAL methods
default_state, step: allows stepping the model recurrently with a hidden state
state_to_tensor, d_state: allows decoding from hidden state</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.base.SequenceModule.d_model">
<em class="property">property </em><code class="sig-name descname">d_model</code><a class="headerlink" href="#espnet2.asr.state_spaces.base.SequenceModule.d_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Model dimension (generally same as input dimension).</p>
<p>This attribute is required for all SequenceModule instantiations.
It is used by the rest of the pipeline
(e.g. model backbone, encoder) to track the internal shapes of the full model.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.base.SequenceModule.d_output">
<em class="property">property </em><code class="sig-name descname">d_output</code><a class="headerlink" href="#espnet2.asr.state_spaces.base.SequenceModule.d_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Output dimension of model.</p>
<p>This attribute is required for all SequenceModule instantiations.
It is used by the rest of the pipeline
(e.g. model backbone, decoder) to track the internal shapes of the full model.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.base.SequenceModule.d_state">
<em class="property">property </em><code class="sig-name descname">d_state</code><a class="headerlink" href="#espnet2.asr.state_spaces.base.SequenceModule.d_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Return dimension of output of self.state_to_tensor.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.base.SequenceModule.default_state">
<code class="sig-name descname">default_state</code><span class="sig-paren">(</span><em class="sig-param">*batch_shape</em>, <em class="sig-param">device=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/base.html#SequenceModule.default_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.base.SequenceModule.default_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Create initial state for a batch of inputs.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.base.SequenceModule.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/base.html#SequenceModule.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.base.SequenceModule.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>A sequence-to-sequence transformation with an optional state.</p>
<p>Generally, this should map a tensor of shape
(batch, length, self.d_model) to (batch, length, self.d_output)</p>
<p>Additionally, it returns a “state” which can be any additional information
For example, RNN and SSM layers may return their hidden state,
while some types of transformer layers
(e.g. Transformer-XL) may want to pass a state as well</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.base.SequenceModule.state_to_tensor">
<em class="property">property </em><code class="sig-name descname">state_to_tensor</code><a class="headerlink" href="#espnet2.asr.state_spaces.base.SequenceModule.state_to_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a function mapping a state to a single tensor.</p>
<p>This method should be implemented if one wants to use
the hidden state insteadof the output sequence for final prediction.
Currently only used with the StateDecoder.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.base.SequenceModule.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/base.html#SequenceModule.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.base.SequenceModule.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step the model recurrently for one step of the input sequence.</p>
<p>For example, this should correspond to unrolling an RNN for one step.
If the forward pass has signature (B, L, H1) -&gt; (B, L, H2),
this method should generally have signature
(B, H1) -&gt; (B, H2) with an optional recurrent state.</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.base.TransposedModule">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.base.</code><code class="sig-name descname">TransposedModule</code><span class="sig-paren">(</span><em class="sig-param">module</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/base.html#TransposedModule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.base.TransposedModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Transpose module.</p>
<p>Wrap a SequenceModule class to accept transposed parameter,
handle state, absorb kwargs</p>
</dd></dl>

</section>
<section id="espnet2-asr-state-spaces-init">
<span id="id30"></span><h2>espnet2.asr.state_spaces.__init__<a class="headerlink" href="#espnet2-asr-state-spaces-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.state_spaces.__init__"></span><p>Initialize sub package.</p>
</section>
<section id="espnet2-asr-state-spaces-model">
<span id="id31"></span><h2>espnet2.asr.state_spaces.model<a class="headerlink" href="#espnet2-asr-state-spaces-model" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.state_spaces.model"></span><dl class="class">
<dt id="espnet2.asr.state_spaces.model.SequenceModel">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.model.</code><code class="sig-name descname">SequenceModel</code><span class="sig-paren">(</span><em class="sig-param">d_model</em>, <em class="sig-param">n_layers=1</em>, <em class="sig-param">transposed=False</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">tie_dropout=False</em>, <em class="sig-param">prenorm=True</em>, <em class="sig-param">n_repeat=1</em>, <em class="sig-param">layer=None</em>, <em class="sig-param">residual=None</em>, <em class="sig-param">norm=None</em>, <em class="sig-param">pool=None</em>, <em class="sig-param">track_norms=True</em>, <em class="sig-param">dropinp=0.0</em>, <em class="sig-param">drop_path=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/model.html#SequenceModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.model.SequenceModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.base.SequenceModule" title="espnet2.asr.state_spaces.base.SequenceModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.base.SequenceModule</span></code></a></p>
<p>Isotropic deep sequence model backbone, in the style of ResNets / Transformers.</p>
<p>The SequenceModel class implements a generic
(batch, length, d_input) -&gt; (batch, length, d_output) transformation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> – Resize input (useful for deep models with residuals)</p></li>
<li><p><strong>n_layers</strong> – Number of layers</p></li>
<li><p><strong>transposed</strong> – Transpose inputs so each layer receives (batch, dim, length)</p></li>
<li><p><strong>dropout</strong> – Dropout parameter applied on every residual and every layer</p></li>
<li><p><strong>tie_dropout</strong> – Tie dropout mask across sequence like nn.Dropout1d/nn.Dropout2d</p></li>
<li><p><strong>prenorm</strong> – Pre-norm vs. post-norm</p></li>
<li><p><strong>n_repeat</strong> – Each layer is repeated n times per stage before applying pooling</p></li>
<li><p><strong>layer</strong> – Layer config, must be specified</p></li>
<li><p><strong>residual</strong> – Residual config</p></li>
<li><p><strong>norm</strong> – Normalization config (e.g. layer vs batch)</p></li>
<li><p><strong>pool</strong> – Config for pooling layer per stage</p></li>
<li><p><strong>track_norms</strong> – Log norms of each layer output</p></li>
<li><p><strong>dropinp</strong> – Input dropout</p></li>
<li><p><strong>drop_path</strong> – Stochastic depth for each residual path</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.state_spaces.model.SequenceModel.d_state">
<em class="property">property </em><code class="sig-name descname">d_state</code><a class="headerlink" href="#espnet2.asr.state_spaces.model.SequenceModel.d_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Return dimension of output of self.state_to_tensor.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.model.SequenceModel.default_state">
<code class="sig-name descname">default_state</code><span class="sig-paren">(</span><em class="sig-param">*batch_shape</em>, <em class="sig-param">device=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/model.html#SequenceModel.default_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.model.SequenceModel.default_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Create initial state for a batch of inputs.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.model.SequenceModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">*args</em>, <em class="sig-param">state=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/model.html#SequenceModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.model.SequenceModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>A sequence-to-sequence transformation with an optional state.</p>
<p>Generally, this should map a tensor of shape
(batch, length, self.d_model) to (batch, length, self.d_output)</p>
<p>Additionally, it returns a “state” which can be any additional information
For example, RNN and SSM layers may return their hidden state,
while some types of transformer layers
(e.g. Transformer-XL) may want to pass a state as well</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.model.SequenceModel.state_to_tensor">
<em class="property">property </em><code class="sig-name descname">state_to_tensor</code><a class="headerlink" href="#espnet2.asr.state_spaces.model.SequenceModel.state_to_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a function mapping a state to a single tensor.</p>
<p>This method should be implemented if one wants to use
the hidden state insteadof the output sequence for final prediction.
Currently only used with the StateDecoder.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.model.SequenceModel.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/model.html#SequenceModel.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.model.SequenceModel.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step the model recurrently for one step of the input sequence.</p>
<p>For example, this should correspond to unrolling an RNN for one step.
If the forward pass has signature (B, L, H1) -&gt; (B, L, H2),
this method should generally have signature
(B, H1) -&gt; (B, H2) with an optional recurrent state.</p>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-state-spaces-ff">
<span id="id32"></span><h2>espnet2.asr.state_spaces.ff<a class="headerlink" href="#espnet2-asr-state-spaces-ff" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.state_spaces.ff"></span><p>Implementation of FFN block in the style of Transformers.</p>
<dl class="class">
<dt id="espnet2.asr.state_spaces.ff.FF">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.ff.</code><code class="sig-name descname">FF</code><span class="sig-paren">(</span><em class="sig-param">d_input</em>, <em class="sig-param">expand=2</em>, <em class="sig-param">d_output=None</em>, <em class="sig-param">transposed=False</em>, <em class="sig-param">activation='gelu'</em>, <em class="sig-param">initializer=None</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">tie_dropout=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/ff.html#FF"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.ff.FF" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.base.SequenceModule" title="espnet2.asr.state_spaces.base.SequenceModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.base.SequenceModule</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.ff.FF.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/ff.html#FF.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.ff.FF.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>A sequence-to-sequence transformation with an optional state.</p>
<p>Generally, this should map a tensor of shape
(batch, length, self.d_model) to (batch, length, self.d_output)</p>
<p>Additionally, it returns a “state” which can be any additional information
For example, RNN and SSM layers may return their hidden state,
while some types of transformer layers
(e.g. Transformer-XL) may want to pass a state as well</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.ff.FF.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">state</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/ff.html#FF.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.ff.FF.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step the model recurrently for one step of the input sequence.</p>
<p>For example, this should correspond to unrolling an RNN for one step.
If the forward pass has signature (B, L, H1) -&gt; (B, L, H2),
this method should generally have signature
(B, H1) -&gt; (B, H2) with an optional recurrent state.</p>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-state-spaces-s4">
<span id="id33"></span><h2>espnet2.asr.state_spaces.s4<a class="headerlink" href="#espnet2-asr-state-spaces-s4" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.state_spaces.s4"></span><p>Standalone version of Structured (Sequence) State Space (S4) model.</p>
<dl class="class">
<dt id="espnet2.asr.state_spaces.s4.OptimModule">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">OptimModule</code><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#OptimModule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.OptimModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Interface for Module that allows registering buffers/parameters with configurable optimizer hyperparameters. # noqa</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.OptimModule.register">
<code class="sig-name descname">register</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">tensor</em>, <em class="sig-param">lr=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#OptimModule.register"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.OptimModule.register" title="Permalink to this definition">¶</a></dt>
<dd><p>Register a tensor with a configurable learning rate and 0 weight decay.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.s4.S4">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">S4</code><span class="sig-paren">(</span><em class="sig-param">d_model</em>, <em class="sig-param">d_state=64</em>, <em class="sig-param">l_max=None</em>, <em class="sig-param">channels=1</em>, <em class="sig-param">bidirectional=False</em>, <em class="sig-param">activation='gelu'</em>, <em class="sig-param">postact='glu'</em>, <em class="sig-param">hyper_act=None</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">tie_dropout=False</em>, <em class="sig-param">bottleneck=None</em>, <em class="sig-param">gate=None</em>, <em class="sig-param">transposed=True</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">**kernel_args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#S4"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.S4" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Initialize S4 module.</p>
<p>d_state: the dimension of the state, also denoted by N
l_max: the maximum kernel length, also denoted by L.</p>
<blockquote>
<div><p>Set l_max=None to always use a global kernel</p>
</div></blockquote>
<dl class="simple">
<dt>channels: can be interpreted as a number of “heads”;</dt><dd><p>the SSM is a map from a 1-dim to C-dim sequence.
It’s not recommended to change this unless desperate for things to tune;
instead, increase d_model for larger models</p>
</dd>
</dl>
<p>bidirectional: if True, convolution kernel will be two-sided</p>
<p>activation: activation in between SS and FF
postact: activation after FF
hyper_act: use a “hypernetwork” multiplication (experimental)
dropout: standard dropout argument. tie_dropout=True ties the dropout</p>
<blockquote>
<div><p>mask across the sequence length, emulating nn.Dropout1d</p>
</div></blockquote>
<dl class="simple">
<dt>transposed: choose backbone axis ordering of</dt><dd><p>(B, L, H) (if False) or (B, H, L) (if True)
[B=batch size, L=sequence length, H=hidden dimension]</p>
</dd>
</dl>
<p>gate: add gated activation (GSS)
bottleneck: reduce SSM dimension (GSS)</p>
<p>See the class SSKernel for the kernel constructor which accepts kernel_args.
Relevant options that are worth considering
and tuning include “mode” + “measure”, “dt_min”, “dt_max”, “lr”</p>
<p>Other options are all experimental and should not need to be configured</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.S4.d_output">
<em class="property">property </em><code class="sig-name descname">d_output</code><a class="headerlink" href="#espnet2.asr.state_spaces.s4.S4.d_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.S4.default_state">
<code class="sig-name descname">default_state</code><span class="sig-paren">(</span><em class="sig-param">*batch_shape</em>, <em class="sig-param">device=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#S4.default_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.S4.default_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.S4.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">u</em>, <em class="sig-param">state=None</em>, <em class="sig-param">rate=1.0</em>, <em class="sig-param">lengths=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#S4.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.S4.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>u: (B H L) if self.transposed else (B L H)
state: (H N) never needed unless you know what you’re doing</p>
<p>Returns: same shape as u</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.S4.setup_step">
<code class="sig-name descname">setup_step</code><span class="sig-paren">(</span><em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#S4.setup_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.S4.setup_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.S4.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">u</em>, <em class="sig-param">state</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#S4.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.S4.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step one time step as a recurrent model.</p>
<p>Intended to be used during validation.</p>
<p>u: (B H)
state: (B H N)
Returns: output (B H), state (B H N)</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.s4.SSKernel">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">SSKernel</code><span class="sig-paren">(</span><em class="sig-param">H</em>, <em class="sig-param">N=64</em>, <em class="sig-param">L=None</em>, <em class="sig-param">measure='legs'</em>, <em class="sig-param">rank=1</em>, <em class="sig-param">channels=1</em>, <em class="sig-param">dt_min=0.001</em>, <em class="sig-param">dt_max=0.1</em>, <em class="sig-param">deterministic=False</em>, <em class="sig-param">lr=None</em>, <em class="sig-param">mode='nplr'</em>, <em class="sig-param">n_ssm=None</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">measure_args={}</em>, <em class="sig-param">**kernel_args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Wrapper around SSKernel parameterizations.</p>
<p>The SSKernel is expected to support the interface
forward()
default_state()
_setup_step()
step()</p>
<p>State Space Kernel which computes the convolution kernel $\bar{K}$.</p>
<dl class="simple">
<dt>H: Number of independent SSM copies;</dt><dd><p>controls the size of the model. Also called d_model in the config.</p>
</dd>
<dt>N: State size (dimensionality of parameters A, B, C).</dt><dd><p>Also called d_state in the config.
Generally shouldn’t need to be adjusted and doens’t affect speed much.</p>
</dd>
<dt>L: Maximum length of convolution kernel, if known.</dt><dd><p>Should work in the majority of cases even if not known.</p>
</dd>
<dt>measure: Options for initialization of (A, B).</dt><dd><p>For NPLR mode, recommendations are “legs”,
“fout”, “hippo” (combination of both).
For Diag mode, recommendations are “diag-inv”,
“diag-lin”, “diag-legs”, and “diag” (combination of diag-inv and diag-lin)</p>
</dd>
<dt>rank: Rank of low-rank correction for NPLR mode.</dt><dd><p>Needs to be increased for measure “legt”</p>
</dd>
<dt>channels: C channels turns the SSM from a 1-dim to C-dim map;</dt><dd><p>can think of it having C separate “heads” per SSM.
This was partly a feature to make it easier to implement bidirectionality;
it is recommended to set channels=1
and adjust H to control parameters instead</p>
</dd>
</dl>
<p>dt_min, dt_max: min and max values for the step size dt (Delta)
mode: Which kernel algorithm to use. ‘nplr’ is the full S4 model;</p>
<blockquote>
<div><p>‘diag’ is the simpler S4D; ‘slow’ is a dense version for testing</p>
</div></blockquote>
<dl class="simple">
<dt>n_ssm: Number of independent trainable (A, B) SSMs,</dt><dd><p>e.g. n_ssm=1 means all A/B parameters are tied across
the H different instantiations of C.
n_ssm=None means all H SSMs are completely independent.
Generally, changing this option can save parameters but doesn’t affect
performance or speed much. This parameter must divide H</p>
</dd>
<dt>lr: Passing in a number (e.g. 0.001) sets</dt><dd><p>attributes of SSM parameers (A, B, dt).
A custom optimizer hook is needed to configure the optimizer
to set the learning rates appropriately for these parameters.</p>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.SSKernel.default_state">
<code class="sig-name descname">default_state</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernel.default_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernel.default_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.SSKernel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">state=None</em>, <em class="sig-param">L=None</em>, <em class="sig-param">rate=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.SSKernel.forward_state">
<code class="sig-name descname">forward_state</code><span class="sig-paren">(</span><em class="sig-param">u</em>, <em class="sig-param">state</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernel.forward_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernel.forward_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward the state through a sequence.</p>
<p>i.e. computes the state after passing chunk through SSM</p>
<p>state: (B, H, N)
u: (B, H, L)</p>
<p>Returns: (B, H, N)</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.SSKernel.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">u</em>, <em class="sig-param">state</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernel.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernel.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.s4.SSKernelDiag">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">SSKernelDiag</code><span class="sig-paren">(</span><em class="sig-param">A</em>, <em class="sig-param">B</em>, <em class="sig-param">C</em>, <em class="sig-param">log_dt</em>, <em class="sig-param">L=None</em>, <em class="sig-param">disc='bilinear'</em>, <em class="sig-param">real_type='exp'</em>, <em class="sig-param">lr=None</em>, <em class="sig-param">bandlimit=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernelDiag"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernelDiag" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.s4.OptimModule" title="espnet2.asr.state_spaces.s4.OptimModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.s4.OptimModule</span></code></a></p>
<p>Version using (complex) diagonal state matrix (S4D).</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.SSKernelDiag.default_state">
<code class="sig-name descname">default_state</code><span class="sig-paren">(</span><em class="sig-param">*batch_shape</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernelDiag.default_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernelDiag.default_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.SSKernelDiag.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">L</em>, <em class="sig-param">state=None</em>, <em class="sig-param">rate=1.0</em>, <em class="sig-param">u=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernelDiag.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernelDiag.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>state: (B, H, N) initial state
rate: sampling rate factor
L: target length</p>
<p>returns:
(C, H, L) convolution kernel (generally C=1)
(B, H, L) output from initial state</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.SSKernelDiag.forward_state">
<code class="sig-name descname">forward_state</code><span class="sig-paren">(</span><em class="sig-param">u</em>, <em class="sig-param">state</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernelDiag.forward_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernelDiag.forward_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.SSKernelDiag.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">u</em>, <em class="sig-param">state</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernelDiag.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernelDiag.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.s4.SSKernelNPLR">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">SSKernelNPLR</code><span class="sig-paren">(</span><em class="sig-param">w</em>, <em class="sig-param">P</em>, <em class="sig-param">B</em>, <em class="sig-param">C</em>, <em class="sig-param">log_dt</em>, <em class="sig-param">L=None</em>, <em class="sig-param">lr=None</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">keops=False</em>, <em class="sig-param">real_type='exp'</em>, <em class="sig-param">real_tolerance=0.001</em>, <em class="sig-param">bandlimit=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernelNPLR"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernelNPLR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.s4.OptimModule" title="espnet2.asr.state_spaces.s4.OptimModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.s4.OptimModule</span></code></a></p>
<p>Stores a representation of and computes the SSKernel function.</p>
<p>K_L(A^dt, B^dt, C) corresponding to a discretized state space,
where A is Normal + Low Rank (NPLR)</p>
<p>Initialize kernel.</p>
<p>L: Maximum length; this module computes an SSM kernel of length L
A is represented by diag(w) - PP^*
w: (S, N) diagonal part
P: (R, S, N) low-rank part</p>
<p>B: (S, N)
C: (C, H, N)
dt: (H) timescale per feature
lr: [dict | float | None] hook to set lr of special parameters (A, B, dt)</p>
<p>Dimensions:
N (or d_state): state size
H (or d_model): total SSM copies
S (or n_ssm): number of trainable copies of (A, B, dt); must divide H
R (or rank): rank of low-rank part
C (or channels): system is 1-dim to C-dim</p>
<p>The forward pass of this Module returns a tensor of shape (C, H, L)</p>
<dl class="simple">
<dt>Note: tensor shape N here denotes half the true state size,</dt><dd><p>because of conjugate symmetry</p>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.SSKernelNPLR.default_state">
<code class="sig-name descname">default_state</code><span class="sig-paren">(</span><em class="sig-param">*batch_shape</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernelNPLR.default_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernelNPLR.default_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.SSKernelNPLR.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">state=None</em>, <em class="sig-param">rate=1.0</em>, <em class="sig-param">L=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernelNPLR.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernelNPLR.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
<p>state: (B, H, N) initial state
rate: sampling rate factor
L: target length</p>
<p>returns:
(C, H, L) convolution kernel (generally C=1)
(B, H, L) output from initial state</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.s4.SSKernelNPLR.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">u</em>, <em class="sig-param">state</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#SSKernelNPLR.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.SSKernelNPLR.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Step one time step as a recurrent model.</p>
<p>Must have called self._setup_step()
and created state with self.default_state() before calling this</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.s4.cauchy_naive">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">cauchy_naive</code><span class="sig-paren">(</span><em class="sig-param">v</em>, <em class="sig-param">z</em>, <em class="sig-param">w</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#cauchy_naive"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.cauchy_naive" title="Permalink to this definition">¶</a></dt>
<dd><p>Naive version.</p>
<p>v, w: (…, N)
z: (…, L)
returns: (…, L)</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.s4.combination">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">combination</code><span class="sig-paren">(</span><em class="sig-param">measures</em>, <em class="sig-param">N</em>, <em class="sig-param">R</em>, <em class="sig-param">S</em>, <em class="sig-param">**ssm_args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#combination"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.combination" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.s4.dplr">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">dplr</code><span class="sig-paren">(</span><em class="sig-param">scaling</em>, <em class="sig-param">N</em>, <em class="sig-param">rank=1</em>, <em class="sig-param">H=1</em>, <em class="sig-param">dtype=torch.float32</em>, <em class="sig-param">real_scale=1.0</em>, <em class="sig-param">imag_scale=1.0</em>, <em class="sig-param">random_real=False</em>, <em class="sig-param">random_imag=False</em>, <em class="sig-param">normalize=False</em>, <em class="sig-param">diagonal=True</em>, <em class="sig-param">random_B=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#dplr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.dplr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.s4.get_logger">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">get_logger</code><span class="sig-paren">(</span><em class="sig-param">name='espnet2.asr.state_spaces.s4'</em>, <em class="sig-param">level=20</em><span class="sig-paren">)</span> &#x2192; logging.Logger<a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#get_logger"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.get_logger" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize multi-GPU-friendly python logger.</p>
</dd></dl>

<dl class="data">
<dt id="espnet2.asr.state_spaces.s4.log">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">log</code><em class="property"> = &lt;Logger espnet2.asr.state_spaces.s4 (INFO)&gt;</em><a class="reference internal" href="../_modules/logging.html#log"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Cauchy and Vandermonde kernels</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.s4.log_vandermonde">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">log_vandermonde</code><span class="sig-paren">(</span><em class="sig-param">v</em>, <em class="sig-param">x</em>, <em class="sig-param">L</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#log_vandermonde"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.log_vandermonde" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Vandermonde product.</p>
<p>v: (…, N)
x: (…, N)
returns: (…, L) sum v x^l</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.s4.log_vandermonde_transpose">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">log_vandermonde_transpose</code><span class="sig-paren">(</span><em class="sig-param">u</em>, <em class="sig-param">v</em>, <em class="sig-param">x</em>, <em class="sig-param">L</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#log_vandermonde_transpose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.log_vandermonde_transpose" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.s4.nplr">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">nplr</code><span class="sig-paren">(</span><em class="sig-param">measure</em>, <em class="sig-param">N</em>, <em class="sig-param">rank=1</em>, <em class="sig-param">dtype=torch.float32</em>, <em class="sig-param">diagonalize_precision=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#nplr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.nplr" title="Permalink to this definition">¶</a></dt>
<dd><p>Decompose as Normal Plus Low-Rank (NPLR).</p>
<p>Return w, p, q, V, B such that
(w - p q^*, B) is unitarily equivalent to the original HiPPO A, B by the matrix V
i.e. A = V[w - p q^*]V^*, B = V B</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.s4.power">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">power</code><span class="sig-paren">(</span><em class="sig-param">L</em>, <em class="sig-param">A</em>, <em class="sig-param">v=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#power"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.power" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute A^L and the scan sum_i A^i v_i.</p>
<p>A: (…, N, N)
v: (…, N, L)</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.s4.rank_correction">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">rank_correction</code><span class="sig-paren">(</span><em class="sig-param">measure</em>, <em class="sig-param">N</em>, <em class="sig-param">rank=1</em>, <em class="sig-param">dtype=torch.float32</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#rank_correction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.rank_correction" title="Permalink to this definition">¶</a></dt>
<dd><p>Return low-rank matrix L such that A + L is normal.</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.s4.rank_zero_only">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">rank_zero_only</code><span class="sig-paren">(</span><em class="sig-param">fn: Callable</em><span class="sig-paren">)</span> &#x2192; Callable<a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#rank_zero_only"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.rank_zero_only" title="Permalink to this definition">¶</a></dt>
<dd><p>Decorator function from PyTorch Lightning.</p>
<p>Function that can be used as a decorator
to enable a function/method being called only on global rank 0.</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.s4.ssm">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">ssm</code><span class="sig-paren">(</span><em class="sig-param">measure</em>, <em class="sig-param">N</em>, <em class="sig-param">R</em>, <em class="sig-param">H</em>, <em class="sig-param">**ssm_args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#ssm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.ssm" title="Permalink to this definition">¶</a></dt>
<dd><p>Dispatcher to create single SSM initialization.</p>
<p>N: state size
R: rank (for DPLR parameterization)
H: number of independent SSM copies</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.asr.state_spaces.s4.transition">
<code class="sig-prename descclassname">espnet2.asr.state_spaces.s4.</code><code class="sig-name descname">transition</code><span class="sig-paren">(</span><em class="sig-param">measure</em>, <em class="sig-param">N</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/s4.html#transition"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.s4.transition" title="Permalink to this definition">¶</a></dt>
<dd><p>A, B transition matrices for different measures.</p>
</dd></dl>

</section>
<section id="espnet2-asr-state-spaces-residual">
<span id="id34"></span><h2>espnet2.asr.state_spaces.residual<a class="headerlink" href="#espnet2-asr-state-spaces-residual" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.state_spaces.residual"></span><p>Implementations of different types of residual functions.</p>
<dl class="class">
<dt id="espnet2.asr.state_spaces.residual.Affine">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.residual.</code><code class="sig-name descname">Affine</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">scalar=True</em>, <em class="sig-param">gamma=0.0</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/residual.html#Affine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.residual.Affine" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.residual.Residual" title="espnet2.asr.state_spaces.residual.Residual"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.residual.Residual</span></code></a></p>
<p>Residual connection with learnable scalar multipliers on the main branch.</p>
<p>scalar: Single scalar multiplier, or one per dimension
scale, power: Initialize to scale * layer_num**(-power)</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.residual.Affine.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">y</em>, <em class="sig-param">transposed</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/residual.html#Affine.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.residual.Affine.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.residual.DecayResidual">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.residual.</code><code class="sig-name descname">DecayResidual</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">power=0.5</em>, <em class="sig-param">l2=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/residual.html#DecayResidual"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.residual.DecayResidual" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.residual.Residual" title="espnet2.asr.state_spaces.residual.Residual"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.residual.Residual</span></code></a></p>
<p>Residual connection that can decay the linear combination depending on depth.</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.residual.DecayResidual.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">y</em>, <em class="sig-param">transposed</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/residual.html#DecayResidual.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.residual.DecayResidual.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.residual.Feedforward">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.residual.</code><code class="sig-name descname">Feedforward</code><span class="sig-paren">(</span><em class="sig-param">*args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/residual.html#Feedforward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.residual.Feedforward" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.residual.Residual" title="espnet2.asr.state_spaces.residual.Residual"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.residual.Residual</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.residual.Highway">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.residual.</code><code class="sig-name descname">Highway</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">scaling_correction=False</em>, <em class="sig-param">elemwise=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/residual.html#Highway"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.residual.Highway" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.state_spaces.residual.Residual" title="espnet2.asr.state_spaces.residual.Residual"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.state_spaces.residual.Residual</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.residual.Highway.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">y</em>, <em class="sig-param">transposed=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/residual.html#Highway.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.residual.Highway.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.state_spaces.residual.Residual">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.state_spaces.residual.</code><code class="sig-name descname">Residual</code><span class="sig-paren">(</span><em class="sig-param">i_layer</em>, <em class="sig-param">d_input</em>, <em class="sig-param">d_model</em>, <em class="sig-param">alpha=1.0</em>, <em class="sig-param">beta=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/residual.html#Residual"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.residual.Residual" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Residual connection with constant affine weights.</p>
<p>Can simulate standard residual, no residual, and “constant gates”.</p>
<dl class="method">
<dt id="espnet2.asr.state_spaces.residual.Residual.d_output">
<em class="property">property </em><code class="sig-name descname">d_output</code><a class="headerlink" href="#espnet2.asr.state_spaces.residual.Residual.d_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.state_spaces.residual.Residual.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">y</em>, <em class="sig-param">transposed</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/state_spaces/residual.html#Residual.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.state_spaces.residual.Residual.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-abs-encoder">
<span id="id35"></span><h2>espnet2.asr.encoder.abs_encoder<a class="headerlink" href="#espnet2-asr-encoder-abs-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.abs_encoder"></span><dl class="class">
<dt id="espnet2.asr.encoder.abs_encoder.AbsEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.abs_encoder.</code><code class="sig-name descname">AbsEncoder</code><a class="reference internal" href="../_modules/espnet2/asr/encoder/abs_encoder.html#AbsEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.encoder.abs_encoder.AbsEncoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/abs_encoder.html#AbsEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.abs_encoder.AbsEncoder.output_size">
<em class="property">abstract </em><code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/abs_encoder.html#AbsEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-vgg-rnn-encoder">
<span id="id36"></span><h2>espnet2.asr.encoder.vgg_rnn_encoder<a class="headerlink" href="#espnet2-asr-encoder-vgg-rnn-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.vgg_rnn_encoder"></span><dl class="class">
<dt id="espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.vgg_rnn_encoder.</code><code class="sig-name descname">VGGRNNEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">rnn_type: str = 'lstm'</em>, <em class="sig-param">bidirectional: bool = True</em>, <em class="sig-param">use_projection: bool = True</em>, <em class="sig-param">num_layers: int = 4</em>, <em class="sig-param">hidden_size: int = 320</em>, <em class="sig-param">output_size: int = 320</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">in_channel: int = 1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/vgg_rnn_encoder.html#VGGRNNEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>VGGRNNEncoder class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – The number of expected features in the input</p></li>
<li><p><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code> becomes a bidirectional LSTM</p></li>
<li><p><strong>use_projection</strong> – Use projection layer or not</p></li>
<li><p><strong>num_layers</strong> – Number of recurrent layers</p></li>
<li><p><strong>hidden_size</strong> – The number of hidden features</p></li>
<li><p><strong>output_size</strong> – The number of output features</p></li>
<li><p><strong>dropout</strong> – dropout probability</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/encoder/vgg_rnn_encoder.html#VGGRNNEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/vgg_rnn_encoder.html#VGGRNNEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-rnn-encoder">
<span id="id37"></span><h2>espnet2.asr.encoder.rnn_encoder<a class="headerlink" href="#espnet2-asr-encoder-rnn-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.rnn_encoder"></span><dl class="class">
<dt id="espnet2.asr.encoder.rnn_encoder.RNNEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.rnn_encoder.</code><code class="sig-name descname">RNNEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">rnn_type: str = 'lstm'</em>, <em class="sig-param">bidirectional: bool = True</em>, <em class="sig-param">use_projection: bool = True</em>, <em class="sig-param">num_layers: int = 4</em>, <em class="sig-param">hidden_size: int = 320</em>, <em class="sig-param">output_size: int = 320</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">subsample: Optional[Sequence[int]] = (2</em>, <em class="sig-param">2</em>, <em class="sig-param">1</em>, <em class="sig-param">1)</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/rnn_encoder.html#RNNEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.rnn_encoder.RNNEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>RNNEncoder class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – The number of expected features in the input</p></li>
<li><p><strong>output_size</strong> – The number of output features</p></li>
<li><p><strong>hidden_size</strong> – The number of hidden features</p></li>
<li><p><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code> becomes a bidirectional LSTM</p></li>
<li><p><strong>use_projection</strong> – Use projection layer or not</p></li>
<li><p><strong>num_layers</strong> – Number of recurrent layers</p></li>
<li><p><strong>dropout</strong> – dropout probability</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.rnn_encoder.RNNEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/encoder/rnn_encoder.html#RNNEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.rnn_encoder.RNNEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.rnn_encoder.RNNEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/rnn_encoder.html#RNNEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.rnn_encoder.RNNEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-whisper-encoder">
<span id="id38"></span><h2>espnet2.asr.encoder.whisper_encoder<a class="headerlink" href="#espnet2-asr-encoder-whisper-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.whisper_encoder"></span><dl class="class">
<dt id="espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.whisper_encoder.</code><code class="sig-name descname">OpenAIWhisperEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int = 1</em>, <em class="sig-param">dropout_rate: float = 0.0</em>, <em class="sig-param">whisper_model: str = 'small'</em>, <em class="sig-param">download_dir: str = None</em>, <em class="sig-param">use_specaug: bool = False</em>, <em class="sig-param">specaug_conf: Optional[dict] = None</em>, <em class="sig-param">do_pad_trim: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/whisper_encoder.html#OpenAIWhisperEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Transformer-based Speech Encoder from OpenAI’s Whisper Model:</p>
<p>URL: <a class="reference external" href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></p>
<dl class="method">
<dt id="espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/whisper_encoder.html#OpenAIWhisperEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder.log_mel_spectrogram">
<code class="sig-name descname">log_mel_spectrogram</code><span class="sig-paren">(</span><em class="sig-param">audio: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/encoder/whisper_encoder.html#OpenAIWhisperEncoder.log_mel_spectrogram"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder.log_mel_spectrogram" title="Permalink to this definition">¶</a></dt>
<dd><p>Use log-mel spectrogram computation native to Whisper training</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/whisper_encoder.html#OpenAIWhisperEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder.pad_or_trim">
<code class="sig-name descname">pad_or_trim</code><span class="sig-paren">(</span><em class="sig-param">array: torch.Tensor</em>, <em class="sig-param">length: int</em>, <em class="sig-param">axis: int = -1</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/encoder/whisper_encoder.html#OpenAIWhisperEncoder.pad_or_trim"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder.pad_or_trim" title="Permalink to this definition">¶</a></dt>
<dd><p>Pad or trim the audio array to N_SAMPLES.</p>
<p>Used in zero-shot inference cases.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder.whisper_encode">
<code class="sig-name descname">whisper_encode</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/encoder/whisper_encoder.html#OpenAIWhisperEncoder.whisper_encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder.whisper_encode" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-conformer-encoder">
<span id="id39"></span><h2>espnet2.asr.encoder.conformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-conformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.conformer_encoder"></span><p>Conformer encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.conformer_encoder.ConformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.conformer_encoder.</code><code class="sig-name descname">ConformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'conv2d'</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 3</em>, <em class="sig-param">macaron_style: bool = False</em>, <em class="sig-param">rel_pos_type: str = 'legacy'</em>, <em class="sig-param">pos_enc_layer_type: str = 'rel_pos'</em>, <em class="sig-param">selfattention_layer_type: str = 'rel_selfattn'</em>, <em class="sig-param">activation_type: str = 'swish'</em>, <em class="sig-param">use_cnn_module: bool = True</em>, <em class="sig-param">zero_triu: bool = False</em>, <em class="sig-param">cnn_module_kernel: int = 31</em>, <em class="sig-param">padding_idx: int = -1</em>, <em class="sig-param">interctc_layer_idx: List[int] = []</em>, <em class="sig-param">interctc_use_conditioning: bool = False</em>, <em class="sig-param">stochastic_depth_rate: Union[float</em>, <em class="sig-param">List[float]] = 0.0</em>, <em class="sig-param">layer_drop_rate: float = 0.0</em>, <em class="sig-param">max_pos_emb_len: int = 5000</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/conformer_encoder.html#ConformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.conformer_encoder.ConformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Conformer encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>int</em>) – Input dimension.</p></li>
<li><p><strong>output_size</strong> (<em>int</em>) – Dimension of attention.</p></li>
<li><p><strong>attention_heads</strong> (<em>int</em>) – The number of heads of multi head attention.</p></li>
<li><p><strong>linear_units</strong> (<em>int</em>) – The number of units of position-wise feed forward.</p></li>
<li><p><strong>num_blocks</strong> (<em>int</em>) – The number of decoder blocks.</p></li>
<li><p><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</p></li>
<li><p><strong>attention_dropout_rate</strong> (<em>float</em>) – Dropout rate in attention.</p></li>
<li><p><strong>positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after adding positional encoding.</p></li>
<li><p><strong>input_layer</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>torch.nn.Module</em><em>]</em>) – Input layer type.</p></li>
<li><p><strong>normalize_before</strong> (<em>bool</em>) – Whether to use layer_norm before the first block.</p></li>
<li><p><strong>concat_after</strong> (<em>bool</em>) – Whether to concat attention layer’s input and output.
If True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
If False, no additional linear will be applied. i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> (<em>str</em>) – “linear”, “conv1d”, or “conv1d-linear”.</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Kernel size of positionwise conv1d layer.</p></li>
<li><p><strong>rel_pos_type</strong> (<em>str</em>) – Whether to use the latest relative positional encoding or
the legacy one. The legacy relative positional encoding will be deprecated
in the future. More Details can be found in
<a class="reference external" href="https://github.com/espnet/espnet/pull/2816">https://github.com/espnet/espnet/pull/2816</a>.</p></li>
<li><p><strong>encoder_pos_enc_layer_type</strong> (<em>str</em>) – Encoder positional encoding layer type.</p></li>
<li><p><strong>encoder_attn_layer_type</strong> (<em>str</em>) – Encoder attention layer type.</p></li>
<li><p><strong>activation_type</strong> (<em>str</em>) – Encoder activation function type.</p></li>
<li><p><strong>macaron_style</strong> (<em>bool</em>) – Whether to use macaron style for positionwise layer.</p></li>
<li><p><strong>use_cnn_module</strong> (<em>bool</em>) – Whether to use convolution module.</p></li>
<li><p><strong>zero_triu</strong> (<em>bool</em>) – Whether to zero the upper triangular part of attention matrix.</p></li>
<li><p><strong>cnn_module_kernel</strong> (<em>int</em>) – Kernerl size of convolution module.</p></li>
<li><p><strong>padding_idx</strong> (<em>int</em>) – Padding idx for input_layer=embed.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.conformer_encoder.ConformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">ctc: espnet2.asr.ctc.CTC = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/conformer_encoder.html#ConformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.conformer_encoder.ConformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – Input tensor (#batch, L, input_size).</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – Input length (#batch).</p></li>
<li><p><strong>prev_states</strong> (<em>torch.Tensor</em>) – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor (#batch, L, output_size).
torch.Tensor: Output length (#batch).
torch.Tensor: Not to be used now.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.conformer_encoder.ConformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/conformer_encoder.html#ConformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.conformer_encoder.ConformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-transformer-encoder">
<span id="id40"></span><h2>espnet2.asr.encoder.transformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-transformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.transformer_encoder"></span><p>Transformer encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.transformer_encoder.TransformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.transformer_encoder.</code><code class="sig-name descname">TransformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: Optional[str] = 'conv2d'</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 1</em>, <em class="sig-param">padding_idx: int = -1</em>, <em class="sig-param">interctc_layer_idx: List[int] = []</em>, <em class="sig-param">interctc_use_conditioning: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/transformer_encoder.html#TransformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.transformer_encoder.TransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Transformer encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>linear_units</strong> – the number of units of position-wise feed forward</p></li>
<li><p><strong>num_blocks</strong> – the number of decoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>attention_dropout_rate</strong> – dropout rate in attention</p></li>
<li><p><strong>positional_dropout_rate</strong> – dropout rate after adding positional encoding</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>concat_after</strong> – whether to concat attention layer’s input and output
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied.
i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> – linear of conv1d</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> – kernel size of positionwise conv1d layer</p></li>
<li><p><strong>padding_idx</strong> – padding_idx for input_layer=embed</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.transformer_encoder.TransformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">ctc: espnet2.asr.ctc.CTC = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/transformer_encoder.html#TransformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.transformer_encoder.TransformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.transformer_encoder.TransformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/transformer_encoder.html#TransformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.transformer_encoder.TransformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-transformer-encoder-multispkr">
<span id="id41"></span><h2>espnet2.asr.encoder.transformer_encoder_multispkr<a class="headerlink" href="#espnet2-asr-encoder-transformer-encoder-multispkr" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.transformer_encoder_multispkr"></span><p>Encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.transformer_encoder_multispkr.TransformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.transformer_encoder_multispkr.</code><code class="sig-name descname">TransformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">num_blocks_sd: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: Optional[str] = 'conv2d'</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 1</em>, <em class="sig-param">padding_idx: int = -1</em>, <em class="sig-param">num_inf: int = 1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/transformer_encoder_multispkr.html#TransformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.transformer_encoder_multispkr.TransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Transformer encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>linear_units</strong> – the number of units of position-wise feed forward</p></li>
<li><p><strong>num_blocks</strong> – the number of recognition encoder blocks</p></li>
<li><p><strong>num_blocks_sd</strong> – the number of speaker dependent encoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>attention_dropout_rate</strong> – dropout rate in attention</p></li>
<li><p><strong>positional_dropout_rate</strong> – dropout rate after adding positional encoding</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>concat_after</strong> – whether to concat attention layer’s input and output
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied.
i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> – linear of conv1d</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> – kernel size of positionwise conv1d layer</p></li>
<li><p><strong>padding_idx</strong> – padding_idx for input_layer=embed</p></li>
<li><p><strong>num_inf</strong> – number of inference output</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.transformer_encoder_multispkr.TransformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/transformer_encoder_multispkr.html#TransformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.transformer_encoder_multispkr.TransformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.transformer_encoder_multispkr.TransformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/transformer_encoder_multispkr.html#TransformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.transformer_encoder_multispkr.TransformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-longformer-encoder">
<span id="id42"></span><h2>espnet2.asr.encoder.longformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-longformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.longformer_encoder"></span><p>Conformer encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.longformer_encoder.LongformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.longformer_encoder.</code><code class="sig-name descname">LongformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str = 'conv2d', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'abs_pos', selfattention_layer_type: str = 'lf_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, padding_idx: int = -1, interctc_layer_idx: List[int] = [], interctc_use_conditioning: bool = False, attention_windows: list = [100, 100, 100, 100, 100, 100], attention_dilation: list = [1, 1, 1, 1, 1, 1], attention_mode: str = 'sliding_chunks'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/longformer_encoder.html#LongformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.longformer_encoder.LongformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.conformer_encoder.ConformerEncoder" title="espnet2.asr.encoder.conformer_encoder.ConformerEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.conformer_encoder.ConformerEncoder</span></code></a></p>
<p>Longformer SA Conformer encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>int</em>) – Input dimension.</p></li>
<li><p><strong>output_size</strong> (<em>int</em>) – Dimension of attention.</p></li>
<li><p><strong>attention_heads</strong> (<em>int</em>) – The number of heads of multi head attention.</p></li>
<li><p><strong>linear_units</strong> (<em>int</em>) – The number of units of position-wise feed forward.</p></li>
<li><p><strong>num_blocks</strong> (<em>int</em>) – The number of decoder blocks.</p></li>
<li><p><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</p></li>
<li><p><strong>attention_dropout_rate</strong> (<em>float</em>) – Dropout rate in attention.</p></li>
<li><p><strong>positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after adding positional encoding.</p></li>
<li><p><strong>input_layer</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>torch.nn.Module</em><em>]</em>) – Input layer type.</p></li>
<li><p><strong>normalize_before</strong> (<em>bool</em>) – Whether to use layer_norm before the first block.</p></li>
<li><p><strong>concat_after</strong> (<em>bool</em>) – Whether to concat attention layer’s input and output.
If True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
If False, no additional linear will be applied. i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> (<em>str</em>) – “linear”, “conv1d”, or “conv1d-linear”.</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Kernel size of positionwise conv1d layer.</p></li>
<li><p><strong>rel_pos_type</strong> (<em>str</em>) – Whether to use the latest relative positional encoding or
the legacy one. The legacy relative positional encoding will be deprecated
in the future. More Details can be found in
<a class="reference external" href="https://github.com/espnet/espnet/pull/2816">https://github.com/espnet/espnet/pull/2816</a>.</p></li>
<li><p><strong>encoder_pos_enc_layer_type</strong> (<em>str</em>) – Encoder positional encoding layer type.</p></li>
<li><p><strong>encoder_attn_layer_type</strong> (<em>str</em>) – Encoder attention layer type.</p></li>
<li><p><strong>activation_type</strong> (<em>str</em>) – Encoder activation function type.</p></li>
<li><p><strong>macaron_style</strong> (<em>bool</em>) – Whether to use macaron style for positionwise layer.</p></li>
<li><p><strong>use_cnn_module</strong> (<em>bool</em>) – Whether to use convolution module.</p></li>
<li><p><strong>zero_triu</strong> (<em>bool</em>) – Whether to zero the upper triangular part of attention matrix.</p></li>
<li><p><strong>cnn_module_kernel</strong> (<em>int</em>) – Kernerl size of convolution module.</p></li>
<li><p><strong>padding_idx</strong> (<em>int</em>) – Padding idx for input_layer=embed.</p></li>
<li><p><strong>attention_windows</strong> (<em>list</em>) – Layer-wise attention window sizes
for longformer self-attn</p></li>
<li><p><strong>attention_dilation</strong> (<em>list</em>) – Layer-wise attention dilation sizes
for longformer self-attn</p></li>
<li><p><strong>attention_mode</strong> (<em>str</em>) – Implementation for longformer self-attn.
Default=”sliding_chunks”
Choose ‘n2’, ‘tvm’ or ‘sliding_chunks’. More details in
<a class="reference external" href="https://github.com/allenai/longformer">https://github.com/allenai/longformer</a></p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.longformer_encoder.LongformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">ctc: espnet2.asr.ctc.CTC = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/longformer_encoder.html#LongformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.longformer_encoder.LongformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – Input tensor (#batch, L, input_size).</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – Input length (#batch).</p></li>
<li><p><strong>prev_states</strong> (<em>torch.Tensor</em>) – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor (#batch, L, output_size).
torch.Tensor: Output length (#batch).
torch.Tensor: Not to be used now.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.longformer_encoder.LongformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/longformer_encoder.html#LongformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.longformer_encoder.LongformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-contextual-block-conformer-encoder">
<span id="id43"></span><h2>espnet2.asr.encoder.contextual_block_conformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-contextual-block-conformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.contextual_block_conformer_encoder"></span><p>Created on Sat Aug 21 17:27:16 2021.</p>
<p>&#64;author: Keqi Deng (UCAS)</p>
<dl class="class">
<dt id="espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.contextual_block_conformer_encoder.</code><code class="sig-name descname">ContextualBlockConformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: Optional[str] = 'conv2d'</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 3</em>, <em class="sig-param">macaron_style: bool = False</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding'&gt;</em>, <em class="sig-param">selfattention_layer_type: str = 'rel_selfattn'</em>, <em class="sig-param">activation_type: str = 'swish'</em>, <em class="sig-param">use_cnn_module: bool = True</em>, <em class="sig-param">cnn_module_kernel: int = 31</em>, <em class="sig-param">padding_idx: int = -1</em>, <em class="sig-param">block_size: int = 40</em>, <em class="sig-param">hop_size: int = 16</em>, <em class="sig-param">look_ahead: int = 16</em>, <em class="sig-param">init_average: bool = True</em>, <em class="sig-param">ctx_pos_enc: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_conformer_encoder.html#ContextualBlockConformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Contextual Block Conformer encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>linear_units</strong> – the number of units of position-wise feed forward</p></li>
<li><p><strong>num_blocks</strong> – the number of decoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>attention_dropout_rate</strong> – dropout rate in attention</p></li>
<li><p><strong>positional_dropout_rate</strong> – dropout rate after adding positional encoding</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>concat_after</strong> – whether to concat attention layer’s input and output
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied.
i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> – linear of conv1d</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> – kernel size of positionwise conv1d layer</p></li>
<li><p><strong>padding_idx</strong> – padding_idx for input_layer=embed</p></li>
<li><p><strong>block_size</strong> – block size for contextual block processing</p></li>
<li><p><strong>hop_Size</strong> – hop size for block processing</p></li>
<li><p><strong>look_ahead</strong> – look-ahead size for block_processing</p></li>
<li><p><strong>init_average</strong> – whether to use average as initial context (otherwise max values)</p></li>
<li><p><strong>ctx_pos_enc</strong> – whether to use positional encoding to the context vectors</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">is_final=True</em>, <em class="sig-param">infer_mode=False</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_conformer_encoder.html#ContextualBlockConformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
<li><p><strong>infer_mode</strong> – whether to be used for inference. This is used to
distinguish between forward_train (train and validate) and
forward_infer (decode).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.forward_infer">
<code class="sig-name descname">forward_infer</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">is_final: bool = True</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_conformer_encoder.html#ContextualBlockConformerEncoder.forward_infer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.forward_infer" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.forward_train">
<code class="sig-name descname">forward_train</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_conformer_encoder.html#ContextualBlockConformerEncoder.forward_train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.forward_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_conformer_encoder.html#ContextualBlockConformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-e-branchformer-encoder">
<span id="id44"></span><h2>espnet2.asr.encoder.e_branchformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-e-branchformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.e_branchformer_encoder"></span><p>E-Branchformer encoder definition.
Reference:</p>
<blockquote>
<div><p>Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan,
Prashant Sridhar, Kyu J. Han, Shinji Watanabe,
“E-Branchformer: Branchformer with Enhanced merging
for speech recognition,” in SLT 2022.</p>
</div></blockquote>
<dl class="class">
<dt id="espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.e_branchformer_encoder.</code><code class="sig-name descname">EBranchformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">attention_layer_type: str = 'rel_selfattn'</em>, <em class="sig-param">pos_enc_layer_type: str = 'rel_pos'</em>, <em class="sig-param">rel_pos_type: str = 'latest'</em>, <em class="sig-param">cgmlp_linear_units: int = 2048</em>, <em class="sig-param">cgmlp_conv_kernel: int = 31</em>, <em class="sig-param">use_linear_after_conv: bool = False</em>, <em class="sig-param">gate_activation: str = 'identity'</em>, <em class="sig-param">num_blocks: int = 12</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: Optional[str] = 'conv2d'</em>, <em class="sig-param">zero_triu: bool = False</em>, <em class="sig-param">padding_idx: int = -1</em>, <em class="sig-param">layer_drop_rate: float = 0.0</em>, <em class="sig-param">max_pos_emb_len: int = 5000</em>, <em class="sig-param">use_ffn: bool = False</em>, <em class="sig-param">macaron_ffn: bool = False</em>, <em class="sig-param">ffn_activation_type: str = 'swish'</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">merge_conv_kernel: int = 3</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/e_branchformer_encoder.html#EBranchformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>E-Branchformer encoder module.</p>
<dl class="method">
<dt id="espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/e_branchformer_encoder.html#EBranchformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – Input tensor (#batch, L, input_size).</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – Input length (#batch).</p></li>
<li><p><strong>prev_states</strong> (<em>torch.Tensor</em>) – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor (#batch, L, output_size).
torch.Tensor: Output length (#batch).
torch.Tensor: Not to be used now.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/e_branchformer_encoder.html#EBranchformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoderLayer">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.e_branchformer_encoder.</code><code class="sig-name descname">EBranchformerEncoderLayer</code><span class="sig-paren">(</span><em class="sig-param">size: int, attn: torch.nn.modules.module.Module, cgmlp: torch.nn.modules.module.Module, feed_forward: Optional[torch.nn.modules.module.Module], feed_forward_macaron: Optional[torch.nn.modules.module.Module], dropout_rate: float, merge_conv_kernel: int = 3</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/e_branchformer_encoder.html#EBranchformerEncoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>E-Branchformer encoder layer module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em>) – model dimension</p></li>
<li><p><strong>attn</strong> – standard self-attention or efficient attention</p></li>
<li><p><strong>cgmlp</strong> – ConvolutionalGatingMLP</p></li>
<li><p><strong>feed_forward</strong> – feed-forward module, optional</p></li>
<li><p><strong>feed_forward</strong> – macaron-style feed-forward module, optional</p></li>
<li><p><strong>dropout_rate</strong> (<em>float</em>) – dropout probability</p></li>
<li><p><strong>merge_conv_kernel</strong> (<em>int</em>) – kernel size of the depth-wise conv in merge module</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoderLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x_input</em>, <em class="sig-param">mask</em>, <em class="sig-param">cache=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/e_branchformer_encoder.html#EBranchformerEncoderLayer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoderLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute encoded features.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_input</strong> (<em>Union</em><em>[</em><em>Tuple</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Input tensor w/ or w/o pos emb.
- w/ pos emb: Tuple of tensors [(#batch, time, size), (1, time, size)].
- w/o pos emb: Tensor (#batch, time, size).</p></li>
<li><p><strong>mask</strong> (<em>torch.Tensor</em>) – Mask tensor for the input (#batch, 1, time).</p></li>
<li><p><strong>cache</strong> (<em>torch.Tensor</em>) – Cache tensor of the input (#batch, time - 1, size).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor (#batch, time, size).
torch.Tensor: Mask tensor (#batch, time).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-init">
<span id="id45"></span><h2>espnet2.asr.encoder.__init__<a class="headerlink" href="#espnet2-asr-encoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.__init__"></span></section>
<section id="espnet2-asr-encoder-hubert-encoder">
<span id="id46"></span><h2>espnet2.asr.encoder.hubert_encoder<a class="headerlink" href="#espnet2-asr-encoder-hubert-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.hubert_encoder"></span><p>Encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.hubert_encoder.</code><code class="sig-name descname">FairseqHubertEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">hubert_url: str = './'</em>, <em class="sig-param">hubert_dir_path: str = './'</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">normalize_before: bool = False</em>, <em class="sig-param">freeze_finetune_updates: int = 0</em>, <em class="sig-param">dropout_rate: float = 0.0</em>, <em class="sig-param">activation_dropout: float = 0.1</em>, <em class="sig-param">attention_dropout: float = 0.0</em>, <em class="sig-param">mask_length: int = 10</em>, <em class="sig-param">mask_prob: float = 0.75</em>, <em class="sig-param">mask_selection: str = 'static'</em>, <em class="sig-param">mask_other: int = 0</em>, <em class="sig-param">apply_mask: bool = True</em>, <em class="sig-param">mask_channel_length: int = 64</em>, <em class="sig-param">mask_channel_prob: float = 0.5</em>, <em class="sig-param">mask_channel_other: int = 0</em>, <em class="sig-param">mask_channel_selection: str = 'static'</em>, <em class="sig-param">layerdrop: float = 0.1</em>, <em class="sig-param">feature_grad_mult: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>FairSeq Hubert encoder module, used for loading pretrained weight and finetuning</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>hubert_url</strong> – url to Hubert pretrained model</p></li>
<li><p><strong>hubert_dir_path</strong> – directory to download the Wav2Vec2.0 pretrained model.</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>freeze_finetune_updates</strong> – steps that freeze all layers except output layer
before tuning the whole model (nessasary to prevent overfit).</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>activation_dropout</strong> – dropout rate in activation function</p></li>
<li><p><strong>attention_dropout</strong> – dropout rate in attention</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Hubert specific Args:</dt><dd><p>Please refer to:
<a class="reference external" href="https://github.com/pytorch/fairseq/blob/master/fairseq/models/hubert/hubert.py">https://github.com/pytorch/fairseq/blob/master/fairseq/models/hubert/hubert.py</a></p>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward Hubert ASR Encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertEncoder.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.hubert_encoder.</code><code class="sig-name descname">FairseqHubertPretrainEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int = 1</em>, <em class="sig-param">output_size: int = 1024</em>, <em class="sig-param">linear_units: int = 1024</em>, <em class="sig-param">attention_heads: int = 12</em>, <em class="sig-param">num_blocks: int = 12</em>, <em class="sig-param">dropout_rate: float = 0.0</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">activation_dropout_rate: float = 0.0</em>, <em class="sig-param">hubert_dict: str = './dict.txt'</em>, <em class="sig-param">label_rate: int = 100</em>, <em class="sig-param">checkpoint_activations: bool = False</em>, <em class="sig-param">sample_rate: int = 16000</em>, <em class="sig-param">use_amp: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertPretrainEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>FairSeq Hubert pretrain encoder module, only used for pretraining stage</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>linear_units</strong> – dimension of feedforward layers</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>num_blocks</strong> – the number of encoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>attention_dropout_rate</strong> – dropout rate in attention</p></li>
<li><p><strong>hubert_dict</strong> – target dictionary for Hubert pretraining</p></li>
<li><p><strong>label_rate</strong> – label frame rate. -1 for sequence label</p></li>
<li><p><strong>sample_rate</strong> – target sample rate.</p></li>
<li><p><strong>use_amp</strong> – whether to use automatic mixed precision</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.cast_mask_emb">
<code class="sig-name descname">cast_mask_emb</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertPretrainEncoder.cast_mask_emb"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.cast_mask_emb" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor</em>, <em class="sig-param">ys_pad_length: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertPretrainEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward Hubert Pretrain Encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertPretrainEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertPretrainEncoder.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.hubert_encoder.</code><code class="sig-name descname">TorchAudioHuBERTPretrainEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int = None, extractor_mode: str = 'group_norm', extractor_conv_layer_config: Optional[List[Tuple[int, int, int]]] = [(512, 10, 5), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 2, 2), (512, 2, 2)], extractor_conv_bias: bool = False, encoder_embed_dim: int = 768, encoder_projection_dropout: float = 0.1, encoder_pos_conv_kernel: int = 128, encoder_pos_conv_groups: int = 16, encoder_num_layers: int = 12, encoder_num_heads: int = 12, encoder_attention_dropout: float = 0.1, encoder_ff_interm_features: int = 3072, encoder_ff_interm_dropout: float = 0.0, encoder_dropout: float = 0.1, encoder_layer_norm_first: bool = False, encoder_layer_drop: float = 0.05, mask_prob: float = 0.8, mask_selection: str = 'static', mask_other: float = 0.0, mask_length: int = 10, no_mask_overlap: bool = False, mask_min_space: int = 1, mask_channel_prob: float = 0.0, mask_channel_selection: str = 'static', mask_channel_other: float = 0.0, mask_channel_length: int = 10, no_mask_channel_overlap: bool = False, mask_channel_min_space: int = 1, skip_masked: bool = False, skip_nomask: bool = False, num_classes: int = 100, final_dim: int = 256, feature_grad_mult: Optional[float] = 0.1, finetuning: bool = False, freeze_encoder_updates: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#TorchAudioHuBERTPretrainEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Torch Audio Hubert encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>extractor_mode</strong> – Operation mode of feature extractor.
Valid values are “group_norm” or “layer_norm”.</p></li>
<li><p><strong>extractor_conv_layer_config</strong> – Configuration of convolution layers in feature
extractor. List of convolution configuration,
i.e. [(output_channel, kernel_size, stride), …]</p></li>
<li><p><strong>extractor_conv_bias</strong> – Whether to include bias term to each convolution
operation.</p></li>
<li><p><strong>encoder_embed_dim</strong> – The dimension of embedding in encoder.</p></li>
<li><p><strong>encoder_projection_dropout</strong> – The dropout probability applied after the input
feature is projected to “encoder_embed_dim”.</p></li>
<li><p><strong>encoder_pos_conv_kernel</strong> – Kernel size of convolutional positional embeddings.</p></li>
<li><p><strong>encoder_pos_conv_groups</strong> – Number of groups of convolutional positional
embeddings.</p></li>
<li><p><strong>encoder_num_layers</strong> – Number of self attention layers in transformer block.</p></li>
<li><p><strong>encoder_num_heads</strong> – Number of heads in self attention layers.</p></li>
<li><p><strong>encoder_attention_dropout</strong> – Dropout probability applied after softmax in
self-attention layer.</p></li>
<li><p><strong>encoder_ff_interm_features</strong> – Dimension of hidden features in feed forward layer.</p></li>
<li><p><strong>encoder_ff_interm_dropout</strong> – Dropout probability applied in feedforward layer.</p></li>
<li><p><strong>encoder_dropout</strong> – Dropout probability applied at the end of feed forward layer.</p></li>
<li><p><strong>encoder_layer_norm_first</strong> – Control the order of layer norm in transformer layer
and each encoder layer. If True, in transformer layer, layer norm is
applied before features are fed to encoder layers.</p></li>
<li><p><strong>encoder_layer_drop</strong> – Probability to drop each encoder layer during training.</p></li>
<li><p><strong>mask_prob</strong> – Probability for each token to be chosen as start of the span
to be masked.</p></li>
<li><p><strong>mask_selection</strong> – How to choose the mask length.
Options: [static, uniform, normal, poisson].</p></li>
<li><p><strong>mask_other</strong> – Secondary mask argument (used for more complex distributions).</p></li>
<li><p><strong>mask_length</strong> – The lengths of the mask.</p></li>
<li><p><strong>no_mask_overlap</strong> – Whether to allow masks to overlap.</p></li>
<li><p><strong>mask_min_space</strong> – Minimum space between spans (if no overlap is enabled).</p></li>
<li><p><strong>mask_channel_prob</strong> – (float): The probability of replacing a feature with 0.</p></li>
<li><p><strong>mask_channel_selection</strong> – How to choose the mask length for channel masking.
Options: [static, uniform, normal, poisson].</p></li>
<li><p><strong>mask_channel_other</strong> – Secondary mask argument for channel masking(used for more
complex distributions).</p></li>
<li><p><strong>mask_channel_length</strong> – Minimum space between spans (if no overlap is enabled)
for channel masking.</p></li>
<li><p><strong>no_mask_channel_overlap</strong> – Whether to allow channel masks to overlap.</p></li>
<li><p><strong>mask_channel_min_space</strong> – Minimum space between spans for channel
masking(if no overlap is enabled).</p></li>
<li><p><strong>skip_masked</strong> – If True, skip computing losses over masked frames.</p></li>
<li><p><strong>skip_nomask</strong> – If True, skip computing losses over unmasked frames.</p></li>
<li><p><strong>num_classes</strong> – The number of classes in the labels.</p></li>
<li><p><strong>final_dim</strong> – Project final representations and targets to final_dim.</p></li>
<li><p><strong>feature_grad_mult</strong> – The factor to scale the convolutional feature extraction
layer gradients by. The scale factor will not affect the forward pass.</p></li>
<li><p><strong>finetuning</strong> – Whether to finetuning the model with ASR or other tasks.</p></li>
<li><p><strong>freeze_encoder_updates</strong> – The number of steps to freeze the encoder parameters
in ASR finetuning.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Hubert specific Args:</dt><dd><p>Please refer to:
<a class="reference external" href="https://pytorch.org/audio/stable/generated/torchaudio.models.hubert_pretrain_model.html#torchaudio.models.hubert_pretrain_model">https://pytorch.org/audio/stable/generated/torchaudio.models.hubert_pretrain_model.html#torchaudio.models.hubert_pretrain_model</a></p>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor = None</em>, <em class="sig-param">ys_pad_length: torch.Tensor = None</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#TorchAudioHuBERTPretrainEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward Hubert Pretrain Encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#TorchAudioHuBERTPretrainEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#TorchAudioHuBERTPretrainEncoder.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.encoder.hubert_encoder.download_hubert">
<code class="sig-prename descclassname">espnet2.asr.encoder.hubert_encoder.</code><code class="sig-name descname">download_hubert</code><span class="sig-paren">(</span><em class="sig-param">model_url</em>, <em class="sig-param">dir_path</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#download_hubert"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.download_hubert" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-asr-encoder-contextual-block-transformer-encoder">
<span id="id47"></span><h2>espnet2.asr.encoder.contextual_block_transformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-contextual-block-transformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.contextual_block_transformer_encoder"></span><p>Encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.contextual_block_transformer_encoder.</code><code class="sig-name descname">ContextualBlockTransformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: Optional[str] = 'conv2d'</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 1</em>, <em class="sig-param">padding_idx: int = -1</em>, <em class="sig-param">block_size: int = 40</em>, <em class="sig-param">hop_size: int = 16</em>, <em class="sig-param">look_ahead: int = 16</em>, <em class="sig-param">init_average: bool = True</em>, <em class="sig-param">ctx_pos_enc: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Contextual Block Transformer encoder module.</p>
<p>Details in Tsunoo et al. “Transformer ASR with contextual block processing”
(<a class="reference external" href="https://arxiv.org/abs/1910.07204">https://arxiv.org/abs/1910.07204</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>linear_units</strong> – the number of units of position-wise feed forward</p></li>
<li><p><strong>num_blocks</strong> – the number of encoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>attention_dropout_rate</strong> – dropout rate in attention</p></li>
<li><p><strong>positional_dropout_rate</strong> – dropout rate after adding positional encoding</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>concat_after</strong> – whether to concat attention layer’s input and output
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied.
i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> – linear of conv1d</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> – kernel size of positionwise conv1d layer</p></li>
<li><p><strong>padding_idx</strong> – padding_idx for input_layer=embed</p></li>
<li><p><strong>block_size</strong> – block size for contextual block processing</p></li>
<li><p><strong>hop_Size</strong> – hop size for block processing</p></li>
<li><p><strong>look_ahead</strong> – look-ahead size for block_processing</p></li>
<li><p><strong>init_average</strong> – whether to use average as initial context (otherwise max values)</p></li>
<li><p><strong>ctx_pos_enc</strong> – whether to use positional encoding to the context vectors</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">is_final=True</em>, <em class="sig-param">infer_mode=False</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
<li><p><strong>infer_mode</strong> – whether to be used for inference. This is used to
distinguish between forward_train (train and validate) and
forward_infer (decode).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward_infer">
<code class="sig-name descname">forward_infer</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">is_final: bool = True</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder.forward_infer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward_infer" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward_train">
<code class="sig-name descname">forward_train</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder.forward_train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-branchformer-encoder">
<span id="id48"></span><h2>espnet2.asr.encoder.branchformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-branchformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.branchformer_encoder"></span><p>Branchformer encoder definition.</p>
<dl class="simple">
<dt>Reference:</dt><dd><p>Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe,
“Branchformer: Parallel MLP-Attention Architectures to Capture
Local and Global Context for Speech Recognition and Understanding,”
in Proceedings of ICML, 2022.</p>
</dd>
</dl>
<dl class="class">
<dt id="espnet2.asr.encoder.branchformer_encoder.BranchformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.branchformer_encoder.</code><code class="sig-name descname">BranchformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">use_attn: bool = True</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">attention_layer_type: str = 'rel_selfattn'</em>, <em class="sig-param">pos_enc_layer_type: str = 'rel_pos'</em>, <em class="sig-param">rel_pos_type: str = 'latest'</em>, <em class="sig-param">use_cgmlp: bool = True</em>, <em class="sig-param">cgmlp_linear_units: int = 2048</em>, <em class="sig-param">cgmlp_conv_kernel: int = 31</em>, <em class="sig-param">use_linear_after_conv: bool = False</em>, <em class="sig-param">gate_activation: str = 'identity'</em>, <em class="sig-param">merge_method: str = 'concat'</em>, <em class="sig-param">cgmlp_weight: Union[float</em>, <em class="sig-param">List[float]] = 0.5</em>, <em class="sig-param">attn_branch_drop_rate: Union[float</em>, <em class="sig-param">List[float]] = 0.0</em>, <em class="sig-param">num_blocks: int = 12</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: Optional[str] = 'conv2d'</em>, <em class="sig-param">zero_triu: bool = False</em>, <em class="sig-param">padding_idx: int = -1</em>, <em class="sig-param">stochastic_depth_rate: Union[float</em>, <em class="sig-param">List[float]] = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/branchformer_encoder.html#BranchformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.branchformer_encoder.BranchformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Branchformer encoder module.</p>
<dl class="method">
<dt id="espnet2.asr.encoder.branchformer_encoder.BranchformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/branchformer_encoder.html#BranchformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.branchformer_encoder.BranchformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – Input tensor (#batch, L, input_size).</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – Input length (#batch).</p></li>
<li><p><strong>prev_states</strong> (<em>torch.Tensor</em>) – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor (#batch, L, output_size).
torch.Tensor: Output length (#batch).
torch.Tensor: Not to be used now.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.branchformer_encoder.BranchformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/branchformer_encoder.html#BranchformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.branchformer_encoder.BranchformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.encoder.branchformer_encoder.BranchformerEncoderLayer">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.branchformer_encoder.</code><code class="sig-name descname">BranchformerEncoderLayer</code><span class="sig-paren">(</span><em class="sig-param">size: int, attn: Optional[torch.nn.modules.module.Module], cgmlp: Optional[torch.nn.modules.module.Module], dropout_rate: float, merge_method: str, cgmlp_weight: float = 0.5, attn_branch_drop_rate: float = 0.0, stochastic_depth_rate: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/branchformer_encoder.html#BranchformerEncoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.branchformer_encoder.BranchformerEncoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Branchformer encoder layer module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em>) – model dimension</p></li>
<li><p><strong>attn</strong> – standard self-attention or efficient attention, optional</p></li>
<li><p><strong>cgmlp</strong> – ConvolutionalGatingMLP, optional</p></li>
<li><p><strong>dropout_rate</strong> (<em>float</em>) – dropout probability</p></li>
<li><p><strong>merge_method</strong> (<em>str</em>) – concat, learned_ave, fixed_ave</p></li>
<li><p><strong>cgmlp_weight</strong> (<em>float</em>) – weight of the cgmlp branch, between 0 and 1,
used if merge_method is fixed_ave</p></li>
<li><p><strong>attn_branch_drop_rate</strong> (<em>float</em>) – probability of dropping the attn branch,
used if merge_method is learned_ave</p></li>
<li><p><strong>stochastic_depth_rate</strong> (<em>float</em>) – stochastic depth probability</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.branchformer_encoder.BranchformerEncoderLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x_input</em>, <em class="sig-param">mask</em>, <em class="sig-param">cache=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/branchformer_encoder.html#BranchformerEncoderLayer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.branchformer_encoder.BranchformerEncoderLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute encoded features.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_input</strong> (<em>Union</em><em>[</em><em>Tuple</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Input tensor w/ or w/o pos emb.
- w/ pos emb: Tuple of tensors [(#batch, time, size), (1, time, size)].
- w/o pos emb: Tensor (#batch, time, size).</p></li>
<li><p><strong>mask</strong> (<em>torch.Tensor</em>) – Mask tensor for the input (#batch, 1, time).</p></li>
<li><p><strong>cache</strong> (<em>torch.Tensor</em>) – Cache tensor of the input (#batch, time - 1, size).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor (#batch, time, size).
torch.Tensor: Mask tensor (#batch, time).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-wav2vec2-encoder">
<span id="id49"></span><h2>espnet2.asr.encoder.wav2vec2_encoder<a class="headerlink" href="#espnet2-asr-encoder-wav2vec2-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.wav2vec2_encoder"></span><p>Encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.wav2vec2_encoder.</code><code class="sig-name descname">FairSeqWav2Vec2Encoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">w2v_url: str</em>, <em class="sig-param">w2v_dir_path: str = './'</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">normalize_before: bool = False</em>, <em class="sig-param">freeze_finetune_updates: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#FairSeqWav2Vec2Encoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>FairSeq Wav2Vec2 encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>w2v_url</strong> – url to Wav2Vec2.0 pretrained model</p></li>
<li><p><strong>w2v_dir_path</strong> – directory to download the Wav2Vec2.0 pretrained model.</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>finetune_last_n_layers</strong> – last n layers to be finetuned in Wav2Vec2.0
0 means to finetune every layer if freeze_w2v=False.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#FairSeqWav2Vec2Encoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward FairSeqWav2Vec2 Encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#FairSeqWav2Vec2Encoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#FairSeqWav2Vec2Encoder.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.download_w2v">
<code class="sig-prename descclassname">espnet2.asr.encoder.wav2vec2_encoder.</code><code class="sig-name descname">download_w2v</code><span class="sig-paren">(</span><em class="sig-param">model_url</em>, <em class="sig-param">dir_path</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#download_w2v"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.download_w2v" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-asr-specaug-specaug">
<span id="id50"></span><h2>espnet2.asr.specaug.specaug<a class="headerlink" href="#espnet2-asr-specaug-specaug" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.specaug.specaug"></span><p>SpecAugment module.</p>
<dl class="class">
<dt id="espnet2.asr.specaug.specaug.SpecAug">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.specaug.specaug.</code><code class="sig-name descname">SpecAug</code><span class="sig-paren">(</span><em class="sig-param">apply_time_warp: bool = True, time_warp_window: int = 5, time_warp_mode: str = 'bicubic', apply_freq_mask: bool = True, freq_mask_width_range: Union[int, Sequence[int]] = (0, 20), num_freq_mask: int = 2, apply_time_mask: bool = True, time_mask_width_range: Union[int, Sequence[int], None] = None, time_mask_width_ratio_range: Union[float, Sequence[float], None] = None, num_time_mask: int = 2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/specaug/specaug.html#SpecAug"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.specaug.specaug.SpecAug" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.specaug.abs_specaug.AbsSpecAug" title="espnet2.asr.specaug.abs_specaug.AbsSpecAug"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.specaug.abs_specaug.AbsSpecAug</span></code></a></p>
<p>Implementation of SpecAug.</p>
<dl>
<dt>Reference:</dt><dd><p>Daniel S. Park et al.
“SpecAugment: A Simple Data</p>
<blockquote>
<div><p>Augmentation Method for Automatic Speech Recognition”</p>
</div></blockquote>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using cuda mode, time_warp doesn’t have reproducibility
due to <cite>torch.nn.functional.interpolate</cite>.</p>
</div>
<dl class="method">
<dt id="espnet2.asr.specaug.specaug.SpecAug.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">x_lengths=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/specaug/specaug.html#SpecAug.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.specaug.specaug.SpecAug.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-specaug-abs-specaug">
<span id="id51"></span><h2>espnet2.asr.specaug.abs_specaug<a class="headerlink" href="#espnet2-asr-specaug-abs-specaug" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.specaug.abs_specaug"></span><dl class="class">
<dt id="espnet2.asr.specaug.abs_specaug.AbsSpecAug">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.specaug.abs_specaug.</code><code class="sig-name descname">AbsSpecAug</code><a class="reference internal" href="../_modules/espnet2/asr/specaug/abs_specaug.html#AbsSpecAug"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.specaug.abs_specaug.AbsSpecAug" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Abstract class for the augmentation of spectrogram</p>
<p>The process-flow:</p>
<p>Frontend  -&gt; SpecAug -&gt; Normalization -&gt; Encoder -&gt; Decoder</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.specaug.abs_specaug.AbsSpecAug.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x: torch.Tensor</em>, <em class="sig-param">x_lengths: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/specaug/abs_specaug.html#AbsSpecAug.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.specaug.abs_specaug.AbsSpecAug.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-specaug-init">
<span id="id52"></span><h2>espnet2.asr.specaug.__init__<a class="headerlink" href="#espnet2-asr-specaug-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.specaug.__init__"></span></section>
<section id="espnet2-asr-postencoder-init">
<span id="id53"></span><h2>espnet2.asr.postencoder.__init__<a class="headerlink" href="#espnet2-asr-postencoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.postencoder.__init__"></span></section>
<section id="espnet2-asr-postencoder-abs-postencoder">
<span id="id54"></span><h2>espnet2.asr.postencoder.abs_postencoder<a class="headerlink" href="#espnet2-asr-postencoder-abs-postencoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.postencoder.abs_postencoder"></span><dl class="class">
<dt id="espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.postencoder.abs_postencoder.</code><code class="sig-name descname">AbsPostEncoder</code><a class="reference internal" href="../_modules/espnet2/asr/postencoder/abs_postencoder.html#AbsPostEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/postencoder/abs_postencoder.html#AbsPostEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder.output_size">
<em class="property">abstract </em><code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/postencoder/abs_postencoder.html#AbsPostEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-postencoder-hugging-face-transformers-postencoder">
<span id="id55"></span><h2>espnet2.asr.postencoder.hugging_face_transformers_postencoder<a class="headerlink" href="#espnet2-asr-postencoder-hugging-face-transformers-postencoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.postencoder.hugging_face_transformers_postencoder"></span><p>Hugging Face Transformers PostEncoder.</p>
<dl class="class">
<dt id="espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.postencoder.hugging_face_transformers_postencoder.</code><code class="sig-name descname">HuggingFaceTransformersPostEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">model_name_or_path: str</em>, <em class="sig-param">length_adaptor_n_layers: int = 0</em>, <em class="sig-param">lang_token_id: int = -1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/postencoder/hugging_face_transformers_postencoder.html#HuggingFaceTransformersPostEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder" title="espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder</span></code></a></p>
<p>Hugging Face Transformers PostEncoder.</p>
<p>Initialize the module.</p>
<dl class="method">
<dt id="espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/postencoder/hugging_face_transformers_postencoder.html#HuggingFaceTransformersPostEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/postencoder/hugging_face_transformers_postencoder.html#HuggingFaceTransformersPostEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the output size.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/postencoder/hugging_face_transformers_postencoder.html#HuggingFaceTransformersPostEncoder.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-preencoder-sinc">
<span id="id56"></span><h2>espnet2.asr.preencoder.sinc<a class="headerlink" href="#espnet2-asr-preencoder-sinc" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.preencoder.sinc"></span><p>Sinc convolutions for raw audio input.</p>
<dl class="class">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.preencoder.sinc.</code><code class="sig-name descname">LightweightSincConvs</code><span class="sig-paren">(</span><em class="sig-param">fs: Union[int</em>, <em class="sig-param">str</em>, <em class="sig-param">float] = 16000</em>, <em class="sig-param">in_channels: int = 1</em>, <em class="sig-param">out_channels: int = 256</em>, <em class="sig-param">activation_type: str = 'leakyrelu'</em>, <em class="sig-param">dropout_type: str = 'dropout'</em>, <em class="sig-param">windowing_type: str = 'hamming'</em>, <em class="sig-param">scale_type: str = 'mel'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder" title="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder</span></code></a></p>
<p>Lightweight Sinc Convolutions.</p>
<p>Instead of using precomputed features, end-to-end speech recognition
can also be done directly from raw audio using sinc convolutions, as
described in “Lightweight End-to-End Speech Recognition from Raw Audio
Data Using Sinc-Convolutions” by Kürzinger et al.
<a class="reference external" href="https://arxiv.org/abs/2010.07597">https://arxiv.org/abs/2010.07597</a></p>
<p>To use Sinc convolutions in your model instead of the default f-bank
frontend, set this module as your pre-encoder with <cite>preencoder: sinc</cite>
and use the input of the sliding window frontend with
<cite>frontend: sliding_window</cite> in your yaml configuration file.
So that the process flow is:</p>
<p>Frontend (SlidingWindow) -&gt; SpecAug -&gt; Normalization -&gt;
Pre-encoder (LightweightSincConvs) -&gt; Encoder -&gt; Decoder</p>
<p>Note that this method also performs data augmentation in time domain
(vs. in spectral domain in the default frontend).
Use <cite>plot_sinc_filters.py</cite> to visualize the learned Sinc filters.</p>
<p>Initialize the module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fs</strong> – Sample rate.</p></li>
<li><p><strong>in_channels</strong> – Number of input channels.</p></li>
<li><p><strong>out_channels</strong> – Number of output channels (for each input channel).</p></li>
<li><p><strong>activation_type</strong> – Choice of activation function.</p></li>
<li><p><strong>dropout_type</strong> – Choice of dropout function.</p></li>
<li><p><strong>windowing_type</strong> – Choice of windowing function.</p></li>
<li><p><strong>scale_type</strong> – Choice of filter-bank initialization scale.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs.espnet_initialization_fn">
<code class="sig-name descname">espnet_initialization_fn</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs.espnet_initialization_fn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs.espnet_initialization_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize sinc filters with filterbank values.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply Lightweight Sinc Convolutions.</p>
<p>The input shall be formatted as (B, T, C_in, D_in)
with B as batch size, T as time dimension, C_in as channels,
and D_in as feature dimension.</p>
<p>The output will then be (B, T, C_out*D_out)
with C_out and D_out as output dimensions.</p>
<p>The current module structure only handles D_in=400, so that D_out=1.
Remark for the multichannel case: C_out is the number of out_channels
given at initialization multiplied with C_in.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs.gen_lsc_block">
<code class="sig-name descname">gen_lsc_block</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int</em>, <em class="sig-param">out_channels: int</em>, <em class="sig-param">depthwise_kernel_size: int = 9</em>, <em class="sig-param">depthwise_stride: int = 1</em>, <em class="sig-param">depthwise_groups=None</em>, <em class="sig-param">pointwise_groups=0</em>, <em class="sig-param">dropout_probability: float = 0.15</em>, <em class="sig-param">avgpool=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs.gen_lsc_block"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs.gen_lsc_block" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a convolutional block for Lightweight Sinc convolutions.</p>
<p>Each block consists of either a depthwise or a depthwise-separable
convolutions together with dropout, (batch-)normalization layer, and
an optional average-pooling layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> – Number of input channels.</p></li>
<li><p><strong>out_channels</strong> – Number of output channels.</p></li>
<li><p><strong>depthwise_kernel_size</strong> – Kernel size of the depthwise convolution.</p></li>
<li><p><strong>depthwise_stride</strong> – Stride of the depthwise convolution.</p></li>
<li><p><strong>depthwise_groups</strong> – Number of groups of the depthwise convolution.</p></li>
<li><p><strong>pointwise_groups</strong> – Number of groups of the pointwise convolution.</p></li>
<li><p><strong>dropout_probability</strong> – Dropout probability in the block.</p></li>
<li><p><strong>avgpool</strong> – If True, an AvgPool layer is inserted.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Neural network building block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Sequential</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs.output_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the output size.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.preencoder.sinc.SpatialDropout">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.preencoder.sinc.</code><code class="sig-name descname">SpatialDropout</code><span class="sig-paren">(</span><em class="sig-param">dropout_probability: float = 0.15</em>, <em class="sig-param">shape: Union[tuple</em>, <em class="sig-param">list</em>, <em class="sig-param">None] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#SpatialDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.SpatialDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Spatial dropout module.</p>
<p>Apply dropout to full channels on tensors of input (B, C, D)</p>
<p>Initialize.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dropout_probability</strong> – Dropout probability.</p></li>
<li><p><strong>shape</strong> (<em>tuple</em><em>, </em><em>list</em>) – Shape of input tensors.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.SpatialDropout.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#SpatialDropout.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.SpatialDropout.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward of spatial dropout module.</p>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-preencoder-abs-preencoder">
<span id="id57"></span><h2>espnet2.asr.preencoder.abs_preencoder<a class="headerlink" href="#espnet2-asr-preencoder-abs-preencoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.preencoder.abs_preencoder"></span><dl class="class">
<dt id="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.preencoder.abs_preencoder.</code><code class="sig-name descname">AbsPreEncoder</code><a class="reference internal" href="../_modules/espnet2/asr/preencoder/abs_preencoder.html#AbsPreEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/preencoder/abs_preencoder.html#AbsPreEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder.output_size">
<em class="property">abstract </em><code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/preencoder/abs_preencoder.html#AbsPreEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-preencoder-linear">
<span id="id58"></span><h2>espnet2.asr.preencoder.linear<a class="headerlink" href="#espnet2-asr-preencoder-linear" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.preencoder.linear"></span><p>Linear Projection.</p>
<dl class="class">
<dt id="espnet2.asr.preencoder.linear.LinearProjection">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.preencoder.linear.</code><code class="sig-name descname">LinearProjection</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/linear.html#LinearProjection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.linear.LinearProjection" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder" title="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder</span></code></a></p>
<p>Linear Projection Preencoder.</p>
<p>Initialize the module.</p>
<dl class="method">
<dt id="espnet2.asr.preencoder.linear.LinearProjection.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/preencoder/linear.html#LinearProjection.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.linear.LinearProjection.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.linear.LinearProjection.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/preencoder/linear.html#LinearProjection.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.linear.LinearProjection.output_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the output size.</p>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-preencoder-init">
<span id="id59"></span><h2>espnet2.asr.preencoder.__init__<a class="headerlink" href="#espnet2-asr-preencoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.preencoder.__init__"></span></section>
<section id="espnet2-asr-layers-cgmlp">
<span id="id60"></span><h2>espnet2.asr.layers.cgmlp<a class="headerlink" href="#espnet2-asr-layers-cgmlp" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.layers.cgmlp"></span><p>MLP with convolutional gating (cgMLP) definition.</p>
<p class="rubric">References</p>
<p><a class="reference external" href="https://openreview.net/forum?id=RA-zVvZLYIy">https://openreview.net/forum?id=RA-zVvZLYIy</a>
<a class="reference external" href="https://arxiv.org/abs/2105.08050">https://arxiv.org/abs/2105.08050</a></p>
<dl class="class">
<dt id="espnet2.asr.layers.cgmlp.ConvolutionalGatingMLP">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.layers.cgmlp.</code><code class="sig-name descname">ConvolutionalGatingMLP</code><span class="sig-paren">(</span><em class="sig-param">size: int</em>, <em class="sig-param">linear_units: int</em>, <em class="sig-param">kernel_size: int</em>, <em class="sig-param">dropout_rate: float</em>, <em class="sig-param">use_linear_after_conv: bool</em>, <em class="sig-param">gate_activation: str</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/layers/cgmlp.html#ConvolutionalGatingMLP"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.layers.cgmlp.ConvolutionalGatingMLP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Convolutional Gating MLP (cgMLP).</p>
<dl class="method">
<dt id="espnet2.asr.layers.cgmlp.ConvolutionalGatingMLP.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">mask</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/layers/cgmlp.html#ConvolutionalGatingMLP.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.layers.cgmlp.ConvolutionalGatingMLP.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.layers.cgmlp.ConvolutionalSpatialGatingUnit">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.layers.cgmlp.</code><code class="sig-name descname">ConvolutionalSpatialGatingUnit</code><span class="sig-paren">(</span><em class="sig-param">size: int</em>, <em class="sig-param">kernel_size: int</em>, <em class="sig-param">dropout_rate: float</em>, <em class="sig-param">use_linear_after_conv: bool</em>, <em class="sig-param">gate_activation: str</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/layers/cgmlp.html#ConvolutionalSpatialGatingUnit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.layers.cgmlp.ConvolutionalSpatialGatingUnit" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Convolutional Spatial Gating Unit (CSGU).</p>
<dl class="method">
<dt id="espnet2.asr.layers.cgmlp.ConvolutionalSpatialGatingUnit.espnet_initialization_fn">
<code class="sig-name descname">espnet_initialization_fn</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/layers/cgmlp.html#ConvolutionalSpatialGatingUnit.espnet_initialization_fn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.layers.cgmlp.ConvolutionalSpatialGatingUnit.espnet_initialization_fn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.layers.cgmlp.ConvolutionalSpatialGatingUnit.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">gate_add=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/layers/cgmlp.html#ConvolutionalSpatialGatingUnit.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.layers.cgmlp.ConvolutionalSpatialGatingUnit.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward method</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – (N, T, D)</p></li>
<li><p><strong>gate_add</strong> (<em>torch.Tensor</em>) – (N, T, D/2)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(N, T, D/2)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>out (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-layers-init">
<span id="id61"></span><h2>espnet2.asr.layers.__init__<a class="headerlink" href="#espnet2-asr-layers-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.layers.__init__"></span></section>
<section id="espnet2-asr-layers-fastformer">
<span id="id62"></span><h2>espnet2.asr.layers.fastformer<a class="headerlink" href="#espnet2-asr-layers-fastformer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.layers.fastformer"></span><p>Fastformer attention definition.</p>
<dl class="simple">
<dt>Reference:</dt><dd><p>Wu et al., “Fastformer: Additive Attention Can Be All You Need”
<a class="reference external" href="https://arxiv.org/abs/2108.09084">https://arxiv.org/abs/2108.09084</a>
<a class="reference external" href="https://github.com/wuch15/Fastformer">https://github.com/wuch15/Fastformer</a></p>
</dd>
</dl>
<dl class="class">
<dt id="espnet2.asr.layers.fastformer.FastSelfAttention">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.layers.fastformer.</code><code class="sig-name descname">FastSelfAttention</code><span class="sig-paren">(</span><em class="sig-param">size</em>, <em class="sig-param">attention_heads</em>, <em class="sig-param">dropout_rate</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/layers/fastformer.html#FastSelfAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.layers.fastformer.FastSelfAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Fast self-attention used in Fastformer.</p>
<dl class="method">
<dt id="espnet2.asr.layers.fastformer.FastSelfAttention.espnet_initialization_fn">
<code class="sig-name descname">espnet_initialization_fn</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/layers/fastformer.html#FastSelfAttention.espnet_initialization_fn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.layers.fastformer.FastSelfAttention.espnet_initialization_fn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.layers.fastformer.FastSelfAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad</em>, <em class="sig-param">mask</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/layers/fastformer.html#FastSelfAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.layers.fastformer.FastSelfAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – (batch, time, size = n_heads * attn_dim)</p></li>
<li><p><strong>mask</strong> – (batch, 1, time), nonpadding is 1, padding is 0</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(batch, time, size)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.layers.fastformer.FastSelfAttention.init_weights">
<code class="sig-name descname">init_weights</code><span class="sig-paren">(</span><em class="sig-param">module</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/layers/fastformer.html#FastSelfAttention.init_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.layers.fastformer.FastSelfAttention.init_weights" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.layers.fastformer.FastSelfAttention.transpose_for_scores">
<code class="sig-name descname">transpose_for_scores</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/layers/fastformer.html#FastSelfAttention.transpose_for_scores"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.layers.fastformer.FastSelfAttention.transpose_for_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshape and transpose to compute scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> – (batch, time, size = n_heads * attn_dim)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(batch, n_heads, time, attn_dim)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-decoder-transducer-decoder">
<span id="id63"></span><h2>espnet2.asr.decoder.transducer_decoder<a class="headerlink" href="#espnet2-asr-decoder-transducer-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.transducer_decoder"></span><p>(RNN-)Transducer decoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.decoder.transducer_decoder.TransducerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transducer_decoder.</code><code class="sig-name descname">TransducerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">rnn_type: str = 'lstm'</em>, <em class="sig-param">num_layers: int = 1</em>, <em class="sig-param">hidden_size: int = 320</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">dropout_embed: float = 0.0</em>, <em class="sig-param">embed_pad: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transducer_decoder.html#TransducerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transducer_decoder.TransducerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a></p>
<p>(RNN-)Transducer decoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – Output dimension.</p></li>
<li><p><strong>layers_type</strong> – (RNN-)Decoder layers type.</p></li>
<li><p><strong>num_layers</strong> – Number of decoder layers.</p></li>
<li><p><strong>hidden_size</strong> – Number of decoder units per layer.</p></li>
<li><p><strong>dropout</strong> – Dropout rate for decoder layers.</p></li>
<li><p><strong>dropout_embed</strong> – Dropout rate for embedding layer.</p></li>
<li><p><strong>embed_pad</strong> – Embed/Blank symbol ID.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.decoder.transducer_decoder.TransducerDecoder.batch_score">
<code class="sig-name descname">batch_score</code><span class="sig-paren">(</span><em class="sig-param">hyps: Union[List[espnet2.asr.transducer.beam_search_transducer.Hypothesis], List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis]], dec_states: Tuple[torch.Tensor, Optional[torch.Tensor]], cache: Dict[str, Any], use_lm: bool</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transducer_decoder.html#TransducerDecoder.batch_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transducer_decoder.TransducerDecoder.batch_score" title="Permalink to this definition">¶</a></dt>
<dd><p>One-step forward hypotheses.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hyps</strong> – Hypotheses.</p></li>
<li><p><strong>states</strong> – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))</p></li>
<li><p><strong>cache</strong> – Pairs of (dec_out, dec_states) for each label sequences. (keys)</p></li>
<li><p><strong>use_lm</strong> – Whether to compute label ID sequences for LM.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Decoder output sequences. (B, D_dec)
dec_states: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))
lm_labels: Label ID sequences for LM. (B,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dec_out</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transducer_decoder.TransducerDecoder.create_batch_states">
<code class="sig-name descname">create_batch_states</code><span class="sig-paren">(</span><em class="sig-param">states: Tuple[torch.Tensor, Optional[torch.Tensor]], new_states: List[Tuple[torch.Tensor, Optional[torch.Tensor]]], check_list: Optional[List] = None</em><span class="sig-paren">)</span> &#x2192; List[Tuple[torch.Tensor, Optional[torch.Tensor]]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transducer_decoder.html#TransducerDecoder.create_batch_states"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transducer_decoder.TransducerDecoder.create_batch_states" title="Permalink to this definition">¶</a></dt>
<dd><p>Create decoder hidden states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>states</strong> – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))</p></li>
<li><p><strong>new_states</strong> – Decoder hidden states. [N x ((1, D_dec), (1, D_dec))]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>states</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transducer_decoder.TransducerDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">labels: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/decoder/transducer_decoder.html#TransducerDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transducer_decoder.TransducerDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode source label sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>labels</strong> – Label ID sequences. (B, L)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Decoder output sequences. (B, T, U, D_dec)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dec_out</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transducer_decoder.TransducerDecoder.init_state">
<code class="sig-name descname">init_state</code><span class="sig-paren">(</span><em class="sig-param">batch_size: int</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Optional[torch._VariableFunctionsClass.tensor]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transducer_decoder.html#TransducerDecoder.init_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transducer_decoder.TransducerDecoder.init_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize decoder states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_size</strong> – Batch size.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Initial decoder hidden states. ((N, B, D_dec), (N, B, D_dec))</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transducer_decoder.TransducerDecoder.rnn_forward">
<code class="sig-name descname">rnn_forward</code><span class="sig-paren">(</span><em class="sig-param">sequence: torch.Tensor, state: Tuple[torch.Tensor, Optional[torch.Tensor]]</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transducer_decoder.html#TransducerDecoder.rnn_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transducer_decoder.TransducerDecoder.rnn_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode source label sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sequence</strong> – RNN input sequences. (B, D_emb)</p></li>
<li><p><strong>state</strong> – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>RNN output sequences. (B, D_dec)
(h_next, c_next): Decoder hidden states. (N, B, D_dec), (N, B, D_dec))</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>sequence</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transducer_decoder.TransducerDecoder.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">hyp: espnet2.asr.transducer.beam_search_transducer.Hypothesis, cache: Dict[str, Any]</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transducer_decoder.html#TransducerDecoder.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transducer_decoder.TransducerDecoder.score" title="Permalink to this definition">¶</a></dt>
<dd><p>One-step forward hypothesis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hyp</strong> – Hypothesis.</p></li>
<li><p><strong>cache</strong> – Pairs of (dec_out, state) for each label sequence. (key)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Decoder output sequence. (1, D_dec)
new_state: Decoder hidden states. ((N, 1, D_dec), (N, 1, D_dec))
label: Label ID for LM. (1,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dec_out</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transducer_decoder.TransducerDecoder.select_state">
<code class="sig-name descname">select_state</code><span class="sig-paren">(</span><em class="sig-param">states: Tuple[torch.Tensor, Optional[torch.Tensor]], idx: int</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transducer_decoder.html#TransducerDecoder.select_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transducer_decoder.TransducerDecoder.select_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Get specified ID state from decoder hidden states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>states</strong> – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))</p></li>
<li><p><strong>idx</strong> – State ID to extract.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Decoder hidden state for given ID.</dt><dd><p>((N, 1, D_dec), (N, 1, D_dec))</p>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transducer_decoder.TransducerDecoder.set_device">
<code class="sig-name descname">set_device</code><span class="sig-paren">(</span><em class="sig-param">device: torch.device</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transducer_decoder.html#TransducerDecoder.set_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transducer_decoder.TransducerDecoder.set_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Set GPU device to use.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> – Device ID.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-decoder-rnn-decoder">
<span id="id64"></span><h2>espnet2.asr.decoder.rnn_decoder<a class="headerlink" href="#espnet2-asr-decoder-rnn-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.rnn_decoder"></span><dl class="class">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.rnn_decoder.</code><code class="sig-name descname">RNNDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">rnn_type: str = 'lstm'</em>, <em class="sig-param">num_layers: int = 1</em>, <em class="sig-param">hidden_size: int = 320</em>, <em class="sig-param">sampling_probability: float = 0.0</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">context_residual: bool = False</em>, <em class="sig-param">replace_sos: bool = False</em>, <em class="sig-param">num_encs: int = 1</em>, <em class="sig-param">att_conf: dict = {'aconv_chans': 10</em>, <em class="sig-param">'aconv_filts': 100</em>, <em class="sig-param">'adim': 320</em>, <em class="sig-param">'aheads': 4</em>, <em class="sig-param">'atype': 'location'</em>, <em class="sig-param">'awin': 5</em>, <em class="sig-param">'han_conv_chans': -1</em>, <em class="sig-param">'han_conv_filts': 100</em>, <em class="sig-param">'han_dim': 320</em>, <em class="sig-param">'han_heads': 4</em>, <em class="sig-param">'han_mode': False</em>, <em class="sig-param">'han_type': None</em>, <em class="sig-param">'han_win': 5</em>, <em class="sig-param">'num_att': 1</em>, <em class="sig-param">'num_encs': 1}</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em>, <em class="sig-param">hlens</em>, <em class="sig-param">ys_in_pad</em>, <em class="sig-param">ys_in_lens</em>, <em class="sig-param">strm_idx=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.init_state">
<code class="sig-name descname">init_state</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.init_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.init_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Get an initial state for decoding (optional).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – The encoded feature tensor</p>
</dd>
</dl>
<p>Returns: initial state</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.rnn_forward">
<code class="sig-name descname">rnn_forward</code><span class="sig-paren">(</span><em class="sig-param">ey</em>, <em class="sig-param">z_list</em>, <em class="sig-param">c_list</em>, <em class="sig-param">z_prev</em>, <em class="sig-param">c_prev</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.rnn_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.rnn_forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">yseq</em>, <em class="sig-param">state</em>, <em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score new token (required).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y</strong> (<em>torch.Tensor</em>) – 1D torch.int64 prefix tokens.</p></li>
<li><p><strong>state</strong> – Scorer state for prefix tokens</p></li>
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – The encoder feature that generates ys.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple of</dt><dd><p>scores for next token that has a shape of <cite>(n_vocab)</cite>
and next state for ys</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple[torch.Tensor, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.zero_state">
<code class="sig-name descname">zero_state</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.zero_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.zero_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.decoder.rnn_decoder.build_attention_list">
<code class="sig-prename descclassname">espnet2.asr.decoder.rnn_decoder.</code><code class="sig-name descname">build_attention_list</code><span class="sig-paren">(</span><em class="sig-param">eprojs: int</em>, <em class="sig-param">dunits: int</em>, <em class="sig-param">atype: str = 'location'</em>, <em class="sig-param">num_att: int = 1</em>, <em class="sig-param">num_encs: int = 1</em>, <em class="sig-param">aheads: int = 4</em>, <em class="sig-param">adim: int = 320</em>, <em class="sig-param">awin: int = 5</em>, <em class="sig-param">aconv_chans: int = 10</em>, <em class="sig-param">aconv_filts: int = 100</em>, <em class="sig-param">han_mode: bool = False</em>, <em class="sig-param">han_type=None</em>, <em class="sig-param">han_heads: int = 4</em>, <em class="sig-param">han_dim: int = 320</em>, <em class="sig-param">han_conv_chans: int = -1</em>, <em class="sig-param">han_conv_filts: int = 100</em>, <em class="sig-param">han_win: int = 5</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#build_attention_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.build_attention_list" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-asr-decoder-mlm-decoder">
<span id="id65"></span><h2>espnet2.asr.decoder.mlm_decoder<a class="headerlink" href="#espnet2-asr-decoder-mlm-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.mlm_decoder"></span><p>Masked LM Decoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.decoder.mlm_decoder.MLMDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.mlm_decoder.</code><code class="sig-name descname">MLMDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/mlm_decoder.html#MLMDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.mlm_decoder.MLMDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.decoder.mlm_decoder.MLMDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad: torch.Tensor</em>, <em class="sig-param">hlens: torch.Tensor</em>, <em class="sig-param">ys_in_pad: torch.Tensor</em>, <em class="sig-param">ys_in_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/mlm_decoder.html#MLMDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.mlm_decoder.MLMDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward decoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hs_pad</strong> – encoded memory, float32  (batch, maxlen_in, feat)</p></li>
<li><p><strong>hlens</strong> – (batch)</p></li>
<li><p><strong>ys_in_pad</strong> – input token ids, int64 (batch, maxlen_out)
if input_layer == “embed”
input tensor (batch, maxlen_out, #mels) in the other cases</p></li>
<li><p><strong>ys_in_lens</strong> – (batch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>tuple containing:
x: decoded token score before softmax (batch, maxlen_out, token)</p>
<blockquote>
<div><p>if use_output_layer is True,</p>
</div></blockquote>
<p>olens: (batch, )</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(tuple)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-decoder-s4-decoder">
<span id="id66"></span><h2>espnet2.asr.decoder.s4_decoder<a class="headerlink" href="#espnet2-asr-decoder-s4-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.s4_decoder"></span><p>Decoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.decoder.s4_decoder.S4Decoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.s4_decoder.</code><code class="sig-name descname">S4Decoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">dropinp: float = 0.0</em>, <em class="sig-param">dropout: float = 0.25</em>, <em class="sig-param">prenorm: bool = True</em>, <em class="sig-param">n_layers: int = 16</em>, <em class="sig-param">transposed: bool = False</em>, <em class="sig-param">tie_dropout: bool = False</em>, <em class="sig-param">n_repeat=1</em>, <em class="sig-param">layer=None</em>, <em class="sig-param">residual=None</em>, <em class="sig-param">norm=None</em>, <em class="sig-param">pool=None</em>, <em class="sig-param">track_norms=True</em>, <em class="sig-param">drop_path: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/s4_decoder.html#S4Decoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.s4_decoder.S4Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a>, <a class="reference internal" href="espnet.nets.html#espnet.nets.scorer_interface.BatchScorerInterface" title="espnet.nets.scorer_interface.BatchScorerInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.scorer_interface.BatchScorerInterface</span></code></a></p>
<p>S4 decoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – output dim</p></li>
<li><p><strong>encoder_output_size</strong> – dimension of hidden vector</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>dropinp</strong> – input dropout</p></li>
<li><p><strong>dropout</strong> – dropout parameter applied on every residual and every layer</p></li>
<li><p><strong>prenorm</strong> – pre-norm vs. post-norm</p></li>
<li><p><strong>n_layers</strong> – number of layers</p></li>
<li><p><strong>transposed</strong> – transpose inputs so each layer receives (batch, dim, length)</p></li>
<li><p><strong>tie_dropout</strong> – tie dropout mask across sequence like nn.Dropout1d/nn.Dropout2d</p></li>
<li><p><strong>n_repeat</strong> – each layer is repeated n times per stage before applying pooling</p></li>
<li><p><strong>layer</strong> – layer config, must be specified</p></li>
<li><p><strong>residual</strong> – residual config</p></li>
<li><p><strong>norm</strong> – normalization config (e.g. layer vs batch)</p></li>
<li><p><strong>pool</strong> – config for pooling layer per stage</p></li>
<li><p><strong>track_norms</strong> – log norms of each layer output</p></li>
<li><p><strong>drop_path</strong> – drop rate for stochastic depth</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.decoder.s4_decoder.S4Decoder.batch_score">
<code class="sig-name descname">batch_score</code><span class="sig-paren">(</span><em class="sig-param">ys: torch.Tensor, states: List[Any], xs: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, List[Any]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/s4_decoder.html#S4Decoder.batch_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.s4_decoder.S4Decoder.batch_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score new token batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ys</strong> (<em>torch.Tensor</em>) – torch.int64 prefix tokens (n_batch, ylen).</p></li>
<li><p><strong>states</strong> (<em>List</em><em>[</em><em>Any</em><em>]</em>) – Scorer states for prefix tokens.</p></li>
<li><p><strong>xs</strong> (<em>torch.Tensor</em>) – The encoder feature that generates ys (n_batch, xlen, n_feat).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple of</dt><dd><p>batchfied scores for next token with shape of <cite>(n_batch, n_vocab)</cite>
and next state list for ys.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple[torch.Tensor, List[Any]]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.s4_decoder.S4Decoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad: torch.Tensor</em>, <em class="sig-param">hlens: torch.Tensor</em>, <em class="sig-param">ys_in_pad: torch.Tensor</em>, <em class="sig-param">ys_in_lens: torch.Tensor</em>, <em class="sig-param">state=None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/s4_decoder.html#S4Decoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.s4_decoder.S4Decoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward decoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hs_pad</strong> – encoded memory, float32  (batch, maxlen_in, feat)</p></li>
<li><p><strong>hlens</strong> – (batch)</p></li>
<li><p><strong>ys_in_pad</strong> – input token ids, int64 (batch, maxlen_out)
if input_layer == “embed”
input tensor (batch, maxlen_out, #mels) in the other cases</p></li>
<li><p><strong>ys_in_lens</strong> – (batch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>tuple containing:</p>
<dl class="simple">
<dt>x: decoded token score before softmax (batch, maxlen_out, token)</dt><dd><p>if use_output_layer is True,</p>
</dd>
</dl>
<p>olens: (batch, )</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(tuple)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.s4_decoder.S4Decoder.init_state">
<code class="sig-name descname">init_state</code><span class="sig-paren">(</span><em class="sig-param">x: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/s4_decoder.html#S4Decoder.init_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.s4_decoder.S4Decoder.init_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize state.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.s4_decoder.S4Decoder.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">ys</em>, <em class="sig-param">state</em>, <em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/s4_decoder.html#S4Decoder.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.s4_decoder.S4Decoder.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score new token (required).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y</strong> (<em>torch.Tensor</em>) – 1D torch.int64 prefix tokens.</p></li>
<li><p><strong>state</strong> – Scorer state for prefix tokens</p></li>
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – The encoder feature that generates ys.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple of</dt><dd><p>scores for next token that has a shape of <cite>(n_vocab)</cite>
and next state for ys</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple[torch.Tensor, Any]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-decoder-hugging-face-transformers-decoder">
<span id="id67"></span><h2>espnet2.asr.decoder.hugging_face_transformers_decoder<a class="headerlink" href="#espnet2-asr-decoder-hugging-face-transformers-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.hugging_face_transformers_decoder"></span><p>Hugging Face Transformers Decoder.</p>
<dl class="class">
<dt id="espnet2.asr.decoder.hugging_face_transformers_decoder.HuggingFaceTransformersDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.hugging_face_transformers_decoder.</code><code class="sig-name descname">HuggingFaceTransformersDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">model_name_or_path: str</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/hugging_face_transformers_decoder.html#HuggingFaceTransformersDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.hugging_face_transformers_decoder.HuggingFaceTransformersDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a></p>
<p>Hugging Face Transformers Decoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_output_size</strong> – dimension of encoder attention</p></li>
<li><p><strong>model_name_or_path</strong> – Hugging Face Transformers model name</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.decoder.hugging_face_transformers_decoder.HuggingFaceTransformersDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad: torch.Tensor</em>, <em class="sig-param">hlens: torch.Tensor</em>, <em class="sig-param">ys_in_pad: torch.Tensor</em>, <em class="sig-param">ys_in_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/hugging_face_transformers_decoder.html#HuggingFaceTransformersDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.hugging_face_transformers_decoder.HuggingFaceTransformersDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward decoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hs_pad</strong> – encoded memory, float32  (batch, maxlen_in, feat)</p></li>
<li><p><strong>hlens</strong> – (batch)</p></li>
<li><p><strong>ys_in_pad</strong> – input tensor (batch, maxlen_out, #mels)</p></li>
<li><p><strong>ys_in_lens</strong> – (batch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>tuple containing:</p>
<dl class="simple">
<dt>x: decoded token score before softmax (batch, maxlen_out, token)</dt><dd><p>if use_output_layer is True,</p>
</dd>
</dl>
<p>olens: (batch, )</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(tuple)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.hugging_face_transformers_decoder.HuggingFaceTransformersDecoder.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/hugging_face_transformers_decoder.html#HuggingFaceTransformersDecoder.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.hugging_face_transformers_decoder.HuggingFaceTransformersDecoder.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-decoder-init">
<span id="id68"></span><h2>espnet2.asr.decoder.__init__<a class="headerlink" href="#espnet2-asr-decoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.__init__"></span></section>
<section id="espnet2-asr-decoder-transformer-decoder">
<span id="id69"></span><h2>espnet2.asr.decoder.transformer_decoder<a class="headerlink" href="#espnet2-asr-decoder-transformer-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.transformer_decoder"></span><p>Decoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">BaseTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a>, <a class="reference internal" href="espnet.nets.html#espnet.nets.scorer_interface.BatchScorerInterface" title="espnet.nets.scorer_interface.BatchScorerInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.scorer_interface.BatchScorerInterface</span></code></a></p>
<p>Base class of Transfomer decoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – output dim</p></li>
<li><p><strong>encoder_output_size</strong> – dimension of attention</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>linear_units</strong> – the number of units of position-wise feed forward</p></li>
<li><p><strong>num_blocks</strong> – the number of decoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>self_attention_dropout_rate</strong> – dropout rate for attention</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>use_output_layer</strong> – whether to use output layer</p></li>
<li><p><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>concat_after</strong> – whether to concat attention layer’s input and output
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied.
i.e. x -&gt; x + att(x)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.batch_score">
<code class="sig-name descname">batch_score</code><span class="sig-paren">(</span><em class="sig-param">ys: torch.Tensor, states: List[Any], xs: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, List[Any]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder.batch_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.batch_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score new token batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ys</strong> (<em>torch.Tensor</em>) – torch.int64 prefix tokens (n_batch, ylen).</p></li>
<li><p><strong>states</strong> (<em>List</em><em>[</em><em>Any</em><em>]</em>) – Scorer states for prefix tokens.</p></li>
<li><p><strong>xs</strong> (<em>torch.Tensor</em>) – The encoder feature that generates ys (n_batch, xlen, n_feat).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple of</dt><dd><p>batchfied scores for next token with shape of <cite>(n_batch, n_vocab)</cite>
and next state list for ys.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple[torch.Tensor, List[Any]]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad: torch.Tensor</em>, <em class="sig-param">hlens: torch.Tensor</em>, <em class="sig-param">ys_in_pad: torch.Tensor</em>, <em class="sig-param">ys_in_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward decoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hs_pad</strong> – encoded memory, float32  (batch, maxlen_in, feat)</p></li>
<li><p><strong>hlens</strong> – (batch)</p></li>
<li><p><strong>ys_in_pad</strong> – input token ids, int64 (batch, maxlen_out)
if input_layer == “embed”
input tensor (batch, maxlen_out, #mels) in the other cases</p></li>
<li><p><strong>ys_in_lens</strong> – (batch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>tuple containing:</p>
<dl class="simple">
<dt>x: decoded token score before softmax (batch, maxlen_out, token)</dt><dd><p>if use_output_layer is True,</p>
</dd>
</dl>
<p>olens: (batch, )</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(tuple)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.forward_one_step">
<code class="sig-name descname">forward_one_step</code><span class="sig-paren">(</span><em class="sig-param">tgt: torch.Tensor</em>, <em class="sig-param">tgt_mask: torch.Tensor</em>, <em class="sig-param">memory: torch.Tensor</em>, <em class="sig-param">cache: List[torch.Tensor] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, List[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder.forward_one_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.forward_one_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward one step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tgt</strong> – input token ids, int64 (batch, maxlen_out)</p></li>
<li><p><strong>tgt_mask</strong> – input token mask,  (batch, maxlen_out)
dtype=torch.uint8 in PyTorch 1.2-
dtype=torch.bool in PyTorch 1.2+ (include 1.2)</p></li>
<li><p><strong>memory</strong> – encoded memory, float32  (batch, maxlen_in, feat)</p></li>
<li><p><strong>cache</strong> – cached output list of (batch, max_time_out-1, size)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>NN output value and cache per <cite>self.decoders</cite>.
y.shape` is (batch, maxlen_out, token)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>y, cache</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">ys</em>, <em class="sig-param">state</em>, <em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.DynamicConvolution2DTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">DynamicConvolution2DTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">conv_wshare: int = 4</em>, <em class="sig-param">conv_kernel_length: Sequence[int] = (11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11)</em>, <em class="sig-param">conv_usebias: int = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#DynamicConvolution2DTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.DynamicConvolution2DTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.DynamicConvolutionTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">DynamicConvolutionTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">conv_wshare: int = 4</em>, <em class="sig-param">conv_kernel_length: Sequence[int] = (11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11)</em>, <em class="sig-param">conv_usebias: int = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#DynamicConvolutionTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.DynamicConvolutionTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.LightweightConvolution2DTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">LightweightConvolution2DTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">conv_wshare: int = 4</em>, <em class="sig-param">conv_kernel_length: Sequence[int] = (11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11)</em>, <em class="sig-param">conv_usebias: int = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#LightweightConvolution2DTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.LightweightConvolution2DTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">LightweightConvolutionTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">conv_wshare: int = 4</em>, <em class="sig-param">conv_kernel_length: Sequence[int] = (11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11)</em>, <em class="sig-param">conv_usebias: int = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#LightweightConvolutionTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.TransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">TransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">layer_drop_rate: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#TransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.TransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

</section>
<section id="espnet2-asr-decoder-abs-decoder">
<span id="id70"></span><h2>espnet2.asr.decoder.abs_decoder<a class="headerlink" href="#espnet2-asr-decoder-abs-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.abs_decoder"></span><dl class="class">
<dt id="espnet2.asr.decoder.abs_decoder.AbsDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.abs_decoder.</code><code class="sig-name descname">AbsDecoder</code><a class="reference internal" href="../_modules/espnet2/asr/decoder/abs_decoder.html#AbsDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <a class="reference internal" href="espnet.nets.html#espnet.nets.scorer_interface.ScorerInterface" title="espnet.nets.scorer_interface.ScorerInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.scorer_interface.ScorerInterface</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.decoder.abs_decoder.AbsDecoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad: torch.Tensor</em>, <em class="sig-param">hlens: torch.Tensor</em>, <em class="sig-param">ys_in_pad: torch.Tensor</em>, <em class="sig-param">ys_in_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/abs_decoder.html#AbsDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-decoder-whisper-decoder">
<span id="id71"></span><h2>espnet2.asr.decoder.whisper_decoder<a class="headerlink" href="#espnet2-asr-decoder-whisper-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.whisper_decoder"></span><dl class="class">
<dt id="espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.whisper_decoder.</code><code class="sig-name descname">OpenAIWhisperDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">dropout_rate: float = 0.0</em>, <em class="sig-param">whisper_model: str = 'small'</em>, <em class="sig-param">download_dir: str = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/whisper_decoder.html#OpenAIWhisperDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a>, <a class="reference internal" href="espnet.nets.html#espnet.nets.scorer_interface.BatchScorerInterface" title="espnet.nets.scorer_interface.BatchScorerInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.scorer_interface.BatchScorerInterface</span></code></a></p>
<p>Transformer-based Speech-to-Text Decoder from OpenAI’s Whisper Model:</p>
<p>URL: <a class="reference external" href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></p>
<dl class="method">
<dt id="espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder.batch_score">
<code class="sig-name descname">batch_score</code><span class="sig-paren">(</span><em class="sig-param">ys: torch.Tensor, states: List[Any], xs: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, List[Any]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/whisper_decoder.html#OpenAIWhisperDecoder.batch_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder.batch_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score new token batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ys</strong> (<em>torch.Tensor</em>) – torch.int64 prefix tokens (n_batch, ylen).</p></li>
<li><p><strong>states</strong> (<em>List</em><em>[</em><em>Any</em><em>]</em>) – Scorer states for prefix tokens.</p></li>
<li><p><strong>xs</strong> (<em>torch.Tensor</em>) – The encoder feature that generates ys (n_batch, xlen, n_feat).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple of</dt><dd><p>batchfied scores for next token with shape of <cite>(n_batch, n_vocab)</cite>
and next state list for ys.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple[torch.Tensor, List[Any]]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad: torch.Tensor</em>, <em class="sig-param">hlens: torch.Tensor</em>, <em class="sig-param">ys_in_pad: torch.Tensor</em>, <em class="sig-param">ys_in_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/whisper_decoder.html#OpenAIWhisperDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward decoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hs_pad</strong> – encoded memory, float32  (batch, maxlen_in, feat)</p></li>
<li><p><strong>hlens</strong> – (batch)</p></li>
<li><p><strong>ys_in_pad</strong> – input token ids, int64 (batch, maxlen_out)
if input_layer == “embed”
input tensor (batch, maxlen_out, #mels) in the other cases</p></li>
<li><p><strong>ys_in_lens</strong> – (batch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>tuple containing:</p>
<dl class="simple">
<dt>x: decoded token score before softmax (batch, maxlen_out, token)</dt><dd><p>if use_output_layer is True,</p>
</dd>
</dl>
<p>olens: (batch, )</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(tuple)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder.forward_one_step">
<code class="sig-name descname">forward_one_step</code><span class="sig-paren">(</span><em class="sig-param">tgt: torch.Tensor</em>, <em class="sig-param">tgt_mask: torch.Tensor</em>, <em class="sig-param">memory: torch.Tensor</em>, <em class="sig-param">cache: List[torch.Tensor] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, List[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/whisper_decoder.html#OpenAIWhisperDecoder.forward_one_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder.forward_one_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward one step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tgt</strong> – input token ids, int64 (batch, maxlen_out)</p></li>
<li><p><strong>tgt_mask</strong> – input token mask,  (batch, maxlen_out)
dtype=torch.uint8 in PyTorch 1.2-
dtype=torch.bool in PyTorch 1.2+ (include 1.2)</p></li>
<li><p><strong>memory</strong> – encoded memory, float32  (batch, maxlen_in, feat)</p></li>
<li><p><strong>cache</strong> – cached output list of (batch, max_time_out-1, size)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>NN output value and cache per <cite>self.decoders</cite>.
y.shape` is (batch, maxlen_out, token)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>y, cache</p>
</dd>
</dl>
<dl class="simple">
<dt>NOTE (Shih-Lun):</dt><dd><p>cache implementation is ignored for now
for simplicity &amp; correctness</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">ys</em>, <em class="sig-param">state</em>, <em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/whisper_decoder.html#OpenAIWhisperDecoder.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score.</p>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="espnet.nets.html" class="btn btn-neutral float-left" title="espnet.nets package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="espnet2.torch_utils.html" class="btn btn-neutral float-right" title="espnet2.torch_utils package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2017, Shinji Watanabe.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>