flake8-docstrings ready files coverage: 137 / 193 = 70.9844%
============================= test session starts ==============================
platform linux -- Python 3.7.6, pytest-5.4.3, py-1.8.2, pluggy-0.13.1
rootdir: /data4/tanghaoyu/espnet_dev, inifile: setup.cfg, testpaths: test
plugins: typeguard-2.9.1, cov-2.10.0
collected 2879 items

test/test_asr_init.py .........................                          [  0%]
test/test_asr_interface.py ..                                            [  0%]
test/test_batch_beam_search.py .....FFFF....FFFF....FFFF....FFFF....FFFF [  2%]
....FFFF....FFFF....FFFF....FFFF....FFFF....FFFF....FFFF                 [  4%]
test/test_beam_search.py s..s..s..s..sssssssssssssssssssssssss..s..s..s. [  5%]
.s..s..s..s..sssssssssssssssssssssssssssssssssssssssssssssssss..s..s..s. [  8%]
.sssssssssssssssssssssssss..s..s..s..s..s..s..s..sssssssssssssssssssssss [ 10%]
ssssssssssssssssssssssssss..s..s..s..sssssssssssssssssssssssss..s..s..s. [ 13%]
.s..s..s..s..sssssssssssssssssssssssssssssssssssssssssssssssss..s..s..s. [ 15%]
.sssssssssssssssssssssssss..s..s..s..s..s..s..s..sssssssssssssssssssssss [ 18%]
ssssssssssssssssssssssssss..s..s..s..sssssssssssssssssssssssss..s..s..s. [ 20%]
.s..s..s..s..sssssssssssssssssssssssssssssssssssssssssssssssss..s..s..s. [ 23%]
.sssssssssssssssssssssssss..s..s..s..s..s..s..s..sssssssssssssssssssssss [ 25%]
sssssssssssssssssssssssss............ssssssssssssssssssssssss........... [ 28%]
.............ssssssssssssssssssssssssssssssssssssssssssssssss........... [ 30%]
.ssssssssssssssssssssssss........................sssssssssssssssssssssss [ 33%]
sssssssssssssssssssssssssF..F..F..F..ssssssssssssssssssssssssF..F..F..F. [ 35%]
.F..F..F..F..ssssssssssssssssssssssssssssssssssssssssssssssss........... [ 38%]
.ssssssssssssssssssssssss........................sssssssssssssssssssssss [ 40%]
sssssssssssssssssssssssssF..F..F..F..ssssssssssssssssssssssssF..F..F..F. [ 43%]
.F..F..F..F..ssssssssssssssssssssssssssssssssssssssssssssssss........... [ 45%]
.ssssssssssssssssssssssss........................sssssssssssssssssssssss [ 48%]
sssssssssssssssssssssssss                                                [ 49%]
test/test_cli.py ....                                                    [ 49%]
test/test_e2e_asr.py ................................................... [ 51%]
......................................................                   [ 53%]
test/test_e2e_asr_mulenc.py ............................................ [ 54%]
........................................................................ [ 57%]
............................                                             [ 58%]
test/test_e2e_asr_sa_transducer.py ......................                [ 58%]
test/test_e2e_asr_transducer.py ........................................ [ 60%]
........................................                                 [ 61%]
test/test_e2e_asr_transformer.py ..............F.                        [ 62%]
test/test_e2e_compatibility.py ss                                        [ 62%]
test/test_e2e_mt.py ...........................                          [ 63%]
test/test_e2e_mt_transformer.py .......                                  [ 63%]
test/test_e2e_st.py .................................................... [ 65%]
....................                                                     [ 65%]
test/test_e2e_st_transformer.py .............                            [ 66%]
test/test_e2e_tts_fastspeech.py ........................................ [ 67%]
..................................                                       [ 68%]
test/test_e2e_tts_tacotron2.py ......................................... [ 70%]
.......                                                                  [ 70%]
test/test_e2e_tts_transformer.py ....................................... [ 72%]
.....................                                                    [ 72%]
test/test_initialization.py ..                                           [ 72%]
test/test_io_voxforge.py s                                               [ 72%]
test/test_lm.py .s.....s.....s.....s.....s.....s.....                    [ 74%]
test/test_loss.py ......                                                 [ 74%]
test/test_multi_spkrs.py ....                                            [ 74%]
test/test_ngram.py .                                                     [ 74%]
test/test_optimizer.py .....                                             [ 74%]
test/test_positional_encoding.py .........                               [ 74%]
test/test_recog.py ........................                              [ 75%]
test/test_scheduler.py .....                                             [ 75%]
test/test_sentencepiece.py .                                             [ 76%]
test/test_tensorboard.py .                                               [ 76%]
test/test_torch.py ..                                                    [ 76%]
test/test_train_dtype.py s.....s.....s.....s.....s.....s.....            [ 77%]
test/test_transform.py ...                                               [ 77%]
test/test_transformer_decode.py ....                                     [ 77%]
test/test_utils.py ..............                                        [ 78%]
test/espnet2/asr/test_ctc.py ......                                      [ 78%]
test/espnet2/asr/decoder/test_rnn_decoder.py .........                   [ 78%]
test/espnet2/asr/decoder/test_transformer_decoder.py ..........          [ 78%]
test/espnet2/asr/encoder/test_rnn_encoder.py ..................          [ 79%]
test/espnet2/asr/encoder/test_transformer_encoder.py ..........          [ 79%]
test/espnet2/asr/encoder/test_vgg_rnn_encoder.py ..........              [ 80%]
test/espnet2/asr/frontend/test_frontend.py ...........                   [ 80%]
test/espnet2/asr/specaug/test_specaug.py ................                [ 81%]
test/espnet2/bin/test_aggregate_stats_dirs.py ..                         [ 81%]
test/espnet2/bin/test_asr_inference.py ..                                [ 81%]
test/espnet2/bin/test_asr_train.py ..                                    [ 81%]
test/espnet2/bin/test_lm_calc_perplexity.py ..                           [ 81%]
test/espnet2/bin/test_lm_train.py ..                                     [ 81%]
test/espnet2/bin/test_pack.py ..                                         [ 81%]
test/espnet2/bin/test_tokenize_text.py ..                                [ 81%]
test/espnet2/bin/test_tts_inference.py ..                                [ 81%]
test/espnet2/bin/test_tts_train.py ..                                    [ 81%]
test/espnet2/fileio/test_datadir_writer.py .                             [ 81%]
test/espnet2/fileio/test_npy_scp.py ....                                 [ 82%]
test/espnet2/fileio/test_read_text.py .......                            [ 82%]
test/espnet2/fileio/test_sound_scp.py ..                                 [ 82%]
test/espnet2/iterators/test_chunk_iter_factory.py .                      [ 82%]
test/espnet2/iterators/test_multiple_iter_factory.py ..                  [ 82%]
test/espnet2/iterators/test_sequence_iter_factory.py ......              [ 82%]
test/espnet2/layers/test_global_mvn.py ............................      [ 83%]
test/espnet2/layers/test_log_mel.py ....                                 [ 83%]
test/espnet2/layers/test_mask_along_axis.py ............                 [ 84%]
test/espnet2/layers/test_stft.py .....                                   [ 84%]
test/espnet2/layers/test_time_warp.py .....                              [ 84%]
test/espnet2/layers/test_utterance_mvn.py .............                  [ 84%]
test/espnet2/lm/test_seq_rnn.py ..................                       [ 85%]
test/espnet2/main_funcs/test_calculate_all_attentions.py .............   [ 86%]
test/espnet2/optimizers/test_sgd.py .                                    [ 86%]
test/espnet2/samplers/test_build_batch_sampler.py .......                [ 86%]
test/espnet2/samplers/test_folded_batch_sampler.py ..................... [ 87%]
...                                                                      [ 87%]
test/espnet2/samplers/test_length_elements_batch_sampler.py ............ [ 87%]
....................................                                     [ 88%]
test/espnet2/samplers/test_num_elements_batch_sampler.py ............... [ 89%]
.................................                                        [ 90%]
test/espnet2/samplers/test_sorted_batch_sampler.py ..................... [ 91%]
...                                                                      [ 91%]
test/espnet2/samplers/test_unsorted_batch_sampler.py ......              [ 91%]
test/espnet2/schedulers/test_noam_lr.py s                                [ 91%]
test/espnet2/schedulers/test_warmup_lr.py ss                             [ 91%]
test/espnet2/tasks/test_abs_task.py .......                              [ 91%]
test/espnet2/tasks/test_asr.py ......                                    [ 92%]
test/espnet2/tasks/test_lm.py ......                                     [ 92%]
test/espnet2/tasks/test_tts.py ......                                    [ 92%]
test/espnet2/text/test_char_tokenizer.py ...                             [ 92%]
test/espnet2/text/test_cleaner.py ....                                   [ 92%]
test/espnet2/text/test_phoneme_tokenizer.py ...                          [ 92%]
test/espnet2/text/test_sentencepiece_tokenizer.py ..                     [ 92%]
test/espnet2/text/test_token_id_converter.py .......                     [ 93%]
test/espnet2/text/test_word_tokenizer.py ......                          [ 93%]
test/espnet2/torch_utils/test_add_gradient_noise.py .                    [ 93%]
test/espnet2/torch_utils/test_device_funcs.py .........................  [ 94%]
test/espnet2/torch_utils/test_forward_adaptor.py ..                      [ 94%]
test/espnet2/torch_utils/test_initialize.py .......                      [ 94%]
test/espnet2/torch_utils/test_pytorch_version.py .                       [ 94%]
test/espnet2/torch_utils/test_set_all_random_seed.py .                   [ 94%]
test/espnet2/train/test_collate_fn.py .........                          [ 94%]
test/espnet2/train/test_dataset.py ............                          [ 95%]
test/espnet2/train/test_distributed_utils.py ..................          [ 96%]
test/espnet2/train/test_iterable_dataset.py ............                 [ 96%]
test/espnet2/train/test_reporter.py ...................................  [ 97%]
test/espnet2/tts/feats_extract/test_log_mel_fbank.py .....               [ 97%]
test/espnet2/tts/feats_extract/test_log_spectrogram.py .....             [ 98%]
test/espnet2/utils/test_build_dataclass.py ..                            [ 98%]
test/espnet2/utils/test_get_default_kwargs.py .......                    [ 98%]
test/espnet2/utils/test_model_summary.py .                               [ 98%]
test/espnet2/utils/test_nested_dict_action.py ..                         [ 98%]
test/espnet2/utils/test_pack_funcs.py .....                              [ 98%]
test/espnet2/utils/test_sized_dict.py .......                            [ 98%]
test/espnet2/utils/test_types.py ...............................         [ 99%]
test/espnet2/utils/test_yaml_no_alias_safe_dump.py ..                    [100%]

=================================== FAILURES ===================================
_ test_batch_beam_search_equal[transformer-args4-0.0-default-lm_args4-0.0-0.5-0.0-cpu-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.0, device = 'cpu'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24247336
E           Max relative difference: 0.228543
E            x: array(-1.303426, dtype=float32)
E            y: array(-1.060953, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc629c16fb0>
Debug: states: <kenlm.State object at 0x7fc629bfcd30>
Debug: states: <kenlm.State object at 0x7fc629bfca30>
Debug: states: <kenlm.State object at 0x7fc629bfccb0>
Debug: states: <kenlm.State object at 0x7fc629bfc3f0>
Debug: states: <kenlm.State object at 0x7fc629bfce70>
Debug: states: <kenlm.State object at 0x7fc629bfc1b0>
Debug: states: <kenlm.State object at 0x7fc629bfcbf0>
Debug: states: <kenlm.State object at 0x7fc629bfc6b0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:30,361 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:30,442 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:30,475 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:30,475 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:30,539 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:30,540 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:30,540 (beam_search:389) INFO: total log probability: -1.3034262657165527
2020-06-20 16:05:30,540 (beam_search:390) INFO: normalized log probability: -0.6517131328582764
2020-06-20 16:05:30,540 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:30,540 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:30,575 (beam_search:389) INFO: total log probability: -1.06095290184021
2020-06-20 16:05:30,575 (beam_search:390) INFO: normalized log probability: -0.530476450920105
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.3034262657165527
INFO     root:beam_search.py:390 normalized log probability: -0.6517131328582764
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.06095290184021
INFO     root:beam_search.py:390 normalized log probability: -0.530476450920105
_ test_batch_beam_search_equal[transformer-args5-0.0-default-lm_args5-0.0-0.5-0.0-cpu-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.0, device = 'cpu'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.58149054
E           Max relative difference: 0.80546018
E            x: array(-1.303426)
E            y: array(-0.721936)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc73673f9f0>
Debug: states: <kenlm.State object at 0x7fc73673fc30>
Debug: states: <kenlm.State object at 0x7fc73673f8b0>
Debug: states: <kenlm.State object at 0x7fc73673fc70>
Debug: states: <kenlm.State object at 0x7fc73673fb30>
Debug: states: <kenlm.State object at 0x7fc73673f3b0>
Debug: states: <kenlm.State object at 0x7fc73673f8f0>
Debug: states: <kenlm.State object at 0x7fc73673f3f0>
Debug: states: <kenlm.State object at 0x7fc73673fc70>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:30,805 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:30,905 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:30,959 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:30,959 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:31,036 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:31,036 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:31,036 (beam_search:389) INFO: total log probability: -1.3034263327862567
2020-06-20 16:05:31,037 (beam_search:390) INFO: normalized log probability: -0.6517131663931284
2020-06-20 16:05:31,037 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:31,037 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:31,074 (beam_search:389) INFO: total log probability: -0.7219357951111034
2020-06-20 16:05:31,074 (beam_search:390) INFO: normalized log probability: -0.3609678975555517
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.3034263327862567
INFO     root:beam_search.py:390 normalized log probability: -0.6517131663931284
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -0.7219357951111034
INFO     root:beam_search.py:390 normalized log probability: -0.3609678975555517
_ test_batch_beam_search_equal[transformer-args6-0.0-default-lm_args6-0.0-0.5-0.1-cpu-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.1, device = 'cpu'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24247336
E           Max relative difference: 0.25232595
E            x: array(-1.203426, dtype=float32)
E            y: array(-0.960953, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc735ea8530>
Debug: states: <kenlm.State object at 0x7fc735ea8770>
Debug: states: <kenlm.State object at 0x7fc735ea87f0>
Debug: states: <kenlm.State object at 0x7fc735ea8630>
Debug: states: <kenlm.State object at 0x7fc735ea85f0>
Debug: states: <kenlm.State object at 0x7fc735ea8930>
Debug: states: <kenlm.State object at 0x7fc735ea87b0>
Debug: states: <kenlm.State object at 0x7fc735ea88f0>
Debug: states: <kenlm.State object at 0x7fc735ea85b0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:31,152 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:31,256 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:31,284 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:31,284 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:31,335 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:31,336 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:31,336 (beam_search:389) INFO: total log probability: -1.2034262418746948
2020-06-20 16:05:31,336 (beam_search:390) INFO: normalized log probability: -0.6017131209373474
2020-06-20 16:05:31,336 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:31,336 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:31,364 (beam_search:389) INFO: total log probability: -0.960952877998352
2020-06-20 16:05:31,364 (beam_search:390) INFO: normalized log probability: -0.480476438999176
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.2034262418746948
INFO     root:beam_search.py:390 normalized log probability: -0.6017131209373474
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -0.960952877998352
INFO     root:beam_search.py:390 normalized log probability: -0.480476438999176
_ test_batch_beam_search_equal[transformer-args7-0.0-default-lm_args7-0.0-0.5-0.1-cpu-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.1, device = 'cpu'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.58149054
E           Max relative difference: 0.93496876
E            x: array(-1.203426)
E            y: array(-0.621936)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc735ea8070>
Debug: states: <kenlm.State object at 0x7fc735ea88f0>
Debug: states: <kenlm.State object at 0x7fc735ea85b0>
Debug: states: <kenlm.State object at 0x7fc735ea8930>
Debug: states: <kenlm.State object at 0x7fc735ea8470>
Debug: states: <kenlm.State object at 0x7fc735ea8c70>
Debug: states: <kenlm.State object at 0x7fc735ea8570>
Debug: states: <kenlm.State object at 0x7fc735ea8bf0>
Debug: states: <kenlm.State object at 0x7fc735ea8230>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:31,407 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:31,572 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:31,662 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:31,662 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:31,724 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:31,725 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:31,725 (beam_search:389) INFO: total log probability: -1.2034263327862567
2020-06-20 16:05:31,725 (beam_search:390) INFO: normalized log probability: -0.6017131663931283
2020-06-20 16:05:31,725 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:31,725 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:31,754 (beam_search:389) INFO: total log probability: -0.6219357951111034
2020-06-20 16:05:31,754 (beam_search:390) INFO: normalized log probability: -0.3109678975555517
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.2034263327862567
INFO     root:beam_search.py:390 normalized log probability: -0.6017131663931283
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -0.6219357951111034
INFO     root:beam_search.py:390 normalized log probability: -0.3109678975555517
_ test_batch_beam_search_equal[transformer-args12-0.0-default-lm_args12-0.5-0.5-0.0-cpu-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.0, device = 'cpu'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.28273487
E           Max relative difference: 0.15518863
E            x: array(-2.104614, dtype=float32)
E            y: array(-1.821879, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc61c17ab70>
Debug: states: <kenlm.State object at 0x7fc61c17a2f0>
Debug: states: <kenlm.State object at 0x7fc61c17a270>
Debug: states: <kenlm.State object at 0x7fc61c17a1b0>
Debug: states: <kenlm.State object at 0x7fc61c17ab70>
Debug: states: <kenlm.State object at 0x7fc61c17ad30>
Debug: states: <kenlm.State object at 0x7fc61c17a030>
Debug: states: <kenlm.State object at 0x7fc61c17ac30>
Debug: states: <kenlm.State object at 0x7fc61c17ab70>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:32,932 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:33,049 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:33,119 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:33,119 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:33,175 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:33,175 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:33,175 (beam_search:389) INFO: total log probability: -2.1046135425567627
2020-06-20 16:05:33,175 (beam_search:390) INFO: normalized log probability: -1.0523067712783813
2020-06-20 16:05:33,175 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:33,175 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:33,207 (beam_search:389) INFO: total log probability: -1.8218786716461182
2020-06-20 16:05:33,207 (beam_search:390) INFO: normalized log probability: -0.9109393358230591
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.1046135425567627
INFO     root:beam_search.py:390 normalized log probability: -1.0523067712783813
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.8218786716461182
INFO     root:beam_search.py:390 normalized log probability: -0.9109393358230591
_ test_batch_beam_search_equal[transformer-args13-0.0-default-lm_args13-0.5-0.5-0.0-cpu-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.0, device = 'cpu'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.28273482
E           Max relative difference: 0.1551886
E            x: array(-2.104614)
E            y: array(-1.821879)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc735de7770>
Debug: states: <kenlm.State object at 0x7fc735de77f0>
Debug: states: <kenlm.State object at 0x7fc735de7730>
Debug: states: <kenlm.State object at 0x7fc735de7930>
Debug: states: <kenlm.State object at 0x7fc735de78f0>
Debug: states: <kenlm.State object at 0x7fc735de7830>
Debug: states: <kenlm.State object at 0x7fc735de7a70>
Debug: states: <kenlm.State object at 0x7fc735de7730>
Debug: states: <kenlm.State object at 0x7fc735de76b0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:33,312 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:33,383 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:33,468 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:33,468 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:33,535 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:33,536 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:33,536 (beam_search:389) INFO: total log probability: -2.1046135462152105
2020-06-20 16:05:33,536 (beam_search:390) INFO: normalized log probability: -1.0523067731076052
2020-06-20 16:05:33,536 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:33,536 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:33,569 (beam_search:389) INFO: total log probability: -1.8218787297751557
2020-06-20 16:05:33,569 (beam_search:390) INFO: normalized log probability: -0.9109393648875779
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.1046135462152105
INFO     root:beam_search.py:390 normalized log probability: -1.0523067731076052
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.8218787297751557
INFO     root:beam_search.py:390 normalized log probability: -0.9109393648875779
_ test_batch_beam_search_equal[transformer-args14-0.0-default-lm_args14-0.5-0.5-0.1-cpu-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.1, device = 'cpu'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.282735
E           Max relative difference: 0.16420147
E            x: array(-2.004614, dtype=float32)
E            y: array(-1.721879, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7366fa370>
Debug: states: <kenlm.State object at 0x7fc7366fa570>
Debug: states: <kenlm.State object at 0x7fc7366fa4b0>
Debug: states: <kenlm.State object at 0x7fc7366fa5f0>
Debug: states: <kenlm.State object at 0x7fc7366fa3f0>
Debug: states: <kenlm.State object at 0x7fc7366fa4f0>
Debug: states: <kenlm.State object at 0x7fc7366fa630>
Debug: states: <kenlm.State object at 0x7fc7366fa4b0>
Debug: states: <kenlm.State object at 0x7fc7366fa770>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:33,602 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:33,661 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:33,671 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:33,671 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:33,730 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:33,730 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:33,731 (beam_search:389) INFO: total log probability: -2.0046136379241943
2020-06-20 16:05:33,731 (beam_search:390) INFO: normalized log probability: -1.0023068189620972
2020-06-20 16:05:33,731 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:33,731 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:33,764 (beam_search:389) INFO: total log probability: -1.7218786478042603
2020-06-20 16:05:33,764 (beam_search:390) INFO: normalized log probability: -0.8609393239021301
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.0046136379241943
INFO     root:beam_search.py:390 normalized log probability: -1.0023068189620972
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.7218786478042603
INFO     root:beam_search.py:390 normalized log probability: -0.8609393239021301
_ test_batch_beam_search_equal[transformer-args15-0.0-default-lm_args15-0.5-0.5-0.1-cpu-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.1, device = 'cpu'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.28273482
E           Max relative difference: 0.16420135
E            x: array(-2.004614)
E            y: array(-1.721879)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc629bfc6b0>
Debug: states: <kenlm.State object at 0x7fc735e04e30>
Debug: states: <kenlm.State object at 0x7fc735e04ef0>
Debug: states: <kenlm.State object at 0x7fc735e04970>
Debug: states: <kenlm.State object at 0x7fc735e047b0>
Debug: states: <kenlm.State object at 0x7fc735e04b30>
Debug: states: <kenlm.State object at 0x7fc735e04f30>
Debug: states: <kenlm.State object at 0x7fc735e047b0>
Debug: states: <kenlm.State object at 0x7fc735e04eb0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:33,835 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:33,912 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:34,004 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:34,004 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:34,073 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:34,074 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:34,074 (beam_search:389) INFO: total log probability: -2.0046135462152104
2020-06-20 16:05:34,074 (beam_search:390) INFO: normalized log probability: -1.0023067731076052
2020-06-20 16:05:34,074 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:34,074 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:34,107 (beam_search:389) INFO: total log probability: -1.7218787297751557
2020-06-20 16:05:34,107 (beam_search:390) INFO: normalized log probability: -0.8609393648875778
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.0046135462152104
INFO     root:beam_search.py:390 normalized log probability: -1.0023067731076052
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.7218787297751557
INFO     root:beam_search.py:390 normalized log probability: -0.8609393648875778
_ test_batch_beam_search_equal[transformer-args20-0.0-default-lm_args20-0.0-0.5-0.0-cpu-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.0, device = 'cpu'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24247336
E           Max relative difference: 0.228543
E            x: array(-1.303426, dtype=float32)
E            y: array(-1.060953, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7366dff70>
Debug: states: <kenlm.State object at 0x7fc7366fa230>
Debug: states: <kenlm.State object at 0x7fc7366fa7b0>
Debug: states: <kenlm.State object at 0x7fc7366faab0>
Debug: states: <kenlm.State object at 0x7fc7366fa5f0>
Debug: states: <kenlm.State object at 0x7fc7366fa3f0>
Debug: states: <kenlm.State object at 0x7fc7366faab0>
Debug: states: <kenlm.State object at 0x7fc7366fa5f0>
Debug: states: <kenlm.State object at 0x7fc7366fa3f0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:35,529 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:35,649 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:35,698 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:35,698 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:35,749 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:35,750 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:35,750 (beam_search:389) INFO: total log probability: -1.3034262657165527
2020-06-20 16:05:35,750 (beam_search:390) INFO: normalized log probability: -0.6517131328582764
2020-06-20 16:05:35,750 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:35,750 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:35,778 (beam_search:389) INFO: total log probability: -1.06095290184021
2020-06-20 16:05:35,778 (beam_search:390) INFO: normalized log probability: -0.530476450920105
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.3034262657165527
INFO     root:beam_search.py:390 normalized log probability: -0.6517131328582764
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.06095290184021
INFO     root:beam_search.py:390 normalized log probability: -0.530476450920105
_ test_batch_beam_search_equal[transformer-args21-0.0-default-lm_args21-0.0-0.5-0.0-cpu-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.0, device = 'cpu'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.58149054
E           Max relative difference: 0.80546018
E            x: array(-1.303426)
E            y: array(-0.721936)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc736736d70>
Debug: states: <kenlm.State object at 0x7fc736736bf0>
Debug: states: <kenlm.State object at 0x7fc736736630>
Debug: states: <kenlm.State object at 0x7fc7367367f0>
Debug: states: <kenlm.State object at 0x7fc736736970>
Debug: states: <kenlm.State object at 0x7fc736736c70>
Debug: states: <kenlm.State object at 0x7fc7367369b0>
Debug: states: <kenlm.State object at 0x7fc7367368b0>
Debug: states: <kenlm.State object at 0x7fc7367367f0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:35,856 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:35,984 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:36,056 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:36,056 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:36,119 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:36,120 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:36,120 (beam_search:389) INFO: total log probability: -1.3034263327862567
2020-06-20 16:05:36,120 (beam_search:390) INFO: normalized log probability: -0.6517131663931284
2020-06-20 16:05:36,120 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:36,120 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:36,149 (beam_search:389) INFO: total log probability: -0.7219357951111034
2020-06-20 16:05:36,149 (beam_search:390) INFO: normalized log probability: -0.3609678975555517
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.3034263327862567
INFO     root:beam_search.py:390 normalized log probability: -0.6517131663931284
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -0.7219357951111034
INFO     root:beam_search.py:390 normalized log probability: -0.3609678975555517
_ test_batch_beam_search_equal[transformer-args22-0.0-default-lm_args22-0.0-0.5-0.1-cpu-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.1, device = 'cpu'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24247336
E           Max relative difference: 0.25232595
E            x: array(-1.203426, dtype=float32)
E            y: array(-0.960953, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc62a4e8eb0>
Debug: states: <kenlm.State object at 0x7fc62a4e83b0>
Debug: states: <kenlm.State object at 0x7fc62a4e89f0>
Debug: states: <kenlm.State object at 0x7fc62a4e81b0>
Debug: states: <kenlm.State object at 0x7fc62a4e8ef0>
Debug: states: <kenlm.State object at 0x7fc62a4e80f0>
Debug: states: <kenlm.State object at 0x7fc62a4e8270>
Debug: states: <kenlm.State object at 0x7fc62a4e8e30>
Debug: states: <kenlm.State object at 0x7fc62a4e8430>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:36,175 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:36,309 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:36,345 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:36,345 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:36,400 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:36,400 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:36,400 (beam_search:389) INFO: total log probability: -1.2034262418746948
2020-06-20 16:05:36,400 (beam_search:390) INFO: normalized log probability: -0.6017131209373474
2020-06-20 16:05:36,400 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:36,400 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:36,429 (beam_search:389) INFO: total log probability: -0.960952877998352
2020-06-20 16:05:36,429 (beam_search:390) INFO: normalized log probability: -0.480476438999176
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.2034262418746948
INFO     root:beam_search.py:390 normalized log probability: -0.6017131209373474
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -0.960952877998352
INFO     root:beam_search.py:390 normalized log probability: -0.480476438999176
_ test_batch_beam_search_equal[transformer-args23-0.0-default-lm_args23-0.0-0.5-0.1-cpu-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.1, device = 'cpu'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.58149054
E           Max relative difference: 0.93496876
E            x: array(-1.203426)
E            y: array(-0.621936)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc73668a530>
Debug: states: <kenlm.State object at 0x7fc73668a7b0>
Debug: states: <kenlm.State object at 0x7fc73668a830>
Debug: states: <kenlm.State object at 0x7fc73668a7f0>
Debug: states: <kenlm.State object at 0x7fc73668a6b0>
Debug: states: <kenlm.State object at 0x7fc7366d00b0>
Debug: states: <kenlm.State object at 0x7fc73668a5f0>
Debug: states: <kenlm.State object at 0x7fc73668a130>
Debug: states: <kenlm.State object at 0x7fc73668a630>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:36,488 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:36,569 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:36,635 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:36,635 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:36,706 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:36,706 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:36,706 (beam_search:389) INFO: total log probability: -1.2034263327862567
2020-06-20 16:05:36,706 (beam_search:390) INFO: normalized log probability: -0.6017131663931283
2020-06-20 16:05:36,706 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:36,706 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:36,738 (beam_search:389) INFO: total log probability: -0.6219357951111034
2020-06-20 16:05:36,738 (beam_search:390) INFO: normalized log probability: -0.3109678975555517
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.2034263327862567
INFO     root:beam_search.py:390 normalized log probability: -0.6017131663931283
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -0.6219357951111034
INFO     root:beam_search.py:390 normalized log probability: -0.3109678975555517
_ test_batch_beam_search_equal[transformer-args28-0.0-default-lm_args28-0.5-0.5-0.0-cpu-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.0, device = 'cpu'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.27812266
E           Max relative difference: 0.15442069
E            x: array(-2.079194, dtype=float32)
E            y: array(-1.801071, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7366d0c30>
Debug: states: <kenlm.State object at 0x7fc7366d0530>
Debug: states: <kenlm.State object at 0x7fc7366d0670>
Debug: states: <kenlm.State object at 0x7fc7366d0770>
Debug: states: <kenlm.State object at 0x7fc7366d0df0>
Debug: states: <kenlm.State object at 0x7fc7366d0e70>
Debug: states: <kenlm.State object at 0x7fc7366d0a30>
Debug: states: <kenlm.State object at 0x7fc7366d0db0>
Debug: states: <kenlm.State object at 0x7fc7366d0f70>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:37,980 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:38,098 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:38,197 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:38,197 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:38,253 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:38,254 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:38,254 (beam_search:389) INFO: total log probability: -2.0791938304901123
2020-06-20 16:05:38,254 (beam_search:390) INFO: normalized log probability: -1.0395969152450562
2020-06-20 16:05:38,254 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:38,254 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:38,292 (beam_search:389) INFO: total log probability: -1.8010711669921875
2020-06-20 16:05:38,293 (beam_search:390) INFO: normalized log probability: -0.9005355834960938
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.0791938304901123
INFO     root:beam_search.py:390 normalized log probability: -1.0395969152450562
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.8010711669921875
INFO     root:beam_search.py:390 normalized log probability: -0.9005355834960938
_ test_batch_beam_search_equal[transformer-args29-0.0-default-lm_args29-0.5-0.5-0.0-cpu-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.0, device = 'cpu'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.27812256
E           Max relative difference: 0.15442063
E            x: array(-2.079194)
E            y: array(-1.801071)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7366495f0>
Debug: states: <kenlm.State object at 0x7fc7366497f0>
Debug: states: <kenlm.State object at 0x7fc736649830>
Debug: states: <kenlm.State object at 0x7fc736649870>
Debug: states: <kenlm.State object at 0x7fc7366497b0>
Debug: states: <kenlm.State object at 0x7fc736649970>
Debug: states: <kenlm.State object at 0x7fc7366499b0>
Debug: states: <kenlm.State object at 0x7fc7366496b0>
Debug: states: <kenlm.State object at 0x7fc7366498f0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:38,379 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:38,469 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:38,562 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:38,562 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:38,630 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:38,631 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:38,631 (beam_search:389) INFO: total log probability: -2.0791939061521316
2020-06-20 16:05:38,631 (beam_search:390) INFO: normalized log probability: -1.0395969530760658
2020-06-20 16:05:38,631 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:38,631 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:38,662 (beam_search:389) INFO: total log probability: -1.8010713431629464
2020-06-20 16:05:38,662 (beam_search:390) INFO: normalized log probability: -0.9005356715814732
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.0791939061521316
INFO     root:beam_search.py:390 normalized log probability: -1.0395969530760658
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.8010713431629464
INFO     root:beam_search.py:390 normalized log probability: -0.9005356715814732
_ test_batch_beam_search_equal[transformer-args30-0.0-default-lm_args30-0.5-0.5-0.1-cpu-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.1, device = 'cpu'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.27812266
E           Max relative difference: 0.16349855
E            x: array(-1.979194, dtype=float32)
E            y: array(-1.701071, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7365ac0b0>
Debug: states: <kenlm.State object at 0x7fc7365ac2b0>
Debug: states: <kenlm.State object at 0x7fc7365ac1f0>
Debug: states: <kenlm.State object at 0x7fc7365ac330>
Debug: states: <kenlm.State object at 0x7fc7365ac130>
Debug: states: <kenlm.State object at 0x7fc7365ac3b0>
Debug: states: <kenlm.State object at 0x7fc7365ac230>
Debug: states: <kenlm.State object at 0x7fc7365ac470>
Debug: states: <kenlm.State object at 0x7fc7365ac430>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:38,768 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:38,888 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:38,963 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:38,963 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:39,030 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:39,031 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:39,031 (beam_search:389) INFO: total log probability: -1.9791938066482544
2020-06-20 16:05:39,031 (beam_search:390) INFO: normalized log probability: -0.9895969033241272
2020-06-20 16:05:39,031 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:39,031 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:39,069 (beam_search:389) INFO: total log probability: -1.7010711431503296
2020-06-20 16:05:39,069 (beam_search:390) INFO: normalized log probability: -0.8505355715751648
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.9791938066482544
INFO     root:beam_search.py:390 normalized log probability: -0.9895969033241272
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.7010711431503296
INFO     root:beam_search.py:390 normalized log probability: -0.8505355715751648
_ test_batch_beam_search_equal[transformer-args31-0.0-default-lm_args31-0.5-0.5-0.1-cpu-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.1, device = 'cpu'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.2424733
E           Max relative difference: 0.13961561
E            x: array(-1.979194)
E            y: array(-1.736721)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc73661bd70>
Debug: states: <kenlm.State object at 0x7fc7365ac030>
Debug: states: <kenlm.State object at 0x7fc7365aca70>
Debug: states: <kenlm.State object at 0x7fc7365ac0f0>
Debug: states: <kenlm.State object at 0x7fc7365ac430>
Debug: states: <kenlm.State object at 0x7fc7365ac1f0>
Debug: states: <kenlm.State object at 0x7fc7365acb70>
Debug: states: <kenlm.State object at 0x7fc7365ac930>
Debug: states: <kenlm.State object at 0x7fc7365ac0b0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:39,140 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:39,256 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:39,364 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:39,364 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:39,441 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:39,441 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:39,441 (beam_search:389) INFO: total log probability: -1.9791939061521315
2020-06-20 16:05:39,441 (beam_search:390) INFO: normalized log probability: -0.9895969530760658
2020-06-20 16:05:39,441 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:39,442 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:39,478 (beam_search:389) INFO: total log probability: -1.7367206018804335
2020-06-20 16:05:39,478 (beam_search:390) INFO: normalized log probability: -0.8683603009402168
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.9791939061521315
INFO     root:beam_search.py:390 normalized log probability: -0.9895969530760658
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.7367206018804335
INFO     root:beam_search.py:390 normalized log probability: -0.8683603009402168
_ test_batch_beam_search_equal[transformer-args36-0.0-transformer-lm_args36-0.0-0.5-0.0-cpu-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.0, device = 'cpu'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24247336
E           Max relative difference: 0.228543
E            x: array(-1.303426, dtype=float32)
E            y: array(-1.060953, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc736632df0>
Debug: states: <kenlm.State object at 0x7fc736649170>
Debug: states: <kenlm.State object at 0x7fc736649930>
Debug: states: <kenlm.State object at 0x7fc736649370>
Debug: states: <kenlm.State object at 0x7fc736649730>
Debug: states: <kenlm.State object at 0x7fc7366498b0>
Debug: states: <kenlm.State object at 0x7fc7366497f0>
Debug: states: <kenlm.State object at 0x7fc736649a30>
Debug: states: <kenlm.State object at 0x7fc736649970>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:40,816 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:40,924 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:05:40,927 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:40,961 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:40,961 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:41,012 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:41,012 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:41,013 (beam_search:389) INFO: total log probability: -1.3034262657165527
2020-06-20 16:05:41,013 (beam_search:390) INFO: normalized log probability: -0.6517131328582764
2020-06-20 16:05:41,013 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:41,013 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:41,042 (beam_search:389) INFO: total log probability: -1.06095290184021
2020-06-20 16:05:41,042 (beam_search:390) INFO: normalized log probability: -0.530476450920105
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.3034262657165527
INFO     root:beam_search.py:390 normalized log probability: -0.6517131328582764
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.06095290184021
INFO     root:beam_search.py:390 normalized log probability: -0.530476450920105
_ test_batch_beam_search_equal[transformer-args37-0.0-transformer-lm_args37-0.0-0.5-0.0-cpu-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.0, device = 'cpu'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.58149054
E           Max relative difference: 0.80546018
E            x: array(-1.303426)
E            y: array(-0.721936)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7365e1930>
Debug: states: <kenlm.State object at 0x7fc7365e1d30>
Debug: states: <kenlm.State object at 0x7fc7365e1870>
Debug: states: <kenlm.State object at 0x7fc7365e1770>
Debug: states: <kenlm.State object at 0x7fc7365e1230>
Debug: states: <kenlm.State object at 0x7fc7365e1ef0>
Debug: states: <kenlm.State object at 0x7fc7365e1fb0>
Debug: states: <kenlm.State object at 0x7fc7365e1f70>
Debug: states: <kenlm.State object at 0x7fc7365e1770>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:41,093 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:41,172 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:05:41,176 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:41,280 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:41,280 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:41,343 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:41,343 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:41,343 (beam_search:389) INFO: total log probability: -1.3034263327862567
2020-06-20 16:05:41,343 (beam_search:390) INFO: normalized log probability: -0.6517131663931284
2020-06-20 16:05:41,343 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:41,343 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:41,372 (beam_search:389) INFO: total log probability: -0.7219357951111034
2020-06-20 16:05:41,372 (beam_search:390) INFO: normalized log probability: -0.3609678975555517
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.3034263327862567
INFO     root:beam_search.py:390 normalized log probability: -0.6517131663931284
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -0.7219357951111034
INFO     root:beam_search.py:390 normalized log probability: -0.3609678975555517
_ test_batch_beam_search_equal[transformer-args38-0.0-transformer-lm_args38-0.0-0.5-0.1-cpu-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.1, device = 'cpu'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24247336
E           Max relative difference: 0.25232595
E            x: array(-1.203426, dtype=float32)
E            y: array(-0.960953, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7365534b0>
Debug: states: <kenlm.State object at 0x7fc7365536f0>
Debug: states: <kenlm.State object at 0x7fc736553770>
Debug: states: <kenlm.State object at 0x7fc7365535b0>
Debug: states: <kenlm.State object at 0x7fc736553570>
Debug: states: <kenlm.State object at 0x7fc7365538b0>
Debug: states: <kenlm.State object at 0x7fc736553730>
Debug: states: <kenlm.State object at 0x7fc736553870>
Debug: states: <kenlm.State object at 0x7fc736553530>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:41,466 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:41,469 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:05:41,473 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:41,479 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:41,479 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:41,531 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:41,531 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:41,531 (beam_search:389) INFO: total log probability: -1.2034262418746948
2020-06-20 16:05:41,532 (beam_search:390) INFO: normalized log probability: -0.6017131209373474
2020-06-20 16:05:41,532 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:41,532 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:41,562 (beam_search:389) INFO: total log probability: -0.960952877998352
2020-06-20 16:05:41,562 (beam_search:390) INFO: normalized log probability: -0.480476438999176
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.2034262418746948
INFO     root:beam_search.py:390 normalized log probability: -0.6017131209373474
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -0.960952877998352
INFO     root:beam_search.py:390 normalized log probability: -0.480476438999176
_ test_batch_beam_search_equal[transformer-args39-0.0-transformer-lm_args39-0.0-0.5-0.1-cpu-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.1, device = 'cpu'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.58149054
E           Max relative difference: 0.93496876
E            x: array(-1.203426)
E            y: array(-0.621936)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc73650d9f0>
Debug: states: <kenlm.State object at 0x7fc73650d7f0>
Debug: states: <kenlm.State object at 0x7fc73650d8b0>
Debug: states: <kenlm.State object at 0x7fc73650dbb0>
Debug: states: <kenlm.State object at 0x7fc73650d730>
Debug: states: <kenlm.State object at 0x7fc73650d870>
Debug: states: <kenlm.State object at 0x7fc73650d9b0>
Debug: states: <kenlm.State object at 0x7fc73650db70>
Debug: states: <kenlm.State object at 0x7fc73650dbb0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:41,660 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:41,733 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:05:41,737 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:41,807 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:41,807 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:41,879 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:41,879 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:41,880 (beam_search:389) INFO: total log probability: -1.2034263327862567
2020-06-20 16:05:41,880 (beam_search:390) INFO: normalized log probability: -0.6017131663931283
2020-06-20 16:05:41,880 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:41,880 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:41,919 (beam_search:389) INFO: total log probability: -0.6219357951111034
2020-06-20 16:05:41,919 (beam_search:390) INFO: normalized log probability: -0.3109678975555517
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.2034263327862567
INFO     root:beam_search.py:390 normalized log probability: -0.6017131663931283
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -0.6219357951111034
INFO     root:beam_search.py:390 normalized log probability: -0.3109678975555517
_ test_batch_beam_search_equal[transformer-args44-0.0-transformer-lm_args44-0.5-0.5-0.0-cpu-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.0, device = 'cpu'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24247336
E           Max relative difference: 0.12247153
E            x: array(-2.222308, dtype=float32)
E            y: array(-1.979835, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7365533f0>
Debug: states: <kenlm.State object at 0x7fc736553070>
Debug: states: <kenlm.State object at 0x7fc736553870>
Debug: states: <kenlm.State object at 0x7fc7365538b0>
Debug: states: <kenlm.State object at 0x7fc7365530f0>
Debug: states: <kenlm.State object at 0x7fc736553170>
Debug: states: <kenlm.State object at 0x7fc7365532b0>
Debug: states: <kenlm.State object at 0x7fc736553b70>
Debug: states: <kenlm.State object at 0x7fc7365533b0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:43,292 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:43,408 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:05:43,411 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:43,465 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:43,465 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:43,536 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:43,536 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:43,537 (beam_search:389) INFO: total log probability: -2.2223079204559326
2020-06-20 16:05:43,537 (beam_search:390) INFO: normalized log probability: -1.1111539602279663
2020-06-20 16:05:43,537 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:43,537 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:43,574 (beam_search:389) INFO: total log probability: -1.9798345565795898
2020-06-20 16:05:43,574 (beam_search:390) INFO: normalized log probability: -0.9899172782897949
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.2223079204559326
INFO     root:beam_search.py:390 normalized log probability: -1.1111539602279663
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.9798345565795898
INFO     root:beam_search.py:390 normalized log probability: -0.9899172782897949
_ test_batch_beam_search_equal[transformer-args45-0.0-transformer-lm_args45-0.5-0.5-0.0-cpu-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.0, device = 'cpu'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24619281
E           Max relative difference: 0.12458424
E            x: array(-2.222308)
E            y: array(-1.976115)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7365fce70>
Debug: states: <kenlm.State object at 0x7fc7365c6270>
Debug: states: <kenlm.State object at 0x7fc7365c62b0>
Debug: states: <kenlm.State object at 0x7fc7365c60b0>
Debug: states: <kenlm.State object at 0x7fc7365c6370>
Debug: states: <kenlm.State object at 0x7fc7365c61f0>
Debug: states: <kenlm.State object at 0x7fc7365c6030>
Debug: states: <kenlm.State object at 0x7fc7365c6130>
Debug: states: <kenlm.State object at 0x7fc7365c61b0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:43,656 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:43,725 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:05:43,729 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:43,816 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:43,817 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:43,899 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:43,900 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:43,900 (beam_search:389) INFO: total log probability: -2.2223080404301037
2020-06-20 16:05:43,900 (beam_search:390) INFO: normalized log probability: -1.1111540202150518
2020-06-20 16:05:43,900 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:43,900 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:43,938 (beam_search:389) INFO: total log probability: -1.976115227511166
2020-06-20 16:05:43,938 (beam_search:390) INFO: normalized log probability: -0.988057613755583
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.2223080404301037
INFO     root:beam_search.py:390 normalized log probability: -1.1111540202150518
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.976115227511166
INFO     root:beam_search.py:390 normalized log probability: -0.988057613755583
_ test_batch_beam_search_equal[transformer-args46-0.0-transformer-lm_args46-0.5-0.5-0.1-cpu-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.1, device = 'cpu'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24247348
E           Max relative difference: 0.12898661
E            x: array(-2.122308, dtype=float32)
E            y: array(-1.879835, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7365b2430>
Debug: states: <kenlm.State object at 0x7fc7365b2a70>
Debug: states: <kenlm.State object at 0x7fc7365b2bf0>
Debug: states: <kenlm.State object at 0x7fc7365b2ab0>
Debug: states: <kenlm.State object at 0x7fc7365b2e70>
Debug: states: <kenlm.State object at 0x7fc7365b22f0>
Debug: states: <kenlm.State object at 0x7fc7365b2ef0>
Debug: states: <kenlm.State object at 0x7fc7365b23f0>
Debug: states: <kenlm.State object at 0x7fc7365b2d70>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:44,064 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:44,145 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:05:44,149 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:44,246 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:44,246 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:44,316 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:44,317 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:44,317 (beam_search:389) INFO: total log probability: -2.1223080158233643
2020-06-20 16:05:44,317 (beam_search:390) INFO: normalized log probability: -1.0611540079116821
2020-06-20 16:05:44,317 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:44,317 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:44,354 (beam_search:389) INFO: total log probability: -1.879834532737732
2020-06-20 16:05:44,354 (beam_search:390) INFO: normalized log probability: -0.939917266368866
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.1223080158233643
INFO     root:beam_search.py:390 normalized log probability: -1.0611540079116821
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.879834532737732
INFO     root:beam_search.py:390 normalized log probability: -0.939917266368866
_ test_batch_beam_search_equal[transformer-args47-0.0-transformer-lm_args47-0.5-0.5-0.1-cpu-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.1, device = 'cpu'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24619281
E           Max relative difference: 0.13122478
E            x: array(-2.122308)
E            y: array(-1.876115)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc73646c630>
Debug: states: <kenlm.State object at 0x7fc73646c870>
Debug: states: <kenlm.State object at 0x7fc73646c8f0>
Debug: states: <kenlm.State object at 0x7fc73646c8b0>
Debug: states: <kenlm.State object at 0x7fc73646c7b0>
Debug: states: <kenlm.State object at 0x7fc73646c770>
Debug: states: <kenlm.State object at 0x7fc73646c9b0>
Debug: states: <kenlm.State object at 0x7fc73646ca70>
Debug: states: <kenlm.State object at 0x7fc73646c830>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:44,392 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:44,533 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:05:44,537 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:44,628 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:44,628 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:44,710 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:44,710 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:44,710 (beam_search:389) INFO: total log probability: -2.1223080404301036
2020-06-20 16:05:44,710 (beam_search:390) INFO: normalized log probability: -1.0611540202150518
2020-06-20 16:05:44,710 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:44,711 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:44,749 (beam_search:389) INFO: total log probability: -1.8761152275111659
2020-06-20 16:05:44,749 (beam_search:390) INFO: normalized log probability: -0.9380576137555829
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.1223080404301036
INFO     root:beam_search.py:390 normalized log probability: -1.0611540202150518
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.8761152275111659
INFO     root:beam_search.py:390 normalized log probability: -0.9380576137555829
_ test_batch_beam_search_equal[transformer-args52-0.0-default-lm_args52-0.0-0.5-0.0-cuda-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
>           assert expected.yseq.tolist() == actual.yseq.tolist()
E           assert [4, 4] == [4, 2, 1, 1, 2, 4]
E             At index 1 diff: 4 != 2
E             Right contains 4 more items, first extra item: 1
E             Use -v to get the full diff

test/test_batch_beam_search.py:187: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc73659be30>
Debug: states: <kenlm.State object at 0x7fc73659b9f0>
Debug: states: <kenlm.State object at 0x7fc73659bf70>
Debug: states: <kenlm.State object at 0x7fc73659beb0>
Debug: states: <kenlm.State object at 0x7fc73659bcb0>
Debug: states: <kenlm.State object at 0x7fc73659bdb0>
Debug: states: <kenlm.State object at 0x7fc73659bcf0>
Debug: states: <kenlm.State object at 0x7fc73646c030>
Debug: states: <kenlm.State object at 0x7fc73659bef0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:49,704 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:49,845 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:49,856 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:49,856 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:49,969 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:49,970 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:49,970 (beam_search:389) INFO: total log probability: -1.3034262657165527
2020-06-20 16:05:49,970 (beam_search:390) INFO: normalized log probability: -0.6517131328582764
2020-06-20 16:05:49,970 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:49,970 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:50,034 (beam_search:389) INFO: total log probability: 4.319178581237793
2020-06-20 16:05:50,035 (beam_search:390) INFO: normalized log probability: 0.7198631167411804
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.3034262657165527
INFO     root:beam_search.py:390 normalized log probability: -0.6517131328582764
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: 4.319178581237793
INFO     root:beam_search.py:390 normalized log probability: 0.7198631167411804
_ test_batch_beam_search_equal[transformer-args53-0.0-default-lm_args53-0.0-0.5-0.0-cuda-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.2424733
E           Max relative difference: 0.22854292
E            x: array(-1.303426)
E            y: array(-1.060953)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7363d0670>
Debug: states: <kenlm.State object at 0x7fc7363d08b0>
Debug: states: <kenlm.State object at 0x7fc7363d0930>
Debug: states: <kenlm.State object at 0x7fc7363d08f0>
Debug: states: <kenlm.State object at 0x7fc7363d07b0>
Debug: states: <kenlm.State object at 0x7fc7363d0830>
Debug: states: <kenlm.State object at 0x7fc7363d0970>
Debug: states: <kenlm.State object at 0x7fc7363d06b0>
Debug: states: <kenlm.State object at 0x7fc7363d09b0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:50,140 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:50,269 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:50,281 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:50,281 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:50,397 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:50,398 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:50,399 (beam_search:389) INFO: total log probability: -1.3034263327862567
2020-06-20 16:05:50,399 (beam_search:390) INFO: normalized log probability: -0.6517131663931284
2020-06-20 16:05:50,399 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:50,399 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:50,459 (beam_search:389) INFO: total log probability: -1.0609530285145587
2020-06-20 16:05:50,459 (beam_search:390) INFO: normalized log probability: -0.5304765142572794
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.3034263327862567
INFO     root:beam_search.py:390 normalized log probability: -0.6517131663931284
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.0609530285145587
INFO     root:beam_search.py:390 normalized log probability: -0.5304765142572794
_ test_batch_beam_search_equal[transformer-args54-0.0-default-lm_args54-0.0-0.5-0.1-cuda-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
>           assert expected.yseq.tolist() == actual.yseq.tolist()
E           assert [4, 4] == [4, 2, 1, 1, 0, 3, ...]
E             At index 1 diff: 4 != 2
E             Right contains 8 more items, first extra item: 1
E             Use -v to get the full diff

test/test_batch_beam_search.py:187: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7363f1130>
Debug: states: <kenlm.State object at 0x7fc7363f1370>
Debug: states: <kenlm.State object at 0x7fc7363f13f0>
Debug: states: <kenlm.State object at 0x7fc7363f13b0>
Debug: states: <kenlm.State object at 0x7fc7363f11f0>
Debug: states: <kenlm.State object at 0x7fc7363f14f0>
Debug: states: <kenlm.State object at 0x7fc7363f1170>
Debug: states: <kenlm.State object at 0x7fc7363f14b0>
Debug: states: <kenlm.State object at 0x7fc7363f13f0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:50,597 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:50,657 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:50,669 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:50,670 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:50,787 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:50,788 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:50,789 (beam_search:389) INFO: total log probability: -1.2034262418746948
2020-06-20 16:05:50,789 (beam_search:390) INFO: normalized log probability: -0.6017131209373474
2020-06-20 16:05:50,789 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:50,789 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:50,852 (beam_search:389) INFO: total log probability: 0.4085348844528198
2020-06-20 16:05:50,852 (beam_search:390) INFO: normalized log probability: 0.04085348919034004
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.2034262418746948
INFO     root:beam_search.py:390 normalized log probability: -0.6017131209373474
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: 0.4085348844528198
INFO     root:beam_search.py:390 normalized log probability: 0.04085348919034004
_ test_batch_beam_search_equal[transformer-args55-0.0-default-lm_args55-0.0-0.5-0.1-cuda-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.58149054
E           Max relative difference: 0.93496876
E            x: array(-1.203426)
E            y: array(-0.621936)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc736399e70>
Debug: states: <kenlm.State object at 0x7fc7363f1030>
Debug: states: <kenlm.State object at 0x7fc7363f1070>
Debug: states: <kenlm.State object at 0x7fc7363f10b0>
Debug: states: <kenlm.State object at 0x7fc7363f12b0>
Debug: states: <kenlm.State object at 0x7fc7363f10f0>
Debug: states: <kenlm.State object at 0x7fc7363f1270>
Debug: states: <kenlm.State object at 0x7fc7363f1070>
Debug: states: <kenlm.State object at 0x7fc7363f19f0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:50,954 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:51,060 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:51,070 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:51,070 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:51,160 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:51,161 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:51,161 (beam_search:389) INFO: total log probability: -1.2034263327862567
2020-06-20 16:05:51,161 (beam_search:390) INFO: normalized log probability: -0.6017131663931283
2020-06-20 16:05:51,161 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:51,161 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:51,210 (beam_search:389) INFO: total log probability: -0.6219357951111034
2020-06-20 16:05:51,210 (beam_search:390) INFO: normalized log probability: -0.3109678975555517
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.2034263327862567
INFO     root:beam_search.py:390 normalized log probability: -0.6017131663931283
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -0.6219357951111034
INFO     root:beam_search.py:390 normalized log probability: -0.3109678975555517
_ test_batch_beam_search_equal[transformer-args60-0.0-default-lm_args60-0.5-0.5-0.0-cuda-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.28273487
E           Max relative difference: 0.15518863
E            x: array(-2.104614, dtype=float32)
E            y: array(-1.821879, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7365ac0f0>
Debug: states: <kenlm.State object at 0x7fc7364720b0>
Debug: states: <kenlm.State object at 0x7fc73f4efa30>
Debug: states: <kenlm.State object at 0x7fc7364723b0>
Debug: states: <kenlm.State object at 0x7fc736472030>
Debug: states: <kenlm.State object at 0x7fc736472370>
Debug: states: <kenlm.State object at 0x7fc73f4eff30>
Debug: states: <kenlm.State object at 0x7fc736472eb0>
Debug: states: <kenlm.State object at 0x7fc7364723f0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:52,920 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:53,008 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:53,018 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:53,018 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:53,115 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:53,118 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:53,118 (beam_search:389) INFO: total log probability: -2.1046135425567627
2020-06-20 16:05:53,118 (beam_search:390) INFO: normalized log probability: -1.0523067712783813
2020-06-20 16:05:53,118 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:53,118 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:53,171 (beam_search:389) INFO: total log probability: -1.8218786716461182
2020-06-20 16:05:53,171 (beam_search:390) INFO: normalized log probability: -0.9109393358230591
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.1046135425567627
INFO     root:beam_search.py:390 normalized log probability: -1.0523067712783813
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.8218786716461182
INFO     root:beam_search.py:390 normalized log probability: -0.9109393358230591
_ test_batch_beam_search_equal[transformer-args61-0.0-default-lm_args61-0.5-0.5-0.0-cuda-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.28273482
E           Max relative difference: 0.1551886
E            x: array(-2.104614)
E            y: array(-1.821879)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7363b2bf0>
Debug: states: <kenlm.State object at 0x7fc7363b2d70>
Debug: states: <kenlm.State object at 0x7fc7363b27f0>
Debug: states: <kenlm.State object at 0x7fc7363b2870>
Debug: states: <kenlm.State object at 0x7fc7363b2b70>
Debug: states: <kenlm.State object at 0x7fc7363b23f0>
Debug: states: <kenlm.State object at 0x7fc7363b20f0>
Debug: states: <kenlm.State object at 0x7fc7363b27f0>
Debug: states: <kenlm.State object at 0x7fc7363b2730>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:53,358 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:53,472 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:53,482 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:53,482 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:53,579 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:53,580 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:53,580 (beam_search:389) INFO: total log probability: -2.1046135462152105
2020-06-20 16:05:53,580 (beam_search:390) INFO: normalized log probability: -1.0523067731076052
2020-06-20 16:05:53,580 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:53,580 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:53,631 (beam_search:389) INFO: total log probability: -1.8218787297751557
2020-06-20 16:05:53,632 (beam_search:390) INFO: normalized log probability: -0.9109393648875779
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.1046135462152105
INFO     root:beam_search.py:390 normalized log probability: -1.0523067731076052
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.8218787297751557
INFO     root:beam_search.py:390 normalized log probability: -0.9109393648875779
_ test_batch_beam_search_equal[transformer-args62-0.0-default-lm_args62-0.5-0.5-0.1-cuda-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.282735
E           Max relative difference: 0.16420147
E            x: array(-2.004614, dtype=float32)
E            y: array(-1.721879, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc73631c9b0>
Debug: states: <kenlm.State object at 0x7fc73631cbb0>
Debug: states: <kenlm.State object at 0x7fc73631caf0>
Debug: states: <kenlm.State object at 0x7fc73631cd30>
Debug: states: <kenlm.State object at 0x7fc73631cc70>
Debug: states: <kenlm.State object at 0x7fc73631cab0>
Debug: states: <kenlm.State object at 0x7fc73631ccb0>
Debug: states: <kenlm.State object at 0x7fc73631caf0>
Debug: states: <kenlm.State object at 0x7fc73631cb30>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:53,752 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:53,852 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:53,864 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:53,865 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:53,969 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:53,971 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:53,971 (beam_search:389) INFO: total log probability: -2.0046136379241943
2020-06-20 16:05:53,971 (beam_search:390) INFO: normalized log probability: -1.0023068189620972
2020-06-20 16:05:53,971 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:53,971 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:54,029 (beam_search:389) INFO: total log probability: -1.7218786478042603
2020-06-20 16:05:54,029 (beam_search:390) INFO: normalized log probability: -0.8609393239021301
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.0046136379241943
INFO     root:beam_search.py:390 normalized log probability: -1.0023068189620972
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.7218786478042603
INFO     root:beam_search.py:390 normalized log probability: -0.8609393239021301
_ test_batch_beam_search_equal[transformer-args63-0.0-default-lm_args63-0.5-0.5-0.1-cuda-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='lstm', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.28273482
E           Max relative difference: 0.16420135
E            x: array(-2.004614)
E            y: array(-1.721879)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7362fa630>
Debug: states: <kenlm.State object at 0x7fc7362fa430>
Debug: states: <kenlm.State object at 0x7fc7362fa130>
Debug: states: <kenlm.State object at 0x7fc7362fa0f0>
Debug: states: <kenlm.State object at 0x7fc7362fa7f0>
Debug: states: <kenlm.State object at 0x7fc7362fa4f0>
Debug: states: <kenlm.State object at 0x7fc7362fa330>
Debug: states: <kenlm.State object at 0x7fc7362fa130>
Debug: states: <kenlm.State object at 0x7fc7362fac30>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:54,144 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:54,264 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:54,276 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:54,276 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:54,374 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:54,375 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:54,375 (beam_search:389) INFO: total log probability: -2.0046135462152104
2020-06-20 16:05:54,375 (beam_search:390) INFO: normalized log probability: -1.0023067731076052
2020-06-20 16:05:54,375 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:54,375 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:54,429 (beam_search:389) INFO: total log probability: -1.7218787297751557
2020-06-20 16:05:54,429 (beam_search:390) INFO: normalized log probability: -0.8609393648875778
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.0046135462152104
INFO     root:beam_search.py:390 normalized log probability: -1.0023067731076052
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.7218787297751557
INFO     root:beam_search.py:390 normalized log probability: -0.8609393648875778
_ test_batch_beam_search_equal[transformer-args68-0.0-default-lm_args68-0.0-0.5-0.0-cuda-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
>           assert expected.yseq.tolist() == actual.yseq.tolist()
E           assert [4, 4] == [4, 1, 1, 1, 2, 4]
E             At index 1 diff: 4 != 1
E             Right contains 4 more items, first extra item: 1
E             Use -v to get the full diff

test/test_batch_beam_search.py:187: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7362f7ab0>
Debug: states: <kenlm.State object at 0x7fc7362f72f0>
Debug: states: <kenlm.State object at 0x7fc7362f70b0>
Debug: states: <kenlm.State object at 0x7fc7362f70f0>
Debug: states: <kenlm.State object at 0x7fc7362f7fb0>
Debug: states: <kenlm.State object at 0x7fc7362f7ab0>
Debug: states: <kenlm.State object at 0x7fc7362f7bf0>
Debug: states: <kenlm.State object at 0x7fc7362f7a70>
Debug: states: <kenlm.State object at 0x7fc7362f7570>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:55,877 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:55,956 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:55,965 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:55,965 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:56,055 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:56,055 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:56,056 (beam_search:389) INFO: total log probability: -1.3034262657165527
2020-06-20 16:05:56,056 (beam_search:390) INFO: normalized log probability: -0.6517131328582764
2020-06-20 16:05:56,056 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:56,056 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:56,103 (beam_search:389) INFO: total log probability: 4.246079921722412
2020-06-20 16:05:56,103 (beam_search:390) INFO: normalized log probability: 0.7076799869537354
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.3034262657165527
INFO     root:beam_search.py:390 normalized log probability: -0.6517131328582764
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: 4.246079921722412
INFO     root:beam_search.py:390 normalized log probability: 0.7076799869537354
_ test_batch_beam_search_equal[transformer-args69-0.0-default-lm_args69-0.0-0.5-0.0-cuda-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.2424733
E           Max relative difference: 0.22854292
E            x: array(-1.303426)
E            y: array(-1.060953)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc73647acf0>
Debug: states: <kenlm.State object at 0x7fc73647afb0>
Debug: states: <kenlm.State object at 0x7fc73647af30>
Debug: states: <kenlm.State object at 0x7fc73647a5f0>
Debug: states: <kenlm.State object at 0x7fc73647a6f0>
Debug: states: <kenlm.State object at 0x7fc73647aeb0>
Debug: states: <kenlm.State object at 0x7fc73647a730>
Debug: states: <kenlm.State object at 0x7fc73647ad30>
Debug: states: <kenlm.State object at 0x7fc73647a830>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:56,225 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:56,305 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:56,315 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:56,315 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:56,403 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:56,404 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:56,404 (beam_search:389) INFO: total log probability: -1.3034263327862567
2020-06-20 16:05:56,404 (beam_search:390) INFO: normalized log probability: -0.6517131663931284
2020-06-20 16:05:56,404 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:56,404 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:56,450 (beam_search:389) INFO: total log probability: -1.0609530285145587
2020-06-20 16:05:56,451 (beam_search:390) INFO: normalized log probability: -0.5304765142572794
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.3034263327862567
INFO     root:beam_search.py:390 normalized log probability: -0.6517131663931284
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.0609530285145587
INFO     root:beam_search.py:390 normalized log probability: -0.5304765142572794
_ test_batch_beam_search_equal[transformer-args70-0.0-default-lm_args70-0.0-0.5-0.1-cuda-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
>           assert expected.yseq.tolist() == actual.yseq.tolist()
E           assert [4, 4] == [4, 1, 1, 1, 2, 4]
E             At index 1 diff: 4 != 1
E             Right contains 4 more items, first extra item: 1
E             Use -v to get the full diff

test/test_batch_beam_search.py:187: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7364ace30>
Debug: states: <kenlm.State object at 0x7fc7363f10b0>
Debug: states: <kenlm.State object at 0x7fc7363f1df0>
Debug: states: <kenlm.State object at 0x7fc7363f14b0>
Debug: states: <kenlm.State object at 0x7fc7363f1330>
Debug: states: <kenlm.State object at 0x7fc7363f13f0>
Debug: states: <kenlm.State object at 0x7fc7363f1df0>
Debug: states: <kenlm.State object at 0x7fc7363f11b0>
Debug: states: <kenlm.State object at 0x7fc7363f14b0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:56,556 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:56,645 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:56,655 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:56,655 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:56,748 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:56,749 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:56,749 (beam_search:389) INFO: total log probability: -1.2034262418746948
2020-06-20 16:05:56,749 (beam_search:390) INFO: normalized log probability: -0.6017131209373474
2020-06-20 16:05:56,749 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:56,749 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:56,799 (beam_search:389) INFO: total log probability: 4.746079444885254
2020-06-20 16:05:56,799 (beam_search:390) INFO: normalized log probability: 0.791013240814209
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.2034262418746948
INFO     root:beam_search.py:390 normalized log probability: -0.6017131209373474
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: 4.746079444885254
INFO     root:beam_search.py:390 normalized log probability: 0.791013240814209
_ test_batch_beam_search_equal[transformer-args71-0.0-default-lm_args71-0.0-0.5-0.1-cuda-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 6.72110874
E           Max relative difference: 0.84813919
E            x: array(-1.203426)
E            y: array(-7.924535)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc73632d470>
Debug: states: <kenlm.State object at 0x7fc73632d6f0>
Debug: states: <kenlm.State object at 0x7fc73632d770>
Debug: states: <kenlm.State object at 0x7fc73632d730>
Debug: states: <kenlm.State object at 0x7fc73632d5f0>
Debug: states: <kenlm.State object at 0x7fc73632d7b0>
Debug: states: <kenlm.State object at 0x7fc73632d670>
Debug: states: <kenlm.State object at 0x7fc73632d870>
Debug: states: <kenlm.State object at 0x7fc73632d7f0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:56,896 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:56,969 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:56,979 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:56,979 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:57,070 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:57,071 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:57,071 (beam_search:389) INFO: total log probability: -1.2034263327862567
2020-06-20 16:05:57,072 (beam_search:390) INFO: normalized log probability: -0.6017131663931283
2020-06-20 16:05:57,072 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:57,072 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:57,121 (beam_search:389) INFO: total log probability: -7.924535073953111
2020-06-20 16:05:57,121 (beam_search:390) INFO: normalized log probability: -3.9622675369765554
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.2034263327862567
INFO     root:beam_search.py:390 normalized log probability: -0.6017131663931283
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -7.924535073953111
INFO     root:beam_search.py:390 normalized log probability: -3.9622675369765554
_ test_batch_beam_search_equal[transformer-args76-0.0-default-lm_args76-0.5-0.5-0.0-cuda-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.27812254
E           Max relative difference: 0.15442061
E            x: array(-2.079194, dtype=float32)
E            y: array(-1.801071, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc736292470>
Debug: states: <kenlm.State object at 0x7fc736292b70>
Debug: states: <kenlm.State object at 0x7fc736292bb0>
Debug: states: <kenlm.State object at 0x7fc7362920b0>
Debug: states: <kenlm.State object at 0x7fc736292e70>
Debug: states: <kenlm.State object at 0x7fc736292ef0>
Debug: states: <kenlm.State object at 0x7fc736292970>
Debug: states: <kenlm.State object at 0x7fc736292c70>
Debug: states: <kenlm.State object at 0x7fc736292470>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:58,653 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:58,743 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:58,753 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:58,753 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:58,854 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:58,855 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:58,856 (beam_search:389) INFO: total log probability: -2.0791938304901123
2020-06-20 16:05:58,856 (beam_search:390) INFO: normalized log probability: -1.0395969152450562
2020-06-20 16:05:58,856 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:58,856 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:58,912 (beam_search:389) INFO: total log probability: -1.801071286201477
2020-06-20 16:05:58,912 (beam_search:390) INFO: normalized log probability: -0.9005356431007385
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.0791938304901123
INFO     root:beam_search.py:390 normalized log probability: -1.0395969152450562
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.801071286201477
INFO     root:beam_search.py:390 normalized log probability: -0.9005356431007385
_ test_batch_beam_search_equal[transformer-args77-0.0-default-lm_args77-0.5-0.5-0.0-cuda-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.27812256
E           Max relative difference: 0.15442063
E            x: array(-2.079194)
E            y: array(-1.801071)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7362365b0>
Debug: states: <kenlm.State object at 0x7fc7362367b0>
Debug: states: <kenlm.State object at 0x7fc7362367f0>
Debug: states: <kenlm.State object at 0x7fc736236930>
Debug: states: <kenlm.State object at 0x7fc736236630>
Debug: states: <kenlm.State object at 0x7fc7362366b0>
Debug: states: <kenlm.State object at 0x7fc7362368b0>
Debug: states: <kenlm.State object at 0x7fc736236830>
Debug: states: <kenlm.State object at 0x7fc736236770>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:59,080 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:59,168 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:59,179 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:59,179 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:59,273 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:59,274 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:59,275 (beam_search:389) INFO: total log probability: -2.0791939061521316
2020-06-20 16:05:59,275 (beam_search:390) INFO: normalized log probability: -1.0395969530760658
2020-06-20 16:05:59,275 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:59,275 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:59,328 (beam_search:389) INFO: total log probability: -1.8010713431629464
2020-06-20 16:05:59,328 (beam_search:390) INFO: normalized log probability: -0.9005356715814732
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.0791939061521316
INFO     root:beam_search.py:390 normalized log probability: -1.0395969530760658
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.8010713431629464
INFO     root:beam_search.py:390 normalized log probability: -0.9005356715814732
_ test_batch_beam_search_equal[transformer-args78-0.0-default-lm_args78-0.5-0.5-0.1-cuda-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.27812254
E           Max relative difference: 0.16349846
E            x: array(-1.979194, dtype=float32)
E            y: array(-1.701071, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7361d7030>
Debug: states: <kenlm.State object at 0x7fc7361d7230>
Debug: states: <kenlm.State object at 0x7fc7361d7170>
Debug: states: <kenlm.State object at 0x7fc7361d72b0>
Debug: states: <kenlm.State object at 0x7fc7361d7430>
Debug: states: <kenlm.State object at 0x7fc7361d70b0>
Debug: states: <kenlm.State object at 0x7fc7361d7170>
Debug: states: <kenlm.State object at 0x7fc7361d71b0>
Debug: states: <kenlm.State object at 0x7fc7361d72f0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:59,464 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:59,540 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:59,550 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:59,550 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:59,651 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:05:59,652 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:05:59,652 (beam_search:389) INFO: total log probability: -1.9791938066482544
2020-06-20 16:05:59,653 (beam_search:390) INFO: normalized log probability: -0.9895969033241272
2020-06-20 16:05:59,653 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:59,653 (beam_search:354) INFO: min output length: 0
2020-06-20 16:05:59,708 (beam_search:389) INFO: total log probability: -1.7010712623596191
2020-06-20 16:05:59,708 (beam_search:390) INFO: normalized log probability: -0.8505356311798096
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.9791938066482544
INFO     root:beam_search.py:390 normalized log probability: -0.9895969033241272
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.7010712623596191
INFO     root:beam_search.py:390 normalized log probability: -0.8505356311798096
_ test_batch_beam_search_equal[transformer-args79-0.0-default-lm_args79-0.5-0.5-0.1-cuda-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'default'
lm_args = Namespace(dropout_rate=0.0, layer=1, type='gru', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.27812256
E           Max relative difference: 0.16349847
E            x: array(-1.979194)
E            y: array(-1.701071)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7362af330>
Debug: states: <kenlm.State object at 0x7fc7361d78b0>
Debug: states: <kenlm.State object at 0x7fc7362af030>
Debug: states: <kenlm.State object at 0x7fc7361d7270>
Debug: states: <kenlm.State object at 0x7fc7361d7170>
Debug: states: <kenlm.State object at 0x7fc7361d7430>
Debug: states: <kenlm.State object at 0x7fc7361d72f0>
Debug: states: <kenlm.State object at 0x7fc7361d70f0>
Debug: states: <kenlm.State object at 0x7fc7361d79f0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:05:59,780 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:05:59,896 (decoder:112) INFO: decoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:05:59,906 (beam_search:353) INFO: max output length: 9
2020-06-20 16:05:59,906 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:00,005 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:06:00,006 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:06:00,006 (beam_search:389) INFO: total log probability: -1.9791939061521315
2020-06-20 16:06:00,006 (beam_search:390) INFO: normalized log probability: -0.9895969530760658
2020-06-20 16:06:00,006 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:00,006 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:00,060 (beam_search:389) INFO: total log probability: -1.7010713431629463
2020-06-20 16:06:00,060 (beam_search:390) INFO: normalized log probability: -0.8505356715814731
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.9791939061521315
INFO     root:beam_search.py:390 normalized log probability: -0.9895969530760658
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.7010713431629463
INFO     root:beam_search.py:390 normalized log probability: -0.8505356715814731
_ test_batch_beam_search_equal[transformer-args84-0.0-transformer-lm_args84-0.0-0.5-0.0-cuda-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
>           assert expected.yseq.tolist() == actual.yseq.tolist()
E           assert [4, 4] == [4, 2, 1, 1, 2, 4]
E             At index 1 diff: 4 != 2
E             Right contains 4 more items, first extra item: 1
E             Use -v to get the full diff

test/test_batch_beam_search.py:187: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7362a8470>
Debug: states: <kenlm.State object at 0x7fc7362a86f0>
Debug: states: <kenlm.State object at 0x7fc7362a85b0>
Debug: states: <kenlm.State object at 0x7fc7362a8830>
Debug: states: <kenlm.State object at 0x7fc7362a8df0>
Debug: states: <kenlm.State object at 0x7fc7362a8470>
Debug: states: <kenlm.State object at 0x7fc7362a8b70>
Debug: states: <kenlm.State object at 0x7fc7362a8130>
Debug: states: <kenlm.State object at 0x7fc7362a85f0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:06:01,765 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:06:01,824 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:06:01,828 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:06:01,835 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:01,835 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:01,926 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:06:01,927 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:06:01,927 (beam_search:389) INFO: total log probability: -1.3034262657165527
2020-06-20 16:06:01,927 (beam_search:390) INFO: normalized log probability: -0.6517131328582764
2020-06-20 16:06:01,927 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:01,927 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:01,975 (beam_search:389) INFO: total log probability: 4.367118835449219
2020-06-20 16:06:01,975 (beam_search:390) INFO: normalized log probability: 0.7278531193733215
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.3034262657165527
INFO     root:beam_search.py:390 normalized log probability: -0.6517131328582764
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: 4.367118835449219
INFO     root:beam_search.py:390 normalized log probability: 0.7278531193733215
_ test_batch_beam_search_equal[transformer-args85-0.0-transformer-lm_args85-0.0-0.5-0.0-cuda-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.58149054
E           Max relative difference: 0.80546018
E            x: array(-1.303426)
E            y: array(-0.721936)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc73629e8b0>
Debug: states: <kenlm.State object at 0x7fc7361d75b0>
Debug: states: <kenlm.State object at 0x7fc7361d71b0>
Debug: states: <kenlm.State object at 0x7fc7361d74f0>
Debug: states: <kenlm.State object at 0x7fc7361d7770>
Debug: states: <kenlm.State object at 0x7fc7361d7530>
Debug: states: <kenlm.State object at 0x7fc7361d7430>
Debug: states: <kenlm.State object at 0x7fc7361d7570>
Debug: states: <kenlm.State object at 0x7fc7361d74f0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:06:02,100 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:06:02,181 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:06:02,186 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:06:02,193 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:02,193 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:02,281 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:06:02,281 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:06:02,282 (beam_search:389) INFO: total log probability: -1.3034263327862567
2020-06-20 16:06:02,282 (beam_search:390) INFO: normalized log probability: -0.6517131663931284
2020-06-20 16:06:02,282 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:02,282 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:02,328 (beam_search:389) INFO: total log probability: -0.7219357951111034
2020-06-20 16:06:02,328 (beam_search:390) INFO: normalized log probability: -0.3609678975555517
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.3034263327862567
INFO     root:beam_search.py:390 normalized log probability: -0.6517131663931284
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -0.7219357951111034
INFO     root:beam_search.py:390 normalized log probability: -0.3609678975555517
_ test_batch_beam_search_equal[transformer-args86-0.0-transformer-lm_args86-0.0-0.5-0.1-cuda-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.58149046
E           Max relative difference: 0.93496865
E            x: array(-1.203426, dtype=float32)
E            y: array(-0.621936, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc736119c30>
Debug: states: <kenlm.State object at 0x7fc736119e70>
Debug: states: <kenlm.State object at 0x7fc736119ef0>
Debug: states: <kenlm.State object at 0x7fc736119eb0>
Debug: states: <kenlm.State object at 0x7fc736119db0>
Debug: states: <kenlm.State object at 0x7fc736119fb0>
Debug: states: <kenlm.State object at 0x7fc736119f30>
Debug: states: <kenlm.State object at 0x7fc736119df0>
Debug: states: <kenlm.State object at 0x7fc736119db0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:06:02,461 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:06:02,545 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:06:02,549 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:06:02,557 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:02,557 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:02,651 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:06:02,652 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:06:02,652 (beam_search:389) INFO: total log probability: -1.2034262418746948
2020-06-20 16:06:02,652 (beam_search:390) INFO: normalized log probability: -0.6017131209373474
2020-06-20 16:06:02,652 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:02,652 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:02,703 (beam_search:389) INFO: total log probability: -0.6219357848167419
2020-06-20 16:06:02,703 (beam_search:390) INFO: normalized log probability: -0.31096789240837097
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.2034262418746948
INFO     root:beam_search.py:390 normalized log probability: -0.6017131209373474
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -0.6219357848167419
INFO     root:beam_search.py:390 normalized log probability: -0.31096789240837097
_ test_batch_beam_search_equal[transformer-args87-0.0-transformer-lm_args87-0.0-0.5-0.1-cuda-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.0, ngram_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 6.72110874
E           Max relative difference: 0.84813919
E            x: array(-1.203426)
E            y: array(-7.924535)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc736324270>
Debug: states: <kenlm.State object at 0x7fc73632c7b0>
Debug: states: <kenlm.State object at 0x7fc73632c9b0>
Debug: states: <kenlm.State object at 0x7fc73632ca30>
Debug: states: <kenlm.State object at 0x7fc73632cef0>
Debug: states: <kenlm.State object at 0x7fc73632c530>
Debug: states: <kenlm.State object at 0x7fc73632c4b0>
Debug: states: <kenlm.State object at 0x7fc73632cef0>
Debug: states: <kenlm.State object at 0x7fc73632ca70>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:06:02,836 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:06:02,921 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:06:02,926 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:06:03,072 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:03,072 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:03,164 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:06:03,165 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:06:03,165 (beam_search:389) INFO: total log probability: -1.2034263327862567
2020-06-20 16:06:03,165 (beam_search:390) INFO: normalized log probability: -0.6017131663931283
2020-06-20 16:06:03,165 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:03,165 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:03,214 (beam_search:389) INFO: total log probability: -7.924535073953111
2020-06-20 16:06:03,214 (beam_search:390) INFO: normalized log probability: -3.9622675369765554
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -1.2034263327862567
INFO     root:beam_search.py:390 normalized log probability: -0.6017131663931283
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -7.924535073953111
INFO     root:beam_search.py:390 normalized log probability: -3.9622675369765554
_ test_batch_beam_search_equal[transformer-args92-0.0-transformer-lm_args92-0.5-0.5-0.0-cuda-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24619293
E           Max relative difference: 0.1245843
E            x: array(-2.222308, dtype=float32)
E            y: array(-1.976115, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7361358f0>
Debug: states: <kenlm.State object at 0x7fc7361350f0>
Debug: states: <kenlm.State object at 0x7fc7361350b0>
Debug: states: <kenlm.State object at 0x7fc736135030>
Debug: states: <kenlm.State object at 0x7fc7361351f0>
Debug: states: <kenlm.State object at 0x7fc7361353f0>
Debug: states: <kenlm.State object at 0x7fc7361351b0>
Debug: states: <kenlm.State object at 0x7fc736135130>
Debug: states: <kenlm.State object at 0x7fc736135230>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:06:04,884 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:06:04,992 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:06:04,995 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:06:05,003 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:05,003 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:05,124 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:06:05,125 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:06:05,125 (beam_search:389) INFO: total log probability: -2.2223081588745117
2020-06-20 16:06:05,125 (beam_search:390) INFO: normalized log probability: -1.1111540794372559
2020-06-20 16:06:05,125 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:05,125 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:05,188 (beam_search:389) INFO: total log probability: -1.9761152267456055
2020-06-20 16:06:05,188 (beam_search:390) INFO: normalized log probability: -0.9880576133728027
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.2223081588745117
INFO     root:beam_search.py:390 normalized log probability: -1.1111540794372559
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.9761152267456055
INFO     root:beam_search.py:390 normalized log probability: -0.9880576133728027
_ test_batch_beam_search_equal[transformer-args93-0.0-transformer-lm_args93-0.5-0.5-0.0-cuda-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.0)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24619281
E           Max relative difference: 0.12458424
E            x: array(-2.222308)
E            y: array(-1.976115)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc736471a70>
Debug: states: <kenlm.State object at 0x7fc736471730>
Debug: states: <kenlm.State object at 0x7fc736471d30>
Debug: states: <kenlm.State object at 0x7fc7364718b0>
Debug: states: <kenlm.State object at 0x7fc736471e70>
Debug: states: <kenlm.State object at 0x7fc7364716b0>
Debug: states: <kenlm.State object at 0x7fc736471fb0>
Debug: states: <kenlm.State object at 0x7fc7364713f0>
Debug: states: <kenlm.State object at 0x7fc736471db0>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:06:05,340 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:06:05,504 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:06:05,509 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:06:05,519 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:05,519 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:05,671 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:06:05,672 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:06:05,672 (beam_search:389) INFO: total log probability: -2.2223080404301037
2020-06-20 16:06:05,672 (beam_search:390) INFO: normalized log probability: -1.1111540202150518
2020-06-20 16:06:05,672 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:05,672 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:05,749 (beam_search:389) INFO: total log probability: -1.976115227511166
2020-06-20 16:06:05,749 (beam_search:390) INFO: normalized log probability: -0.988057613755583
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.2223080404301037
INFO     root:beam_search.py:390 normalized log probability: -1.1111540202150518
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.976115227511166
INFO     root:beam_search.py:390 normalized log probability: -0.988057613755583
_ test_batch_beam_search_equal[transformer-args94-0.0-transformer-lm_args94-0.5-0.5-0.1-cuda-float32] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float32

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24619305
E           Max relative difference: 0.13122492
E            x: array(-2.122308, dtype=float32)
E            y: array(-1.876115, dtype=float32)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7365c6130>
Debug: states: <kenlm.State object at 0x7fc7365c6bf0>
Debug: states: <kenlm.State object at 0x7fc7365c6a30>
Debug: states: <kenlm.State object at 0x7fc7365c62f0>
Debug: states: <kenlm.State object at 0x7fc7365c68f0>
Debug: states: <kenlm.State object at 0x7fc7365c63b0>
Debug: states: <kenlm.State object at 0x7fc7365c6330>
Debug: states: <kenlm.State object at 0x7fc7365c6eb0>
Debug: states: <kenlm.State object at 0x7fc7365c6830>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:06:05,828 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:06:05,917 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:06:05,921 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:06:05,929 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:05,929 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:06,056 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:06:06,057 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:06:06,057 (beam_search:389) INFO: total log probability: -2.1223082542419434
2020-06-20 16:06:06,057 (beam_search:390) INFO: normalized log probability: -1.0611541271209717
2020-06-20 16:06:06,058 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:06,058 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:06,123 (beam_search:389) INFO: total log probability: -1.8761152029037476
2020-06-20 16:06:06,123 (beam_search:390) INFO: normalized log probability: -0.9380576014518738
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.1223082542419434
INFO     root:beam_search.py:390 normalized log probability: -1.0611541271209717
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.8761152029037476
INFO     root:beam_search.py:390 normalized log probability: -0.9380576014518738
_ test_batch_beam_search_equal[transformer-args95-0.0-transformer-lm_args95-0.5-0.5-0.1-cuda-float64] _

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, ngram_weight=0.5, penalty=0.1)
ctc_weight = 0.0, lm_nn = 'transformer'
lm_args = Namespace(att_unit=2, dropout_rate=0.0, embed_unit=2, head=1, layer=1, pos_enc='none', unit=2)
lm_weight = 0.5, ngram_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float64

    @pytest.mark.parametrize(
        "model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, ngram_weight, \
            bonus, device, dtype",
        [
            (nn, args, ctc, lm_nn, lm_args, lm, ngram, bonus, device, dtype)
            for device in ("cpu", "cuda")
            # (("rnn", rnn_args),)
            for nn, args in (("transformer", transformer_args),)
            for ctc in (0.0,)  # 0.5, 1.0)
            for lm_nn, lm_args in (
                ("default", lstm_lm),
                ("default", gru_lm),
                ("transformer", transformer_lm),
            )
            for lm in (0.0, 0.5)
            for ngram in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float32", "float64")  # TODO(karita): float16
        ],
    )
    def test_batch_beam_search_equal(
        model_class,
        args,
        ctc_weight,
        lm_nn,
        lm_args,
        lm_weight,
        ngram_weight,
        bonus,
        device,
        dtype,
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        # https://github.com/pytorch/pytorch/issues/6351
        torch.backends.cudnn.benchmark = False
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(
            model_class, args, mtlalpha=ctc_weight
        )
        model.eval()
        char_list = train_args.char_list
        lm = dynamic_import_lm(lm_nn, backend="pytorch")(len(char_list), lm_args)
        lm.eval()
        root = os.path.dirname(os.path.abspath(__file__))
        ngram = NgramFullScorer(os.path.join(root, "beam_search_test.arpa"), args.char_list)
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            ngram_weight=ngram_weight,
            minlenratio=0,
            nbest=5,
        )
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        if ngram_weight != 0:
            scorers["ngram"] = ngram
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            ngram=args.ngram_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        with torch.no_grad():
            enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))
    
        legacy_beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        legacy_beam.to(device, dtype=dtype)
        legacy_beam.eval()
    
        beam = BatchBeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
            legacy_nbest_bs = legacy_beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
            nbest_bs = beam(
                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
            )
    
        for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):
            assert expected.yseq.tolist() == actual.yseq.tolist()
            numpy.testing.assert_allclose(
>               expected.score.cpu(), actual.score.cpu(), rtol=1e-6
            )
E           AssertionError: 
E           Not equal to tolerance rtol=1e-06, atol=0
E           
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference: 0.24619281
E           Max relative difference: 0.13122478
E            x: array(-2.122308)
E            y: array(-1.876115)

test/test_batch_beam_search.py:189: AssertionError
----------------------------- Captured stdout call -----------------------------
Debug: states: <kenlm.State object at 0x7fc7362fa0f0>
Debug: states: <kenlm.State object at 0x7fc7362fa370>
Debug: states: <kenlm.State object at 0x7fc7362fa470>
Debug: states: <kenlm.State object at 0x7fc7362fafb0>
Debug: states: <kenlm.State object at 0x7fc7362fae70>
Debug: states: <kenlm.State object at 0x7fc7362fadf0>
Debug: states: <kenlm.State object at 0x7fc7362fa470>
Debug: states: <kenlm.State object at 0x7fc7362fa530>
Debug: states: <kenlm.State object at 0x7fc7362faf70>
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:06:06,241 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:06:06,380 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:06:06,383 (encoder:145) INFO: encoder self-attention layer type = self-attention
Loading the LM will be faster if you build a binary file.
Reading /data4/tanghaoyu/espnet_dev/test/beam_search_test.arpa
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
2020-06-20 16:06:06,392 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:06,392 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:06,512 (beam_search:422) INFO: adding <eos> in the last position in the loop
2020-06-20 16:06:06,513 (beam_search:369) INFO: no hypothesis. Finish decoding.
2020-06-20 16:06:06,514 (beam_search:389) INFO: total log probability: -2.1223080404301036
2020-06-20 16:06:06,514 (beam_search:390) INFO: normalized log probability: -1.0611540202150518
2020-06-20 16:06:06,514 (beam_search:353) INFO: max output length: 9
2020-06-20 16:06:06,514 (beam_search:354) INFO: min output length: 0
2020-06-20 16:06:06,576 (beam_search:389) INFO: total log probability: -1.8761152275111659
2020-06-20 16:06:06,576 (beam_search:390) INFO: normalized log probability: -0.9380576137555829
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:422 adding <eos> in the last position in the loop
INFO     root:beam_search.py:369 no hypothesis. Finish decoding.
INFO     root:beam_search.py:389 total log probability: -2.1223080404301036
INFO     root:beam_search.py:390 normalized log probability: -1.0611540202150518
INFO     root:beam_search.py:353 max output length: 9
INFO     root:beam_search.py:354 min output length: 0
INFO     root:beam_search.py:389 total log probability: -1.8761152275111659
INFO     root:beam_search.py:390 normalized log probability: -0.9380576137555829
___ test_beam_search_equal[transformer-args864-0.0-0.0-0.0-0.0-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.0)
mtlalpha = 0.0, ctc_weight = 0.0, lm_weight = 0.0, bonus = 0.0, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=16, out_features=16, bias=True)
  (linear_weight): Linear(in_features=16, out_features=62, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-1.3467e+00,  3.3228e-01,  2.7539e-01,  1.3682e+00, -2.7759e-01,
           1.2451e+00, -1.2383e+00,  1.3984...01,  2.1387e-01,  6.0974e-02,  1.3584e+00, -5.8252e-01,
           1.8008e+00]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-1.3467e+00,  3.3228e-01,  2.7539e-01,  1.3682e+00, -2.7759e-01,
           1.2451e+00, -1.2383e+00,  1.3984...01,  2.1387e-01,  6.0974e-02,  1.3584e+00, -5.8252e-01,
           1.8008e+00]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-1.3467e+00,  3.3228e-01,  2.7539e-01,  1.3682e+00, -2.7759e-01,
           1.2451e+00, -1.2383e+00,  1.3984...01,  2.1387e-01,  6.0974e-02,  1.3584e+00, -5.8252e-01,
           1.8008e+00]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic Convolution'.
    
        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv.py:97: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:01,769 (encoder:203) INFO: encoder self-attention layer type = dynamic convolution
2020-06-20 16:08:01,884 (decoder:181) INFO: decoder self-attention layer type = dynamic convolution
2020-06-20 16:08:01,965 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:01,965 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:01,966 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:02,018 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:02,018 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:02,018 (e2e_asr_transformer:604) INFO: total log probability: -2.052580952644348
2020-06-20 16:08:02,018 (e2e_asr_transformer:607) INFO: normalized log probability: -0.6841936508814493
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:203 encoder self-attention layer type = dynamic convolution
INFO     root:decoder.py:181 decoder self-attention layer type = dynamic convolution
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -2.052580952644348
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.6841936508814493
___ test_beam_search_equal[transformer-args867-0.0-0.0-0.0-0.1-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.1)
mtlalpha = 0.0, ctc_weight = 0.0, lm_weight = 0.0, bonus = 0.1, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=16, out_features=16, bias=True)
  (linear_weight): Linear(in_features=16, out_features=62, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-1.3467e+00,  3.3228e-01,  2.7539e-01,  1.3682e+00, -2.7759e-01,
           1.2451e+00, -1.2383e+00,  1.3984...01,  2.1387e-01,  6.0974e-02,  1.3584e+00, -5.8252e-01,
           1.8008e+00]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-1.3467e+00,  3.3228e-01,  2.7539e-01,  1.3682e+00, -2.7759e-01,
           1.2451e+00, -1.2383e+00,  1.3984...01,  2.1387e-01,  6.0974e-02,  1.3584e+00, -5.8252e-01,
           1.8008e+00]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-1.3467e+00,  3.3228e-01,  2.7539e-01,  1.3682e+00, -2.7759e-01,
           1.2451e+00, -1.2383e+00,  1.3984...01,  2.1387e-01,  6.0974e-02,  1.3584e+00, -5.8252e-01,
           1.8008e+00]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic Convolution'.
    
        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv.py:97: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:02,968 (encoder:203) INFO: encoder self-attention layer type = dynamic convolution
2020-06-20 16:08:03,116 (decoder:181) INFO: decoder self-attention layer type = dynamic convolution
2020-06-20 16:08:03,169 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:03,169 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:03,169 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:03,221 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:03,221 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:03,221 (e2e_asr_transformer:604) INFO: total log probability: -1.8525809526443482
2020-06-20 16:08:03,221 (e2e_asr_transformer:607) INFO: normalized log probability: -0.6175269842147827
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:203 encoder self-attention layer type = dynamic convolution
INFO     root:decoder.py:181 decoder self-attention layer type = dynamic convolution
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -1.8525809526443482
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.6175269842147827
___ test_beam_search_equal[transformer-args870-0.0-0.0-0.5-0.0-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.0)
mtlalpha = 0.0, ctc_weight = 0.0, lm_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=16, out_features=16, bias=True)
  (linear_weight): Linear(in_features=16, out_features=62, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-1.3467e+00,  3.3228e-01,  2.7539e-01,  1.3682e+00, -2.7759e-01,
           1.2451e+00, -1.2383e+00,  1.3984...01,  2.1387e-01,  6.0974e-02,  1.3584e+00, -5.8252e-01,
           1.8008e+00]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-1.3467e+00,  3.3228e-01,  2.7539e-01,  1.3682e+00, -2.7759e-01,
           1.2451e+00, -1.2383e+00,  1.3984...01,  2.1387e-01,  6.0974e-02,  1.3584e+00, -5.8252e-01,
           1.8008e+00]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-1.3467e+00,  3.3228e-01,  2.7539e-01,  1.3682e+00, -2.7759e-01,
           1.2451e+00, -1.2383e+00,  1.3984...01,  2.1387e-01,  6.0974e-02,  1.3584e+00, -5.8252e-01,
           1.8008e+00]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic Convolution'.
    
        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv.py:97: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:04,096 (encoder:203) INFO: encoder self-attention layer type = dynamic convolution
2020-06-20 16:08:04,208 (decoder:181) INFO: decoder self-attention layer type = dynamic convolution
2020-06-20 16:08:04,297 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:04,297 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:04,297 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:04,348 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:04,348 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:04,348 (e2e_asr_transformer:604) INFO: total log probability: -3.6828460693359375
2020-06-20 16:08:04,348 (e2e_asr_transformer:607) INFO: normalized log probability: -1.2276153564453125
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:203 encoder self-attention layer type = dynamic convolution
INFO     root:decoder.py:181 decoder self-attention layer type = dynamic convolution
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -3.6828460693359375
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -1.2276153564453125
___ test_beam_search_equal[transformer-args873-0.0-0.0-0.5-0.1-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.1)
mtlalpha = 0.0, ctc_weight = 0.0, lm_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=16, out_features=16, bias=True)
  (linear_weight): Linear(in_features=16, out_features=62, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-1.3467e+00,  3.3228e-01,  2.7539e-01,  1.3682e+00, -2.7759e-01,
           1.2451e+00, -1.2383e+00,  1.3984...01,  2.1387e-01,  6.0974e-02,  1.3584e+00, -5.8252e-01,
           1.8008e+00]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-1.3467e+00,  3.3228e-01,  2.7539e-01,  1.3682e+00, -2.7759e-01,
           1.2451e+00, -1.2383e+00,  1.3984...01,  2.1387e-01,  6.0974e-02,  1.3584e+00, -5.8252e-01,
           1.8008e+00]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-1.3467e+00,  3.3228e-01,  2.7539e-01,  1.3682e+00, -2.7759e-01,
           1.2451e+00, -1.2383e+00,  1.3984...01,  2.1387e-01,  6.0974e-02,  1.3584e+00, -5.8252e-01,
           1.8008e+00]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic Convolution'.
    
        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv.py:97: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:05,349 (encoder:203) INFO: encoder self-attention layer type = dynamic convolution
2020-06-20 16:08:05,472 (decoder:181) INFO: decoder self-attention layer type = dynamic convolution
2020-06-20 16:08:05,537 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:05,538 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:05,538 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:05,589 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:05,589 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:05,589 (e2e_asr_transformer:604) INFO: total log probability: -3.4828460693359373
2020-06-20 16:08:05,589 (e2e_asr_transformer:607) INFO: normalized log probability: -1.1609486897786458
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:203 encoder self-attention layer type = dynamic convolution
INFO     root:decoder.py:181 decoder self-attention layer type = dynamic convolution
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -3.4828460693359373
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -1.1609486897786458
___ test_beam_search_equal[transformer-args900-0.5-0.0-0.0-0.0-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.0)
mtlalpha = 0.5, ctc_weight = 0.0, lm_weight = 0.0, bonus = 0.0, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=16, out_features=16, bias=True)
  (linear_weight): Linear(in_features=16, out_features=62, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic Convolution'.
    
        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv.py:97: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:09,847 (encoder:203) INFO: encoder self-attention layer type = dynamic convolution
2020-06-20 16:08:09,872 (decoder:181) INFO: decoder self-attention layer type = dynamic convolution
2020-06-20 16:08:09,880 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:09,880 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:09,880 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:09,944 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:09,944 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:09,944 (e2e_asr_transformer:604) INFO: total log probability: -2.047322392463684
2020-06-20 16:08:09,944 (e2e_asr_transformer:607) INFO: normalized log probability: -0.6824407974878947
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:203 encoder self-attention layer type = dynamic convolution
INFO     root:decoder.py:181 decoder self-attention layer type = dynamic convolution
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -2.047322392463684
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.6824407974878947
___ test_beam_search_equal[transformer-args903-0.5-0.0-0.0-0.1-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.1)
mtlalpha = 0.5, ctc_weight = 0.0, lm_weight = 0.0, bonus = 0.1, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=16, out_features=16, bias=True)
  (linear_weight): Linear(in_features=16, out_features=62, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic Convolution'.
    
        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv.py:97: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:11,080 (encoder:203) INFO: encoder self-attention layer type = dynamic convolution
2020-06-20 16:08:11,164 (decoder:181) INFO: decoder self-attention layer type = dynamic convolution
2020-06-20 16:08:11,199 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:11,200 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:11,200 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:11,269 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:11,269 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:11,269 (e2e_asr_transformer:604) INFO: total log probability: -1.8473223924636841
2020-06-20 16:08:11,269 (e2e_asr_transformer:607) INFO: normalized log probability: -0.615774130821228
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:203 encoder self-attention layer type = dynamic convolution
INFO     root:decoder.py:181 decoder self-attention layer type = dynamic convolution
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -1.8473223924636841
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.615774130821228
___ test_beam_search_equal[transformer-args906-0.5-0.0-0.5-0.0-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.0)
mtlalpha = 0.5, ctc_weight = 0.0, lm_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=16, out_features=16, bias=True)
  (linear_weight): Linear(in_features=16, out_features=62, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic Convolution'.
    
        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv.py:97: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:12,236 (encoder:203) INFO: encoder self-attention layer type = dynamic convolution
2020-06-20 16:08:12,333 (decoder:181) INFO: decoder self-attention layer type = dynamic convolution
2020-06-20 16:08:12,358 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:12,358 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:12,358 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:12,423 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:12,423 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:12,423 (e2e_asr_transformer:604) INFO: total log probability: -3.688187837600708
2020-06-20 16:08:12,423 (e2e_asr_transformer:607) INFO: normalized log probability: -1.2293959458669026
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:203 encoder self-attention layer type = dynamic convolution
INFO     root:decoder.py:181 decoder self-attention layer type = dynamic convolution
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -3.688187837600708
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -1.2293959458669026
___ test_beam_search_equal[transformer-args909-0.5-0.0-0.5-0.1-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.1)
mtlalpha = 0.5, ctc_weight = 0.0, lm_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=16, out_features=16, bias=True)
  (linear_weight): Linear(in_features=16, out_features=62, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic Convolution'.
    
        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv.py:97: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:13,231 (encoder:203) INFO: encoder self-attention layer type = dynamic convolution
2020-06-20 16:08:13,353 (decoder:181) INFO: decoder self-attention layer type = dynamic convolution
2020-06-20 16:08:13,465 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:13,465 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:13,465 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:13,520 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:13,520 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:13,520 (e2e_asr_transformer:604) INFO: total log probability: -3.488187837600708
2020-06-20 16:08:13,520 (e2e_asr_transformer:607) INFO: normalized log probability: -1.162729279200236
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:203 encoder self-attention layer type = dynamic convolution
INFO     root:decoder.py:181 decoder self-attention layer type = dynamic convolution
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -3.488187837600708
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -1.162729279200236
___ test_beam_search_equal[transformer-args912-0.5-0.5-0.0-0.0-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.5, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.0)
mtlalpha = 0.5, ctc_weight = 0.5, lm_weight = 0.0, bonus = 0.0, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=16, out_features=16, bias=True)
  (linear_weight): Linear(in_features=16, out_features=62, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic Convolution'.
    
        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv.py:97: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:14,432 (encoder:203) INFO: encoder self-attention layer type = dynamic convolution
2020-06-20 16:08:14,519 (decoder:181) INFO: decoder self-attention layer type = dynamic convolution
2020-06-20 16:08:14,566 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:14,566 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:14,566 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:14,629 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:14,629 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:14,629 (e2e_asr_transformer:604) INFO: total log probability: -8.638528943061829
2020-06-20 16:08:14,629 (e2e_asr_transformer:607) INFO: normalized log probability: -1.0798161178827286
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:203 encoder self-attention layer type = dynamic convolution
INFO     root:decoder.py:181 decoder self-attention layer type = dynamic convolution
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -8.638528943061829
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -1.0798161178827286
___ test_beam_search_equal[transformer-args915-0.5-0.5-0.0-0.1-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.5, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.1)
mtlalpha = 0.5, ctc_weight = 0.5, lm_weight = 0.0, bonus = 0.1, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=16, out_features=16, bias=True)
  (linear_weight): Linear(in_features=16, out_features=62, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic Convolution'.
    
        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv.py:97: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:15,560 (encoder:203) INFO: encoder self-attention layer type = dynamic convolution
2020-06-20 16:08:15,704 (decoder:181) INFO: decoder self-attention layer type = dynamic convolution
2020-06-20 16:08:15,733 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:15,733 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:15,733 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:15,788 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:15,788 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:15,788 (e2e_asr_transformer:604) INFO: total log probability: -7.938528943061828
2020-06-20 16:08:15,788 (e2e_asr_transformer:607) INFO: normalized log probability: -0.9923161178827286
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:203 encoder self-attention layer type = dynamic convolution
INFO     root:decoder.py:181 decoder self-attention layer type = dynamic convolution
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -7.938528943061828
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.9923161178827286
___ test_beam_search_equal[transformer-args918-0.5-0.5-0.5-0.0-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.5, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.0)
mtlalpha = 0.5, ctc_weight = 0.5, lm_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=16, out_features=16, bias=True)
  (linear_weight): Linear(in_features=16, out_features=62, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic Convolution'.
    
        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv.py:97: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:16,664 (encoder:203) INFO: encoder self-attention layer type = dynamic convolution
2020-06-20 16:08:16,768 (decoder:181) INFO: decoder self-attention layer type = dynamic convolution
2020-06-20 16:08:16,855 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:16,855 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:16,855 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:16,918 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:16,918 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:16,918 (e2e_asr_transformer:604) INFO: total log probability: -14.291749238967896
2020-06-20 16:08:16,918 (e2e_asr_transformer:607) INFO: normalized log probability: -1.786468654870987
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:203 encoder self-attention layer type = dynamic convolution
INFO     root:decoder.py:181 decoder self-attention layer type = dynamic convolution
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -14.291749238967896
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -1.786468654870987
___ test_beam_search_equal[transformer-args921-0.5-0.5-0.5-0.1-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.5, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.1)
mtlalpha = 0.5, ctc_weight = 0.5, lm_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=16, out_features=16, bias=True)
  (linear_weight): Linear(in_features=16, out_features=62, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.7568,  0.7549, -0.8901,  0.9536, -1.0039,  0.2876, -0.7192,
           1.7383, -1.0449,  0.3286, -0.6904,...8398, -0.2208, -0.8218,  0.2438, -0.3499,  1.0889,
          -0.9736,  1.5010]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic Convolution'.
    
        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv.py:97: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:17,816 (encoder:203) INFO: encoder self-attention layer type = dynamic convolution
2020-06-20 16:08:17,900 (decoder:181) INFO: decoder self-attention layer type = dynamic convolution
2020-06-20 16:08:17,935 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:17,935 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:17,935 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:17,990 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:17,990 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:17,990 (e2e_asr_transformer:604) INFO: total log probability: -13.591749238967896
2020-06-20 16:08:17,990 (e2e_asr_transformer:607) INFO: normalized log probability: -1.698968654870987
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:203 encoder self-attention layer type = dynamic convolution
INFO     root:decoder.py:181 decoder self-attention layer type = dynamic convolution
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -13.591749238967896
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -1.698968654870987
__ test_beam_search_equal[transformer-args1080-0.0-0.0-0.0-0.0-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.0)
mtlalpha = 0.0, ctc_weight = 0.0, lm_weight = 0.0, bonus = 0.0, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution2D(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=..._features=62, bias=True)
  (linear_weight_f): Linear(in_features=16, out_features=31, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-1.1650,  1.0039, -0.8794,  1.1416, -0.4888,  0.6484, -0.9814,
           1.7656, -1.2100, -0.5312, -0.5112,...1843, -0.4644, -1.2188, -0.3052, -0.1107,  0.9180,
          -0.4224,  1.7861]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-1.1650,  1.0039, -0.8794,  1.1416, -0.4888,  0.6484, -0.9814,
           1.7656, -1.2100, -0.5312, -0.5112,...1843, -0.4644, -1.2188, -0.3052, -0.1107,  0.9180,
          -0.4224,  1.7861]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-1.1650,  1.0039, -0.8794,  1.1416, -0.4888,  0.6484, -0.9814,
           1.7656, -1.2100, -0.5312, -0.5112,...1843, -0.4644, -1.2188, -0.3052, -0.1107,  0.9180,
          -0.4224,  1.7861]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic 2-Dimentional Convolution'.
    
        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # convolution of frequency axis
        weight_f = self.linear_weight_f(x).view(B * T, 1, k)  # B x T x k
        self.attn_f = weight_f.view(B, T, k).unsqueeze(1)
        xf = F.conv1d(
            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:109: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:47,708 (encoder:224) INFO: encoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:08:47,861 (decoder:206) INFO: decoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:08:47,919 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:47,919 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:47,919 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:47,993 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:47,993 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:47,993 (e2e_asr_transformer:604) INFO: total log probability: -0.9695298671722412
2020-06-20 16:08:47,993 (e2e_asr_transformer:607) INFO: normalized log probability: -0.4847649335861206
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:224 encoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:decoder.py:206 decoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -0.9695298671722412
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.4847649335861206
__ test_beam_search_equal[transformer-args1083-0.0-0.0-0.0-0.1-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.1)
mtlalpha = 0.0, ctc_weight = 0.0, lm_weight = 0.0, bonus = 0.1, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution2D(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=..._features=62, bias=True)
  (linear_weight_f): Linear(in_features=16, out_features=31, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-1.1650,  1.0039, -0.8794,  1.1416, -0.4888,  0.6484, -0.9814,
           1.7656, -1.2100, -0.5312, -0.5112,...1843, -0.4644, -1.2188, -0.3052, -0.1107,  0.9180,
          -0.4224,  1.7861]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-1.1650,  1.0039, -0.8794,  1.1416, -0.4888,  0.6484, -0.9814,
           1.7656, -1.2100, -0.5312, -0.5112,...1843, -0.4644, -1.2188, -0.3052, -0.1107,  0.9180,
          -0.4224,  1.7861]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-1.1650,  1.0039, -0.8794,  1.1416, -0.4888,  0.6484, -0.9814,
           1.7656, -1.2100, -0.5312, -0.5112,...1843, -0.4644, -1.2188, -0.3052, -0.1107,  0.9180,
          -0.4224,  1.7861]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic 2-Dimentional Convolution'.
    
        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # convolution of frequency axis
        weight_f = self.linear_weight_f(x).view(B * T, 1, k)  # B x T x k
        self.attn_f = weight_f.view(B, T, k).unsqueeze(1)
        xf = F.conv1d(
            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:109: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:49,152 (encoder:224) INFO: encoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:08:49,253 (decoder:206) INFO: decoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:08:49,314 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:49,314 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:49,314 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:49,385 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:49,385 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:49,385 (e2e_asr_transformer:604) INFO: total log probability: -0.8695298671722412
2020-06-20 16:08:49,385 (e2e_asr_transformer:607) INFO: normalized log probability: -0.4347649335861206
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:224 encoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:decoder.py:206 decoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -0.8695298671722412
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.4347649335861206
__ test_beam_search_equal[transformer-args1086-0.0-0.0-0.5-0.0-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.0)
mtlalpha = 0.0, ctc_weight = 0.0, lm_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution2D(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=..._features=62, bias=True)
  (linear_weight_f): Linear(in_features=16, out_features=31, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-1.1650,  1.0039, -0.8794,  1.1416, -0.4888,  0.6484, -0.9814,
           1.7656, -1.2100, -0.5312, -0.5112,...1843, -0.4644, -1.2188, -0.3052, -0.1107,  0.9180,
          -0.4224,  1.7861]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-1.1650,  1.0039, -0.8794,  1.1416, -0.4888,  0.6484, -0.9814,
           1.7656, -1.2100, -0.5312, -0.5112,...1843, -0.4644, -1.2188, -0.3052, -0.1107,  0.9180,
          -0.4224,  1.7861]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-1.1650,  1.0039, -0.8794,  1.1416, -0.4888,  0.6484, -0.9814,
           1.7656, -1.2100, -0.5312, -0.5112,...1843, -0.4644, -1.2188, -0.3052, -0.1107,  0.9180,
          -0.4224,  1.7861]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic 2-Dimentional Convolution'.
    
        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # convolution of frequency axis
        weight_f = self.linear_weight_f(x).view(B * T, 1, k)  # B x T x k
        self.attn_f = weight_f.view(B, T, k).unsqueeze(1)
        xf = F.conv1d(
            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:109: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:50,301 (encoder:224) INFO: encoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:08:50,416 (decoder:206) INFO: decoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:08:50,520 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:50,520 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:50,520 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:50,587 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:50,587 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:50,587 (e2e_asr_transformer:604) INFO: total log probability: -1.7375686168670654
2020-06-20 16:08:50,587 (e2e_asr_transformer:607) INFO: normalized log probability: -0.8687843084335327
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:224 encoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:decoder.py:206 decoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -1.7375686168670654
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.8687843084335327
__ test_beam_search_equal[transformer-args1089-0.0-0.0-0.5-0.1-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.1)
mtlalpha = 0.0, ctc_weight = 0.0, lm_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution2D(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=..._features=62, bias=True)
  (linear_weight_f): Linear(in_features=16, out_features=31, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-1.1650,  1.0039, -0.8794,  1.1416, -0.4888,  0.6484, -0.9814,
           1.7656, -1.2100, -0.5312, -0.5112,...1843, -0.4644, -1.2188, -0.3052, -0.1107,  0.9180,
          -0.4224,  1.7861]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-1.1650,  1.0039, -0.8794,  1.1416, -0.4888,  0.6484, -0.9814,
           1.7656, -1.2100, -0.5312, -0.5112,...1843, -0.4644, -1.2188, -0.3052, -0.1107,  0.9180,
          -0.4224,  1.7861]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-1.1650,  1.0039, -0.8794,  1.1416, -0.4888,  0.6484, -0.9814,
           1.7656, -1.2100, -0.5312, -0.5112,...1843, -0.4644, -1.2188, -0.3052, -0.1107,  0.9180,
          -0.4224,  1.7861]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic 2-Dimentional Convolution'.
    
        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # convolution of frequency axis
        weight_f = self.linear_weight_f(x).view(B * T, 1, k)  # B x T x k
        self.attn_f = weight_f.view(B, T, k).unsqueeze(1)
        xf = F.conv1d(
            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:109: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:51,463 (encoder:224) INFO: encoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:08:51,577 (decoder:206) INFO: decoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:08:51,629 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:51,629 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:51,629 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:51,700 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:51,700 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:51,700 (e2e_asr_transformer:604) INFO: total log probability: -1.6375686168670653
2020-06-20 16:08:51,700 (e2e_asr_transformer:607) INFO: normalized log probability: -0.8187843084335327
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:224 encoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:decoder.py:206 decoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -1.6375686168670653
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.8187843084335327
__ test_beam_search_equal[transformer-args1116-0.5-0.0-0.0-0.0-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.0)
mtlalpha = 0.5, ctc_weight = 0.0, lm_weight = 0.0, bonus = 0.0, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution2D(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=..._features=62, bias=True)
  (linear_weight_f): Linear(in_features=16, out_features=31, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic 2-Dimentional Convolution'.
    
        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # convolution of frequency axis
        weight_f = self.linear_weight_f(x).view(B * T, 1, k)  # B x T x k
        self.attn_f = weight_f.view(B, T, k).unsqueeze(1)
        xf = F.conv1d(
            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:109: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:57,837 (encoder:224) INFO: encoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:08:57,948 (decoder:206) INFO: decoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:08:57,978 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:57,978 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:57,979 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:58,050 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:58,050 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:58,050 (e2e_asr_transformer:604) INFO: total log probability: -0.9765853881835938
2020-06-20 16:08:58,050 (e2e_asr_transformer:607) INFO: normalized log probability: -0.4882926940917969
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:224 encoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:decoder.py:206 decoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -0.9765853881835938
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.4882926940917969
__ test_beam_search_equal[transformer-args1119-0.5-0.0-0.0-0.1-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.1)
mtlalpha = 0.5, ctc_weight = 0.0, lm_weight = 0.0, bonus = 0.1, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution2D(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=..._features=62, bias=True)
  (linear_weight_f): Linear(in_features=16, out_features=31, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic 2-Dimentional Convolution'.
    
        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # convolution of frequency axis
        weight_f = self.linear_weight_f(x).view(B * T, 1, k)  # B x T x k
        self.attn_f = weight_f.view(B, T, k).unsqueeze(1)
        xf = F.conv1d(
            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:109: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:08:59,012 (encoder:224) INFO: encoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:08:59,137 (decoder:206) INFO: decoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:08:59,193 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:08:59,193 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:08:59,193 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:08:59,263 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:08:59,263 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:08:59,263 (e2e_asr_transformer:604) INFO: total log probability: -0.8765853881835938
2020-06-20 16:08:59,263 (e2e_asr_transformer:607) INFO: normalized log probability: -0.4382926940917969
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:224 encoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:decoder.py:206 decoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -0.8765853881835938
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.4382926940917969
__ test_beam_search_equal[transformer-args1122-0.5-0.0-0.5-0.0-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.0)
mtlalpha = 0.5, ctc_weight = 0.0, lm_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution2D(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=..._features=62, bias=True)
  (linear_weight_f): Linear(in_features=16, out_features=31, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic 2-Dimentional Convolution'.
    
        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # convolution of frequency axis
        weight_f = self.linear_weight_f(x).view(B * T, 1, k)  # B x T x k
        self.attn_f = weight_f.view(B, T, k).unsqueeze(1)
        xf = F.conv1d(
            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:109: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:09:00,348 (encoder:224) INFO: encoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:09:00,481 (decoder:206) INFO: decoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:09:00,543 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:09:00,543 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:09:00,543 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:09:00,619 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:09:00,620 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:09:00,620 (e2e_asr_transformer:604) INFO: total log probability: -1.8053234815597534
2020-06-20 16:09:00,620 (e2e_asr_transformer:607) INFO: normalized log probability: -0.9026617407798767
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:224 encoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:decoder.py:206 decoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -1.8053234815597534
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.9026617407798767
__ test_beam_search_equal[transformer-args1125-0.5-0.0-0.5-0.1-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.0, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.1)
mtlalpha = 0.5, ctc_weight = 0.0, lm_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution2D(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=..._features=62, bias=True)
  (linear_weight_f): Linear(in_features=16, out_features=31, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic 2-Dimentional Convolution'.
    
        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # convolution of frequency axis
        weight_f = self.linear_weight_f(x).view(B * T, 1, k)  # B x T x k
        self.attn_f = weight_f.view(B, T, k).unsqueeze(1)
        xf = F.conv1d(
            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:109: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:09:01,724 (encoder:224) INFO: encoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:09:01,786 (decoder:206) INFO: decoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:09:01,807 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:09:01,808 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:09:01,808 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:09:01,876 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:09:01,877 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:09:01,877 (e2e_asr_transformer:604) INFO: total log probability: -1.7053234815597533
2020-06-20 16:09:01,877 (e2e_asr_transformer:607) INFO: normalized log probability: -0.8526617407798767
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:224 encoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:decoder.py:206 decoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -1.7053234815597533
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.8526617407798767
__ test_beam_search_equal[transformer-args1128-0.5-0.5-0.0-0.0-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.5, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.0)
mtlalpha = 0.5, ctc_weight = 0.5, lm_weight = 0.0, bonus = 0.0, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution2D(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=..._features=62, bias=True)
  (linear_weight_f): Linear(in_features=16, out_features=31, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic 2-Dimentional Convolution'.
    
        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # convolution of frequency axis
        weight_f = self.linear_weight_f(x).view(B * T, 1, k)  # B x T x k
        self.attn_f = weight_f.view(B, T, k).unsqueeze(1)
        xf = F.conv1d(
            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:109: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:09:02,881 (encoder:224) INFO: encoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:09:02,968 (decoder:206) INFO: decoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:09:02,977 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:09:02,977 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:09:02,977 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:09:03,044 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:09:03,045 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:09:03,045 (e2e_asr_transformer:604) INFO: total log probability: -6.061701536178589
2020-06-20 16:09:03,045 (e2e_asr_transformer:607) INFO: normalized log probability: -1.0102835893630981
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:224 encoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:decoder.py:206 decoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -6.061701536178589
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -1.0102835893630981
__ test_beam_search_equal[transformer-args1131-0.5-0.5-0.0-0.1-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.5, lm_weight=0.0, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.1)
mtlalpha = 0.5, ctc_weight = 0.5, lm_weight = 0.0, bonus = 0.1, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution2D(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=..._features=62, bias=True)
  (linear_weight_f): Linear(in_features=16, out_features=31, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic 2-Dimentional Convolution'.
    
        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # convolution of frequency axis
        weight_f = self.linear_weight_f(x).view(B * T, 1, k)  # B x T x k
        self.attn_f = weight_f.view(B, T, k).unsqueeze(1)
        xf = F.conv1d(
            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:109: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:09:03,951 (encoder:224) INFO: encoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:09:04,063 (decoder:206) INFO: decoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:09:04,098 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:09:04,098 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:09:04,098 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:09:04,168 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:09:04,168 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:09:04,168 (e2e_asr_transformer:604) INFO: total log probability: -5.561701536178589
2020-06-20 16:09:04,168 (e2e_asr_transformer:607) INFO: normalized log probability: -0.9269502560297648
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:224 encoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:decoder.py:206 decoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -5.561701536178589
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -0.9269502560297648
__ test_beam_search_equal[transformer-args1134-0.5-0.5-0.5-0.0-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.5, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.0)
mtlalpha = 0.5, ctc_weight = 0.5, lm_weight = 0.5, bonus = 0.0, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution2D(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=..._features=62, bias=True)
  (linear_weight_f): Linear(in_features=16, out_features=31, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic 2-Dimentional Convolution'.
    
        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # convolution of frequency axis
        weight_f = self.linear_weight_f(x).view(B * T, 1, k)  # B x T x k
        self.attn_f = weight_f.view(B, T, k).unsqueeze(1)
        xf = F.conv1d(
            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:109: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:09:05,184 (encoder:224) INFO: encoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:09:05,316 (decoder:206) INFO: decoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:09:05,344 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:09:05,345 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:09:05,345 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:09:05,444 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:09:05,444 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:09:05,444 (e2e_asr_transformer:604) INFO: total log probability: -10.049347758293152
2020-06-20 16:09:05,444 (e2e_asr_transformer:607) INFO: normalized log probability: -1.6748912930488586
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:224 encoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:decoder.py:206 decoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -10.049347758293152
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -1.6748912930488586
__ test_beam_search_equal[transformer-args1137-0.5-0.5-0.5-0.1-cuda-float16] ___

model_class = 'transformer'
args = Namespace(beam_size=3, ctc_weight=0.5, lm_weight=0.5, maxlenratio=0, minlenratio=0, nbest=5, penalty=0.1)
mtlalpha = 0.5, ctc_weight = 0.5, lm_weight = 0.5, bonus = 0.1, device = 'cuda'
dtype = torch.float16

    @pytest.mark.parametrize(
        "model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype",
        [
            (nn, args, ctc_train, ctc_recog, lm, bonus, device, dtype)
            for device in ("cpu", "cuda")
            for nn, args in (
                ("transformer", transformer_args),
                ("transformer", ldconv_lconv_args),
                ("transformer", ldconv_dconv_args),
                ("transformer", ldconv_lconv2d_args),
                ("transformer", ldconv_dconv2d_args),
                ("rnn", rnn_args),
            )
            for ctc_train in (0.0, 0.5, 1.0)
            for ctc_recog in (0.0, 0.5, 1.0)
            for lm in (0.0, 0.5)
            for bonus in (0.0, 0.1)
            for dtype in ("float16", "float32", "float64")
        ],
    )
    def test_beam_search_equal(
        model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
    ):
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("no cuda device is available")
        if device == "cpu" and dtype == "float16":
            pytest.skip("cpu float16 implementation is not available in pytorch yet")
    
        # seed setting
        torch.manual_seed(123)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = (
            False  # https://github.com/pytorch/pytorch/issues/6351
        )
    
        dtype = getattr(torch, dtype)
        model, x, ilens, y, data, train_args = prepare(model_class, args, mtlalpha=mtlalpha)
        model.eval()
        char_list = train_args.char_list
        lm_args = Namespace(type="lstm", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)
        lm = dynamic_import_lm("default", backend="pytorch")(len(char_list), lm_args)
        lm.eval()
    
        if mtlalpha == 0.0 and ctc_weight > 0.0:
            pytest.skip("no CTC + CTC decoding.")
        if mtlalpha == 1.0 and ctc_weight < 1.0:
            pytest.skip("pure CTC + attention decoding")
    
        # TODO(hirofumi0810): pure CTC beam search is not implemented
        if ctc_weight == 1.0:
            pytest.skip("pure CTC beam search is not implemented")
    
        # test previous beam search
        args = Namespace(
            beam_size=3,
            penalty=bonus,
            ctc_weight=ctc_weight,
            maxlenratio=0,
            lm_weight=lm_weight,
            minlenratio=0,
            nbest=5,
        )
    
        feat = x[0, : ilens[0]].numpy()
        # legacy beam search
        with torch.no_grad():
            nbest = model.recognize(feat, args, char_list, lm.model)
    
        # new beam search
        scorers = model.scorers()
        if lm_weight != 0:
            scorers["lm"] = lm
        scorers["length_bonus"] = LengthBonus(len(char_list))
        weights = dict(
            decoder=1.0 - ctc_weight,
            ctc=ctc_weight,
            lm=args.lm_weight,
            length_bonus=args.penalty,
        )
        model.to(device, dtype=dtype)
        model.eval()
        beam = BeamSearch(
            beam_size=args.beam_size,
            vocab_size=len(char_list),
            weights=weights,
            scorers=scorers,
            token_list=train_args.char_list,
            sos=model.sos,
            eos=model.eos,
            pre_beam_score_key=None if ctc_weight == 1.0 else "decoder",
        )
        beam.to(device, dtype=dtype)
        beam.eval()
        with torch.no_grad():
>           enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))

test/test_beam_search.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
espnet/nets/pytorch_backend/e2e_asr_transformer.py:388: in encode
    enc_output, _ = self.encoder(x, None)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder.py:259: in forward
    xs, masks = self.encoders(xs, masks)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/repeat.py:18: in forward
    args = m(*args)
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
espnet/nets/pytorch_backend/transformer/encoder_layer.py:80: in forward
    x = residual + self.dropout(self.self_attn(x_q, x, x, mask))
tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:489: in __call__
    result = self.forward(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DynamicConvolution2D(
  (linear1): Linear(in_features=16, out_features=32, bias=True)
  (linear2): Linear(in_features=..._features=62, bias=True)
  (linear_weight_f): Linear(in_features=16, out_features=31, bias=True)
  (act): GLU(dim=-1)
)
query = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
key = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
value = tensor([[[-0.3306,  0.3809,  0.0094,  1.5225, -1.3027,  0.9702, -1.9443,
           1.3535, -0.6836, -0.1672, -1.1553,...6602, -0.5791, -0.4395,  0.0897, -0.7358,  0.3079,
          -0.8604,  2.1152]]], device='cuda:0', dtype=torch.float16)
mask = None

    def forward(self, query, key, value, mask):
        """Forward of 'Dynamic 2-Dimentional Convolution'.
    
        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)
    
        Args:
            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
            mask (torch.Tensor): (batch, time1, time2) mask
    
        Return:
            x (torch.Tensor): (batch, time1, d_model) ouput
    
        """
        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        x = query
        B, T, C = x.size()
        H = self.wshare
        k = self.kernel_size
    
        # first liner layer
        x = self.linear1(x)
    
        # GLU activation
        x = self.act(x)
    
        # convolution of frequency axis
        weight_f = self.linear_weight_f(x).view(B * T, 1, k)  # B x T x k
        self.attn_f = weight_f.view(B, T, k).unsqueeze(1)
        xf = F.conv1d(
            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)
    
        # get kernel of convolution
        weight = self.linear_weight(x)  # B x T x kH
        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
>       weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
E       RuntimeError: _th_zero_ is not implemented for type torch.HalfTensor

espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:109: RuntimeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:09:06,404 (encoder:224) INFO: encoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:09:06,501 (decoder:206) INFO: decoder self-attention layer type = dynamic convolution 2-dimentional
2020-06-20 16:09:06,562 (e2e_asr_transformer:425) INFO: input lengths: 9
2020-06-20 16:09:06,563 (e2e_asr_transformer:441) INFO: max output length: 9
2020-06-20 16:09:06,563 (e2e_asr_transformer:442) INFO: min output length: 0
2020-06-20 16:09:06,638 (e2e_asr_transformer:548) INFO: adding <eos> in the last postion in the loop
2020-06-20 16:09:06,638 (e2e_asr_transformer:578) INFO: no hypothesis. Finish decoding.
2020-06-20 16:09:06,638 (e2e_asr_transformer:604) INFO: total log probability: -9.549347758293152
2020-06-20 16:09:06,638 (e2e_asr_transformer:607) INFO: normalized log probability: -1.5915579597155254
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:224 encoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:decoder.py:206 decoder self-attention layer type = dynamic convolution 2-dimentional
INFO     root:e2e_asr_transformer.py:425 input lengths: 9
INFO     root:e2e_asr_transformer.py:441 max output length: 9
INFO     root:e2e_asr_transformer.py:442 min output length: 0
INFO     root:e2e_asr_transformer.py:548 adding <eos> in the last postion in the loop
INFO     root:e2e_asr_transformer.py:578 no hypothesis. Finish decoding.
INFO     root:e2e_asr_transformer.py:604 total log probability: -9.549347758293152
INFO     root:e2e_asr_transformer.py:607 normalized log probability: -1.5915579597155254
__________________________ test_transformer_parallel ___________________________

    def test_transformer_parallel():
        if not torch.cuda.is_available():
            return
    
        args = make_arg()
        model, x, ilens, y, data = prepare("pytorch", args)
        model = torch.nn.DataParallel(model).cuda()
        logging.debug(ilens)
        # test acc is almost 100%
        optim = torch.optim.Adam(model.parameters(), 0.02)
        max_acc = 0.0
        for i in range(40):
            loss = model(x, torch.as_tensor(ilens), y)
            optim.zero_grad()
>           acc = float(model.module.acc)

test/test_e2e_asr_transformer.py:371: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = E2E(
  (encoder): Encoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 16, kernel...ar(in_features=16, out_features=5, bias=True)
  )
  (criterion): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
)
name = 'acc'

    def __getattr__(self, name):
        if '_parameters' in self.__dict__:
            _parameters = self.__dict__['_parameters']
            if name in _parameters:
                return _parameters[name]
        if '_buffers' in self.__dict__:
            _buffers = self.__dict__['_buffers']
            if name in _buffers:
                return _buffers[name]
        if '_modules' in self.__dict__:
            modules = self.__dict__['_modules']
            if name in modules:
                return modules[name]
        raise AttributeError("'{}' object has no attribute '{}'".format(
>           type(self).__name__, name))
E       AttributeError: 'E2E' object has no attribute 'acc'

tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py:535: AttributeError
----------------------------- Captured stderr call -----------------------------
2020-06-20 16:12:54,099 (encoder:145) INFO: encoder self-attention layer type = self-attention
2020-06-20 16:12:54,102 (decoder:112) INFO: decoder self-attention layer type = self-attention
2020-06-20 16:12:54,127 (e2e_asr:54) INFO: mtl loss:1.9440726041793823
2020-06-20 16:12:54,127 (e2e_asr:54) INFO: mtl loss:1.6202538013458252
2020-06-20 16:12:54,127 (e2e_asr:54) INFO: mtl loss:1.7200827598571777
------------------------------ Captured log call -------------------------------
INFO     root:encoder.py:145 encoder self-attention layer type = self-attention
INFO     root:decoder.py:112 decoder self-attention layer type = self-attention
INFO     root:e2e_asr.py:54 mtl loss:1.9440726041793823
INFO     root:e2e_asr.py:54 mtl loss:1.6202538013458252
INFO     root:e2e_asr.py:54 mtl loss:1.7200827598571777
=============================== warnings summary ===============================
tools/venv/lib/python3.7/site-packages/google/protobuf/descriptor.py:47
  /data4/tanghaoyu/espnet_dev/tools/venv/lib/python3.7/site-packages/google/protobuf/descriptor.py:47: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
    from google.protobuf.pyext import _message

tools/venv/lib/python3.7/site-packages/google/protobuf/internal/well_known_types.py:788
  /data4/tanghaoyu/espnet_dev/tools/venv/lib/python3.7/site-packages/google/protobuf/internal/well_known_types.py:788: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
    collections.MutableMapping.register(Struct)

tools/venv/lib/python3.7/site-packages/librosa/util/decorators.py:9
  /data4/tanghaoyu/espnet_dev/tools/venv/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.
  Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.
    from numba.decorators import jit as optional_jit

tools/venv/lib/python3.7/site-packages/librosa/util/decorators.py:9
  /data4/tanghaoyu/espnet_dev/tools/venv/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.
  Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.
    from numba.decorators import jit as optional_jit

test/test_beam_search.py: 180 tests with warnings
test/test_e2e_asr_transformer.py: 1 test with warning
  /data4/tanghaoyu/espnet_dev/espnet/nets/pytorch_backend/transformer/dynamic_conv.py:55: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
    nn.init.xavier_uniform(self.linear_weight.weight)

test/test_beam_search.py: 180 tests with warnings
test/test_e2e_asr_transformer.py: 1 test with warning
  /data4/tanghaoyu/espnet_dev/espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:57: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
    nn.init.xavier_uniform(self.linear_weight.weight)

test/test_beam_search.py: 180 tests with warnings
test/test_e2e_asr_transformer.py: 1 test with warning
  /data4/tanghaoyu/espnet_dev/espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py:59: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
    nn.init.xavier_uniform(self.linear_weight_f.weight)

test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict17]
test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict17]
  /data4/tanghaoyu/espnet_dev/tools/venv/lib/python3.7/site-packages/chainer/link.py:201: DeprecationWarning: Link._device_id is left only for backward compatibility and likely to be removed. Use Link.device instead.
    DeprecationWarning)

test/test_e2e_asr.py: 1 test with warning
test/test_e2e_asr_mulenc.py: 2 tests with warnings
test/test_e2e_asr_transformer.py: 1 test with warning
test/test_e2e_mt.py: 1 test with warning
test/test_e2e_st.py: 1 test with warning
test/test_e2e_tts_fastspeech.py: 16 tests with warnings
test/test_e2e_tts_tacotron2.py: 9 tests with warnings
test/test_e2e_tts_transformer.py: 13 tests with warnings
  /data4/tanghaoyu/espnet_dev/tools/venv/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
    warnings.warn('Was asked to gather along dimension 0, but all '

test/test_e2e_asr_sa_transducer.py: 18 tests with warnings
test/test_e2e_asr_transformer.py: 11 tests with warnings
test/test_e2e_mt_transformer.py: 5 tests with warnings
test/test_e2e_st_transformer.py: 11 tests with warnings
  /data4/tanghaoyu/espnet_dev/espnet/nets/pytorch_backend/transformer/plot.py:44: UserWarning: tight_layout: falling back to Agg renderer
    fig.tight_layout()

test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict4]
  /data4/tanghaoyu/espnet_dev/tools/venv/lib/python3.7/site-packages/matplotlib/image.py:452: UserWarning: Warning: converting a masked element to nan.
    dv = np.float64(self.norm.vmax) - np.float64(self.norm.vmin)

test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict4]
  /data4/tanghaoyu/espnet_dev/tools/venv/lib/python3.7/site-packages/matplotlib/image.py:459: UserWarning: Warning: converting a masked element to nan.
    a_min = np.float64(newmin)

test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict4]
  /data4/tanghaoyu/espnet_dev/tools/venv/lib/python3.7/site-packages/matplotlib/image.py:464: UserWarning: Warning: converting a masked element to nan.
    a_max = np.float64(newmax)

test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict4]
  <string>:6: UserWarning: Warning: converting a masked element to nan.

test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict4]
  /data4/tanghaoyu/espnet_dev/tools/venv/lib/python3.7/site-packages/numpy/core/_asarray.py:85: UserWarning: Warning: converting a masked element to nan.
    return array(a, dtype, copy=False, order=order)

test/test_multi_spkrs.py: 1 test with warning
test/espnet2/asr/frontend/test_frontend.py: 11 tests with warnings
test/espnet2/layers/test_log_mel.py: 4 tests with warnings
test/espnet2/tts/feats_extract/test_log_mel_fbank.py: 5 tests with warnings
  /data4/tanghaoyu/espnet_dev/tools/venv/lib/python3.7/site-packages/librosa/filters.py:196: FutureWarning: norm=1 behavior will change in librosa 0.8.0. To maintain forward compatibility, use norm='slaney' instead.
    FutureWarning)

test/espnet2/fileio/test_datadir_writer.py::test_DatadirWriter
  /data4/tanghaoyu/espnet_dev/espnet2/fileio/datadir_writer.py:52: UserWarning: Duplicated: ccccc
    warnings.warn(f"Duplicated: {key}")

test/espnet2/fileio/test_datadir_writer.py::test_DatadirWriter
  /data4/tanghaoyu/espnet_dev/espnet2/fileio/datadir_writer.py:71: UserWarning: Ids are mismatching between /tmp/pytest-of-tanghaoyu/pytest-19/test_DatadirWriter0/aa and /tmp/pytest-of-tanghaoyu/pytest-19/test_DatadirWriter0/aa2
    f"Ids are mismatching between "

test/espnet2/torch_utils/test_device_funcs.py::test_force_gatherable[obj6]
  /data4/tanghaoyu/espnet_dev/espnet2/torch_utils/device_funcs.py:70: UserWarning: <class 'test.espnet2.torch_utils.test_device_funcs.Data'> may not be gatherable by DataParallel
    warnings.warn(f"{type(data)} may not be gatherable by DataParallel")

test/espnet2/train/test_reporter.py::test_matplotlib_plot
test/espnet2/train/test_reporter.py::test_tensorboard_add_scalar
  /data4/tanghaoyu/espnet_dev/espnet2/train/reporter.py:264: UserWarning: The stats of the previous epoch=2doesn't exist.
    f"The stats of the previous epoch={self.epoch - 1}"

test/espnet2/train/test_reporter.py::test_zero_weight
  /data4/tanghaoyu/espnet_dev/espnet2/train/reporter.py:85: UserWarning: weight is zero
    warnings.warn("weight is zero")

test/espnet2/train/test_reporter.py::test_register_nan
  /data4/tanghaoyu/espnet_dev/espnet2/train/reporter.py:90: UserWarning: No valid stats found
    warnings.warn("No valid stats found")

test/espnet2/train/test_reporter.py::test_start_middle_epoch
test/espnet2/train/test_reporter.py::test_measure_time
test/espnet2/train/test_reporter.py::test_measure_iter_time
  /data4/tanghaoyu/espnet_dev/espnet2/train/reporter.py:264: UserWarning: The stats of the previous epoch=1doesn't exist.
    f"The stats of the previous epoch={self.epoch - 1}"

test/espnet2/train/test_reporter.py::test_aggregate
  /data4/tanghaoyu/espnet_dev/espnet2/train/reporter.py:66: UserWarning: No stats found
    warnings.warn("No stats found")

test/espnet2/tts/feats_extract/test_log_mel_fbank.py::test_forward
test/espnet2/tts/feats_extract/test_log_mel_fbank.py::test_backward_leaf_in
test/espnet2/tts/feats_extract/test_log_mel_fbank.py::test_backward_not_leaf_in
test/espnet2/tts/feats_extract/test_log_mel_fbank.py::test_output_size
test/espnet2/tts/feats_extract/test_log_mel_fbank.py::test_get_parameters
  /data4/tanghaoyu/espnet_dev/tools/venv/lib/python3.7/site-packages/librosa/filters.py:235: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.
    warnings.warn('Empty filters detected in mel frequency basis. '

-- Docs: https://docs.pytest.org/en/latest/warnings.html

----------- coverage: platform linux, python 3.7.6-final-0 -----------
Name                                                                   Stmts   Miss  Cover
------------------------------------------------------------------------------------------
espnet/__init__.py                                                         6      2    67%
espnet/asr/__init__.py                                                     0      0   100%
espnet/asr/asr_mix_utils.py                                               86     86     0%
espnet/asr/asr_utils.py                                                  340    211    38%
espnet/asr/chainer_backend/__init__.py                                     0      0   100%
espnet/asr/chainer_backend/asr.py                                        209    209     0%
espnet/asr/pytorch_backend/__init__.py                                     0      0   100%
espnet/asr/pytorch_backend/asr.py                                        536    536     0%
espnet/asr/pytorch_backend/asr_init.py                                   117     34    71%
espnet/asr/pytorch_backend/asr_mix.py                                    246    246     0%
espnet/asr/pytorch_backend/recog.py                                       74     74     0%
espnet/bin/__init__.py                                                     0      0   100%
espnet/bin/asr_enhance.py                                                 61     61     0%
espnet/bin/asr_recog.py                                                  102    102     0%
espnet/bin/asr_train.py                                                  173     61    65%
espnet/bin/lm_train.py                                                    95     51    46%
espnet/bin/mt_train.py                                                   136    136     0%
espnet/bin/mt_trans.py                                                    64     64     0%
espnet/bin/st_train.py                                                   150    150     0%
espnet/bin/st_trans.py                                                    69     69     0%
espnet/bin/tts_decode.py                                                  61     61     0%
espnet/bin/tts_train.py                                                   93     93     0%
espnet/lm/__init__.py                                                      0      0   100%
espnet/lm/chainer_backend/__init__.py                                      0      0   100%
espnet/lm/chainer_backend/extlm.py                                       135    135     0%
espnet/lm/chainer_backend/lm.py                                          250    164    34%
espnet/lm/lm_utils.py                                                    155    116    25%
espnet/lm/pytorch_backend/__init__.py                                      0      0   100%
espnet/lm/pytorch_backend/extlm.py                                       143     68    52%
espnet/lm/pytorch_backend/lm.py                                          201    201     0%
espnet/mt/__init__.py                                                      0      0   100%
espnet/mt/mt_utils.py                                                     35     35     0%
espnet/mt/pytorch_backend/__init__.py                                      0      0   100%
espnet/mt/pytorch_backend/mt.py                                          232    232     0%
espnet/nets/__init__.py                                                    0      0   100%
espnet/nets/asr_interface.py                                              29      3    90%
espnet/nets/batch_beam_search.py                                          78      7    91%
espnet/nets/beam_search.py                                               161      5    97%
espnet/nets/chainer_backend/__init__.py                                    0      0   100%
espnet/nets/chainer_backend/asr_interface.py                               6      0   100%
espnet/nets/chainer_backend/ctc.py                                        64      2    97%
espnet/nets/chainer_backend/deterministic_embed_id.py                     88     22    75%
espnet/nets/chainer_backend/e2e_asr.py                                    94     11    88%
espnet/nets/chainer_backend/e2e_asr_transformer.py                       298     95    68%
espnet/nets/chainer_backend/nets_utils.py                                  5      0   100%
espnet/nets/chainer_backend/rnn/__init__.py                                0      0   100%
espnet/nets/chainer_backend/rnn/attentions.py                            105      0   100%
espnet/nets/chainer_backend/rnn/decoders.py                              246     19    92%
espnet/nets/chainer_backend/rnn/encoders.py                              117      2    98%
espnet/nets/chainer_backend/rnn/training.py                              107    107     0%
espnet/nets/chainer_backend/transformer/__init__.py                        0      0   100%
espnet/nets/chainer_backend/transformer/attention.py                      38      0   100%
espnet/nets/chainer_backend/transformer/ctc.py                            54     39    28%
espnet/nets/chainer_backend/transformer/decoder.py                        49      2    96%
espnet/nets/chainer_backend/transformer/decoder_layer.py                  28      0   100%
espnet/nets/chainer_backend/transformer/embedding.py                      18      0   100%
espnet/nets/chainer_backend/transformer/encoder.py                        52      9    83%
espnet/nets/chainer_backend/transformer/encoder_layer.py                  24      0   100%
espnet/nets/chainer_backend/transformer/label_smoothing_loss.py           35      2    94%
espnet/nets/chainer_backend/transformer/layer_norm.py                      7      0   100%
espnet/nets/chainer_backend/transformer/mask.py                            6      0   100%
espnet/nets/chainer_backend/transformer/plot.py                           56     45    20%
espnet/nets/chainer_backend/transformer/positionwise_feed_forward.py      19      0   100%
espnet/nets/chainer_backend/transformer/subsampling.py                    45     11    76%
espnet/nets/chainer_backend/transformer/training.py                      133    101    24%
espnet/nets/ctc_prefix_score.py                                          153      1    99%
espnet/nets/e2e_asr_common.py                                            185     63    66%
espnet/nets/e2e_mt_common.py                                              34     20    41%
espnet/nets/lm_interface.py                                               22      1    95%
espnet/nets/mt_interface.py                                               21      9    57%
espnet/nets/pytorch_backend/__init__.py                                    0      0   100%
espnet/nets/pytorch_backend/ctc.py                                        71      3    96%
espnet/nets/pytorch_backend/e2e_asr.py                                   261     50    81%
espnet/nets/pytorch_backend/e2e_asr_mix.py                               347    187    46%
espnet/nets/pytorch_backend/e2e_asr_mulenc.py                            306     76    75%
espnet/nets/pytorch_backend/e2e_asr_transducer.py                        168     35    79%
espnet/nets/pytorch_backend/e2e_asr_transformer.py                       265     13    95%
espnet/nets/pytorch_backend/e2e_mt.py                                    177     65    63%
espnet/nets/pytorch_backend/e2e_mt_transformer.py                        222     57    74%
espnet/nets/pytorch_backend/e2e_st.py                                    277     83    70%
espnet/nets/pytorch_backend/e2e_st_transformer.py                        247     42    83%
espnet/nets/pytorch_backend/e2e_tts_fastspeech.py                        259      9    97%
espnet/nets/pytorch_backend/e2e_tts_tacotron2.py                         250      9    96%
espnet/nets/pytorch_backend/e2e_tts_transformer.py                       349     36    90%
espnet/nets/pytorch_backend/fastspeech/__init__.py                         0      0   100%
espnet/nets/pytorch_backend/fastspeech/duration_calculator.py             31      1    97%
espnet/nets/pytorch_backend/fastspeech/duration_predictor.py              35      0   100%
espnet/nets/pytorch_backend/fastspeech/length_regulator.py                21      0   100%
espnet/nets/pytorch_backend/frontends/__init__.py                          0      0   100%
espnet/nets/pytorch_backend/frontends/beamformer.py                       24      0   100%
espnet/nets/pytorch_backend/frontends/dnn_beamformer.py                   72      1    99%
espnet/nets/pytorch_backend/frontends/dnn_wpe.py                          34      1    97%
espnet/nets/pytorch_backend/frontends/feature_transform.py               102     59    42%
espnet/nets/pytorch_backend/frontends/frontend.py                         51      3    94%
espnet/nets/pytorch_backend/frontends/mask_estimator.py                   38      2    95%
espnet/nets/pytorch_backend/initialization.py                             30      2    93%
espnet/nets/pytorch_backend/lm/__init__.py                                 0      0   100%
espnet/nets/pytorch_backend/lm/default.py                                147     12    92%
espnet/nets/pytorch_backend/lm/seq_rnn.py                                 60      8    87%
espnet/nets/pytorch_backend/lm/transformer.py                             63      1    98%
espnet/nets/pytorch_backend/nets_utils.py                                118     18    85%
espnet/nets/pytorch_backend/rnn/__init__.py                                0      0   100%
espnet/nets/pytorch_backend/rnn/attentions.py                            749     14    98%
espnet/nets/pytorch_backend/rnn/decoders.py                              578     39    93%
espnet/nets/pytorch_backend/rnn/encoders.py                              141      3    98%
espnet/nets/pytorch_backend/streaming/__init__.py                          0      0   100%
espnet/nets/pytorch_backend/streaming/segment.py                          62     23    63%
espnet/nets/pytorch_backend/streaming/window.py                           36     11    69%
espnet/nets/pytorch_backend/tacotron2/__init__.py                          0      0   100%
espnet/nets/pytorch_backend/tacotron2/cbhg.py                             85      0   100%
espnet/nets/pytorch_backend/tacotron2/decoder.py                         222      2    99%
espnet/nets/pytorch_backend/tacotron2/encoder.py                          46      1    98%
espnet/nets/pytorch_backend/transducer/__init__.py                         0      0   100%
espnet/nets/pytorch_backend/transducer/initializer.py                     18      4    78%
espnet/nets/pytorch_backend/transducer/loss.py                            12      0   100%
espnet/nets/pytorch_backend/transducer/rnn_decoders.py                   285     10    96%
espnet/nets/pytorch_backend/transducer/transformer_decoder.py            104      9    91%
espnet/nets/pytorch_backend/transducer/transformer_decoder_layer.py       42      5    88%
espnet/nets/pytorch_backend/transducer/utils.py                           20      0   100%
espnet/nets/pytorch_backend/transducer/vgg.py                             21      0   100%
espnet/nets/pytorch_backend/transformer/__init__.py                        0      0   100%
espnet/nets/pytorch_backend/transformer/add_sos_eos.py                    10      0   100%
espnet/nets/pytorch_backend/transformer/attention.py                      36      0   100%
espnet/nets/pytorch_backend/transformer/decoder.py                        91      1    99%
espnet/nets/pytorch_backend/transformer/decoder_layer.py                  57      0   100%
espnet/nets/pytorch_backend/transformer/dynamic_conv.py                   55      2    96%
espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py                 64      2    97%
espnet/nets/pytorch_backend/transformer/embedding.py                      43      1    98%
espnet/nets/pytorch_backend/transformer/encoder.py                        86      2    98%
espnet/nets/pytorch_backend/transformer/encoder_layer.py                  42      0   100%
espnet/nets/pytorch_backend/transformer/initializer.py                    23     17    26%
espnet/nets/pytorch_backend/transformer/label_smoothing_loss.py           28      0   100%
espnet/nets/pytorch_backend/transformer/layer_norm.py                     10      0   100%
espnet/nets/pytorch_backend/transformer/lightconv.py                      45      2    96%
espnet/nets/pytorch_backend/transformer/lightconv2d.py                    51      2    96%
espnet/nets/pytorch_backend/transformer/mask.py                           16      2    88%
espnet/nets/pytorch_backend/transformer/multi_layer_conv.py               20      0   100%
espnet/nets/pytorch_backend/transformer/optimizer.py                      35     35     0%
espnet/nets/pytorch_backend/transformer/plot.py                           84     34    60%
espnet/nets/pytorch_backend/transformer/positionwise_feed_forward.py      10      0   100%
espnet/nets/pytorch_backend/transformer/repeat.py                          9      0   100%
espnet/nets/pytorch_backend/transformer/subsampling.py                    16      0   100%
espnet/nets/pytorch_backend/wavenet.py                                   183    183     0%
espnet/nets/scorer_interface.py                                           16      0   100%
espnet/nets/scorers/__init__.py                                            0      0   100%
espnet/nets/scorers/ctc.py                                                21      0   100%
espnet/nets/scorers/length_bonus.py                                       12      0   100%
espnet/nets/scorers/ngram.py                                              37      1    97%
espnet/nets/st_interface.py                                               10      3    70%
espnet/nets/tts_interface.py                                              22      5    77%
espnet/optimizer/__init__.py                                               0      0   100%
espnet/optimizer/chainer.py                                               38      0   100%
espnet/optimizer/factory.py                                               23      4    83%
espnet/optimizer/parser.py                                                15      0   100%
espnet/optimizer/pytorch.py                                               28      0   100%
espnet/scheduler/__init__.py                                               0      0   100%
espnet/scheduler/chainer.py                                               14      0   100%
espnet/scheduler/pytorch.py                                               14      0   100%
espnet/scheduler/scheduler.py                                             72      1    99%
espnet/st/__init__.py                                                      0      0   100%
espnet/st/pytorch_backend/__init__.py                                      0      0   100%
espnet/st/pytorch_backend/st.py                                          251    251     0%
espnet/transform/__init__.py                                               0      0   100%
espnet/transform/add_deltas.py                                            24      1    96%
espnet/transform/channel_selector.py                                      20      4    80%
espnet/transform/cmvn.py                                                  82     41    50%
espnet/transform/functional.py                                            34      3    91%
espnet/transform/perturb.py                                              178    178     0%
espnet/transform/spec_augment.py                                          91     91     0%
espnet/transform/spectrogram.py                                           93     40    57%
espnet/transform/transform_interface.py                                    9      3    67%
espnet/transform/transformation.py                                        67     22    67%
espnet/transform/wpe.py                                                   13     13     0%
espnet/tts/__init__.py                                                     0      0   100%
espnet/tts/pytorch_backend/__init__.py                                     0      0   100%
espnet/tts/pytorch_backend/tts.py                                        341    341     0%
espnet/utils/__init__.py                                                   0      0   100%
espnet/utils/check_kwargs.py                                              11      2    82%
espnet/utils/cli_readers.py                                              136     26    81%
espnet/utils/cli_utils.py                                                 14      1    93%
espnet/utils/cli_writers.py                                              131     17    87%
espnet/utils/dataset.py                                                   48     48     0%
espnet/utils/deterministic_utils.py                                       25     19    24%
espnet/utils/dynamic_import.py                                             9      1    89%
espnet/utils/fill_missing_args.py                                         13      0   100%
espnet/utils/io_utils.py                                                 261    142    46%
espnet/utils/spec_augment.py                                             183    183     0%
espnet/utils/training/__init__.py                                          0      0   100%
espnet/utils/training/batchfy.py                                         214     66    69%
espnet/utils/training/evaluator.py                                        12      0   100%
espnet/utils/training/iterators.py                                        34     19    44%
espnet/utils/training/tensorboard_logger.py                               22      5    77%
espnet/utils/training/train_utils.py                                      13      9    31%
espnet2/__init__.py                                                        0      0   100%
espnet2/asr/__init__.py                                                    0      0   100%
espnet2/asr/ctc.py                                                        44      1    98%
espnet2/asr/decoder/__init__.py                                            0      0   100%
espnet2/asr/decoder/abs_decoder.py                                         6      0   100%
espnet2/asr/decoder/rnn_decoder.py                                       149     39    74%
espnet2/asr/decoder/transformer_decoder.py                                66      2    97%
espnet2/asr/encoder/__init__.py                                            0      0   100%
espnet2/asr/encoder/abs_encoder.py                                         6      0   100%
espnet2/asr/encoder/rnn_encoder.py                                        42      0   100%
espnet2/asr/encoder/transformer_encoder.py                                55      0   100%
espnet2/asr/encoder/vgg_rnn_encoder.py                                    39      0   100%
espnet2/asr/espnet_model.py                                              105     77    27%
espnet2/asr/frontend/__init__.py                                           0      0   100%
espnet2/asr/frontend/abs_frontend.py                                       5      0   100%
espnet2/asr/frontend/default.py                                           45      0   100%
espnet2/asr/specaug/abs_specaug.py                                         5      0   100%
espnet2/asr/specaug/specaug.py                                            35      1    97%
espnet2/bin/__init__.py                                                    0      0   100%
espnet2/bin/aggregate_stats_dirs.py                                       54     35    35%
espnet2/bin/asr_inference.py                                             133     70    47%
espnet2/bin/asr_train.py                                                   6      0   100%
espnet2/bin/launch.py                                                    139    139     0%
espnet2/bin/lm_calc_perplexity.py                                         95     49    48%
espnet2/bin/lm_train.py                                                    6      0   100%
espnet2/bin/pack.py                                                       37      6    84%
espnet2/bin/split_scps.py                                                 59     59     0%
espnet2/bin/tokenize_text.py                                             114     75    34%
espnet2/bin/tts_inference.py                                             108     51    53%
espnet2/bin/tts_train.py                                                   6      0   100%
espnet2/fileio/__init__.py                                                 0      0   100%
espnet2/fileio/datadir_writer.py                                          49      0   100%
espnet2/fileio/npy_scp.py                                                 48      0   100%
espnet2/fileio/rand_gen_dataset.py                                        34      0   100%
espnet2/fileio/read_text.py                                               47      0   100%
espnet2/fileio/sound_scp.py                                               63      0   100%
espnet2/iterators/__init__.py                                              0      0   100%
espnet2/iterators/abs_iter_factory.py                                      4      0   100%
espnet2/iterators/chunk_iter_factory.py                                   97     23    76%
espnet2/iterators/multiple_iter_factory.py                                24      0   100%
espnet2/iterators/sequence_iter_factory.py                                43      0   100%
espnet2/layers/__init__.py                                                 0      0   100%
espnet2/layers/abs_normalize.py                                            5      0   100%
espnet2/layers/global_mvn.py                                              68      0   100%
espnet2/layers/inversible_interface.py                                     5      0   100%
espnet2/layers/log_mel.py                                                 25      0   100%
espnet2/layers/mask_along_axis.py                                         55      4    93%
espnet2/layers/stft.py                                                    41      0   100%
espnet2/layers/time_warp.py                                               38      2    95%
espnet2/layers/utterance_mvn.py                                           38      0   100%
espnet2/lm/__init__.py                                                     0      0   100%
espnet2/lm/abs_model.py                                                    6      0   100%
espnet2/lm/espnet_model.py                                                39     25    36%
espnet2/lm/seq_rnn.py                                                     48      0   100%
espnet2/main_funcs/__init__.py                                             0      0   100%
espnet2/main_funcs/average_nbest_models.py                                39     31    21%
espnet2/main_funcs/calculate_all_attentions.py                            74      0   100%
espnet2/main_funcs/collect_stats.py                                       67     47    30%
espnet2/optimizers/__init__.py                                             0      0   100%
espnet2/optimizers/sgd.py                                                  6      0   100%
espnet2/samplers/__init__.py                                               0      0   100%
espnet2/samplers/abs_sampler.py                                            6      0   100%
espnet2/samplers/build_batch_sampler.py                                   30      1    97%
espnet2/samplers/folded_batch_sampler.py                                  73     11    85%
espnet2/samplers/length_batch_sampler.py                                  77      9    88%
espnet2/samplers/num_elements_batch_sampler.py                            84     13    85%
espnet2/samplers/sorted_batch_sampler.py                                  41      5    88%
espnet2/samplers/unsorted_batch_sampler.py                                29      2    93%
espnet2/schedulers/__init__.py                                             0      0   100%
espnet2/schedulers/abs_scheduler.py                                       15      2    87%
espnet2/schedulers/noam_lr.py                                             24     12    50%
espnet2/schedulers/warmup_lr.py                                           17      7    59%
espnet2/tasks/__init__.py                                                  0      0   100%
espnet2/tasks/abs_task.py                                                548    274    50%
espnet2/tasks/asr.py                                                     129     52    60%
espnet2/tasks/lm.py                                                       89     29    67%
espnet2/tasks/tts.py                                                     108     43    60%
espnet2/text/__init__.py                                                   0      0   100%
espnet2/text/abs_tokenizer.py                                              5      0   100%
espnet2/text/build_tokenizer.py                                           28     18    36%
espnet2/text/char_tokenizer.py                                            38      5    87%
espnet2/text/cleaner.py                                                   23      5    78%
espnet2/text/phoneme_tokenizer.py                                         54     21    61%
espnet2/text/sentencepiece_tokenizer.py                                   24      0   100%
espnet2/text/token_id_converter.py                                        42      0   100%
espnet2/text/word_tokenizer.py                                            35      7    80%
espnet2/torch_utils/__init__.py                                            0      0   100%
espnet2/torch_utils/add_gradient_noise.py                                  9      0   100%
espnet2/torch_utils/device_funcs.py                                       39      0   100%
espnet2/torch_utils/forward_adaptor.py                                    13      0   100%
espnet2/torch_utils/initialize.py                                         48      0   100%
espnet2/torch_utils/load_pretrained_model.py                              22     15    32%
espnet2/torch_utils/pytorch_version.py                                     6      0   100%
espnet2/torch_utils/recursive_op.py                                       40     32    20%
espnet2/torch_utils/set_all_random_seed.py                                 7      0   100%
espnet2/train/__init__.py                                                  0      0   100%
espnet2/train/abs_espnet_model.py                                          6      0   100%
espnet2/train/class_choices.py                                            41      4    90%
espnet2/train/collate_fn.py                                               42      0   100%
espnet2/train/dataset.py                                                 174     34    80%
espnet2/train/distributed_utils.py                                       176     44    75%
espnet2/train/iterable_dataset.py                                        105     10    90%
espnet2/train/preprocessor.py                                             42     24    43%
espnet2/train/reporter.py                                                309      7    98%
espnet2/train/trainer.py                                                 289    224    22%
espnet2/tts/__init__.py                                                    0      0   100%
espnet2/tts/abs_tts.py                                                     6      0   100%
espnet2/tts/espnet_model.py                                               30     15    50%
espnet2/tts/feats_extract/__init__.py                                      0      0   100%
espnet2/tts/feats_extract/abs_feats_extract.py                             7      0   100%
espnet2/tts/feats_extract/log_mel_fbank.py                                38      0   100%
espnet2/tts/feats_extract/log_spectrogram.py                              27      0   100%
espnet2/tts/tacotron2.py                                                 113     90    20%
espnet2/utils/__init__.py                                                  0      0   100%
espnet2/utils/build_dataclass.py                                          11      0   100%
espnet2/utils/get_default_kwargs.py                                       34      0   100%
espnet2/utils/griffin_lim.py                                              49     34    31%
espnet2/utils/model_summary.py                                            27      0   100%
espnet2/utils/nested_dict_action.py                                       39      0   100%
espnet2/utils/pack_funcs.py                                              105      4    96%
espnet2/utils/sized_dict.py                                               46      5    89%
espnet2/utils/types.py                                                    50      1    98%
espnet2/utils/yaml_no_alias_safe_dump.py                                   6      0   100%
------------------------------------------------------------------------------------------
TOTAL                                                                  22054   8285    62%

============================ slowest test durations ============================
7.40s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_parallel
4.67s call     test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict2]
4.44s call     test/test_recog.py::test_recognition_results[bgrup-gru-espnet.nets.chainer_backend.e2e_asr-4]
4.43s call     test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict10]
4.30s call     test/test_recog.py::test_batch_beam_search[vggbgrup-gru-espnet.nets.pytorch_backend.e2e_asr]
4.02s call     test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict9]
3.96s call     test/test_e2e_asr_transformer.py::test_transformer_trainable_and_decodable[chainer-model_dict10]
3.92s call     test/test_e2e_asr_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict4]
3.81s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args48-0.0-default-lm_args48-0.0-0.0-0.0-cuda-float32]
3.77s call     test/test_recog.py::test_recognition_results_with_lm[bgrup-gru-espnet.nets.chainer_backend.e2e_asr-4]
3.74s call     test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict1]
3.72s call     test/test_recog.py::test_batch_beam_search[vggblstmp-lstm-espnet.nets.pytorch_backend.e2e_asr]
3.71s call     test/test_e2e_asr.py::test_multi_gpu_trainable[espnet.nets.chainer_backend.e2e_asr]
3.70s call     test/test_recog.py::test_batch_beam_search[bgrup-gru-espnet.nets.pytorch_backend.e2e_asr]
3.46s call     test/test_recog.py::test_recognition_results_with_lm[blstmp-lstm-espnet.nets.chainer_backend.e2e_asr-0]
3.45s call     test/test_recog.py::test_batch_beam_search[blstmp-lstm-espnet.nets.pytorch_backend.e2e_asr]
3.35s call     test/test_recog.py::test_recognition_results[blstmp-lstm-espnet.nets.chainer_backend.e2e_asr-0]
3.03s call     test/test_recog.py::test_recognition_results[vggbgrup-gru-espnet.nets.chainer_backend.e2e_asr-6]
3.03s call     test/test_e2e_asr_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict2]
3.01s call     test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict8]
2.71s call     test/test_recog.py::test_recognition_results_with_lm[vggbgrup-gru-espnet.nets.chainer_backend.e2e_asr-6]
2.56s call     test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict7]
2.53s call     test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict5]
2.51s call     test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict0]
2.51s call     test/test_e2e_asr_mulenc.py::test_gradient_noise_injection[pytorch-3]
2.51s call     test/test_e2e_asr_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict6]
2.48s call     test/test_e2e_asr_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict0]
2.41s call     test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict6]
2.36s call     test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict3]
2.36s call     test/test_e2e_mt_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict3]
2.35s call     test/test_e2e_st_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict4]
2.34s call     test/test_recog.py::test_recognition_results_with_lm[vggblstmp-lstm-espnet.nets.chainer_backend.e2e_asr-2]
2.31s call     test/test_e2e_asr_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict5]
2.31s call     test/test_e2e_asr_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict8]
2.28s call     test/test_recog.py::test_recognition_results[vggblstmp-lstm-espnet.nets.chainer_backend.e2e_asr-2]
2.23s call     test/test_e2e_asr_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict7]
2.20s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict80]
2.14s call     test/test_e2e_mt_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict1]
2.14s call     test/test_e2e_mt_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict0]
2.12s call     test/test_e2e_mt_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict4]
2.11s call     test/test_e2e_mt_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict2]
1.93s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic3-recog_dic3]
1.91s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic7-recog_dic7]
1.83s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic15-recog_dic15]
1.77s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic11-recog_dic11]
1.68s call     test/espnet2/utils/test_sized_dict.py::test_SizedDict_shared
1.68s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict79]
1.67s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic6-recog_dic6]
1.66s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic4-recog_dic4]
1.66s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic9-recog_dic9]
1.66s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic1-recog_dic1]
1.65s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic8-recog_dic8]
1.65s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic5-recog_dic5]
1.64s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic2-recog_dic2]
1.61s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic10-recog_dic10]
1.61s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic0-recog_dic0]
1.61s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic17-recog_dic17]
1.60s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic14-recog_dic14]
1.56s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic13-recog_dic13]
1.55s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic16-recog_dic16]
1.55s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_trainable_and_decodable[train_dic12-recog_dic12]
1.44s call     test/test_e2e_asr_mulenc.py::test_sortagrad_trainable[pytorch-3]
1.40s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict89]
1.33s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict53]
1.32s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict27]
1.32s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict74]
1.32s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict64]
1.31s call     test/test_e2e_asr.py::test_window_streaming_e2e_encoder_and_ctc_with_offline_attention
1.31s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict100]
1.30s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict78]
1.29s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict95]
1.27s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict98]
1.21s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict88]
1.21s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict25]
1.20s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict105]
1.20s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict77]
1.19s call     test/test_e2e_asr_mulenc.py::test_sortagrad_trainable_with_batch_frames[pytorch-3]
1.19s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict69]
1.19s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict97]
1.19s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict103]
1.19s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict65]
1.18s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict99]
1.18s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict72]
1.17s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict66]
1.17s call     test/test_e2e_asr_mulenc.py::test_sortagrad_trainable[pytorch-2]
1.16s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict83]
1.16s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict68]
1.15s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict101]
1.15s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict102]
1.15s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict76]
1.14s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict84]
1.13s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict73]
1.13s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict38]
1.13s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict67]
1.12s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict92]
1.10s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict75]
1.10s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict91]
1.10s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict71]
1.09s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict46]
1.09s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict82]
1.09s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict54]
1.09s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict104]
1.08s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict93]
1.07s setup    test/espnet2/text/test_phoneme_tokenizer.py::test_token2text[g2p_en]
1.07s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict96]
1.07s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict94]
1.07s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict85]
1.06s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict90]
1.06s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict62]
1.05s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict61]
1.05s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict87]
1.02s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict63]
1.02s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict81]
1.02s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict18]
1.02s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict11]
1.01s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict39]
1.01s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict26]
1.00s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict58]
0.99s call     test/test_e2e_asr_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict9]
0.99s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict8-inference_dict8]
0.98s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict24]
0.98s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict70]
0.97s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict21]
0.97s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict86]
0.96s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict20]
0.95s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict8]
0.94s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict49]
0.92s call     test/test_e2e_asr_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict3]
0.92s setup    test/espnet2/text/test_phoneme_tokenizer.py::test_repr[g2p_en]
0.92s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict22]
0.91s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict60]
0.91s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict12]
0.90s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict59]
0.90s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict13]
0.90s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict19-inference_dict19]
0.89s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict48]
0.89s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict41]
0.89s setup    test/espnet2/text/test_phoneme_tokenizer.py::test_text2tokens[g2p_en]
0.88s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict32]
0.87s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict45]
0.87s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict9]
0.86s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict43]
0.86s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict44]
0.86s call     test/test_e2e_asr_transformer.py::test_transformer_trainable_and_decodable[pytorch-model_dict1]
0.85s call     test/test_multi_spkrs.py::test_dnn_beamformer[True-True-3-2-espnet.nets.pytorch_backend.e2e_asr_mix]
0.85s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict42]
0.85s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict17]
0.84s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict52]
0.84s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict50]
0.84s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict36]
0.84s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict23]
0.84s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict16]
0.83s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict33]
0.83s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict10]
0.82s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict57]
0.82s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict40]
0.82s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict35]
0.81s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict55]
0.81s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-model_dict56]
0.81s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict0]
0.81s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict4]
0.80s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict15]
0.80s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict37]
0.80s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict1]
0.80s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict51]
0.79s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict34]
0.78s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict14]
0.78s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict47]
0.76s call     test/espnet2/train/test_distributed_utils.py::test_init_cpu
0.75s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict29]
0.75s call     test/test_e2e_asr_mulenc.py::test_sortagrad_trainable_with_batch_frames[pytorch-2]
0.75s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict19]
0.74s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict14-inference_dict14]
0.74s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict6]
0.74s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict28]
0.72s call     test/test_e2e_asr.py::test_segment_streaming_e2e
0.72s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict31]
0.71s call     test/test_e2e_asr.py::test_sortagrad_trainable[chainer]
0.70s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict30]
0.69s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict7]
0.68s call     test/espnet2/train/test_distributed_utils.py::test_init_cpu4
0.66s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict5]
0.66s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict2]
0.64s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict9-inference_dict9]
0.64s call     test/test_e2e_asr_mulenc.py::test_gradient_noise_injection[pytorch-2]
0.63s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict20-inference_dict20]
0.63s call     test/espnet2/train/test_distributed_utils.py::test_init_cpu2
0.62s call     test/test_e2e_asr.py::test_loss_and_ctc_grad[vggblstmp]
0.62s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict21-inference_dict21]
0.62s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict26-inference_dict26]
0.61s call     test/test_recog.py::test_recognition_results_with_lm[vggblstmp-lstm-espnet.nets.pytorch_backend.e2e_asr-3]
0.60s call     test/espnet2/train/test_distributed_utils.py::test_init_cpu5
0.60s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict5-inference_dict5]
0.60s call     test/test_e2e_asr_mulenc.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-model_dict3]
0.58s call     test/test_recog.py::test_recognition_results_with_lm[vggbgrup-gru-espnet.nets.pytorch_backend.e2e_asr-7]
0.57s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict2-inference_dict2]
0.57s call     test/test_e2e_asr_mulenc.py::test_sortagrad_trainable_with_batch_bins[pytorch-2]
0.55s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args93-0.0-transformer-lm_args93-0.5-0.5-0.0-cuda-float64]
0.55s call     test/test_recog.py::test_recognition_results[vggbgrup-gru-espnet.nets.pytorch_backend.e2e_asr-7]
0.54s call     test/test_recog.py::test_recognition_results[vggblstmp-lstm-espnet.nets.pytorch_backend.e2e_asr-3]
0.53s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict3-inference_dict3]
0.53s call     test/test_recog.py::test_recognition_results_with_lm[blstmp-lstm-espnet.nets.pytorch_backend.e2e_asr-1]
0.53s call     test/test_recog.py::test_recognition_results_with_lm[bgrup-gru-espnet.nets.pytorch_backend.e2e_asr-5]
0.52s call     test/test_e2e_asr.py::test_sortagrad_trainable_with_batch_frames[chainer]
0.51s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict13-inference_dict13]
0.51s call     test/test_e2e_asr.py::test_loss_and_ctc_grad[blstmp]
0.51s call     test/test_initialization.py::test_lecun_init_chainer
0.51s call     test/test_beam_search.py::test_beam_search_equal[transformer-args901-0.5-0.0-0.0-0.0-cuda-float32]
0.50s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args87-0.0-transformer-lm_args87-0.0-0.5-0.1-cuda-float64]
0.50s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1133-0.5-0.5-0.0-0.1-cuda-float64]
0.50s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1025-0.5-0.5-0.0-0.1-cuda-float64]
0.49s call     test/test_e2e_asr.py::test_sortagrad_trainable_with_batch_bins[chainer]
0.49s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1135-0.5-0.5-0.5-0.0-cuda-float32]
0.49s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[None-enc.-None-dec., att.-0.0]
0.48s call     test/test_beam_search.py::test_beam_search_equal[transformer-args221-0.0-0.0-0.0-0.1-cpu-float64]
0.48s call     test/test_beam_search.py::test_beam_search_equal[transformer-args904-0.5-0.0-0.0-0.1-cuda-float32]
0.48s call     test/test_e2e_asr_mulenc.py::test_sortagrad_trainable_with_batch_bins[pytorch-3]
0.48s call     test/test_e2e_st.py::test_sortagrad_trainable_with_batch_frames[pytorch]
0.48s call     test/test_beam_search.py::test_beam_search_equal[transformer-args689-0.5-0.0-0.0-0.1-cuda-float64]
0.48s call     test/test_e2e_asr.py::test_sortagrad_trainable_with_batch_frames[pytorch]
0.47s call     test/test_beam_search.py::test_beam_search_equal[transformer-args656-0.0-0.0-0.5-0.0-cuda-float64]
0.47s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1124-0.5-0.0-0.5-0.0-cuda-float64]
0.47s call     test/test_beam_search.py::test_beam_search_equal[transformer-args902-0.5-0.0-0.0-0.0-cuda-float64]
0.47s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args60-0.0-default-lm_args60-0.5-0.5-0.0-cuda-float32]
0.47s call     test/test_recog.py::test_recognition_results[blstmp-lstm-espnet.nets.pytorch_backend.e2e_asr-1]
0.47s call     test/test_recog.py::test_recognition_results[bgrup-gru-espnet.nets.pytorch_backend.e2e_asr-5]
0.47s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1138-0.5-0.5-0.5-0.1-cuda-float32]
0.47s call     test/test_beam_search.py::test_beam_search_equal[transformer-args687-0.5-0.0-0.0-0.1-cuda-float16]
0.47s call     test/test_beam_search.py::test_beam_search_equal[transformer-args433-0.0-0.0-0.0-0.0-cpu-float32]
0.47s call     test/test_beam_search.py::test_beam_search_equal[transformer-args806-0.5-0.5-0.0-0.0-cuda-float64]
0.47s call     test/test_beam_search.py::test_beam_search_equal[transformer-args813-0.5-0.5-0.5-0.1-cuda-float16]
0.46s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1081-0.0-0.0-0.0-0.0-cuda-float32]
0.46s call     test/test_beam_search.py::test_beam_search_equal[transformer-args705-0.5-0.5-0.5-0.1-cuda-float16]
0.46s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1123-0.5-0.0-0.5-0.0-cuda-float32]
0.46s call     test/test_beam_search.py::test_beam_search_equal[transformer-args699-0.5-0.5-0.0-0.1-cuda-float16]
0.46s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1084-0.0-0.0-0.0-0.1-cuda-float32]
0.46s call     test/test_beam_search.py::test_beam_search_equal[transformer-args871-0.0-0.0-0.5-0.0-cuda-float32]
0.46s call     test/test_beam_search.py::test_beam_search_equal[transformer-args865-0.0-0.0-0.0-0.0-cuda-float32]
0.46s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1008-0.5-0.0-0.0-0.0-cuda-float16]
0.46s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict1]
0.46s call     test/test_beam_search.py::test_beam_search_equal[transformer-args707-0.5-0.5-0.5-0.1-cuda-float64]
0.46s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args90-0.0-transformer-lm_args90-0.5-0.0-0.1-cuda-float32]
0.45s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1028-0.5-0.5-0.5-0.0-cuda-float64]
0.45s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1120-0.5-0.0-0.0-0.1-cuda-float32]
0.45s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args61-0.0-default-lm_args61-0.5-0.5-0.0-cuda-float64]
0.45s call     test/test_beam_search.py::test_beam_search_equal[transformer-args157-0.5-0.5-0.0-0.0-cpu-float32]
0.45s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict4-inference_dict4]
0.45s call     test/test_beam_search.py::test_beam_search_equal[transformer-args700-0.5-0.5-0.0-0.1-cuda-float32]
0.44s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1127-0.5-0.0-0.5-0.1-cuda-float64]
0.44s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args95-0.0-transformer-lm_args95-0.5-0.5-0.1-cuda-float64]
0.44s call     test/test_beam_search.py::test_beam_search_equal[transformer-args50-0.5-0.5-0.0-0.0-cpu-float64]
0.44s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict25-inference_dict25]
0.44s call     test/test_beam_search.py::test_beam_search_equal[transformer-args794-0.5-0.0-0.0-0.0-cuda-float64]
0.44s call     test/test_beam_search.py::test_beam_search_equal[transformer-args218-0.0-0.0-0.0-0.0-cpu-float64]
0.44s call     test/test_beam_search.py::test_beam_search_equal[transformer-args434-0.0-0.0-0.0-0.0-cpu-float64]
0.44s call     test/test_beam_search.py::test_beam_search_equal[transformer-args653-0.0-0.0-0.0-0.1-cuda-float64]
0.44s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict2]
0.44s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1118-0.5-0.0-0.0-0.0-cuda-float64]
0.44s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args1-0.0-default-lm_args1-0.0-0.0-0.0-cpu-float64]
0.44s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1139-0.5-0.5-0.5-0.1-cuda-float64]
0.44s call     test/test_beam_search.py::test_beam_search_equal[transformer-args814-0.5-0.5-0.5-0.1-cuda-float32]
0.43s call     test/test_beam_search.py::test_beam_search_equal[transformer-args56-0.5-0.5-0.5-0.0-cpu-float64]
0.43s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args80-0.0-transformer-lm_args80-0.0-0.0-0.0-cuda-float32]
0.43s call     test/test_beam_search.py::test_beam_search_equal[transformer-args982-0.0-0.0-0.5-0.1-cuda-float32]
0.43s call     test/test_beam_search.py::test_beam_search_equal[transformer-args923-0.5-0.5-0.5-0.1-cuda-float64]
0.43s call     test/test_beam_search.py::test_beam_search_equal[transformer-args116-0.0-0.0-0.5-0.0-cpu-float64]
0.43s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1126-0.5-0.0-0.5-0.1-cuda-float32]
0.43s call     test/test_beam_search.py::test_beam_search_equal[transformer-args217-0.0-0.0-0.0-0.0-cpu-float32]
0.43s call     test/test_beam_search.py::test_beam_search_equal[transformer-args374-0.5-0.5-0.0-0.0-cpu-float64]
0.43s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1016-0.5-0.0-0.5-0.0-cuda-float64]
0.42s call     test/test_beam_search.py::test_beam_search_equal[transformer-args695-0.5-0.0-0.5-0.1-cuda-float64]
0.42s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args74-0.0-default-lm_args74-0.5-0.0-0.1-cuda-float32]
0.42s call     test/test_beam_search.py::test_beam_search_equal[transformer-args684-0.5-0.0-0.0-0.0-cuda-float16]
0.42s call     test/test_beam_search.py::test_beam_search_equal[transformer-args654-0.0-0.0-0.5-0.0-cuda-float16]
0.42s call     test/test_beam_search.py::test_beam_search_equal[transformer-args440-0.0-0.0-0.5-0.0-cpu-float64]
0.42s call     test/test_beam_search.py::test_beam_search_equal[transformer-args809-0.5-0.5-0.0-0.1-cuda-float64]
0.42s call     test/test_beam_search.py::test_beam_search_equal[transformer-args763-0.0-0.0-0.5-0.0-cuda-float32]
0.42s call     test/test_beam_search.py::test_beam_search_equal[transformer-args2-0.0-0.0-0.0-0.0-cpu-float64]
0.42s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict25]
0.42s call     test/test_beam_search.py::test_beam_search_equal[transformer-args765-0.0-0.0-0.5-0.1-cuda-float16]
0.42s call     test/test_beam_search.py::test_beam_search_equal[transformer-args810-0.5-0.5-0.5-0.0-cuda-float16]
0.42s call     test/test_beam_search.py::test_beam_search_equal[transformer-args263-0.5-0.0-0.5-0.1-cpu-float64]
0.42s call     test/test_beam_search.py::test_beam_search_equal[transformer-args373-0.5-0.5-0.0-0.0-cpu-float32]
0.42s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args52-0.0-default-lm_args52-0.0-0.5-0.0-cuda-float32]
0.42s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1023-0.5-0.5-0.0-0.1-cuda-float16]
0.41s call     test/test_beam_search.py::test_beam_search_equal[transformer-args362-0.5-0.0-0.0-0.0-cpu-float64]
0.41s call     test/test_beam_search.py::test_beam_search_equal[transformer-args383-0.5-0.5-0.5-0.1-cpu-float64]
0.41s call     test/test_beam_search.py::test_beam_search_equal[transformer-args490-0.5-0.5-0.5-0.1-cpu-float32]
0.41s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args91-0.0-transformer-lm_args91-0.5-0.0-0.1-cuda-float64]
0.41s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args56-0.0-default-lm_args56-0.5-0.0-0.0-cuda-float32]
0.41s call     test/test_beam_search.py::test_beam_search_equal[transformer-args693-0.5-0.0-0.5-0.1-cuda-float16]
0.41s call     test/test_beam_search.py::test_beam_search_equal[transformer-args922-0.5-0.5-0.5-0.1-cuda-float32]
0.41s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict1-inference_dict1]
0.41s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1082-0.0-0.0-0.0-0.0-cuda-float64]
0.41s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args77-0.0-default-lm_args77-0.5-0.5-0.0-cuda-float64]
0.41s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict17]
0.41s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args46-0.0-transformer-lm_args46-0.5-0.5-0.1-cpu-float32]
0.40s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args53-0.0-default-lm_args53-0.0-0.5-0.0-cuda-float64]
0.40s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1090-0.0-0.0-0.5-0.1-cuda-float32]
0.40s call     test/test_beam_search.py::test_beam_search_equal[transformer-args972-0.0-0.0-0.0-0.0-cuda-float16]
0.40s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1026-0.5-0.5-0.5-0.0-cuda-float16]
0.40s call     test/test_beam_search.py::test_beam_search_equal[transformer-args692-0.5-0.0-0.5-0.0-cuda-float64]
0.40s call     test/test_beam_search.py::test_beam_search_equal[transformer-args811-0.5-0.5-0.5-0.0-cuda-float32]
0.40s call     test/test_beam_search.py::test_beam_search_equal[transformer-args706-0.5-0.5-0.5-0.1-cuda-float32]
0.40s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1088-0.0-0.0-0.5-0.0-cuda-float64]
0.40s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict24-inference_dict24]
0.40s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args81-0.0-transformer-lm_args81-0.0-0.0-0.0-cuda-float64]
0.40s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args31-0.0-default-lm_args31-0.5-0.5-0.1-cpu-float64]
0.40s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args54-0.0-default-lm_args54-0.0-0.5-0.1-cuda-float32]
0.40s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args30-0.0-default-lm_args30-0.5-0.5-0.1-cpu-float32]
0.40s call     test/test_beam_search.py::test_beam_search_equal[transformer-args696-0.5-0.5-0.0-0.0-cuda-float16]
0.39s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict0-inference_dict0]
0.39s call     test/test_beam_search.py::test_beam_search_equal[transformer-args669-0.0-0.5-0.5-0.1-cuda-float16]
0.39s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict6]
0.39s call     test/test_beam_search.py::test_beam_search_equal[transformer-args981-0.0-0.0-0.5-0.1-cuda-float16]
0.39s call     test/test_beam_search.py::test_beam_search_equal[transformer-args443-0.0-0.0-0.5-0.1-cpu-float64]
0.39s call     test/test_beam_search.py::test_beam_search_equal[transformer-args657-0.0-0.0-0.5-0.1-cuda-float16]
0.39s call     test/test_beam_search.py::test_beam_search_equal[transformer-args804-0.5-0.5-0.0-0.0-cuda-float16]
0.39s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args92-0.0-transformer-lm_args92-0.5-0.5-0.0-cuda-float32]
0.39s call     test/test_beam_search.py::test_beam_search_equal[transformer-args797-0.5-0.0-0.0-0.1-cuda-float64]
0.39s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict7-inference_dict7]
0.39s call     test/test_beam_search.py::test_beam_search_equal[transformer-args4-0.0-0.0-0.0-0.1-cpu-float32]
0.39s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args63-0.0-default-lm_args63-0.5-0.5-0.1-cuda-float64]
0.39s call     test/test_beam_search.py::test_beam_search_equal[transformer-args807-0.5-0.5-0.0-0.1-cuda-float16]
0.39s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args62-0.0-default-lm_args62-0.5-0.5-0.1-cuda-float32]
0.39s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict18]
0.39s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1010-0.5-0.0-0.0-0.0-cuda-float64]
0.39s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args47-0.0-transformer-lm_args47-0.5-0.5-0.1-cpu-float64]
0.39s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-enc.-None-dec., att.-0.5]
0.39s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1132-0.5-0.5-0.0-0.1-cuda-float32]
0.39s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args34-0.0-transformer-lm_args34-0.0-0.0-0.1-cpu-float32]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args266-0.5-0.5-0.0-0.0-cpu-float64]
0.38s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args58-0.0-default-lm_args58-0.5-0.0-0.1-cuda-float32]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args166-0.5-0.5-0.5-0.1-cpu-float32]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1029-0.5-0.5-0.5-0.1-cuda-float16]
0.38s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict0]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1129-0.5-0.5-0.0-0.0-cuda-float32]
0.38s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args82-0.0-transformer-lm_args82-0.0-0.0-0.1-cuda-float32]
0.38s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args75-0.0-default-lm_args75-0.5-0.0-0.1-cuda-float64]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1031-0.5-0.5-0.5-0.1-cuda-float64]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1091-0.0-0.0-0.5-0.1-cuda-float64]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1027-0.5-0.5-0.5-0.0-cuda-float32]
0.38s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic54-recog_dic54]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args812-0.5-0.5-0.5-0.0-cuda-float64]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args655-0.0-0.0-0.5-0.0-cuda-float32]
0.38s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args7-0.0-default-lm_args7-0.0-0.5-0.1-cpu-float64]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args872-0.0-0.0-0.5-0.0-cuda-float64]
0.38s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args32-0.0-transformer-lm_args32-0.0-0.0-0.0-cpu-float32]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args484-0.5-0.5-0.0-0.1-cpu-float32]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args651-0.0-0.0-0.0-0.1-cuda-float16]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args658-0.0-0.0-0.5-0.1-cuda-float32]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1014-0.5-0.0-0.5-0.0-cuda-float16]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args808-0.5-0.5-0.0-0.1-cuda-float32]
0.38s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-enc.enc.0-None-dec., att.-0.5]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1011-0.5-0.0-0.0-0.1-cuda-float16]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args800-0.5-0.0-0.5-0.0-cuda-float64]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args439-0.0-0.0-0.5-0.0-cpu-float32]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1130-0.5-0.5-0.0-0.0-cuda-float64]
0.38s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args28-0.0-default-lm_args28-0.5-0.5-0.0-cpu-float32]
0.38s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict15-inference_dict15]
0.38s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict16]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1021-0.5-0.5-0.0-0.0-cuda-float32]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args694-0.5-0.0-0.5-0.1-cuda-float32]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args652-0.0-0.0-0.0-0.1-cuda-float32]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args917-0.5-0.5-0.0-0.1-cuda-float64]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args697-0.5-0.5-0.0-0.0-cuda-float32]
0.38s call     test/test_beam_search.py::test_beam_search_equal[transformer-args869-0.0-0.0-0.0-0.1-cuda-float64]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1022-0.5-0.5-0.0-0.0-cuda-float64]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args910-0.5-0.0-0.5-0.1-cuda-float32]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1119-0.5-0.0-0.0-0.1-cuda-float16]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args151-0.5-0.0-0.5-0.0-cpu-float32]
0.37s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args88-0.0-transformer-lm_args88-0.5-0.0-0.0-cuda-float32]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args649-0.0-0.0-0.0-0.0-cuda-float32]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1080-0.0-0.0-0.0-0.0-cuda-float16]
0.37s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args78-0.0-default-lm_args78-0.5-0.5-0.1-cuda-float32]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args472-0.5-0.0-0.0-0.1-cpu-float32]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args274-0.5-0.5-0.5-0.1-cpu-float32]
0.37s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args50-0.0-default-lm_args50-0.0-0.0-0.1-cuda-float32]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1024-0.5-0.5-0.0-0.1-cuda-float32]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args796-0.5-0.0-0.0-0.1-cuda-float32]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args920-0.5-0.5-0.5-0.0-cuda-float64]
0.37s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args66-0.0-default-lm_args66-0.0-0.0-0.1-cuda-float32]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args115-0.0-0.0-0.5-0.0-cpu-float32]
0.37s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args86-0.0-transformer-lm_args86-0.0-0.5-0.1-cuda-float32]
0.37s call     test/test_e2e_asr.py::test_sortagrad_trainable[pytorch]
0.37s call     test/test_beam_search.py::test_beam_search_equal[transformer-args482-0.5-0.5-0.0-0.0-cpu-float64]
0.36s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args26-0.0-default-lm_args26-0.5-0.0-0.1-cpu-float32]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args875-0.0-0.0-0.5-0.1-cuda-float64]
0.36s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args83-0.0-transformer-lm_args83-0.0-0.0-0.1-cuda-float64]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args112-0.0-0.0-0.0-0.1-cpu-float32]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args257-0.5-0.0-0.0-0.1-cpu-float64]
0.36s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict9]
0.36s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args12-0.0-default-lm_args12-0.5-0.5-0.0-cpu-float32]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args161-0.5-0.5-0.0-0.1-cpu-float64]
0.36s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args59-0.0-default-lm_args59-0.5-0.0-0.1-cuda-float64]
0.36s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args94-0.0-transformer-lm_args94-0.5-0.5-0.1-cuda-float32]
0.36s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args44-0.0-transformer-lm_args44-0.5-0.5-0.0-cpu-float32]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args913-0.5-0.5-0.0-0.0-cuda-float32]
0.36s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args21-0.0-default-lm_args21-0.0-0.5-0.0-cpu-float64]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args53-0.5-0.5-0.0-0.1-cpu-float64]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args977-0.0-0.0-0.0-0.1-cuda-float64]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1-0.0-0.0-0.0-0.0-cpu-float32]
0.36s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args19-0.0-default-lm_args19-0.0-0.0-0.1-cpu-float64]
0.36s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args17-0.0-default-lm_args17-0.0-0.0-0.0-cpu-float64]
0.36s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args29-0.0-default-lm_args29-0.5-0.5-0.0-cpu-float64]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args874-0.0-0.0-0.5-0.1-cuda-float32]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args659-0.0-0.0-0.5-0.1-cuda-float64]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args916-0.5-0.5-0.0-0.1-cuda-float32]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args648-0.0-0.0-0.0-0.0-cuda-float16]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args701-0.5-0.5-0.0-0.1-cuda-float64]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args866-0.0-0.0-0.0-0.0-cuda-float64]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args253-0.5-0.0-0.0-0.0-cpu-float32]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args815-0.5-0.5-0.5-0.1-cuda-float64]
0.36s call     test/test_beam_search.py::test_beam_search_equal[transformer-args376-0.5-0.5-0.0-0.1-cpu-float32]
0.35s call     test/test_beam_search.py::test_beam_search_equal[transformer-args46-0.5-0.0-0.5-0.1-cpu-float32]
0.35s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args45-0.0-transformer-lm_args45-0.5-0.5-0.0-cpu-float64]
0.35s call     test/test_beam_search.py::test_beam_search_equal[transformer-args756-0.0-0.0-0.0-0.0-cuda-float16]
0.35s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict15]
0.35s call     test/test_beam_search.py::test_beam_search_equal[transformer-args803-0.5-0.0-0.5-0.1-cuda-float64]
0.35s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1122-0.5-0.0-0.5-0.0-cuda-float16]
0.35s call     test/test_beam_search.py::test_beam_search_equal[transformer-args470-0.5-0.0-0.0-0.0-cpu-float64]
0.35s call     test/test_recog.py::test_batch_beam_search[vggblstmp-lstm-espnet.nets.chainer_backend.e2e_asr]
0.35s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args13-0.0-default-lm_args13-0.5-0.5-0.0-cpu-float64]
0.35s call     test/test_beam_search.py::test_beam_search_equal[transformer-args365-0.5-0.0-0.0-0.1-cpu-float64]
0.35s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args65-0.0-default-lm_args65-0.0-0.0-0.0-cuda-float64]
0.35s call     test/test_beam_search.py::test_beam_search_equal[transformer-args911-0.5-0.0-0.5-0.1-cuda-float64]
0.35s call     test/test_beam_search.py::test_beam_search_equal[transformer-args702-0.5-0.5-0.5-0.0-cuda-float16]
0.35s call     test/test_beam_search.py::test_beam_search_equal[transformer-args919-0.5-0.5-0.5-0.0-cuda-float32]
0.35s call     test/test_beam_search.py::test_beam_search_equal[transformer-args224-0.0-0.0-0.5-0.0-cpu-float64]
0.35s call     test/test_beam_search.py::test_beam_search_equal[transformer-args870-0.0-0.0-0.5-0.0-cuda-float16]
0.35s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args39-0.0-transformer-lm_args39-0.0-0.5-0.1-cpu-float64]
0.35s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args70-0.0-default-lm_args70-0.0-0.5-0.1-cuda-float32]
0.35s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args76-0.0-default-lm_args76-0.5-0.5-0.0-cuda-float32]
0.35s call     test/test_beam_search.py::test_beam_search_equal[transformer-args163-0.5-0.5-0.5-0.0-cpu-float32]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args704-0.5-0.5-0.5-0.0-cuda-float64]
0.34s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args49-0.0-default-lm_args49-0.0-0.0-0.0-cuda-float64]
0.34s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args68-0.0-default-lm_args68-0.0-0.5-0.0-cuda-float32]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args478-0.5-0.0-0.5-0.1-cpu-float32]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1086-0.0-0.0-0.5-0.0-cuda-float16]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1030-0.5-0.5-0.5-0.1-cuda-float32]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args41-0.5-0.0-0.0-0.1-cpu-float64]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1012-0.5-0.0-0.0-0.1-cuda-float32]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args331-0.0-0.0-0.5-0.0-cpu-float32]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args481-0.5-0.5-0.0-0.0-cpu-float32]
0.34s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args16-0.0-default-lm_args16-0.0-0.0-0.0-cpu-float32]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args975-0.0-0.0-0.0-0.1-cuda-float16]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args326-0.0-0.0-0.0-0.0-cpu-float64]
0.34s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args3-0.0-default-lm_args3-0.0-0.0-0.1-cpu-float64]
0.34s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args79-0.0-default-lm_args79-0.5-0.5-0.1-cuda-float64]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1116-0.5-0.0-0.0-0.0-cuda-float16]
0.34s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict14]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args167-0.5-0.5-0.5-0.1-cpu-float64]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args798-0.5-0.0-0.5-0.0-cuda-float16]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args908-0.5-0.0-0.5-0.0-cuda-float64]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args226-0.0-0.0-0.5-0.1-cpu-float32]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args909-0.5-0.0-0.5-0.1-cuda-float16]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args335-0.0-0.0-0.5-0.1-cpu-float64]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args686-0.5-0.0-0.0-0.0-cuda-float64]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args382-0.5-0.5-0.5-0.1-cpu-float32]
0.34s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args40-0.0-transformer-lm_args40-0.5-0.0-0.0-cpu-float32]
0.34s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args51-0.0-default-lm_args51-0.0-0.0-0.1-cuda-float64]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args119-0.0-0.0-0.5-0.1-cpu-float64]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args703-0.5-0.5-0.5-0.0-cuda-float32]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args767-0.0-0.0-0.5-0.1-cuda-float64]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args436-0.0-0.0-0.0-0.1-cpu-float32]
0.34s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args41-0.0-transformer-lm_args41-0.5-0.0-0.0-cpu-float64]
0.34s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict11-inference_dict11]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args760-0.0-0.0-0.0-0.1-cuda-float32]
0.34s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args85-0.0-transformer-lm_args85-0.0-0.5-0.0-cuda-float64]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args269-0.5-0.5-0.0-0.1-cpu-float64]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args164-0.5-0.5-0.5-0.0-cpu-float64]
0.34s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1020-0.5-0.5-0.0-0.0-cuda-float16]
0.34s call     test/test_e2e_asr.py::test_mtl_loss[blstmp]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args275-0.5-0.5-0.5-0.1-cpu-float64]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args271-0.5-0.5-0.5-0.0-cpu-float32]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args118-0.0-0.0-0.5-0.1-cpu-float32]
0.33s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic43-recog_dic43]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args905-0.5-0.0-0.0-0.1-cuda-float64]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args328-0.0-0.0-0.0-0.1-cpu-float32]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args766-0.0-0.0-0.5-0.1-cuda-float32]
0.33s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args55-0.0-default-lm_args55-0.0-0.5-0.1-cuda-float64]
0.33s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args15-0.0-default-lm_args15-0.5-0.5-0.1-cpu-float64]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args691-0.5-0.0-0.5-0.0-cuda-float32]
0.33s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args72-0.0-default-lm_args72-0.5-0.0-0.0-cuda-float32]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1015-0.5-0.0-0.5-0.0-cuda-float32]
0.33s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args69-0.0-default-lm_args69-0.0-0.5-0.0-cuda-float64]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args759-0.0-0.0-0.0-0.1-cuda-float16]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args907-0.5-0.0-0.5-0.0-cuda-float32]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args685-0.5-0.0-0.0-0.0-cuda-float32]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args799-0.5-0.0-0.5-0.0-cuda-float32]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args44-0.5-0.0-0.5-0.0-cpu-float64]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args38-0.5-0.0-0.0-0.0-cpu-float64]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1018-0.5-0.0-0.5-0.1-cuda-float32]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args5-0.0-0.0-0.0-0.1-cpu-float64]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1121-0.5-0.0-0.0-0.1-cuda-float64]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args488-0.5-0.5-0.5-0.0-cpu-float64]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args58-0.5-0.5-0.5-0.1-cpu-float32]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args868-0.0-0.0-0.0-0.1-cuda-float32]
0.33s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args84-0.0-transformer-lm_args84-0.0-0.5-0.0-cuda-float32]
0.33s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1087-0.0-0.0-0.5-0.0-cuda-float32]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args864-0.0-0.0-0.0-0.0-cuda-float16]
0.32s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args89-0.0-transformer-lm_args89-0.5-0.0-0.0-cuda-float64]
0.32s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-enc.enc.0-None-dec., att.-0.0]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1137-0.5-0.5-0.5-0.1-cuda-float16]
0.32s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict19]
0.32s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic0-recog_dic0]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args473-0.5-0.0-0.0-0.1-cpu-float64]
0.32s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict3]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args146-0.5-0.0-0.0-0.0-cpu-float64]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args974-0.0-0.0-0.0-0.0-cuda-float64]
0.32s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args37-0.0-transformer-lm_args37-0.0-0.5-0.0-cpu-float64]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args688-0.5-0.0-0.0-0.1-cuda-float32]
0.32s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict5]
0.32s call     test/test_multi_spkrs.py::test_recognition_results_multi_outputs[vggblstmp-lstm-2-True-espnet.nets.pytorch_backend.e2e_asr_mix-0]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args764-0.0-0.0-0.5-0.0-cuda-float64]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args805-0.5-0.5-0.0-0.0-cuda-float32]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args873-0.0-0.0-0.5-0.1-cuda-float16]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args160-0.5-0.5-0.0-0.1-cpu-float32]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args918-0.5-0.5-0.5-0.0-cuda-float16]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1085-0.0-0.0-0.0-0.1-cuda-float64]
0.32s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[None-enc.-True-test-0.5]
0.32s call     test/test_e2e_asr.py::test_gradient_noise_injection[pytorch]
0.32s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1117-0.5-0.0-0.0-0.0-cuda-float32]
0.32s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args57-0.0-default-lm_args57-0.5-0.0-0.0-cuda-float64]
0.32s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args8-0.0-default-lm_args8-0.5-0.0-0.0-cpu-float32]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args983-0.0-0.0-0.5-0.1-cuda-float64]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args770-0.0-0.5-0.0-0.0-cuda-float64]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args487-0.5-0.5-0.5-0.0-cpu-float32]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args158-0.5-0.5-0.0-0.0-cpu-float64]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args109-0.0-0.0-0.0-0.0-cpu-float32]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args795-0.5-0.0-0.0-0.1-cuda-float16]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1134-0.5-0.5-0.5-0.0-cuda-float16]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args479-0.5-0.0-0.5-0.1-cpu-float64]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args265-0.5-0.5-0.0-0.0-cpu-float32]
0.31s call     test/test_e2e_asr.py::test_chainer_ctc_type
0.31s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args43-0.0-transformer-lm_args43-0.5-0.0-0.1-cpu-float64]
0.31s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args25-0.0-default-lm_args25-0.5-0.0-0.0-cpu-float64]
0.31s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args67-0.0-default-lm_args67-0.0-0.0-0.1-cuda-float64]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args223-0.0-0.0-0.5-0.0-cpu-float32]
0.31s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic48-recog_dic48]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args377-0.5-0.5-0.0-0.1-cpu-float64]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args787-0.0-1.0-0.5-0.0-cuda-float32]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args28-0.0-1.0-0.0-0.1-cpu-float32]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1013-0.5-0.0-0.0-0.1-cuda-float64]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args980-0.0-0.0-0.5-0.0-cuda-float64]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1089-0.0-0.0-0.5-0.1-cuda-float16]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args52-0.5-0.5-0.0-0.1-cpu-float32]
0.31s call     test/test_beam_search.py::test_beam_search_equal[transformer-args867-0.0-0.0-0.0-0.1-cuda-float16]
0.31s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args5-0.0-default-lm_args5-0.0-0.5-0.0-cpu-float64]
0.30s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args71-0.0-default-lm_args71-0.0-0.5-0.1-cuda-float64]
0.30s call     test/test_multi_spkrs.py::test_recognition_results_multi_outputs[vggbgrup-gru-2-True-espnet.nets.pytorch_backend.e2e_asr_mix-1]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args130-0.0-0.5-0.5-0.1-cpu-float32]
0.30s call     test/test_recog.py::test_batch_beam_search[vggbgrup-gru-espnet.nets.chainer_backend.e2e_asr]
0.30s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args20-0.0-default-lm_args20-0.0-0.5-0.0-cpu-float32]
0.30s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict23-inference_dict23]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args698-0.5-0.5-0.0-0.0-cuda-float64]
0.30s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[None-enc.-True-dec.embed.-0.5]
0.30s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-enc.-True-dec., att.-0.0]
0.30s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args23-0.0-default-lm_args23-0.0-0.5-0.1-cpu-float64]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args802-0.5-0.0-0.5-0.1-cuda-float32]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args757-0.0-0.0-0.0-0.0-cuda-float32]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args976-0.0-0.0-0.0-0.1-cuda-float32]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args485-0.5-0.5-0.0-0.1-cpu-float64]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args912-0.5-0.5-0.0-0.0-cuda-float16]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1146-0.5-1.0-0.5-0.0-cuda-float16]
0.30s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic49-recog_dic49]
0.30s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict12-inference_dict12]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args47-0.5-0.0-0.5-0.1-cpu-float64]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args350-0.0-1.0-0.0-0.0-cpu-float64]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args154-0.5-0.0-0.5-0.1-cpu-float32]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args650-0.0-0.0-0.0-0.0-cuda-float64]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args152-0.5-0.0-0.5-0.0-cpu-float64]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1151-0.5-1.0-0.5-0.1-cuda-float64]
0.30s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1083-0.0-0.0-0.0-0.1-cuda-float16]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args915-0.5-0.5-0.0-0.1-cuda-float16]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args256-0.5-0.0-0.0-0.1-cpu-float32]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args978-0.0-0.0-0.5-0.0-cuda-float16]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args367-0.5-0.0-0.5-0.0-cpu-float32]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args892-0.0-1.0-0.0-0.1-cuda-float32]
0.29s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[None-enc.-True-dec.embed.-0.0]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args758-0.0-0.0-0.0-0.0-cuda-float64]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1136-0.5-0.5-0.5-0.0-cuda-float64]
0.29s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args64-0.0-default-lm_args64-0.0-0.0-0.0-cuda-float32]
0.29s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[None-enc.-None-dec., att.-1.0]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args380-0.5-0.5-0.5-0.0-cpu-float64]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args254-0.5-0.0-0.0-0.0-cpu-float64]
0.29s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args73-0.0-default-lm_args73-0.5-0.0-0.0-cuda-float64]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args445-0.0-0.5-0.0-0.0-cpu-float32]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1039-0.5-1.0-0.5-0.0-cuda-float32]
0.29s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args42-0.0-transformer-lm_args42-0.5-0.0-0.1-cpu-float32]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args26-0.0-1.0-0.0-0.0-cpu-float64]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1002-0.0-1.0-0.5-0.0-cuda-float16]
0.29s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args9-0.0-default-lm_args9-0.5-0.0-0.0-cpu-float64]
0.29s call     test/espnet2/asr/encoder/test_vgg_rnn_encoder.py::test_Encoder_forward_backward[True-True-gru]
0.29s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args36-0.0-transformer-lm_args36-0.0-0.5-0.0-cpu-float32]
0.29s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict18-inference_dict18]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args761-0.0-0.0-0.0-0.1-cuda-float64]
0.29s call     test/test_e2e_asr.py::test_mtl_loss[vggblstmp]
0.29s call     test/test_e2e_asr.py::test_gpu_trainable[espnet.nets.chainer_backend.e2e_asr]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args690-0.5-0.0-0.5-0.0-cuda-float16]
0.29s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args18-0.0-default-lm_args18-0.0-0.0-0.1-cpu-float32]
0.29s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict9]
0.29s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict17-inference_dict17]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args994-0.0-0.5-0.5-0.1-cuda-float32]
0.29s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic51-recog_dic51]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args239-0.0-0.5-0.5-0.1-cpu-float64]
0.29s call     test/test_beam_search.py::test_beam_search_equal[transformer-args801-0.5-0.0-0.5-0.1-cuda-float16]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1131-0.5-0.5-0.0-0.1-cuda-float16]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args460-0.0-1.0-0.0-0.1-cpu-float32]
0.28s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic62-recog_dic62]
0.28s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-enc.enc.0-True-dec.embed.,dec.decoder.1-0.5]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args491-0.5-0.5-0.5-0.1-cpu-float64]
0.28s call     test/espnet2/train/test_distributed_utils.py::test_resolve_distributed_mode_slurm3
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args793-0.5-0.0-0.0-0.0-cuda-float32]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args155-0.5-0.0-0.5-0.1-cpu-float64]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args334-0.0-0.0-0.5-0.1-cpu-float32]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1098-0.0-0.5-0.5-0.0-cuda-float16]
0.28s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args6-0.0-default-lm_args6-0.0-0.5-0.1-cpu-float32]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args395-0.5-1.0-0.5-0.1-cpu-float64]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args782-0.0-1.0-0.0-0.0-cuda-float64]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args914-0.5-0.5-0.0-0.0-cuda-float64]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args442-0.0-0.0-0.5-0.1-cpu-float32]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args979-0.0-0.0-0.5-0.0-cuda-float32]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args364-0.5-0.0-0.0-0.1-cpu-float32]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1000-0.0-1.0-0.0-0.1-cuda-float32]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args25-0.0-1.0-0.0-0.0-cpu-float32]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args973-0.0-0.0-0.0-0.0-cuda-float32]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args475-0.5-0.0-0.5-0.0-cpu-float32]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1019-0.5-0.0-0.5-0.1-cuda-float64]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args241-0.0-1.0-0.0-0.0-cpu-float32]
0.28s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic4-recog_dic4]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args379-0.5-0.5-0.5-0.0-cpu-float32]
0.28s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict6-inference_dict6]
0.28s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1017-0.5-0.0-0.5-0.1-cuda-float16]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args368-0.5-0.0-0.5-0.0-cpu-float64]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1009-0.5-0.0-0.0-0.0-cuda-float32]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args143-0.0-1.0-0.5-0.1-cpu-float64]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args286-0.5-1.0-0.5-0.1-cpu-float32]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args762-0.0-0.0-0.5-0.0-cuda-float16]
0.27s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args10-0.0-default-lm_args10-0.5-0.0-0.1-cpu-float32]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args272-0.5-0.5-0.5-0.0-cpu-float64]
0.27s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict24]
0.27s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic45-recog_dic45]
0.27s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic44-recog_dic44]
0.27s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic36-recog_dic36]
0.27s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args22-0.0-default-lm_args22-0.0-0.5-0.1-cpu-float32]
0.27s call     test/espnet2/asr/encoder/test_vgg_rnn_encoder.py::test_Encoder_forward_backward[False-True-gru]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1037-0.5-1.0-0.0-0.1-cuda-float64]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args220-0.0-0.0-0.0-0.1-cpu-float32]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args821-0.5-1.0-0.0-0.1-cuda-float64]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args332-0.0-0.0-0.5-0.0-cpu-float64]
0.27s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-enc.-True-dec., att.-1.0]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args16-0.0-0.5-0.0-0.1-cpu-float32]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1036-0.5-1.0-0.0-0.1-cuda-float32]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args370-0.5-0.0-0.5-0.1-cpu-float32]
0.27s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict24]
0.27s call     test/espnet2/asr/frontend/test_frontend.py::test_frontend_backward_multi_channel[False-True-True]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args990-0.0-0.5-0.5-0.0-cuda-float16]
0.27s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args27-0.0-default-lm_args27-0.5-0.0-0.1-cpu-float64]
0.27s call     test/test_beam_search.py::test_beam_search_equal[transformer-args784-0.0-1.0-0.0-0.1-cuda-float32]
0.27s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict7]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1007-0.0-1.0-0.5-0.1-cuda-float64]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args878-0.0-0.5-0.0-0.0-cuda-float64]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args11-0.0-0.0-0.5-0.1-cpu-float64]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args476-0.5-0.0-0.5-0.0-cpu-float64]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args227-0.0-0.0-0.5-0.1-cpu-float64]
0.26s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-test-None-dec., att.-0.5]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args260-0.5-0.0-0.5-0.0-cpu-float64]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1038-0.5-1.0-0.5-0.0-cuda-float16]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args29-0.0-1.0-0.0-0.1-cpu-float64]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1100-0.0-0.5-0.5-0.0-cuda-float64]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args891-0.0-1.0-0.0-0.1-cuda-float16]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args355-0.0-1.0-0.5-0.0-cpu-float32]
0.26s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict30]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args921-0.5-0.5-0.5-0.1-cuda-float16]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args247-0.0-1.0-0.5-0.0-cpu-float32]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args437-0.0-0.0-0.0-0.1-cpu-float64]
0.26s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict6]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args262-0.5-0.0-0.5-0.1-cpu-float32]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args251-0.0-1.0-0.5-0.1-cpu-float64]
0.26s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args33-0.0-transformer-lm_args33-0.0-0.0-0.0-cpu-float64]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args903-0.5-0.0-0.0-0.1-cuda-float16]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1108-0.0-1.0-0.0-0.1-cuda-float32]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1150-0.5-1.0-0.5-0.1-cuda-float32]
0.26s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[None-enc.-True-test-1.0]
0.26s call     test/test_beam_search.py::test_beam_search_equal[transformer-args268-0.5-0.5-0.0-0.1-cpu-float32]
0.26s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict26]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args671-0.0-0.5-0.5-0.1-cuda-float64]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args110-0.0-0.0-0.0-0.0-cpu-float64]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args673-0.0-1.0-0.0-0.0-cuda-float32]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args935-0.5-1.0-0.5-0.1-cuda-float64]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args280-0.5-1.0-0.0-0.1-cpu-float32]
0.25s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict54]
0.25s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict17]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args361-0.5-0.0-0.0-0.0-cpu-float32]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args31-0.0-1.0-0.5-0.0-cpu-float32]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args997-0.0-1.0-0.0-0.0-cuda-float32]
0.25s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-enc.enc.0, enc.enc.1-True-dec.embed.,dec.decoder.1-1.0]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args284-0.5-1.0-0.5-0.0-cpu-float64]
0.25s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict5]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args131-0.0-0.5-0.5-0.1-cpu-float64]
0.25s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict10]
0.25s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic5-recog_dic5]
0.25s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic50-recog_dic50]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args986-0.0-0.5-0.0-0.0-cuda-float64]
0.25s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic60-recog_dic60]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1113-0.0-1.0-0.5-0.1-cuda-float16]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args906-0.5-0.0-0.5-0.0-cuda-float16]
0.25s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic27-recog_dic27]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args371-0.5-0.0-0.5-0.1-cpu-float64]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args142-0.0-1.0-0.5-0.1-cpu-float32]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args929-0.5-1.0-0.0-0.1-cuda-float64]
0.25s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args2-0.0-default-lm_args2-0.0-0.0-0.1-cpu-float32]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args985-0.0-0.5-0.0-0.0-cuda-float32]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args7-0.0-0.0-0.5-0.0-cpu-float32]
0.25s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-enc.enc.0-None-dec., att.-1.0]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args14-0.0-0.5-0.0-0.0-cpu-float64]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1001-0.0-1.0-0.0-0.1-cuda-float64]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args148-0.5-0.0-0.0-0.1-cpu-float32]
0.25s call     test/test_beam_search.py::test_beam_search_equal[transformer-args59-0.5-0.5-0.5-0.1-cpu-float64]
0.25s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic63-recog_dic63]
0.25s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[None-enc.-True-dec., att.-0.0]
0.24s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic1-recog_dic1]
0.24s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic33-recog_dic33]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args827-0.5-1.0-0.5-0.1-cuda-float64]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args823-0.5-1.0-0.5-0.0-cuda-float32]
0.24s call     test/test_e2e_asr.py::test_chainer_save_and_load
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args494-0.5-1.0-0.0-0.0-cpu-float64]
0.24s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict1]
0.24s call     test/espnet2/asr/encoder/test_vgg_rnn_encoder.py::test_Encoder_forward_backward[False-True-lstm]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args455-0.0-0.5-0.5-0.1-cpu-float64]
0.24s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args0-0.0-default-lm_args0-0.0-0.0-0.0-cpu-float32]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args10-0.0-0.0-0.5-0.1-cpu-float32]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1105-0.0-1.0-0.0-0.0-cuda-float32]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args8-0.0-0.0-0.5-0.0-cpu-float64]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args340-0.0-0.5-0.0-0.1-cpu-float32]
0.24s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[None-enc.-True-dec., att.-0.5]
0.24s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict25]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1094-0.0-0.5-0.0-0.0-cuda-float64]
0.24s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-test-None-dec., att.-1.0]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1125-0.5-0.0-0.5-0.1-cuda-float16]
0.24s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic26-recog_dic26]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args329-0.0-0.0-0.0-0.1-cpu-float64]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1035-0.5-1.0-0.0-0.1-cuda-float16]
0.24s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict16]
0.24s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict1]
0.24s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict22]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args991-0.0-0.5-0.5-0.0-cuda-float32]
0.24s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict17]
0.24s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict12]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args774-0.0-0.5-0.5-0.0-cuda-float16]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1096-0.0-0.5-0.0-0.1-cuda-float32]
0.24s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1114-0.0-1.0-0.5-0.1-cuda-float32]
0.23s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict21]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args664-0.0-0.5-0.0-0.1-cuda-float32]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args769-0.0-0.5-0.0-0.0-cuda-float32]
0.23s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict10-inference_dict10]
0.23s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args4-0.0-default-lm_args4-0.0-0.5-0.0-cpu-float32]
0.23s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict10]
0.23s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args35-0.0-transformer-lm_args35-0.0-0.0-0.1-cpu-float64]
0.23s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict25]
0.23s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic64-recog_dic64]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args896-0.0-1.0-0.5-0.0-cuda-float64]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args281-0.5-1.0-0.0-0.1-cpu-float64]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1110-0.0-1.0-0.5-0.0-cuda-float16]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args672-0.0-1.0-0.0-0.0-cuda-float16]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args49-0.5-0.5-0.0-0.0-cpu-float32]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1099-0.0-0.5-0.5-0.0-cuda-float32]
0.23s call     test/test_initialization.py::test_lecun_init_torch
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args40-0.5-0.0-0.0-0.1-cpu-float32]
0.23s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict8]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args113-0.0-0.0-0.0-0.1-cpu-float64]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args876-0.0-0.5-0.0-0.0-cuda-float16]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1093-0.0-0.5-0.0-0.0-cuda-float32]
0.23s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-enc.-True-dec., att.-0.5]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args458-0.0-1.0-0.0-0.0-cpu-float64]
0.23s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict15]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args791-0.0-1.0-0.5-0.1-cuda-float64]
0.23s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[None-enc.-True-dec., att.-1.0]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args826-0.5-1.0-0.5-0.1-cuda-float32]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1104-0.0-1.0-0.0-0.0-cuda-float16]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args665-0.0-0.5-0.0-0.1-cuda-float64]
0.23s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict32]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args790-0.0-1.0-0.5-0.1-cuda-float32]
0.23s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict2]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args718-0.5-1.0-0.5-0.1-cuda-float32]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1101-0.0-0.5-0.5-0.1-cuda-float16]
0.23s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict4]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args993-0.0-0.5-0.5-0.1-cuda-float16]
0.23s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict22]
0.23s call     test/test_beam_search.py::test_beam_search_equal[transformer-args989-0.0-0.5-0.0-0.1-cuda-float64]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args259-0.5-0.0-0.5-0.0-cpu-float32]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args820-0.5-1.0-0.0-0.1-cuda-float32]
0.22s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict23]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1042-0.5-1.0-0.5-0.1-cuda-float32]
0.22s call     test/test_e2e_mt.py::test_sortagrad_trainable_with_batch_frames[pytorch]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args493-0.5-1.0-0.0-0.0-cpu-float32]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args711-0.5-1.0-0.0-0.1-cuda-float16]
0.22s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[tacotron2-model_dict14]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args899-0.0-1.0-0.5-0.1-cuda-float64]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1111-0.0-1.0-0.5-0.0-cuda-float32]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args897-0.0-1.0-0.5-0.1-cuda-float16]
0.22s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1189-0.0-0.0-0.0-0.0-cuda-float32]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1115-0.0-1.0-0.5-0.1-cuda-float64]
0.22s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict8]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1107-0.0-1.0-0.0-0.1-cuda-float16]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args890-0.0-1.0-0.0-0.0-cuda-float64]
0.22s call     test/espnet2/asr/frontend/test_frontend.py::test_frontend_backward_multi_channel[True-True-True]
0.22s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args24-0.0-default-lm_args24-0.5-0.0-0.0-cpu-float32]
0.22s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict7]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args792-0.5-0.0-0.0-0.0-cuda-float16]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args675-0.0-1.0-0.0-0.1-cuda-float16]
0.22s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict46]
0.22s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict8]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1095-0.0-0.5-0.0-0.1-cuda-float16]
0.22s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[None-enc.-True-dec.embed.-1.0]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1109-0.0-1.0-0.0-0.1-cuda-float64]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args469-0.5-0.0-0.0-0.0-cpu-float32]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args786-0.0-1.0-0.5-0.0-cuda-float16]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args43-0.5-0.0-0.5-0.0-cpu-float32]
0.22s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict18]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args55-0.5-0.5-0.5-0.0-cpu-float32]
0.22s call     test/test_beam_search.py::test_beam_search_equal[transformer-args717-0.5-1.0-0.5-0.1-cuda-float16]
0.22s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-enc.enc.0, enc.enc.1-True-dec., att.-0.0]
0.21s call     test/test_beam_search.py::test_beam_search_equal[transformer-args145-0.5-0.0-0.0-0.0-cpu-float32]
0.21s call     test/test_beam_search.py::test_beam_search_equal[transformer-args663-0.0-0.5-0.0-0.1-cuda-float16]
0.21s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict53]
0.21s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1128-0.5-0.5-0.0-0.0-cuda-float16]
0.21s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict13]
0.21s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1141-0.5-1.0-0.0-0.0-cuda-float32]
0.21s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict27]
0.21s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[None-enc.-True-test-0.0]
0.21s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1005-0.0-1.0-0.5-0.1-cuda-float16]
0.21s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict1]
0.21s call     test/test_beam_search.py::test_beam_search_equal[transformer-args32-0.0-1.0-0.5-0.0-cpu-float64]
0.21s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict12]
0.21s call     test/test_beam_search.py::test_beam_search_equal[transformer-args662-0.0-0.5-0.0-0.0-cuda-float64]
0.21s call     test/test_recog.py::test_batch_beam_search[blstmp-lstm-espnet.nets.chainer_backend.e2e_asr]
0.21s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict0]
0.21s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict20]
0.21s call     test/test_beam_search.py::test_beam_search_equal[transformer-args877-0.0-0.5-0.0-0.0-cuda-float32]
0.21s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1097-0.0-0.5-0.0-0.1-cuda-float64]
0.21s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic52-recog_dic52]
0.21s call     test/test_beam_search.py::test_beam_search_equal[transformer-args716-0.5-1.0-0.5-0.0-cuda-float64]
0.21s call     test/test_beam_search.py::test_beam_search_equal[transformer-args995-0.0-0.5-0.5-0.1-cuda-float64]
0.20s call     test/espnet2/asr/encoder/test_vgg_rnn_encoder.py::test_Encoder_forward_backward[True-True-lstm]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args23-0.0-0.5-0.5-0.1-cpu-float64]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1106-0.0-1.0-0.0-0.0-cuda-float64]
0.20s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic3-recog_dic3]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args930-0.5-1.0-0.5-0.0-cuda-float16]
0.20s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict31]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args245-0.0-1.0-0.0-0.1-cpu-float64]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args338-0.0-0.5-0.0-0.0-cpu-float64]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args250-0.0-1.0-0.5-0.1-cpu-float32]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args925-0.5-1.0-0.0-0.0-cuda-float32]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args283-0.5-1.0-0.5-0.0-cpu-float32]
0.20s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict29]
0.20s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict20]
0.20s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict16-inference_dict16]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args176-0.5-1.0-0.5-0.0-cpu-float64]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args128-0.0-0.5-0.5-0.0-cpu-float64]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args238-0.0-0.5-0.5-0.1-cpu-float32]
0.20s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1247-0.5-0.5-0.5-0.1-cuda-float64]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args446-0.0-0.5-0.0-0.0-cpu-float64]
0.20s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict12]
0.20s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict0]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args674-0.0-1.0-0.0-0.0-cuda-float64]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args822-0.5-1.0-0.5-0.0-cuda-float16]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args149-0.5-0.0-0.0-0.1-cpu-float64]
0.20s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic25-recog_dic25]
0.20s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic47-recog_dic47]
0.20s call     test/espnet2/asr/frontend/test_frontend.py::test_frontend_backward_multi_channel[False-False-True]
0.20s call     test/test_beam_search.py::test_beam_search_equal[transformer-args677-0.0-1.0-0.0-0.1-cuda-float64]
0.20s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_backward[True-LSTM]
0.20s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict8]
0.20s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args11-0.0-default-lm_args11-0.5-0.0-0.1-cpu-float64]
0.19s call     test/espnet2/asr/frontend/test_frontend.py::test_frontend_backward_multi_channel[False-True-False]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1162-1.0-0.0-0.5-0.1-cuda-float32]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1142-0.5-1.0-0.0-0.0-cuda-float64]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1033-0.5-1.0-0.0-0.0-cuda-float32]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1040-0.5-1.0-0.5-0.0-cuda-float64]
0.19s call     test/test_recog.py::test_batch_beam_search[bgrup-gru-espnet.nets.chainer_backend.e2e_asr]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args248-0.0-1.0-0.5-0.0-cpu-float64]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args208-1.0-1.0-0.0-0.1-cpu-float32]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1043-0.5-1.0-0.5-0.1-cuda-float64]
0.19s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[tacotron2-model_dict27]
0.19s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict42]
0.19s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict38]
0.19s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[True-test-None-dec., att.-0.0]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args74-1.0-0.0-0.0-0.0-cpu-float64]
0.19s call     test/espnet2/tasks/test_abs_task.py::test_main_print_config
0.19s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict2]
0.19s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic28-recog_dic28]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1032-0.5-1.0-0.0-0.0-cuda-float16]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args454-0.0-0.5-0.5-0.1-cpu-float32]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1056-1.0-0.5-0.0-0.0-cuda-float16]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args785-0.0-1.0-0.0-0.1-cuda-float64]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args353-0.0-1.0-0.0-0.1-cpu-float64]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args992-0.0-0.5-0.5-0.0-cuda-float64]
0.19s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args14-0.0-default-lm_args14-0.5-0.5-0.1-cpu-float32]
0.19s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict55]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args325-0.0-0.0-0.0-0.0-cpu-float32]
0.19s call     test/test_beam_search.py::test_beam_search_equal[transformer-args788-0.0-1.0-0.5-0.0-cuda-float64]
0.19s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict67]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args887-0.0-0.5-0.5-0.1-cuda-float64]
0.18s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict43]
0.18s call     test/test_e2e_st.py::test_sortagrad_trainable_with_batch_bins[pytorch]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args184-1.0-0.0-0.0-0.1-cpu-float32]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args898-0.0-1.0-0.5-0.1-cuda-float32]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args660-0.0-0.5-0.0-0.0-cuda-float16]
0.18s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict11]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args771-0.0-0.5-0.0-0.1-cuda-float16]
0.18s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict16]
0.18s call     test/espnet2/asr/frontend/test_frontend.py::test_frontend_backward_multi_channel[True-False-True]
0.18s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic55-recog_dic55]
0.18s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict61]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args668-0.0-0.5-0.5-0.0-cuda-float64]
0.18s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict45]
0.18s call     test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args38-0.0-transformer-lm_args38-0.0-0.5-0.1-cpu-float32]
0.18s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_trainable_and_decodable[model_dict22-inference_dict22]
0.18s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict28]
0.18s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic2-recog_dic2]
0.18s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic13-recog_dic13]
0.18s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict40]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args998-0.0-1.0-0.0-0.0-cuda-float64]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args783-0.0-1.0-0.0-0.1-cuda-float16]
0.18s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict48]
0.18s call     test/test_asr_init.py::test_pytorch_trainable_transferable_and_decodable[None-enc.-None-dec., att.-0.5]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args768-0.0-0.5-0.0-0.0-cuda-float16]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args772-0.0-0.5-0.0-0.1-cuda-float32]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args341-0.0-0.5-0.0-0.1-cpu-float64]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args244-0.0-1.0-0.0-0.1-cpu-float32]
0.18s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic56-recog_dic56]
0.18s call     test/test_beam_search.py::test_beam_search_equal[transformer-args497-0.5-1.0-0.0-0.1-cpu-float64]
0.18s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict57]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args79-1.0-0.0-0.5-0.0-cpu-float32]
0.17s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict4]
0.17s call     test/test_e2e_mt.py::test_sortagrad_trainable[pytorch]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args80-1.0-0.0-0.5-0.0-cpu-float64]
0.17s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict62]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1006-0.0-1.0-0.5-0.1-cuda-float32]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args819-0.5-1.0-0.0-0.1-cuda-float16]
0.17s call     test/espnet2/asr/encoder/test_vgg_rnn_encoder.py::test_Encoder_forward_backward[True-False-lstm]
0.17s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[tacotron2-model_dict26]
0.17s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict7]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args928-0.5-1.0-0.0-0.1-cuda-float32]
0.17s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict5]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args127-0.0-0.5-0.5-0.0-cpu-float32]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args670-0.0-0.5-0.5-0.1-cuda-float32]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args337-0.0-0.5-0.0-0.0-cpu-float32]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args924-0.5-1.0-0.0-0.0-cuda-float16]
0.17s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic14-recog_dic14]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1003-0.0-1.0-0.5-0.0-cuda-float32]
0.17s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic40-recog_dic40]
0.17s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict34]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args71-0.5-1.0-0.5-0.1-cpu-float64]
0.17s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict33]
0.17s call     test/test_e2e_st.py::test_gradient_noise_injection[pytorch]
0.17s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict28]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1092-0.0-0.5-0.0-0.0-cuda-float16]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args816-0.5-1.0-0.0-0.0-cuda-float16]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args931-0.5-1.0-0.5-0.0-cuda-float32]
0.17s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict21]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args211-1.0-1.0-0.5-0.0-cpu-float32]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1145-0.5-1.0-0.0-0.1-cuda-float64]
0.17s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1140-0.5-1.0-0.0-0.0-cuda-float16]
0.16s call     test/test_e2e_asr.py::test_sortagrad_trainable_with_batch_bins[pytorch]
0.16s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict19]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args70-0.5-1.0-0.5-0.1-cpu-float32]
0.16s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict9]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args73-1.0-0.0-0.0-0.0-cpu-float32]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args62-0.5-1.0-0.0-0.0-cpu-float64]
0.16s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[tacotron2-model_dict22]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args666-0.0-0.5-0.5-0.0-cuda-float16]
0.16s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict14]
0.16s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic61-recog_dic61]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args121-0.0-0.5-0.0-0.0-cpu-float32]
0.16s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict0]
0.16s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict3]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args278-0.5-1.0-0.0-0.0-cpu-float64]
0.16s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_backward[False-LSTM]
0.16s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic31-recog_dic31]
0.16s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict17]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args344-0.0-0.5-0.5-0.0-cpu-float64]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args61-0.5-1.0-0.0-0.0-cpu-float32]
0.16s call     test/espnet2/asr/encoder/test_vgg_rnn_encoder.py::test_Encoder_forward_backward[True-False-gru]
0.16s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict11]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args499-0.5-1.0-0.5-0.0-cpu-float32]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args403-1.0-0.0-0.5-0.0-cpu-float32]
0.16s call     test/test_tensorboard.py::test_tensorboard_evaluator
0.16s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic29-recog_dic29]
0.16s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict65]
0.16s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict50]
0.16s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict23]
0.16s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic59-recog_dic59]
0.16s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict39]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args17-0.0-0.5-0.0-0.1-cpu-float64]
0.16s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic32-recog_dic32]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args356-0.0-1.0-0.5-0.0-cpu-float64]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args179-0.5-1.0-0.5-0.1-cpu-float64]
0.16s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict41]
0.16s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict13]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args845-1.0-0.5-0.0-0.1-cuda-float64]
0.16s call     test/test_e2e_mt.py::test_sortagrad_trainable_with_batch_bins[pytorch]
0.16s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic46-recog_dic46]
0.16s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1240-0.5-0.5-0.0-0.1-cuda-float32]
0.16s call     test/espnet2/train/test_reporter.py::test_matplotlib_plot
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args133-0.0-1.0-0.0-0.0-cpu-float32]
0.16s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic38-recog_dic38]
0.16s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1149-0.5-1.0-0.5-0.1-cuda-float16]
0.16s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict27]
0.16s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict36]
0.16s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict46]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args851-1.0-0.5-0.5-0.1-cuda-float64]
0.15s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic58-recog_dic58]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args961-1.0-1.0-0.0-0.0-cuda-float32]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args401-1.0-0.0-0.0-0.1-cpu-float64]
0.15s call     test/test_e2e_tts_transformer.py::test_transformer_trainable_and_decodable[model_dict7]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args305-1.0-0.5-0.0-0.1-cpu-float64]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args676-0.0-1.0-0.0-0.1-cuda-float32]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args415-1.0-0.5-0.5-0.0-cpu-float32]
0.15s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1243-0.5-0.5-0.5-0.0-cuda-float32]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args741-1.0-0.5-0.5-0.1-cuda-float16]
0.15s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict10]
0.15s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic30-recog_dic30]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args352-0.0-1.0-0.0-0.1-cpu-float32]
0.15s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict47]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args13-0.0-0.5-0.0-0.0-cpu-float32]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args710-0.5-1.0-0.0-0.0-cuda-float64]
0.15s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict8]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args755-1.0-1.0-0.5-0.1-cuda-float64]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args193-1.0-0.5-0.0-0.0-cpu-float32]
0.15s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict35]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1152-1.0-0.0-0.0-0.0-cuda-float16]
0.15s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict4]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1067-1.0-0.5-0.5-0.1-cuda-float64]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args34-0.0-1.0-0.5-0.1-cpu-float32]
0.15s call     test/espnet2/asr/frontend/test_frontend.py::test_frontend_backward_multi_channel[True-True-False]
0.15s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict3]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1046-1.0-0.0-0.0-0.0-cuda-float64]
0.15s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[subsample1-False-True-lstm]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args103-1.0-1.0-0.5-0.0-cpu-float32]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1160-1.0-0.0-0.5-0.0-cuda-float64]
0.15s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1241-0.5-0.5-0.0-0.1-cuda-float64]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args277-0.5-1.0-0.0-0.0-cpu-float32]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1148-0.5-1.0-0.5-0.0-cuda-float64]
0.15s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict11]
0.15s call     test/test_beam_search.py::test_beam_search_equal[transformer-args999-0.0-1.0-0.0-0.1-cuda-float16]
0.15s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict18]
0.15s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict60]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args730-1.0-0.0-0.5-0.1-cuda-float32]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args320-1.0-1.0-0.5-0.0-cpu-float64]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args101-1.0-1.0-0.0-0.1-cpu-float64]
0.14s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict24]
0.14s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict42]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1047-1.0-0.0-0.0-0.1-cuda-float16]
0.14s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic37-recog_dic37]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args313-1.0-1.0-0.0-0.0-cpu-float32]
0.14s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic57-recog_dic57]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1034-0.5-1.0-0.0-0.0-cuda-float64]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args952-1.0-0.5-0.0-0.1-cuda-float32]
0.14s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic15-recog_dic15]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args190-1.0-0.0-0.5-0.1-cpu-float32]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args298-1.0-0.0-0.5-0.1-cpu-float32]
0.14s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[None-True-True-lstm]
0.14s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.chainer_backend.e2e_asr-model_dict6]
0.14s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic22-recog_dic22]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args932-0.5-1.0-0.5-0.0-cuda-float64]
0.14s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict66]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args185-1.0-0.0-0.0-0.1-cpu-float64]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args719-0.5-1.0-0.5-0.1-cuda-float64]
0.14s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[None-False-True-lstm]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args200-1.0-0.5-0.5-0.0-cpu-float64]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args859-1.0-1.0-0.5-0.0-cuda-float32]
0.14s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic34-recog_dic34]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args773-0.0-0.5-0.0-0.1-cuda-float64]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args953-1.0-0.5-0.0-0.1-cuda-float64]
0.14s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1245-0.5-0.5-0.5-0.1-cuda-float16]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args85-1.0-0.5-0.0-0.0-cpu-float32]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args242-0.0-1.0-0.0-0.0-cpu-float64]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args824-0.5-1.0-0.5-0.0-cuda-float64]
0.14s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic53-recog_dic53]
0.14s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict56]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1147-0.5-1.0-0.5-0.0-cuda-float32]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args406-1.0-0.0-0.5-0.1-cpu-float32]
0.14s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[tacotron2-model_dict23]
0.14s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1045-1.0-0.0-0.0-0.0-cuda-float32]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args100-1.0-1.0-0.0-0.1-cpu-float32]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args775-0.0-0.5-0.5-0.0-cuda-float32]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args199-1.0-0.5-0.5-0.0-cpu-float32]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args412-1.0-0.5-0.0-0.1-cpu-float32]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args963-1.0-1.0-0.0-0.1-cuda-float16]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args937-1.0-0.0-0.0-0.0-cuda-float32]
0.13s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic12-recog_dic12]
0.13s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict19]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args302-1.0-0.5-0.0-0.0-cpu-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args744-1.0-1.0-0.0-0.0-cuda-float16]
0.13s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1238-0.5-0.5-0.0-0.0-cuda-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1065-1.0-0.5-0.5-0.1-cuda-float16]
0.13s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict13]
0.13s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict19]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args987-0.0-0.5-0.0-0.1-cuda-float16]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args836-1.0-0.0-0.5-0.0-cuda-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args862-1.0-1.0-0.5-0.1-cuda-float32]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args900-0.5-0.0-0.0-0.0-cuda-float16]
0.13s call     test/test_lm.py::test_lm_trainable_and_decodable[transformer-lm_args33-cuda-float16]
0.13s call     test/espnet2/train/test_reporter.py::test_register[None-None]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args169-0.5-1.0-0.0-0.0-cpu-float32]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args91-1.0-0.5-0.5-0.0-cpu-float32]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args535-1.0-1.0-0.5-0.0-cpu-float32]
0.13s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict2]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args708-0.5-1.0-0.0-0.0-cuda-float16]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1161-1.0-0.0-0.5-0.1-cuda-float16]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args938-1.0-0.0-0.0-0.0-cuda-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args956-1.0-0.5-0.5-0.0-cuda-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args962-1.0-1.0-0.0-0.0-cuda-float64]
0.13s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict39]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args304-1.0-0.5-0.0-0.1-cpu-float32]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args413-1.0-0.5-0.0-0.1-cpu-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args290-1.0-0.0-0.0-0.0-cpu-float64]
0.13s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict45]
0.13s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args893-0.0-1.0-0.0-0.1-cuda-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[rnn-args548-0.0-0.0-0.5-0.0-cpu-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args888-0.0-1.0-0.0-0.0-cuda-float16]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1079-1.0-1.0-0.5-0.1-cuda-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1190-0.0-0.0-0.0-0.0-cuda-float64]
0.13s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict51]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1051-1.0-0.0-0.5-0.0-cuda-float32]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1062-1.0-0.5-0.5-0.0-cuda-float16]
0.13s call     test/test_lm.py::test_lm_trainable_and_decodable[transformer-lm_args34-cuda-float32]
0.13s call     test/espnet2/asr/frontend/test_frontend.py::test_frontend_backward
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args729-1.0-0.0-0.5-0.1-cuda-float16]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args509-1.0-0.0-0.0-0.1-cpu-float64]
0.13s call     test/test_lm.py::test_lm_trainable_and_decodable[transformer-lm_args35-cuda-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1103-0.0-0.5-0.5-0.1-cuda-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1242-0.5-0.5-0.5-0.0-cuda-float16]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1061-1.0-0.5-0.0-0.1-cuda-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1173-1.0-0.5-0.5-0.1-cuda-float16]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args425-1.0-1.0-0.0-0.1-cpu-float64]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args853-1.0-1.0-0.0-0.0-cuda-float32]
0.13s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict52]
0.13s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict18]
0.13s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict12]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args936-1.0-0.0-0.0-0.0-cuda-float16]
0.13s call     test/test_beam_search.py::test_beam_search_equal[transformer-args858-1.0-1.0-0.5-0.0-cuda-float16]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args191-1.0-0.0-0.5-0.1-cpu-float64]
0.12s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1244-0.5-0.5-0.5-0.0-cuda-float64]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args968-1.0-1.0-0.5-0.0-cuda-float64]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args496-0.5-1.0-0.0-0.1-cpu-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args343-0.0-0.5-0.5-0.0-cpu-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args748-1.0-1.0-0.0-0.1-cuda-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args98-1.0-1.0-0.0-0.0-cpu-float64]
0.12s call     test/espnet2/asr/frontend/test_frontend.py::test_frontend_backward_multi_channel[True-False-False]
0.12s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1246-0.5-0.5-0.5-0.1-cuda-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args77-1.0-0.0-0.0-0.1-cpu-float64]
0.12s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict5]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args712-0.5-1.0-0.0-0.1-cuda-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args385-0.5-1.0-0.0-0.0-cpu-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args843-1.0-0.5-0.0-0.1-cuda-float16]
0.12s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_backward[True-GRU]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args825-0.5-1.0-0.5-0.1-cuda-float16]
0.12s call     test/espnet2/asr/encoder/test_transformer_encoder.py::test_Encoder_forward_backward[conv1d-conv2d]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args197-1.0-0.5-0.0-0.1-cpu-float64]
0.12s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict9]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args781-0.0-1.0-0.0-0.0-cuda-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1044-1.0-0.0-0.0-0.0-cuda-float16]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args178-0.5-1.0-0.5-0.1-cpu-float32]
0.12s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict14]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1072-1.0-1.0-0.0-0.1-cuda-float32]
0.12s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic11-recog_dic11]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args134-0.0-1.0-0.0-0.0-cpu-float64]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args754-1.0-1.0-0.5-0.1-cuda-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args196-1.0-0.5-0.0-0.1-cpu-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args292-1.0-0.0-0.0-0.1-cpu-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args82-1.0-0.0-0.5-0.1-cpu-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args37-0.5-0.0-0.0-0.0-cpu-float32]
0.12s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict37]
0.12s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict11]
0.12s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict15]
0.12s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict22]
0.12s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1198-0.0-0.0-0.5-0.1-cuda-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args83-1.0-0.0-0.5-0.1-cpu-float64]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1074-1.0-1.0-0.5-0.0-cuda-float16]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args789-0.0-1.0-0.5-0.1-cuda-float16]
0.12s call     test/test_beam_search.py::test_beam_search_equal[rnn-args541-0.0-0.0-0.0-0.0-cpu-float32]
0.12s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_backward[False-GRU]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1004-0.0-1.0-0.5-0.0-cuda-float64]
0.12s call     test/espnet2/asr/frontend/test_frontend.py::test_frontend_backward_multi_channel[False-False-False]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args732-1.0-0.5-0.0-0.0-cuda-float16]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1112-0.0-1.0-0.5-0.0-cuda-float64]
0.12s call     test/test_e2e_asr.py::test_multi_gpu_trainable[espnet.nets.pytorch_backend.e2e_asr]
0.12s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict3]
0.12s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict23]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args212-1.0-1.0-0.5-0.0-cpu-float64]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args104-1.0-1.0-0.5-0.0-cpu-float64]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args461-0.0-1.0-0.0-0.1-cpu-float64]
0.12s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict16]
0.12s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict10]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args842-1.0-0.5-0.0-0.0-cuda-float64]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args840-1.0-0.5-0.0-0.0-cuda-float16]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args299-1.0-0.0-0.5-0.1-cpu-float64]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1163-1.0-0.0-0.5-0.1-cuda-float64]
0.12s call     test/test_e2e_st.py::test_mtl_loss[vggblstmp]
0.12s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict21]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args839-1.0-0.0-0.5-0.1-cuda-float64]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args533-1.0-1.0-0.0-0.1-cpu-float64]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args720-1.0-0.0-0.0-0.0-cuda-float16]
0.12s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1233-0.5-0.0-0.5-0.1-cuda-float16]
0.12s call     test/test_lm.py::test_lm_trainable_and_decodable[transformer-lm_args28-cuda-float32]
0.12s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1235-0.5-0.0-0.5-0.1-cuda-float64]
0.12s call     test/test_beam_search.py::test_beam_search_equal[transformer-args939-1.0-0.0-0.0-0.1-cuda-float16]
0.11s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[tacotron2-model_dict25]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1050-1.0-0.0-0.5-0.0-cuda-float16]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1239-0.5-0.5-0.0-0.1-cuda-float16]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args314-1.0-1.0-0.0-0.0-cpu-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args301-1.0-0.5-0.0-0.0-cpu-float32]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args841-1.0-0.5-0.0-0.0-cuda-float32]
0.11s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict47]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args409-1.0-0.5-0.0-0.0-cpu-float32]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1237-0.5-0.5-0.0-0.0-cuda-float32]
0.11s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[None-False-True-gru]
0.11s call     test/espnet2/asr/encoder/test_transformer_encoder.py::test_Encoder_forward_backward[conv1d-linear-conv2d]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1177-1.0-1.0-0.0-0.0-cuda-float32]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args971-1.0-1.0-0.5-0.1-cuda-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args89-1.0-0.5-0.0-0.1-cpu-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args747-1.0-1.0-0.0-0.1-cuda-float16]
0.11s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[subsample1-False-True-gru]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args960-1.0-1.0-0.0-0.0-cuda-float16]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args879-0.0-0.5-0.0-0.1-cuda-float16]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1071-1.0-1.0-0.0-0.1-cuda-float16]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args427-1.0-1.0-0.5-0.0-cpu-float32]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1179-1.0-1.0-0.0-0.1-cuda-float16]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1153-1.0-0.0-0.0-0.0-cuda-float32]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args545-0.0-0.0-0.0-0.1-cpu-float64]
0.11s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict44]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args969-1.0-1.0-0.5-0.1-cuda-float16]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1193-0.0-0.0-0.0-0.1-cuda-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1229-0.5-0.0-0.0-0.1-cuda-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args419-1.0-0.5-0.5-0.1-cpu-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1169-1.0-0.5-0.0-0.1-cuda-float64]
0.11s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict21]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1059-1.0-0.5-0.0-0.1-cuda-float16]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1236-0.5-0.5-0.0-0.0-cuda-float16]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args188-1.0-0.0-0.5-0.0-cpu-float64]
0.11s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict48]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args984-0.0-0.5-0.0-0.0-cuda-float16]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args725-1.0-0.0-0.0-0.1-cuda-float64]
0.11s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict15]
0.11s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict43]
0.11s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict37]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args838-1.0-0.0-0.5-0.1-cuda-float32]
0.11s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[transformer-model_dict20]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args863-1.0-1.0-0.5-0.1-cuda-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args817-0.5-1.0-0.0-0.0-cuda-float32]
0.11s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict41]
0.11s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[tacotron2-model_dict14]
0.11s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic24-recog_dic24]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1234-0.5-0.0-0.5-0.1-cuda-float32]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1170-1.0-0.5-0.5-0.0-cuda-float16]
0.11s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[tacotron2-model_dict15]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1199-0.0-0.0-0.5-0.1-cuda-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args861-1.0-1.0-0.5-0.1-cuda-float16]
0.11s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict26]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1230-0.5-0.0-0.5-0.0-cuda-float16]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1064-1.0-0.5-0.5-0.0-cuda-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args323-1.0-1.0-0.5-0.1-cpu-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args296-1.0-0.0-0.5-0.0-cpu-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args745-1.0-1.0-0.0-0.0-cuda-float32]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args530-1.0-1.0-0.0-0.0-cpu-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1195-0.0-0.0-0.5-0.0-cuda-float32]
0.11s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1048-1.0-0.0-0.0-0.1-cuda-float32]
0.11s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict14]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1192-0.0-0.0-0.0-0.1-cuda-float32]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1232-0.5-0.0-0.5-0.0-cuda-float64]
0.11s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1191-0.0-0.0-0.0-0.1-cuda-float16]
0.11s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict49]
0.11s call     test/test_e2e_asr.py::test_zero_length_target[vggblstmp]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args92-1.0-0.5-0.5-0.0-cpu-float64]
0.10s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict40]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1185-1.0-1.0-0.5-0.1-cuda-float16]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1049-1.0-0.0-0.0-0.1-cuda-float64]
0.10s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict20]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args837-1.0-0.0-0.5-0.1-cuda-float16]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1228-0.5-0.0-0.0-0.1-cuda-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args407-1.0-0.0-0.5-0.1-cpu-float64]
0.10s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[None-True-True-gru]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1224-0.5-0.0-0.0-0.0-cuda-float16]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1057-1.0-0.5-0.0-0.0-cuda-float32]
0.10s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic10-recog_dic10]
0.10s call     test/test_e2e_st.py::test_mtl_loss[blstmp]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args397-1.0-0.0-0.0-0.0-cpu-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args753-1.0-1.0-0.5-0.1-cuda-float16]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args828-1.0-0.0-0.0-0.0-cuda-float16]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args293-1.0-0.0-0.0-0.1-cpu-float64]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args203-1.0-0.5-0.5-0.1-cpu-float64]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args599-0.5-0.5-0.5-0.1-cpu-float64]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1171-1.0-0.5-0.5-0.0-cuda-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1078-1.0-1.0-0.5-0.1-cuda-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args577-0.5-0.0-0.0-0.0-cpu-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1197-0.0-0.0-0.5-0.1-cuda-float16]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1077-1.0-1.0-0.5-0.1-cuda-float16]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict10]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args596-0.5-0.5-0.5-0.0-cpu-float64]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args598-0.5-0.5-0.5-0.1-cpu-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1231-0.5-0.0-0.5-0.0-cuda-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args404-1.0-0.0-0.5-0.0-cpu-float64]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args854-1.0-1.0-0.0-0.0-cuda-float64]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args229-0.0-0.5-0.0-0.0-cpu-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1164-1.0-0.5-0.0-0.0-cuda-float16]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args547-0.0-0.0-0.5-0.0-cpu-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args76-1.0-0.0-0.0-0.1-cpu-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args322-1.0-1.0-0.5-0.1-cpu-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1176-1.0-1.0-0.0-0.0-cuda-float16]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict9]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict11]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args595-0.5-0.5-0.5-0.0-cpu-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1194-0.0-0.0-0.5-0.0-cuda-float16]
0.10s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic42-recog_dic42]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict11]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args715-0.5-1.0-0.5-0.0-cuda-float32]
0.10s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict13]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1175-1.0-0.5-0.5-0.1-cuda-float64]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args752-1.0-1.0-0.5-0.0-cuda-float64]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args852-1.0-1.0-0.0-0.0-cuda-float16]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1167-1.0-0.5-0.0-0.1-cuda-float16]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args416-1.0-0.5-0.5-0.0-cpu-float64]
0.10s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_gpu_trainable_and_decodable[model_dict3-inference_dict3]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict1]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1168-1.0-0.5-0.0-0.1-cuda-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args86-1.0-0.5-0.0-0.0-cpu-float64]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict7]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict3]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict12]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args846-1.0-0.5-0.5-0.0-cuda-float16]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[tacotron2-model_dict15]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args593-0.5-0.5-0.0-0.1-cpu-float64]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict10]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict0]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1070-1.0-1.0-0.0-0.0-cuda-float64]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict4]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args592-0.5-0.5-0.0-0.1-cpu-float32]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1196-0.0-0.0-0.5-0.0-cuda-float64]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args122-0.0-0.5-0.0-0.0-cpu-float64]
0.10s call     test/test_beam_search.py::test_beam_search_equal[rnn-args590-0.5-0.5-0.0-0.0-cpu-float64]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict6]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args398-1.0-0.0-0.0-0.0-cpu-float64]
0.10s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict1]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict0]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args941-1.0-0.0-0.0-0.1-cuda-float64]
0.10s call     test/test_lm.py::test_lm_trainable_and_decodable[transformer-lm_args32-cpu-float64]
0.10s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict2]
0.10s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict58]
0.10s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic41-recog_dic41]
0.10s call     test/test_beam_search.py::test_beam_search_equal[transformer-args829-1.0-0.0-0.0-0.0-cuda-float32]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict8]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict9]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args860-1.0-1.0-0.5-0.0-cuda-float64]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args307-1.0-0.5-0.5-0.0-cpu-float32]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict5]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict2]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args749-1.0-1.0-0.0-0.1-cuda-float64]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict1]
0.09s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic35-recog_dic35]
0.09s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict38]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict12]
0.09s call     test/test_e2e_asr_mulenc.py::test_multi_gpu_trainable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3]
0.09s call     test/espnet2/asr/encoder/test_transformer_encoder.py::test_Encoder_forward_backward[conv1d-linear]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args934-0.5-1.0-0.5-0.1-cuda-float32]
0.09s call     test/test_e2e_tts_transformer.py::test_forward_and_inference_are_equal[model_dict6]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_multi_gpu_trainable[transformer-model_dict13]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict13]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args424-1.0-1.0-0.0-0.1-cpu-float32]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict4]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args743-1.0-0.5-0.5-0.1-cuda-float64]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args319-1.0-1.0-0.5-0.0-cpu-float32]
0.09s call     test/test_train_dtype.py::test_train_pytorch_dtype[float32-cpu-transformer-conf2]
0.09s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1188-0.0-0.0-0.0-0.0-cuda-float16]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args295-1.0-0.0-0.5-0.0-cpu-float32]
0.09s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict44]
0.09s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1227-0.5-0.0-0.0-0.1-cuda-float16]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args661-0.0-0.5-0.0-0.0-cuda-float32]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict5]
0.09s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_gpu_trainable_and_decodable[model_dict7-inference_dict7]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict3]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args400-1.0-0.0-0.0-0.1-cpu-float32]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1183-1.0-1.0-0.5-0.0-cuda-float32]
0.09s call     test/test_beam_search.py::test_beam_search_equal[rnn-args589-0.5-0.5-0.0-0.0-cpu-float32]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict7]
0.09s call     test/test_e2e_tts_transformer.py::test_forward_and_inference_are_equal[model_dict5]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict6]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args97-1.0-1.0-0.0-0.0-cpu-float32]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args926-0.5-1.0-0.0-0.0-cuda-float64]
0.09s call     test/espnet2/iterators/test_chunk_iter_factory.py::test_ChunkIterFactory
0.09s call     test/test_lm.py::test_lm_trainable_and_decodable[transformer-lm_args29-cuda-float64]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_gpu_trainable_and_decodable[transformer-model_dict8]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args933-0.5-1.0-0.5-0.1-cuda-float16]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args731-1.0-0.0-0.5-0.1-cuda-float64]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args22-0.0-0.5-0.5-0.1-cpu-float32]
0.09s call     test/test_beam_search.py::test_beam_search_equal[rnn-args551-0.0-0.0-0.5-0.1-cpu-float64]
0.09s call     test/test_e2e_tts_transformer.py::test_forward_and_inference_are_equal[model_dict0]
0.09s call     test/test_beam_search.py::test_beam_search_equal[rnn-args587-0.5-0.0-0.5-0.1-cpu-float64]
0.09s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1226-0.5-0.0-0.0-0.0-cuda-float64]
0.09s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1225-0.5-0.0-0.0-0.0-cuda-float32]
0.09s call     test/test_beam_search.py::test_beam_search_equal[rnn-args550-0.0-0.0-0.5-0.1-cpu-float32]
0.09s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict6]
0.09s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict11]
0.09s call     test/test_beam_search.py::test_beam_search_equal[rnn-args584-0.5-0.0-0.5-0.0-cpu-float64]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args289-1.0-0.0-0.0-0.0-cpu-float32]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args194-1.0-0.5-0.0-0.0-cpu-float64]
0.09s call     test/test_e2e_tts_transformer.py::test_forward_and_inference_are_equal[model_dict4]
0.09s call     test/test_beam_search.py::test_beam_search_equal[rnn-args583-0.5-0.0-0.5-0.0-cpu-float32]
0.09s call     test/test_beam_search.py::test_beam_search_equal[rnn-args586-0.5-0.0-0.5-0.1-cpu-float32]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args954-1.0-0.5-0.5-0.0-cuda-float16]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args410-1.0-0.5-0.0-0.0-cpu-float64]
0.09s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic16-recog_dic16]
0.09s call     test/test_e2e_tts_transformer.py::test_forward_and_inference_are_equal[model_dict3]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1068-1.0-1.0-0.0-0.0-cuda-float16]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args724-1.0-0.0-0.0-0.1-cuda-float32]
0.09s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_trainable_and_decodable[tacotron2-model_dict24]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args536-1.0-1.0-0.5-0.0-cpu-float64]
0.09s call     test/test_lm.py::test_lm_trainable_and_decodable[seq_rnn-lm_args15-cuda-float16]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args678-0.0-1.0-0.5-0.0-cuda-float16]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1069-1.0-1.0-0.0-0.0-cuda-float32]
0.09s call     test/test_beam_search.py::test_beam_search_equal[transformer-args885-0.0-0.5-0.5-0.1-cuda-float16]
0.09s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[subsample1-True-True-lstm]
0.09s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict63]
0.09s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_multi_gpu_trainable[model_dict4]
0.08s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic39-recog_dic39]
0.08s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict29]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1144-0.5-1.0-0.0-0.1-cuda-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args742-1.0-0.5-0.5-0.1-cuda-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1184-1.0-1.0-0.5-0.0-cuda-float64]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args68-0.5-1.0-0.5-0.0-cpu-float64]
0.08s call     test/test_train_dtype.py::test_train_pytorch_dtype[float32-cpu-transformer-conf8]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args511-1.0-0.0-0.5-0.0-cpu-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[rnn-args542-0.0-0.0-0.0-0.0-cpu-float64]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args855-1.0-1.0-0.0-0.1-cuda-float16]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args751-1.0-1.0-0.5-0.0-cuda-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1076-1.0-1.0-0.5-0.0-cuda-float64]
0.08s call     test/test_beam_search.py::test_beam_search_equal[rnn-args581-0.5-0.0-0.0-0.1-cpu-float64]
0.08s call     test/test_e2e_st.py::test_sortagrad_trainable[pytorch]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args107-1.0-1.0-0.5-0.1-cpu-float64]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args88-1.0-0.5-0.0-0.1-cpu-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args830-1.0-0.0-0.0-0.0-cuda-float64]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args750-1.0-1.0-0.5-0.0-cuda-float16]
0.08s call     test/espnet2/layers/test_stft.py::test_backward_leaf_in
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args527-1.0-0.5-0.5-0.1-cpu-float64]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args940-1.0-0.0-0.0-0.1-cuda-float32]
0.08s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_gpu_trainable_and_decodable[model_dict4-inference_dict4]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args209-1.0-1.0-0.0-0.1-cpu-float64]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args532-1.0-1.0-0.0-0.1-cpu-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[rnn-args578-0.5-0.0-0.0-0.0-cpu-float64]
0.08s call     test/test_lm.py::test_lm_trainable_and_decodable[transformer-lm_args31-cpu-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[rnn-args580-0.5-0.0-0.0-0.1-cpu-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args844-1.0-0.5-0.0-0.1-cuda-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args740-1.0-0.5-0.5-0.0-cuda-float64]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1041-0.5-1.0-0.5-0.1-cuda-float16]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args428-1.0-1.0-0.5-0.0-cpu-float64]
0.08s call     test/test_beam_search.py::test_beam_search_equal[rnn-args544-0.0-0.0-0.0-0.1-cpu-float32]
0.08s call     test/test_lm.py::test_lm_trainable_and_decodable[transformer-lm_args25-cpu-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1182-1.0-1.0-0.5-0.0-cuda-float16]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1066-1.0-0.5-0.5-0.1-cuda-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args964-1.0-1.0-0.0-0.1-cuda-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1060-1.0-0.5-0.0-0.1-cuda-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1063-1.0-0.5-0.5-0.0-cuda-float32]
0.08s call     test/test_e2e_asr.py::test_zero_length_target[blstmp]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1186-1.0-1.0-0.5-0.1-cuda-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args187-1.0-0.0-0.5-0.0-cpu-float32]
0.08s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict10]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args746-1.0-1.0-0.0-0.0-cuda-float64]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args308-1.0-0.5-0.5-0.0-cpu-float64]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1180-1.0-1.0-0.0-0.1-cuda-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args418-1.0-0.5-0.5-0.1-cpu-float32]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args215-1.0-1.0-0.5-0.1-cpu-float64]
0.08s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1178-1.0-1.0-0.0-0.0-cuda-float64]
0.08s call     test/test_e2e_asr_mulenc.py::test_gpu_trainable[espnet.nets.pytorch_backend.e2e_asr_mulenc-3]
0.08s call     test/espnet2/fileio/test_datadir_writer.py::test_DatadirWriter
0.08s call     test/test_lm.py::test_lm_trainable_and_decodable[default-lm_args5-cuda-float64]
0.08s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict12]
0.07s call     test/test_lm.py::test_lm_trainable_and_decodable[transformer-lm_args27-cuda-float16]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1166-1.0-0.5-0.0-0.0-cuda-float64]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1154-1.0-0.0-0.0-0.0-cuda-float64]
0.07s call     test/espnet2/text/test_phoneme_tokenizer.py::test_text2tokens[g2p_en]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args94-1.0-0.5-0.5-0.1-cpu-float32]
0.07s call     test/test_lm.py::test_lm_trainable_and_decodable[seq_rnn-lm_args16-cuda-float32]
0.07s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[subsample1-True-True-gru]
0.07s call     test/espnet2/train/test_dataset.py::test_ESPnetDataset_sound_scp
0.07s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict35]
0.07s call     test/test_lm.py::test_lm_trainable_and_decodable[default-lm_args4-cuda-float32]
0.07s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict31]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args733-1.0-0.5-0.0-0.0-cuda-float32]
0.07s call     test/test_lm.py::test_lm_trainable_and_decodable[seq_rnn-lm_args23-cuda-float64]
0.07s call     test/test_lm.py::test_lm_trainable_and_decodable[default-lm_args10-cuda-float32]
0.07s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict59]
0.07s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict36]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args526-1.0-0.5-0.5-0.1-cpu-float32]
0.07s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic17-recog_dic17]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args514-1.0-0.0-0.5-0.1-cpu-float32]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args457-0.0-1.0-0.0-0.0-cpu-float32]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args996-0.0-1.0-0.0-0.0-cuda-float16]
0.07s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-multi_head_multi_res_loc]
0.07s call     test/test_e2e_asr_mulenc.py::test_multi_gpu_trainable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args529-1.0-1.0-0.0-0.0-cpu-float32]
0.07s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-multi_head_loc]
0.07s call     test/test_train_dtype.py::test_train_pytorch_dtype[float32-cpu-transformer-conf14]
0.07s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.chainer_backend.e2e_asr-location]
0.07s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict2]
0.07s call     test/test_e2e_asr_mulenc.py::test_torch_save_and_load[3]
0.07s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_multi_gpu_trainable[model_dict2]
0.07s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict30]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args721-1.0-0.0-0.0-0.0-cuda-float32]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args539-1.0-1.0-0.5-0.1-cpu-float64]
0.07s call     test/test_lm.py::test_lm_trainable_and_decodable[seq_rnn-lm_args19-cpu-float32]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args955-1.0-0.5-0.5-0.0-cuda-float32]
0.07s call     test/espnet2/asr/encoder/test_transformer_encoder.py::test_Encoder_output_size
0.07s call     test/test_lm.py::test_lm_trainable_and_decodable[default-lm_args3-cuda-float16]
0.07s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic23-recog_dic23]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args538-1.0-1.0-0.5-0.1-cpu-float32]
0.07s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict0]
0.07s call     test/test_lm.py::test_lm_trainable_and_decodable[default-lm_args11-cuda-float64]
0.07s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_gpu_trainable_and_decodable[model_dict11-inference_dict11]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args181-1.0-0.0-0.0-0.0-cpu-float32]
0.07s call     test/test_lm.py::test_lm_trainable_and_decodable[default-lm_args1-cpu-float32]
0.07s call     test/test_lm.py::test_lm_trainable_and_decodable[seq_rnn-lm_args22-cuda-float32]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args106-1.0-1.0-0.5-0.1-cpu-float32]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args950-1.0-0.5-0.0-0.0-cuda-float64]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args512-1.0-0.0-0.5-0.0-cpu-float64]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args202-1.0-0.5-0.5-0.1-cpu-float32]
0.07s call     test/espnet2/asr/encoder/test_transformer_encoder.py::test_Encoder_forward_backward[conv1d-None]
0.07s call     test/test_lm.py::test_lm_trainable_and_decodable[seq_rnn-lm_args17-cuda-float64]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args388-0.5-1.0-0.0-0.1-cpu-float32]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args727-1.0-0.0-0.5-0.0-cuda-float32]
0.07s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1058-1.0-0.5-0.0-0.0-cuda-float64]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args214-1.0-1.0-0.5-0.1-cpu-float32]
0.06s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic21-recog_dic21]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args136-0.0-1.0-0.0-0.1-cpu-float32]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args310-1.0-0.5-0.5-0.1-cpu-float32]
0.06s call     test/test_lm.py::test_lm_trainable_and_decodable[default-lm_args9-cuda-float16]
0.06s call     test/espnet2/asr/encoder/test_transformer_encoder.py::test_Encoder_forward_backward[conv1d-embed]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1172-1.0-0.5-0.5-0.0-cuda-float64]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1174-1.0-0.5-0.5-0.1-cuda-float32]
0.06s call     test/espnet2/tasks/test_asr.py::test_main_print_config
0.06s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic9-recog_dic9]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict0]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args713-0.5-1.0-0.0-0.1-cuda-float64]
0.06s call     test/test_lm.py::test_lm_trainable_and_decodable[transformer-lm_args26-cpu-float64]
0.06s call     test/espnet2/asr/frontend/test_frontend.py::test_frontend_repr
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict4]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args942-1.0-0.0-0.5-0.0-cuda-float16]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args124-0.0-0.5-0.0-0.1-cpu-float32]
0.06s call     test/test_multi_spkrs.py::test_pit_process[vggblstmp-lstm-2-espnet.nets.pytorch_backend.e2e_asr_mix-0]
0.06s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_multi_gpu_trainable[model_dict1]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args709-0.5-1.0-0.0-0.0-cuda-float32]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict1]
0.06s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_score[True-LSTM]
0.06s call     test/test_lm.py::test_lm_trainable_and_decodable[default-lm_args8-cpu-float64]
0.06s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic19-recog_dic19]
0.06s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-location_recurrent]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args850-1.0-0.5-0.5-0.1-cuda-float32]
0.06s call     test/test_e2e_asr_mulenc.py::test_gpu_trainable[espnet.nets.pytorch_backend.e2e_asr_mulenc-2]
0.06s call     test/test_lm.py::test_lm_trainable_and_decodable[seq_rnn-lm_args21-cuda-float16]
0.06s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[None-False-False-lstm]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args967-1.0-1.0-0.5-0.0-cuda-float32]
0.06s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[None-True-False-lstm]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict6]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict9]
0.06s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_multi_gpu_trainable[model_dict8]
0.06s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-coverage_location]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict10]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args948-1.0-0.5-0.0-0.0-cuda-float16]
0.06s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-location2d]
0.06s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-multi_head_add]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args857-1.0-1.0-0.0-0.1-cuda-float64]
0.06s call     test/test_lm.py::test_lm_trainable_and_decodable[seq_rnn-lm_args14-cpu-float64]
0.06s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-location]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args889-0.0-1.0-0.0-0.0-cuda-float32]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args431-1.0-1.0-0.5-0.1-cpu-float64]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict3]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict2]
0.06s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic7-recog_dic7]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args726-1.0-0.0-0.5-0.0-cuda-float16]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1073-1.0-1.0-0.0-0.1-cuda-float64]
0.06s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_gpu_trainable_and_decodable[model_dict1-inference_dict1]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict5]
0.06s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-multi_head_dot]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict4]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict7]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict0]
0.06s call     test/test_lm.py::test_lm_trainable_and_decodable[default-lm_args7-cpu-float32]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args95-1.0-0.5-0.5-0.1-cpu-float64]
0.06s call     test/espnet2/tasks/test_asr.py::test_print_config_and_load_it
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args182-1.0-0.0-0.0-0.0-cpu-float64]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_gpu_trainable_and_decodable[model_dict9]
0.06s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_multi_gpu_trainable[model_dict3]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict8]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args856-1.0-1.0-0.0-0.1-cuda-float32]
0.06s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict12]
0.06s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict11]
0.06s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_gpu_trainable_and_decodable[model_dict8-inference_dict8]
0.06s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[subsample1-False-False-lstm]
0.06s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[None-True-False-gru]
0.06s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict10]
0.06s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_multi_gpu_trainable[model_dict0]
0.06s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-coverage]
0.06s call     test/test_beam_search.py::test_beam_search_equal[transformer-args723-1.0-0.0-0.0-0.1-cuda-float16]
0.05s call     test/test_beam_search.py::test_beam_search_equal[transformer-args317-1.0-1.0-0.0-0.1-cpu-float64]
0.05s call     test/espnet2/fileio/test_npy_scp.py::test_NpyScpReader
0.05s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1181-1.0-1.0-0.0-0.1-cuda-float64]
0.05s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_multi_gpu_trainable[model_dict6]
0.05s call     test/espnet2/asr/decoder/test_transformer_decoder.py::test_TransformerDecoder_backward[True-True-linear]
0.05s call     test/espnet2/asr/encoder/test_vgg_rnn_encoder.py::test_Encoder_forward_backward[False-False-gru]
0.05s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-add]
0.05s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_gpu_trainable_and_decodable[model_dict10-inference_dict10]
0.05s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_gpu_trainable_and_decodable[model_dict9-inference_dict9]
0.05s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-dot]
0.05s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_multi_gpu_trainable[model_dict7]
0.05s call     test/test_e2e_tts_transformer.py::test_transformer_multi_gpu_trainable[model_dict11]
0.05s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict5]
0.05s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_gpu_trainable_and_decodable[model_dict6-inference_dict6]
0.05s call     test/test_lm.py::test_lm_trainable_and_decodable[seq_rnn-lm_args20-cpu-float64]
0.05s call     test/test_beam_search.py::test_beam_search_equal[transformer-args421-1.0-1.0-0.0-0.0-cpu-float32]
0.05s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict33]
0.05s call     test/test_beam_search.py::test_beam_search_equal[transformer-args206-1.0-1.0-0.0-0.0-cpu-float64]
0.05s call     test/test_beam_search.py::test_beam_search_equal[transformer-args346-0.0-0.5-0.5-0.1-cpu-float32]
0.05s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.chainer_backend.e2e_asr-noatt]
0.05s call     test/test_beam_search.py::test_beam_search_equal[transformer-args67-0.5-1.0-0.5-0.0-cpu-float32]
0.05s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic6-recog_dic6]
0.05s call     test/test_lm.py::test_lm_trainable_and_decodable[default-lm_args2-cpu-float64]
0.05s call     test/test_beam_search.py::test_beam_search_equal[transformer-args927-0.5-1.0-0.0-0.1-cuda-float16]
0.05s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-3-noatt]
0.05s call     test/espnet2/samplers/test_build_batch_sampler.py::test_build_batch_sampler[unsorted]
0.05s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-multi_head_loc]
0.05s call     test/espnet2/asr/encoder/test_transformer_encoder.py::test_Encoder_forward_backward[conv1d-linear-linear]
0.05s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-multi_head_multi_res_loc]
0.05s call     test/test_beam_search.py::test_beam_search_equal[transformer-args422-1.0-1.0-0.0-0.0-cpu-float64]
0.05s call     test/espnet2/asr/encoder/test_transformer_encoder.py::test_Encoder_forward_backward[conv1d-linear-embed]
0.05s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict4]
0.05s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict34]
0.05s call     test/espnet2/asr/encoder/test_transformer_encoder.py::test_Encoder_forward_backward[conv1d-linear-None]
0.05s call     test/test_lm.py::test_lm_trainable_and_decodable[seq_rnn-lm_args13-cpu-float32]
0.05s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[subsample1-False-False-gru]
0.05s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[None-False-False-gru]
0.05s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_gpu_trainable_and_decodable[model_dict0-inference_dict0]
0.05s call     test/test_train_dtype.py::test_train_pytorch_dtype[float16-cuda-transformer-conf1]
0.05s call     test/test_beam_search.py::test_beam_search_equal[transformer-args958-1.0-0.5-0.5-0.1-cuda-float32]
0.05s call     test/test_e2e_asr_mulenc.py::test_torch_save_and_load[2]
0.05s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.chainer_backend.e2e_asr-dot]
0.05s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict12]
0.05s call     test/espnet2/fileio/test_npy_scp.py::test_SoundScpWriter
0.05s call     test/espnet2/tasks/test_tts.py::test_print_config_and_load_it
0.05s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict3]
0.05s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_gpu_trainable_and_decodable[model_dict2-inference_dict2]
0.05s call     test/test_beam_search.py::test_beam_search_equal[transformer-args835-1.0-0.0-0.5-0.0-cuda-float32]
0.05s call     test/espnet2/asr/decoder/test_rnn_decoder.py::test_RNNDecoder_backward[lstm-True]
0.05s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1075-1.0-1.0-0.5-0.0-cuda-float32]
0.05s call     test/test_beam_search.py::test_beam_search_equal[transformer-args895-0.0-1.0-0.5-0.0-cuda-float32]
0.05s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_score[False-LSTM]
0.05s call     test/espnet2/asr/encoder/test_vgg_rnn_encoder.py::test_Encoder_forward_backward[False-False-lstm]
0.05s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[subsample1-True-False-lstm]
0.05s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict2]
0.05s call     test/espnet2/main_funcs/test_calculate_all_attentions.py::test_calculate_all_attentions[multi_head_multi_res_loc]
0.04s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict9]
0.04s call     test/espnet2/main_funcs/test_calculate_all_attentions.py::test_calculate_all_attentions[multi_head_loc]
0.04s call     test/espnet2/asr/test_ctc.py::test_ctc_forward_backward[builtin]
0.04s call     test/test_e2e_asr.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_asr-model_dict32]
0.04s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict8]
0.04s call     test/test_beam_search.py::test_beam_search_equal[transformer-args524-1.0-0.5-0.5-0.0-cpu-float64]
0.04s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-location_recurrent]
0.04s call     test/test_e2e_tts_transformer.py::test_forward_and_inference_are_equal[model_dict1]
0.04s call     test/test_e2e_tts_fastspeech.py::test_initialization[model_dict8]
0.04s call     test/test_e2e_tts_fastspeech.py::test_initialization[model_dict7]
0.04s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-coverage_location]
0.04s call     test/test_beam_search.py::test_beam_search_equal[transformer-args523-1.0-0.5-0.5-0.0-cpu-float32]
0.04s call     test/test_e2e_tts_fastspeech.py::test_initialization[model_dict0]
0.04s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-location]
0.04s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-location2d]
0.04s setup    test/espnet2/text/test_sentencepiece_tokenizer.py::test_repr
0.04s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-multi_head_add]
0.04s call     test/test_train_dtype.py::test_train_pytorch_dtype[float16-cuda-transformer-conf7]
0.04s call     test/test_train_dtype.py::test_train_pytorch_dtype[float16-cuda-transformer-conf13]
0.04s call     test/test_beam_search.py::test_beam_search_equal[transformer-args667-0.0-0.5-0.5-0.0-cuda-float32]
0.04s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_backward[True-RNN_TANH]
0.04s call     test/test_e2e_tts_fastspeech.py::test_initialization[model_dict6]
0.04s call     test/test_train_dtype.py::test_train_pytorch_dtype[float64-cuda-transformer-conf5]
0.04s call     test/test_train_dtype.py::test_train_pytorch_dtype[float32-cuda-transformer-conf3]
0.04s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_backward[False-RNN_TANH]
0.04s call     test/test_train_dtype.py::test_train_pytorch_dtype[float32-cuda-transformer-conf15]
0.04s call     test/test_train_dtype.py::test_train_pytorch_dtype[float64-cuda-transformer-conf17]
0.04s call     test/test_train_dtype.py::test_train_pytorch_dtype[float64-cuda-transformer-conf11]
0.04s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict6]
0.04s call     test/test_e2e_tts_fastspeech.py::test_initialization[model_dict4]
0.04s call     test/test_train_dtype.py::test_train_pytorch_dtype[float32-cuda-transformer-conf9]
0.04s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-multi_head_dot]
0.04s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_forward_backward[subsample1-True-False-gru]
0.04s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict1]
0.04s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_backward[True-RNN_RELU]
0.04s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_backward[False-RNN_RELU]
0.04s call     test/test_e2e_tts_fastspeech.py::test_initialization[model_dict2]
0.04s call     test/test_beam_search.py::test_beam_search_equal[transformer-args965-1.0-1.0-0.0-0.1-cuda-float64]
0.04s call     test/test_e2e_tts_fastspeech.py::test_initialization[model_dict1]
0.04s call     test/test_e2e_tts_fastspeech.py::test_initialization[model_dict3]
0.04s call     test/test_e2e_tts_fastspeech.py::test_initialization[model_dict5]
0.04s call     test/test_beam_search.py::test_beam_search_equal[transformer-args287-0.5-1.0-0.5-0.1-cpu-float64]
0.04s call     test/espnet2/train/test_iterable_dataset.py::test_ESPnetDataset_sound_scp
0.04s call     test/test_beam_search.py::test_beam_search_equal[transformer-args35-0.0-1.0-0.5-0.1-cpu-float64]
0.04s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-noatt]
0.04s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-coverage]
0.04s call     test/test_e2e_tts_transformer.py::test_forward_and_inference_are_equal[model_dict2]
0.04s call     test/test_train_dtype.py::test_train_pytorch_dtype[float32-cpu-rnn-conf26]
0.04s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict6]
0.04s call     test/test_beam_search.py::test_beam_search_equal[transformer-args880-0.0-0.5-0.0-0.1-cuda-float32]
0.04s call     test/test_beam_search.py::test_beam_search_equal[transformer-args394-0.5-1.0-0.5-0.1-cpu-float32]
0.04s call     test/test_sentencepiece.py::test_spm_compatibility
0.04s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-add]
0.04s call     test/test_beam_search.py::test_beam_search_equal[transformer-args503-0.5-1.0-0.5-0.1-cpu-float64]
0.04s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_gpu_trainable_and_decodable[model_dict5-inference_dict5]
0.04s call     test/test_beam_search.py::test_beam_search_equal[transformer-args20-0.0-0.5-0.5-0.0-cpu-float64]
0.04s call     test/test_e2e_asr_mulenc.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr_mulenc-2-dot]
0.04s call     test/test_beam_search.py::test_beam_search_equal[transformer-args949-1.0-0.5-0.0-0.0-cuda-float32]
0.04s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict7]
0.04s call     test/test_e2e_mt.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_mt-model_dict7]
0.04s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_score[True-GRU]
0.04s call     test/test_e2e_tts_tacotron2.py::test_tacotron2_multi_gpu_trainable[model_dict5]
0.04s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_score[False-GRU]
0.03s call     test/test_e2e_st.py::test_multi_gpu_trainable[espnet.nets.pytorch_backend.e2e_st]
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float64-cuda-rnn-conf23]
0.03s call     test/espnet2/tasks/test_lm.py::test_print_config_and_load_it
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args957-1.0-0.5-0.5-0.1-cuda-float16]
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args966-1.0-1.0-0.5-0.0-cuda-float16]
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float32-cuda-rnn-conf33]
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float32-cuda-rnn-conf27]
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args970-1.0-1.0-0.5-0.1-cuda-float32]
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args140-0.0-1.0-0.5-0.0-cpu-float64]
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float64-cpu-rnn-conf34]
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float64-cpu-rnn-conf28]
0.03s call     test/espnet2/asr/decoder/test_transformer_decoder.py::test_TransformerDecoder_backward[False-True-linear]
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args894-0.0-1.0-0.5-0.0-cuda-float16]
0.03s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict4]
0.03s call     test/espnet2/asr/decoder/test_transformer_decoder.py::test_TransformerDecoder_backward[False-False-linear]
0.03s call     test/espnet2/asr/decoder/test_transformer_decoder.py::test_TransformerDecoder_backward[True-False-embed]
0.03s call     test/espnet2/asr/decoder/test_transformer_decoder.py::test_TransformerDecoder_backward[False-True-embed]
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float64-cpu-rnn-conf22]
0.03s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict5]
0.03s call     test/espnet2/optimizers/test_sgd.py::test_SGD
0.03s call     test/espnet2/asr/decoder/test_transformer_decoder.py::test_TransformerDecoder_backward[False-False-embed]
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float32-cpu-rnn-conf32]
0.03s call     test/espnet2/main_funcs/test_calculate_all_attentions.py::test_calculate_all_attentions[multi_head_add]
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float32-cuda-rnn-conf21]
0.03s call     test/test_e2e_asr_transformer.py::test_transformer_parallel
0.03s call     test/espnet2/asr/encoder/test_vgg_rnn_encoder.py::test_Encoder_output_size
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float32-cpu-rnn-conf20]
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float64-cuda-rnn-conf35]
0.03s call     test/espnet2/tts/feats_extract/test_log_spectrogram.py::test_backward_leaf_in
0.03s call     test/espnet2/bin/test_tts_train.py::test_get_parser
0.03s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic18-recog_dic18]
0.03s call     test/espnet2/tts/feats_extract/test_log_mel_fbank.py::test_backward_leaf_in
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float64-cuda-rnn-conf29]
0.03s call     test/espnet2/main_funcs/test_calculate_all_attentions.py::test_calculate_all_attentions[multi_head_dot]
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1165-1.0-0.5-0.0-0.0-cuda-float32]
0.03s call     test/espnet2/asr/decoder/test_transformer_decoder.py::test_TransformerDecoder_backward[True-False-linear]
0.03s call     test/espnet2/tasks/test_abs_task.py::test_main_with_no_args
0.03s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic8-recog_dic8]
0.03s call     test/espnet2/asr/decoder/test_transformer_decoder.py::test_TransformerDecoder_backward[True-True-embed]
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args679-0.0-1.0-0.5-0.0-cuda-float32]
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float16-cuda-rnn-conf25]
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float16-cuda-rnn-conf31]
0.03s call     test/espnet2/bin/test_asr_train.py::test_main
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float64-cpu-transformer-conf4]
0.03s call     test/test_e2e_asr.py::test_gpu_trainable[espnet.nets.pytorch_backend.e2e_asr]
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float64-cpu-transformer-conf10]
0.03s call     test/espnet2/tasks/test_abs_task.py::test_print_config_and_load_it
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float64-cpu-transformer-conf16]
0.03s call     test/espnet2/asr/specaug/test_specaug.py::test_SpecAuc[False-True-False]
0.03s call     test/espnet2/tts/feats_extract/test_log_spectrogram.py::test_forward
0.03s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr-dot]
0.03s call     test/espnet2/bin/test_asr_train.py::test_get_parser
0.03s call     test/test_train_dtype.py::test_train_pytorch_dtype[float16-cuda-rnn-conf19]
0.03s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr-noatt]
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args947-1.0-0.0-0.5-0.1-cuda-float64]
0.03s call     test/espnet2/tts/feats_extract/test_log_mel_fbank.py::test_forward
0.03s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_trainable_and_decodable[train_dic20-recog_dic20]
0.03s call     test/espnet2/train/test_dataset.py::test_ESPnetDataset_rand_float
0.03s call     test/espnet2/asr/encoder/test_rnn_encoder.py::test_Encoder_output_size
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args466-0.0-1.0-0.5-0.1-cpu-float32]
0.03s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr-multi_head_multi_res_loc]
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args886-0.0-0.5-0.5-0.1-cuda-float32]
0.03s call     test/espnet2/train/test_dataset.py::test_ESPnetDataset_rand_int
0.03s call     test/espnet2/text/test_token_id_converter.py::test_tokens2ids
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1159-1.0-0.0-0.5-0.0-cuda-float32]
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args988-0.0-0.5-0.0-0.1-cuda-float32]
0.03s call     test/espnet2/bin/test_lm_train.py::test_get_parser
0.03s call     test/test_beam_search.py::test_beam_search_equal[transformer-args959-1.0-0.5-0.5-0.1-cuda-float64]
0.03s setup    test/espnet2/text/test_word_tokenizer.py::test_repr[None]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args776-0.0-0.5-0.5-0.0-cuda-float64]
0.02s call     test/espnet2/torch_utils/test_initialize.py::test_initialize[chainer]
0.02s call     test/test_e2e_mt.py::test_loss[blstm]
0.02s call     test/espnet2/samplers/test_build_batch_sampler.py::test_build_batch_sampler[numel]
0.02s call     test/espnet2/main_funcs/test_calculate_all_attentions.py::test_calculate_all_attentions[location_recurrent]
0.02s call     test/espnet2/samplers/test_build_batch_sampler.py::test_build_batch_sampler[folded]
0.02s call     test/espnet2/samplers/test_build_batch_sampler.py::test_build_batch_sampler[length]
0.02s call     test/espnet2/torch_utils/test_forward_adaptor.py::test_ForwardAdaptor
0.02s call     test/espnet2/fileio/test_npy_scp.py::test_NpyScpWriter
0.02s call     test/test_e2e_st.py::test_model_trainable_and_decodable[espnet.nets.pytorch_backend.e2e_st-model_dict3]
0.02s call     test/espnet2/layers/test_utterance_mvn.py::test_repr
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args714-0.5-1.0-0.5-0.0-cuda-float16]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1143-0.5-1.0-0.0-0.1-cuda-float16]
0.02s setup    test/espnet2/train/test_dataset.py::test_ESPnetDataset_sound_scp
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1102-0.0-0.5-0.5-0.1-cuda-float32]
0.02s call     test/espnet2/samplers/test_build_batch_sampler.py::test_build_batch_sampler[sorted]
0.02s setup    test/espnet2/text/test_char_tokenizer.py::test_repr
0.02s setup    test/espnet2/train/test_iterable_dataset.py::test_ESPnetDataset_sound_scp
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[multi_head_loc]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[multi_head_multi_res_loc]
0.02s call     test/test_e2e_st.py::test_gpu_trainable[espnet.nets.pytorch_backend.e2e_st]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1187-1.0-1.0-0.5-0.1-cuda-float64]
0.02s call     test/espnet2/fileio/test_read_text.py::test_load_num_sequence_text[text_int]
0.02s setup    test/espnet2/text/test_cleaner.py::test_repr[tacotron]
0.02s call     test/espnet2/layers/test_global_mvn.py::test_repl[True-True]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args728-1.0-0.0-0.5-0.0-cuda-float64]
0.02s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr-multi_head_loc]
0.02s call     test/test_e2e_asr.py::test_torch_save_and_load
0.02s call     test/test_e2e_st.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_st-multi_head_loc]
0.02s call     test/espnet2/iterators/test_multiple_iter_factory.py::test_MultpleIterFactory[True]
0.02s call     test/test_e2e_st.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_st-multi_head_multi_res_loc]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_multi_gpu_trainable[pytorch]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args951-1.0-0.5-0.0-0.1-cuda-float16]
0.02s call     test/test_e2e_st.py::test_torch_save_and_load
0.02s call     test/espnet2/utils/test_model_summary.py::test_model_summary
0.02s call     test/espnet2/tasks/test_tts.py::test_main_print_config
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args848-1.0-0.5-0.5-0.0-cuda-float64]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args430-1.0-1.0-0.5-0.1-cpu-float32]
0.02s call     test/espnet2/main_funcs/test_calculate_all_attentions.py::test_calculate_all_attentions[location2d]
0.02s call     test/test_e2e_st.py::test_zero_length_target[vggblstmp]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args847-1.0-0.5-0.5-0.0-cuda-float32]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[location_recurrent]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args205-1.0-1.0-0.0-0.0-cpu-float32]
0.02s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr-location_recurrent]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[coverage_location]
0.02s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr-coverage_location]
0.02s call     test/espnet2/main_funcs/test_calculate_all_attentions.py::test_calculate_all_attentions[location]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1052-1.0-0.0-0.5-0.0-cuda-float64]
0.02s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr-location]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[multi_head_add]
0.02s call     test/espnet2/main_funcs/test_calculate_all_attentions.py::test_calculate_all_attentions[coverage_location]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[location]
0.02s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr-location2d]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[location2d]
0.02s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr-multi_head_add]
0.02s call     test/test_e2e_st.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_st-coverage_location]
0.02s call     test/test_e2e_st.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_st-location_recurrent]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args448-0.0-0.5-0.0-0.1-cpu-float32]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args517-1.0-0.5-0.0-0.0-cpu-float32]
0.02s call     test/test_e2e_st.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_st-multi_head_add]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args311-1.0-0.5-0.5-0.1-cpu-float64]
0.02s call     test/test_e2e_mt.py::test_multi_gpu_trainable[espnet.nets.pytorch_backend.e2e_mt]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args515-1.0-0.0-0.5-0.1-cpu-float64]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args722-1.0-0.0-0.0-0.0-cuda-float64]
0.02s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr-add]
0.02s call     test/test_e2e_st.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_st-location]
0.02s call     test/test_e2e_st.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_st-location2d]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[multi_head_dot]
0.02s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr-multi_head_dot]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[coverage]
0.02s setup    test/test_cli.py::test_KaldiReader[mat]
0.02s call     test/espnet2/asr/decoder/test_rnn_decoder.py::test_RNNDecoder_backward[lstm-False]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args735-1.0-0.5-0.0-0.1-cuda-float16]
0.02s call     test/test_e2e_st.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_st-multi_head_dot]
0.02s call     test/espnet2/asr/decoder/test_transformer_decoder.py::test_TransformerDecoder_init_state
0.02s call     test/test_e2e_asr.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_asr-coverage]
0.02s call     test/test_e2e_tts_transformer.py::test_attention_masking[model_dict0]
0.02s call     test/test_e2e_st.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_st-coverage]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[dot]
0.02s call     test/test_e2e_st.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_st-add]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[add]
0.02s call     test/test_e2e_st.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_st-dot]
0.02s call     test/espnet2/main_funcs/test_calculate_all_attentions.py::test_calculate_all_attentions[coverage]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[noatt0]
0.02s call     test/test_e2e_st.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_st-noatt]
0.02s call     test/test_asr_interface.py::test_asr_build[transformer-pytorch]
0.02s call     test/espnet2/asr/decoder/test_rnn_decoder.py::test_RNNDecoder_backward[gru-True]
0.02s call     test/test_e2e_tts_fastspeech.py::test_duration_calculator
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args139-0.0-1.0-0.5-0.0-cpu-float32]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_calculate_all_attentions[noatt1]
0.02s call     test/test_e2e_asr_transducer.py::test_pytorch_transducer_gpu_trainable[pytorch]
0.02s call     test/espnet2/asr/decoder/test_rnn_decoder.py::test_RNNDecoder_backward[gru-False]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args518-1.0-0.5-0.0-0.0-cpu-float64]
0.02s call     test/espnet2/tts/feats_extract/test_log_mel_fbank.py::test_backward_not_leaf_in
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args316-1.0-1.0-0.0-0.1-cpu-float32]
0.02s call     test/espnet2/main_funcs/test_calculate_all_attentions.py::test_calculate_all_attentions[add]
0.02s call     test/test_beam_search.py::test_beam_search_equal[transformer-args172-0.5-1.0-0.0-0.1-cpu-float32]
0.02s call     test/espnet2/main_funcs/test_calculate_all_attentions.py::test_calculate_all_attentions[dot]
0.02s call     test/espnet2/tts/feats_extract/test_log_spectrogram.py::test_backward_not_leaf_in
0.02s call     test/espnet2/tasks/test_lm.py::test_main_print_config
0.02s call     test/test_e2e_mt.py::test_gpu_trainable[espnet.nets.pytorch_backend.e2e_mt]
0.02s call     test/test_e2e_asr_sa_transducer.py::test_sa_transducer_mask[pytorch]
0.01s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_inference[1.0]
0.01s call     test/espnet2/layers/test_stft.py::test_backward_not_leaf_in
0.01s call     test/espnet2/main_funcs/test_calculate_all_attentions.py::test_calculate_all_attentions[noatt]
0.01s setup    test/espnet2/text/test_sentencepiece_tokenizer.py::test_text2tokens
0.01s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_inference[2.0]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args502-0.5-1.0-0.5-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args170-0.5-1.0-0.0-0.0-cpu-float64]
0.01s call     test/test_e2e_tts_fastspeech.py::test_fastspeech_inference[0.5]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args232-0.0-0.5-0.0-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args452-0.0-0.5-0.5-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args65-0.5-1.0-0.0-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args451-0.0-0.5-0.5-0.0-cpu-float32]
0.01s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_score[False-RNN_TANH]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args500-0.5-1.0-0.5-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args386-0.5-1.0-0.0-0.0-cpu-float64]
0.01s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_score[True-RNN_TANH]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args778-0.0-0.5-0.5-0.1-cuda-float32]
0.01s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_score[False-RNN_RELU]
0.01s call     test/espnet2/lm/test_seq_rnn.py::test_SequentialRNNLM_score[True-RNN_RELU]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args449-0.0-0.5-0.0-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args230-0.0-0.5-0.0-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args64-0.5-1.0-0.0-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args779-0.0-0.5-0.5-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1295-1.0-1.0-0.5-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args777-0.0-0.5-0.5-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args236-0.0-0.5-0.5-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args463-0.0-1.0-0.5-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args233-0.0-0.5-0.0-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args682-0.0-1.0-0.5-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args680-0.0-1.0-0.5-0.0-cuda-float64]
0.01s setup    test/espnet2/train/test_dataset.py::test_ESPnetDataset_pipe_wav
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args235-0.0-0.5-0.5-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args881-0.0-0.5-0.0-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args464-0.0-1.0-0.5-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args467-0.0-1.0-0.5-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args391-0.5-1.0-0.5-0.0-cpu-float32]
0.01s setup    test/espnet2/samplers/test_length_elements_batch_sampler.py::test_LengthBatchSampler_len[True-False-ascending-ascending]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args389-0.5-1.0-0.0-0.1-cpu-float64]
0.01s call     test/espnet2/asr/decoder/test_rnn_decoder.py::test_RNNDecoder_init_state[lstm-True]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args137-0.0-1.0-0.0-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args347-0.0-0.5-0.5-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args358-0.0-1.0-0.5-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args681-0.0-1.0-0.5-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args19-0.0-0.5-0.5-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args125-0.0-0.5-0.0-0.1-cpu-float64]
0.01s call     test/test_e2e_asr_transformer.py::test_v0_3_transformer_input_compatibility
0.01s call     test/test_asr_interface.py::test_asr_build[rnn-pytorch]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args780-0.0-1.0-0.0-0.0-cuda-float16]
0.01s call     test/test_transformer_decode.py::test_decoder_cache[True]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args173-0.5-1.0-0.0-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args683-0.0-1.0-0.5-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args884-0.0-0.5-0.5-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args818-0.5-1.0-0.0-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args882-0.0-0.5-0.5-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args175-0.5-1.0-0.5-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args610-0.5-1.0-0.5-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args392-0.5-1.0-0.5-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args359-0.0-1.0-0.5-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args883-0.0-0.5-0.5-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1200-0.0-0.5-0.0-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args559-0.0-0.5-0.5-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1257-0.5-1.0-0.5-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1201-0.0-0.5-0.0-0.0-cuda-float32]
0.01s call     test/test_transformer_decode.py::test_decoder_cache[False]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1254-0.5-1.0-0.5-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1293-1.0-1.0-0.5-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args349-0.0-1.0-0.0-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1282-1.0-0.5-0.5-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1221-0.0-1.0-0.5-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1277-1.0-0.5-0.0-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args571-0.0-1.0-0.5-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1251-0.5-1.0-0.0-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1271-1.0-0.0-0.5-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1207-0.0-0.5-0.5-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1205-0.0-0.5-0.0-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args607-0.5-1.0-0.5-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args569-0.0-1.0-0.0-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1222-0.0-1.0-0.5-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args562-0.0-0.5-0.5-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args608-0.5-1.0-0.5-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1289-1.0-1.0-0.0-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1249-0.5-1.0-0.0-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1217-0.0-1.0-0.0-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args632-1.0-0.5-0.5-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1281-1.0-0.5-0.5-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1248-0.5-1.0-0.0-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1209-0.0-0.5-0.5-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1250-0.5-1.0-0.0-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1261-1.0-0.0-0.0-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1255-0.5-1.0-0.5-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args560-0.0-0.5-0.5-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1275-1.0-0.5-0.0-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args553-0.0-0.5-0.0-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args634-1.0-0.5-0.5-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1259-0.5-1.0-0.5-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1285-1.0-1.0-0.0-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1269-1.0-0.0-0.5-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1203-0.0-0.5-0.0-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1274-1.0-0.5-0.0-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1253-0.5-1.0-0.0-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1286-1.0-1.0-0.0-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args566-0.0-1.0-0.0-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args568-0.0-1.0-0.0-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args557-0.0-0.5-0.0-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1219-0.0-1.0-0.5-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1202-0.0-0.5-0.0-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1265-1.0-0.0-0.0-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1208-0.0-0.5-0.5-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args575-0.0-1.0-0.5-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args554-0.0-0.5-0.0-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1211-0.0-0.5-0.5-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1266-1.0-0.0-0.5-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1273-1.0-0.5-0.0-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1291-1.0-1.0-0.5-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1213-0.0-1.0-0.0-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args643-1.0-1.0-0.5-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args602-0.5-1.0-0.0-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args617-1.0-0.0-0.0-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1210-0.0-0.5-0.5-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1218-0.0-1.0-0.5-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1214-0.0-1.0-0.0-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1294-1.0-1.0-0.5-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args619-1.0-0.0-0.5-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1263-1.0-0.0-0.0-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args625-1.0-0.5-0.0-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1278-1.0-0.5-0.5-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1287-1.0-1.0-0.0-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args637-1.0-1.0-0.0-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1270-1.0-0.0-0.5-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args605-0.5-1.0-0.0-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1206-0.0-0.5-0.5-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args628-1.0-0.5-0.0-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1292-1.0-1.0-0.5-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1279-1.0-0.5-0.5-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args626-1.0-0.5-0.0-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1215-0.0-1.0-0.0-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1283-1.0-0.5-0.5-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args614-1.0-0.0-0.0-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args623-1.0-0.0-0.5-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1223-0.0-1.0-0.5-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1258-0.5-1.0-0.5-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args601-0.5-1.0-0.0-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args616-1.0-0.0-0.0-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1262-1.0-0.0-0.0-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1267-1.0-0.0-0.5-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args646-1.0-1.0-0.5-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1290-1.0-1.0-0.5-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1272-1.0-0.5-0.0-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args644-1.0-1.0-0.5-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args635-1.0-0.5-0.5-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args641-1.0-1.0-0.0-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args638-1.0-1.0-0.0-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args604-0.5-1.0-0.0-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args613-1.0-0.0-0.0-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args572-0.0-1.0-0.5-0.0-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args565-0.0-1.0-0.0-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1212-0.0-1.0-0.0-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args563-0.0-0.5-0.5-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1252-0.5-1.0-0.0-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1276-1.0-0.5-0.0-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1268-1.0-0.0-0.5-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args629-1.0-0.5-0.0-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1220-0.0-1.0-0.5-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1280-1.0-0.5-0.5-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args556-0.0-0.5-0.0-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args622-1.0-0.0-0.5-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args611-0.5-1.0-0.5-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1204-0.0-0.5-0.0-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1256-0.5-1.0-0.5-0.0-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args631-1.0-0.5-0.5-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1264-1.0-0.0-0.0-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args574-0.0-1.0-0.5-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1216-0.0-1.0-0.0-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1260-1.0-0.0-0.0-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1288-1.0-1.0-0.0-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args1284-1.0-1.0-0.0-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args647-1.0-1.0-0.5-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args620-1.0-0.0-0.5-0.0-cpu-float64]
0.01s call     test/test_e2e_asr_transformer.py::test_transformer_mask[pytorch]
0.01s call     test/test_beam_search.py::test_beam_search_equal[rnn-args640-1.0-1.0-0.0-0.1-cpu-float32]
0.01s call     test/test_e2e_mt.py::test_zero_length_target[blstm]
0.01s call     test/test_e2e_st.py::test_zero_length_target[blstmp]
0.01s call     test/test_e2e_st_transformer.py::test_transformer_mask[pytorch]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args849-1.0-0.5-0.5-0.1-cuda-float16]
0.01s call     test/espnet2/tasks/test_asr.py::test_main_with_no_args
0.01s call     test/espnet2/tasks/test_tts.py::test_main_with_no_args
0.01s call     test/espnet2/utils/test_pack_funcs.py::test_pack_unpack
0.01s call     test/test_utils.py::test_load_inputs_and_targets_new_format
0.01s call     test/test_e2e_mt_transformer.py::test_transformer_mask[pytorch]
0.01s setup    test/espnet2/train/test_iterable_dataset.py::test_ESPnetDataset_pipe_wav
0.01s call     test/test_e2e_mt.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_mt-multi_head_dot]
0.01s call     test/espnet2/asr/decoder/test_rnn_decoder.py::test_RNNDecoder_init_state[lstm-False]
0.01s call     test/espnet2/tasks/test_lm.py::test_main_with_no_args
0.01s call     test/test_e2e_mt.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_mt-multi_head_add]
0.01s call     test/test_transformer_decode.py::test_encoder_cache[True]
0.01s teardown test/espnet2/utils/test_yaml_no_alias_safe_dump.py::test_yaml_no_alias_safe_dump[data1-desired1]
0.01s call     test/test_lm.py::test_lm
0.01s call     test/test_utils.py::test_load_inputs_and_targets_legacy_format_multi_inputs
0.01s call     test/test_transformer_decode.py::test_encoder_cache[False]
0.01s call     test/espnet2/asr/decoder/test_rnn_decoder.py::test_RNNDecoder_init_state[gru-True]
0.01s call     test/espnet2/asr/decoder/test_rnn_decoder.py::test_RNNDecoder_init_state[gru-False]
0.01s call     test/test_e2e_mt.py::test_torch_save_and_load
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args734-1.0-0.5-0.0-0.0-cuda-float64]
0.01s call     test/test_e2e_mt.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_mt-coverage]
0.01s call     test/espnet2/tasks/test_asr.py::test_add_arguments_help
0.01s call     test/espnet2/tasks/test_asr.py::test_main_help
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1155-1.0-0.0-0.0-0.1-cuda-float16]
0.01s call     test/test_cli.py::test_KaldiReader[sound.hdf5]
0.01s call     test/test_e2e_mt.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_mt-add]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1156-1.0-0.0-0.0-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1157-1.0-0.0-0.0-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1158-1.0-0.0-0.5-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args736-1.0-0.5-0.0-0.1-cuda-float32]
0.01s call     test/test_optimizer.py::test_optimizer_backend_compatible[adam]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args508-1.0-0.0-0.0-0.1-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args505-1.0-0.0-0.0-0.0-cpu-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args521-1.0-0.5-0.0-0.1-cpu-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args738-1.0-0.5-0.5-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1055-1.0-0.0-0.5-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args834-1.0-0.0-0.5-0.0-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args506-1.0-0.0-0.0-0.0-cpu-float64]
0.01s call     test/espnet2/train/test_reporter.py::test_logging
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args946-1.0-0.0-0.5-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1054-1.0-0.0-0.5-0.1-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args739-1.0-0.5-0.5-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args520-1.0-0.5-0.0-0.1-cpu-float32]
0.01s call     test/test_e2e_mt.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_mt-dot]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args944-1.0-0.0-0.5-0.0-cuda-float64]
0.01s call     test/espnet2/tasks/test_tts.py::test_main_help
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args945-1.0-0.0-0.5-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args833-1.0-0.0-0.0-0.1-cuda-float64]
0.01s call     test/espnet2/tasks/test_tts.py::test_add_arguments_help
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args1053-1.0-0.0-0.5-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args737-1.0-0.5-0.0-0.1-cuda-float64]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args943-1.0-0.0-0.5-0.0-cuda-float32]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args831-1.0-0.0-0.0-0.1-cuda-float16]
0.01s call     test/test_beam_search.py::test_beam_search_equal[transformer-args832-1.0-0.0-0.0-0.1-cuda-float32]
0.01s call     test/espnet2/tasks/test_lm.py::test_add_arguments_help
0.01s call     test/espnet2/tasks/test_lm.py::test_main_help
0.01s call     test/test_e2e_mt.py::test_calculate_all_attentions[espnet.nets.pytorch_backend.e2e_mt-noatt]
0.01s call     test/espnet2/layers/test_log_mel.py::test_repr
0.01s call     test/espnet2/tasks/test_abs_task.py::test_main_help
0.01s call     test/espnet2/iterators/test_sequence_iter_factory.py::test_SequenceIterFactory_deterministic[None]
0.01s call     test/espnet2/iterators/test_sequence_iter_factory.py::test_SequenceIterFactory_deterministic[collate_func]
0.01s call     test/test_transform.py::test_preprocessing
0.01s call     test/espnet2/tasks/test_abs_task.py::test_add_arguments_help
0.01s call     test/espnet2/train/test_reporter.py::test_tensorboard_add_scalar

(0.00 durations hidden.  Use -vv to show these durations.)
=========================== short test summary info ============================
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args4-0.0-default-lm_args4-0.0-0.5-0.0-cpu-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args5-0.0-default-lm_args5-0.0-0.5-0.0-cpu-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args6-0.0-default-lm_args6-0.0-0.5-0.1-cpu-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args7-0.0-default-lm_args7-0.0-0.5-0.1-cpu-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args12-0.0-default-lm_args12-0.5-0.5-0.0-cpu-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args13-0.0-default-lm_args13-0.5-0.5-0.0-cpu-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args14-0.0-default-lm_args14-0.5-0.5-0.1-cpu-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args15-0.0-default-lm_args15-0.5-0.5-0.1-cpu-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args20-0.0-default-lm_args20-0.0-0.5-0.0-cpu-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args21-0.0-default-lm_args21-0.0-0.5-0.0-cpu-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args22-0.0-default-lm_args22-0.0-0.5-0.1-cpu-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args23-0.0-default-lm_args23-0.0-0.5-0.1-cpu-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args28-0.0-default-lm_args28-0.5-0.5-0.0-cpu-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args29-0.0-default-lm_args29-0.5-0.5-0.0-cpu-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args30-0.0-default-lm_args30-0.5-0.5-0.1-cpu-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args31-0.0-default-lm_args31-0.5-0.5-0.1-cpu-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args36-0.0-transformer-lm_args36-0.0-0.5-0.0-cpu-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args37-0.0-transformer-lm_args37-0.0-0.5-0.0-cpu-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args38-0.0-transformer-lm_args38-0.0-0.5-0.1-cpu-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args39-0.0-transformer-lm_args39-0.0-0.5-0.1-cpu-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args44-0.0-transformer-lm_args44-0.5-0.5-0.0-cpu-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args45-0.0-transformer-lm_args45-0.5-0.5-0.0-cpu-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args46-0.0-transformer-lm_args46-0.5-0.5-0.1-cpu-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args47-0.0-transformer-lm_args47-0.5-0.5-0.1-cpu-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args52-0.0-default-lm_args52-0.0-0.5-0.0-cuda-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args53-0.0-default-lm_args53-0.0-0.5-0.0-cuda-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args54-0.0-default-lm_args54-0.0-0.5-0.1-cuda-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args55-0.0-default-lm_args55-0.0-0.5-0.1-cuda-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args60-0.0-default-lm_args60-0.5-0.5-0.0-cuda-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args61-0.0-default-lm_args61-0.5-0.5-0.0-cuda-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args62-0.0-default-lm_args62-0.5-0.5-0.1-cuda-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args63-0.0-default-lm_args63-0.5-0.5-0.1-cuda-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args68-0.0-default-lm_args68-0.0-0.5-0.0-cuda-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args69-0.0-default-lm_args69-0.0-0.5-0.0-cuda-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args70-0.0-default-lm_args70-0.0-0.5-0.1-cuda-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args71-0.0-default-lm_args71-0.0-0.5-0.1-cuda-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args76-0.0-default-lm_args76-0.5-0.5-0.0-cuda-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args77-0.0-default-lm_args77-0.5-0.5-0.0-cuda-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args78-0.0-default-lm_args78-0.5-0.5-0.1-cuda-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args79-0.0-default-lm_args79-0.5-0.5-0.1-cuda-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args84-0.0-transformer-lm_args84-0.0-0.5-0.0-cuda-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args85-0.0-transformer-lm_args85-0.0-0.5-0.0-cuda-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args86-0.0-transformer-lm_args86-0.0-0.5-0.1-cuda-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args87-0.0-transformer-lm_args87-0.0-0.5-0.1-cuda-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args92-0.0-transformer-lm_args92-0.5-0.5-0.0-cuda-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args93-0.0-transformer-lm_args93-0.5-0.5-0.0-cuda-float64]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args94-0.0-transformer-lm_args94-0.5-0.5-0.1-cuda-float32]
FAILED test/test_batch_beam_search.py::test_batch_beam_search_equal[transformer-args95-0.0-transformer-lm_args95-0.5-0.5-0.1-cuda-float64]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args864-0.0-0.0-0.0-0.0-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args867-0.0-0.0-0.0-0.1-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args870-0.0-0.0-0.5-0.0-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args873-0.0-0.0-0.5-0.1-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args900-0.5-0.0-0.0-0.0-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args903-0.5-0.0-0.0-0.1-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args906-0.5-0.0-0.5-0.0-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args909-0.5-0.0-0.5-0.1-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args912-0.5-0.5-0.0-0.0-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args915-0.5-0.5-0.0-0.1-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args918-0.5-0.5-0.5-0.0-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args921-0.5-0.5-0.5-0.1-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args1080-0.0-0.0-0.0-0.0-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args1083-0.0-0.0-0.0-0.1-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args1086-0.0-0.0-0.5-0.0-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args1089-0.0-0.0-0.5-0.1-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args1116-0.5-0.0-0.0-0.0-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args1119-0.5-0.0-0.0-0.1-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args1122-0.5-0.0-0.5-0.0-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args1125-0.5-0.0-0.5-0.1-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args1128-0.5-0.5-0.0-0.0-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args1131-0.5-0.5-0.0-0.1-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args1134-0.5-0.5-0.5-0.0-cuda-float16]
FAILED test/test_beam_search.py::test_beam_search_equal[transformer-args1137-0.5-0.5-0.5-0.1-cuda-float16]
FAILED test/test_e2e_asr_transformer.py::test_transformer_parallel - Attribut...
==== 73 failed, 1852 passed, 954 skipped, 680 warnings in 619.54s (0:10:19) ====
