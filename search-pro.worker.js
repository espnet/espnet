const V=Object.entries,et=Object.fromEntries,st="ENTRIES",L="KEYS",T="VALUES",_="";class D{set;_type;_path;constructor(t,s){const n=t._tree,o=Array.from(n.keys());this.set=t,this._type=s,this._path=o.length>0?[{node:n,keys:o}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===_)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==_).join("")}value(){return E(this._path).node.get(_)}result(){switch(this._type){case T:return this.value();case L:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],nt=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const o=t.length+1,u=o+s,i=new Uint8Array(u*o).fill(s+1);for(let r=0;r<o;++r)i[r]=r;for(let r=1;r<u;++r)i[r*o]=r;return R(e,t,s,n,i,1,o,""),n},R=(e,t,s,n,o,u,i,r)=>{const d=u*i;t:for(const c of e.keys())if(c===_){const a=o[d-1];a<=s&&n.set(r,[e.get(c),a])}else{let a=u;for(let h=0;h<c.length;++h,++a){const g=c[h],m=i*a,p=m-i;let l=o[m];const f=Math.max(0,a-s-1),y=Math.min(i-1,a+s);for(let F=f;F<y;++F){const v=g!==t[F],z=o[p+F]+ +v,A=o[p+F+1]+1,w=o[m+F]+1,j=o[m+F+1]=Math.min(z,A,w);j<l&&(l=j)}if(l>s)continue t}R(e.get(c),t,s,n,o,a,i,r+c)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[o,u]=O(n);for(const i of o.keys())if(i!==_&&i.startsWith(u)){const r=new Map;return r.set(i.slice(u.length),o.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ot(this._tree,t)}entries(){return new D(this,st)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return nt(this._tree,t,s)}get(t){const s=k(this._tree,t);return s!==void 0?s.get(_):void 0}has(t){const s=k(this._tree,t);return s!==void 0&&s.has(_)}keys(){return new D(this,L)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,I(this._tree,t).set(_,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=I(this._tree,t);return n.set(_,s(n.get(_))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=I(this._tree,t);let o=n.get(_);return o===void 0&&n.set(_,o=s()),o}values(){return new D(this,T)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,o]of t)s.set(n,o);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==_&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},k=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==_&&t.startsWith(s))return k(e.get(s),t.slice(s.length))},I=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const u of e.keys())if(u!==_&&t[n]===u[0]){const i=Math.min(s-n,u.length);let r=1;for(;r<i&&t[n+r]===u[r];)++r;const d=e.get(u);if(r===u.length)e=d;else{const c=new Map;c.set(u.slice(r),d),e.set(t.slice(n,n+r),c),e.delete(u),e=c}n+=r;continue t}const o=new Map;return e.set(t.slice(n),o),o}return e},ot=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(_),s.size===0)W(n);else if(s.size===1){const[o,u]=s.entries().next().value;q(n,o,u)}}},W=e=>{if(e.length===0)return;const[t,s]=O(e);if(t.delete(s),t.size===0)W(e.slice(0,-1));else if(t.size===1){const[n,o]=t.entries().next().value;n!==_&&q(e.slice(0,-1),n,o)}},q=(e,t,s)=>{if(e.length===0)return;const[n,o]=O(e);n.set(o+t,s),n.delete(o)},O=e=>e[e.length-1],ut=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},it=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,M="or",$="and",rt="and_not",ct=(e,t)=>{e.includes(t)||e.push(t)},N=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},P=({score:e},{score:t})=>t-e,lt=()=>new Map,b=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},G=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,ht={[M]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:o,terms:u,match:i}=t.get(s);n.score=n.score+o,n.match=Object.assign(n.match,i),N(n.terms,u)}}return e},[$]:(e,t)=>{const s=new Map;for(const n of t.keys()){const o=e.get(n);if(o==null)continue;const{score:u,terms:i,match:r}=t.get(n);N(o.terms,i),s.set(n,{score:o.score+u,terms:o.terms,match:Object.assign(o.match,r)})}return s},[rt]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},dt=(e,t,s,n,o,u)=>{const{k:i,b:r,d}=u;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/o)))},at=e=>(t,s,n)=>{const o=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,u=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:o,prefix:u}},H=(e,t,s,n)=>{for(const o of Object.keys(e._fieldIds))if(e._fieldIds[o]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${o}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},ft=(e,t,s,n)=>{if(!e._index.has(n)){H(e,s,t,n);return}const o=e._index.fetch(n,lt),u=o.get(t);u==null||u.get(s)==null?H(e,s,t,n):u.get(s)<=1?u.size<=1?o.delete(t):u.delete(s):u.set(s,u.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},gt={k:1.2,b:.7,d:.5},mt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(it),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},J={combineWith:M,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:gt},pt={combineWith:$,prefix:(e,t,s)=>t===s.length-1},Ft={batchSize:1e3,batchWait:10},U={minDirtFactor:.1,minDirtCount:20},_t={...Ft,...U},K=Symbol("*"),yt=(e,t)=>{const s=new Map,n={...e._options.searchOptions,...t};for(const[o,u]of e._documentIds){const i=n.boostDocument?n.boostDocument(u,"",e._storedFields.get(o)):1;s.set(o,{score:i,terms:[],match:{}})}return s},X=(e,t=M)=>{if(e.length===0)return new Map;const s=t.toLowerCase(),n=ht[s];if(!n)throw new Error(`Invalid combination operator: ${t}`);return e.reduce(n)||new Map},S=(e,t,s,n,o,u,i,r,d=new Map)=>{if(o==null)return d;for(const c of Object.keys(u)){const a=u[c],h=e._fieldIds[c],g=o.get(h);if(g==null)continue;let m=g.size;const p=e._avgFieldLength[h];for(const l of g.keys()){if(!e._documentIds.has(l)){ft(e,h,l,s),m-=1;continue}const f=i?i(e._documentIds.get(l),s,e._storedFields.get(l)):1;if(!f)continue;const y=g.get(l),F=e._fieldLength.get(l)[h],v=dt(y,m,e._documentCount,F,p,r),z=n*a*f*v,A=d.get(l);if(A){A.score+=z,ct(A.terms,t);const w=G(A.match,s);w?w.push(c):A.match[s]=[c]}else d.set(l,{score:z,terms:[t],match:{[s]:[c]}})}}return d},At=(e,t,s)=>{const n={...e._options.searchOptions,...s},o=(n.fields||e._options.fields).reduce((l,f)=>({...l,[f]:G(n.boost,f)||1}),{}),{boostDocument:u,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:c,prefix:a}={...J.weights,...i},h=e._index.get(t.term),g=S(e,t.term,t.term,1,h,o,u,d);let m,p;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const l=t.fuzzy===!0?.2:t.fuzzy,f=l<1?Math.min(r,Math.round(t.term.length*l)):l;f&&(p=e._index.fuzzyGet(t.term,f))}if(m)for(const[l,f]of m){const y=l.length-t.term.length;if(!y)continue;p?.delete(l);const F=a*l.length/(l.length+.3*y);S(e,t.term,l,F,f,o,u,d,g)}if(p)for(const l of p.keys()){const[f,y]=p.get(l);if(!y)continue;const F=c*l.length/(l.length+y);S(e,t.term,l,F,f,o,u,d,g)}return g},Y=(e,t,s={})=>{if(t===K)return yt(e,s);if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(g=>Y(e,g,a));return X(h,a.combineWith)}const{tokenize:n,processTerm:o,searchOptions:u}=e._options,i={tokenize:n,processTerm:o,...u,...s},{tokenize:r,processTerm:d}=i,c=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(at(i)).map(a=>At(e,a,i));return X(c,i.combineWith)},Q=(e,t,s={})=>{const n=Y(e,t,s),o=[];for(const[u,{score:i,terms:r,match:d}]of n){const c=r.length||1,a={id:e._documentIds.get(u),score:i*c,terms:Object.keys(d),queryTerms:r,match:d};Object.assign(a,e._storedFields.get(u)),(s.filter==null||s.filter(a))&&o.push(a)}return t===K&&s.boostDocument==null&&e._options.searchOptions.boostDocument==null||o.sort(P),o},Ct=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:u,terms:i}of Q(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=u,d.count+=1):n.set(r,{score:u,terms:i,count:1})}const o=[];for(const[u,{score:i,terms:r,count:d}]of n)o.push({suggestion:u,terms:r,score:i/d});return o.sort(P),o};class Et{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?_t:t.autoVacuum;this._options={...mt,...t,autoVacuum:s,searchOptions:{...J,...t.searchOptions||{}},autoSuggestOptions:{...pt,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=U,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const o={};for(const[u,i]of n)o[u]=Object.fromEntries(i);t.push([s,o])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:o,fieldLength:u,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:c},a)=>{if(c!==1&&c!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new Et(a);h._documentCount=t,h._nextId=s,h._documentIds=b(n),h._idToShortId=new Map,h._fieldIds=o,h._fieldLength=b(u),h._avgFieldLength=i,h._storedFields=b(r),h._dirtCount=d||0,h._index=new C;for(const[g,m]of h._documentIds)h._idToShortId.set(m,g);for(const[g,m]of e){const p=new Map;for(const l of Object.keys(m)){let f=m[l];c===1&&(f=f.ds),p.set(parseInt(l,10),b(f))}h._index.set(g,p)}return h},B=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),o=[];let u=0,i=0;const r=(c,a=!1)=>{let h="";i===0?h=c.length>20?`… ${c.slice(-20)}`:c:a?h=c.length+i>100?`${c.slice(0,100-i)}… `:c:h=c.length>20?`${c.slice(0,20)} … ${c.slice(-20)}`:c,h&&o.push(h),i+=h.length,a||(o.push(["mark",t]),i+=t.length,i>=100&&o.push(" …"))};let d=s.indexOf(n,u);if(d===-1)return null;for(;d>=0;){const c=d+n.length;if(r(e.slice(u,d)),u=c,i>100)break;d=s.indexOf(n,u)}return i<100&&r(e.slice(u),!0),o},wt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),xt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),Z=(e,t,s={})=>{const n={};return Q(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...s}).forEach(o=>{const{id:u,terms:i,score:r}=o,d=u.includes("@"),c=u.includes("#"),[a,h]=u.split(/[#@]/),g=Number(a),m=i.sort((l,f)=>l.length-f.length).filter((l,f)=>i.slice(f+1).every(y=>!y.includes(l))),{contents:p}=n[g]??={title:"",contents:[]};if(d)p.push([{type:"customField",id:g,index:h,display:m.map(l=>o.c.map(f=>B(f,l))).flat().filter(l=>l!==null)},r]);else{const l=m.map(f=>B(o.h,f)).filter(f=>f!==null);if(l.length&&p.push([{type:c?"heading":"title",id:g,...c&&{anchor:h},display:l},r]),"t"in o)for(const f of o.t){const y=m.map(F=>B(f,F)).filter(F=>F!==null);y.length&&p.push([{type:"text",id:g,...c&&{anchor:h},display:y},r])}}}),V(n).sort(([,o],[,u])=>"max"==="total"?wt(o,u):xt(o,u)).map(([o,{title:u,contents:i}])=>{if(!u){const r=ut(t,o);r&&(u=r.h)}return{title:u,contents:i.map(([r])=>r)}})},tt=(e,t,s={})=>{const n=Ct(t,e,{fuzzy:.2,maxFuzzy:3,...s}).map(({suggestion:o})=>o);return e.includes(" ")?n:n.filter(o=>!o.includes(" "))},bt=et(V(JSON.parse("{\"/\":{\"documentCount\":2556,\"nextId\":2556,\"documentIds\":{\"0\":\"0\",\"1\":\"0#building-the-homepage\",\"2\":\"0#key-points\",\"3\":\"0#step-by-step-guide-to-ci-doc-sh\",\"4\":\"2\",\"5\":\"2#espnet-ez\",\"6\":\"2#weakly-supervised-learning\",\"7\":\"2#self-supervised-learning\",\"8\":\"2#speaker-embeddings\",\"9\":\"2#tts\",\"10\":\"2#speech-translation\",\"11\":\"2#speech-enhancement\",\"12\":\"2#spoken-language-understanding\",\"13\":\"2#singing-voice-synthesis\",\"14\":\"2#unsupervised-asr\",\"15\":\"2#speech-summarization\",\"16\":\"2#exporting-models-to-onnx\",\"17\":\"3\",\"18\":\"3#devcontainer\",\"19\":\"3#implemented-dev-containers\",\"20\":\"4\",\"21\":\"4#docker\",\"22\":\"4#execute-in-docker\",\"23\":\"4#espnet-2-recipes\",\"24\":\"4#using-gpu-based-containers\",\"25\":\"4#using-cpu-based-container\",\"26\":\"4#local-builds\",\"27\":\"4#deprecated\",\"28\":\"4#tags\",\"29\":\"4#ubuntu-18-04\",\"30\":\"5\",\"31\":\"5#install\",\"32\":\"5#style-check-using-flake8-docstrings\",\"33\":\"5#generate-html\",\"34\":\"5#deploy\",\"35\":\"6\",\"36\":\"6#usage\",\"37\":\"6#directory-structure\",\"38\":\"6#execution-of-example-scripts\",\"39\":\"6#logging\",\"40\":\"6#change-options-in-run-sh\",\"41\":\"6#use-of-gpu\",\"42\":\"6#espnet1-transducer\",\"43\":\"6#architecture\",\"44\":\"6#multi-task-learning\",\"45\":\"6#inference\",\"46\":\"6#additional-notes\",\"47\":\"6#changing-the-training-configuration\",\"48\":\"6#how-to-set-minibatch\",\"49\":\"6#how-to-use-finetuning\",\"50\":\"6#transfer-learning\",\"51\":\"6#freezing\",\"52\":\"6#important-notes\",\"53\":\"6#chainer-and-pytorch-backends\",\"54\":\"7\",\"55\":\"7#examples\",\"56\":\"7#single-node-with-4gpus-with-distributed-mode\",\"57\":\"7#to-enable-sharded-training\",\"58\":\"7#_2host-and-2gpus-for-each-host-with-multiprocessing-distributed-mode\",\"59\":\"7#rank-and-world-size\",\"60\":\"7#about-init-method\",\"61\":\"7#_2hosts-which-have-2gpus-and-1gpu-respectively\",\"62\":\"7#_2hosts-and-2gpus-for-each-node-using-slurm-with-multiprocessing-distributed\",\"63\":\"7#_5gpus-with-3nodes-using-slurm\",\"64\":\"7#_2hosts-and-2gpus-for-each-node-using-mpi-with-multiprocessing-distributed\",\"65\":\"7#espnet2-bin-launch\",\"66\":\"7#troubleshooting-for-nccl-with-ethernet-case\",\"67\":\"7#the-rules-of-nccl-socket-ifname\",\"68\":\"8\",\"69\":\"8#quick-usage\",\"70\":\"8#why-is-audio-file-formatting-necessary\",\"71\":\"8#the-audio-file-formats-supported-in-espnet2\",\"72\":\"8#use-case\",\"73\":\"8#case1-extract-segmentations-with-long-recoding\",\"74\":\"8#case2-extract-audio-data-from-video-codec-use-non-supported-format-by-soundfile\",\"75\":\"8#case3-convert-nist-sphere-files-to-wav\",\"76\":\"8#case4-using-a-mechanism-for-multi-channels-inputs\",\"77\":\"9\",\"78\":\"9#task-class\",\"79\":\"9#data-input-system\",\"80\":\"9#scp-file\",\"81\":\"9#required-data-names-and-optional-data-names\",\"82\":\"9#customize-collate-fn-for-pytorch-data-loader\",\"83\":\"10\",\"84\":\"10#show-usage\",\"85\":\"10#configuration-file\",\"86\":\"10#change-the-configuration-for-dict-type-value\",\"87\":\"10#resume-training-process\",\"88\":\"10#transfer-learning-fine-tuning-using-pretrained-model\",\"89\":\"10#freeze-parameters\",\"90\":\"10#change-logging-interval\",\"91\":\"10#change-the-number-of-iterations-in-each-epoch\",\"92\":\"10#weights-biases-integration\",\"93\":\"10#multi-gpus\",\"94\":\"10#the-relation-between-mini-batch-size-and-number-of-gpus\",\"95\":\"10#change-mini-batch-type\",\"96\":\"10#batch-type-unsorted\",\"97\":\"10#batch-type-sorted\",\"98\":\"10#batch-type-folded\",\"99\":\"10#batch-type-length\",\"100\":\"10#batch-type-numel\",\"101\":\"10#batch-type-catbel\",\"102\":\"10#gradient-accumulating\",\"103\":\"10#automatic-mixed-precision-training\",\"104\":\"10#reproducibility-and-determinization\",\"105\":\"11\",\"106\":\"11#main-changes-from-espnet1\",\"107\":\"11#understanding-espnet2-recipes\",\"108\":\"11#change-directory-to-the-base-directory\",\"109\":\"11#directory-structure-of-each-recipe\",\"110\":\"11#change-the-configuration\",\"111\":\"11#run-sh\",\"112\":\"11#seeing-the-training-status\",\"113\":\"11#show-the-log-file\",\"114\":\"11#show-the-training-status-in-a-image-file\",\"115\":\"11#use-tensorboard\",\"116\":\"11#instruction-for-run-sh\",\"117\":\"11#how-to-parse-command-line-arguments-in-shell-scripts\",\"118\":\"11#start-from-a-specified-stage-and-stop-at-a-specified-stage\",\"119\":\"11#change-the-configuration-for-training\",\"120\":\"11#change-the-number-of-parallel-jobs\",\"121\":\"11#multi-gpus-training-and-distributed-training\",\"122\":\"11#various-tips\",\"123\":\"11#relationship-between-mini-batch-size-and-number-of-gpus\",\"124\":\"11#use-specified-experiment-directory-for-evaluation\",\"125\":\"11#evaluation-without-training-using-pretrained-model\",\"126\":\"11#evaluation-using-openai-whisper\",\"127\":\"11#packing-and-sharing-your-trained-model\",\"128\":\"11#usage-of-self-supervised-learning-representations-as-feature\",\"129\":\"11#streaming-asr\",\"130\":\"11#training\",\"131\":\"11#decoding\",\"132\":\"11#faq\",\"133\":\"11#real-time-factor-and-latency\",\"134\":\"11#usage\",\"135\":\"11#notes\",\"136\":\"11#example\",\"137\":\"11#limitations\",\"138\":\"11#transducer-asr\",\"139\":\"11#general-usage\",\"140\":\"11#architecture\",\"141\":\"11#encoder\",\"142\":\"11#decoder\",\"143\":\"11#joint-network\",\"144\":\"11#multi-task-learning\",\"145\":\"11#inference\",\"146\":\"11#streaming\",\"147\":\"11#training-1\",\"148\":\"11#decoding-1\",\"149\":\"11#faq-1\",\"150\":\"11#how-to-add-a-new-block-type-to-the-custom-encoder\",\"151\":\"12\",\"152\":\"12#how-to-build-espnet-on-a-cloud-machine-such-as-gcp-aws-etc\",\"153\":\"12#modulenotfounderror-no-module-named-espnet-or-etc\",\"154\":\"12#to-detect-the-installation-problem-with-a-normal-installation\",\"155\":\"13\",\"156\":\"13#how-to-cite-espnet\",\"157\":\"14\",\"158\":\"14#installation\",\"159\":\"14#requirements\",\"160\":\"14#supported-linux-distributions-and-other-requirements\",\"161\":\"14#step-1-optional-install-kaldi\",\"162\":\"14#step-2-installation-espnet\",\"163\":\"14#step-3-optional-custom-tool-installation\",\"164\":\"14#check-installation\",\"165\":\"15\",\"166\":\"15#select-job-scheduler\",\"167\":\"15#usage-of-run-pl\",\"168\":\"15#configuration\",\"169\":\"16\",\"170\":\"16#common-usages\",\"171\":\"16#espnet1\",\"172\":\"16#espnet2\",\"173\":\"16#multiple-gpu-tips\",\"174\":\"16#start-from-the-middle-stage-or-stop-at-the-specified-stage\",\"175\":\"16#ctc-attention-and-hybrid-ctc-attention\",\"176\":\"17\",\"177\":\"17#demo\",\"178\":\"17#asr-speech-recognition\",\"179\":\"17#se-speech-enhancement-separation\",\"180\":\"17#slu-spoken-language-understanding\",\"181\":\"17#tts-text-to-speech\",\"182\":\"17#other-utilities\",\"183\":\"17#espnet-ez\",\"184\":\"17#asr-speech-recognition-1\",\"185\":\"17#st-speech-to-text-translation\",\"186\":\"17#slu-spoken-language-understanding-1\",\"187\":\"17#tts-text-to-speech-1\",\"188\":\"17#svs-singing-voice-synthesis\",\"189\":\"17#course\",\"190\":\"17#cmu-speechprocessing-spring2023\",\"191\":\"17#cmu-speechrecognition-fall2022\",\"192\":\"17#cmu-speechrecognition-fall2021\",\"193\":\"17#espnet1-legacy\",\"194\":\"18\",\"195\":\"18#run-espnet-with-your-own-corpus\",\"196\":\"18#about-kaldi-style-data-directory\",\"197\":\"18#for-developers-how-to-make-port-new-recipe\",\"198\":\"19\",\"199\":\"19#table-of-contents\",\"200\":\"19#recipe-flow\",\"201\":\"19#how-to-run\",\"202\":\"19#related-works\",\"203\":\"20\",\"204\":\"20#table-of-contents\",\"205\":\"20#recipe-flow\",\"206\":\"20#how-to-run\",\"207\":\"20#related-works\",\"208\":\"21\",\"209\":\"22\",\"210\":\"22#table-of-contents\",\"211\":\"22#recipe-flow\",\"212\":\"22#how-to-run\",\"213\":\"22#about-data-directory\",\"214\":\"22#supported-models\",\"215\":\"23\",\"216\":\"23#table-of-contents\",\"217\":\"23#recipe-flow\",\"218\":\"23#how-to-run\",\"219\":\"23#supported-models\",\"220\":\"23#faq\",\"221\":\"24\",\"222\":\"25\",\"223\":\"25#introduction-to-enh-sh\",\"224\":\"25#for-developers-instructions-on-creating-a-new-recipe\",\"225\":\"25#instructions-on-creating-a-new-model\",\"226\":\"26\",\"227\":\"26#table-of-contents\",\"228\":\"26#introduction-to-enh-asr-sh\",\"229\":\"27\",\"230\":\"28\",\"231\":\"29\",\"232\":\"29#differences-from-other-recipes\",\"233\":\"30\",\"234\":\"30#table-of-contents\",\"235\":\"30#recipe-flow\",\"236\":\"30#how-to-run\",\"237\":\"31\",\"238\":\"32\",\"239\":\"33\",\"240\":\"34\",\"241\":\"34#table-of-contents\",\"242\":\"34#recipe-flow\",\"243\":\"34#how-to-run\",\"244\":\"34#related-work\",\"245\":\"35\",\"246\":\"35#features\",\"247\":\"35#how-to-use\",\"248\":\"35#requirements\",\"249\":\"35#troubleshooting\",\"250\":\"36\",\"251\":\"37\",\"252\":\"38\",\"253\":\"38#table-of-contents\",\"254\":\"38#recipe-flow\",\"255\":\"38#how-to-run\",\"256\":\"38#related-works\",\"257\":\"39\",\"258\":\"39#differences-from-other-recipes\",\"259\":\"39#hubert-pre-training-in-ssl1\",\"260\":\"40\",\"261\":\"40#results\",\"262\":\"40#offline-st-models\",\"263\":\"40#code-components\",\"264\":\"41\",\"265\":\"41#table-of-contents\",\"266\":\"41#recipe-flow\",\"267\":\"41#how-to-run\",\"268\":\"41#about-data-directory\",\"269\":\"41#score-preparation\",\"270\":\"41#supported-text-cleaner\",\"271\":\"41#supported-text-frontend\",\"272\":\"41#supported-models\",\"273\":\"42\",\"274\":\"42#table-of-contents\",\"275\":\"42#recipe-flow\",\"276\":\"42#how-to-run\",\"277\":\"42#about-data-directory\",\"278\":\"42#score-preparation\",\"279\":\"42#supported-text-cleaner\",\"280\":\"42#supported-text-frontend\",\"281\":\"42#supported-pretrained-model\",\"282\":\"42#supported-models\",\"283\":\"43\",\"284\":\"43#table-of-contents\",\"285\":\"43#recipe-flow\",\"286\":\"43#how-to-run\",\"287\":\"43#supported-text-frontend\",\"288\":\"43#supported-text-cleaner\",\"289\":\"43#supported-models\",\"290\":\"43#faq\",\"291\":\"44\",\"292\":\"45\",\"293\":\"46\",\"294\":\"46#named-arguments\",\"295\":\"47\",\"296\":\"47#named-arguments\",\"297\":\"47#model-configuration-related\",\"298\":\"47#text-converter-related\",\"299\":\"47#ctc-segmentation-related\",\"300\":\"47#input-output-arguments\",\"301\":\"48\",\"302\":\"48#named-arguments\",\"303\":\"48#input-data-related\",\"304\":\"48#the-model-configuration-related\",\"305\":\"48#quantization-related\",\"306\":\"48#beam-search-related\",\"307\":\"48#text-converter-related\",\"308\":\"48#partially-ar-related\",\"309\":\"49\",\"310\":\"49#named-arguments\",\"311\":\"49#input-data-related\",\"312\":\"49#the-model-configuration-related\",\"313\":\"49#decoding-related\",\"314\":\"49#text-converter-related\",\"315\":\"50\",\"316\":\"50#named-arguments\",\"317\":\"50#input-data-related\",\"318\":\"50#the-model-configuration-related\",\"319\":\"50#beam-search-related\",\"320\":\"50#text-converter-related\",\"321\":\"51\",\"322\":\"51#named-arguments\",\"323\":\"51#input-data-related\",\"324\":\"51#the-model-configuration-related\",\"325\":\"51#beam-search-related\",\"326\":\"51#text-converter-related\",\"327\":\"52\",\"328\":\"52#named-arguments\",\"329\":\"52#input-data-related\",\"330\":\"52#the-model-configuration-related\",\"331\":\"53\",\"332\":\"53#named-arguments\",\"333\":\"53#input-data-related\",\"334\":\"53#the-model-configuration-related\",\"335\":\"54\",\"336\":\"54#named-arguments\",\"337\":\"54#input-data-related\",\"338\":\"54#the-model-configuration-related\",\"339\":\"54#data-loading-related\",\"340\":\"54#diarize-speech-related\",\"341\":\"54#enh-diar-related\",\"342\":\"55\",\"343\":\"55#named-arguments\",\"344\":\"55#input-data-related\",\"345\":\"55#output-data-related\",\"346\":\"55#the-model-configuration-related\",\"347\":\"55#data-loading-related\",\"348\":\"55#separatespeech-related\",\"349\":\"56\",\"350\":\"56#named-arguments\",\"351\":\"56#input-data-related\",\"352\":\"56#output-data-related\",\"353\":\"56#the-model-configuration-related\",\"354\":\"56#data-loading-related\",\"355\":\"56#separatespeech-related\",\"356\":\"57\",\"357\":\"57#named-arguments\",\"358\":\"57#input-data-related\",\"359\":\"57#dnsmos-related\",\"360\":\"57#pesq-related\",\"361\":\"58\",\"362\":\"58#named-arguments\",\"363\":\"58#input-data-related\",\"364\":\"58#output-data-related\",\"365\":\"58#the-model-configuration-related\",\"366\":\"58#data-loading-related\",\"367\":\"58#separatespeech-related\",\"368\":\"59\",\"369\":\"59#named-arguments\",\"370\":\"59#input-data-related\",\"371\":\"59#the-model-configuration-related\",\"372\":\"60\",\"373\":\"60#named-arguments\",\"374\":\"61\",\"375\":\"61#positional-arguments\",\"376\":\"61#named-arguments\",\"377\":\"62\",\"378\":\"62#named-arguments\",\"379\":\"62#input-data-related\",\"380\":\"62#the-model-configuration-related\",\"381\":\"62#distributed-training-related\",\"382\":\"62#trainer-initialization-related\",\"383\":\"62#cudnn-mode-related\",\"384\":\"62#the-inference-hyperparameter-related\",\"385\":\"63\",\"386\":\"63#named-arguments\",\"387\":\"63#input-data-related\",\"388\":\"63#the-model-configuration-related\",\"389\":\"64\",\"390\":\"64#named-arguments\",\"391\":\"64#input-data-related\",\"392\":\"64#the-model-configuration-related\",\"393\":\"64#quantization-related\",\"394\":\"64#beam-search-related\",\"395\":\"64#text-converter-related\",\"396\":\"65\",\"397\":\"65#named-arguments\",\"398\":\"65#input-data-related\",\"399\":\"65#the-model-configuration-related\",\"400\":\"65#beam-search-related\",\"401\":\"65#text-converter-related\",\"402\":\"66\",\"403\":\"66#sub-commands\",\"404\":\"67\",\"405\":\"67#named-arguments\",\"406\":\"68\",\"407\":\"68#named-arguments\",\"408\":\"68#input-data-related\",\"409\":\"68#the-model-configuration-related\",\"410\":\"68#decoding-related\",\"411\":\"68#spectrogram-based-generation-related\",\"412\":\"68#beam-search-discrete-unit-multi-pass-related\",\"413\":\"68#vocoder-related\",\"414\":\"68#text-converter-related\",\"415\":\"69\",\"416\":\"69#named-arguments\",\"417\":\"69#model-configuration-related\",\"418\":\"69#text-converter-related\",\"419\":\"69#ctc-segmentation-related\",\"420\":\"69#input-output-arguments\",\"421\":\"70\",\"422\":\"70#named-arguments\",\"423\":\"70#input-data-related\",\"424\":\"70#model-configuration-related\",\"425\":\"70#quantization-related\",\"426\":\"70#beam-search-related\",\"427\":\"70#text-converter-related\",\"428\":\"70#partially-ar-related\",\"429\":\"71\",\"430\":\"71#named-arguments\",\"431\":\"71#input-data-related\",\"432\":\"71#the-model-configuration-related\",\"433\":\"71#quantization-related\",\"434\":\"71#beam-search-related\",\"435\":\"71#text-converter-related\",\"436\":\"72\",\"437\":\"72#named-arguments\",\"438\":\"72#input-data-related\",\"439\":\"72#model-configuration-related\",\"440\":\"72#quantization-related\",\"441\":\"72#beam-search-related\",\"442\":\"73\",\"443\":\"73#named-arguments\",\"444\":\"73#input-data-related\",\"445\":\"73#the-model-configuration-related\",\"446\":\"73#quantization-related\",\"447\":\"73#beam-search-related\",\"448\":\"73#text-converter-related\",\"449\":\"74\",\"450\":\"74#named-arguments\",\"451\":\"74#input-data-related\",\"452\":\"74#the-model-configuration-related\",\"453\":\"74#distributed-training-related\",\"454\":\"74#trainer-initialization-related\",\"455\":\"74#cudnn-mode-related\",\"456\":\"74#the-inference-hyperparameter-related\",\"457\":\"75\",\"458\":\"75#named-arguments\",\"459\":\"75#input-data-related\",\"460\":\"75#the-model-configuration-related\",\"461\":\"76\",\"462\":\"76#named-arguments\",\"463\":\"77\",\"464\":\"77#named-arguments\",\"465\":\"77#input-data-related\",\"466\":\"77#the-model-configuration-related\",\"467\":\"77#beam-search-related\",\"468\":\"77#text-converter-related\",\"469\":\"78\",\"470\":\"78#named-arguments\",\"471\":\"78#input-data-related\",\"472\":\"78#the-model-configuration-related\",\"473\":\"78#beam-search-related\",\"474\":\"78#text-converter-related\",\"475\":\"79\",\"476\":\"79#named-arguments\",\"477\":\"79#input-data-related\",\"478\":\"79#the-model-configuration-related\",\"479\":\"79#decoding-related\",\"480\":\"79#vocoder-related\",\"481\":\"80\",\"482\":\"80#named-arguments\",\"483\":\"80#write-vocabulary-mode-related\",\"484\":\"81\",\"485\":\"81#named-arguments\",\"486\":\"81#input-data-related\",\"487\":\"81#the-model-configuration-related\",\"488\":\"81#decoding-related\",\"489\":\"81#vocoder-related\",\"490\":\"82\",\"491\":\"82#named-arguments\",\"492\":\"82#input-data-related\",\"493\":\"82#the-model-configuration-related\",\"494\":\"82#decoding-related\",\"495\":\"82#vocoder-related\",\"496\":\"83\",\"497\":\"83#named-arguments\",\"498\":\"84\",\"499\":\"84#named-arguments\",\"500\":\"84#input-data-related\",\"501\":\"84#the-model-configuration-related\",\"502\":\"84#quantization-related\",\"503\":\"84#beam-search-related\",\"504\":\"84#text-converter-related\",\"505\":\"85\",\"506\":\"85#named-arguments\",\"507\":\"85#input-data-related\",\"508\":\"85#the-model-configuration-related\",\"509\":\"85#beam-search-related\",\"510\":\"85#text-converter-related\",\"511\":\"86\",\"512\":\"86#named-arguments\",\"513\":\"87\",\"514\":\"88\",\"515\":\"89\",\"516\":\"90\",\"517\":\"91\",\"518\":\"92\",\"519\":\"93\",\"520\":\"94\",\"521\":\"95\",\"522\":\"96\",\"523\":\"97\",\"524\":\"98\",\"525\":\"99\",\"526\":\"100\",\"527\":\"101\",\"528\":\"102\",\"529\":\"103\",\"530\":\"104\",\"531\":\"105\",\"532\":\"106\",\"533\":\"107\",\"534\":\"108\",\"535\":\"109\",\"536\":\"110\",\"537\":\"111\",\"538\":\"112\",\"539\":\"112#positional-arguments\",\"540\":\"112#named-arguments\",\"541\":\"113\",\"542\":\"113#named-arguments\",\"543\":\"114\",\"544\":\"114#named-arguments\",\"545\":\"115\",\"546\":\"115#positional-arguments\",\"547\":\"115#named-arguments\",\"548\":\"116\",\"549\":\"116#positional-arguments\",\"550\":\"116#named-arguments\",\"551\":\"117\",\"552\":\"117#positional-arguments\",\"553\":\"117#named-arguments\",\"554\":\"118\",\"555\":\"118#positional-arguments\",\"556\":\"119\",\"557\":\"119#positional-arguments\",\"558\":\"120\",\"559\":\"120#positional-arguments\",\"560\":\"120#named-arguments\",\"561\":\"121\",\"562\":\"121#positional-arguments\",\"563\":\"121#named-arguments\",\"564\":\"122\",\"565\":\"122#positional-arguments\",\"566\":\"122#named-arguments\",\"567\":\"123\",\"568\":\"123#positional-arguments\",\"569\":\"123#named-arguments\",\"570\":\"124\",\"571\":\"124#positional-arguments\",\"572\":\"125\",\"573\":\"125#positional-arguments\",\"574\":\"125#named-arguments\",\"575\":\"126\",\"576\":\"126#positional-arguments\",\"577\":\"126#named-arguments\",\"578\":\"127\",\"579\":\"127#positional-arguments\",\"580\":\"127#named-arguments\",\"581\":\"128\",\"582\":\"128#positional-arguments\",\"583\":\"129\",\"584\":\"129#positional-arguments\",\"585\":\"129#named-arguments\",\"586\":\"130\",\"587\":\"130#positional-arguments\",\"588\":\"130#named-arguments\",\"589\":\"131\",\"590\":\"131#positional-arguments\",\"591\":\"131#named-arguments\",\"592\":\"132\",\"593\":\"132#positional-arguments\",\"594\":\"133\",\"595\":\"133#named-arguments\",\"596\":\"134\",\"597\":\"134#named-arguments\",\"598\":\"135\",\"599\":\"135#named-arguments\",\"600\":\"136\",\"601\":\"136#positional-arguments\",\"602\":\"136#named-arguments\",\"603\":\"137\",\"604\":\"137#positional-arguments\",\"605\":\"137#named-arguments\",\"606\":\"138\",\"607\":\"138#positional-arguments\",\"608\":\"138#named-arguments\",\"609\":\"139\",\"610\":\"139#positional-arguments\",\"611\":\"140\",\"612\":\"140#positional-arguments\",\"613\":\"140#named-arguments\",\"614\":\"141\",\"615\":\"142\",\"616\":\"143\",\"617\":\"144\",\"618\":\"145\",\"619\":\"146\",\"620\":\"147\",\"621\":\"148\",\"622\":\"149\",\"623\":\"150\",\"624\":\"151\",\"625\":\"152\",\"626\":\"153\",\"627\":\"154\",\"628\":\"155\",\"629\":\"156\",\"630\":\"157\",\"631\":\"158\",\"632\":\"159\",\"633\":\"160\",\"634\":\"161\",\"635\":\"162\",\"636\":\"163\",\"637\":\"164\",\"638\":\"165\",\"639\":\"166\",\"640\":\"167\",\"641\":\"168\",\"642\":\"169\",\"643\":\"170\",\"644\":\"171\",\"645\":\"172\",\"646\":\"173\",\"647\":\"174\",\"648\":\"175\",\"649\":\"176\",\"650\":\"177\",\"651\":\"178\",\"652\":\"179\",\"653\":\"180\",\"654\":\"181\",\"655\":\"182\",\"656\":\"183\",\"657\":\"184\",\"658\":\"185\",\"659\":\"186\",\"660\":\"187\",\"661\":\"188\",\"662\":\"189\",\"663\":\"190\",\"664\":\"191\",\"665\":\"192\",\"666\":\"193\",\"667\":\"194\",\"668\":\"195\",\"669\":\"196\",\"670\":\"197\",\"671\":\"198\",\"672\":\"199\",\"673\":\"200\",\"674\":\"201\",\"675\":\"202\",\"676\":\"203\",\"677\":\"203#note\",\"678\":\"204\",\"679\":\"204#note\",\"680\":\"205\",\"681\":\"205#note\",\"682\":\"206\",\"683\":\"206#note\",\"684\":\"207\",\"685\":\"207#note\",\"686\":\"208\",\"687\":\"208#note\",\"688\":\"209\",\"689\":\"210\",\"690\":\"210#note\",\"691\":\"211\",\"692\":\"212\",\"693\":\"213\",\"694\":\"213#note\",\"695\":\"214\",\"696\":\"215\",\"697\":\"216\",\"698\":\"217\",\"699\":\"218\",\"700\":\"219\",\"701\":\"220\",\"702\":\"221\",\"703\":\"222\",\"704\":\"223\",\"705\":\"223#note\",\"706\":\"224\",\"707\":\"225\",\"708\":\"226\",\"709\":\"227\",\"710\":\"228\",\"711\":\"229\",\"712\":\"230\",\"713\":\"231\",\"714\":\"231#note\",\"715\":\"232\",\"716\":\"233\",\"717\":\"234\",\"718\":\"235\",\"719\":\"235#note\",\"720\":\"236\",\"721\":\"236#note\",\"722\":\"237\",\"723\":\"237#note\",\"724\":\"238\",\"725\":\"239\",\"726\":\"240\",\"727\":\"241\",\"728\":\"242\",\"729\":\"243\",\"730\":\"244\",\"731\":\"245\",\"732\":\"246\",\"733\":\"247\",\"734\":\"248\",\"735\":\"249\",\"736\":\"250\",\"737\":\"251\",\"738\":\"252\",\"739\":\"252#note\",\"740\":\"253\",\"741\":\"254\",\"742\":\"254#note\",\"743\":\"255\",\"744\":\"256\",\"745\":\"257\",\"746\":\"258\",\"747\":\"259\",\"748\":\"260\",\"749\":\"261\",\"750\":\"262\",\"751\":\"263\",\"752\":\"264\",\"753\":\"264#note\",\"754\":\"265\",\"755\":\"266\",\"756\":\"267\",\"757\":\"268\",\"758\":\"268#note\",\"759\":\"269\",\"760\":\"270\",\"761\":\"271\",\"762\":\"272\",\"763\":\"273\",\"764\":\"274\",\"765\":\"275\",\"766\":\"276\",\"767\":\"277\",\"768\":\"278\",\"769\":\"279\",\"770\":\"280\",\"771\":\"281\",\"772\":\"282\",\"773\":\"283\",\"774\":\"284\",\"775\":\"285\",\"776\":\"286\",\"777\":\"287\",\"778\":\"288\",\"779\":\"288#note\",\"780\":\"289\",\"781\":\"290\",\"782\":\"290#note\",\"783\":\"291\",\"784\":\"292\",\"785\":\"293\",\"786\":\"294\",\"787\":\"295\",\"788\":\"296\",\"789\":\"296#note\",\"790\":\"297\",\"791\":\"298\",\"792\":\"298#note\",\"793\":\"299\",\"794\":\"300\",\"795\":\"301\",\"796\":\"302\",\"797\":\"302#note\",\"798\":\"303\",\"799\":\"303#note\",\"800\":\"304\",\"801\":\"305\",\"802\":\"306\",\"803\":\"307\",\"804\":\"307#note\",\"805\":\"308\",\"806\":\"308#note\",\"807\":\"309\",\"808\":\"309#note\",\"809\":\"310\",\"810\":\"310#note\",\"811\":\"311\",\"812\":\"311#note\",\"813\":\"312\",\"814\":\"312#note\",\"815\":\"313\",\"816\":\"313#note\",\"817\":\"314\",\"818\":\"314#position-wise-feedforward-components\",\"819\":\"314#other-arguments\",\"820\":\"315\",\"821\":\"316\",\"822\":\"316#note\",\"823\":\"317\",\"824\":\"318\",\"825\":\"319\",\"826\":\"319#note\",\"827\":\"320\",\"828\":\"321\",\"829\":\"322\",\"830\":\"323\",\"831\":\"324\",\"832\":\"325\",\"833\":\"326\",\"834\":\"326#note\",\"835\":\"327\",\"836\":\"327#note\",\"837\":\"328\",\"838\":\"328#note\",\"839\":\"329\",\"840\":\"329#note\",\"841\":\"330\",\"842\":\"331\",\"843\":\"331#note\",\"844\":\"332\",\"845\":\"332#note\",\"846\":\"333\",\"847\":\"334\",\"848\":\"335\",\"849\":\"336\",\"850\":\"337\",\"851\":\"338\",\"852\":\"339\",\"853\":\"339#note\",\"854\":\"340\",\"855\":\"340#note\",\"856\":\"341\",\"857\":\"341#note\",\"858\":\"342\",\"859\":\"343\",\"860\":\"344\",\"861\":\"344#note\",\"862\":\"345\",\"863\":\"345#note\",\"864\":\"346\",\"865\":\"346#note\",\"866\":\"347\",\"867\":\"348\",\"868\":\"349\",\"869\":\"350\",\"870\":\"351\",\"871\":\"352\",\"872\":\"353\",\"873\":\"354\",\"874\":\"355\",\"875\":\"356\",\"876\":\"357\",\"877\":\"358\",\"878\":\"359\",\"879\":\"360\",\"880\":\"361\",\"881\":\"362\",\"882\":\"363\",\"883\":\"364\",\"884\":\"365\",\"885\":\"366\",\"886\":\"367\",\"887\":\"368\",\"888\":\"369\",\"889\":\"370\",\"890\":\"371\",\"891\":\"372\",\"892\":\"373\",\"893\":\"374\",\"894\":\"375\",\"895\":\"376\",\"896\":\"377\",\"897\":\"378\",\"898\":\"379\",\"899\":\"380\",\"900\":\"381\",\"901\":\"382\",\"902\":\"383\",\"903\":\"384\",\"904\":\"385\",\"905\":\"386\",\"906\":\"387\",\"907\":\"388\",\"908\":\"389\",\"909\":\"390\",\"910\":\"391\",\"911\":\"392\",\"912\":\"393\",\"913\":\"394\",\"914\":\"395\",\"915\":\"396\",\"916\":\"397\",\"917\":\"398\",\"918\":\"399\",\"919\":\"400\",\"920\":\"401\",\"921\":\"402\",\"922\":\"403\",\"923\":\"404\",\"924\":\"405\",\"925\":\"406\",\"926\":\"407\",\"927\":\"408\",\"928\":\"409\",\"929\":\"410\",\"930\":\"411\",\"931\":\"412\",\"932\":\"412#note\",\"933\":\"413\",\"934\":\"413#note\",\"935\":\"414\",\"936\":\"415\",\"937\":\"416\",\"938\":\"417\",\"939\":\"418\",\"940\":\"419\",\"941\":\"420\",\"942\":\"421\",\"943\":\"422\",\"944\":\"423\",\"945\":\"424\",\"946\":\"425\",\"947\":\"426\",\"948\":\"427\",\"949\":\"428\",\"950\":\"429\",\"951\":\"429#note\",\"952\":\"430\",\"953\":\"430#note\",\"954\":\"431\",\"955\":\"432\",\"956\":\"433\",\"957\":\"433#note\",\"958\":\"434\",\"959\":\"435\",\"960\":\"436\",\"961\":\"436#note\",\"962\":\"437\",\"963\":\"438\",\"964\":\"438#note\",\"965\":\"439\",\"966\":\"439#note\",\"967\":\"440\",\"968\":\"440#note\",\"969\":\"441\",\"970\":\"441#note\",\"971\":\"442\",\"972\":\"443\",\"973\":\"444\",\"974\":\"445\",\"975\":\"446\",\"976\":\"447\",\"977\":\"448\",\"978\":\"449\",\"979\":\"450\",\"980\":\"451\",\"981\":\"452\",\"982\":\"453\",\"983\":\"454\",\"984\":\"455\",\"985\":\"456\",\"986\":\"456#examples\",\"987\":\"457\",\"988\":\"457#examples\",\"989\":\"458\",\"990\":\"458#examples\",\"991\":\"459\",\"992\":\"459#examples\",\"993\":\"460\",\"994\":\"460#examples\",\"995\":\"461\",\"996\":\"462\",\"997\":\"462#examples\",\"998\":\"463\",\"999\":\"463#examples\",\"1000\":\"464\",\"1001\":\"465\",\"1002\":\"465#examples\",\"1003\":\"466\",\"1004\":\"466#examples\",\"1005\":\"467\",\"1006\":\"467#examples\",\"1007\":\"468\",\"1008\":\"468#examples\",\"1009\":\"469\",\"1010\":\"469#examples\",\"1011\":\"470\",\"1012\":\"470#examples\",\"1013\":\"471\",\"1014\":\"471#examples\",\"1015\":\"472\",\"1016\":\"472#examples\",\"1017\":\"473\",\"1018\":\"473#examples\",\"1019\":\"474\",\"1020\":\"474#examples\",\"1021\":\"475\",\"1022\":\"476\",\"1023\":\"476#examples\",\"1024\":\"477\",\"1025\":\"477#examples\",\"1026\":\"478\",\"1027\":\"478#examples\",\"1028\":\"479\",\"1029\":\"480\",\"1030\":\"481\",\"1031\":\"481#note\",\"1032\":\"482\",\"1033\":\"482#note\",\"1034\":\"483\",\"1035\":\"483#note\",\"1036\":\"484\",\"1037\":\"484#note\",\"1038\":\"485\",\"1039\":\"485#note\",\"1040\":\"486\",\"1041\":\"486#note\",\"1042\":\"487\",\"1043\":\"487#note\",\"1044\":\"488\",\"1045\":\"488#note\",\"1046\":\"489\",\"1047\":\"489#note\",\"1048\":\"490\",\"1049\":\"490#note\",\"1050\":\"491\",\"1051\":\"492\",\"1052\":\"492#note\",\"1053\":\"493\",\"1054\":\"494\",\"1055\":\"495\",\"1056\":\"495#note\",\"1057\":\"496\",\"1058\":\"496#note\",\"1059\":\"497\",\"1060\":\"497#note\",\"1061\":\"498\",\"1062\":\"499\",\"1063\":\"500\",\"1064\":\"501\",\"1065\":\"502\",\"1066\":\"503\",\"1067\":\"503#note\",\"1068\":\"504\",\"1069\":\"504#note\",\"1070\":\"505\",\"1071\":\"506\",\"1072\":\"507\",\"1073\":\"508\",\"1074\":\"509\",\"1075\":\"510\",\"1076\":\"511\",\"1077\":\"511#note\",\"1078\":\"512\",\"1079\":\"512#note\",\"1080\":\"513\",\"1081\":\"513#note\",\"1082\":\"514\",\"1083\":\"514#note\",\"1084\":\"515\",\"1085\":\"515#note\",\"1086\":\"516\",\"1087\":\"517\",\"1088\":\"517#note\",\"1089\":\"518\",\"1090\":\"518#note\",\"1091\":\"519\",\"1092\":\"519#note\",\"1093\":\"520\",\"1094\":\"520#note\",\"1095\":\"521\",\"1096\":\"521#note\",\"1097\":\"522\",\"1098\":\"522#note\",\"1099\":\"523\",\"1100\":\"523#note\",\"1101\":\"524\",\"1102\":\"524#note\",\"1103\":\"525\",\"1104\":\"525#note\",\"1105\":\"526\",\"1106\":\"526#note\",\"1107\":\"527\",\"1108\":\"528\",\"1109\":\"528#note\",\"1110\":\"529\",\"1111\":\"529#note\",\"1112\":\"530\",\"1113\":\"531\",\"1114\":\"532\",\"1115\":\"532#note\",\"1116\":\"533\",\"1117\":\"534\",\"1118\":\"535\",\"1119\":\"536\",\"1120\":\"537\",\"1121\":\"537#note\",\"1122\":\"538\",\"1123\":\"538#note\",\"1124\":\"539\",\"1125\":\"540\",\"1126\":\"541\",\"1127\":\"542\",\"1128\":\"543\",\"1129\":\"544\",\"1130\":\"545\",\"1131\":\"546\",\"1132\":\"547\",\"1133\":\"548\",\"1134\":\"549\",\"1135\":\"549#note\",\"1136\":\"550\",\"1137\":\"551\",\"1138\":\"551#note\",\"1139\":\"552\",\"1140\":\"552#note\",\"1141\":\"553\",\"1142\":\"554\",\"1143\":\"554#note\",\"1144\":\"555\",\"1145\":\"556\",\"1146\":\"556#note\",\"1147\":\"557\",\"1148\":\"558\",\"1149\":\"559\",\"1150\":\"559#note\",\"1151\":\"560\",\"1152\":\"560#note\",\"1153\":\"561\",\"1154\":\"561#note\",\"1155\":\"562\",\"1156\":\"563\",\"1157\":\"564\",\"1158\":\"565\",\"1159\":\"566\",\"1160\":\"566#note\",\"1161\":\"567\",\"1162\":\"568\",\"1163\":\"569\",\"1164\":\"570\",\"1165\":\"571\",\"1166\":\"571#note\",\"1167\":\"572\",\"1168\":\"573\",\"1169\":\"573#note\",\"1170\":\"574\",\"1171\":\"575\",\"1172\":\"576\",\"1173\":\"577\",\"1174\":\"578\",\"1175\":\"579\",\"1176\":\"580\",\"1177\":\"581\",\"1178\":\"581#note\",\"1179\":\"582\",\"1180\":\"583\",\"1181\":\"584\",\"1182\":\"585\",\"1183\":\"586\",\"1184\":\"587\",\"1185\":\"588\",\"1186\":\"588#note\",\"1187\":\"589\",\"1188\":\"589#note\",\"1189\":\"590\",\"1190\":\"591\",\"1191\":\"591#note\",\"1192\":\"592\",\"1193\":\"592#note\",\"1194\":\"593\",\"1195\":\"593#note\",\"1196\":\"594\",\"1197\":\"594#note\",\"1198\":\"595\",\"1199\":\"596\",\"1200\":\"597\",\"1201\":\"597#note\",\"1202\":\"598\",\"1203\":\"598#note\",\"1204\":\"599\",\"1205\":\"600\",\"1206\":\"600#note\",\"1207\":\"601\",\"1208\":\"602\",\"1209\":\"603\",\"1210\":\"604\",\"1211\":\"605\",\"1212\":\"605#note\",\"1213\":\"606\",\"1214\":\"606#note\",\"1215\":\"607\",\"1216\":\"607#note\",\"1217\":\"608\",\"1218\":\"609\",\"1219\":\"610\",\"1220\":\"610#note\",\"1221\":\"611\",\"1222\":\"612\",\"1223\":\"613\",\"1224\":\"614\",\"1225\":\"615\",\"1226\":\"616\",\"1227\":\"616#note\",\"1228\":\"617\",\"1229\":\"618\",\"1230\":\"619\",\"1231\":\"619#note\",\"1232\":\"620\",\"1233\":\"621\",\"1234\":\"621#note\",\"1235\":\"622\",\"1236\":\"623\",\"1237\":\"623#note\",\"1238\":\"624\",\"1239\":\"624#note\",\"1240\":\"625\",\"1241\":\"625#note\",\"1242\":\"626\",\"1243\":\"626#note\",\"1244\":\"627\",\"1245\":\"628\",\"1246\":\"629\",\"1247\":\"630\",\"1248\":\"631\",\"1249\":\"631#note\",\"1250\":\"632\",\"1251\":\"633\",\"1252\":\"634\",\"1253\":\"635\",\"1254\":\"635#note\",\"1255\":\"636\",\"1256\":\"636#note\",\"1257\":\"637\",\"1258\":\"637#note\",\"1259\":\"638\",\"1260\":\"638#note\",\"1261\":\"639\",\"1262\":\"640\",\"1263\":\"640#note\",\"1264\":\"641\",\"1265\":\"642\",\"1266\":\"642#note\",\"1267\":\"643\",\"1268\":\"644\",\"1269\":\"645\",\"1270\":\"646\",\"1271\":\"647\",\"1272\":\"648\",\"1273\":\"649\",\"1274\":\"650\",\"1275\":\"651\",\"1276\":\"652\",\"1277\":\"653\",\"1278\":\"654\",\"1279\":\"655\",\"1280\":\"656\",\"1281\":\"657\",\"1282\":\"658\",\"1283\":\"659\",\"1284\":\"660\",\"1285\":\"660#note\",\"1286\":\"661\",\"1287\":\"661#note\",\"1288\":\"662\",\"1289\":\"662#note\",\"1290\":\"663\",\"1291\":\"664\",\"1292\":\"665\",\"1293\":\"666\",\"1294\":\"667\",\"1295\":\"668\",\"1296\":\"669\",\"1297\":\"670\",\"1298\":\"671\",\"1299\":\"672\",\"1300\":\"673\",\"1301\":\"674\",\"1302\":\"675\",\"1303\":\"676\",\"1304\":\"677\",\"1305\":\"678\",\"1306\":\"679\",\"1307\":\"680\",\"1308\":\"681\",\"1309\":\"682\",\"1310\":\"683\",\"1311\":\"684\",\"1312\":\"685\",\"1313\":\"686\",\"1314\":\"687\",\"1315\":\"688\",\"1316\":\"689\",\"1317\":\"690\",\"1318\":\"691\",\"1319\":\"692\",\"1320\":\"693\",\"1321\":\"694\",\"1322\":\"695\",\"1323\":\"696\",\"1324\":\"697\",\"1325\":\"698\",\"1326\":\"699\",\"1327\":\"700\",\"1328\":\"701\",\"1329\":\"702\",\"1330\":\"703\",\"1331\":\"704\",\"1332\":\"705\",\"1333\":\"706\",\"1334\":\"707\",\"1335\":\"708\",\"1336\":\"709\",\"1337\":\"710\",\"1338\":\"711\",\"1339\":\"712\",\"1340\":\"713\",\"1341\":\"714\",\"1342\":\"715\",\"1343\":\"716\",\"1344\":\"717\",\"1345\":\"718\",\"1346\":\"719\",\"1347\":\"720\",\"1348\":\"721\",\"1349\":\"722\",\"1350\":\"723\",\"1351\":\"724\",\"1352\":\"725\",\"1353\":\"726\",\"1354\":\"727\",\"1355\":\"728\",\"1356\":\"729\",\"1357\":\"730\",\"1358\":\"731\",\"1359\":\"732\",\"1360\":\"733\",\"1361\":\"734\",\"1362\":\"735\",\"1363\":\"736\",\"1364\":\"737\",\"1365\":\"738\",\"1366\":\"739\",\"1367\":\"740\",\"1368\":\"741\",\"1369\":\"742\",\"1370\":\"743\",\"1371\":\"744\",\"1372\":\"745\",\"1373\":\"746\",\"1374\":\"747\",\"1375\":\"748\",\"1376\":\"749\",\"1377\":\"750\",\"1378\":\"751\",\"1379\":\"752\",\"1380\":\"753\",\"1381\":\"754\",\"1382\":\"755\",\"1383\":\"756\",\"1384\":\"756#note\",\"1385\":\"757\",\"1386\":\"758\",\"1387\":\"759\",\"1388\":\"759#note\",\"1389\":\"760\",\"1390\":\"761\",\"1391\":\"762\",\"1392\":\"763\",\"1393\":\"763#note\",\"1394\":\"764\",\"1395\":\"765\",\"1396\":\"766\",\"1397\":\"767\",\"1398\":\"768\",\"1399\":\"768#note\",\"1400\":\"769\",\"1401\":\"770\",\"1402\":\"771\",\"1403\":\"772\",\"1404\":\"773\",\"1405\":\"773#note\",\"1406\":\"774\",\"1407\":\"774#note\",\"1408\":\"775\",\"1409\":\"776\",\"1410\":\"777\",\"1411\":\"778\",\"1412\":\"778#note\",\"1413\":\"779\",\"1414\":\"779#note\",\"1415\":\"780\",\"1416\":\"780#note\",\"1417\":\"781\",\"1418\":\"781#note\",\"1419\":\"782\",\"1420\":\"783\",\"1421\":\"783#note\",\"1422\":\"784\",\"1423\":\"784#note\",\"1424\":\"785\",\"1425\":\"785#note\",\"1426\":\"786\",\"1427\":\"786#note\",\"1428\":\"787\",\"1429\":\"787#note\",\"1430\":\"788\",\"1431\":\"788#note\",\"1432\":\"789\",\"1433\":\"790\",\"1434\":\"790#note\",\"1435\":\"791\",\"1436\":\"791#note\",\"1437\":\"792\",\"1438\":\"792#note\",\"1439\":\"793\",\"1440\":\"793#note\",\"1441\":\"794\",\"1442\":\"795\",\"1443\":\"795#note\",\"1444\":\"796\",\"1445\":\"796#note\",\"1446\":\"797\",\"1447\":\"797#note\",\"1448\":\"798\",\"1449\":\"798#note\",\"1450\":\"799\",\"1451\":\"799#note\",\"1452\":\"800\",\"1453\":\"800#note\",\"1454\":\"801\",\"1455\":\"801#note\",\"1456\":\"802\",\"1457\":\"802#note\",\"1458\":\"803\",\"1459\":\"803#note\",\"1460\":\"804\",\"1461\":\"804#note\",\"1462\":\"805\",\"1463\":\"805#note\",\"1464\":\"806\",\"1465\":\"806#note\",\"1466\":\"807\",\"1467\":\"808\",\"1468\":\"809\",\"1469\":\"810\",\"1470\":\"810#note\",\"1471\":\"811\",\"1472\":\"812\",\"1473\":\"813\",\"1474\":\"814\",\"1475\":\"815\",\"1476\":\"816\",\"1477\":\"817\",\"1478\":\"818\",\"1479\":\"819\",\"1480\":\"820\",\"1481\":\"821\",\"1482\":\"822\",\"1483\":\"823\",\"1484\":\"824\",\"1485\":\"825\",\"1486\":\"826\",\"1487\":\"827\",\"1488\":\"828\",\"1489\":\"829\",\"1490\":\"830\",\"1491\":\"831\",\"1492\":\"832\",\"1493\":\"833\",\"1494\":\"834\",\"1495\":\"835\",\"1496\":\"836\",\"1497\":\"837\",\"1498\":\"838\",\"1499\":\"839\",\"1500\":\"840\",\"1501\":\"841\",\"1502\":\"842\",\"1503\":\"843\",\"1504\":\"844\",\"1505\":\"845\",\"1506\":\"846\",\"1507\":\"847\",\"1508\":\"848\",\"1509\":\"849\",\"1510\":\"849#note\",\"1511\":\"850\",\"1512\":\"850#note\",\"1513\":\"851\",\"1514\":\"852\",\"1515\":\"853\",\"1516\":\"854\",\"1517\":\"855\",\"1518\":\"855#note\",\"1519\":\"856\",\"1520\":\"857\",\"1521\":\"858\",\"1522\":\"859\",\"1523\":\"859#note\",\"1524\":\"860\",\"1525\":\"861\",\"1526\":\"862\",\"1527\":\"863\",\"1528\":\"863#note\",\"1529\":\"864\",\"1530\":\"865\",\"1531\":\"865#note\",\"1532\":\"866\",\"1533\":\"867\",\"1534\":\"868\",\"1535\":\"869\",\"1536\":\"870\",\"1537\":\"871\",\"1538\":\"871#note\",\"1539\":\"872\",\"1540\":\"872#note\",\"1541\":\"873\",\"1542\":\"873#note\",\"1543\":\"874\",\"1544\":\"874#note\",\"1545\":\"875\",\"1546\":\"876\",\"1547\":\"877\",\"1548\":\"878\",\"1549\":\"879\",\"1550\":\"879#note\",\"1551\":\"880\",\"1552\":\"881\",\"1553\":\"882\",\"1554\":\"883\",\"1555\":\"883#note\",\"1556\":\"884\",\"1557\":\"885\",\"1558\":\"886\",\"1559\":\"887\",\"1560\":\"888\",\"1561\":\"889\",\"1562\":\"890\",\"1563\":\"891\",\"1564\":\"892\",\"1565\":\"893\",\"1566\":\"894\",\"1567\":\"895\",\"1568\":\"896\",\"1569\":\"897\",\"1570\":\"898\",\"1571\":\"899\",\"1572\":\"900\",\"1573\":\"901\",\"1574\":\"902\",\"1575\":\"903\",\"1576\":\"904\",\"1577\":\"905\",\"1578\":\"906\",\"1579\":\"907\",\"1580\":\"908\",\"1581\":\"909\",\"1582\":\"910\",\"1583\":\"911\",\"1584\":\"912\",\"1585\":\"913\",\"1586\":\"914\",\"1587\":\"915\",\"1588\":\"916\",\"1589\":\"917\",\"1590\":\"918\",\"1591\":\"919\",\"1592\":\"920\",\"1593\":\"921\",\"1594\":\"922\",\"1595\":\"923\",\"1596\":\"924\",\"1597\":\"925\",\"1598\":\"926\",\"1599\":\"927\",\"1600\":\"928\",\"1601\":\"929\",\"1602\":\"930\",\"1603\":\"931\",\"1604\":\"932\",\"1605\":\"933\",\"1606\":\"934\",\"1607\":\"935\",\"1608\":\"936\",\"1609\":\"937\",\"1610\":\"938\",\"1611\":\"939\",\"1612\":\"940\",\"1613\":\"941\",\"1614\":\"942\",\"1615\":\"943\",\"1616\":\"944\",\"1617\":\"945\",\"1618\":\"946\",\"1619\":\"947\",\"1620\":\"948\",\"1621\":\"949\",\"1622\":\"950\",\"1623\":\"951\",\"1624\":\"952\",\"1625\":\"953\",\"1626\":\"954\",\"1627\":\"955\",\"1628\":\"956\",\"1629\":\"957\",\"1630\":\"958\",\"1631\":\"959\",\"1632\":\"960\",\"1633\":\"961\",\"1634\":\"962\",\"1635\":\"963\",\"1636\":\"964\",\"1637\":\"965\",\"1638\":\"966\",\"1639\":\"966#note\",\"1640\":\"967\",\"1641\":\"968\",\"1642\":\"969\",\"1643\":\"970\",\"1644\":\"970#examples\",\"1645\":\"971\",\"1646\":\"972\",\"1647\":\"972#examples\",\"1648\":\"973\",\"1649\":\"974\",\"1650\":\"975\",\"1651\":\"976\",\"1652\":\"977\",\"1653\":\"977#note\",\"1654\":\"978\",\"1655\":\"979\",\"1656\":\"980\",\"1657\":\"981\",\"1658\":\"981#note\",\"1659\":\"982\",\"1660\":\"983\",\"1661\":\"984\",\"1662\":\"985\",\"1663\":\"985#note\",\"1664\":\"986\",\"1665\":\"987\",\"1666\":\"988\",\"1667\":\"989\",\"1668\":\"990\",\"1669\":\"991\",\"1670\":\"992\",\"1671\":\"993\",\"1672\":\"994\",\"1673\":\"995\",\"1674\":\"996\",\"1675\":\"997\",\"1676\":\"998\",\"1677\":\"999\",\"1678\":\"999#note\",\"1679\":\"1000\",\"1680\":\"1001\",\"1681\":\"1002\",\"1682\":\"1003\",\"1683\":\"1004\",\"1684\":\"1005\",\"1685\":\"1006\",\"1686\":\"1007\",\"1687\":\"1008\",\"1688\":\"1009\",\"1689\":\"1010\",\"1690\":\"1011\",\"1691\":\"1012\",\"1692\":\"1013\",\"1693\":\"1014\",\"1694\":\"1015\",\"1695\":\"1016\",\"1696\":\"1017\",\"1697\":\"1018\",\"1698\":\"1019\",\"1699\":\"1020\",\"1700\":\"1021\",\"1701\":\"1022\",\"1702\":\"1023\",\"1703\":\"1024\",\"1704\":\"1025\",\"1705\":\"1026\",\"1706\":\"1027\",\"1707\":\"1028\",\"1708\":\"1029\",\"1709\":\"1030\",\"1710\":\"1031\",\"1711\":\"1032\",\"1712\":\"1033\",\"1713\":\"1034\",\"1714\":\"1035\",\"1715\":\"1036\",\"1716\":\"1037\",\"1717\":\"1038\",\"1718\":\"1039\",\"1719\":\"1040\",\"1720\":\"1041\",\"1721\":\"1042\",\"1722\":\"1043\",\"1723\":\"1044\",\"1724\":\"1045\",\"1725\":\"1046\",\"1726\":\"1047\",\"1727\":\"1048\",\"1728\":\"1049\",\"1729\":\"1050\",\"1730\":\"1051\",\"1731\":\"1052\",\"1732\":\"1053\",\"1733\":\"1054\",\"1734\":\"1055\",\"1735\":\"1056\",\"1736\":\"1057\",\"1737\":\"1058\",\"1738\":\"1059\",\"1739\":\"1060\",\"1740\":\"1061\",\"1741\":\"1062\",\"1742\":\"1063\",\"1743\":\"1064\",\"1744\":\"1065\",\"1745\":\"1066\",\"1746\":\"1067\",\"1747\":\"1068\",\"1748\":\"1069\",\"1749\":\"1070\",\"1750\":\"1071\",\"1751\":\"1072\",\"1752\":\"1073\",\"1753\":\"1073#note\",\"1754\":\"1074\",\"1755\":\"1074#note\",\"1756\":\"1075\",\"1757\":\"1076\",\"1758\":\"1077\",\"1759\":\"1078\",\"1760\":\"1079\",\"1761\":\"1080\",\"1762\":\"1081\",\"1763\":\"1082\",\"1764\":\"1083\",\"1765\":\"1084\",\"1766\":\"1085\",\"1767\":\"1086\",\"1768\":\"1087\",\"1769\":\"1088\",\"1770\":\"1089\",\"1771\":\"1090\",\"1772\":\"1091\",\"1773\":\"1092\",\"1774\":\"1092#examples\",\"1775\":\"1093\",\"1776\":\"1094\",\"1777\":\"1095\",\"1778\":\"1096\",\"1779\":\"1097\",\"1780\":\"1098\",\"1781\":\"1099\",\"1782\":\"1100\",\"1783\":\"1101\",\"1784\":\"1102\",\"1785\":\"1103\",\"1786\":\"1104\",\"1787\":\"1105\",\"1788\":\"1106\",\"1789\":\"1107\",\"1790\":\"1108\",\"1791\":\"1109\",\"1792\":\"1110\",\"1793\":\"1111\",\"1794\":\"1112\",\"1795\":\"1113\",\"1796\":\"1114\",\"1797\":\"1115\",\"1798\":\"1116\",\"1799\":\"1117\",\"1800\":\"1118\",\"1801\":\"1119\",\"1802\":\"1120\",\"1803\":\"1121\",\"1804\":\"1122\",\"1805\":\"1122#examples\",\"1806\":\"1123\",\"1807\":\"1124\",\"1808\":\"1125\",\"1809\":\"1126\",\"1810\":\"1127\",\"1811\":\"1128\",\"1812\":\"1128#note\",\"1813\":\"1129\",\"1814\":\"1130\",\"1815\":\"1131\",\"1816\":\"1132\",\"1817\":\"1133\",\"1818\":\"1134\",\"1819\":\"1135\",\"1820\":\"1136\",\"1821\":\"1137\",\"1822\":\"1137#examples\",\"1823\":\"1138\",\"1824\":\"1139\",\"1825\":\"1140\",\"1826\":\"1141\",\"1827\":\"1141#examples\",\"1828\":\"1142\",\"1829\":\"1143\",\"1830\":\"1143#examples\",\"1831\":\"1144\",\"1832\":\"1145\",\"1833\":\"1146\",\"1834\":\"1147\",\"1835\":\"1148\",\"1836\":\"1149\",\"1837\":\"1150\",\"1838\":\"1151\",\"1839\":\"1152\",\"1840\":\"1153\",\"1841\":\"1154\",\"1842\":\"1155\",\"1843\":\"1156\",\"1844\":\"1157\",\"1845\":\"1158\",\"1846\":\"1158#examples\",\"1847\":\"1159\",\"1848\":\"1160\",\"1849\":\"1161\",\"1850\":\"1162\",\"1851\":\"1163\",\"1852\":\"1164\",\"1853\":\"1165\",\"1854\":\"1166\",\"1855\":\"1167\",\"1856\":\"1167#examples\",\"1857\":\"1168\",\"1858\":\"1169\",\"1859\":\"1170\",\"1860\":\"1171\",\"1861\":\"1172\",\"1862\":\"1173\",\"1863\":\"1174\",\"1864\":\"1175\",\"1865\":\"1176\",\"1866\":\"1177\",\"1867\":\"1178\",\"1868\":\"1179\",\"1869\":\"1180\",\"1870\":\"1181\",\"1871\":\"1182\",\"1872\":\"1183\",\"1873\":\"1184\",\"1874\":\"1185\",\"1875\":\"1186\",\"1876\":\"1187\",\"1877\":\"1188\",\"1878\":\"1189\",\"1879\":\"1190\",\"1880\":\"1191\",\"1881\":\"1192\",\"1882\":\"1192#examples\",\"1883\":\"1193\",\"1884\":\"1194\",\"1885\":\"1195\",\"1886\":\"1196\",\"1887\":\"1197\",\"1888\":\"1198\",\"1889\":\"1199\",\"1890\":\"1199#examples\",\"1891\":\"1200\",\"1892\":\"1201\",\"1893\":\"1202\",\"1894\":\"1203\",\"1895\":\"1204\",\"1896\":\"1205\",\"1897\":\"1206\",\"1898\":\"1207\",\"1899\":\"1208\",\"1900\":\"1209\",\"1901\":\"1210\",\"1902\":\"1210#examples\",\"1903\":\"1211\",\"1904\":\"1211#examples\",\"1905\":\"1212\",\"1906\":\"1212#examples\",\"1907\":\"1213\",\"1908\":\"1214\",\"1909\":\"1214#examples\",\"1910\":\"1215\",\"1911\":\"1216\",\"1912\":\"1216#examples\",\"1913\":\"1217\",\"1914\":\"1218\",\"1915\":\"1219\",\"1916\":\"1220\",\"1917\":\"1221\",\"1918\":\"1222\",\"1919\":\"1223\",\"1920\":\"1224\",\"1921\":\"1225\",\"1922\":\"1226\",\"1923\":\"1227\",\"1924\":\"1228\",\"1925\":\"1229\",\"1926\":\"1230\",\"1927\":\"1231\",\"1928\":\"1232\",\"1929\":\"1233\",\"1930\":\"1234\",\"1931\":\"1235\",\"1932\":\"1236\",\"1933\":\"1236#examples\",\"1934\":\"1237\",\"1935\":\"1238\",\"1936\":\"1239\",\"1937\":\"1240\",\"1938\":\"1241\",\"1939\":\"1241#note\",\"1940\":\"1242\",\"1941\":\"1242#note\",\"1942\":\"1243\",\"1943\":\"1243#note\",\"1944\":\"1244\",\"1945\":\"1245\",\"1946\":\"1245#note\",\"1947\":\"1246\",\"1948\":\"1247\",\"1949\":\"1248\",\"1950\":\"1249\",\"1951\":\"1250\",\"1952\":\"1251\",\"1953\":\"1252\",\"1954\":\"1253\",\"1955\":\"1254\",\"1956\":\"1254#examples\",\"1957\":\"1255\",\"1958\":\"1255#note\",\"1959\":\"1256\",\"1960\":\"1257\",\"1961\":\"1258\",\"1962\":\"1259\",\"1963\":\"1260\",\"1964\":\"1261\",\"1965\":\"1262\",\"1966\":\"1263\",\"1967\":\"1264\",\"1968\":\"1264#note\",\"1969\":\"1265\",\"1970\":\"1265#note\",\"1971\":\"1266\",\"1972\":\"1267\",\"1973\":\"1267#note\",\"1974\":\"1268\",\"1975\":\"1269\",\"1976\":\"1269#note\",\"1977\":\"1270\",\"1978\":\"1271\",\"1979\":\"1271#note\",\"1980\":\"1272\",\"1981\":\"1272#note\",\"1982\":\"1273\",\"1983\":\"1273#note\",\"1984\":\"1274\",\"1985\":\"1275\",\"1986\":\"1275#note\",\"1987\":\"1276\",\"1988\":\"1277\",\"1989\":\"1277#note\",\"1990\":\"1278\",\"1991\":\"1279\",\"1992\":\"1280\",\"1993\":\"1281\",\"1994\":\"1282\",\"1995\":\"1283\",\"1996\":\"1284\",\"1997\":\"1285\",\"1998\":\"1286\",\"1999\":\"1287\",\"2000\":\"1288\",\"2001\":\"1289\",\"2002\":\"1290\",\"2003\":\"1291\",\"2004\":\"1292\",\"2005\":\"1293\",\"2006\":\"1294\",\"2007\":\"1295\",\"2008\":\"1296\",\"2009\":\"1297\",\"2010\":\"1298\",\"2011\":\"1299\",\"2012\":\"1300\",\"2013\":\"1301\",\"2014\":\"1302\",\"2015\":\"1303\",\"2016\":\"1304\",\"2017\":\"1305\",\"2018\":\"1306\",\"2019\":\"1307\",\"2020\":\"1308\",\"2021\":\"1309\",\"2022\":\"1310\",\"2023\":\"1311\",\"2024\":\"1312\",\"2025\":\"1313\",\"2026\":\"1314\",\"2027\":\"1314#note\",\"2028\":\"1315\",\"2029\":\"1315#note\",\"2030\":\"1316\",\"2031\":\"1316#note\",\"2032\":\"1317\",\"2033\":\"1317#note\",\"2034\":\"1318\",\"2035\":\"1318#note\",\"2036\":\"1319\",\"2037\":\"1320\",\"2038\":\"1321\",\"2039\":\"1322\",\"2040\":\"1323\",\"2041\":\"1324\",\"2042\":\"1325\",\"2043\":\"1326\",\"2044\":\"1327\",\"2045\":\"1328\",\"2046\":\"1329\",\"2047\":\"1330\",\"2048\":\"1331\",\"2049\":\"1332\",\"2050\":\"1333\",\"2051\":\"1334\",\"2052\":\"1335\",\"2053\":\"1336\",\"2054\":\"1337\",\"2055\":\"1338\",\"2056\":\"1339\",\"2057\":\"1340\",\"2058\":\"1341\",\"2059\":\"1342\",\"2060\":\"1343\",\"2061\":\"1344\",\"2062\":\"1345\",\"2063\":\"1346\",\"2064\":\"1347\",\"2065\":\"1348\",\"2066\":\"1349\",\"2067\":\"1350\",\"2068\":\"1351\",\"2069\":\"1352\",\"2070\":\"1353\",\"2071\":\"1354\",\"2072\":\"1355\",\"2073\":\"1356\",\"2074\":\"1357\",\"2075\":\"1358\",\"2076\":\"1359\",\"2077\":\"1360\",\"2078\":\"1361\",\"2079\":\"1362\",\"2080\":\"1363\",\"2081\":\"1364\",\"2082\":\"1365\",\"2083\":\"1366\",\"2084\":\"1367\",\"2085\":\"1368\",\"2086\":\"1369\",\"2087\":\"1370\",\"2088\":\"1371\",\"2089\":\"1372\",\"2090\":\"1373\",\"2091\":\"1374\",\"2092\":\"1375\",\"2093\":\"1376\",\"2094\":\"1377\",\"2095\":\"1378\",\"2096\":\"1379\",\"2097\":\"1380\",\"2098\":\"1381\",\"2099\":\"1382\",\"2100\":\"1383\",\"2101\":\"1384\",\"2102\":\"1385\",\"2103\":\"1386\",\"2104\":\"1387\",\"2105\":\"1388\",\"2106\":\"1389\",\"2107\":\"1390\",\"2108\":\"1391\",\"2109\":\"1392\",\"2110\":\"1393\",\"2111\":\"1394\",\"2112\":\"1395\",\"2113\":\"1396\",\"2114\":\"1397\",\"2115\":\"1398\",\"2116\":\"1399\",\"2117\":\"1400\",\"2118\":\"1401\",\"2119\":\"1402\",\"2120\":\"1403\",\"2121\":\"1404\",\"2122\":\"1405\",\"2123\":\"1406\",\"2124\":\"1407\",\"2125\":\"1407#note\",\"2126\":\"1408\",\"2127\":\"1409\",\"2128\":\"1410\",\"2129\":\"1411\",\"2130\":\"1412\",\"2131\":\"1413\",\"2132\":\"1414\",\"2133\":\"1415\",\"2134\":\"1416\",\"2135\":\"1417\",\"2136\":\"1418\",\"2137\":\"1419\",\"2138\":\"1420\",\"2139\":\"1421\",\"2140\":\"1422\",\"2141\":\"1423\",\"2142\":\"1424\",\"2143\":\"1425\",\"2144\":\"1426\",\"2145\":\"1427\",\"2146\":\"1428\",\"2147\":\"1429\",\"2148\":\"1430\",\"2149\":\"1431\",\"2150\":\"1432\",\"2151\":\"1433\",\"2152\":\"1434\",\"2153\":\"1435\",\"2154\":\"1436\",\"2155\":\"1437\",\"2156\":\"1438\",\"2157\":\"1439\",\"2158\":\"1440\",\"2159\":\"1441\",\"2160\":\"1442\",\"2161\":\"1443\",\"2162\":\"1444\",\"2163\":\"1445\",\"2164\":\"1446\",\"2165\":\"1447\",\"2166\":\"1448\",\"2167\":\"1449\",\"2168\":\"1450\",\"2169\":\"1450#note\",\"2170\":\"1451\",\"2171\":\"1451#note\",\"2172\":\"1452\",\"2173\":\"1452#note\",\"2174\":\"1453\",\"2175\":\"1453#note\",\"2176\":\"1454\",\"2177\":\"1455\",\"2178\":\"1455#note\",\"2179\":\"1456\",\"2180\":\"1456#note\",\"2181\":\"1457\",\"2182\":\"1457#note\",\"2183\":\"1458\",\"2184\":\"1459\",\"2185\":\"1460\",\"2186\":\"1460#note\",\"2187\":\"1461\",\"2188\":\"1462\",\"2189\":\"1462#note\",\"2190\":\"1463\",\"2191\":\"1464\",\"2192\":\"1465\",\"2193\":\"1465#note\",\"2194\":\"1466\",\"2195\":\"1466#note\",\"2196\":\"1467\",\"2197\":\"1467#note\",\"2198\":\"1468\",\"2199\":\"1468#note\",\"2200\":\"1469\",\"2201\":\"1469#note\",\"2202\":\"1470\",\"2203\":\"1471\",\"2204\":\"1471#note\",\"2205\":\"1472\",\"2206\":\"1472#note\",\"2207\":\"1473\",\"2208\":\"1474\",\"2209\":\"1475\",\"2210\":\"1475#note\",\"2211\":\"1476\",\"2212\":\"1476#note\",\"2213\":\"1477\",\"2214\":\"1478\",\"2215\":\"1479\",\"2216\":\"1480\",\"2217\":\"1480#note\",\"2218\":\"1481\",\"2219\":\"1482\",\"2220\":\"1483\",\"2221\":\"1484\",\"2222\":\"1485\",\"2223\":\"1486\",\"2224\":\"1486#note\",\"2225\":\"1486#note-1\",\"2226\":\"1487\",\"2227\":\"1488\",\"2228\":\"1489\",\"2229\":\"1490\",\"2230\":\"1491\",\"2231\":\"1492\",\"2232\":\"1493\",\"2233\":\"1494\",\"2234\":\"1495\",\"2235\":\"1496\",\"2236\":\"1497\",\"2237\":\"1498\",\"2238\":\"1499\",\"2239\":\"1500\",\"2240\":\"1501\",\"2241\":\"1502\",\"2242\":\"1503\",\"2243\":\"1504\",\"2244\":\"1505\",\"2245\":\"1506\",\"2246\":\"1507\",\"2247\":\"1508\",\"2248\":\"1509\",\"2249\":\"1510\",\"2250\":\"1511\",\"2251\":\"1512\",\"2252\":\"1513\",\"2253\":\"1514\",\"2254\":\"1515\",\"2255\":\"1516\",\"2256\":\"1517\",\"2257\":\"1518\",\"2258\":\"1519\",\"2259\":\"1520\",\"2260\":\"1521\",\"2261\":\"1522\",\"2262\":\"1523\",\"2263\":\"1524\",\"2264\":\"1525\",\"2265\":\"1526\",\"2266\":\"1527\",\"2267\":\"1528\",\"2268\":\"1529\",\"2269\":\"1530\",\"2270\":\"1531\",\"2271\":\"1532\",\"2272\":\"1533\",\"2273\":\"1534\",\"2274\":\"1535\",\"2275\":\"1536\",\"2276\":\"1537\",\"2277\":\"1538\",\"2278\":\"1539\",\"2279\":\"1540\",\"2280\":\"1541\",\"2281\":\"1542\",\"2282\":\"1543\",\"2283\":\"1544\",\"2284\":\"1545\",\"2285\":\"1546\",\"2286\":\"1547\",\"2287\":\"1548\",\"2288\":\"1549\",\"2289\":\"1550\",\"2290\":\"1550#examples\",\"2291\":\"1551\",\"2292\":\"1552\",\"2293\":\"1553\",\"2294\":\"1554\",\"2295\":\"1555\",\"2296\":\"1556\",\"2297\":\"1557\",\"2298\":\"1558\",\"2299\":\"1558#examples\",\"2300\":\"1559\",\"2301\":\"1560\",\"2302\":\"1561\",\"2303\":\"1562\",\"2304\":\"1563\",\"2305\":\"1563#examples\",\"2306\":\"1563#note\",\"2307\":\"1564\",\"2308\":\"1565\",\"2309\":\"1566\",\"2310\":\"1567\",\"2311\":\"1568\",\"2312\":\"1569\",\"2313\":\"1570\",\"2314\":\"1571\",\"2315\":\"1571#examples\",\"2316\":\"1572\",\"2317\":\"1573\",\"2318\":\"1574\",\"2319\":\"1575\",\"2320\":\"1576\",\"2321\":\"1577\",\"2322\":\"1578\",\"2323\":\"1579\",\"2324\":\"1580\",\"2325\":\"1581\",\"2326\":\"1581#note\",\"2327\":\"1582\",\"2328\":\"1583\",\"2329\":\"1584\",\"2330\":\"1585\",\"2331\":\"1586\",\"2332\":\"1587\",\"2333\":\"1588\",\"2334\":\"1589\",\"2335\":\"1590\",\"2336\":\"1591\",\"2337\":\"1592\",\"2338\":\"1593\",\"2339\":\"1594\",\"2340\":\"1595\",\"2341\":\"1596\",\"2342\":\"1597\",\"2343\":\"1597#examples\",\"2344\":\"1598\",\"2345\":\"1599\",\"2346\":\"1600\",\"2347\":\"1601\",\"2348\":\"1602\",\"2349\":\"1603\",\"2350\":\"1604\",\"2351\":\"1605\",\"2352\":\"1605#examples\",\"2353\":\"1606\",\"2354\":\"1607\",\"2355\":\"1608\",\"2356\":\"1609\",\"2357\":\"1610\",\"2358\":\"1611\",\"2359\":\"1612\",\"2360\":\"1613\",\"2361\":\"1614\",\"2362\":\"1615\",\"2363\":\"1616\",\"2364\":\"1617\",\"2365\":\"1618\",\"2366\":\"1619\",\"2367\":\"1620\",\"2368\":\"1621\",\"2369\":\"1622\",\"2370\":\"1623\",\"2371\":\"1624\",\"2372\":\"1625\",\"2373\":\"1626\",\"2374\":\"1627\",\"2375\":\"1628\",\"2376\":\"1629\",\"2377\":\"1629#examples\",\"2378\":\"1630\",\"2379\":\"1631\",\"2380\":\"1632\",\"2381\":\"1633\",\"2382\":\"1634\",\"2383\":\"1635\",\"2384\":\"1636\",\"2385\":\"1637\",\"2386\":\"1638\",\"2387\":\"1639\",\"2388\":\"1640\",\"2389\":\"1641\",\"2390\":\"1642\",\"2391\":\"1643\",\"2392\":\"1644\",\"2393\":\"1645\",\"2394\":\"1646\",\"2395\":\"1647\",\"2396\":\"1648\",\"2397\":\"1649\",\"2398\":\"1650\",\"2399\":\"1651\",\"2400\":\"1652\",\"2401\":\"1653\",\"2402\":\"1653#note\",\"2403\":\"1654\",\"2404\":\"1655\",\"2405\":\"1655#note\",\"2406\":\"1655#note-1\",\"2407\":\"1656\",\"2408\":\"1657\",\"2409\":\"1658\",\"2410\":\"1658#note\",\"2411\":\"1659\",\"2412\":\"1660\",\"2413\":\"1661\",\"2414\":\"1662\",\"2415\":\"1662#note\",\"2416\":\"1663\",\"2417\":\"1663#note\",\"2418\":\"1664\",\"2419\":\"1664#note\",\"2420\":\"1665\",\"2421\":\"1666\",\"2422\":\"1667\",\"2423\":\"1668\",\"2424\":\"1669\",\"2425\":\"1670\",\"2426\":\"1671\",\"2427\":\"1672\",\"2428\":\"1673\",\"2429\":\"1674\",\"2430\":\"1675\",\"2431\":\"1676\",\"2432\":\"1677\",\"2433\":\"1678\",\"2434\":\"1679\",\"2435\":\"1679#note\",\"2436\":\"1680\",\"2437\":\"1681\",\"2438\":\"1682\",\"2439\":\"1683\",\"2440\":\"1684\",\"2441\":\"1685\",\"2442\":\"1686\",\"2443\":\"1687\",\"2444\":\"1687#note\",\"2445\":\"1688\",\"2446\":\"1689\",\"2447\":\"1690\",\"2448\":\"1691\",\"2449\":\"1692\",\"2450\":\"1692#note\",\"2451\":\"1693\",\"2452\":\"1693#note\",\"2453\":\"1694\",\"2454\":\"1694#note\",\"2455\":\"1695\",\"2456\":\"1696\",\"2457\":\"1696#note\",\"2458\":\"1697\",\"2459\":\"1697#note\",\"2460\":\"1698\",\"2461\":\"1698#note\",\"2462\":\"1699\",\"2463\":\"1700\",\"2464\":\"1701\",\"2465\":\"1702\",\"2466\":\"1702#note\",\"2467\":\"1703\",\"2468\":\"1703#note\",\"2469\":\"1704\",\"2470\":\"1705\",\"2471\":\"1706\",\"2472\":\"1707\",\"2473\":\"1708\",\"2474\":\"1709\",\"2475\":\"1710\",\"2476\":\"1711\",\"2477\":\"1712\",\"2478\":\"1713\",\"2479\":\"1713#examples\",\"2480\":\"1714\",\"2481\":\"1715\",\"2482\":\"1716\",\"2483\":\"1717\",\"2484\":\"1718\",\"2485\":\"1718#examples\",\"2486\":\"1719\",\"2487\":\"1720\",\"2488\":\"1720#examples\",\"2489\":\"1721\",\"2490\":\"1722\",\"2491\":\"1723\",\"2492\":\"1724\",\"2493\":\"1724#examples\",\"2494\":\"1725\",\"2495\":\"1726\",\"2496\":\"1727\",\"2497\":\"1728\",\"2498\":\"1729\",\"2499\":\"1730\",\"2500\":\"1730#examples\",\"2501\":\"1731\",\"2502\":\"1731#examples\",\"2503\":\"1732\",\"2504\":\"1733\",\"2505\":\"1733#examples\",\"2506\":\"1734\",\"2507\":\"1735\",\"2508\":\"1736\",\"2509\":\"1737\",\"2510\":\"1738\",\"2511\":\"1739\",\"2512\":\"1740\",\"2513\":\"1741\",\"2514\":\"1742\",\"2515\":\"1743\",\"2516\":\"1744\",\"2517\":\"1745\",\"2518\":\"1746\",\"2519\":\"1747\",\"2520\":\"1748\",\"2521\":\"1749\",\"2522\":\"1750\",\"2523\":\"1751\",\"2524\":\"1752\",\"2525\":\"1753\",\"2526\":\"1754\",\"2527\":\"1755\",\"2528\":\"1756\",\"2529\":\"1757\",\"2530\":\"1758\",\"2531\":\"1759\",\"2532\":\"1760\",\"2533\":\"1761\",\"2534\":\"1762\",\"2535\":\"1763\",\"2536\":\"1764\",\"2537\":\"1765\",\"2538\":\"1766\",\"2539\":\"1767\",\"2540\":\"1768\",\"2541\":\"1769\",\"2542\":\"1770\",\"2543\":\"1771\",\"2544\":\"1772\",\"2545\":\"1773\",\"2546\":\"1774\",\"2547\":\"1775\",\"2548\":\"1776\",\"2549\":\"1777\",\"2550\":\"1778\",\"2551\":\"1779\",\"2552\":\"1780\",\"2553\":\"1781\",\"2554\":\"1782\",\"2555\":\"1783\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[7,24],\"1\":[3,59],\"2\":[2,28],\"3\":[4,202],\"4\":[2],\"5\":[2,39],\"6\":[3,77],\"7\":[3,35],\"8\":[2,47],\"9\":[1,65],\"10\":[2,69],\"11\":[2,73],\"12\":[3,45],\"13\":[3,44],\"14\":[2,34],\"15\":[2,44],\"16\":[4,27],\"17\":[1],\"18\":[1,49],\"19\":[3,54],\"20\":[1],\"21\":[1],\"22\":[3,120],\"23\":[3,37],\"24\":[4,113],\"25\":[4,45],\"26\":[2,128],\"27\":[1,31],\"28\":[1,22],\"29\":[3,18],\"30\":[3],\"31\":[1,71],\"32\":[5,73],\"33\":[2,21],\"34\":[1,20],\"35\":[1],\"36\":[1,41],\"37\":[2,60],\"38\":[4,73],\"39\":[1,185],\"40\":[5,42],\"41\":[3,89],\"42\":[2,46],\"43\":[1,198],\"44\":[3,131],\"45\":[1,133],\"46\":[2,90],\"47\":[4,95],\"48\":[4,98],\"49\":[4,41],\"50\":[2,97],\"51\":[1,51],\"52\":[2,50],\"53\":[4,30],\"54\":[2,40],\"55\":[1,40],\"56\":[6,43],\"57\":[4,30],\"58\":[10,37],\"59\":[4,33],\"60\":[3,78],\"61\":[7,27],\"62\":[10,54],\"63\":[4,32],\"64\":[10,29],\"65\":[1,3],\"66\":[6,72],\"67\":[3,126],\"68\":[9,62],\"69\":[2,108],\"70\":[7,111],\"71\":[7,146],\"72\":[2],\"73\":[6,39],\"74\":[13,50],\"75\":[7,18],\"76\":[8,43],\"77\":[8],\"78\":[2,124],\"79\":[3,144],\"80\":[1,78],\"81\":[1,104],\"82\":[5,145],\"83\":[5],\"84\":[2,116],\"85\":[2,45],\"86\":[7,48],\"87\":[3,44],\"88\":[7,34],\"89\":[2,11],\"90\":[3,23],\"91\":[8,90],\"92\":[3,44],\"93\":[2,26],\"94\":[10,60],\"95\":[4,49],\"96\":[1,107],\"97\":[1,90],\"98\":[1,94],\"99\":[1,64],\"100\":[1,69],\"101\":[1,85],\"102\":[2,115],\"103\":[4,9],\"104\":[3,78],\"105\":[1],\"106\":[4,132],\"107\":[3,80],\"108\":[5,58],\"109\":[5,43],\"110\":[3,96],\"111\":[1,24],\"112\":[4],\"113\":[4,75],\"114\":[8,22],\"115\":[2,5],\"116\":[4],\"117\":[10,46],\"118\":[8,59],\"119\":[5,77],\"120\":[6,31],\"121\":[5,37],\"122\":[2],\"123\":[9,39],\"124\":[6,52],\"125\":[6,38],\"126\":[4,114],\"127\":[6,94],\"128\":[8,161],\"129\":[2,20],\"130\":[1,51],\"131\":[1,17],\"132\":[1,44],\"133\":[5,63],\"134\":[1,56],\"135\":[1,71],\"136\":[1,107],\"137\":[1,36],\"138\":[2,117],\"139\":[2,160],\"140\":[1,37],\"141\":[1,283],\"142\":[1,149],\"143\":[2,57],\"144\":[3,90],\"145\":[1,148],\"146\":[1,47],\"147\":[1,84],\"148\":[1,59],\"149\":[1],\"150\":[11,108],\"151\":[1],\"152\":[14,46],\"153\":[8,52],\"154\":[8,24],\"155\":[1],\"156\":[4,66],\"157\":[1],\"158\":[1],\"159\":[1,55],\"160\":[6,53],\"161\":[5,121],\"162\":[4,249],\"163\":[6,79],\"164\":[2,36],\"165\":[4,49],\"166\":[3,48],\"167\":[4,76],\"168\":[1,124],\"169\":[1],\"170\":[2],\"171\":[1,5],\"172\":[1,5],\"173\":[3,119],\"174\":[9,41],\"175\":[4,149],\"176\":[2],\"177\":[1],\"178\":[4,24],\"179\":[5,21],\"180\":[5,14],\"181\":[5,12],\"182\":[2,13],\"183\":[2],\"184\":[4,25],\"185\":[6,27],\"186\":[5,17],\"187\":[5,20],\"188\":[5,20],\"189\":[1],\"190\":[3,74],\"191\":[3,24],\"192\":[3,11],\"193\":[3,35],\"194\":[2,46],\"195\":[6,105],\"196\":[5,147],\"197\":[9,208],\"198\":[6,25],\"199\":[3,56],\"200\":[2,282],\"201\":[3,157],\"202\":[2,113],\"203\":[6,26],\"204\":[3,62],\"205\":[2,215],\"206\":[3,49],\"207\":[2,66],\"208\":[5,23],\"209\":[1,10],\"210\":[3,47],\"211\":[2,229],\"212\":[3,108],\"213\":[3,158],\"214\":[2,11],\"215\":[2,10],\"216\":[3,39],\"217\":[2,148],\"218\":[3,152],\"219\":[2,34],\"220\":[1,65],\"221\":[2,10],\"222\":[2,82],\"223\":[4,351],\"224\":[9,198],\"225\":[6,139],\"226\":[4,21],\"227\":[3,64],\"228\":[5,123],\"229\":[5,11],\"230\":[6,11],\"231\":[3,15],\"232\":[4,105],\"233\":[2,45],\"234\":[3,45],\"235\":[2,155],\"236\":[3,54],\"237\":[2,10],\"238\":[2,10],\"239\":[3,10],\"240\":[7,130],\"241\":[3],\"242\":[2,376],\"243\":[3,468],\"244\":[2,46],\"245\":[2,50],\"246\":[1,119],\"247\":[3,134],\"248\":[1,34],\"249\":[1,39],\"250\":[3,10],\"251\":[3,10],\"252\":[2,51],\"253\":[3,39],\"254\":[2,163],\"255\":[3,49],\"256\":[2,35],\"257\":[3,15],\"258\":[4,88],\"259\":[5,124],\"260\":[4,87],\"261\":[1,102],\"262\":[3,282],\"263\":[2,92],\"264\":[3,10],\"265\":[3,87],\"266\":[2,186],\"267\":[3,499],\"268\":[3,202],\"269\":[2,311],\"270\":[3,19],\"271\":[3,46],\"272\":[2,36],\"273\":[4,37],\"274\":[3,95],\"275\":[2,219],\"276\":[3,502],\"277\":[3,200],\"278\":[2,311],\"279\":[3,19],\"280\":[3,46],\"281\":[3,9],\"282\":[2,30],\"283\":[3,10],\"284\":[3,146],\"285\":[2,338],\"286\":[3,671],\"287\":[3,287],\"288\":[3,42],\"289\":[2,88],\"290\":[1,434],\"291\":[6,10],\"292\":[4,10],\"293\":[4,29],\"294\":[2],\"295\":[3,90],\"296\":[2],\"297\":[3],\"298\":[3],\"299\":[3],\"300\":[3],\"301\":[3,149],\"302\":[2],\"303\":[3],\"304\":[4],\"305\":[2],\"306\":[3],\"307\":[3],\"308\":[3],\"309\":[4,81],\"310\":[2],\"311\":[3],\"312\":[4],\"313\":[2],\"314\":[3],\"315\":[4,112],\"316\":[2],\"317\":[3],\"318\":[4],\"319\":[3],\"320\":[3],\"321\":[4,102],\"322\":[2],\"323\":[3],\"324\":[4],\"325\":[3],\"326\":[3],\"327\":[3,64],\"328\":[2],\"329\":[3],\"330\":[4],\"331\":[3,69],\"332\":[2],\"333\":[3],\"334\":[4],\"335\":[3,94],\"336\":[2],\"337\":[3],\"338\":[4],\"339\":[3],\"340\":[3],\"341\":[4],\"342\":[3,92],\"343\":[2],\"344\":[3],\"345\":[3],\"346\":[4],\"347\":[3],\"348\":[2],\"349\":[4,79],\"350\":[2],\"351\":[3],\"352\":[3],\"353\":[4],\"354\":[3],\"355\":[2],\"356\":[3,72],\"357\":[2],\"358\":[3],\"359\":[2],\"360\":[2],\"361\":[4,88],\"362\":[2],\"363\":[3],\"364\":[3],\"365\":[4],\"366\":[3],\"367\":[2],\"368\":[4,77],\"369\":[2],\"370\":[3],\"371\":[4],\"372\":[5,34],\"373\":[2],\"374\":[2,47],\"375\":[2],\"376\":[2],\"377\":[3,182],\"378\":[2],\"379\":[3],\"380\":[4],\"381\":[3],\"382\":[3],\"383\":[3],\"384\":[4],\"385\":[4,66],\"386\":[2],\"387\":[3],\"388\":[4],\"389\":[3,94],\"390\":[2],\"391\":[3],\"392\":[4],\"393\":[2],\"394\":[3],\"395\":[3],\"396\":[3,95],\"397\":[2],\"398\":[3],\"399\":[4],\"400\":[3],\"401\":[3],\"402\":[2,25],\"403\":[2,41],\"404\":[3,62],\"405\":[2],\"406\":[3,112],\"407\":[2],\"408\":[3],\"409\":[4],\"410\":[2],\"411\":[4],\"412\":[7],\"413\":[2],\"414\":[3],\"415\":[4,92],\"416\":[2],\"417\":[3],\"418\":[3],\"419\":[3],\"420\":[3],\"421\":[3,128],\"422\":[2],\"423\":[3],\"424\":[3],\"425\":[2],\"426\":[3],\"427\":[3],\"428\":[3],\"429\":[4,109],\"430\":[2],\"431\":[3],\"432\":[4],\"433\":[2],\"434\":[3],\"435\":[3],\"436\":[4,81],\"437\":[2],\"438\":[3],\"439\":[3],\"440\":[2],\"441\":[3],\"442\":[3,112],\"443\":[2],\"444\":[3],\"445\":[4],\"446\":[2],\"447\":[3],\"448\":[3],\"449\":[4,222],\"450\":[2],\"451\":[3],\"452\":[4],\"453\":[3],\"454\":[3],\"455\":[3],\"456\":[4],\"457\":[3,60],\"458\":[2],\"459\":[3],\"460\":[4],\"461\":[3,28],\"462\":[2],\"463\":[3,119],\"464\":[2],\"465\":[3],\"466\":[4],\"467\":[3],\"468\":[3],\"469\":[4,134],\"470\":[2],\"471\":[3],\"472\":[4],\"473\":[3],\"474\":[3],\"475\":[3,91],\"476\":[2],\"477\":[3],\"478\":[4],\"479\":[2],\"480\":[2],\"481\":[3,99],\"482\":[2],\"483\":[4],\"484\":[3,104],\"485\":[2],\"486\":[3],\"487\":[4],\"488\":[2],\"489\":[2],\"490\":[3,104],\"491\":[2],\"492\":[3],\"493\":[4],\"494\":[2],\"495\":[2],\"496\":[4,63],\"497\":[2],\"498\":[3,88],\"499\":[2],\"500\":[3],\"501\":[4],\"502\":[2],\"503\":[3],\"504\":[3],\"505\":[4,108],\"506\":[2],\"507\":[3],\"508\":[4],\"509\":[3],\"510\":[3],\"511\":[4,45],\"512\":[2],\"513\":[2,20],\"514\":[2,38],\"515\":[3,40],\"516\":[3,67],\"517\":[3,15],\"518\":[5,34],\"519\":[2,11],\"520\":[4,41],\"521\":[4,50],\"522\":[3,18],\"523\":[3,58],\"524\":[3,42],\"525\":[3,41],\"526\":[3,31],\"527\":[3,94],\"528\":[4,9],\"529\":[3,7],\"530\":[3,5],\"531\":[3,11],\"532\":[4,5],\"533\":[5,12],\"534\":[3,5],\"535\":[3,57],\"536\":[3,177],\"537\":[3,54],\"538\":[2,22],\"539\":[2],\"540\":[2],\"541\":[3,31],\"542\":[2],\"543\":[3,37],\"544\":[2],\"545\":[3,22],\"546\":[2],\"547\":[2],\"548\":[4,58],\"549\":[2],\"550\":[2],\"551\":[4,52],\"552\":[2],\"553\":[2],\"554\":[4,15],\"555\":[2],\"556\":[2,10],\"557\":[2],\"558\":[5,42],\"559\":[2],\"560\":[2],\"561\":[3,35],\"562\":[2],\"563\":[2],\"564\":[5,16],\"565\":[2],\"566\":[2],\"567\":[4,23],\"568\":[2],\"569\":[2],\"570\":[2,16],\"571\":[2],\"572\":[2,13],\"573\":[2],\"574\":[2],\"575\":[5,27],\"576\":[2],\"577\":[2],\"578\":[2,26],\"579\":[2],\"580\":[2],\"581\":[2,14],\"582\":[2],\"583\":[2,23],\"584\":[2],\"585\":[2],\"586\":[3,22],\"587\":[2],\"588\":[2],\"589\":[4,24],\"590\":[2],\"591\":[2],\"592\":[5,18],\"593\":[2],\"594\":[2,16],\"595\":[2],\"596\":[4,17],\"597\":[2],\"598\":[2,12],\"599\":[2],\"600\":[2,14],\"601\":[2],\"602\":[2],\"603\":[2,29],\"604\":[2],\"605\":[2],\"606\":[3,40],\"607\":[2],\"608\":[2],\"609\":[2,10],\"610\":[2],\"611\":[2,13],\"612\":[2],\"613\":[2],\"614\":[6,92],\"615\":[5,57],\"616\":[6,201],\"617\":[7,113],\"618\":[7,106],\"619\":[7,71],\"620\":[7,125],\"621\":[8,61],\"622\":[7,86],\"623\":[7,66],\"624\":[7,121],\"625\":[6,183],\"626\":[5,83],\"627\":[6,88],\"628\":[6,52],\"629\":[5,59],\"630\":[9,64],\"631\":[6,52],\"632\":[6,68],\"633\":[7,135],\"634\":[6,174],\"635\":[5,69],\"636\":[8,106],\"637\":[11,111],\"638\":[9,68],\"639\":[8,96],\"640\":[5,64],\"641\":[6,113],\"642\":[7,89],\"643\":[6,138],\"644\":[7,113],\"645\":[8,68],\"646\":[9,54],\"647\":[9,69],\"648\":[5,55],\"649\":[8,89],\"650\":[5,61],\"651\":[6,103],\"652\":[5,66],\"653\":[5,36],\"654\":[8,52],\"655\":[8,32],\"656\":[8,25],\"657\":[8,25],\"658\":[8,23],\"659\":[8,27],\"660\":[8,23],\"661\":[8,109],\"662\":[8,26],\"663\":[7,38],\"664\":[5,71],\"665\":[8,49],\"666\":[5,42],\"667\":[7,54],\"668\":[10,22],\"669\":[7,58],\"670\":[7,33],\"671\":[7,33],\"672\":[8,30],\"673\":[8,26],\"674\":[5,127],\"675\":[5,75],\"676\":[5,46],\"677\":[1,34],\"678\":[5,47],\"679\":[1,38],\"680\":[5,41],\"681\":[1,38],\"682\":[5,41],\"683\":[1,38],\"684\":[5,41],\"685\":[1,38],\"686\":[5,56],\"687\":[1,34],\"688\":[6,12],\"689\":[6,60],\"690\":[1,34],\"691\":[5,132],\"692\":[5,198],\"693\":[5,42],\"694\":[1,34],\"695\":[6,1],\"696\":[6,216],\"697\":[7,182],\"698\":[5,14],\"699\":[5,209],\"700\":[5,109],\"701\":[5,93],\"702\":[5,88],\"703\":[9,164],\"704\":[10,72],\"705\":[1,62],\"706\":[4,147],\"707\":[6,1],\"708\":[6,1],\"709\":[5,188],\"710\":[7,175],\"711\":[7,175],\"712\":[5,56],\"713\":[5,53],\"714\":[1,34],\"715\":[5,58],\"716\":[10,67],\"717\":[10,68],\"718\":[6,49],\"719\":[1,34],\"720\":[5,113],\"721\":[1,37],\"722\":[5,55],\"723\":[1,34],\"724\":[6,117],\"725\":[6,117],\"726\":[6,96],\"727\":[6,78],\"728\":[6,117],\"729\":[6,84],\"730\":[6,35],\"731\":[5,73],\"732\":[5,73],\"733\":[7,148],\"734\":[6,133],\"735\":[6,86],\"736\":[6,108],\"737\":[6,107],\"738\":[6,117],\"739\":[1,41],\"740\":[6,74],\"741\":[5,37],\"742\":[1,35],\"743\":[7,37],\"744\":[6,107],\"745\":[5,97],\"746\":[5,121],\"747\":[5,137],\"748\":[5,120],\"749\":[5,65],\"750\":[6,60],\"751\":[6,21],\"752\":[5,56],\"753\":[1,37],\"754\":[6,33],\"755\":[10,185],\"756\":[5,171],\"757\":[6,40],\"758\":[1,34],\"759\":[5,87],\"760\":[7,177],\"761\":[7,43],\"762\":[7,48],\"763\":[7,34],\"764\":[11,31],\"765\":[6,44],\"766\":[5,73],\"767\":[5,73],\"768\":[5,220],\"769\":[6,29],\"770\":[5,77],\"771\":[5,87],\"772\":[5,31],\"773\":[9,175],\"774\":[5,199],\"775\":[5,106],\"776\":[5,34],\"777\":[5,149],\"778\":[6,75],\"779\":[1,43],\"780\":[5,206],\"781\":[6,59],\"782\":[1,34],\"783\":[6,65],\"784\":[6,122],\"785\":[10,201],\"786\":[7,139],\"787\":[5,150],\"788\":[6,39],\"789\":[1,37],\"790\":[5,159],\"791\":[5,79],\"792\":[1,70],\"793\":[6,44],\"794\":[6,62],\"795\":[6,60],\"796\":[5,89],\"797\":[1,88],\"798\":[5,86],\"799\":[1,37],\"800\":[7,110],\"801\":[10,25],\"802\":[11,30],\"803\":[10,48],\"804\":[1,88],\"805\":[5,36],\"806\":[1,38],\"807\":[5,44],\"808\":[1,34],\"809\":[6,55],\"810\":[1,34],\"811\":[6,35],\"812\":[1,34],\"813\":[6,35],\"814\":[1,34],\"815\":[5,91],\"816\":[1,41],\"817\":[6,89],\"818\":[5,29],\"819\":[3,106],\"820\":[5,175],\"821\":[6,214],\"822\":[1,54],\"823\":[6,67],\"824\":[6,147],\"825\":[5,36],\"826\":[1,34],\"827\":[6,43],\"828\":[6,204],\"829\":[6,165],\"830\":[6,194],\"831\":[5,120],\"832\":[5,44],\"833\":[4,95],\"834\":[1,34],\"835\":[6,35],\"836\":[1,34],\"837\":[6,42],\"838\":[1,34],\"839\":[5,37],\"840\":[1,34],\"841\":[5,24],\"842\":[6,35],\"843\":[1,34],\"844\":[6,35],\"845\":[1,34],\"846\":[5,244],\"847\":[5,136],\"848\":[5,75],\"849\":[6,136],\"850\":[5,163],\"851\":[5,71],\"852\":[6,66],\"853\":[1,34],\"854\":[5,58],\"855\":[1,34],\"856\":[6,57],\"857\":[1,34],\"858\":[6,20],\"859\":[6,114],\"860\":[6,40],\"861\":[1,38],\"862\":[6,85],\"863\":[1,37],\"864\":[5,68],\"865\":[1,49],\"866\":[7,159],\"867\":[7,157],\"868\":[9,13],\"869\":[7,40],\"870\":[6,1],\"871\":[7,1],\"872\":[7,1],\"873\":[8,15],\"874\":[8,12],\"875\":[8,12],\"876\":[8,13],\"877\":[6,14],\"878\":[12,115],\"879\":[12,115],\"880\":[11,18],\"881\":[12,152],\"882\":[13,137],\"883\":[13,136],\"884\":[13,176],\"885\":[5,11],\"886\":[11,17],\"887\":[8,19],\"888\":[10,15],\"889\":[5,11],\"890\":[5,11],\"891\":[6,12],\"892\":[6,12],\"893\":[7,12],\"894\":[8,13],\"895\":[6,24],\"896\":[10,12],\"897\":[9,12],\"898\":[9,11],\"899\":[10,14],\"900\":[5,9],\"901\":[6,9],\"902\":[7,14],\"903\":[7,11],\"904\":[10,12],\"905\":[9,11],\"906\":[7,11],\"907\":[7,18],\"908\":[11,24],\"909\":[9,12],\"910\":[6,11],\"911\":[7,53],\"912\":[6,53],\"913\":[7,10],\"914\":[7,10],\"915\":[7,9],\"916\":[10,15],\"917\":[8,1],\"918\":[11,22],\"919\":[11,69],\"920\":[9,13],\"921\":[8,114],\"922\":[9,140],\"923\":[9,12],\"924\":[6,43],\"925\":[8,22],\"926\":[6,21],\"927\":[6,80],\"928\":[7,24],\"929\":[8,34],\"930\":[9,28],\"931\":[10,31],\"932\":[1,77],\"933\":[10,31],\"934\":[1,77],\"935\":[7,78],\"936\":[8,93],\"937\":[8,94],\"938\":[6,27],\"939\":[7,67],\"940\":[12,14],\"941\":[6,22],\"942\":[7,18],\"943\":[7,31],\"944\":[6,16],\"945\":[6,12],\"946\":[11,13],\"947\":[6,55],\"948\":[5,44],\"949\":[6,58],\"950\":[5,45],\"951\":[1,41],\"952\":[5,41],\"953\":[1,34],\"954\":[5,77],\"955\":[5,43],\"956\":[5,42],\"957\":[1,34],\"958\":[5,104],\"959\":[5,72],\"960\":[5,78],\"961\":[1,43],\"962\":[7,41],\"963\":[5,43],\"964\":[1,34],\"965\":[5,41],\"966\":[1,37],\"967\":[4,41],\"968\":[1,42],\"969\":[6,47],\"970\":[1,39],\"971\":[6,42],\"972\":[6,40],\"973\":[6,43],\"974\":[5,134],\"975\":[6,41],\"976\":[5,48],\"977\":[5,45],\"978\":[6,119],\"979\":[5,62],\"980\":[6,104],\"981\":[6,40],\"982\":[6,62],\"983\":[7,11],\"984\":[7,27],\"985\":[5,21],\"986\":[1,23],\"987\":[6,28],\"988\":[1,14],\"989\":[6,30],\"990\":[1,16],\"991\":[5,29],\"992\":[1,31],\"993\":[6,38],\"994\":[1,95],\"995\":[5,13],\"996\":[5,20],\"997\":[1,29],\"998\":[5,21],\"999\":[1,26],\"1000\":[5,87],\"1001\":[4,14],\"1002\":[1,75],\"1003\":[5,24],\"1004\":[1,26],\"1005\":[5,17],\"1006\":[1,21],\"1007\":[5,24],\"1008\":[1,90],\"1009\":[5,48],\"1010\":[1,25],\"1011\":[5,43],\"1012\":[1,29],\"1013\":[5,17],\"1014\":[1,25],\"1015\":[5,25],\"1016\":[1,31],\"1017\":[5,18],\"1018\":[1,25],\"1019\":[7,28],\"1020\":[1,21],\"1021\":[5,25],\"1022\":[5,36],\"1023\":[1,13],\"1024\":[5,22],\"1025\":[1,24],\"1026\":[6,32],\"1027\":[1,19],\"1028\":[6,31],\"1029\":[6,176],\"1030\":[5,46],\"1031\":[1,75],\"1032\":[5,40],\"1033\":[1,34],\"1034\":[5,46],\"1035\":[1,76],\"1036\":[6,45],\"1037\":[1,39],\"1038\":[4,41],\"1039\":[1,42],\"1040\":[5,51],\"1041\":[1,34],\"1042\":[7,50],\"1043\":[1,37],\"1044\":[5,47],\"1045\":[1,42],\"1046\":[5,37],\"1047\":[1,34],\"1048\":[5,37],\"1049\":[1,34],\"1050\":[6,60],\"1051\":[5,90],\"1052\":[1,34],\"1053\":[7,115],\"1054\":[6,54],\"1055\":[6,40],\"1056\":[1,34],\"1057\":[7,49],\"1058\":[1,34],\"1059\":[6,49],\"1060\":[1,34],\"1061\":[5,155],\"1062\":[5,180],\"1063\":[5,89],\"1064\":[6,144],\"1065\":[5,40],\"1066\":[7,90],\"1067\":[1,34],\"1068\":[6,39],\"1069\":[1,34],\"1070\":[5,69],\"1071\":[6,73],\"1072\":[5,47],\"1073\":[5,52],\"1074\":[5,45],\"1075\":[5,39],\"1076\":[7,44],\"1077\":[1,34],\"1078\":[5,72],\"1079\":[1,37],\"1080\":[5,57],\"1081\":[1,34],\"1082\":[5,40],\"1083\":[1,34],\"1084\":[5,48],\"1085\":[1,34],\"1086\":[5,55],\"1087\":[6,42],\"1088\":[1,34],\"1089\":[6,42],\"1090\":[1,34],\"1091\":[6,43],\"1092\":[1,34],\"1093\":[6,46],\"1094\":[1,34],\"1095\":[7,40],\"1096\":[1,34],\"1097\":[7,40],\"1098\":[1,34],\"1099\":[7,40],\"1100\":[1,34],\"1101\":[7,40],\"1102\":[1,34],\"1103\":[6,51],\"1104\":[1,34],\"1105\":[7,40],\"1106\":[1,34],\"1107\":[5,173],\"1108\":[5,57],\"1109\":[1,34],\"1110\":[10,57],\"1111\":[1,34],\"1112\":[5,89],\"1113\":[5,94],\"1114\":[6,42],\"1115\":[1,34],\"1116\":[6,56],\"1117\":[5,141],\"1118\":[5,132],\"1119\":[5,98],\"1120\":[5,60],\"1121\":[1,34],\"1122\":[5,56],\"1123\":[1,34],\"1124\":[7,128],\"1125\":[7,176],\"1126\":[7,170],\"1127\":[7,106],\"1128\":[6,39],\"1129\":[6,12],\"1130\":[6,158],\"1131\":[5,157],\"1132\":[7,73],\"1133\":[5,92],\"1134\":[5,77],\"1135\":[1,34],\"1136\":[5,125],\"1137\":[6,85],\"1138\":[1,34],\"1139\":[5,98],\"1140\":[1,41],\"1141\":[5,151],\"1142\":[5,35],\"1143\":[1,34],\"1144\":[6,25],\"1145\":[5,106],\"1146\":[1,34],\"1147\":[6,73],\"1148\":[5,44],\"1149\":[5,47],\"1150\":[1,34],\"1151\":[6,37],\"1152\":[1,34],\"1153\":[6,95],\"1154\":[1,34],\"1155\":[4,240],\"1156\":[6,171],\"1157\":[5,252],\"1158\":[6,110],\"1159\":[5,39],\"1160\":[1,34],\"1161\":[6,53],\"1162\":[5,122],\"1163\":[6,44],\"1164\":[6,88],\"1165\":[5,48],\"1166\":[1,34],\"1167\":[7,56],\"1168\":[5,69],\"1169\":[1,34],\"1170\":[7,75],\"1171\":[7,58],\"1172\":[7,98],\"1173\":[7,55],\"1174\":[7,50],\"1175\":[7,55],\"1176\":[6,72],\"1177\":[7,45],\"1178\":[1,34],\"1179\":[5,42],\"1180\":[6,56],\"1181\":[6,66],\"1182\":[5,44],\"1183\":[5,44],\"1184\":[5,44],\"1185\":[5,111],\"1186\":[1,34],\"1187\":[7,38],\"1188\":[1,34],\"1189\":[6,53],\"1190\":[5,40],\"1191\":[1,34],\"1192\":[5,37],\"1193\":[1,34],\"1194\":[5,37],\"1195\":[1,34],\"1196\":[6,38],\"1197\":[1,34],\"1198\":[5,52],\"1199\":[6,56],\"1200\":[6,40],\"1201\":[1,34],\"1202\":[5,107],\"1203\":[1,37],\"1204\":[7,71],\"1205\":[6,48],\"1206\":[1,34],\"1207\":[5,58],\"1208\":[5,63],\"1209\":[8,122],\"1210\":[7,124],\"1211\":[5,97],\"1212\":[1,37],\"1213\":[6,41],\"1214\":[1,34],\"1215\":[5,43],\"1216\":[1,34],\"1217\":[6,133],\"1218\":[6,54],\"1219\":[7,38],\"1220\":[1,34],\"1221\":[6,55],\"1222\":[5,52],\"1223\":[5,46],\"1224\":[5,113],\"1225\":[5,118],\"1226\":[5,37],\"1227\":[1,34],\"1228\":[7,108],\"1229\":[6,57],\"1230\":[6,39],\"1231\":[1,34],\"1232\":[5,117],\"1233\":[6,43],\"1234\":[1,34],\"1235\":[6,164],\"1236\":[6,50],\"1237\":[1,34],\"1238\":[7,56],\"1239\":[1,34],\"1240\":[6,51],\"1241\":[1,34],\"1242\":[7,56],\"1243\":[1,34],\"1244\":[6,53],\"1245\":[5,105],\"1246\":[7,139],\"1247\":[7,85],\"1248\":[7,50],\"1249\":[1,34],\"1250\":[5,127],\"1251\":[5,132],\"1252\":[5,141],\"1253\":[6,93],\"1254\":[1,49],\"1255\":[5,72],\"1256\":[1,34],\"1257\":[5,75],\"1258\":[1,34],\"1259\":[5,111],\"1260\":[1,38],\"1261\":[5,154],\"1262\":[6,145],\"1263\":[1,34],\"1264\":[5,152],\"1265\":[5,81],\"1266\":[1,34],\"1267\":[5,131],\"1268\":[6,145],\"1269\":[5,245],\"1270\":[5,257],\"1271\":[5,252],\"1272\":[5,40],\"1273\":[5,94],\"1274\":[5,113],\"1275\":[7,44],\"1276\":[7,38],\"1277\":[7,44],\"1278\":[5,152],\"1279\":[5,196],\"1280\":[5,296],\"1281\":[7,250],\"1282\":[7,202],\"1283\":[5,240],\"1284\":[6,37],\"1285\":[1,34],\"1286\":[6,40],\"1287\":[1,34],\"1288\":[7,38],\"1289\":[1,34],\"1290\":[6,128],\"1291\":[7,14],\"1292\":[6,21],\"1293\":[7,35],\"1294\":[6,15],\"1295\":[6,10],\"1296\":[6,28],\"1297\":[6,10],\"1298\":[6,15],\"1299\":[7,13],\"1300\":[8,17],\"1301\":[12,106],\"1302\":[8,18],\"1303\":[7,23],\"1304\":[7,24],\"1305\":[7,17],\"1306\":[11,91],\"1307\":[6,10],\"1308\":[7,81],\"1309\":[7,119],\"1310\":[8,70],\"1311\":[9,121],\"1312\":[7,16],\"1313\":[6,9],\"1314\":[6,47],\"1315\":[6,49],\"1316\":[7,86],\"1317\":[8,39],\"1318\":[7,88],\"1319\":[9,95],\"1320\":[7,68],\"1321\":[7,80],\"1322\":[9,96],\"1323\":[7,65],\"1324\":[7,16],\"1325\":[6,28],\"1326\":[9,41],\"1327\":[8,142],\"1328\":[6,75],\"1329\":[7,40],\"1330\":[8,164],\"1331\":[11,27],\"1332\":[7,39],\"1333\":[5,44],\"1334\":[5,296],\"1335\":[7,13],\"1336\":[6,12],\"1337\":[6,12],\"1338\":[6,9],\"1339\":[8,11],\"1340\":[6,12],\"1341\":[9,11],\"1342\":[6,13],\"1343\":[6,10],\"1344\":[12,16],\"1345\":[12,16],\"1346\":[7,25],\"1347\":[7,28],\"1348\":[7,16],\"1349\":[6,9],\"1350\":[7,84],\"1351\":[7,39],\"1352\":[7,29],\"1353\":[5,10],\"1354\":[6,90],\"1355\":[6,13],\"1356\":[6,61],\"1357\":[6,19],\"1358\":[6,11],\"1359\":[6,15],\"1360\":[6,8],\"1361\":[6,34],\"1362\":[7,10],\"1363\":[6,9],\"1364\":[7,10],\"1365\":[7,10],\"1366\":[8,12],\"1367\":[6,12],\"1368\":[7,53],\"1369\":[6,14],\"1370\":[7,20],\"1371\":[11,91],\"1372\":[12,106],\"1373\":[7,23],\"1374\":[7,32],\"1375\":[7,35],\"1376\":[4,46],\"1377\":[6,55],\"1378\":[8,1],\"1379\":[7,1],\"1380\":[8,1],\"1381\":[5,53],\"1382\":[7,30],\"1383\":[7,41],\"1384\":[1,34],\"1385\":[7,93],\"1386\":[7,33],\"1387\":[7,59],\"1388\":[1,34],\"1389\":[5,203],\"1390\":[5,102],\"1391\":[5,129],\"1392\":[7,114],\"1393\":[1,34],\"1394\":[7,13],\"1395\":[6,109],\"1396\":[5,158],\"1397\":[5,112],\"1398\":[6,41],\"1399\":[1,37],\"1400\":[9,136],\"1401\":[5,214],\"1402\":[5,131],\"1403\":[5,139],\"1404\":[6,45],\"1405\":[1,37],\"1406\":[6,87],\"1407\":[1,34],\"1408\":[5,194],\"1409\":[5,125],\"1410\":[5,107],\"1411\":[7,50],\"1412\":[1,34],\"1413\":[7,66],\"1414\":[1,36],\"1415\":[7,40],\"1416\":[1,40],\"1417\":[7,40],\"1418\":[1,34],\"1419\":[7,101],\"1420\":[7,109],\"1421\":[1,36],\"1422\":[7,80],\"1423\":[1,38],\"1424\":[7,59],\"1425\":[1,34],\"1426\":[8,56],\"1427\":[1,34],\"1428\":[7,59],\"1429\":[1,34],\"1430\":[8,60],\"1431\":[1,34],\"1432\":[8,22],\"1433\":[6,41],\"1434\":[1,37],\"1435\":[6,40],\"1436\":[1,37],\"1437\":[8,36],\"1438\":[1,34],\"1439\":[9,65],\"1440\":[1,34],\"1441\":[8,145],\"1442\":[7,69],\"1443\":[1,34],\"1444\":[8,87],\"1445\":[1,34],\"1446\":[7,67],\"1447\":[1,34],\"1448\":[8,87],\"1449\":[1,34],\"1450\":[7,156],\"1451\":[1,34],\"1452\":[8,163],\"1453\":[1,36],\"1454\":[7,151],\"1455\":[1,34],\"1456\":[8,160],\"1457\":[1,34],\"1458\":[7,106],\"1459\":[1,34],\"1460\":[8,111],\"1461\":[1,34],\"1462\":[7,56],\"1463\":[1,34],\"1464\":[8,35],\"1465\":[1,34],\"1466\":[5,209],\"1467\":[5,128],\"1468\":[5,129],\"1469\":[9,130],\"1470\":[1,34],\"1471\":[7,11],\"1472\":[7,11],\"1473\":[11,15],\"1474\":[12,17],\"1475\":[9,17],\"1476\":[9,16],\"1477\":[9,30],\"1478\":[9,36],\"1479\":[9,15],\"1480\":[9,14],\"1481\":[10,17],\"1482\":[9,18],\"1483\":[9,17],\"1484\":[11,82],\"1485\":[9,40],\"1486\":[7,12],\"1487\":[7,14],\"1488\":[9,11],\"1489\":[9,24],\"1490\":[9,16],\"1491\":[10,21],\"1492\":[9,13],\"1493\":[7,47],\"1494\":[8,48],\"1495\":[10,14],\"1496\":[8,10],\"1497\":[10,15],\"1498\":[10,15],\"1499\":[11,16],\"1500\":[7,9],\"1501\":[9,24],\"1502\":[9,42],\"1503\":[9,13],\"1504\":[10,14],\"1505\":[7,23],\"1506\":[8,24],\"1507\":[9,11],\"1508\":[5,40],\"1509\":[5,101],\"1510\":[1,34],\"1511\":[5,122],\"1512\":[1,34],\"1513\":[5,140],\"1514\":[6,80],\"1515\":[5,74],\"1516\":[5,71],\"1517\":[6,44],\"1518\":[1,34],\"1519\":[7,108],\"1520\":[7,56],\"1521\":[6,140],\"1522\":[7,49],\"1523\":[1,37],\"1524\":[7,76],\"1525\":[7,66],\"1526\":[6,340],\"1527\":[6,37],\"1528\":[1,34],\"1529\":[7,71],\"1530\":[5,53],\"1531\":[1,34],\"1532\":[5,10],\"1533\":[6,103],\"1534\":[6,86],\"1535\":[7,98],\"1536\":[7,100],\"1537\":[6,38],\"1538\":[1,34],\"1539\":[7,107],\"1540\":[1,41],\"1541\":[5,48],\"1542\":[1,34],\"1543\":[5,48],\"1544\":[1,34],\"1545\":[7,96],\"1546\":[7,137],\"1547\":[6,40],\"1548\":[5,183],\"1549\":[6,120],\"1550\":[1,34],\"1551\":[6,150],\"1552\":[6,300],\"1553\":[5,346],\"1554\":[6,49],\"1555\":[1,47],\"1556\":[7,92],\"1557\":[9,13],\"1558\":[8,83],\"1559\":[6,36],\"1560\":[7,16],\"1561\":[7,14],\"1562\":[7,11],\"1563\":[6,11],\"1564\":[6,12],\"1565\":[7,13],\"1566\":[7,15],\"1567\":[8,11],\"1568\":[6,13],\"1569\":[7,12],\"1570\":[8,14],\"1571\":[6,11],\"1572\":[7,10],\"1573\":[7,10],\"1574\":[7,12],\"1575\":[6,10],\"1576\":[5,40],\"1577\":[6,69],\"1578\":[7,21],\"1579\":[7,24],\"1580\":[7,21],\"1581\":[6,71],\"1582\":[7,83],\"1583\":[6,63],\"1584\":[6,54],\"1585\":[6,110],\"1586\":[6,52],\"1587\":[6,57],\"1588\":[6,54],\"1589\":[6,58],\"1590\":[7,67],\"1591\":[6,48],\"1592\":[5,141],\"1593\":[5,83],\"1594\":[5,109],\"1595\":[5,123],\"1596\":[5,108],\"1597\":[5,122],\"1598\":[5,309],\"1599\":[6,286],\"1600\":[6,295],\"1601\":[6,54],\"1602\":[6,46],\"1603\":[6,62],\"1604\":[5,106],\"1605\":[5,135],\"1606\":[5,144],\"1607\":[6,96],\"1608\":[6,80],\"1609\":[6,102],\"1610\":[6,135],\"1611\":[7,103],\"1612\":[7,103],\"1613\":[7,91],\"1614\":[7,75],\"1615\":[7,79],\"1616\":[7,96],\"1617\":[7,47],\"1618\":[6,109],\"1619\":[6,125],\"1620\":[9,66],\"1621\":[9,70],\"1622\":[7,129],\"1623\":[6,24],\"1624\":[7,68],\"1625\":[5,295],\"1626\":[6,254],\"1627\":[6,71],\"1628\":[5,125],\"1629\":[8,40],\"1630\":[9,1],\"1631\":[8,56],\"1632\":[7,31],\"1633\":[7,30],\"1634\":[8,26],\"1635\":[8,27],\"1636\":[9,28],\"1637\":[7,39],\"1638\":[4,58],\"1639\":[1,34],\"1640\":[5,103],\"1641\":[5,82],\"1642\":[6,21],\"1643\":[7,60],\"1644\":[1,63],\"1645\":[6,118],\"1646\":[6,59],\"1647\":[1,72],\"1648\":[6,26],\"1649\":[6,14],\"1650\":[6,78],\"1651\":[8,20],\"1652\":[5,44],\"1653\":[1,34],\"1654\":[5,61],\"1655\":[4,126],\"1656\":[5,88],\"1657\":[7,39],\"1658\":[1,34],\"1659\":[5,20],\"1660\":[5,75],\"1661\":[5,29],\"1662\":[5,120],\"1663\":[1,34],\"1664\":[6,77],\"1665\":[6,89],\"1666\":[5,49],\"1667\":[5,45],\"1668\":[5,149],\"1669\":[4,95],\"1670\":[5,72],\"1671\":[5,72],\"1672\":[5,58],\"1673\":[5,49],\"1674\":[5,41],\"1675\":[9,20],\"1676\":[4,47],\"1677\":[4,36],\"1678\":[1,96],\"1679\":[4,55],\"1680\":[5,72],\"1681\":[4,31],\"1682\":[6,16],\"1683\":[6,88],\"1684\":[8,23],\"1685\":[8,23],\"1686\":[4,48],\"1687\":[5,53],\"1688\":[7,17],\"1689\":[5,45],\"1690\":[5,45],\"1691\":[5,41],\"1692\":[5,83],\"1693\":[5,10],\"1694\":[4,47],\"1695\":[7,20],\"1696\":[4,9],\"1697\":[5,55],\"1698\":[5,90],\"1699\":[4,24],\"1700\":[4,35],\"1701\":[7,26],\"1702\":[5,106],\"1703\":[6,17],\"1704\":[8,85],\"1705\":[8,100],\"1706\":[8,98],\"1707\":[8,86],\"1708\":[8,104],\"1709\":[8,107],\"1710\":[8,121],\"1711\":[8,111],\"1712\":[8,113],\"1713\":[8,101],\"1714\":[8,98],\"1715\":[8,115],\"1716\":[8,127],\"1717\":[5,82],\"1718\":[6,17],\"1719\":[7,194],\"1720\":[8,195],\"1721\":[9,193],\"1722\":[7,42],\"1723\":[6,59],\"1724\":[6,65],\"1725\":[6,235],\"1726\":[7,104],\"1727\":[8,110],\"1728\":[5,23],\"1729\":[7,55],\"1730\":[7,109],\"1731\":[6,112],\"1732\":[8,25],\"1733\":[7,39],\"1734\":[6,24],\"1735\":[11,129],\"1736\":[8,102],\"1737\":[10,61],\"1738\":[8,53],\"1739\":[8,55],\"1740\":[8,56],\"1741\":[8,55],\"1742\":[8,63],\"1743\":[8,55],\"1744\":[8,55],\"1745\":[8,63],\"1746\":[10,53],\"1747\":[8,45],\"1748\":[8,92],\"1749\":[9,150],\"1750\":[8,200],\"1751\":[9,130],\"1752\":[9,52],\"1753\":[1,72],\"1754\":[9,66],\"1755\":[1,9],\"1756\":[9,91],\"1757\":[9,92],\"1758\":[8,117],\"1759\":[9,116],\"1760\":[7,68],\"1761\":[6,13],\"1762\":[7,39],\"1763\":[6,17],\"1764\":[9,71],\"1765\":[6,1],\"1766\":[8,67],\"1767\":[5,1],\"1768\":[8,134],\"1769\":[8,1],\"1770\":[9,89],\"1771\":[9,74],\"1772\":[6,18],\"1773\":[6,17],\"1774\":[1,16],\"1775\":[7,35],\"1776\":[5,20],\"1777\":[5,11],\"1778\":[6,22],\"1779\":[9,64],\"1780\":[6,19],\"1781\":[6,23],\"1782\":[10,68],\"1783\":[9,41],\"1784\":[8,96],\"1785\":[8,105],\"1786\":[8,69],\"1787\":[7,69],\"1788\":[9,88],\"1789\":[8,89],\"1790\":[8,91],\"1791\":[5,27],\"1792\":[9,1],\"1793\":[6,17],\"1794\":[8,140],\"1795\":[10,72],\"1796\":[8,37],\"1797\":[6,21],\"1798\":[6,58],\"1799\":[6,64],\"1800\":[6,66],\"1801\":[8,53],\"1802\":[5,24],\"1803\":[7,40],\"1804\":[6,37],\"1805\":[1,53],\"1806\":[8,181],\"1807\":[8,51],\"1808\":[8,67],\"1809\":[10,38],\"1810\":[8,96],\"1811\":[8,57],\"1812\":[1,50],\"1813\":[5,16],\"1814\":[8,67],\"1815\":[9,136],\"1816\":[8,74],\"1817\":[8,108],\"1818\":[8,75],\"1819\":[6,21],\"1820\":[8,60],\"1821\":[6,24],\"1822\":[1,101],\"1823\":[6,21],\"1824\":[5,73],\"1825\":[6,18],\"1826\":[6,19],\"1827\":[1,18],\"1828\":[6,18],\"1829\":[6,18],\"1830\":[1,18],\"1831\":[6,1],\"1832\":[5,18],\"1833\":[5,73],\"1834\":[6,23],\"1835\":[5,24],\"1836\":[5,24],\"1837\":[8,60],\"1838\":[8,33],\"1839\":[9,82],\"1840\":[6,1],\"1841\":[6,1],\"1842\":[8,37],\"1843\":[7,85],\"1844\":[5,18],\"1845\":[5,18],\"1846\":[1,39],\"1847\":[10,63],\"1848\":[6,34],\"1849\":[7,38],\"1850\":[5,19],\"1851\":[8,64],\"1852\":[5,20],\"1853\":[5,20],\"1854\":[7,114],\"1855\":[8,39],\"1856\":[1,49],\"1857\":[5,14],\"1858\":[9,37],\"1859\":[8,15],\"1860\":[9,53],\"1861\":[10,31],\"1862\":[5,109],\"1863\":[8,94],\"1864\":[10,42],\"1865\":[10,32],\"1866\":[10,49],\"1867\":[10,41],\"1868\":[10,43],\"1869\":[10,21],\"1870\":[9,47],\"1871\":[11,37],\"1872\":[10,30],\"1873\":[9,33],\"1874\":[8,13],\"1875\":[6,15],\"1876\":[5,29],\"1877\":[9,32],\"1878\":[9,44],\"1879\":[8,13],\"1880\":[8,41],\"1881\":[8,65],\"1882\":[1,18],\"1883\":[8,106],\"1884\":[7,1],\"1885\":[8,19],\"1886\":[8,13],\"1887\":[7,13],\"1888\":[10,31],\"1889\":[9,16],\"1890\":[1,9],\"1891\":[13,43],\"1892\":[8,43],\"1893\":[9,28],\"1894\":[10,23],\"1895\":[9,53],\"1896\":[7,21],\"1897\":[9,29],\"1898\":[8,17],\"1899\":[5,18],\"1900\":[5,28],\"1901\":[10,61],\"1902\":[1,29],\"1903\":[9,61],\"1904\":[1,28],\"1905\":[9,32],\"1906\":[1,19],\"1907\":[10,53],\"1908\":[8,33],\"1909\":[1,13],\"1910\":[9,28],\"1911\":[7,15],\"1912\":[1,7],\"1913\":[10,35],\"1914\":[10,41],\"1915\":[9,27],\"1916\":[9,22],\"1917\":[7,39],\"1918\":[10,25],\"1919\":[8,59],\"1920\":[10,61],\"1921\":[10,44],\"1922\":[5,1],\"1923\":[4,18],\"1924\":[5,23],\"1925\":[5,21],\"1926\":[8,35],\"1927\":[8,35],\"1928\":[8,35],\"1929\":[7,1],\"1930\":[7,1],\"1931\":[8,31],\"1932\":[9,33],\"1933\":[1,22],\"1934\":[10,40],\"1935\":[8,15],\"1936\":[12,56],\"1937\":[10,42],\"1938\":[5,77],\"1939\":[1,34],\"1940\":[5,90],\"1941\":[1,57],\"1942\":[6,111],\"1943\":[1,70],\"1944\":[6,87],\"1945\":[5,113],\"1946\":[1,71],\"1947\":[4,103],\"1948\":[5,40],\"1949\":[6,61],\"1950\":[6,40],\"1951\":[5,52],\"1952\":[10,16],\"1953\":[8,16],\"1954\":[4,16],\"1955\":[5,32],\"1956\":[1,11],\"1957\":[5,81],\"1958\":[1,47],\"1959\":[5,99],\"1960\":[5,78],\"1961\":[5,86],\"1962\":[4,52],\"1963\":[7,45],\"1964\":[6,13],\"1965\":[5,94],\"1966\":[5,84],\"1967\":[6,51],\"1968\":[1,36],\"1969\":[6,43],\"1970\":[1,36],\"1971\":[5,57],\"1972\":[7,44],\"1973\":[1,45],\"1974\":[5,42],\"1975\":[5,124],\"1976\":[1,77],\"1977\":[5,69],\"1978\":[8,66],\"1979\":[1,48],\"1980\":[9,86],\"1981\":[1,48],\"1982\":[8,70],\"1983\":[1,48],\"1984\":[6,61],\"1985\":[5,43],\"1986\":[1,34],\"1987\":[6,49],\"1988\":[6,43],\"1989\":[1,34],\"1990\":[7,45],\"1991\":[6,79],\"1992\":[5,230],\"1993\":[5,228],\"1994\":[5,112],\"1995\":[5,174],\"1996\":[6,104],\"1997\":[5,143],\"1998\":[5,18],\"1999\":[6,27],\"2000\":[6,219],\"2001\":[6,194],\"2002\":[6,35],\"2003\":[6,32],\"2004\":[7,33],\"2005\":[6,32],\"2006\":[6,50],\"2007\":[5,79],\"2008\":[6,44],\"2009\":[7,11],\"2010\":[5,20],\"2011\":[5,20],\"2012\":[5,20],\"2013\":[5,21],\"2014\":[7,63],\"2015\":[6,80],\"2016\":[5,89],\"2017\":[7,48],\"2018\":[5,100],\"2019\":[5,57],\"2020\":[5,76],\"2021\":[6,57],\"2022\":[9,1],\"2023\":[10,1],\"2024\":[9,1],\"2025\":[9,1],\"2026\":[5,38],\"2027\":[1,36],\"2028\":[7,40],\"2029\":[1,36],\"2030\":[5,38],\"2031\":[1,36],\"2032\":[5,38],\"2033\":[1,36],\"2034\":[5,39],\"2035\":[1,36],\"2036\":[9,1],\"2037\":[9,1],\"2038\":[9,1],\"2039\":[5,91],\"2040\":[5,74],\"2041\":[9,1],\"2042\":[9,1],\"2043\":[5,92],\"2044\":[5,175],\"2045\":[5,102],\"2046\":[9,1],\"2047\":[9,1],\"2048\":[9,1],\"2049\":[6,125],\"2050\":[9,1],\"2051\":[9,1],\"2052\":[9,1],\"2053\":[9,1],\"2054\":[8,101],\"2055\":[6,87],\"2056\":[5,85],\"2057\":[8,1],\"2058\":[9,1],\"2059\":[9,1],\"2060\":[9,1],\"2061\":[9,22],\"2062\":[9,1],\"2063\":[9,1],\"2064\":[9,1],\"2065\":[5,116],\"2066\":[5,79],\"2067\":[10,1],\"2068\":[11,1],\"2069\":[11,1],\"2070\":[10,1],\"2071\":[12,1],\"2072\":[10,1],\"2073\":[11,1],\"2074\":[11,1],\"2075\":[9,1],\"2076\":[10,1],\"2077\":[11,1],\"2078\":[10,1],\"2079\":[11,1],\"2080\":[11,1],\"2081\":[9,1],\"2082\":[10,1],\"2083\":[10,1],\"2084\":[10,1],\"2085\":[10,1],\"2086\":[11,1],\"2087\":[10,1],\"2088\":[11,16],\"2089\":[11,1],\"2090\":[12,1],\"2091\":[11,1],\"2092\":[11,1],\"2093\":[11,1],\"2094\":[12,1],\"2095\":[9,16],\"2096\":[11,13],\"2097\":[10,1],\"2098\":[11,1],\"2099\":[11,1],\"2100\":[11,1],\"2101\":[4,61],\"2102\":[9,16],\"2103\":[9,1],\"2104\":[10,1],\"2105\":[9,1],\"2106\":[13,1],\"2107\":[13,1],\"2108\":[11,1],\"2109\":[12,1],\"2110\":[12,1],\"2111\":[12,1],\"2112\":[12,1],\"2113\":[10,1],\"2114\":[10,1],\"2115\":[9,13],\"2116\":[10,20],\"2117\":[10,1],\"2118\":[10,1],\"2119\":[12,1],\"2120\":[10,1],\"2121\":[10,1],\"2122\":[9,1],\"2123\":[9,1],\"2124\":[5,59],\"2125\":[1,38],\"2126\":[5,85],\"2127\":[5,127],\"2128\":[7,50],\"2129\":[5,130],\"2130\":[7,335],\"2131\":[6,166],\"2132\":[5,86],\"2133\":[7,173],\"2134\":[5,208],\"2135\":[5,1],\"2136\":[7,249],\"2137\":[7,109],\"2138\":[7,53],\"2139\":[7,87],\"2140\":[6,29],\"2141\":[5,53],\"2142\":[5,51],\"2143\":[5,128],\"2144\":[7,62],\"2145\":[5,61],\"2146\":[6,58],\"2147\":[6,71],\"2148\":[8,44],\"2149\":[7,40],\"2150\":[10,39],\"2151\":[9,82],\"2152\":[8,17],\"2153\":[7,15],\"2154\":[5,9],\"2155\":[6,79],\"2156\":[9,38],\"2157\":[6,41],\"2158\":[6,1],\"2159\":[8,25],\"2160\":[8,23],\"2161\":[7,23],\"2162\":[6,67],\"2163\":[7,11],\"2164\":[6,17],\"2165\":[8,1],\"2166\":[7,28],\"2167\":[5,99],\"2168\":[6,69],\"2169\":[1,34],\"2170\":[5,40],\"2171\":[1,34],\"2172\":[5,39],\"2173\":[1,38],\"2174\":[5,40],\"2175\":[1,38],\"2176\":[7,172],\"2177\":[6,40],\"2178\":[1,34],\"2179\":[6,41],\"2180\":[1,34],\"2181\":[6,40],\"2182\":[1,34],\"2183\":[7,101],\"2184\":[5,167],\"2185\":[6,40],\"2186\":[1,34],\"2187\":[6,95],\"2188\":[5,63],\"2189\":[1,37],\"2190\":[5,69],\"2191\":[5,170],\"2192\":[5,87],\"2193\":[1,37],\"2194\":[5,38],\"2195\":[1,36],\"2196\":[6,48],\"2197\":[1,34],\"2198\":[5,111],\"2199\":[1,37],\"2200\":[6,39],\"2201\":[1,34],\"2202\":[6,41],\"2203\":[6,98],\"2204\":[1,37],\"2205\":[6,38],\"2206\":[1,36],\"2207\":[5,60],\"2208\":[5,89],\"2209\":[5,86],\"2210\":[1,37],\"2211\":[5,37],\"2212\":[1,36],\"2213\":[6,46],\"2214\":[6,46],\"2215\":[5,80],\"2216\":[5,113],\"2217\":[1,50],\"2218\":[5,56],\"2219\":[5,67],\"2220\":[5,119],\"2221\":[5,124],\"2222\":[4,55],\"2223\":[6,150],\"2224\":[1,96],\"2225\":[1,9],\"2226\":[5,83],\"2227\":[7,82],\"2228\":[6,129],\"2229\":[5,122],\"2230\":[5,1],\"2231\":[6,127],\"2232\":[6,95],\"2233\":[6,9],\"2234\":[5,1],\"2235\":[5,231],\"2236\":[6,241],\"2237\":[5,61],\"2238\":[6,91],\"2239\":[5,299],\"2240\":[4,315],\"2241\":[5,87],\"2242\":[6,11],\"2243\":[6,11],\"2244\":[8,15],\"2245\":[4,305],\"2246\":[4,115],\"2247\":[5,82],\"2248\":[4,115],\"2249\":[5,273],\"2250\":[4,114],\"2251\":[4,115],\"2252\":[5,116],\"2253\":[4,186],\"2254\":[5,120],\"2255\":[5,122],\"2256\":[5,121],\"2257\":[4,116],\"2258\":[5,48],\"2259\":[4,115],\"2260\":[4,116],\"2261\":[4,115],\"2262\":[4,99],\"2263\":[4,121],\"2264\":[5,115],\"2265\":[4,90],\"2266\":[4,115],\"2267\":[4,115],\"2268\":[4,121],\"2269\":[4,115],\"2270\":[4,121],\"2271\":[4,121],\"2272\":[5,116],\"2273\":[4,116],\"2274\":[5,18],\"2275\":[5,31],\"2276\":[6,43],\"2277\":[5,48],\"2278\":[8,32],\"2279\":[6,21],\"2280\":[5,52],\"2281\":[5,27],\"2282\":[5,12],\"2283\":[7,44],\"2284\":[5,35],\"2285\":[5,34],\"2286\":[5,53],\"2287\":[5,58],\"2288\":[5,24],\"2289\":[4,16],\"2290\":[1,15],\"2291\":[6,32],\"2292\":[5,28],\"2293\":[4,42],\"2294\":[6,11],\"2295\":[7,12],\"2296\":[9,14],\"2297\":[7,12],\"2298\":[7,57],\"2299\":[1,22],\"2300\":[6,11],\"2301\":[7,12],\"2302\":[9,14],\"2303\":[7,12],\"2304\":[6,38],\"2305\":[1,45],\"2306\":[1,34],\"2307\":[6,59],\"2308\":[9,29],\"2309\":[7,46],\"2310\":[9,82],\"2311\":[7,55],\"2312\":[4,42],\"2313\":[9,12],\"2314\":[6,34],\"2315\":[1,9],\"2316\":[5,9],\"2317\":[6,9],\"2318\":[6,15],\"2319\":[6,11],\"2320\":[6,15],\"2321\":[7,10],\"2322\":[7,11],\"2323\":[6,18],\"2324\":[4,18],\"2325\":[6,118],\"2326\":[1,34],\"2327\":[7,145],\"2328\":[4,9],\"2329\":[4,26],\"2330\":[4,20],\"2331\":[4,30],\"2332\":[4,17],\"2333\":[5,23],\"2334\":[5,66],\"2335\":[5,25],\"2336\":[4,78],\"2337\":[5,70],\"2338\":[5,63],\"2339\":[5,35],\"2340\":[5,41],\"2341\":[4,32],\"2342\":[4,46],\"2343\":[1,25],\"2344\":[4,77],\"2345\":[4,59],\"2346\":[4,68],\"2347\":[5,66],\"2348\":[5,79],\"2349\":[4,10],\"2350\":[5,61],\"2351\":[5,37],\"2352\":[1,27],\"2353\":[4,84],\"2354\":[5,165],\"2355\":[6,405],\"2356\":[4,65],\"2357\":[4,31],\"2358\":[4,8],\"2359\":[4,100],\"2360\":[4,70],\"2361\":[4,72],\"2362\":[4,59],\"2363\":[4,69],\"2364\":[4,84],\"2365\":[5,56],\"2366\":[5,42],\"2367\":[4,69],\"2368\":[4,73],\"2369\":[4,118],\"2370\":[4,69],\"2371\":[5,67],\"2372\":[5,78],\"2373\":[4,19],\"2374\":[4,15],\"2375\":[5,8],\"2376\":[5,34],\"2377\":[1,36],\"2378\":[6,42],\"2379\":[4,18],\"2380\":[6,34],\"2381\":[7,16],\"2382\":[7,15],\"2383\":[7,13],\"2384\":[7,44],\"2385\":[7,44],\"2386\":[6,15],\"2387\":[7,16],\"2388\":[8,11],\"2389\":[8,11],\"2390\":[5,19],\"2391\":[5,8],\"2392\":[6,9],\"2393\":[7,14],\"2394\":[6,10],\"2395\":[6,9],\"2396\":[5,8],\"2397\":[5,14],\"2398\":[6,19],\"2399\":[7,15],\"2400\":[6,10],\"2401\":[6,42],\"2402\":[1,43],\"2403\":[4,55],\"2404\":[6,66],\"2405\":[1,46],\"2406\":[1,42],\"2407\":[6,54],\"2408\":[5,95],\"2409\":[6,68],\"2410\":[1,42],\"2411\":[4,286],\"2412\":[4,288],\"2413\":[5,82],\"2414\":[7,65],\"2415\":[1,46],\"2416\":[8,85],\"2417\":[1,46],\"2418\":[7,69],\"2419\":[1,46],\"2420\":[5,45],\"2421\":[6,29],\"2422\":[8,47],\"2423\":[4,293],\"2424\":[5,81],\"2425\":[6,82],\"2426\":[5,44],\"2427\":[5,82],\"2428\":[5,111],\"2429\":[6,95],\"2430\":[6,73],\"2431\":[4,273],\"2432\":[4,286],\"2433\":[6,81],\"2434\":[6,74],\"2435\":[1,110],\"2436\":[6,34],\"2437\":[6,13],\"2438\":[6,41],\"2439\":[6,34],\"2440\":[7,41],\"2441\":[5,21],\"2442\":[6,39],\"2443\":[6,66],\"2444\":[1,34],\"2445\":[4,59],\"2446\":[5,101],\"2447\":[5,264],\"2448\":[5,87],\"2449\":[6,46],\"2450\":[1,34],\"2451\":[5,42],\"2452\":[1,34],\"2453\":[5,44],\"2454\":[1,38],\"2455\":[5,30],\"2456\":[5,42],\"2457\":[1,36],\"2458\":[5,72],\"2459\":[1,34],\"2460\":[5,73],\"2461\":[1,36],\"2462\":[5,121],\"2463\":[5,46],\"2464\":[5,42],\"2465\":[5,36],\"2466\":[1,34],\"2467\":[5,35],\"2468\":[1,34],\"2469\":[5,50],\"2470\":[6,54],\"2471\":[6,50],\"2472\":[6,54],\"2473\":[6,52],\"2474\":[5,53],\"2475\":[4,8],\"2476\":[4,14],\"2477\":[6,14],\"2478\":[6,24],\"2479\":[1,28],\"2480\":[8,76],\"2481\":[5,16],\"2482\":[5,57],\"2483\":[4,12],\"2484\":[6,13],\"2485\":[1,22],\"2486\":[4,11],\"2487\":[5,14],\"2488\":[1,12],\"2489\":[6,20],\"2490\":[4,52],\"2491\":[8,14],\"2492\":[6,13],\"2493\":[1,21],\"2494\":[3,7],\"2495\":[5,50],\"2496\":[5,9],\"2497\":[5,9],\"2498\":[4,9],\"2499\":[5,10],\"2500\":[1,19],\"2501\":[5,10],\"2502\":[1,7],\"2503\":[6,10],\"2504\":[6,12],\"2505\":[1,21],\"2506\":[4,11],\"2507\":[7,16],\"2508\":[1,3],\"2509\":[2],\"2510\":[1],\"2511\":[1],\"2512\":[1],\"2513\":[2],\"2514\":[2],\"2515\":[1],\"2516\":[1],\"2517\":[1],\"2518\":[1],\"2519\":[1],\"2520\":[1],\"2521\":[1],\"2522\":[1],\"2523\":[1],\"2524\":[2],\"2525\":[2],\"2526\":[2],\"2527\":[1],\"2528\":[1],\"2529\":[1],\"2530\":[1],\"2531\":[1],\"2532\":[1],\"2533\":[2],\"2534\":[1],\"2535\":[1],\"2536\":[1],\"2537\":[1],\"2538\":[1],\"2539\":[1],\"2540\":[1],\"2541\":[1],\"2542\":[1],\"2543\":[1],\"2544\":[1],\"2545\":[1],\"2546\":[1],\"2547\":[1],\"2548\":[1],\"2549\":[1],\"2550\":[2],\"2551\":[1],\"2552\":[1],\"2553\":[1],\"2554\":[1],\"2555\":[1]},\"averageFieldLength\":[4.970657276995311,64.04127011049479],\"storedFields\":{\"0\":{\"h\":\"Developer's Guide to the ESPnet Homepage\",\"t\":[\"This document outlines the process of automatically generating the ESPnet homepage. It provides step-by-step instructions for building the homepage and details the underlying operations during the generation process.\"]},\"1\":{\"h\":\"Building the Homepage\",\"t\":[\"Clone the ESPnet Repository: Begin by cloning the ESPnet repository from GitHub.\",\"Generate the activate_python.sh Script:\",\"You can generate this file by running either setup_miniforge.sh or setup_venv.sh.\",\"Alternatively, create your own virtual environment and manually write the command to activate it in activate_python.sh.\",\"Run activate_python.sh: Execute this script to activate the Python environment.\",\"Install the Dependencies: Install the necessary dependencies using the following commands:\",\"espnet[all] espnet[doc] k2 chainer\",\"Build the Homepage: Run the following script to generate the homepage:\",\"./ci/doc.sh\"]},\"2\":{\"h\":\"Key Points\",\"t\":[\"The homepage is built using VuePress, a static site generator that converts Markdown files into a website.\",\"The primary function of ci/doc.sh is to generate Markdown files for all documentation.\"]},\"3\":{\"h\":\"Step-by-Step Guide to\",\"t\":[\"build_and_convert Function: This function generates documentation for shell scripts by invoking ./doc/usage2rst.sh on all scripts in the specified directory ($1). The usage2rst.sh script executes each script with the --help option and saves the output as an RST file in the $2/<shell_name>.rst directory.\",\"Temporary Files Directory: All temporary files, including RST files, are stored in the _gen directory.\",\"./doc/argparse2rst.py Script: This script generates documentation for Python tools located in espnet/bin, espnet2/bin, and utils/. These scripts are executable from the command line, so their documentation is separated from the package information.\",\"./doc/notebook2rst.sh Script: This script generates the demo section by pulling the notebook repository and converting Jupyter Notebook (.ipynb) files into Markdown.\",\"./doc/members2rst.py Script: This script generates RST files for all docstrings. It separates out any docstrings for classes or functions that are not class members and excludes private functions (those starting with _). The generated RST files are saved in ./_gen/guide.\",\"Sphinx Build Process: After copying all necessary files to the _gen directory, run sphinx-build within _gen. Running Sphinx directly in the doc directory could cause issues, including potential document corruption. Some files, particularly those ending with _train (e.g., espnet2/bin/asr_train.py), are excluded from the documentation to avoid errors.\",\"VuePress Directory Setup: Copy the Markdown files from the doc directory, along with files generated in steps 4 and 6, into the vuepress/src directory. This is where VuePress recognizes the pages for the site.\",\"Language Support Adjustment: VuePress doesn’t support some of the programming languages used in code blocks. To address this, we include a command to replace unsupported language codes with equivalent ones.\",\"Generate Navigation Files: Create the navbar.yml and sidebar.yml files to define the menus displayed at the top and side of the webpage. For more details, refer to the VuePress-Hope documentation on navbar configuration and sidebar configuration.\",\"Finalize the Build: Install Node.js and the necessary dependencies, then build the homepage. To preview the page, comment out the docs:build line and uncomment the docs:dev line in the script.\"]},\"4\":{\"h\":\"Additional citations\"},\"5\":{\"h\":\"ESPnet-EZ\",\"t\":[\"@inproceedings{hayashi2020espnet, title={ESPnet-EZ: Python-only ESPnet for Easy Fine-tuning and Integration}, author={Masao Someki and Kwanghee Choi and Siddhant Arora and William Chen and Samuele Cornell and Jionghao Han and Yifan Peng and Jiatong Shi and Vaibhav Srivastav and Shinji Watanabe}, booktitle={SLT}, year={2024}, }\"]},\"6\":{\"h\":\"Weakly-supervised Learning\",\"t\":[\"# OWSM @inproceedings{peng2023reproducing, title={Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data}, author={Peng, Yifan and Tian, Jinchuan and Yan, Brian and Berrebbi, Dan and Chang, Xuankai and Li, Xinjian and Shi, Jiatong and Arora, Siddhant and Chen, William and Sharma, Roshan and others}, booktitle={ASRU}, year={2023}, } # OWSM v3.1 @inproceedings{peng2023reproducing, title={OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer}, author={Peng, Yifan and Tian, Jinchuan and Chen, William and Arora, Siddhant and Yan, Brian and Sudo, Yui and Shakeel, Muhammad and Choi, Kwanghee and Shi, Jiatong and Chang, Xuankai and others}, booktitle={Interspeech}, year={2024}, } # OWSM v3.2 @inproceedings{tian2024effects, title={On the Effects of Heterogeneous Data Sources on Speech-to-Text Foundation Models}, author={Tian, Jinchuan and Peng, Yifan and Chen, William and Choi, Kwanghee and Livescu, Karen and Watanabe, Shinji}, booktitle={Interspeech}, year={2024}, }\"]},\"7\":{\"h\":\"Self-supervised Learning\",\"t\":[\"# HuBERT reproduction @inproceedings{chen2023reducing, title={Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute}, author={William Chen and Xuankai Chang and Yifan Peng and Zhaoheng Ni and Soumi Maiti and Shinji Watanabe}, booktitle={Interspeech}, year={2023}, }\"]},\"8\":{\"h\":\"Speaker Embeddings\",\"t\":[\"@inproceedings{jung2024espnet, title={ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models}, author={Jung, Jee-weon and Zhang, Wangyou and Shi, Jiatong and Aldeneh, Zakaria and Higuchi, Takuya and Theobald, Barry-John and Abdelaziz, Ahmed Hussen and Watanabe, Shinji}, booktitle={Interspeech}, year={2024}, }\"]},\"9\":{\"h\":\"TTS\",\"t\":[\"@inproceedings{hayashi2020espnet, title={ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit}, author={Hayashi, Tomoki and Yamamoto, Ryuichi and Inoue, Katsuki and Yoshimura, Takenori and Watanabe, Shinji and Toda, Tomoki and Takeda, Kazuya and Zhang, Yu and Tan, Xu}, booktitle={ICASSP}, year={2020}, } @article{hayashi2021espnet2, title={ESPnet2-TTS: Extending the Edge of TTS Research}, author={Hayashi, Tomoki and Yamamoto, Ryuichi and Yoshimura, Takenori and Wu, Peter and Shi, Jiatong and Saeki, Takaaki and Ju, Yooncheol and Yasuda, Yusuke and Takamichi, Shinnosuke and Watanabe, Shinji}, journal={arXiv preprint arXiv:2110.07840}, year={2021} }\"]},\"10\":{\"h\":\"Speech Translation\",\"t\":[\"@inproceedings{inaguma2020espnet, title={ESPnet-ST: All-in-one speech translation toolkit}, author={Inaguma, Hirofumi and Kiyono, Shun and Duh, Kevin and Karita, Shigeki and Soplin, Nelson Enrique Yalta and Hayashi, Tomoki and Watanabe, Shinji}, booktitle={ACL Demos}, year={2020}, } @inproceedings{yan2023espnet, title={ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit}, author={Yan, Brian and Shi, Jiatong and Tang, Yun and Inaguma, Hirofumi and Peng, Yifan and Dalmia, Siddharth and Polak, Peter and Fernandes, Patrick and Berrebbi, Dan and Hayashi, Tomoki and Zhang, Xiaohui and Ni, Zhaoheng and Hira, Moto and Maiti, Soumi and Pino, Juan and Watanabe, Shinji}, booktitle={ACL Demos}, year={2023}, }\"]},\"11\":{\"h\":\"Speech Enhancement\",\"t\":[\"@inproceedings{li2020espnetse, title={ESPnet-SE: End-to-End Speech Enhancement and Separation Toolkit Designed for ASR Integration}, author={Chenda Li and Jing Shi and Wangyou Zhang and Aswin Shanmugam Subramanian and Xuankai Chang and Naoyuki Kamo and Moto Hira and Tomoki Hayashi and Christoph Boeddeker and Zhuo Chen and Shinji Watanabe}, booktitle={SLT}, year={2021}, } @inproceedings{lu2022espnetsep, author={Yen-Ju Lu and Xuankai Chang and Chenda Li and Wangyou Zhang and Samuele Cornell and Zhaoheng Ni and Yoshiki Masuyama and Brian Yan and Robin Scheibler and Zhong-Qiu Wang and Yu Tsao and Yanmin Qian and Shinji Watanabe}, title={ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding}, booktitle={Interspeech}, year={2022}, }\"]},\"12\":{\"h\":\"Spoken Language Understanding\",\"t\":[\"@inproceedings{arora2021espnet, title={ESPnet-SLU: Advancing Spoken Language Understanding through ESPnet}, author={Arora, Siddhant and Dalmia, Siddharth and Denisov, Pavel and Chang, Xuankai and Ueda, Yushi and Peng, Yifan and Zhang, Yuekai and Kumar, Sujay and Ganesan, Karthik and Yan, Brian and Vu, Ngoc Thang and Black, Alan W and Watanabe, Shinji}, booktitle={ICASSP}, year={2022}, }\"]},\"13\":{\"h\":\"Singing Voice Synthesis\",\"t\":[\"@inproceedings{shi2022muskits, author={Shi, Jiatong and Guo, Shuai and Qian, Tao and Huo, Nan and Hayashi, Tomoki and Wu, Yuning and Xu, Frank and Chang, Xuankai and Li, Huazhe and Wu, Peter and Watanabe, Shinji and Jin, Qin}, title={Muskits: an End-to-End Music Processing Toolkit for Singing Voice Synthesis}, booktitle={Interspeech}, year={2022}, }\"]},\"14\":{\"h\":\"Unsupervised ASR\",\"t\":[\"@inproceedings{gao2022euro, title={EURO: ESPnet Unsupervised ASR Open-source Toolkit}, author={Gao, Dongji and Shi, Jiatong and Chuang, Shun-Po and Garcia, Leibny Paola and Lee, Hung-yi and Watanabe, Shinji and Khudanpur, Sanjeev}, booktitle={ICASSP}, year={2023} }\"]},\"15\":{\"h\":\"Speech summarization\",\"t\":[\"@inproceedings{sharma2023espnet, title={Espnet-Summ: Introducing a Novel Large Dataset, Toolkit, and a Cross-Corpora Evaluation of Speech Summarization Systems}, author={Sharma, Roshan and Chen, William and Kano, Takatomo and Sharma, Ruchira and Arora, Siddhant and Watanabe, Shinji and Ogawa, Atsunori and Delcroix, Marc and Singh, Rita and Raj, Bhiksha}, booktitle={ASRU}, year={2023}, }\"]},\"16\":{\"h\":\"Exporting models to ONNX\",\"t\":[\"@inproceedings{someki2022espnet, title={ESPnet-ONNX: Bridging a Gap Between Research and Production}, author={Someki, Masao and Higuchi, Yosuke and Hayashi, Tomoki and Watanabe, Shinji}, booktitle={APSIPA ASC}, year={2022}, }\"]},\"17\":{\"h\":\"\"},\"18\":{\"h\":\"Devcontainer\",\"t\":[\"The Visual Studio Code Dev Containers (VSCode) extension lets you use a container as a full-featured development environment. It allows you to open any folder inside (or mounted into) a container and take advantage of Visual Studio Code's full feature set. You can find detailed information about Dev Containers at https://code.visualstudio.com/docs/devcontainers/containers\"]},\"19\":{\"h\":\"Implemented Dev Containers\",\"t\":[\"You can find the availables dev containers at .devcontainer. To launch the current working space in a dev container, first you need to have the Dev Container extension installed in VSCode. You can launch the container from F1 > Dev Containers: Rebuild and Reopen Container.\",\"Select the preferred container from the displayed list:\",\"This will build and launch the container automatically, with Python installed and the ESPnet package already installed. You can launch tasks and edit the code without any issue.\"]},\"20\":{\"h\":\"\"},\"21\":{\"h\":\"Docker\"},\"22\":{\"h\":\"Execute in docker\",\"t\":[\"To work inside a docker container, execute run.sh located inside the docker directory. It will download the requested image and build a container to execute the main program specified by the following GPU, ASR example, and outside directory information, as follows:\",\"$ cd docker $ ./run.sh --docker-gpu 0 --docker-egs chime4/asr1 --docker-folders /export/corpora4/CHiME4/CHiME3 --dlayers 1 --ngpu 1\",\"Optionally, you can set the CUDA version with the arguments --docker-cuda respectively (default version set at CUDA=9.1). The docker container can be built based on the CUDA installed in your computer if you empty this arguments. By default, all GPU-based images are built with NCCL v2 and CUDNN v7. The arguments required for the docker configuration have a prefix \\\"--docker\\\" (e.g., --docker-gpu, --docker-egs, --docker-folders). run.sh accept all normal ESPnet arguments, which must be followed by these docker arguments. All docker containers are executed using the same user as your login account. If you want to run the docker in root access, add the flag --is-root to command line. In addition, you can pass any environment variable using --docker-env (e.g., --docker-env \\\"foo=path\\\")\"]},\"23\":{\"h\":\"ESPnet 2 Recipes\",\"t\":[\"To work with recipes of ESPnet 2, you will need to add the flag --is-egs2 to the command line:\",\"$ cd docker $ ./run.sh --docker-gpu 0 --is-egs2 --docker-egs an4/asr1 --ngpu1\",\"Remember to add the flag before the arguments you want to pass to the recipe run.sh file.\"]},\"24\":{\"h\":\"Using GPU-based containers\",\"t\":[\"You can run any bash script implemented in the egs folder using --docker-cmd:\",\"$ cd docker $ ./run.sh --docker-gpu 0 --docker-egs chime4/asr1 --docker-cmd foo.sh --arg_1 <arg_1> --arg_2 <arg_2>\",\"The arguments for the desired script should follow the docker arguments. run.sh is the default script to be executed.\",\"Multiple GPUs should be specified with the following options:\",\"$ cd docker $ ./run.sh --docker-gpu 0,1,2 --docker-egs chime5/asr1 --docker-folders /export/corpora4/CHiME5 --ngpu 3\",\"Note that all experimental files and results are created under the normal example directories (egs/<example>/).\",\"Multiple folders and environment variables should be specified with commas and without spaces:\",\"$ cd docker $ ./run.sh --docker-gpu 0 --docker-egs chime4/asr1 --docker-folders /export/corpus/CHiME4,/export/corpus/LDC/LDC93S6B,/export/corpus/LDC/LDC94S13B --docker-env \\\"CHIME4_CORPUS=/export/corpus/CHiME4/CHiME3,WSJ0_CORPUS=/export/corpus/LDC/LDC93S6B,WSJ1_CORPUS=/export/corpus/LDC/LDC94S13B\\\" --ngpu 1\",\"Remember that for some recipes, you first need to download the Corpus before running the experiments, such as CHiME, WSJ, and LDC corporas. You will need to set the directories where these were downloaded and replace them in the recipe (e.g.: CHIME4_CORPUS=/<dir_where_chime4_was_downloaded>/CHiME4/CHiME3)\"]},\"25\":{\"h\":\"Using CPU-based container\",\"t\":[\"You can train a model in CPU using the following command:\",\"$ cd docker $ ./run.sh --docker-gpu -1 --docker-egs an4/asr1 --ngpu 0\",\"The script will build a docker if your are using a user different from root user. To use containers with root access add the flag --is-root to the command line.\"]},\"26\":{\"h\":\"Local builds\",\"t\":[\"When building the docker container on a local machine, the espnet source is downloaded from the github espnet master branch. However, in some cases, \\\"local\\\" builds are preferable, that are built based on the source code from the local repository:\",\"After writing own modifications on the espnet code, the build environment, etc., and to test it in the docker container. Prebuilt docker containers do not import these.\",\"Reproducability: It is possible to go back to an espnet version at a certain commit and test the neural network with an older version of a library.\",\"The script build.sh supports making local builds for this purpose. During the docker build process, the local espnet source code is imported through a git archive based on git HEAD (the previous commit), and copied over within a file.\",\"For example, a local build that the base image from Docker Hub (espnet/espnet:runtime, based on Ubuntu 16), that already contains a kaldi installation, using Cuda 10.0:\",\"./build.sh local 10.0\",\"Also, docker images can also be built based on the Ubuntu version specified in prebuilt/runtime/Dockerfile (currently set to Ubuntu 18.04), in this example case using the cpu:\",\"./build.sh fully_local cpu\",\"Local container builds then are started by adding the flag --is-local when using run.sh, e.g., for the Cuda 10.0 image:\",\"$ ./run.sh --is-local --docker_cuda 10.0 --docker_gpu 0 ...\"]},\"27\":{\"h\":\"Deprecated\",\"t\":[\"Containers build on ubuntu-16.04 will be deprecated and no longer receive support. However, these container will remain in Docker Hub. To use containers with ubuntu 16.04, empty the flag --docker_os.\"]},\"28\":{\"h\":\"Tags\",\"t\":[\"Runtime: Base image for ESPnet. It includes libraries and Kaldi installation.\",\"CPU: Image to execute only in CPU.\",\"GPU: Image to execute examples with GPU support.\"]},\"29\":{\"h\":\"Ubuntu 18.04\",\"t\":[\"Pytorch 1.3.1, No warp-ctc:\",\"cuda10.1-cudnn7 (docker/prebuilt/gpu/10.1/cudnn7/Dockerfile)\",\"Pytorch 1.0.1, warp-ctc:\",\"cuda10.0-cudnn7 (docker/prebuilt/gpu/10.0/cudnn7/Dockerfile)\",\"cpu-u18 (docker/prebuilt/devel/Dockerfile)\"]},\"30\":{\"h\":\"ESPnet document generation\"},\"31\":{\"h\":\"Install\",\"t\":[\"We use sphinx to generate HTML documentation.\",\"# Clean conda env for docs $ cd <espnet_root> $ conda create -p ./envs python=3.10 $ conda activate ./envs # Requirements $ pip install -e \\\".[all]\\\" $ pip install -e \\\".[doc]\\\" $ conda install conda-forge::ffmpeg $ conda install conda-forge::nodejs==22.6.0 # (Optional requirement) To use flake8-docstrings $ pip install -U flake8-docstrings\",\"If you used the above clean conda environment, you have write your own . tools/activate_python.sh. The example will be:\",\"#!/usr/bin/env bash # You might check $CONDA_EXE to find the <conda_root> . <conda_root>/miniconda/etc/profile.d/conda.sh && conda activate <espnet_root>/envs\"]},\"32\":{\"h\":\"Style check using flake8-docstrings\",\"t\":[\"You can check that your docstring style is correct by ci/test_flake8.sh using flake8-docstrings. Note that many existing files have been added to the black list in the script to avoid this check by default. You can freely remove files that you wanna improve.\",\"# in ci/test_flake8.sh # you can improve poorly written docstrings from here flake8_black_list=\\\"\\\\ espnet/__init__.py espnet/asr/asr_mix_utils.py espnet/asr/asr_utils.py espnet/asr/chainer_backend/asr.py ... \\\" # --extend-ignore for wip files for flake8-docstrings flake8 --extend-ignore=D test utils doc ${flake8_black_list} # white list of files that should support flake8-docstrings flake8 espnet --exclude=${flake8_black_list//$'\\\\n'/,}\",\"DO NOT ADD NEW FILES TO THIS BLACK LIST!\"]},\"33\":{\"h\":\"Generate HTML\",\"t\":[\"You can generate and test the webpage using sphinx Makefile.\",\"$ cd <espnet_root> $ ./ci/doc.sh $ npm run docs:dev\"]},\"34\":{\"h\":\"Deploy\",\"t\":[\"When your PR is merged into master branch, our CI will automatically deploy your sphinx html into https://espnet.github.io/espnet/.\"]},\"35\":{\"h\":\"\"},\"36\":{\"h\":\"Usage\",\"t\":[\"If you're a new user, we suggest checking out the ESPnet2 tutorial as ESPnet1 is an older implementation. The majority of the development has now shifted to ESPnet2. Please be aware that certain information in this document may be outdated due to this shift.\"]},\"37\":{\"h\":\"Directory structure\",\"t\":[\"espnet/ # Python modules utils/ # Utility scripts of ESPnet test/ # Unit test test_utils/ # Unit test for executable scripts egs/ # The complete recipe for each corpora an4/ # AN4 is tiny corpus and can be obtained freely, so it might be suitable for tutorial asr1/ # ASR recipe - run.sh # Executable script - cmd.sh # To select the backend for job scheduler - path.sh # Setup script for environment variables - conf/ # Containing Configuration files - steps/ # The steps scripts from Kaldi - utils/ # The utils scripts from Kaldi tts1/ # TTS recipe ...\"]},\"38\":{\"h\":\"Execution of example scripts\",\"t\":[\"Move to an example directory under the egs directory. We prepare several major ASR benchmarks including WSJ, CHiME-4, and TED. The following directory is an example of performing ASR experiment with the CMU Census Database (AN4) recipe.\",\"$ cd egs/an4/asr1\",\"Once move to the directory, then, execute the following main script with a chainer backend:\",\"$ ./run.sh --backend chainer\",\"or execute the following main script with a pytorch backend:\",\"$ ./run.sh --backend pytorch\",\"With this main script, you can perform a full procedure of ASR experiments including\",\"Data download\",\"Data preparation (Kaldi style)\",\"Feature extraction (Kaldi style)\",\"Dictionary and JSON format data preparation\",\"Training based on chainer or pytorch.\",\"Recognition and scoring\"]},\"39\":{\"h\":\"Logging\",\"t\":[\"The training progress (loss and accuracy for training and validation data) can be monitored with the following command\",\"$ tail -f exp/${expdir}/train.log\",\"When we use ./run.sh --verbose 0 (--verbose 0 is default in most recipes), it gives you the following information\",\"epoch iteration main/loss main/loss_ctc main/loss_att validation/main/loss validation/main/loss_ctc validation/main/loss_att main/acc validation/main/acc elapsed_time eps : : 6 89700 63.7861 83.8041 43.768 0.731425 136184 1e-08 6 89800 71.5186 93.9897 49.0475 0.72843 136320 1e-08 6 89900 72.1616 94.3773 49.9459 0.730052 136473 1e-08 7 90000 64.2985 84.4583 44.1386 72.506 94.9823 50.0296 0.740617 0.72476 137936 1e-08 7 90100 81.6931 106.74 56.6462 0.733486 138049 1e-08 7 90200 74.6084 97.5268 51.6901 0.731593 138175 1e-08 total [#################.................................] 35.54% this epoch [#####.............................................] 10.84% 91300 iter, 7 epoch / 20 epochs 0.71428 iters/sec. Estimated time to finish: 2 days, 16:23:34.613215.\",\"Note that the an4 recipe uses --verbose 1 as default since this recipe is often used for a debugging purpose.\",\"In addition Tensorboard events are automatically logged in the tensorboard/${expname} folder. Therefore, when you install Tensorboard, you can easily compare several experiments by using\",\"$ tensorboard --logdir tensorboard\",\"and connecting to the given address (default : localhost:6006). This will provide the following information: Note that we would not include the installation of Tensorboard to simplify our installation process. Please install it manually (pip install tensorflow; pip install tensorboard) when you want to use Tensorboard.\"]},\"40\":{\"h\":\"Change options in run.sh\",\"t\":[\"We rely on utils/parse_options.sh to paser command line arguments in shell script and it's used in run.sh:\",\"e.g. If the script has ngpu option\",\"#!/usr/bin/env bash # run.sh ngpu=1 . utils/parse_options.sh echo ${ngpu}\",\"Then you can change the value as following:\",\"$ ./run.sh --ngpu 2 echo 2\"]},\"41\":{\"h\":\"Use of GPU\",\"t\":[\"Training: If you want to use GPUs in your experiment, please set --ngpu option in run.sh appropriately, e.g.,\",\" # use single gpu $ ./run.sh --ngpu 1 # use multi-gpu $ ./run.sh --ngpu 3 # if you want to specify gpus, set CUDA_VISIBLE_DEVICES as follows # (Note that if you use slurm, this specification is not needed) $ CUDA_VISIBLE_DEVICES=0,1,2 ./run.sh --ngpu 3 # use cpu $ ./run.sh --ngpu 0\",\"Default setup uses a single GPU (--ngpu 1).\",\"ASR decoding: ESPnet also supports the GPU-based decoding for fast recognition. \",\"Please manually remove the following lines in run.sh:\",\"#### use CPU for decoding ngpu=0\",\"Set 1 or more values for --batchsize option in asr_recog.py to enable GPU decoding\",\"And execute the script (e.g., run.sh --stage 5 --ngpu 1)\",\"You'll achieve significant speed improvement by using the GPU decoding\"]},\"42\":{\"h\":\"ESPnet1 Transducer\",\"t\":[\"Important: If you encounter any issue related to Transducer loss, please open an issue in our fork of warp-transducer.\",\"ESPnet supports models trained with Transducer loss, aka Transducer models. To train such model, the following should be set in the training config:\",\"criterion: loss model-module: \\\"espnet.nets.pytorch_backend.e2e_asr_transducer:E2E\\\"\"]},\"43\":{\"h\":\"Architecture\",\"t\":[\"Several Transducer architectures are currently available in ESPnet:\",\"RNN-Transducer (default, e.g.: etype: blstm with dtype: lstm)\",\"Custom-Transducer (e.g.: etype: custom and dtype: custom)\",\"Mixed Custom/RNN-Transducer (e.g: etype: custom with dtype: lstm)\",\"The architecture specification is separated for the encoder and decoder part, and defined by the user through, respectively, etype and dtype in the training config. If custom is specified for either, a customizable architecture will be used for the corresponding part. Otherwise, an RNN-based architecture will be selected.\",\"Here, the custom architecture is a unique feature of the Transducer model in ESPnet. It was made available to add some flexibility in the architecture definition and ease the reproduction of some SOTA Transducer models mixing different layers types or parameters within the same model part (encoder or decoder). As such, the architecture definition is different compared to the RNN architecture :\",\"Each block (or layer) of the custom architecture should be specified individually through enc-block-arch or/and dec-block-arch parameters:\",\"# e.g: Conv-Transformer encoder etype: custom enc-block-arch: - type: conv1d idim: 80 odim: 32 kernel_size: [3, 7] stride: [1, 2] - type: conv1d idim: 32 odim: 32 kernel_size: 3 stride: 2 - type: conv1d idim: 32 odim: 384 kernel_size: 3 stride: 1 - type: transformer d_hidden: 384 d_ff: 1536 heads: 4\",\"Different block types are allowed for the custom encoder (tdnn, conformer or transformer) and the custom decoder (causal-conv1d or transformer). Each one has a set of mandatory and optional parameters :\",\"# 1D convolution (TDNN) block - type: conv1d idim: [Input dimension. (int)] odim: [Output dimension. (int)] kernel_size: [Size of the context window. (int or tuple)] stride (optional): [Stride of the sliding blocks. (int or tuple, default = 1)] dilation (optional): [Parameter to control the stride of elements within the neighborhood. (int or tuple, default = 1)] groups (optional): [Number of blocked connections from input channels to output channels. (int, default = 1) bias (optional): [Whether to add a learnable bias to the output. (bool, default = True)] use-relu (optional): [Whether to use a ReLU activation after convolution. (bool, default = True)] use-batchnorm: [Whether to use batch normalization after convolution. (bool, default = False)] dropout-rate (optional): [Dropout-rate for TDNN block. (float, default = 0.0)] # Transformer - type: transformer d_hidden: [Input/output dimension of Transformer block. (int)] d_ff: [Hidden dimension of the Feed-forward module. (int)] heads: [Number of heads in multi-head attention. (int)] dropout-rate (optional): [Dropout-rate for Transformer block. (float, default = 0.0)] pos-dropout-rate (optional): [Dropout-rate for positional encoding module. (float, default = 0.0)] att-dropout-rate (optional): [Dropout-rate for attention module. (float, default = 0.0)] # Conformer - type: conformer d_hidden: [Input/output dimension of Conformer block (int)] d_ff: [Hidden dimension of the Feed-forward module. (int)] heads: [Number of heads in multi-head attention. (int)] macaron_style: [Whether to use macaron style. (bool)] use_conv_mod: [Whether to use convolutional module. (bool)] conv_mod_kernel (required if use_conv_mod = True): [Number of kernel in convolutional module. (int)] dropout-rate (optional): [Dropout-rate for Transformer block. (float, default = 0.0)] pos-dropout-rate (optional): [Dropout-rate for positional encoding module. (float, default = 0.0)] att-dropout-rate (optional): [Dropout-rate for attention module. (float, default = 0.0)] # Causal Conv1d - type: causal-conv1d idim: [Input dimension. (int)] odim: [Output dimension. (int)] kernel_size: [Size of the context window. (int)] stride (optional): [Stride of the sliding blocks. (int, default = 1)] dilation (optional): [Parameter to control the stride of elements within the neighborhood. (int, default = 1)] groups (optional): [Number of blocked connections from input channels to output channels. (int, default = 1) bias (optional): [Whether to add a learnable bias to the output. (bool, default = True)] use-relu (optional): [Whether to use a ReLU activation after convolution. (bool, default = True)] use-batchnorm: [Whether to use batch normalization after convolution. (bool, default = False)] dropout-rate (optional): [Dropout-rate for TDNN block. (float, default = 0.0)]\",\"The defined architecture can be repeated by specifying the total number of blocks/layers in the architecture through enc-block-repeat or/and dec-block-repeat parameters:\",\"# e.g.: 2x (Causal-Conv1d + Transformer) decoder dtype: transformer dec-block-arch: - type: causal-conv1d idim: 256 odim: 256 kernel_size: 5 - type: transformer d_hidden: 256 d_ff: 256 heads: 4 dropout-rate: 0.1 att-dropout-rate: 0.4 dec-block-repeat: 2\"]},\"44\":{\"h\":\"Multi-task learning\",\"t\":[\"We also support multi-task learning with various auxiliary losses, such as: CTC, cross-entropy w/ label-smoothing (LM loss), auxiliary Transducer, and symmetric KL divergence. The four losses can be simultaneously trained with main Transducer loss to jointly optimize the total loss defined as:\",\"augmented Transducer training\",\"where the losses are respectively, in order: The main Transducer loss, the CTC loss, the auxiliary Transducer loss, the symmetric KL divergence loss, and the LM loss. Lambda values define their respective contribution to the overall loss. Additionally, each loss can be independently selected or omitted depending on the task.\",\"Each loss can be defined in the training config alongside its specific options, such as follow:\",\"# Transducer loss (L1) transducer-loss-weight: [Weight of the main Transducer loss (float)] # CTC loss (L2) use-ctc-loss: True ctc-loss-weight (optional): [Weight of the CTC loss. (float, default = 0.5)] ctc-loss-dropout-rate (optional): [Dropout rate for encoder output representation. (float, default = 0.0)] # Auxiliary Transducer loss (L3) use-aux-transducer-loss: True aux-transducer-loss-weight (optional): [Weight of the auxiliary Transducer loss. (float, default = 0.4)] aux-transducer-loss-enc-output-layers (required if use-aux-transducer-loss = True): [List of intermediate encoder layer IDs to compute auxiliary Transducer loss(es). (list)] aux-transducer-loss-mlp-dim (optional): [Hidden dimension for the MLP network. (int, default = 320)] aux-transducer-loss-mlp-dropout-rate: [Dropout rate for the MLP network. (float, default = 0.0)] # Symmetric KL divergence loss (L4) # Note: It can be only used in addition to the auxiliary Transducer loss. use-symm-kl-div-loss: True symm-kl-div-loss-weight (optional): [Weight of the symmetric KL divergence loss. (float, default = 0.2)] # LM loss (L5) use-lm-loss: True lm-loss-weight (optional): [Weight of the LM loss. (float, default = 0.2)] lm-loss-smoothing-rate: [Smoothing rate for LM loss. If > 0, label smoothing is enabled. (float, default = 0.0)]\"]},\"45\":{\"h\":\"Inference\",\"t\":[\"Various decoding algorithms are also available for Transducer by setting beam-size and search-type parameters in decode config.\",\"Greedy search constrained to one emission by timestep (beam-size: 1).\",\"Beam search algorithm without prefix search (beam-size: >1 and search-type: default).\",\"Time Synchronous Decoding [Saon et al., 2020] (beam-size: >1 and search-type: tsd).\",\"Alignment-Length Synchronous Decoding [Saon et al., 2020] (beam-size: >1 and search-type: alsd).\",\"N-step Constrained beam search modified from [Kim et al., 2020] (beam-size: >1 and search-type: default).\",\"modified Adaptive Expansion Search, based on [Kim et al., 2021] and NSC (beam-size: >1 and search-type: maes).\",\"The algorithms share two parameters to control beam size (beam-size) and final hypotheses normalization (score-norm-transducer). The specific parameters for each algorithm are:\",\"# Default beam search search-type: default # Time-synchronous decoding search-type: tsd max-sym-exp: [Number of maximum symbol expansions at each time step (int)] # Alignement-length decoding search-type: alsd u-max: [Maximum output sequence length (int)] # N-step Constrained beam search search-type: nsc nstep: [Number of maximum expansion steps at each time step (int)] # nstep = max-sym-exp + 1 (blank) prefix-alpha: [Maximum prefix length in prefix search (int)] # modified Adaptive Expansion Search search-type: maes nstep: [Number of maximum expansion steps at each time step (int, > 1)] prefix-alpha: [Maximum prefix length in prefix search (int)] expansion-gamma: [Number of additional candidates in expanded hypotheses selection (int)] expansion-beta: [Allowed logp difference for prune-by-value method (float, > 0)]\",\"Except for the default algorithm, the described parameters are used to control the performance and decoding speed. The optimal values for each parameter are task-dependent; a high value will typically increase decoding time to focus on performance while a low value will improve decoding time at the expense of performance.\"]},\"46\":{\"h\":\"Additional notes\",\"t\":[\"Similarly to training with CTC, Transducer does not output the validation accuracy. Thus, the optimum model is selected with its loss value (i.e., --recog_model model.loss.best).\",\"There are several differences between MTL and Transducer training/decoding options. The users should refer to espnet/espnet/nets/pytorch_backend/e2e_asr_transducer.py for an overview and espnet/espnet/nets/pytorch_backend/transducer/arguments for all possible arguments.\",\"FastEmit regularization [Yu et al., 2021] is available through --fastemit-lambda training parameter (default = 0.0).\",\"RNN-decoder pre-initialization using an LM is supported. Note that regular decoder keys are expected. The LM state dict keys (predictor.*) will be renamed according to AM state dict keys (dec.*).\",\"Transformer-decoder pre-initialization using a Transformer LM is not supported yet.\"]},\"47\":{\"h\":\"Changing the training configuration\",\"t\":[\"The default configurations for training and decoding are written in conf/train.yaml and conf/decode.yaml respectively. It can be overwritten by specific arguments: e.g.\",\"# e.g. asr_train.py --config conf/train.yaml --batch-size 24 # e.g.--config2 and --config3 are also provided and the latter option can overwrite the former. asr_train.py --config conf/train.yaml --config2 conf/new.yaml\",\"In this way, you need to edit run.sh and it might be inconvenient sometimes. Instead of giving arguments directly, we recommend you to modify the yaml file and give it to run.sh:\",\"# e.g. ./run.sh --train-config conf/train_modified.yaml # e.g. ./run.sh --train-config conf/train_modified.yaml --decode-config conf/decode_modified.yaml\",\"We also provide a utility to generate a yaml file from the input yaml file:\",\"# e.g. You can give any parameters as '-a key=value' and '-a' is repeatable. # This generates new file at 'conf/train_batch-size24_epochs10.yaml' ./run.sh --train-config $(change_yaml.py conf/train.yaml -a batch-size=24 -a epochs=10) # e.g. '-o' option specifies the output file name instead of auto named file. ./run.sh --train-config $(change_yaml.py conf/train.yaml -o conf/train2.yaml -a batch-size=24)\"]},\"48\":{\"h\":\"How to set minibatch\",\"t\":[\"From espnet v0.4.0, we have three options in --batch-count to specify minibatch size (see espnet.utils.batchfy for implementation);\",\"--batch-count seq --batch-seqs 32 --batch-seq-maxlen-in 800 --batch-seq-maxlen-out 150.\",\"This option is compatible to the old setting before v0.4.0. This counts the minibatch size as the number of sequences and reduces the size when the maximum length of the input or output sequences is greater than 800 or 150, respectively.\",\"--batch-count bin --batch-bins 100000.\",\"This creates the minibatch that has the maximum number of bins under 100 in the padded input/output minibatch tensor (i.e., max(ilen) * idim + max(olen) * odim). Basically, this option makes training iteration faster than --batch-count seq. If you already has the best --batch-seqs x config, try --batch-bins $((x * (mean(ilen) * idim + mean(olen) * odim))).\",\"--batch-count frame --batch-frames-in 800 --batch-frames-out 100 --batch-frames-inout 900.\",\"This creates the minibatch that has the maximum number of input, output and input+output frames under 800, 100 and 900, respectively. You can set one of --batch-frames-xxx partially. Like --batch-bins, this option makes training iteration faster than --batch-count seq. If you already has the best --batch-seqs x config, try --batch-frames-in $((x * (mean(ilen) * idim)) --batch-frames-out $((x * mean(olen) * odim)).\"]},\"49\":{\"h\":\"How to use finetuning\",\"t\":[\"ESPnet currently supports two finetuning operations: transfer learning and freezing. We expect the user to define the following options in its main training config (e.g.: conf/train*.yaml). If needed, they can be directly passed to (asr|tts|vc)_train.py by adding the prefix -- to the options.\"]},\"50\":{\"h\":\"Transfer learning\",\"t\":[\"Transfer learning option is split between encoder initialization (enc-init) and decoder initialization (dec-init). However, the same model can be specified for both options.\",\"Each option takes a snapshot path (e.g.: [espnet_model_path]/results/snapshot.ep.1) or model path (e.g.: [espnet_model_path]/results/model.loss.best) as argument.\",\"Additionally, a list of encoder and decoder modules (separated by a comma) can also be specified to control the modules to transfer with the options enc-init-mods and dec-init-mods.\",\"For each specified module, we only expect a partial match with the start of the target model module name. Thus, multiple modules can be specified with the same key if they share a common prefix.\",\"Mandatory: enc-init: /home/usr/espnet/egs/vivos/asr1/exp/train_nodev_pytorch_train/results/model.loss.best -> specify a pre-trained model on VIVOS for transfer learning. Example 1: enc-init-mods: 'enc.' -> transfer all encoder parameters. Example 2: enc-init-mods: 'enc.embed.,enc.0.' -> transfer encoder embedding layer and first layer parameters.\"]},\"51\":{\"h\":\"Freezing\",\"t\":[\"Freezing option can be enabled with freeze-mods, (freeze_param in espnet2).\",\"The option take a list of model modules (separated by a comma) as argument. As previously, we do not expect a complete match for the specified modules.\",\"Example 1: freeze-mods: 'enc.embed.' -> freeze encoder embedding layer parameters. Example 2: freeze-mods: 'dec.embed,dec.0.' -> freeze decoder embedding layer and first layer parameters. Example 3 (espnet2): freeze_param: 'encoder.embed' -> freeze encoder embedding layer parameters.\"]},\"52\":{\"h\":\"Important notes\",\"t\":[\"Given a pre-trained source model, the modules specified for transfer learning are expected to have the same parameters (i.e.: layers and units) as the target model modules.\",\"We also support initialization with a pre-trained RNN LM for the RNN-Transducer decoder.\",\"RNN models use different key names for encoder and decoder parts compared to Transformer, Conformer or Custom models: \",\"RNN model use enc. for encoder part and dec. for decoder part.\",\"Transformer/Conformer/Custom model use encoder. for encoder part and decoder. for decoder part.\"]},\"53\":{\"h\":\"Chainer and Pytorch backends\",\"t\":[\"Chainer\",\"Pytorch\",\"Performance\",\"◎\",\"◎\",\"Speed\",\"○\",\"◎\",\"Multi-GPU\",\"supported\",\"supported\",\"VGG-like encoder\",\"supported\",\"supported\",\"Transformer\",\"supported\",\"supported\",\"RNNLM integration\",\"supported\",\"supported\",\"#Attention types\",\"3 (no attention, dot, location)\",\"12 including variants of multihead\",\"TTS recipe support\",\"no support\",\"supported\"]},\"54\":{\"h\":\"Distributed training\",\"t\":[\"ESPnet2 provides some kinds of data-parallel distributed training.\",\"DP/DDP\",\"Single/Multi host\",\"Option\",\"Multi-processing with single host\",\"DistributedDataParallel\",\"Single\",\"--ngpu N-GPU --multiprocessing_distributed true\",\"Multi-threading with single host\",\"DataParallel\",\"Single\",\"--ngpu N-GPU --multiprocessing_distributed false\",\"Multi-processing with N-HOST jobs with N-GPU for each host (=N-HOSTxN-GPU nodes)\",\"DistributedDataParallel\",\"Multi\",\"--dist_world_size N-HOST --ngpu N-GPU --multiprocessing_distributed true\",\"Multi-threading with N-HOST jobs with N-GPU for each host (=N-HOSTxN-GPU nodes)\",\"DistributedDataParallel\",\"Multi\",\"--dist_world_size N-HOST --ngpu N-GPU --multiprocessing_distributed false\",\"N-NODE jobs with 1-GPU for each node\",\"DistributedDataParallel\",\"Single/Multi\",\"--dist_world_size N-NODE --ngpu 1\"]},\"55\":{\"h\":\"Examples\",\"t\":[\"Note: The behavior of batch size in ESPnet2 during multi-GPU training is different from that in ESPnet1. In ESPnet2, the total batch size is not changed regardless of the number of GPUs. Therefore, you need to manually increase the batch size if you increase the number of GPUs. Please refer to this doc for more information.\"]},\"56\":{\"h\":\"Single node with 4GPUs with distributed mode\",\"t\":[\"% python -m espnet2.bin.asr_train --ngpu 4 --multiprocessing_distributed true\",\"You can disable distributed mode and switch to threading based data parallel as follows:\",\"% python -m espnet2.bin.asr_train --ngpu 4 --multiprocessing_distributed false\",\"If you meet some errors with distributed mode, please try single gpu mode or multi-GPUs with --multiprocessing_distributed false before reporting the issue.\"]},\"57\":{\"h\":\"To enable sharded training\",\"t\":[\"We supports sharded training provided by fairscale\",\"% python -m espnet2.bin.asr_train --ngpu 4 --multiprocessing_distributed true --sharded_ddp true\",\"Note that the other features of fairscale are not supported now.\"]},\"58\":{\"h\":\"2Host and 2GPUs for each host with multiprocessing distributed mode\",\"t\":[\"Note that multiprocessing distributed mode assumes the same number of GPUs for each node.\",\"(host1) % python -m espnet2.bin.asr_train \\\\ --multiprocessing_distributed true \\\\ --ngpu 2 \\\\ --dist_rank 0 \\\\ --dist_world_size 2 \\\\ --dist_master_addr host1 \\\\ --dist_master_port <any-free-port> (host2) % python -m espnet2.bin.asr_train \\\\ --multiprocessing_distributed true \\\\ --ngpu 2 \\\\ --dist_rank 1 \\\\ --dist_world_size 2 \\\\ --dist_master_addr host1 \\\\ --dist_master_port <any-free-port>\"]},\"59\":{\"h\":\"RANK and WORLD_SIZE\",\"t\":[\"--dist_rank and --dist_world_size indicate RANK and WORLD_SIZE in terms of MPI; i.e., they indicate the id of each processe and the number of processes respectively. They can be also specified by the environment variables ${RANK} and ${WORLD_SIZE}.\"]},\"60\":{\"h\":\"About init method\",\"t\":[\"See: https://pytorch.org/docs/stable/distributed.html#tcp-initialization\",\"There are two ways to initialize and these methods can be interchanged in all examples.\",\"TCP initialization\",\"# These three are equivalent: --dist_master_addr <rank0-host> --dist_master_port <any-free-port> --dist_init_method \\\"tcp://<rank0-host>:<any-free-port>\\\" export MASTER_ADDR=<rank0-host> MASTER_PORT=<any-free-port>\",\"Shared file system initialization\",\"--dist_init_method \\\"file:///nfs/some/where/filename\\\"\",\"This initialization might be failed if the previous file is existing. I recommend you to use a random file name to avoid to reuse it. e.g.\",\"--dist_init_method \\\"file://$(pwd)/.dist_init_$(openssl rand -base64 12)\\\"\"]},\"61\":{\"h\":\"2Hosts which have 2GPUs and 1GPU respectively\",\"t\":[\"(host1) % python -m espnet2.bin.asr_train \\\\ --ngpu 1 \\\\ --multiprocessing_distributed false \\\\ --dist_rank 0 \\\\ --dist_world_size 3 \\\\ --dist_master_addr host1 \\\\ --dist_master_port <any-free-port> (host1) % python -m espnet2.bin.asr_train \\\\ --ngpu 1 \\\\ --multiprocessing_distributed false \\\\ --dist_rank 1 \\\\ --dist_world_size 3 \\\\ --dist_master_addr host1 \\\\ --dist_master_port <any-free-port> (host2) % python -m espnet2.bin.asr_train \\\\ --ngpu 1 \\\\ --multiprocessing_distributed false \\\\ --dist_rank 2 \\\\ --dist_world_size 3 \\\\ --dist_master_addr host1 \\\\ --dist_master_port <any-free-port>\"]},\"62\":{\"h\":\"2Hosts and 2GPUs for each node using with multiprocessing distributed\",\"t\":[\" % srun -c2 -N2 --gres gpu:2 \\\\ python -m espnet2.bin.asr_train --ngpu 2 --multiprocessing_distributed true \\\\ --dist_launcher slurm \\\\ --dist_init_method \\\"file://$(pwd)/.dist_init_$(openssl rand -base64 12)\\\"\",\"I recommend shared-file initialization in this case because the host will be determined after submitting the job, therefore we can't tell the free port number before.\"]},\"63\":{\"h\":\"5GPUs with 3nodes using\",\"t\":[\"(Not tested)\",\"% srun -n5 -N3 --gpus-per-task 1 \\\\ python -m espnet2.bin.asr_train --ngpu 1 --multiprocessing_distributed false \\\\ --dist_launcher slurm \\\\ --dist_init_method \\\"file://$(pwd)/.dist_init_$(openssl rand -base64 12)\\\"\"]},\"64\":{\"h\":\"2Hosts and 2GPUs for each node using with multiprocessing distributed\",\"t\":[\" % mpirun -np 2 -host host1,host2 \\\\ python -m espnet2.bin.asr_train --ngpu 2 --multiprocessing_distributed true \\\\ --dist_launcher mpi \\\\ --dist_init_method \\\"file://$(pwd)/.dist_init_$(openssl rand -base64 12)\\\"\"]},\"65\":{\"h\":\"\",\"t\":[\"Coming soon...\"]},\"66\":{\"h\":\"Troubleshooting for NCCL with Ethernet case\",\"t\":[\"NCCL WARN Connect to 192.168.1.51<51890> failed : No route to host\",\"Reason: Firewall?\",\"Need to free all ports?\",\"NCCL INFO Call to connect returned Connection refused, retrying\",\"Reason: NIC is found, but connection is refused?\",\"Set NCCL_SOCKET_IFNAME=<appropriate_interface>\",\"NCCL WARN Bootstrap : no socket interface found\",\"Reason: Any NIC are not found . (Maybe NCCL_SOCKET_IFNAME is incorrect)\",\"Set NCCL_SOCKET_IFNAME=<appropriate_interface>.\",\"NCCL WARN peer mapping resources exhausted\",\"???\",\"https://devtalk.nvidia.com/default/topic/970010/cuda-programming-and-performance/cuda-peer-resources-error-when-running-on-more-than-8-k80s-aws-p2-16xlarge-/post/4994583/#4994583\"]},\"67\":{\"h\":\"The rules of\",\"t\":[\"See: https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html\",\"The default value is NCCL_SOCKET_IFNAME=^lo,docker.\",\"Support two syntax: white list or black list\",\"White list e.g.: NCCL_SOCKET_IFNAME=eth,em\",\"It's enough to specify the prefix only. You don't need to set it as eth0.\",\"Blacklist e.g.: ^virbr,lo,docker.\",\"If multiple network interfaces are found in your environment, the first is selected. \",\"You can check your environment by ifconfig for example. https://www.cyberciti.biz/faq/linux-list-network-interfaces-names-command/\",\"Note that lo is the first normally, so lo must be filtered.\",\"My recommended setting for a non-virtual environment\",\"NCCL_SOCKET_IFNAME=en,eth,em,bond\",\"Or, NCCL_SOCKET_IFNAME=^lo,docker,virbr,vmnet,vboxnet,wl,ww,ppp\",\"The prefix of network interface name\",\"Note\",\"lo\",\"Loopback.\",\"eth\",\"Ethernet. Classically used.\",\"em\",\"Ethernet. Dell machine?\",\"en\",\"Ethernet (Used in recent Linux. e.g CentOS7)\",\"wlan\",\"Wireless\",\"wl\",\"Wireless LAN (Used in recent Linux)\",\"ww\",\"Wireless wan (Used in recent Linux)\",\"ib\",\"IP over IB\",\"bond\",\"Bonding of multiple ethernets\",\"virbr\",\"Virtual bridge\",\"docker,vmnet,vboxnet\",\"Virtual machine\",\"ppp\",\"Point to point\"]},\"68\":{\"h\":\"Converting audio file formats using format_wav_scp.py\",\"t\":[\"The format_wav_scp.py is an utility to convert the audio format of the files specified wav.scp and the format_wav_scp.sh is a shell script wrapping format_wav_scp.py. In the typical case, in the stage3 of the template recipe, format_wav_scp.sh is used to convert the audio file format of your original corpus to the audio format which you actually want to feed to the DNN model.\",\"format_wav_scp.py and format_wav_scp.sh has same function of generation wav.scp from wav.scp, but format_wav_scp.sh is different in that it has the capability of parallel processing.\",\"wav.scp -> [format_wav_scp.py] -> wav.scp wav.scp -> [format_wav_scp.sh] -> wav.scp\",\"Note that format_wav_scp.py dumps audio files with linear PCM with sint16 regardless the input audio format.\"]},\"69\":{\"h\":\"Quick usage\",\"t\":[\"At the first, you need to prepare a text file named as wav.scp:\",\"ID_a /some_where/a.wav ID_b /some_where2/b.wav ...\",\"ID_aand ID_b are the IDs which you can name arbitrarily to specify audio files. Note that we don't assume any directory stuctures for the audio files.\",\"# Please change directory before using our shell scripts cd egs2/some_corpus/some_task cmd=utils/run.pl nj=10 # Number of parallel jobs audio_format=flac # The audio codec of output files fs=16k # The sampling frequency of output files ref_channels=0 # If the input data has multiple channels and you want to use only a single channel in the file (please spicify the channel with 0-based number) ./scripts/audio/format_wav_scp.sh --nj \\\"${nj}\\\" --cmd \\\"${cmd}\\\" --audio_format \\\"${audio_format}\\\" --fs \\\"${fs}\\\" --ref_channels \\\"${ref_channels}\\\" somewhere/wav.scp output_dir # Then, you can find output_dir/wav.scp\",\"See also:\",\"About wav.scp: https://github.com/espnet/data_example\",\"About cmd: Using job scheduling system\"]},\"70\":{\"h\":\"Why is audio file formatting necessary?\",\"t\":[\"The audio data included in the corpus obtained from the source website are distributed in various audio file formats, i.e., the audio codec (wav of linear PCM, flac, mp3, DSD, u-law, a-lawor etc.), the sampling frequency (48khz, 44.1khz, 16khz, 8khz, or etc.), the bit depth (uint8, sint16, sint32, float20, float32 or etc.), the number of channels (monaural, stereo, or more than 2ch), the byter order(little endian or big endian).\",\"When you try to develop a new recipe with a corpus that is not yet prepared in our recipes, of course, you can also try to use the audio data as they are without any formatting. However, in a typical case, the configuration of our DNN model may assume the specific audio format, especially regarding the sampling frequency and the data precision. If you are conservative with your new recipe, we recommend converting them to the original recipe's audio format. For example, 16khz and sint16 audio is typically used in our ASR recipes.\"]},\"71\":{\"h\":\"The audio file formats supported in ESPnet2\",\"t\":[\"ESPnet adopts python soundifile for data loading, and, thus the supported audio codecs depend on libsndfile.\",\"You can check the supported audio codecs of soundfile with the following command:\",\"import soundfile print(soundfile.available_formats())\",\"Note that the wav.scp of Kaldi originally requires that the audio format is wav with pcm_s16le type, but wav.scp of ESPnet2 can handle all audio formats supported by soundfile. e.g. You can use flac format in wav.scp for the input/output of format_wav_scp.py.\",\"Depending on the situation, you may choose one of the following codecs:\",\"Codec\",\"Compression\",\"Maximum channnels\",\"Maximum sampling frequency\",\"Note\",\"wav (Microsoft wav with linear pcm)\",\"No\",\"1024\",\"-\",\"flac\",\"Lossless\",\"8\",\"192khz\",\"mp3\",\"Lossy\",\"2\",\"48khz\",\"The patent of MP3 has expired\",\"ogg (Vorbis)\",\"Lossy\",\"255\",\"192khz\",\"Segmentation fault happens\",\"By default, we select flac because flac can convert linear pcm files with compression rate of ~55 % without data loss. flac is helpful to reduce the IO load, especially, when training with a large amount of corpus. If you would like to change it to the other format, please use --audio_format option for run.sh.\",\"cd egs2/some_corpus/some_task ./run.sh --audio_format mp3\",\"Note that if the audio files in your corpus are disributed with lossy audio codec, such as MP3, it's better to keep the file format to avoid the duplication of the full corpus with the uncompressed format. If the input audio format type is exactly same as the output format, format_wav_scp.py avoid the gengeration of the output files and reuse the input files.\"]},\"72\":{\"h\":\"Use case\"},\"73\":{\"h\":\"Case1: Extract segmentations with long recoding\",\"t\":[\"Create wav.scp and segments with the format of The format is <utterance_id> <wav_id> <start_time> <end_time> (second unit).\",\"wav.scp:\",\"record_a a.wav ...\",\"segments:\",\"segment_a record_a 0.98 11.56 segment_a record_a 12.34 15.43 ...\",\"Then, you can extract the segments with:\",\"./scripts/audio/format_wav_scp.sh --segments segments wav.scp output_dir\"]},\"74\":{\"h\":\"Case2: Extract audio data from video codec / Use non supported format by soundfile\",\"t\":[\"ffmpeg is required. Create wav.scp as following:\",\"ID_a ffmpeg -i \\\"ID_a.mp4\\\" -f wav -af pan=\\\"1c|c0=c0\\\" -acodec pcm_s16le - | ID_b ffmpeg -i \\\"ID_b.mp4\\\" -f wav -af pan=\\\"1c|c0=c0\\\" -acodec pcm_s16le - | ...\",\"Note: -af pan is pan filter. \",\"<num>c| specifies <num> of output channels\",\"|c<out-channel>=c<in-channel> assigns <in-channel>th channel of input stream into <out-channel>th channel of output stream\",\"Caution: -map_channel option is deprecated and will be removed.\"]},\"75\":{\"h\":\"Case3: Convert NIST Sphere files to wav\",\"t\":[\"sph2pipe is required. Create wav.scp as following:\",\"ID_a sph2pipe -f wav -p -c 1 ID_a.sph | ID_b sph2pipe -f wav -p -c 1 ID_b.sph | ...\"]},\"76\":{\"h\":\"Case4: Using a mechanism for multi channels inputs\",\"t\":[\"If you are going to generate multi channels audio file from monaural audio files, create the following wav.scp:\",\"ID_a a1.wav a2.wav ...\",\"and run the following commands:\",\"./scripts/audio/format_wav_scp.sh --multi_columns_input true wav.scp output_dir\",\"Conversely, if you and going to monaural audio files from multi channels audio files\",\"./scripts/audio/format_wav_scp.sh --multi_columns_output true wav.scp output_dir\",\"Then, you can get wav.scp like the following file:\",\"ID_a output_dir/IDa-CH0.wav output_dir/ID_a-CH1.wav ...\"]},\"77\":{\"h\":\"Task class and data input system for training\"},\"78\":{\"h\":\"Task class\",\"t\":[\"In ESpnet1, we have too many duplicated python modules. One of the big purposes of ESPnet2 is to provide a common interface and enable us to focus more on the unique parts of each task.\",\"Task class is a common system to build training tools for each task, ASR, TTS, LM, etc. inspired by Fairseq Task idea. To build your task, only you have to do is just inheriting AbsTask class:\",\"from espnet2.tasks.abs_task import AbsTask from espnet2.train.abs_espnet_model import AbsESPnetModel class NewModel(ESPnetModel): def forward(self, input, target): (...) # loss: The loss of the task. Must be a scalar value. # stats: A dict object, used for logging and validation criterion # weight: A scalar value that is used for normalization of loss and stats values among each mini-batches. # In many cases, this value should be equal to the mini-batch-size return loss, stats, weight class NewTask(AbsTask): @classmethod def add_task_arguments(cls, parser): parser.add_arguments(...) (...) @classmethod def build_collate_fn(cls, args: argparse.Namespace) (...) @classmethod def build_preprocess_fn(cls, args, train): (...) @classmethod def required_data_names(cls, inference: bool = False): (...) @classmethod def optional_data_names(cls, inference: bool = False): (...) @classmethod def build_model(cls, args): return NewModel(...) if __name__ == \\\"__main__\\\": # Start training NewTask.main()\"]},\"79\":{\"h\":\"Data input system\",\"t\":[\"Espnet2 also provides a command line interface to describe the training corpus. On the contrary, unlike fairseq or training system such as pytorch-lightning, our Task class doesn't have an interface for building the dataset explicitly. This is because we aim at the task related to speech/text only, so we don't need such general system so far.\",\"The following is an example of the command lint arguments:\",\"python -m espnet2.bin.asr_train \\\\ --train_data_path_and_name_and_type=/some/path/tr/wav.scp,speech,sound \\\\ --train_data_path_and_name_and_type=/some/path/tr/token_int,text,text_int \\\\ --valid_data_path_and_name_and_type=/some/path/dev/wav.scp,speech,sound \\\\ --valid_data_path_and_name_and_type=/some/path/dev/token_int,text,text_int\",\"First of all, our mini-batch is always a dict object:\",\"# In training iteration for batch in iterator: # e.g. batch = {\\\"speech\\\": ..., \\\"text\\\": ...} # Forward model(**batch)\",\"Where the model is same as the model built by Task.build_model().\",\"You can flexibly construct this mini-batch object using --*_data_path_and_name_and_type. --*_data_path_and_name_and_type can be repeated as you need and each --*_data_path_and_name_and_type corresponds to an element in the mini-batch. Also, keep in mind that there is no distinction between input and target data.\",\"The argument of --train_data_path_and_name_and_type should be given as three values separated by commas, like <file-path>,<key-name>,<file-format>.\",\"key-name specify the key of dict\",\"file-path is a file/directory path for the data source.\",\"file-format indicates the format of file specified by file-path. e.g. sound, kaldi_ark, or etc.\"]},\"80\":{\"h\":\"file\",\"t\":[\"You can show the supported file format using --help option.\",\"python -m espnet2.bin.asr_train --help\",\"Almost all formats are referred as scp file according to Kaldi-ASR. scp is just a text file which has two columns for each line: The first indicates the sample id and the second is some value. e.g. file path, transcription, a sequence of numbers.\",\"format=npy\",\"sample_id_a /some/path/a.npy sample_id_b /some/path/b.npy\",\"format=sound\",\"sample_id_a /some/path/a.flac sample_id_b /some/path/a.wav\",\"format=kaldi_ark\",\"sample_id_a /some/path/a.ark:1234 sample_id_b /some/path/a.ark:5678\",\"format=text_int\",\"sample_id_a 10 2 4 4 sample_id_b 3 2 0 1 6 2\",\"format=text\",\"sample_id_a hello world sample_id_b It is rainy today\"]},\"81\":{\"h\":\"and\",\"t\":[\"Though an arbitrary dictionary can be created by this system, each task assumes that the specific key is given for a specific purpose. e.g. ASR Task requires speech and text keys and each value is used for input data and target data respectively. See again the methods of Task class: required_data_names() and optional_data_names().\",\"class NewTask(AbsTask): @classmethod def required_data_names(cls, inference: bool = False): if not inference: retval = (\\\"input\\\", \\\"target\\\") else: retval = (\\\"input\\\",) return retval @classmethod def optional_data_names(cls, inference: bool = False): retval = (\\\"auxially_feature\\\",) return retval\",\"required_data_names() determines the mandatory data names and optional_data_names() gives optional data. It means that the other names are allowed to given by command line arguments.\",\"# The following is the expected argument python -m new_task \\\\ --train_data_path_and_name_and_type=filepath,input,sometype \\\\ --train_data_path_and_name_and_type=filepath,target,sometype \\\\ --train_data_path_and_name_and_type=filepath,auxially_feature,sometype # The following raises an error python -m new_task \\\\ --train_data_path_and_name_and_type=filepath,unknown,sometype\",\"The intention of this system is just an assertion check, so if feel unnecessary, you can turn off this checking with --allow_variable_data_keys true.\",\"# Ignore assertion checking for data names python -m new_task \\\\ --train_data_path_and_name_and_type=filepath,unknown_name,sometype \\\\ --allow_variable_data_keys true\"]},\"82\":{\"h\":\"Customize for PyTorch data loader\",\"t\":[\"Task class has a method to customize collate_fn:\",\"class NewTask(AbsTask): @classmethod def build_collate_fn(cls, args: argparse.Namespace): ...\",\"collate_fn is an argument of torch.utils.data.DataLoader and it can modify the data which is received from data-loader. e.g.:\",\"def collate_fn(data): # data is a list of the return value of Dataset class: modified_data = (...touch data) return modified_data from torch.utils.data import DataLoader data_loader = DataLoader(dataset, collate_fn=collate_fn) for modified_data in data_loader: ...\",\"The type of argument is determined by the input dataset class and our dataset is always espnet2.train.dataset.ESPnetDataset, which the return value is a tuple of sample id and a dict of tensor,\",\"batch = (\\\"sample_id\\\", {\\\"speech\\\": tensor, \\\"text\\\": tensor})\",\"Therefore, the type is a list of dict of tensor.\",\"data = [ (\\\"sample_id\\\", {\\\"speech\\\": tensor, \\\"text\\\": tensor}), (\\\"sample_id2\\\", {\\\"speech\\\": tensor, \\\"text\\\": tensor}), ... ]\",\"The return type of collate_fn is supposed to be a tuple of list and a dict of tensor in espnet2, so the collate_fn for Task must transform the data type to it.\",\"for ids, batch in data_loader: model(**batch)\",\"We provide common collate_fn and this function can support many cases, so you might not need to customize it. This collate_fn is aware of variable sequence features for seq2seq task:\",\"The first axis of the sequence tensor from dataset must be length axis: e.g. (Length, Dim), (Length, Dim, Dim2), or (Length, ...)\",\"It's not necessary to make the lengths of each sample unified and they are stacked with zero-padding. \",\"The value of padding can be changed.\",\"from espnet2.train.collate_fn import CommonCollateFn @classmethod def build_collate_fn(cls, args): # float_pad_value is used for float-tensor and int_pad_value is used for int-tensor return CommonCollateFn(float_pad_value=0.0, int_pad_value=-1)\",\"Tensors which represent the length of each samples are also appended\",\"batch = {\\\"speech\\\": ..., \\\"speech_lengths\\\": ..., \\\"text\\\": ..., \\\"text_lengths\\\": ...}\",\"If the feature is not sequential data, this behavior can be disabled.\",\"python -m new_task --train_data_path_and_name_and_type=filepath,foo,npy\",\"@classmethod def build_collate_fn(cls, args): return CommonCollateFn(not_sequence=[\\\"foo\\\"])\"]},\"83\":{\"h\":\"Change the configuration for training\"},\"84\":{\"h\":\"Show usage\",\"t\":[\"There are two ways to show the command line options: --help and --print_config\",\"# Show the command line option python -m espnet2.bin.asr_train --help # Show the all configuration in yaml format python -m espnet2.bin.asr_train --print_config\",\"In this section, we use espnet2.bin.asr_train for an example, but the other training tools based on Task class have the same interface, so you can replace it to another command.\",\"Note that ESPnet2 always selects_ instead of - for the separation for the option name to avoid confusion.\",\"# Bad --batch-size # Good --batch_size\",\"A notable feature of --print_config is that it shows the configuration parsing with the given arguments dynamically: You can look up the parameters for a changeable class.\",\"% # Show parameters of Adam optimizer % python -m espnet2.bin.asr_train --optim adam --print_config ... optim: adam optim_conf: lr: 0.001 betas: - 0.9 - 0.999 eps: 1.0e-08 weight_decay: 0 amsgrad: false ... % # Show parameters of ReduceLROnPlateau scheduler % python -m espnet2.bin.asr_train --scheduler ReduceLROnPlateau --print_config ... scheduler: reducelronplateau scheduler_conf: mode: min factor: 0.1 patience: 10 verbose: false threshold: 0.0001 threshold_mode: rel cooldown: 0 min_lr: 0 eps: 1.0e-08 ...\"]},\"85\":{\"h\":\"Configuration file\",\"t\":[\"You can find the configuration files for DNN training in conf/train_*.yaml.\",\"ls conf/\",\"We adopt ConfigArgParse for this configuration system. The configuration in YAML format has an equivalent effect to the command line argument. e.g. The following two are equivalent:\",\"# config.yaml foo: 3 bar: 4\",\"python -m espnet2.bin.asr_train --config conf/config.yaml python -m espnet2.bin.asr_train --foo 3 --bar 4\"]},\"86\":{\"h\":\"Change the configuration for dict type value\",\"t\":[\"Some parameters are named as *_conf, e.g. optim_conf, decoder_conf and they has the dict type value. We also provide a way to configure the nested value in such a dict object.\",\"# e.g. Change parameters one by one python -m espnet2.bin.asr_train --optim_conf lr=0.1 --optim_conf rho=0.8 # e.g. Give the parameters in yaml format python -m espnet2.bin.asr_train --optim_conf \\\"{lr: 0.1, rho: 0.8}\\\"\"]},\"87\":{\"h\":\"Resume training process\",\"t\":[\"python -m espnet2.bin.asr_train --resume true\",\"The state of the training process is saved at the end of every epoch as checkpoint.pth and the training process can be resumed from the start of the next epoch. Checkpoint includes the following states.\",\"Model state\",\"Optimizer states\",\"Scheduler states\",\"Reporter state\",\"torch.cuda.amp state (from torch=1.6)\"]},\"88\":{\"h\":\"Transfer learning / Fine tuning using pretrained model\",\"t\":[\"Use --init_param <file_path>:<src_key>:<dst_key>:<exclude_keys>\",\"# Load all parameters python -m espnet2.bin.asr_train --init_param model.pth # Load only the parameters starting with \\\"decoder\\\" python -m espnet2.bin.asr_train --init_param model.pth:decoder # Load only the parameters starting with \\\"decoder\\\" and set it to model.decoder python -m espnet2.bin.asr_train --init_param model.pth:decoder:decoder # Set parameters to model.decoder python -m espnet2.bin.asr_train --init_param decoder.pth::decoder # Load all parameters excluding \\\"decoder.embed\\\" python -m espnet2.bin.asr_train --init_param model.pth:::decoder.embed # Load all parameters excluding \\\"encoder\\\" and \\\"decoder.embed\\\" python -m espnet2.bin.asr_train --init_param model.pth:::encoder,decoder.embed\"]},\"89\":{\"h\":\"Freeze parameters\",\"t\":[\"python -m espnet2.bin.asr_train --freeze_param encoder.enc encoder.decoder\"]},\"90\":{\"h\":\"Change logging interval\",\"t\":[\"The result in the middle state of the training will be shown by the specified number:\",\"python -m espnet2.bin.asr_train --log_interval 100\"]},\"91\":{\"h\":\"Change the number of iterations in each epoch\",\"t\":[\"By default, an epoch indicates using up whole data in the training corpus and the following steps will also run after training for every epoch:\",\"Validation\",\"Saving model and checkpoint\",\"Show result in the epoch\",\"Sometimes the validation after training with a whole corpus is too coarse if using large corpus. For that case, --num_iters_per_epoch can restrict the number of iteration of each epoch.\",\"python -m espnet2.bin.asr_train --num_iters_per_epoch 1000\",\"Note that the training process can't be resumed at the middle of an epoch because data iterators are stateless, but don't worry it! Our iterator is built at the start of each epoch and the random seed is fixed by the epoch number, just like:\",\"epoch_iter_factory = Task.build_epoch_iter_factory() for epoch in range(max_epoch): iterator = epoch_iter_factory.build_iter(epoch)\",\"Therefore, the training can be resumed at the start of the epoch.\"]},\"92\":{\"h\":\"Weights & Biases integration\",\"t\":[\"About Weights & Biases: https://docs.wandb.com/\",\"Installation and setup\",\"See: https://docs.wandb.com/quickstart\",\"wandb login\",\"Enable wandb\",\"python -m espnet2.bin.asr_train --use_wandb true\",\"and go to the shown URL.\",\"[Option] To use HTTPS PROXY\",\"export HTTPS_PROXY=...your proxy export CURL_CA_BUNDLE=your.pem export CURL_CA_BUNDLE= # Disable SSL certificate verification\"]},\"93\":{\"h\":\"Multi GPUs\",\"t\":[\"python -m espnet2.bin.asr_train --ngpu 2\",\"Just using CUDA_VISIBLE_DEVICES to specify the device number:\",\"CUDA_VISIBLE_DEVICES=2,3 python -m espnet2.bin.asr_train --ngpu 2\",\"About distributed training, see Distributed training.\"]},\"94\":{\"h\":\"The relation between mini-batch size and number of GPUs\",\"t\":[\"The batch-size can be changed as follows:\",\"# Change both of the batch_size for training and validation python -m espnet2.bin.asr_train --batch_size 20 # Change the batch_size for validation python -m espnet2.bin.asr_train --valid_batch_size 200\",\"The behavior for batch-size during multi-GPU training is different from that of ESPNet1.\",\"ESPNet1: The batch-size will be multiplied by the number of GPUs.\",\"python -m espnet.bin.asr_train --batch_size 10 --ngpu 2 # Actual batch_size is 20 and each GPU devices are assigned to 10\",\"ESPnet2: The batch-size is not changed regardless of the number of GPUs. \",\"Therefore, you should set a more number of batch-size than that of GPUs.\",\"python -m espnet.bin.asr_train --batch_size 10 --ngpu 2 # Actual batch_size is 10 and each GPU devices are assigned to 5\"]},\"95\":{\"h\":\"Change mini-batch type\",\"t\":[\"We adopt variable mini-batch size with considering the dimension of the input features to make the best use of the GPU memory.\",\"There are 6 types:\",\"batch_type\",\"Option to change batch-size\",\"Variable batch-size\",\"Requirement\",\"unsorted\",\"--batch_size\",\"No\",\"-\",\"sorted\",\"--batch_size\",\"No\",\"Length information of features\",\"folded\",\"--batch_size\",\"Yes\",\"Length information of features\",\"length\",\"--batch_bins\",\"Yes\",\"Length information of features\",\"numel\",\"--batch_bins\",\"Yes\",\"Shape information of features\",\"catbel\",\"--batch_size\",\"No\",\"-\",\"Note that --batch_size is ignored if --batch_type=length or --batch_type=numel.\"]},\"96\":{\"h\":\"\",\"t\":[\"This mode has nothing special feature and just creates constant-size mini-batches without any sorting by the length order. If you intend to use ESPnet as not Seq2Seq task, this type may be suitable.\",\"Unlike the other mode, this mode doesn't require the information of the feature dimension. In other words, it's not mandatory to prepare shape_file:\",\"python -m espnet.bin.asr_train \\\\ --batch_size 10 --batch_type unsorted \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --train_shape_file \\\"train.scp\\\" \\\\ --valid_shape_file \\\"valid.scp\\\"\",\"This system might seem strange and you might also feel --*_shape_file is verbose because the training corpus can be described totally only using --*_data_path_and_name_and_type.\",\"From the viewpoint of the implementation, we separate the data source for the Dataset and BatchSampler in the term of PyTorch and --*_data_path_and_name_and_type and --*_shape_file correspond to them respectively. From the viewpoint of the training strategy, because variable batch-size is supported according to the length/dimension of each feature, thus we need to prepare the shape information before training.\"]},\"97\":{\"h\":\"\",\"t\":[\"This mode creates constant-size mini-batches with sorting by the length order. This mode requires the information of the length.\",\"python -m espnet.bin.asr_train \\\\ --batch_size 10 --batch_type sorted \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --train_data_path_and_name_and_type \\\"train2.scp,feats2,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid2.scp,feats2,npy\\\" \\\\ --train_shape_file \\\"train_length.txt\\\" \\\\ --valid_shape_file \\\"valid_length.txt\\\"\",\"e.g. length.txt\",\"sample_id1 1230 sample_id2 156 sample_id3 890 ...\",\"Where the fist column indicates the sample id and the second is the length of the corresponding feature. You can see that shape file is input instead in our recipes.\",\"e.g. shape.txt\",\"sample_id1 1230,80 sample_id2 156,80 sample_id3 890,80 ...\",\"This file describes the full information of the feature shape; The first number is the length of the sequence and the second or later are the dimension of feature: Length,Dim1,Dim2,....\",\"Only the first number is referred for --batch_type sorted, --batch_type folded and --batch_type length, and the shape information is required only when --batch_type numel.\"]},\"98\":{\"h\":\"\",\"t\":[\"In ESPnet1, this mode is referred as seq.\",\"This mode creates mini-batch which has the size of base_batch_size // max_i(1 + L_i // f_i). Where L_i is the maximum length in the mini-batch for ith feature and f_i is the --fold length corresponding to the feature. This mode requires the information of length.\",\"python -m espnet.bin.asr_train \\\\ --batch_size 20 --batch_type folded \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --train_data_path_and_name_and_type \\\"train2.scp,feats2,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid2.scp,feats2,npy\\\" \\\\ --train_shape_file \\\"train_length.scp\\\" \\\\ --train_shape_file \\\"train_length2.scp\\\" \\\\ --valid_shape_file \\\"valid_length.scp\\\" \\\\ --valid_shape_file \\\"valid_length2.scp\\\" \\\\ --fold_length 5000 \\\\ --fold_length 300\",\"Note that the repeat number of *_shape_file must equal to the number of --fold_length, but you don't need to input same number of shape files as the number of data file. i.e. You can give it as follows:\",\"python -m espnet.bin.asr_train \\\\ --batch_size 20 --batch_type folded \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --train_data_path_and_name_and_type \\\"train2.scp,feats2,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid2.scp,feats2,npy\\\" \\\\ --train_shape_file \\\"train_length.txt\\\" \\\\ --valid_shape_file \\\"valid_length.txt\\\" \\\\ --fold_length 5000\",\"In this example, the length of the first feature is considered while the second can be ignored. This technique can be also applied for --batch_type length and --batch_type numel.\"]},\"99\":{\"h\":\"\",\"t\":[\"In ESPnet1, this mode is referred as frame.\",\"You need to specify --batch_bins to determine the mini-batch size instead of --batch_size. Each mini-batch has equal number of bins as possible counting by the total length in the mini-batch; i.e. bins = sum(len(feat) for feats in batch for feat in feats). This mode requires the information of length.\",\"python -m espnet.bin.asr_train \\\\ --batch_bins 10000 --batch_type length \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --train_data_path_and_name_and_type \\\"train2.scp,feats2,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid2.scp,feats2,npy\\\" \\\\ --train_shape_file \\\"train_length.txt\\\" \\\\ --train_shape_file \\\"train_length2.txt\\\" \\\\ --valid_shape_file \\\"valid_length.txt\\\" \\\\ --valid_shape_file \\\"valid_length2.txt\\\" \\\\\"]},\"100\":{\"h\":\"\",\"t\":[\"In ESPnet1, this mode is referred as bins.\",\"You need to specify --batch_bins to determine the mini-batch size instead of --batch_size. Each mini-batches has equal number of bins as possible counting by the total number of elements; i.e. bins = sum(numel(feat) for feats in batch for feat in feats), where numel returns the infinite product of the shape of each feature; shape[0] * shape[1] * ...\",\"python -m espnet.bin.asr_train \\\\ --batch_bins 200000 --batch_type numel \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --train_data_path_and_name_and_type \\\"train2.scp,feats2,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid2.scp,feats2,npy\\\" \\\\ --train_shape_file \\\"train_shape.txt\\\" \\\\ --train_shape_file \\\"train_shape2.txt\\\" \\\\ --valid_shape_file \\\"valid_shape.txt\\\" \\\\ --valid_shape_file \\\"valid_shape2.txt\\\"\"]},\"101\":{\"h\":\"\",\"t\":[\"This type of batch_type focuses on the case of classification tasks. It guarantees that within each mini-batch, all samples belong to different classes. --batch_size is used to determine the mini-batch size. This batch type does not go along with the default sequence iterator_type. It is instead designed to be used with category iterator_type. Therefore, instead of explicitely giving --batch_type catbel, it is more recommended to give --iterator_type category which will automatically set batch_type to catbel. It is also important to use a preprocessor that adjusts the sample duration to enable mini-batch construction. One example would be espnet2/train/preprocessor/SpkPreprocessor.\",\"python -m espnet.bin.spk_train \\\\ --batch_bins 256 --iterator_type category \\\\ --train_data_path_and_name_and_type \\\"train.scp,feats,npy\\\" \\\\ --valid_data_path_and_name_and_type \\\"valid.scp,feats,npy\\\" \\\\ --train_shape_file \\\"train_shape.txt\\\" \\\\ --valid_shape_file \\\"valid_shape.txt\\\" \\\\\"]},\"102\":{\"h\":\"Gradient accumulating\",\"t\":[\"There are several ways to deal with larger model architectures than the capacity of your GPU device memory during training.\",\"Using a larger number of GPUs\",\"Using a half decision tensor\",\"Using torch.utils.checkpoint\",\"Gradient accumulating\",\"Gradient accumulating is a technique to handle larger mini-batch than available size.\",\"Split a mini-batch into several numbers and forward and backward for each piece and accumulate the gradients ony by one, while optimizer's updating is invoked every the number of forwarding just like following:\",\"# accum_grad is the number of pieces for i, batch in enumerate(iterator): loss = net(batch) (loss / accum_grad).backward() # Gradients are accumulated if i % accum_grad: optim.update() optim.zero_grads()\",\"Give --accum_grad <int> to use this option.\",\"python -m espnet.bin.asr_train --accum_grad 2\",\"The effective batch_size becomes almost same as accum_grad * batch_size except for:\",\"The random state\",\"Some statistical layers based on mini-batch e.g. BatchNormalization\",\"The case that the batch_size is not unified for each iteration.\"]},\"103\":{\"h\":\"Automatic Mixed Precision training\",\"t\":[\"python -m espnet.bin.asr_train --use_amp true\"]},\"104\":{\"h\":\"Reproducibility and determinization\",\"t\":[\"There are some possibilities to make training non-reproducible.\",\"Initialization of parameters that come from PyTorch/ESPnet version difference.\",\"Reducing order for float values during multi GPUs training. \",\"I don't know whether NCCL is deterministic or not.\",\"Random seed difference \",\"We fixed the random seed for each epoch.\",\"CuDNN or some non-deterministic operations for CUDA: See https://pytorch.org/docs/stable/notes/randomness.html\",\"By default, CuDNN performs deterministic mode in our training and it can be turned off by:\",\"python -m espnet.bin.asr_train --cudnn_deterministic false\"]},\"105\":{\"h\":\"ESPnet2\"},\"106\":{\"h\":\"Main changes from ESPnet1\",\"t\":[\"Chainer free\",\"Discarding Chainer completely.\",\"The development of Chainer is stopped at v7: https://chainer.org/announcement/2019/12/05/released-v7.html\",\"Kaldi free\",\"It's not mandatory to compile Kaldi.\",\"If you find some recipes requiring Kaldi mandatory, please report it. It should be dealt with as a bug in ESPnet2.\",\"We still support the features made by Kaldi optionally.\",\"We still follow Kaldi style. i.e. depending on utils/ of Kaldi.\",\"On the fly feature extraction & text preprocessing for training \",\"You don't need to create the feature file before training, but just input wave data directly.\",\"We support both raw wave input and extracted features.\",\"The preprocessing for text, tokenization to characters, or sentencepieces, can be also applied during training.\",\"Support self-supervised learning representations from s3prl\",\"Discarding the JSON format describing the training corpus. \",\"Why do we discard the JSON format? Because a dict object generated from a large JSON file requires much memory and it also takes much time to parse such a large JSON file.\",\"Support distributed data-parallel training (Not enough tested) \",\"Single node multi GPU training with DistributedDataParallel is also supported.\"]},\"107\":{\"h\":\"Understanding ESPnet2 Recipes\",\"t\":[\"Recipe is a set of scripts that enables users to fully reproduce the experiment, such as data preparation, model definition, training, evaluation, and model release.\",\"You can find the new recipes in egs2 (shorthand for Examples for ESPnet2):\",\"espnet/ # Python modules of espnet1 espnet2/ # Python modules of espnet2 egs/ # espnet1 recipes egs2/ # espnet2 recipes\",\"The egs2 recipes are always structured by egs2/<dataset>/<task>. So, for example, the user should be able to fully reproduce the experiment by the following:\",\"# Dataset: an4, Task: ASR cd egs2/an4/asr1/ # Run the full experiment ./run.sh\",\"Note that the usage of recipes is almost the same as that of ESPnet1.\",\"Now, let's go step-by-step on how exactly the recipes work.\"]},\"108\":{\"h\":\"Change directory to the base directory\",\"t\":[\"# e.g. cd egs2/an4/asr1/\",\"an4 is a tiny corpus and can be freely obtained, so it might be suitable for this tutorial. You can perform any other recipes as the same way. e.g. wsj, librispeech, and etc.\",\"Keep in mind that all scripts should be ran at the level of egs2/<dataset>/<task>.\",\"# Doesn't work cd egs2/an4/ ./asr1/run.sh ./asr1/scripts/<some-script>.sh # Doesn't work cd egs2/an4/asr1/local/ ./data.sh # Works cd egs2/an4/asr1 ./run.sh ./scripts/<some-script>.sh\"]},\"109\":{\"h\":\"Directory structure of each recipe\",\"t\":[\"egs2/an4/asr1/ - conf/ # Configuration files for training, inference, etc. - scripts/ # Bash utilities of espnet2 - pyscripts/ # Python utilities of espnet2 - steps/ # From Kaldi utilities - utils/ # From Kaldi utilities - db.sh # The directory path of each corpora - path.sh # Setup script for environment variables - cmd.sh # Configuration for your backend of job scheduler - run.sh # Entry point - asr.sh # Invoked by run.sh\"]},\"110\":{\"h\":\"Change the configuration\",\"t\":[\"You need to modify db.sh for specifying your corpus before executing run.sh. For example, when you touch the recipe of egs2/wsj, you need to change the paths of WSJ0 and WSJ1 in db.sh.\",\"Some corpora can be freely obtained from the WEB and they are written as \\\"downloads/\\\" at the initial state. You can also change them to your corpus path if it's already downloaded.\",\"path.sh is used to set up the environment for run.sh. Note that the Python interpreter used for ESPnet is not the current Python of your terminal, but it's the Python which was installed at tools/. Thus you need to source path.sh to use this Python.\",\". path.sh python\",\"cmd.sh is used for specifying the backend of the job scheduler. If you don't have such a system in your local machine environment, you don't need to change anything about this file. See Using Job scheduling system\"]},\"111\":{\"h\":\"\",\"t\":[\"./run.sh\",\"run.sh is an example script, which we often call as \\\"recipe\\\", to run all stages related to DNN experiments; data-preparation, training, and evaluation.\"]},\"112\":{\"h\":\"Seeing the training status\"},\"113\":{\"h\":\"Show the log file\",\"t\":[\"% tail -f exp/*_train_*/train.log [host] 2020-04-05 16:34:54,278 (trainer:192) INFO: 2/40epoch started. Estimated time to finish: 7 minutes and 58.63 seconds [host] 2020-04-05 16:34:56,315 (trainer:453) INFO: 2epoch:train:1-10batch: iter_time=0.006, forward_time=0.076, loss=50.873, los s_att=35.801, loss_ctc=65.945, acc=0.471, backward_time=0.072, optim_step_time=0.006, lr_0=1.000, train_time=0.203 [host] 2020-04-05 16:34:58,046 (trainer:453) INFO: 2epoch:train:11-20batch: iter_time=4.280e-05, forward_time=0.068, loss=44.369 , loss_att=28.776, loss_ctc=59.962, acc=0.506, backward_time=0.055, optim_step_time=0.006, lr_0=1.000, train_time=0.173\"]},\"114\":{\"h\":\"Show the training status in a image file\",\"t\":[\"# Accuracy plot # (eog is Eye of GNOME Image Viewer) eog exp/*_train_*/images/acc.img # Attention plot eog exp/*_train_*/att_ws/<sample-id>/<param-name>.img\"]},\"115\":{\"h\":\"Use tensorboard\",\"t\":[\"tensorboard --logdir exp/*_train_*/tensorboard/\"]},\"116\":{\"h\":\"Instruction for run.sh\"},\"117\":{\"h\":\"How to parse command-line arguments in shell scripts?\",\"t\":[\"All shell scripts in espnet/espnet2 depend on utils/parse_options.sh to parase command line arguments.\",\"e.g. If the script has ngpu option\",\"#!/usr/bin/env bash # run.sh ngpu=1 . utils/parse_options.sh echo ${ngpu}\",\"Then you can change the value as follows:\",\"$ ./run.sh --ngpu 2 echo 2\",\"You can also show the help message:\",\"./run.sh --help\"]},\"118\":{\"h\":\"Start from a specified stage and stop at a specified stage\",\"t\":[\"The procedures in run.sh can be divided into some stages, e.g. data preparation, training, and evaluation. You can specify the starting stage and the stopping stage.\",\"./run.sh --stage 2 --stop-stage 6\",\"There are also some altenative otpions to skip specified stages:\",\"run.sh --skip_data_prep true # Skip data preparation stages. run.sh --skip_train true # Skip training stages. run.sh --skip_eval true # Skip decoding and evaluation stages. run.sh --skip_packing false --skip_upload_hf false # Enable packing and uploading to huggingface stages.\",\"Note that skip_upload and skip_upload_hf are true by default. Please change them to false when uploading your model.\"]},\"119\":{\"h\":\"Change the configuration for training\",\"t\":[\"Please keep in mind that run.sh is a wrapper script of several tools including DNN training command. You need to do one of the following two ways to change the training configuration.\",\"# Give a configuration file ./run.sh --asr_config conf/train_asr.yaml # Give arguments to \\\"espnet2/bin/asr_train.py\\\" directly ./run.sh --asr_args \\\"--foo arg --bar arg2\\\"\",\"e.g. To change learning rate for the LM training\",\"./run.sh --lm_args \\\"--optim_conf lr=0.1\\\"\",\"This is the case of ASR training and you need to replace it accordingly for the other task. e.g. For TTS\",\"./run.sh --tts_args \\\"--optim_conf lr=0.1\\\"\",\"See Change the configuration for training for more detail about the usage of training tools.\"]},\"120\":{\"h\":\"Change the number of parallel jobs\",\"t\":[\"./run.sh --nj 10 # Chnage the number of parallels for data preparation stages. ./run.sh --inference_nj 10 # Chnage the number of parallels for inference jobs.\",\"We also support submitting jobs to multiple hosts to accelerate your experiment: See Using Job scheduling system\"]},\"121\":{\"h\":\"Multi GPUs training and distributed training\",\"t\":[\"./run.sh --ngpu 4 # 4GPUs in a single node ./run.sh --ngpu 2 --num_nodes 2 # 2GPUs x 2nodes\",\"Note that you need to setup your environment correctly to use distributed training. See the following two:\",\"Distributed training\",\"Using Job scheduling system\"]},\"122\":{\"h\":\"Various tips\"},\"123\":{\"h\":\"Relationship between mini-batch size and number of GPUs\",\"t\":[\"The behavior of batch size in ESPnet2 during multi-GPU training is different from that in ESPnet1. In ESPnet2, the total batch size is not changed regardless of the number of GPUs. Therefore, you need to manually increase the batch size if you increase the number of GPUs. Please refer to this doc for more information.\"]},\"124\":{\"h\":\"Use specified experiment directory for evaluation\",\"t\":[\"If you already have trained a model, you may wonder how to give it to run.sh when you'll evaluate it later. By default the directory name is determined according to given options, asr_args, lm_args, or etc. You can overwrite it by --asr_exp and --lm_exp.\",\"# For ASR recipe ./run.sh --skip_data_prep true --skip_train true --asr_exp <your_asr_exp_directory> --lm_exp <your_lm_exp_directory> # For TTS recipe ./run.sh --skip_data_prep true --skip_train true --tts_exp <your_tts_exp_directory>\"]},\"125\":{\"h\":\"Evaluation without training using pretrained model\",\"t\":[\"./run.sh --download_model <model_name> --skip_train true\",\"You need to fill model_name by yourself. You can search for pretrained models on Hugging Face using the tag espnet See the following link about our pretrained models: https://github.com/espnet/espnet_model_zoo\"]},\"126\":{\"h\":\"Evaluation using OpenAI Whisper\",\"t\":[\"ESPnet2 provides a script to run inference and scoring using OpenAI's Whisper. This can be used to evaluate speech generation models. Here is an example:\",\"#!/usr/bin/env bash # Set bash to 'debug' mode, it will exit on : # -e 'error', -u 'undefined variable', -o ... 'error in pipeline', -x 'print commands', set -e set -u set -o pipefail whisper_tag=medium # whisper model tag, e.g., small, medium, large, etc cleaner=whisper_en hyp_cleaner=whisper_en nj=1 test_sets=\\\"test/WSJ/test_eval92\\\" # decode_options is used in Whisper model's transcribe method decode_options=\\\"{language: en, task: transcribe, temperature: 0, beam_size: 10, fp16: False}\\\" for x in ${test_sets}; do wavscp=dump/raw/${x}/wav.scp # path to wav.scp outdir=whisper-${whisper_tag}_outputs/${x} # path to save output gt_text=dump/raw/${x}/text # path to groundtruth text file (for scoring only) scripts/utils/evaluate_asr.sh \\\\ --whisper_tag ${whisper_tag} \\\\ --nj ${nj} \\\\ --gpu_inference true \\\\ --stage 2 \\\\ --stop_stage 3 \\\\ --cleaner ${cleaner} \\\\ --hyp_cleaner ${hyp_cleaner} \\\\ --decode_options \\\"${decode_options}\\\" \\\\ --gt_text ${gt_text} \\\\ ${wavscp} \\\\ ${outdir} done\"]},\"127\":{\"h\":\"Packing and sharing your trained model\",\"t\":[\"ESPnet encourages you to share your results using platforms like Hugging Face.\",\"For sharing your models, the last three stages of each task simplify this process. The model is packed into a zip file and uploaded to the selected platform (one or both).\",\"For Hugging Face, you need to first create a repository (<my_repo> = <user_name>/<repo_name>). Remember to install git-lfs before continuing. Then, execute run.sh as follows:\",\"./run.sh --stage <packing stage> --skip-packing false --skip-upload-hf false --hf-repo <my_repo>\",\"The stage number differs according to the task. Please read the task-specific shell script (e.g., asr1/asr.sh) to see the number to specify. The packed model can be uploaded to huggingface by setting the previously mentioned flags.\"]},\"128\":{\"h\":\"Usage of Self-Supervised Learning Representations as feature\",\"t\":[\"ESPnet supports self-supervised learning representations (SSLR) to replace traditional spectrum features. In some cases, SSLRs can boost the performance.\",\"To use SSLRs in your task, you need to make several modifications.\",\"Install S3PRL by tools/installers/install_s3prl.sh.\",\"If HuBERT / Wav2Vec is needed, fairseq should be installed by tools/installers/install_fairseq.sh.\",\"Here's various tips for using SSLRs.\",\"To reduce the time used in collect_stats step, please specify --feats_normalize uttmvn in run.sh and pass it as arguments to asr.sh or other task-specific scripts. (Recommended)\",\"In the configuration file, specify the frontend and preencoder. Taking HuBERT as an example: The upstream name can be whatever supported in S3PRL. multilayer-feature=True means the final representation is a weighted-sum of all layers' hidden states from SSLR model.\",\"frontend: s3prl frontend_conf: frontend_conf: upstream: hubert_large_ll60k # Note: If the upstream is changed, please change the input_size in the preencoder. download_dir: ./hub multilayer_feature: True\",\"Here the preencoder is to reduce the input dimension to the encoder, to reduce the memory cost. The input_size depends on the upstream model, while the output_size can be set to any values.\",\"preencoder: linear preencoder_conf: input_size: 1024 # Note: If the upstream is changed, please change this value accordingly. output_size: 80\",\"Because the shift sizes of different upstream models are different, e.g. HuBERT and Wav2Vec2.0 have 20ms frameshift. Sometimes, the downsampling rate (input_layer) in the encoder configuration need to be changed. For example, using input_layer: conv2d2 will results in a total frameshift of 40ms, which is enough for some tasks.\"]},\"129\":{\"h\":\"Streaming ASR\",\"t\":[\"ESPnet supports streaming Transformer/Conformer ASR with blockwise synchronous beam search. For more details, please refer to the paper.\"]},\"130\":{\"h\":\"Training\",\"t\":[\"To achieve streaming ASR, please employ blockwise Transformer/Conformer encoder in the configuration file. Taking blockwise Transformer as an example: The encoder name can be contextual_block_transformer or contextual_block_conformer.\",\"encoder: contextual_block_transformer encoder_conf: block_size: 40 # block size for block processing hop_size: 16 # hop size for block processing look_ahead: 16 # look-ahead size for block processing init_average: true # whether to use average input as initial context ctx_pos_enc: true # whether to use positional encoding for the context vectors\"]},\"131\":{\"h\":\"Decoding\",\"t\":[\"To enable online decoding, the argument --use_streaming true should be added to run.sh.\",\"./run.sh --stage 12 --use_streaming true\"]},\"132\":{\"h\":\"FAQ\",\"t\":[\"Issue about 'NoneType' object has no attribute 'max' during training: Please make sure you employ forward_train function during traininig, check more details here.\",\"I successfully trained the model, but encountered the above issue during decoding: You may forget to specify --use_streaming true to select streaming inference.\"]},\"133\":{\"h\":\"Real-Time-Factor and Latency\",\"t\":[\"In order to calculate real-time-factor and (non-streaming) latency the script utils/calculate_rtf.py has been reworked and can now be used for both ESPnet1 and ESPnet2. The script calculates inference times based on time markers in the decoding log files and reports the average real-time-factor (RTF) and average latency over all decoded utterances. For ESPnet2, the script will automatically be run (see Limitations section below) after the decoding stage has finished but can also be run as a stand-alone script:\"]},\"134\":{\"h\":\"Usage\",\"t\":[\"usage: calculate_rtf.py [-h] [--log-dir LOG_DIR] [--log-name {decode,asr_inference}] [--input-shift INPUT_SHIFT] [--start-times-marker {input lengths,speech length}] [--end-times-marker {prediction,best hypo}] calculate real time factor (RTF) optional arguments: -h, --help show this help message and exit --log-dir LOG_DIR path to logging directory --log-name {decode,asr_inference} name of logfile, e.g., 'decode' (espnet1) and 'asr_inference' (espnet2) --input-shift INPUT_SHIFT shift of inputs in milliseconds --start-times-marker {input lengths,speech length} String marking start of decoding in logfile, e.g., 'input lengths' (espnet1) and 'speech length' (espnet2) --end-times-marker {prediction,best hypo} String marking end of decoding in logfile, e.g., 'prediction' (espnet1) and 'best hypo' (espnet2)\"]},\"135\":{\"h\":\"Notes\",\"t\":[\"Default settings still target ESPnet1 usage:\",\"--log-name 'decode' --input-shift 10.0 --start-times-marker 'input lengths' --end-times-marker 'prediction'\",\"For ESPnet2, other frame shifts than 10ms are possible via different front-end/feature configurations. So different to ESPnet1, which logs the input feature frames at a fixed 10ms frame shift, in ESPnet2 the number of speech samples is logged instead and the audio sample shift in milliseconds (1/sampleRate x 1000) needs to be specified for --input-shift parameter (see --input-shift 0.0625 in example below for 16000 Hz sample rate).\"]},\"136\":{\"h\":\"Example\",\"t\":[\"From espnet/egs2/librispeech/asr1 the following call runs the decoding stage with pretrained ESPnet2 model:\",\"./run.sh --stage 12 --use_streaming false --skip_data_prep true --skip_train true --download_model byan/librispeech_asr_train_asr_conformer_raw_bpe_batch_bins30000000_accum_grad3_optim_conflr0.001_sp\",\"Results for latency and rtf calculation on Librispeech test_clean subset can then be found in espnet/egs2/librispeech/asr1/exp/byan/librispeech_asr_train_asr_conformer_raw_bpe_batch_bins30000000_accum_grad3_optim_conflr0.001_sp/decode_asr_lm_lm_train_lm_transformer2_en_bpe5000_valid.loss.ave_asr_model_valid.acc.ave/test_clean/logdir/calculate_rtf.log file:\",\"# ../../../utils/calculate_rtf.py --log-dir exp/byan/librispeech_asr_train_asr_conformer_raw_bpe_batch_bins30000000_accum_grad3_optim_conflr0.001_sp/decode_as r_lm_lm_train_lm_transformer2_en_bpe5000_valid.loss.ave_asr_model_valid.acc.ave/test_clean/logdir --log-name asr_inference --input-shift 0.0625 --start-times- marker \\\"speech length\\\" --end-times-marker \\\"best hypo\\\" Total audio duration: 19452.481 [sec] Total decoding time: 137762.231 [sec] RTF: 7.082 Latency: 52581.004 [ms/sentence]\"]},\"137\":{\"h\":\"Limitations\",\"t\":[\"Only non-streaming inference mode is supported currently\",\"The decoding stage 12 in asr.sh automatically runs the rtf & latency calculation if \\\"asr_inference_tool == \\\"espnet2.bin.asr_inference\\\"; other inference tools like k2 & maskctc are still left to do\"]},\"138\":{\"h\":\"Transducer ASR\",\"t\":[\"Important: If you encounter any issue related to warp-transducer, please open an issue in our forked repo.\",\"ESPnet2 supports models trained with the (RNN-)Tranducer loss, aka Transducer models. Currently, two versions of these models exist within ESPnet2: one under asr and the other under asr_transducer. The first one is designed as a supplement of CTC-Attention ASR models while the second is designed independently and purely for the Transducer task. For that, we rely on ESPnetASRTransducerModel instead of ESPnetASRModel and a new task called ASRTransducerTask is used in place of ASRTask.\",\"For the user, it means two things. First, some features or modules may not be supported depending on the version used. Second, the usage of some common ASR features or modules may differ between the models. In addition, some core modules (e.g.: preencoder or postencoder) may be missing in the standalone version until validation.\",\"The following sections of this tutorial are dedicated to the introduction of the version under asr_transducer. Thus, the user should keep in mind that most features described here may not be available in the other version.\"]},\"139\":{\"h\":\"General usage\",\"t\":[\"To enable Transducer model training or decoding in your experiments, the following option should be supplied to asr.sh in your run.sh:\",\"asr.sh --asr_task asr_transducer [...]\",\"For Transducer loss computation during training, we rely by default on a fork of warp-transducer. The installation procedure is described here.\",\"Note: We made available FastEmit regularization [Yu et al., 2021] during loss computation. To enable it, fastemit_lambda need to be set in model_conf:\",\"model_conf: fastemit_lambda: Regularization parameter for FastEmit. (float, default = 0.0)\",\"Optionnaly, we also support training with the Pruned RNN-T loss [Kuang et al. 2022] made available in the k2 toolkit. To use it, the parameter use_k2_pruned_loss should be set to True in model_conf. From here, the loss computation can be controlled by setting the following parameters through k2_pruned_loss_args in model_conf:\",\"model_conf: use_k2_pruned_loss: True k2_pruned_loss_args: prune_range: How many tokens by frame are used compute the pruned loss. (int, default = 5) simple_loss_scaling: The weight to scale the simple loss after warm-up. (float, default = 0.5) lm_scale: The scale factor to smooth the LM part. (float, default = 0.0) am_scale: The scale factor to smooth the AM part. (float, default = 0.0) loss_type: Define the type of path to take for loss computation, either 'regular', 'smoothed' or 'constrained'. (str, default = \\\"regular\\\")\",\"Note: Because the number of tokens emitted by timestep can be restricted during training with this version, we also make available the parameter validation_nstep. It let the users apply similar constraints during the validation process, when reporting CER or/and WER:\",\"model_conf: validation_nstep: Maximum number of symbol expansions at each time step when reporting CER or/and WER using mAES.\",\"For more information, see section Inference and \\\"modified Adaptive Expansion Search\\\" algorithm.\"]},\"140\":{\"h\":\"Architecture\",\"t\":[\"The architecture is composed of three modules: encoder, decoder and joint network. Each module has one (or three) config(s) with various parameters in order to configure the internal parts. The following sections describe the mandatory and optional parameters for each module.\"]},\"141\":{\"h\":\"Encoder\",\"t\":[\"For the encoder, we propose a unique encoder type encapsulating the following blocks: Branchformer, Conformer, Conv 1D and E-Branchformer. It is similar to the custom encoder in ESPnet1, meaning we don't need to set the parameter encoder: [type] here. Instead, the encoder architecture is defined by three configurations passed to encoder_conf:\",\"input_conf (Dict): The configuration for the input block.\",\"main_conf (Dict): The main configuration for the parameters shared across all blocks.\",\"body_conf (List[Dict]): The list of configurations for each block of the encoder architecture but the input block.\",\"The first and second configurations are optional. If needed, the following parameters can be modified in each configuration:\",\"main_conf: pos_wise_act_type: Conformer position-wise feed-forward activation type. (str, default = \\\"swish\\\") conv_mod_act_type: Conformer convolution module activation type. (str, default = \\\"swish\\\") pos_enc_dropout_rate: Dropout rate for the positional encoding layer, if used. (float, default = 0.0) pos_enc_max_len: Positional encoding maximum length. (int, default = 5000) simplified_att_score: Whether to use simplified attention score computation. (bool, default = False) norm_type: X-former normalization module type. (str, default = \\\"layer_norm\\\") conv_mod_norm_type: Branchformer convolution module normalization type. (str, default = \\\"layer_norm\\\") after_norm_eps: Epsilon value for the final normalization module. (float, default = 1e-05 or 0.25 for BasicNorm) after_norm_partial: Partial value for the final normalization module, if norm_type = 'rms_norm'. (float, default = -1.0) blockdrop_rate: Probability threshold of dropping out each encoder block. (float, default = 0.0) # For more information on the parameters below, please refer to espnet2/asr_transducer/activation.py ftswish_threshold: Threshold value for FTSwish activation formulation. ftswish_mean_shift: Mean shifting value for FTSwish activation formulation. hardtanh_min_val: Minimum value of the linear region range for HardTanh activation. (float, default = -1.0) hardtanh_max_val: Maximum value of the linear region range for HardTanh. (float, default = 1.0) leakyrelu_neg_slope: Negative slope value for LeakyReLU activation formulation. smish_alpha: Alpha value for Smish variant activation fomulation. (float, default = 1.0) smish_beta: Beta value for Smish variant activation formulation. (float, default = 1.0) softplus_beta: Beta value for softplus activation formulation in Mish activation. (float, default = 1.0) softplus_threshold: Values above this revert to a linear function in Mish activation. (int, default = 20) swish_beta: Beta value for E-Swish activation formulation. (float, default = 20) input_conf: block_type: Input block type, either \\\"conv2d\\\" or \\\"vgg\\\". (str, default = \\\"conv2d\\\") conv_size: Convolution output size. For \\\"vgg\\\", the two convolution outputs can be controlled by passing a tuple. (int, default = 256) subsampling_factor: Subsampling factor of the input block, either 2 (only conv2d), 4 or 6. (int, default = 4)\",\"The only mandatory configuration is body_conf, defining the encoder body architecture block by block. Each block has its own set of mandatory and optional parameters depending on the type, defined by block_type:\",\" # Branchformer - block_type: branchformer hidden_size: Hidden (and output) dimension. (int) linear_size: Dimension of the Linear layers. (int) conv_mod_kernel_size: Size of the convolving kernel in the ConvolutionalSpatialGatingUnit module. (int) heads (optional): Number of heads in multi-head attention. (int, default = 4) norm_eps (optional): Epsilon value for the normalization module. (float, default = 1e-05 or 0.25 for BasicNorm) norm_partial (optional): Partial value for the normalization module, if norm_type = 'rms_norm'. (float, default = -1.0) conv_mod_norm_eps (optional): Epsilon value for ConvolutionalSpatialGatingUnit module normalization. (float, default = 1e-05 or 0.25 for BasicNorm) conv_mod_norm_partial (optional): Partial value for the ConvolutionalSpatialGatingUnit module normalization, if conv_norm_type = 'rms_norm'. (float, default = -1.0) dropout_rate (optional): Dropout rate for some intermediate layers. (float, default = 0.0) att_dropout_rate (optional): Dropout rate for the attention module. (float, default = 0.0) # Conformer - block_type: conformer hidden_size: Hidden (and output) dimension. (int) linear_size: Dimension of feed-forward module. (int) conv_mod_kernel_size: Size of the convolving kernel in the ConformerConvolution module. (int) heads (optional): Number of heads in multi-head attention. (int, default = 4) norm_eps (optional): Epsilon value for normalization module. (float, default = 1e-05 or 0.25 for BasicNorm) norm_partial (optional): Partial value for the normalization module, if norm_type = 'rms_norm'. (float, default = -1.0) conv_mod_norm_eps (optional): Epsilon value for Batchnorm1d in the ConformerConvolution module. (float, default = 1e-05) conv_mod_norm_momentum (optional): Momentum value for Batchnorm1d in the ConformerConvolution module. (float, default = 0.1) dropout_rate (optional): Dropout rate for some intermediate layers. (float, default = 0.0) att_dropout_rate (optional): Dropout rate for the attention module. (float, default = 0.0) pos_wise_dropout_rate (optional): Dropout rate for the position-wise feed-forward module. (float, default = 0.0) # Conv 1D - block_type: conv1d output_size: Output size. (int) kernel_size: Size of the convolving kernel. (int or Tuple) stride (optional): Stride of the sliding blocks. (int or tuple, default = 1) dilation (optional): Parameter to control the stride of elements within the neighborhood. (int or tuple, default = 1) groups (optional): Number of blocked connections from input channels to output channels. (int, default = 1) bias (optional): Whether to add a learnable bias to the output. (bool, default = True) relu (optional): Whether to use a ReLU activation after convolution. (bool, default = True) batch_norm: Whether to use batch normalization after convolution. (bool, default = False) dropout_rate (optional): Dropout rate for the Conv1d outputs. (float, default = 0.0) # E-Branchformer - block_type: ebranchformer hidden_size: Hidden (and output) dimension. (int) linear_size: Dimension of the feed-forward module and othger linear layers. (int) conv_mod_kernel_size: Size of the convolving kernel in the ConvolutionalSpatialGatingUnit module. (int) depthwise_conv_kernel_size: Size of the convolving kernel in the DepthwiseConvolution module. (int, default = conv_mod_kernel_size) heads (optional): Number of heads in multi-head attention. (int, default = 4) norm_eps (optional): Epsilon value for the normalization module. (float, default = 1e-05 or 0.25 for BasicNorm) norm_partial (optional): Partial value for the normalization module, if norm_type = 'rms_norm'. (float, default = -1.0) conv_mod_norm_eps (optional): Epsilon value for ConvolutionalSpatialGatingUnit module normalization. (float, default = 1e-05 or 0.25 for BasicNorm) conv_mod_norm_partial (optional): Partial value for the ConvolutionalSpatialGatingUnit module normalization, if conv_norm_type = 'rms_norm'. (float, default = -1.0) dropout_rate (optional): Dropout rate for some intermediate layers. (float, default = 0.0) att_dropout_rate (optional): Dropout rate for the attention module. (float, default = 0.0)\",\"In addition, each block has a parameter num_blocks to build N times the defined block (int, default = 1). This is useful if you want to use a group of blocks sharing the same parameters without writing each configuration.\",\"Example 1: conv 2D + 2x Conv 1D + 14x Conformer.\",\"encoder_conf: main_conf: pos_wise_act_type: swish pos_enc_dropout_rate: 0.1 conv_mod_act_type: swish input_conf: block_type: conv2d conv_size: 256 subsampling_factor: 4 body_conf: - block_type: conv1d output_size: 128 kernel_size: 3 - block_type: conv1d output_size: 256 kernel_size: 2 - block_type: conformer linear_size: 1024 hidden_size: 256 heads: 8 dropout_rate: 0.1 pos_wise_dropout_rate: 0.1 att_dropout_rate: 0.1 conv_mod_kernel_size: 31 num_blocks: 14\"]},\"142\":{\"h\":\"Decoder\",\"t\":[\"For the decoder, four types of blocks are available: stateless ('stateless'), RNN ('rnn'), MEGA ('mega') or RWKV ('rwkv'). Contrary to the encoder, the parameters are shared across the blocks, meaning we only define one block in the configuration. The type of the stack of blocks is defined by passing the corresponding type string to the parameter decoder. The internal parts are defined through the field decoder_conf containing the following controlable parameters:\",\"decoder_conf: embed_size: Size of the embedding layer (int, default = 256). num_blocks: Number of decoder blocks/layers (int, default = 4 for MEGA or 1 for RNN). rnn_type (RNN only): Type of RNN cells (int, default = \\\"lstm\\\"). hidden_size (RNN only): Size of the hidden layers (int, default = 256). block_size (MEGA/RWKV only): Size of the block's input/output (int, default = 512). linear_size (MEGA/RWKV only): Feed-Forward module hidden size (int, default = 1024). attention_size (RWKV only): Hidden-size of the attention module. (int, default = None). context_size (RWKV only): Context size for the WKV kernel module (int, default = 1024). qk_size (MEGA only): Shared query and key size for attention module (int, default = 128). v_size (MEGA only): Value size for attention module (int, default = 1024). chunk_size (MEGA only): Chunk size for attention computation (int, default = -1, i.e. full context). num_heads (MEGA only): Number of EMA heads (int, default = 4). rel_pos_bias (MEGA only): Type of relative position bias in attention module (str, default = \\\"simple\\\"). max_positions (MEGA only): Maximum number of position for RelativePositionBias (int, default = 2048). truncation_length (MEGA only): Maximum length for truncation in EMA module (int, default = None). normalization_type (MEGA/RWKV only): Normalization layer type (str, default = \\\"layer_norm\\\"). normalization_args (MEGA/RKWV only): Normalization layer arguments (dict, default = {}). activation_type (MEGA only): Activation function type (str, default = \\\"swish\\\"). activation_args (MEGA only): Activation function arguments (dict, default = {}). rescale_every (RWKV only): Whether to rescale input every N blocks during inference (int, default = 0) dropout_rate (excl. RWKV): Dropout rate for main block modules (float, default = 0.0). embed_dropout_rate: Dropout rate for embedding layer (float, default = 0.0). att_dropout_rate (MEGA/RWKV only): Dropout rate for the attention module. ema_dropout_rate (MEGA only): Dropout rate for the EMA module. ffn_dropout_rate (MEGA/RWKV only): Dropout rate for the feed-forward module.\",\"Example 1: RNN decoder.\",\"decoder: rnn decoder_conf: rnn_type: lstm num_layers: 2 embed_size: 256 hidden_size: 256 dropout_rate: 0.1 embed_dropout_rate: 0.1\",\"Example 2: MEGA decoder.\",\"decoder: mega decoder_conf: block_size: 256 linear_size: 2048 qk_size: 128 v_size: 1024 max_positions: 1024 num_heads: 4 rel_pos_bias_type: \\\"rotary\\\" chunk_size: 256 num_blocks: 6 dropout_rate: 0.1 ffn_dropout_rate: 0.1 att_dropout_rate: 0.1 embed_dropout_rate: 0.1\"]},\"143\":{\"h\":\"Joint network\",\"t\":[\"Currently, we only propose the standard joint network module composed of three linear layers and an activation function. The module definition is optional but the following parameters can be modified through the configuration parameter joint_network_conf:\",\"joint_network_conf: joint_space_size: Size of the joint space (int, default = 256). joint_act_type: Type of activation in the joint network (str, default = \\\"tanh\\\"). \",\"The options related to the activation functions can also be modified through the parameters introduced in the Encoder section (See main_conf description).\"]},\"144\":{\"h\":\"Multi-task learning\",\"t\":[\"We also support multi-task learning with two auxiliary tasks: CTC and cross-entropy w/ label smoothing option (called LM loss here). The auxiliary tasks contribute to the overal task defined as:\",\"L_tot = (λ_trans x L_trans) + (λ_auxCTC x L_auxCTC) + (λ_auxLM x L_auxLM)\",\"where the losses (L_*) are respectively, in order: The Transducer loss, the CTC loss and the LM loss. Lambda values define their respective contribution to the total loss. Each task can be parameterized using the following options, passed to model_conf:\",\"model_conf: transducer_weight: Weight of the Transducer loss (float, default = 1.0) auxiliary_ctc_weight: Weight of the CTC loss. (float, default = 0.0) auxiliary_ctc_dropout_rate: Dropout rate for the CTC loss inputs. (float, default = 0.0) auxiliary_lm_loss_weight: Weight of the LM loss. (float, default = 0.2) auxiliary_lm_loss_smoothing: Smoothing rate for LM loss. If > 0, label smoothing is enabled. (float, default = 0.0)\",\"Note: We do not support other auxiliary tasks in ESPnet2 yet.\"]},\"145\":{\"h\":\"Inference\",\"t\":[\"Various decoding algorithms are also available for Transducer by setting search_type parameter in your decode config:\",\"Beam search algorithm without prefix search [Graves, 2012]. (search_type: default)\",\"Time Synchronous Decoding [Saon et al., 2020]. (search_type: tsd)\",\"Alignment-Length Synchronous Decoding [Saon et al., 2020]. (search_type: alsd)\",\"modified Adaptive Expansion Search, based on [Kim et al., 2021] and [Boyer et al., 2021]. (search_type: maes)\",\"The algorithms share two parameters to control the beam size (beam_size) and the partial/final hypotheses normalization (score_norm). In addition, three algorithms have specific parameters:\",\"Time-synchronous decoding\",\"search_type: tsd max_sym_exp : Number of maximum symbol expansions at each time step. (int > 1, default = 3)\",\"Alignement-Length Synchronous decoding\",\"search_type: alsd u_max: Maximum expected target sequence length. (int, default = 50)\",\"Modified Adaptive Expansion Search\",\"search_type: maes nstep: Number of maximum expansion steps at each time step (int, default = 2) expansion_gamma: Number of additional candidates in expanded hypotheses selection. (int, default = 2) expansion_beta: Allowed logp difference for prune-by-value method. (float, default = 2.3)\",\"Note: Except for the default algorithm, the described parameters are used to control the performance and decoding speed. The optimal values for each parameter are task-dependent; a high value will typically increase decoding time to focus on performance while a low value will improve decoding time at the expense of performance.\",\"Note 2: The algorithms in the standalone version are the same as the one in the other version.. However, due to design choices, some parts were reworked and minor optimizations were added in the same time.\"]},\"146\":{\"h\":\"Streaming\",\"t\":[\"To enable streaming capabilities for Transducer models, we support dynamic chunk training and chunk-by-chunk decoding as proposed in [Zhang et al., 2021]. Our implementation is based on the version proposed in Icefall, based itself on the original WeNet one.\",\"For a complete explanation on the different procedure and parameters, we refer the reader to the corresponding paper.\"]},\"147\":{\"h\":\"Training\",\"t\":[\"To train a streaming model, the parameter dynamic_chunk_training should be set to True in main_conf (See section Encoder. From here, the user has access to two parameters in order to control the dynamic chunk selection (short_chunk_threshold and short_chunk_size) and another one to control the left context in the causal convolution and the attention module (num_left_chunks).\",\"All these parameters can be configured through main_conf, introduced in the Encoder section:\",\"dynamic_chunk_training: Whether to train streaming model with dynamic chunks. (bool, default = False) short_chunk_threshold: Chunk length threshold (in percent) for dynamic chunk selection. (int, default = 0.75) short_chunk_size: Minimum number of frames during dynamic chunk training. (int, default = 25) num_left_chunks: The number of left chunks the attention module can see during training, where the actual size is defined by `short_chunk_threshold` and `short_chunk_size`. (int, default = 0, i.e. full context)\"]},\"148\":{\"h\":\"Decoding\",\"t\":[\"To perform chunk-by-chunk inference, the parameter streaming should be set to True in the decoding configuration (otherwise, offline decoding will be performed). Two parameters are available to control the decoding process:\",\"decoding_window: The input audio length, in milliseconds, to process during decoding. (int, default = 640) left_context: Number of previous frames (AFTER subsampling) the attention module can see in current chunk. (int, default = 32)\",\"Note: All search algorithms but ALSD are available with chunk-by-chunk inference.\"]},\"149\":{\"h\":\"FAQ\"},\"150\":{\"h\":\"How to add a new block type to the custom encoder?\",\"t\":[\"Provided paths are relative to the directory: espnet2/asr_transducer/encoder/\",\"Adding support to a new block type can be achieved in three main steps:\",\"Write your need block class in encoder/blocks/. The class should have the following methods: __init__(...), forward(...) (training + offline), chunk_forward(...) (online decoding), reset_streaming_cache(...) (online cache definition). For more details on implementing internal parts, we refer the user to the existing block definition and the Streaming section.\",\"In building.py, write a block constructor method and add a new condition in build_body_blocks(...) for your block type, calling the constructor method. If you need additional parameters to share across blocks, you can add them in build_main_parameters(...) and pass main_conf to your constructor.\",\"In validation.py, add new conditions to `validate_block_arguments(...) in order to set and validate the mandatory block parameters before building (if not already covered).\",\"For additional information or examples, please refer to the named files. If you need to add other classes related to the new block, they should be added within the block class or in modules/.\"]},\"151\":{\"h\":\"FAQ\"},\"152\":{\"h\":\"How to build espnet on a cloud machine such as GCP, AWS, etc.?\",\"t\":[\"Our documentation, Installation, assumes that some basic tools are already installed in your machine, gcc, make, etc., so you need also to install them if you don't have them. They are undocumented, but the configuration of our CI may help you because it also builds the environment from scratch with install.sh\"]},\"153\":{\"h\":\"ModuleNotFoundError: No module named 'espnet', or etc.\",\"t\":[\"Firstly, you definitely missed some installation processes. Please read Installation again before posting an issue. If you still have a problem, then please try to manual installation.\",\". tools/activate_python.sh pip install <some-tools> conda install <some-tools>\",\"If you need to install some packages not distributed in pypi, e.g. k2, try to use the installer scripts in espnet.\",\"cd tools ./installers/install_warp-transducer.sh\"]},\"154\":{\"h\":\"To detect the installation problem with a normal installation\",\"t\":[\"Check where your python is\",\"$ . tools/activate_python.sh $ which python # Normally, it should point to <espnet-root>/tools/venv\",\"Check the installation of espnet\",\"$ python >>> import espnet\"]},\"155\":{\"h\":\"ESPnet\"},\"156\":{\"h\":\"How to cite ESPnet\",\"t\":[\"@inproceedings{watanabe18_interspeech, title = {ESPnet: End-to-End Speech Processing Toolkit}, author = {Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai}, year = {2018}, booktitle = {Proc. Interspeech}, pages = {2207--2211}, doi = {10.21437/Interspeech.2018-1456}, issn = {2958-1796}, }\",\"To cite individual modules, models, or recipes, please refer to Additional Citations.\"]},\"157\":{\"h\":\"\"},\"158\":{\"h\":\"Installation\"},\"159\":{\"h\":\"Requirements\",\"t\":[\"Python 3.7+\",\"gcc 4.9+ for PyTorch1.10.2+\",\"(If you'll use a conda environment at the installation step2, the following packages are installed using conda, so you can skip them.)\",\"cmake3 for some extensions\",\"# For Ubuntu $ sudo apt-get install cmake\",\"sox\",\"# For Ubuntu $ sudo apt-get install sox # For CentOS $ sudo yum install sox\",\"flac (This is not required when installing, but used in some recipes)\",\"# For Ubuntu $ sudo apt-get install flac # For CentOS $ sudo yum install flac\"]},\"160\":{\"h\":\"Supported Linux distributions and other requirements\",\"t\":[\"We support the following Linux distributions with CI. If you want to build your own Linux by yourself, please also check our CI configurations to prepare the appropriate environments.\",\"ubuntu18\",\"centos7\",\"debian11\",\"Windows10 (installation only) \",\"We can conduct complete experiments based on WSL-2 (Ubuntu 20.04). See the link and #4909 for details (Thanks, @Bereket-Desbele!)\",\"MacOS12 (installation only)\"]},\"161\":{\"h\":\"Step 1) [Optional] Install Kaldi\",\"t\":[\"If you use ESPnet1 (under egs/), you must compile Kaldi.\",\"If you use ESPnet2 (under egs2/), You can skip the installation of Kaldi.\",\"Click to compile Kaldi...\",\"Related links:\",\"Kaldi Github\",\"Kaldi Documentation\",\"Downloading and installing Kaldi\",\"The build process (how Kaldi is compiled)\",\"Kaldi INSTALL\",\"Kaldi's requirements:\",\"OS: Ubuntu, CentOS, MacOSX, Windows, Cygwin, etc.\",\"GCC >= 4.7\",\"Git clone Kaldi\",\"$ cd <any-place> $ git clone https://github.com/kaldi-asr/kaldi\",\"Install tools\",\"$ cd <kaldi-root>/tools $ make -j <NUM-CPU>\",\"Select BLAS library from ATLAS, OpenBLAS, or MKL\",\"OpenBLAS\",\"$ cd <kaldi-root>/tools $ ./extras/install_openblas.sh\",\"MKL (You need sudo privilege)\",\"$ cd <kaldi-root>/tools $ sudo ./extras/install_mkl.sh\",\"ATLAS (You need sudo privilege)\",\"# Ubuntu $ sudo apt-get install libatlas-base-dev\",\"Compile Kaldi & install\",\"$ cd <kaldi-root>/src # [By default MKL is used] ESPnet uses only a feature extractor, so you can disable CUDA $ ./configure --use-cuda=no # [With OpenBLAS] # $ ./configure --openblas-root=../tools/OpenBLAS/install --use-cuda=no # If you'll use CUDA # ./configure --cudatk-dir=/usr/local/cuda-10.0 $ make -j clean depend; make -j <NUM-CPU>\",\"We also have prebuilt Kaldi binaries.\"]},\"162\":{\"h\":\"Step 2) Installation ESPnet\",\"t\":[\"Git clone ESPnet\",\"$ cd <any-place> $ git clone https://github.com/espnet/espnet\",\"[Optional] Put compiled Kaldi under espnet/tools\",\"If you have compiled Kaldi at Step 1, put it under tools.\",\"$ cd <espnet-root>/tools $ ln -s <kaldi-root> .\",\"Setup Python environment\",\"You must create <espnet-root>/tools/activate_python.sh to specify the Python interpreter used in espnet recipes. (To understand how ESPnet specifies Python, see path.sh for example.)\",\"We also have some scripts to generate tools/activate_python.sh.\",\"Option A) Setup conda environment\",\"$ cd <espnet-root>/tools $ ./setup_miniforge.sh [output-dir-name|default=venv] [conda-env-name|default=root] [python-version|default=none] # e.g. $ ./setup_miniforge.sh miniconda espnet 3.8\",\"This script tries to create a new miniconda if the output directory doesn't exist. If you already have conda and you'll use it, then,\",\"$ cd <espnet-root>/tools $ CONDA_ROOT=${CONDA_PREFIX}/../.. # CONDA_PREFIX is an environment variable set by ${CONDA_ROOT}/etc/profile.d/conda.sh $ ./setup_miniforge.sh ${CONDA_ROOT} [conda-env-name] [python-version] # e.g. $ ./setup_miniforge.sh ${CONDA_ROOT} espnet 3.8\",\"Option B) Setup venv from the system Python\",\"$ cd <espnet-root>/tools $ ./setup_venv.sh $(command -v python3)\",\"Option C) Setup system Python environment\",\"$ cd <espnet-root>/tools $ ./setup_python.sh $(command -v python3)\",\"Option D) Without setting the Python environment\",\"Option C and Option D are almost the same. This option might be suitable for Google colab.\",\"$ cd <espnet-root>/tools $ rm -f activate_python.sh && touch activate_python.sh\",\"Install ESPnet via pyproject.toml\",\"ESPnet adopts the modern Python packaging standard using pyproject.toml. You can install ESPnet with a simple pip command pip install -e ., but the required dependencies vary depending on the task you want to run. Therefore, we recommend installing the appropriate extra dependencies for your specific task. For example:\",\"For ASR:\",\"$ cd <espnet-root> $ pip install -e \\\".[asr]\\\"\",\"For TTS:\",\"$ cd <espnet-root> $ pip install -e \\\".[tts]\\\"\",\"For all tasks (ASR, TTS, enhancement, speaker tasks):\",\"$ cd <espnet-root> $ pip install -e \\\".[all]\\\"\",\"You can also install development tools (linters, test runners) or documentation tools:\",\"$ cd <espnet-root> $ pip install -e \\\".[dev,test,doc]\\\"\",\"Check installation Run the following to validate your installation:\",\"cd tools bash -c \\\". ./activate_python.sh; python3 check_install.py\\\"\",\"[Optional] How dependency groups are organized The ESPnet pyproject.toml defines optional dependency groups:\",\"Group\",\"Purpose\",\"asr\",\"ASR-specific dependencies\",\"asr2\",\"ASR2-specific dependencies\",\"tts\",\"TTS-specific dependencies\",\"enh\",\"Speech enhancement\",\"st\",\"Speech Translation\",\"s2t\",\"Speech to Text (e.g., OWSM)\",\"spk\",\"Speaker recognition\",\"dev\",\"Code formatting and linting\",\"test\",\"Unit test dependencies\",\"doc\",\"Documentation generation\",\"all\",\"All of the above (except dev)\",\"You can mix and match groups as needed:\",\"pip install -e \\\".[asr,tts,test]\\\"\",\"Install ESPnet (Legacy)\",\"$ cd <espnet-root>/tools $ make\",\"The Makefile tries to install ESPnet and all dependencies, including PyTorch. You can also specify the PyTorch version, for example:\",\"$ cd <espnet-root>/tools $ make TH_VERSION=1.10.1\",\"Note that the CUDA version is derived from nvcc command. If you'd like to specify the other CUDA version, you need to give CUDA_VERSION.\",\"$ cd <espnet-root>/tools $ make TH_VERSION=1.10.1 CUDA_VERSION=11.3\",\"If you don't have nvcc command, packages are installed for CPU mode by default. If you'll turn it on manually, give CPU_ONLY option.\",\"$ cd <espnet-root>/tools $ make CPU_ONLY=0\"]},\"163\":{\"h\":\"Step 3) [Optional] Custom tool installation\",\"t\":[\"Some packages used only for specific tasks, e.g., Transducer ASR, Japanese TTS, etc. are not installed by default, so if you meet some installation error when running these recipes, you need to install them optionally.\",\"e.g.\",\"To install Warp Transducer\",\"cd <espnet-root>/tools cuda_root=<cuda-root> # e.g. <cuda-root> = /usr/local/cuda bash -c \\\". activate_python.sh; . ./setup_cuda_env.sh $cuda_root; ./installers/install_warp-transducer.sh\\\"\",\"To install PyOpenJTalk\",\"cd <espnet-root>/tools bash -c \\\". activate_python.sh; ./installers/install_pyopenjtalk.sh\\\"\",\"To install a module using pip: e.g. to install ipython\",\"cd <espnet-root>/tools bash -c \\\". activate_python.sh; pip install ipython\\\"\",\"In addition to the python libraries, you can also install several non-python libraries in the conda environment, e.g.,\",\"cd <espnet-root>/tools bash -c \\\". activate_python.sh; conda install cmake\\\"\"]},\"164\":{\"h\":\"Check installation\",\"t\":[\"You can check whether your installation is successfully finished by\",\"cd <espnet-root>/tools bash -c \\\". ./activate_python.sh; . ./extra_path.sh; python3 check_install.py\\\"\",\"Note that this check is always called in the last stage of the above installation.\"]},\"165\":{\"h\":\"Using job scheduling system\",\"t\":[\"Our recipes support some Job scheduling systems, SGE, PBS/Torque, and Slurm, according to Parallelization in Kaldi. By default, the job runs at local machine. If there are any Job scheduling systems in your environment, you can submit more number of Jobs with multiple machines.\",\"Please ask the administrator to install it if you have multiple machines.\"]},\"166\":{\"h\":\"Select Job scheduler\",\"t\":[\"cmd.sh is a configuration file and it's used by run.sh to set some shell variables. These shell variables should be set as one of following perl scripts:\",\"cmd\",\"Backend\",\"configuration file\",\"run.pl\",\"Local machine (default)\",\"-\",\"queue.pl\",\"Sun grid engine, or grid endine like tool\",\"conf/queue.conf\",\"slurm.pl\",\"Slurm\",\"conf/slurm.conf\",\"pbs.pl\",\"PBS/Torque\",\"conf/pbs.conf\",\"ssh.pl\",\"SSH\",\".queue/machines\"]},\"167\":{\"h\":\"Usage of run.pl\",\"t\":[\"run.pl, queue.pl, slurm.pl, pbs.pl and ssh.pl have a unified interface, therefore we can assign any one of them to ${cmd} in the shell script:\",\"nj=4 ${cmd} JOB=1:${nj} JOB.log echo JOB\",\"JOB=1:${nj} indicates the parallelization, which is known as \\\"array-job\\\", with ${nj} number of jobs. JOB.log is a destination of the stdout and stderr from jobs. The string of JOB will be changed to the job number if it's included in the log file name or command line arguments. i.e. The following commands are almost equivalent to the above:\",\"echo 1 &> 1.log & echo 2 &> 2.log & echo 3 &> 3.log & echo 4 &> 4.log & wait\"]},\"168\":{\"h\":\"Configuration\",\"t\":[\"You also need to modify the configuration file for a specific job scheduler to change command-line options to submit jobs e.g. queue setting, resource request, etc.\",\"The following text is an example of conf/queue.conf.\",\"# Default configuration command qsub -v PATH -cwd -S /bin/bash -j y -l arch=*64* option mem=* -l mem_free=$0,ram_free=$0 option mem=0 # Do not add anything to qsub_opts option num_threads=* -pe smp $0 option num_threads=1 # Do not add anything to qsub_opts option max_jobs_run=* -tc $0 default gpu=0 option gpu=0 option gpu=* -l gpu=$0 -q g.q\",\"Note that the queue/partition name, -q g.q, is an example, so you must change it to the existing queue/partition in your cluster.\",\"You can't use the specific options depending on each system in our scripts, e.g. you can't use -q option for queue.pl directly. Instead, you can use --mem, --num_threads, --max_jobs_run, and --gpu in this case.\",\"Take a look at the following:\",\"option gpu=* -l gpu=$0 -q g.q\",\"This line means that the optional argument specified by the second column, gpu=*, will be converted to the options after it: -l gpu=$0 -q g.q:\",\"queue.pl --gpu 2\",\"will be converted to\",\"qsub -v PATH -cwd -S /bin/bash -j y -l arch=*64* -l gpu=2 -q g.q\",\"You can also add a new option for your system using this syntax.\",\"option foo=* --bar $0\"]},\"169\":{\"h\":\"\"},\"170\":{\"h\":\"Common usages\"},\"171\":{\"h\":\"ESPnet1\",\"t\":[\"Please first check ESPnet1 tutorial\"]},\"172\":{\"h\":\"ESPnet2\",\"t\":[\"Please first check ESPnet2 tutorial\"]},\"173\":{\"h\":\"Multiple GPU TIPs\",\"t\":[\"Note that if you want to use multiple GPUs, the installation of nccl is required before setup.\",\"Currently, espnet1 only supports multiple GPU training within a single node. The distributed setup across multiple nodes is only supported in espnet2.\",\"We don't support multiple GPU inference. Instead, please split the recognition task for multiple jobs and distribute these split jobs to multiple GPUs.\",\"If you cannot get enough speed improvement with multiple GPUs, you should first check the GPU usage by nvidia-smi. If the GPU-Util percentage is low, the bottleneck will come from disk access. You can apply data prefetching by --n-iter-processes 2 in your run.sh to mitigate the problem. Note that this data prefetching consumes a lot of CPU memory, so please be careful when you increase the number of processes.\",\"The behavior of batch size in ESPnet2 during multi-GPU training is different from that in ESPnet1. In ESPnet2, the total batch size is not changed regardless of the number of GPUs. Therefore, you need to manually increase the batch size if you increase the number of GPUs. Please refer to this doc for more information.\"]},\"174\":{\"h\":\"Start from the middle stage or stop at the specified stage\",\"t\":[\"run.sh has multiple stages, including data preparation, training, etc., so you may likely want to start from the specified stage if some stages failed for some reason, for example.\",\"You can start from the specified stage as follows and stop the process at the specified stage:\",\"# Start from 3rd stage and stop at 5th stage $ ./run.sh --stage 3 --stop-stage 5\"]},\"175\":{\"h\":\"CTC, attention, and hybrid CTC/attention\",\"t\":[\"ESPnet can easily switch the model's training/decoding mode from CTC, attention, and hybrid CTC/attention.\",\"Each mode can be trained by specifying mtlalpha (espnet1) ctc_weight (espnet2):\",\"espnet1\",\"# hybrid CTC/attention (default) mtlalpha: 0.3 # CTC mtlalpha: 1.0 # attention mtlalpha: 0.0\",\"espnet2\",\"# hybrid CTC/attention (default) model_conf: ctc_weight: 0.3 # CTC model_conf: ctc_weight: 1.0 # attention model_conf: ctc_weight: 0.0\",\"Decoding for each mode can be done using the following decoding configurations:\",\"espnet1\",\"# hybrid CTC/attention (default) ctc-weight: 0.3 beam-size: 10 # CTC ctc-weight: 1.0 ## for best path decoding api: v1 # default setting (can be omitted) ## for prefix search decoding w/ beam search api: v2 beam-size: 10 # attention ctc-weight: 0.0 beam-size: 10 maxlenratio: 0.8 minlenratio: 0.3\",\"espnet2\",\"# hybrid CTC/attention (default) ctc_weight: 0.3 beam_size: 10 # CTC ctc_weight: 1.0 beam_size: 10 # attention ctc_weight: 0.0 beam_size: 10 maxlenratio: 0.8 minlenratio: 0.3\",\"The CTC mode does not compute the validation accuracy, and the optimum model is selected with its loss value, e.g.,\",\"espnet1\",\"best_model_criterion: - - valid - cer_ctc - min\",\"espnet2\",\"./run.sh --recog_model model.loss.best\",\"The pure attention mode requires setting the maximum and minimum hypothesis length (--maxlenratio and --minlenratio) appropriately. In general, if you have more insertion errors, you can decrease the maxlenratio value, while if you have more deletion errors, you can increase the minlenratio value. Note that the optimum values depend on the ratio of the input frame and output label lengths, which are changed for each language and each BPE unit.\",\"Negative maxlenratio can be used to set the constant maximum hypothesis length independently from the number of input frames. If maxlenratio is set to -1, the decoding will always stop after the first output, which can be used to emulate the utterance classification tasks. This is suitable for some spoken language understanding and speaker identification tasks.\",\"About the effectiveness of hybrid CTC/attention during training and recognition, see [2] and [3]. For example, hybrid CTC/attention is not sensitive to the above maximum and minimum hypothesis heuristics.\"]},\"176\":{\"h\":\"ESPnet Notebooks\"},\"177\":{\"h\":\"Demo\"},\"178\":{\"h\":\"ASR (Speech recognition)\",\"t\":[\"asr_realtime_demo.ipynb: ASR realtime inference with various pre-trained models.\",\"asr_transfer_learning_demo.ipynb: Demo on how to use pre-trained ASR models for fine-tuning.\",\"streaming_asr_demo.ipynb: Streaming ASR realtime inference with pre-trained models.\"]},\"179\":{\"h\":\"SE (Speech enhancement/separation)\",\"t\":[\"se_demo.ipynb: Speech enhancement/separation inference with various pre-trained models.\",\"se_demo_for_waspaa_2021.ipynb: WASPAA2021 version of ESPnet-SE demo.\"]},\"180\":{\"h\":\"SLU (Spoken language understanding)\",\"t\":[\"2pass_slu_demo.ipynb: Two pass spoken language understanding pre-trained model examples.\"]},\"181\":{\"h\":\"TTS (Text-to-speech)\",\"t\":[\"tts_realtime_demo.ipynb: TTS realtime inference with various pre-trained models.\"]},\"182\":{\"h\":\"Other utilities\",\"t\":[\"onnx_conversion_demo.ipynb: How to convert ESPnet models into ONNX format.\"]},\"183\":{\"h\":\"ESPnet-EZ\"},\"184\":{\"h\":\"ASR (Speech recognition)\",\"t\":[\"train_from_scratch.ipynb: Training an ASR model with ESPnet-EZ on LibriSpeech-100.\",\"ASR_finetune_owsm.ipynb: Fine-tuning the weakly-supervised model (OWSM) with ESPnet-EZ on custom dataset.\"]},\"185\":{\"h\":\"ST (Speech-to-text translation)\",\"t\":[\"integrate_huggingface.ipynb: Integrating the weakly-supervised model (OWSM) and huggingface's pre-trained language model with ESPnet-EZ on MuST-C-v2.\",\"ST_finetune_owsm.ipynb: Fine-tuning the weakly-supervised model (OWSM) with ESPnet-EZ on MuST-C-v2.\"]},\"186\":{\"h\":\"SLU (Spoken language understanding)\",\"t\":[\"SLU_finetune_owsm.ipynb: Fine-tuning the weakly-supervised model (OWSM) with ESPnet-EZ on SLURP.\"]},\"187\":{\"h\":\"TTS (Text-to-speech)\",\"t\":[\"TTS_finetune_vctk_dump.ipynb: Fine-tuning a pre-trained VITS model with ESPnet-EZ on the VCTK dataset.\"]},\"188\":{\"h\":\"SVS (Singing voice synthesis)\",\"t\":[\"SVS_finetune_ace-kising.ipynb: Fine-tuning a pre-trained VISinger 2 model with ESPnet-EZ on ACE-KiSing.\"]},\"189\":{\"h\":\"Course\"},\"190\":{\"h\":\"CMU SpeechProcessing Spring2023\",\"t\":[\"assignment0_data-prep.ipynb: Course assignment on how to prepare ESPnet-format data.\",\"assignment1_espnet-tutorial.ipynb: A simplified version of previous year's new task tutorial.\",\"assignemnt3_spk.ipynb: Examples of using ESPnet to extract speaker embeddings and conduct speaker recognition.\",\"assignment4_ssl.ipynb: Exploration on using self-supervised speech representation to ESPnet ASR training.\",\"assignment5_st.ipynb: Examples of state-of-the-art speech translation models in ESPnet.\",\"assignment6_slu.ipynb: Examples of state-of-the-art spoken language understanding models in ESPnet.\",\"assignment7_se.ipynb: Examples of state-of-the-art speech enhancement/separation in ESPnet.\",\"assignment8_tts.ipynb: A student version of espnet2-tts realtime demonstration.\",\"s2st_demo.ipynb: An example of existing speech-to-speech translation model for ESPnet.\"]},\"191\":{\"h\":\"CMU SpeechRecognition Fall2022\",\"t\":[\"recipe_tutorial.ipynb: A general tutorial of stage-by-stage explanation of ESPnet2 recipes (with new functions).\",\"new_task_tutorial.ipynb: A tutorial on how to add new models/tasks to ESPnet framework.\"]},\"192\":{\"h\":\"CMU SpeechRecognition Fall2021\",\"t\":[\"general_tutorial.ipynb: A general tutorial of stage-by-stage explanation of ESPnet2 recipes.\"]},\"193\":{\"h\":\"ESPnet1 (Legacy)\",\"t\":[\"asr_library.ipynb: Speech recognition library explanation with network training.\",\"asr_recipe.ipynb: Speech recognition recipe explanation.\",\"pretrained.ipynb: Tutorial on how to use pre-trained models.\",\"st_demo.ipynb: Speech translation demonstration with a TTS model to achieve speech-to-speech translation.\",\"tts_realtime_demo.ipynb: TTS demonstration with different pre-trained TTS models.\",\"tts_recipe.ipynb: Stage explanation for TTS recipes.\"]},\"194\":{\"h\":\"Recipe Template\",\"t\":[\"Recipe template is used to build recipes easily. It is designed to support the common functionalities and requirements that each individual tasks often has.\",\"Table of Contentsgenerated with DocToc\",\"Recipe Template\",\"Run ESPnet with your own corpus\",\"About Kaldi style data directory\",\"(For developers) How to make/port new recipe?\"]},\"195\":{\"h\":\"Run ESPnet with your own corpus\",\"t\":[\"Copying a template directory\",\"% task=asr1 # enh1, tts1, mt1, st1 % egs2/TEMPLATE/${task}/setup.sh egs2/foo/${task}\",\"Create egs2/foo/${task}/data directory to put your corpus: See https://github.com/espnet/data_example or next section.\",\"Run (e.g. asr case)\",\"cd egs2/foo/${task} # We always assume that our scripts are executed at this directory. # Assuming Stage1 creating `data`, so you can skip it if you have `data`. ./asr.sh \\\\ --stage 2 \\\\ --ngpu 1 \\\\ --train_set train \\\\ --valid_set valid \\\\ --test_sets \\\"test\\\" \\\\ --lm_train_text \\\"data/train/text\\\" # Use CUDA_VISIBLE_DEVICES to specify a gpu device id # If you meet CUDA out of memory error, change `batch_bins` ( or `batch_size`)\",\"For more detail\",\"Read the config files: e.g. https://github.com/espnet/espnet/tree/master/egs2/librispeech/asr1/conf\",\"Read the main script: e.g. https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/asr1/asr.sh\",\"Documentation: https://espnet.github.io/espnet/\"]},\"196\":{\"h\":\"About Kaldi style data directory\",\"t\":[\"Each directory of training set, development set, and evaluation set, has same directory structure. See also http://kaldi-asr.org/doc/data_prep.html about Kaldi data structure. We recommend you running mini_an4 recipe and checking the contents of data/ by yourself.\",\"cd egs2/mini_an4/asr1 ./run.sh\",\"Directory structure\",\"data/ train/ - text # The transcription - wav.scp # Wave file path - utt2spk # A file mapping utterance-id to speaker-id - spk2utt # A file mapping speaker-id to utterance-id - segments # [Option] Specifying start and end time of each utterance dev/ ... test/ ...\",\"text format\",\"uttidA &lt;transcription&gt; uttidB &lt;transcription&gt; ...\",\"wav.scp format\",\"uttidA /path/to/uttidA.wav uttidB /path/to/uttidB.wav ...\",\"utt2spk format\",\"uttidA speakerA uttidB speakerB uttidC speakerA uttidD speakerB ...\",\"spk2utt format\",\"speakerA uttidA uttidC ... speakerB uttidB uttidD ... ...\",\"Note that spk2utt file can be generated by utt2spk, and utt2spk can be generated by spk2utt, so it's enough to create either one of them.\",\"utils/utt2spk_to_spk2utt.pl data/train/utt2spk > data/train/spk2utt utils/spk2utt_to_utt2spk.pl data/train/spk2utt > data/train/utt2spk\",\"If your corpus doesn't include speaker information, give the same speaker id as the utterance id to satisfy the directory format, otherwise give the same speaker id for all utterances (Actually we don't use speaker information for asr recipe now).\",\"uttidA uttidA uttidB uttidB ...\",\"OR\",\"uttidA dummy uttidB dummy ...\",\"[Option] segments format\",\"If the audio data is originally long recording, about > ~1 hour, and each audio file includes multiple utterances in each section, you need to create segments file to specify the start time and end time of each utterance. The format is &lt;utterance_id&gt; &lt;wav_id&gt; &lt;start_time&gt; &lt;end_time&gt;.\",\"sw02001-A_000098-001156 sw02001-A 0.98 11.56 ...\",\"Note that if using segments, wav.scp has &lt;wav_id&gt; which corresponds to the segments instead of utterance_id.\",\"sw02001-A /path/to/sw02001-A.wav ...\",\"Once you complete creating the data directory, it's better to check it by utils/validate_data_dir.sh.\",\"utils/validate_data_dir.sh --no-feats data/train utils/validate_data_dir.sh --no-feats data/dev utils/validate_data_dir.sh --no-feats data/test\"]},\"197\":{\"h\":\"(For developers) How to make/port new recipe?\",\"t\":[\"ESPnet2 doesn't prepare different recipes for each corpus unlike ESPnet1, but we prepare common recipes for each task, which are named as asr.sh, enh.sh, tts.sh, or etc. We carefully designed these common scripts to perform with any types of corpus, so ideally you can train using your own corpus without modifying almost all parts of these recipes. Only you have to do is just creating local/data.sh.\",\"Create directory in egs/\",\"% task=asr1 # enh1, tts1, mt1, st1 % egs2/TEMPLATE/${task}/setup.sh egs2/foo/${task}\",\"Create run.sh and local/data.sh somehow\",\"% cd egs2/foo/${task} % cp ../../mini_an4/${task}/run.sh . % vi run.sh\",\"run.sh is a thin wrapper of a common recipe for each task as follows,\",\"# The contents of run.sh ./asr.sh \\\\ --train_set train \\\\ --valid_set dev \\\\ --test_sets \\\"dev test1 test2\\\" \\\\ --lm_train_text \\\"data/train/text\\\" \\\"$@\\\"\",\"We use a common recipe, thus you must absorb the difference of each corpus by the command line options of asr.sh.\",\"We expect that local/data.sh generates training data (e.g., data/train), validation data (e.g., data/dev), and (multiple) test data (e.g, data/test1 and data/test2), which have Kaldi style (See stage1 of asr.sh).\",\"Note that some corpora only provide the test data and would not officially prepare the development set. In this case, you can prepare the validation data by extracting the part of the training data and regard the rest of training data as a new training data by yourself (e.g., check egs2/csj/asr1/local/data.sh).\",\"Also, the validation data used during training must be a single data directory. If you have multiple validation data directories, you must combine them by using utils/combine_data.sh.\",\"On the other hand, the recipe accepts multiple test data directories during inference. So, you can include the validation data to evaluate the ASR performance of the validation data.\",\"If you'll create your recipe from scratch, you have to understand Kaldi data structure. See the next section.\",\"If you'll port the recipe from ESPnet1 or Kaldi, you need to embed the data preparation part of the original recipe in local/data.sh. Note that the common steps include Feature extraction, Speed Perturbation, and Removing long/short utterances, so you don't need to do them at local/data.sh\",\"If the recipe uses some corpora and they are not listed in db.sh, then write it.\",\"... YOUR_CORPUS= ...\",\"If the recipe depends on some special tools, then write the requirements to local/path.sh\",\"path.sh:\",\"# e.g. flac command is required if ! which flac &> /dev/null; then echo \\\"Error: flac is not installed\\\" return 1 fi\"]},\"198\":{\"h\":\"Automatic Speech Recognition (Multi-tasking)\",\"t\":[\"This is a template of ASR1 Multi-tasking recipe for ESPnet2. This README provides comprehensive instructions on how to enhance ASR1 for prompt-based multi-task learning.\"]},\"199\":{\"h\":\"Table of Contents\",\"t\":[\"Recipe flow\",\"1. Data preparation\",\"2. Speed perturbation\",\"3. Generate dump folder\",\"4. Removal of long / short data\",\"5. Input / Output Token list generation\",\"6. LM statistics collection\",\"7. LM training\",\"8. LM perplexity\",\"9. Ngram-LM training\",\"10. ASR statistics collection\",\"11. ASR training\",\"12. ASR inference\",\"13. ASR scoring\",\"14-16. (Optional) Pack results for upload\",\"How to run\",\"SLU Multi-task training\",\"Related works\"]},\"200\":{\"h\":\"Recipe flow\",\"t\":[\"ASR1 recipe consists of 13 stages.\",\"Data preparation\",\"Data preparation stage.\",\"ESPnet format:\",\"It calls local/data.sh to creates Kaldi-style data directories in data/ for training, validation, and evaluation sets. In addition to the files in the asr1 recipe, it generates an additional file called prompt that specifies the task to be performed for the given utterance..\",\"prompt format\",\"uttidA &lt;prompt&gt; uttidB &lt;prompt&gt; ...\",\"See also:\",\"About Kaldi-style data directory\",\"Speed perturbation\",\"Augment training data with speed perturbation. data/${train_set}_spXX would be generated (XX means the speed factor). This step is optional.\",\"Generate dump folder\",\"Dumping stage. This stage move necessary files for training from data folder to dump folder.\",\"Removal of long / short data\",\"This stage is the same as that in ASR recipes. At this stage, the dump directories for all datasets on which multi-tasking is to be performed are merged by simple concatenation.\",\"Input / Output Token list generation\",\"Token list (BPE / Char / etc) generation for both input and targets. Additionally, for Whisper tokenization, you have the option to incorporate special tokens into the Whisper vocabulary using the --nlsyms_txt flag. If you are utilizing task specifiers for prompt-based multi-tasking, similar to the original Whisper formulation, it is necessary to include these task specifiers in the Whisper vocabulary.\",\"LM statistics collection\",\"Neural-network (NN) based Language model (LM) is optional for ASR task. You can skip stage 5-8 by setting --use_lm false. Statistics calculation stage. It collects the shape information of LM texts and calculates statistics for LM training.\",\"LM training\",\"NN-based LM model training stage. You can change the training setting via --lm_config and --lm_args options.\",\"See also:\",\"Supported models.\",\"Change the configuration for training\",\"Distributed training\",\"LM perplexity\",\"NN-based LM evaluation stage. Perplexity (PPL) is computed against the trained model\",\"See also:\",\"Change the configuration for training\",\"N-gram LM training\",\"N-gram-based LM model training stage.\",\"ASR statistics collection\",\"Statistics calculation stage. It collects the shape information of input and output texts for ASR training.\",\"Prompt based multi-tasking\",\"Instructions: \",\"To enable prompt-based multi-task learning across multiple tasks in English, ensure that --use_prompt is set to True. By default, this setting replaces the task specifier in the Whisper formulation with the one specified in the prompt file to perform multi-task learning across multiple tasks in English. Please refer to stage 5 for instructions on adding task specifiers to the Whisper vocabulary.\",\"If you want to perform prompt-based multi-task learning across multiple tasks in multiple languages, additionally, set --use_lang_prompt to true. This step replaces both the language and task specifiers in the Whisper formulation with those specified in the prompt file and can also introduce a new dataset specifier. Please ensure that task, dataset, and language specifiers are all included in the Whisper vocabulary for this option to work.\",\"(Optional) To use natural language phrases for prompt-based multi-tasking, set --use_nlp_prompt to true. In this case, you do not need to make any modifications to the Whisper vocabulary.\",\"ASR training\",\"ASR model training stage. You can change the training setting via --asr_config and --asr_args options. You need to follow similar steps as described in stage 10 to perform prompt based multi-task learning.\",\"See also:\",\"Supported models.\",\"Change the configuration for training\",\"Distributed training\",\"ASR inference\",\"ASR inference stage.\",\"Prompt based multi-tasking\",\"Instructions: \",\"If you have incorporated any special tokens into the Whisper vocabulary, make sure to specify the file containing these special tokens as prompt_token_file in decoder config.\",\"If you are utilizing task, language, and dataset specifiers, please specify these specifiers as lang_prompt_token in decoder config.\",\"If you are employing a natural language phrase as a prompt, specify the phrase as nlp_prompt_token in decoder config.\",\"To perform language identification and voice activity detection, we follow the Whisper's pre-training setupwhere we predict language id and no speech tags immediately after the start of the transcript tag. Hence for these tasks, set lid prompt to true.\",\"ASR scoring\",\"ASR scoring stage: error rates (char / word / token) are computed.\",\"(Optional) Pack results for upload\",\"Packing stage. It packs the trained model files to prepare for uploading to Hugging Face.\",\"See also:\",\"ESPnet Model Zoo\",\"15: (Optional) Upload model\",\"Upload the trained model to Hugging Face for sharing. Additional information at Docs.\"]},\"201\":{\"h\":\"How to run\",\"t\":[\"SLU-Multi-task-training\",\"Here, we show the procedure to run multi-tasking learning across 14 speech classification tasks.\",\"Create a dump directory using the following recipes: . asvspoof, speechcommands, grabo, lt_speech_commands, arabic_sc, fsc, voxforge/lid1, iemocap, accentdb, mustard, mustard_plus_plus, voxceleb1, freesound and esc50. You can do this by running the following command in each of these recipes:\",\"$ ./run.sh --stop_stage 4\",\"Note: Download all the dataset zip files first before creating dump directory. Please refer to https://github.com/ga642381/SpeechPrompt-v2/blob/main/docs/dataset.md to download all datasets.\",\"Move to the egs2/uslu14/asr1 recipe directory. Generate the prompt file by running\",\"$ python local/create_*_prompt.py\",\"Concatenate wav.scp, prompt, text, utt2spk, spk2utt, utt2num_samples from all train and valid dump folders in each of the dump directories and create two new directories, dump/raw/train_combined and dump/raw/valid to contain the combined data. Start training using:\",\"$ ./run.sh --stage 5 --stop_stage 11\",\"Run decoding for each of the datasets, i.e., test_&lt;dataset&gt;, with the specified inference_config, e.g., conf/decode_asr_&lt;task&gt;.yaml, using the following command:\",\"$ ./run.sh --stage 12 --stop_stage 12 --inference_config conf/decode_asr_&lt;task&gt;.yaml --test_sets test_&lt;dataset&gt;\",\"For some tasks, you need to clean prediction files using python local/clean_emotion_pred.py, python local/check_lid_results.py, python local/check_vad_results.py. To get accuracy, run\",\"$ ./run.sh --stage 13 --stop_stage 13 --inference_config conf/decode_asr_&lt;task&gt;.yaml --test_sets test_&lt;dataset&gt;\",\"For tasks where you need to compute f1 or weighted_f1, run python local/compute_f1.py and python local/compute_weighted_f1.py.\"]},\"202\":{\"h\":\"Related works\",\"t\":[\" @misc{arora2023universlu, title={UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network}, author={Siddhant Arora and Hayato Futami and Jee-weon Jung and Yifan Peng and Roshan Sharma and Yosuke Kashiwagi and Emiru Tsunoo and Shinji Watanabe}, year={2023}, eprint={2310.02973}, archivePrefix={arXiv}, primaryClass={cs.CL} } @InProceedings{pmlr-v202-radford23a, title = {Robust Speech Recognition via Large-Scale Weak Supervision}, author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and Mcleavey, Christine and Sutskever, Ilya}, booktitle = {Proceedings of the 40th International Conference on Machine Learning}, pages = {28492--28518}, year = {2023}, editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan}, volume = {202}, series = {Proceedings of Machine Learning Research}, month = {23--29 Jul}, publisher = {PMLR}, }\"]},\"203\":{\"h\":\"Automatic Speech Recognition with Discrete Units\",\"t\":[\"This is a template of ASR2 recipe for ESPnet2. The difference from ASR1 is that discrete tokens are used as input instead of conventional audios / spectrum features.\"]},\"204\":{\"h\":\"Table of Contents\",\"t\":[\"Recipe flow\",\"1. Data preparation\",\"2. Speed perturbation\",\"3. Wav format\",\"4. Removal of long / short data\",\"5. Generate discrete tokens\",\"6. Generate dump raw folder\",\"7. Input and Output token list generation\",\"8. LM statistics collection\",\"9. LM training\",\"10. LM perplexity\",\"11. Ngram-LM training\",\"12. ASR statistics collection\",\"13. ASR training\",\"14. ASR inference\",\"15. ASR scoring\",\"16-18. (Optional) Pack results for upload\",\"How to run\",\"LibriSpeech training\",\"Related works\"]},\"205\":{\"h\":\"Recipe flow\",\"t\":[\"ASR2 recipe consists of 15 stages.\",\"Data preparation\",\"Data preparation stage.\",\"ESPnet format:\",\"It calls local/data.sh to creates Kaldi-style data directories in data/ for training, validation, and evaluation sets. It's the same as asr1 tasks.\",\"See also:\",\"About Kaldi-style data directory\",\"Speed perturbation\",\"Augment training data with speed perturbation. data/${train_set}_spXX would be generated (XX means the speed factor). This step is optional.\",\"Wav format\",\"Format the wave files in wav.scp to a single format (wav / flac / kaldi_ark).\",\"Removal of long / short data\",\"Remove utterances by the following conditions\",\"Too short / long utterances.\",\"0-length in target text.\",\"Generate discrete tokens\",\"The discrete tokens of the input speech signals are generated. For ASR2 task, the input is the discrete tokens (from self-supervised learning (SSL) features) and the target is the ASR transcriptions. After getting the discrete tokens (usually in integers), they will be converted to CJK characters, which are more convenient in tokenization.\",\"Input / Target / Process of data preparation\",\"Stages: \",\"Generate SSL features for train / valid / test sets.\",\"Train the K-Means model on a subset from training data.\",\"Generate K-Means-based discrete tokens for train / valid / test sets.\",\"(Optional) Measure the discrete tokens quality if forced-alignment is accessible.\",\"Generate dump raw folder\",\"This stage move necessary files for training from dump/extracted folder to dump/raw folder.\",\"Input and Output token list generation\",\"Token list (BPE / Char / etc) generation for both input and targets.\",\"LM statistics collection\",\"Neural-network (NN) based Language model (LM) is optional for ASR task. You can skip stage 5-8 by setting --use_lm false. Statistics calculation stage. It collects the shape information of LM texts and calculates statistics for LM training.\",\"LM training\",\"NN-based LM model training stage. You can change the training setting via --lm_config and --lm_args options.\",\"See also:\",\"Supported models.\",\"Change the configuration for training\",\"Distributed training\",\"LM perplexity\",\"NN-based LM evaluation stage. Perplexity (PPL) is computed against the trained model\",\"See also:\",\"Change the configuration for training\",\"N-gram LM training\",\"N-gram-based LM model training stage.\",\"ASR statistics collection\",\"Statistics calculation stage. It collects the shape information of input and output texts for ASR training.\",\"ASR training\",\"ASR model training stage. You can change the training setting via --asr_config and --asr_args options.\",\"See also:\",\"Supported models.\",\"Change the configuration for training\",\"Distributed training\",\"ASR inference\",\"ASR inference stage.\",\"ASR scoring\",\"ASR scoring stage: error rates (char / word / token) are computed.\",\"(Optional) Pack results for upload\",\"Packing stage. It packs the trained model files to prepare for uploading to Hugging Face.\",\"See also:\",\"ESPnet Model Zoo\",\"17: (Optional) Upload model\",\"Upload the trained model to Hugging Face for sharing. Additional information at Docs.\"]},\"206\":{\"h\":\"How to run\",\"t\":[\"LibriSpeech Training\",\"Here, we show the procedure to run the recipe using egs2/librispeech/asr2.\",\"Move on the recipe directory.\",\"$ cd egs2/librispeech/asr2\",\"Modify LIBRISPEECH variable in db.sh if you want to change the download directory.\",\"$ vim db.sh\",\"Modify cmd.sh and conf/*.conf if you want to use job scheduler. See the detail in using job scheduling system.\",\"$ vim cmd.sh\",\"Run run.sh, which conducts all of the stages explained above.\",\"$ ./run.sh\"]},\"207\":{\"h\":\"Related works\",\"t\":[\"@INPROCEEDINGS{9054224, author={Baevski, Alexei and Mohamed, Abdelrahman}, booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Effectiveness of Self-Supervised Pre-Training for ASR}, year={2020}, volume={}, number={}, pages={7694-7698}, doi={10.1109/ICASSP40776.2020.9054224}} @article{chang2023exploration, title={Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning}, author={Chang, Xuankai and Yan, Brian and Fujita, Yuya and Maekaku, Takashi and Watanabe, Shinji}, journal={arXiv preprint arXiv:2305.18108}, year={2023} }\"]},\"208\":{\"h\":\"Speaker Verification Spoofing and Countermeasures\",\"t\":[\"This recipe is part of the tutorial for adding new tasks, models, and recipes to ESPnet. Refer to the tutorial and the corresponding Jupyter notebook for more details.\"]},\"209\":{\"h\":\"Classification\",\"t\":[\"This is a template of cls1 recipe for ESPnet2.\"]},\"210\":{\"h\":\"Table of Contents\",\"t\":[\"Recipe flow\",\"1. Database-dependent data preparation\",\"2. Wav dump preparation\",\"3. Filtering\",\"4. Token list generation\",\"5. CLS statistics collection\",\"6. CLS training\",\"7. CLS inference\",\"8. Scoring\",\"9. Model packing\",\"10. Upload to HuggingFace\",\"How to run\",\"Evaluation\",\"About data directory\",\"Problems you might encounter\",\"1. Torcheval not found\",\"Supported Models\"]},\"211\":{\"h\":\"Recipe flow\",\"t\":[\"CLS recipe consists of 10 stages.\",\"Database-dependent data preparation\",\"Data preparation stage. It calls local/data.sh to creates Kaldi-style data directories for training, validation, and evaluation sets.\",\"See also:\",\"About data directory\",\"Score preparation\",\"Wav dump preparation\",\"This recipe supports --feats_type raw option. This means we will run a wav dumping stage which reformats wav.scp in data directories. This process standardizes all data to common sampling rate and data format.\",\"Filtering\",\"Filtering stage. Processing stage to remove long and short utterances from the training and validation sets. You can change the threshold values via --min_wav_duration and --max_wav_duration.\",\"Empty text will also be removed. If your audio sample lacks a label in multi-label setting then use the &lt;blank&gt; symbol. TODO(shikhar): This feature will be supported in a later PR.\",\"Token list generation\",\"Token list generation stage. It generates token list (dictionary) from text file. We only support --token_type=word option. This means that each unique space-separated word in the text file becomes a class/label for classification. Please note that this process is case-sensitive.\",\"NOTE: Data preparation will end in stage 4. You can skip data preparation (stage 1 ~ stage 4) via --skip_data_prep option.\",\"CLS statistics collection\",\"Statistics calculation stage. It collects the shape information of the input and output and calculates statistics for feature normalization (mean and variance over training and validation sets).\",\"CLS training\",\"Classification model training stage. You can change the training setting via --train_config and --cls_args options.\",\"See also:\",\"Supported models.\",\"Change the configuration for training\",\"Distributed training\",\"Training process will end in stage 6. You can skip training process (stage 5 ~ stage 6) via --skip_train option.\",\"CLS inference\",\"Classification model decoding stage. This stage outputs two files: text and score.\",\"Example text file as20k-eval-0 Music as20k-eval-1\",\"For multi-class classification each row will have exactly one class. For multi-label classification each row can have any number of labels (zero or more). The above example is a multi-label output text file.\",\"Example score file as20k-eval-0 0.5590277314186096 0.451458394527435 ... as20k-eval-1 0.00023992260685190558 0.00012396479723975062 ...\",\"Each row of both multi-class and multi-label classification models will have probabilities for all tokens (in the same order as they are present in the token_list).\",\"We use a threshold of 0.5 for multi-label classification, and use argmax for multi-class classification. You can choose to just produce probabilities for the predicted class/labels in the score file with output_all_probabilities=false flag.\",\"Scoring\",\"Evaluation stage. It produces mAP and accuracy metrics.\",\"Model packing\",\"Packing stage. It packs the trained model files. Set skip_upload to False.\",\"Model upload\",\"Upload stage. It uploads the trained model files. Provide hf_repo and set skip_upload to False.\"]},\"212\":{\"h\":\"How to run\",\"t\":[\"TOOD(shikhar): Change this to a recipe which downloads data (perhaps beans) later.\",\"Here, we show the procedure to run the recipe using egs2/as20k/cls1.\",\"Move on the recipe directory.\",\"$ cd egs2/as20k/cls1\",\"Modify AUDIOSET variable in db.sh to specify location where you have the AudioSet dataset.\",\"$ vim db.sh\",\"Modify cmd.sh and conf/*.conf if you want to use job scheduler. See the detail in using job scheduling system.\",\"$ vim cmd.sh\",\"Run run.sh, which conducts all of the stages explained above.\",\"$ ./run.sh\",\"For the first time, we recommend performing each stage step-by-step via --stage and --stop_stage options.\",\"$ ./run.sh --stage 1 --stop_stage 1 $ ./run.sh --stage 2 --stop_stage 2 ... $ ./run.sh --stage 7 --stop_stage 7\",\"This might help you understand each stage's processing and directory structure.\",\"Evaluation\",\"Here we show the example command to calculate classification metrics:\",\" cd egs2/&lt;recipe_name&gt;/cls1 . ./path.sh python3 pyscripts/utils/cls_score.py \\\\ -gtxt data/text \\\\ -ptxt exp/cls_&lt;split&gt;/text \\\\ -pscore exp/cls_&lt;split&gt;/score \\\\ -tok data/token_list\"]},\"213\":{\"h\":\"About data directory\",\"t\":[\"Each directory of training set, development set, and evaluation set, has same directory structure. See also https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE#about-kaldi-style-data-directory about Kaldi data structure.\",\"Directory structure\",\"data/ ├── train/ # Training set directory │ ├── text # The transcription │ ├── wav.scp # Wave file path │ ├── utt2spk # A file mapping utterance-id to speaker-id │ ├── spk2utt # A file mapping speaker-id to utterance-id | ├── dev/ │ ... ├── eval/ │ ... └── token_list # token list file ...\",\"text format\",\"uttidA &lt;class_a&gt; uttidB &lt;class_b1&gt; &lt;class_b2&gt; ...\",\"Note that for multi-class classification each uttid should be associated with exactly one class. For multi-label classification, each uttid should have at least one label. (TODO) We will support the case with no label in the future with the &lt;blank&gt; symbol.\",\"wav.scp format\",\"uttidA /path/to/uttidA.wav uttidB /path/to/uttidB.wav ...\",\"utt2spk format\",\"uttidA speakerA uttidB speakerB uttidC speakerA uttidD speakerB ...\",\"spk2utt format\",\"speakerA uttidA uttidC ... speakerB uttidB uttidD ... ...\",\"Note that spk2utt file can be generated by utt2spk, and utt2spk can be generated by spk2utt, so it's enough to create either one of them.\",\"utils/utt2spk_to_spk2utt.pl data/train/utt2spk > data/train/spk2utt utils/spk2utt_to_utt2spk.pl data/train/spk2utt > data/train/utt2spk\",\"If your corpus doesn't include speaker information, give the same speaker id as the utterance id to satisfy the directory format, otherwise give the same speaker id for all utterances (Actually we don't use speaker information for cls1 recipe now).\",\"uttidA uttidA uttidB uttidB ...\",\"OR\",\"uttidA dummy uttidB dummy ...\",\"Once you complete creating the data directory, it's good to check it by utils/validate_data_dir.sh.\",\"utils/validate_data_dir.sh --no-feats data/train utils/validate_data_dir.sh --no-feats data/dev utils/validate_data_dir.sh --no-feats data/test\",\"Problems you might encounter\",\"Below are some common errors to watch out for:\",\"Torcheval not found\",\"Run pip install torcheval\"]},\"214\":{\"h\":\"Supported Models\",\"t\":[\"TODO(shikhar): Add details about BEATs once it is trained.\"]},\"215\":{\"h\":\"Speech Codec\",\"t\":[\"This is a template of Codec recipe for ESPnet2.\"]},\"216\":{\"h\":\"Table of Contents\",\"t\":[\"Recipe flow\",\"1. Data preparation\",\"2. Wav dump / Embedding preparation\",\"3. Removal of long / short data\",\"4. Codec statistics collection\",\"5. Codec training\",\"6. Codec decoding\",\"7. Codec Scoring\",\"8-9. (Optional) Pack results for upload\",\"How to run\",\"Basic training\",\"Scoring\",\"Supported Models\",\"FAQ\"]},\"217\":{\"h\":\"Recipe flow\",\"t\":[\"Codec recipe consists of 9 stages.\",\"Data preparation\",\"Data preparation stage. You have two methods to generate the data:\",\"ESPnet format:\",\"It calls local/data.sh to creates Kaldi-style data directories in data/ for training, validation, and evaluation sets.\",\"Noted that since we usually just need waveform to train the model, we can just use wav.scp to train a model. However, if you would like to use additional information (transcription, speaker information) for evaluation, we may want to have full-kaldi-style supports.\",\"See also:\",\"About Kaldi-style data directory\",\"Wav dump / Embedding preparation\",\"Wav dumping stage. This stage reformats wav.scp in data directories.\",\"If you specify kaldi, then we additionally extract mfcc features and vad decision.\",\"Removal of long / short data\",\"Processing stage to remove long and short utterances from the training and validation data. You can change the threshold values via --min_wav_duration and --max_wav_duration.\",\"Codec statistics collection\",\"Statistics calculation stage. It collects the shape information of the input and output and calculates statistics for feature normalization (mean and variance over training data) if needed.\",\"Codec training\",\"Codec model training stage. You can change the training setting via --train_config and --train_args options.\",\"See also:\",\"Supported models.\",\"Change the configuration for training\",\"Distributed training\",\"Codec decoding\",\"Codec model decoding stage. You can change the decoding setting via --inference_config and --inference_args.\",\"See also:\",\"Change the configuration for training\",\"Codec Scoring\",\"Codec model scoring stage. The scoring is supported by VERSA. You can change the scoring setting via --scoring_config and --scoring_args.\",\"See also:\",\"Change the configuration for training\",\"VERSA documents\",\"(Optional) Pack results for upload\",\"Packing stage. It packs the trained model files as a preparation for uploading to Hugging Face.\",\"(Optional) Upload model to Hugging Face\",\"Upload the trained model to Hugging Face for sharing. Additional information at Docs.\"]},\"218\":{\"h\":\"How to run\",\"t\":[\"Here, we show the procedure to run the recipe using egs2/libritts/codec1.\",\"Move on the recipe directory.\",\"$ cd egs2/libritts/codec1\",\"Modify libritts variable in db.sh if you want to change the download directory.\",\"$ vim db.sh\",\"Modify cmd.sh and conf/*.conf if you want to use job scheduler. See the detail in using job scheduling system.\",\"$ vim cmd.sh\",\"Run run.sh, which conducts all of the stages explained above.\",\"$ ./run.sh\",\"As a default, we train Tacotron2 (conf/train.yaml) with feats_type=raw + token_type=phn.\",\"Then, you can get the following directories in the recipe directory.\",\"├── data/ # Kaldi-style data directory │ ├── dev/ # validation set │ ├── eval1/ # evaluation set │ └── tr_no_dev/ # training set ├── dump/ # feature dump directory │ └── raw/ │ ├── org/ │ │ ├── tr_no_dev/ # training set before filtering │ │ └── dev/ # validation set before filtering │ ├── eval1/ # evaluation set │ ├── dev/ # validation set after filtering │ └── tr_no_dev/ # training set after filtering └── exp/ # experiment directory ├── codec_stats_raw_phn_tacotron_g2p_en_no_space # statistics └── codec_train_raw_phn_tacotron_g2p_en_no_space # model ├── tensorboard/ # tensorboard log ├── images/ # plot of training curves ├── decode_train.loss.ave/ # decoded results │ ├── dev/ # validation set │ └── eval1/ # evaluation set │ ├── wav/ # generated wav via Griffin-Lim │ ├── feats_type # feature type │ └── speech_shape # shape info of generated features ├── config.yaml # config used for the training ├── train.log # training log ├── *epoch.pth # model parameter file ├── checkpoint.pth # model + optimizer + scheduler parameter file ├── latest.pth # symlink to latest model parameter ├── *.ave_5best.pth # model averaged parameters └── *.best.pth # symlink to the best model parameter loss\",\"For the first time, we recommend performing each stage step-by-step via --stage and --stop-stage options.\",\"$ ./run.sh --stage 1 --stop-stage 1 $ ./run.sh --stage 2 --stop-stage 2 ... $ ./run.sh --stage 8 --stop-stage 8\",\"This might helps you to understand each stage's processing and directory structure.\"]},\"219\":{\"h\":\"Supported Models\",\"t\":[\"You can train the following models by changing *.yaml config for --train_config option in codec.sh.\",\"Current support models (You can refer to libritts recipe as we usually start from the corpus).\",\"SoundStream\",\"Encodec\",\"DAC\",\"FunCodec (Freq-Codec)\",\"HiFiCodec\"]},\"220\":{\"h\":\"FAQ\",\"t\":[\"Pre-trained codec models and usage\",\"We provide pre-trained codec models in ESPnet huggingface\",\"A quick usage of pre-trained models is as follows:\",\"from espnet2.bin.gan_codec_inference import AudioCoding import numpy as np # the model tag can be found in ESPnet huggingface models codec_api = AudioCoding.from_pretrained(model_tag=\\\"espnet/libritts_soundstream16k\\\") audio_info = codec_api(np.zeros(16000, dtype=np.float32))\",\"For advanced usage (e.g., batch tokenization and auto-packing to other tasks), see also egs2/TEMPLATE/codec1/scripts/feats/codec_tokenization.sh\"]},\"221\":{\"h\":\"Speaker Diarisation\",\"t\":[\"This is a template of diar1 recipe for ESPnet2.\"]},\"222\":{\"h\":\"Speech Enhancement\",\"t\":[\"This is the common recipe for ESPnet2 speech enhancement frontend.\",\"Table of Contents\",\"Introduction to enh.sh\",\"Stage 1: Data preparation\",\"Stage 2: Speech perturbation\",\"Stage 3: Format wav.scp\",\"Stage 4: Remove short data\",\"Stage 5: Collect stats for the enhancement task.\",\"Stage 6: Enhancement task Training\",\"Stage 7: Speech Enhancement inferencing\",\"Stage 8: Scoring\",\"Stage 9: Decode with a pretrained ASR model\",\"Stage 10: Scoring with a pretrained ASR model\",\"Stage 11: Pack model\",\"Stage 12: Upload model to Zenodo (Deprecated)\",\"Stage 13: Upload model to Hugging Face\",\"(For developers) Instructions on creating a new recipe\",\"Step 1 Create recipe directory\",\"Step 2 Write scripts for data preparation\",\"Step 3 Prepare training configuration\",\"Step 4 Prepare run.sh\",\"Instructions on creating a new model\",\"Step 1 Create model scripts\",\"Step 2 Add the new model to related scripts\",\"Step 3 [Optional] Create new loss functions\",\"Step 4 Create unit tests for the new model\"]},\"223\":{\"h\":\"Introduction to enh.sh\",\"t\":[\"In egs2/TEMPLATE/enh1/enh.sh, 13 stages are included.\",\"Stage 1: Data preparation\",\"This stage is similar to stage 1 in asr.sh.\",\"Stage 2: Speech perturbation\",\"Speech perturbation is widely used in the ASR task, but rarely in speech enhancement. Some of our initial experiments have shown that speech perturbation works on wsj0_2mix. We are conducting more experiments to make sure if it works. The speech perturbation procedure is almost the same as that in ASR, we have copied scripts/utils/perturb_data_dir_speed.sh to scripts/utils/perturb_enh_data_dir_speed.sh and made some minor modifications to support the speech perturbation for more scp files rather than wav.scp only.\",\"Stage 3: Format wav.scp\",\"Format scp files such as wav.scp. The scp files include:\",\"wav.scp: wav file list of mixed/noisy input signals.\",\"spk{}.scp: wav file list of speech reference signals. {} can be 1, 2, ..., depending on the number of speakers in the input signal in wav.scp.\",\"noise{}.scp (optional): wav file list of noise reference signals. {} can be 1, 2, ..., depending on the number of noise types in the input signal in wav.scp. The file(s) are required when --use_noise_ref true is specified. Also related to the variable noise_type_num.\",\"dereverb{}.scp (optional): wav file list of dereverberation reference signals (for training a dereverberation model). This file is required when --use_dereverb_ref true is specified. Also related to the variable dereverb_ref_num.\",\"utt2category: (optional) the category info of each utterance. This file can help the batch sampler to load the same category utterances in each batch. One usage case is that users want to load the simulation data and real data in different batches.\",\"Stage 4: Remove short data\",\"This stage is the same as that in ASR recipes.\",\"Stage 5: Collect stats for the enhancement task.\",\"Same as the ASR task, we collect the data stats before training. Related new python files are:\",\"espnet2/ ├── bin │ └── enh_train.py └── tasks └── enh.py\",\"TheEnhancementTask defined in espnet2/tasks/enh.py is called in espnet2/bin/enh_train.py. In the collect_stats mode. the behavior of EnhancementTask is the same as the ABSTask.\",\"Stage 6: Enhancement task Training\",\"We have created EnhancementTask in espnet2/tasks/enh.py, which is used to train the ESPnetEnhancementModel(AbsESPnetModel) defined in espnet2/enh/espnet_model.py. In EnhancementTask, the speech enhancement or separation models follow the encoder-separator-decoder style, and several encoders, decoders and separators are implemented. Although it is currently defined as an independent task, the models from EnhancementTask can be easily called by other tasks or even jointly trained with other tasks (see egs2/TEMPLATE/enh_asr1/, egs2/TEMPLATE/enh_st1/).\",\"Now we support adding noise, reverberation, interference speech on the fly by specifying preprocessor in the configuration. For example, to use EnhPreprocessor, one can specify preprocessor: \\\"enh\\\" in the configuration and specify --extra_wav_list in run.sh. Check PR #4321 for more details.\",\"We also support possible integration of other speech enhancement/separation toolkits (e.g. Asteroid), so that models trained with other speech enhancement/separation toolkits can be reused/evaluated on ESPnet for downstream tasks such as ASR.\",\"Related arguments in enh.sh include:\",\"--ref_num\",\"--enh_args\",\"--enh_config\",\"--enh_exp\",\"--ngpu\",\"--num_nodes\",\"--init_param\",\"--use_dereverb_ref\",\"--use_noise_ref\",\"--extra_wav_list\",\"Related python files:\",\"espnet2/ ├── bin │ ├── enh_inference.py │ ├── enh_scoring.py │ └── enh_train.py ├── enh │ ├── abs_enh.py │ ├── decoder │ │ ├── abs_decoder.py │ │ ├── conv_decoder.py │ │ ├── null_decoder.py │ │ └── stft_decoder.py │ ├── encoder │ │ ├── abs_encoder.py │ │ ├── conv_encoder.py │ │ ├── null_encoder.py │ │ └── stft_encoder.py │ ├── espnet_model.py │ ├── layers │ │ ├── beamformer.py │ │ ├── complex_utils.py │ │ ├── complexnn.py │ │ ├── conv_utils.py │ │ ├── dc_crn.py │ │ ├── dnn_beamformer.py │ │ ├── dnn_wpe.py │ │ ├── dpmulcat.py │ │ ├── dprnn.py │ │ ├── dptnet.py │ │ ├── fasnet.py │ │ ├── ifasnet.py │ │ ├── mask_estimator.py │ │ ├── skim.py │ │ ├── tcn.py │ │ └── wpe.py │ ├── loss │ │ ├── criterions │ │ │ ├── abs_loss.py │ │ │ ├── tf_domain.py │ │ │ └── time_domain.py │ │ └── wrappers │ │ ├── abs_wrapper.py │ │ ├── dpcl_solver.py │ │ ├── fixed_order.py │ │ ├── multilayer_pit_solver.py │ │ └── pit_solver.py │ └── separator │ ├── abs_separator.py │ ├── asteroid_models.py │ ├── conformer_separator.py │ ├── dan_separator.py │ ├── dc_crn_separator.py │ ├── dccrn_separator.py │ ├── dpcl_separator.py │ ├── dpcl_e2e_separator.py │ ├── dprnn_separator.py │ ├── dptnet_separator.py │ ├── fasnet_separator.py │ ├── neural_beamformer.py │ ├── rnn_separator.py │ ├── skim_separator.py │ ├── svoice_separator.py │ ├── tcn_separator.py │ └── transformer_separator.py └── tasks └── enh.py\",\"Stage 7: Speech Enhancement inferencing\",\"This stage generates the enhanced or separated speech with the trained model. The generated audio files will be placed at ${expdir}/${eval_set}/logdir and scp files for them will be created in ${expdir}/${eval_set}.\",\"Related arguments in enh.sh include:\",\"--ref_num\",\"--fs\",\"--gpu_inference\",\"--inference_args\",\"--inference_model\",\"--inference_nj\",\"--inference_enh_config\",\"Now we support changing the model attributes (such as beamformer_type in NeuralBeamformer) in this stage, by specifying --inference_enh_config. Check PR #4251 for more details.\",\"Related new python files:\",\"espnet2/ └── bin └── enh_inference.py\",\"Stage 8: Scoring\",\"This stage is used to do the scoring for speech enhancement. Scoring results for each ${eval_set} will be summarized in ${expdir}/RESULTS.TXT.\",\"Related arguments in enh.sh include:\",\"--scoring_protocol\",\"--ref_channel\",\"Related new python files:\",\"egs2 └── TEMPLATE └── enh1 └── scripts └── utils └── show_enh_score.sh espnet2/ └── bin └── enh_scoring.py\",\"Stage 9: Decode with a pretrained ASR model\",\"Same as Stage 11 in the ASR task. The enhanced speech in Stage 7 is fed into a pretrained ASR model (specified as \\\"${asr_exp}\\\"/\\\"${decode_asr_model}\\\") for decoding.\",\"Related arguments in enh.sh include:\",\"--score_with_asr\",\"--asr_exp\",\"--decode_args\",\"--decode_asr_model\",\"--gpu_inference\",\"--inference_nj\",\"Related python files:\",\"espnet2/ └── bin └── asr_inference.py\",\"Stage 10: Scoring with a pretrained ASR model\",\"Same as Stage 12 in the ASR task. The decoding results in Stage 11 are scored to calculate the average CER/WER/TER.\",\"Stage 11: Pack model\",\"Just the same as other tasks. A new entry for packing speech enhancement models is added in espnet2/bin/pack.py.\",\"Stage 12: Upload model to Zenodo (Deprecated)\",\"Upload the trained speech enhancement/separation model to Zenodo for sharing.\",\"Stage 13: Upload model to Hugging Face\",\"Upload the trained speech enhancement/separation model to Hugging Face for sharing. Additional information at Docs.\"]},\"224\":{\"h\":\"(For developers) Instructions on creating a new recipe\",\"t\":[\"Step 1 Create recipe directory\",\"First, run the following command to create the directory for the new recipe from our template:\",\"egs2/TEMPLATE/enh1/setup.sh egs2/&lt;your_recipe_name&gt;/enh1\",\"Please follow the name convention in other recipes.\",\"For the following steps, we assume the operations are done under the directory egs2/&lt;your_recipe_name&gt;/enh1/.\",\"Step 2 Write scripts for data preparation\",\"Prepare local/data.sh, which will be used in stage 1 in enh.sh. It can take some arguments as input, see egs2/wsj0_2mix/enh1/local/data.sh for reference.\",\"The script local/data.sh should finally generate Kaldi-style data directories under &lt;recipe_dir&gt;/data/. Each subset directory should contain at least 4 files:\",\"&lt;recipe-dir&gt;/data/&lt;subset-name&gt;/ ├── spk1.scp (clean speech references) ├── spk2utt ├── utt2spk └── wav.scp (noisy speech)\",\"Optionally, it can also contain noise{}.scp and dereverb{}.scp, which point to the corresponding noise and dereverberated references respectively. {} can be 1, 2, ..., depending on the number of noise types (dereverberated signals) in the input signal in wav.scp.\",\"Make sure to sort the scp and other related files as in Kaldi. Also, remember to run . ./path.sh in local/data.sh before sorting, because it will force sorting to be byte-wise, i.e. export LC_ALL=C.\",\"Please follow the style in other recipes as much as possible. Check egs2/chime4/enh1/local/data.sh for reference.\",\"Remember to check your new scripts with shellcheck, otherwise they may fail the tests in ci/test_shell.sh.\",\"Step 3 Prepare training configuration\",\"Prepare training configuration files (e.g. train.yaml) under conf/.\",\"If you have multiple configuration files, it is recommended to put them under conf/tuning/, and create a symbolic link conf/tuning/train.yaml pointing to the config file with the best performance.\",\"Please trim trailing whitespace in each line.\",\"Step 4 Prepare run.sh\",\"Write run.sh to provide a template entry script, so that users can easily run your recipe by ./run.sh. Check egs2/wsj0_2mix/enh1/run.sh for reference.\",\"Please ensure that the argument --ref_num in run.sh is consistent with the num_spk (under separator_conf) in the training configuration files created in last step, except in MixIT training. In MixIT, the argument --inf_num in run.sh should be consistent with the num_spk (under separator_conf).\",\"If your recipes provide references for noise and/or dereverberation, you can set the argument --use_noise_ref true and/or --use_dereverb_ref true in run.sh.\"]},\"225\":{\"h\":\"Instructions on creating a new model\",\"t\":[\"The current ESPnet-SE tool adopts an encoder-separator-decoder architecture for all models, e.g.\",\"For Time-Frequency masking models, the encoder and decoder would be stft_encoder.py and stft_decoder.py respectively, and the separator can be dprnn_separator.py, rnn_separator.py, tcn_separator.py, transformer_separator.py and so on. For TasNet, the encoder and decoder are conv_encoder.py and conv_decoder.py respectively. The separator is tcn_separator.py.\",\"Step 1 Create model scripts\",\"For encoder, separator, and decoder models, create new scripts under espnet2/enh/encoder/, espnet2/enh/separator/, and espnet2/enh/decoder/, respectively.\",\"For a separator model, please make sure it implements the num_spk property. In addition, it is recommended to also support predict_noise for estimating the noise signal. Check espnet2/enh/separator/tcn_separator.py for reference.\",\"Please follow the coding style as mentioned in CONTRIBUTING.md.\",\"Remember to format your new scripts to match the styles in black, flake8, and isort, otherwise they may fail the tests in ci/test_python.sh.\",\"Step 2 Add the new model to related scripts\",\"In espnet2/tasks/enh.py, add your new model to the corresponding ClassChoices, e.g.\",\"For encoders, add &lt;key&gt;=&lt;your-model&gt; to encoder_choices.\",\"For decoders, add &lt;key&gt;=&lt;your-model&gt; to decoder_choices.\",\"For separators, add &lt;key&gt;=&lt;your-model&gt; to separator_choices.\",\"Step 3 [Optional] Create new loss functions\",\"If you want to use a new loss function for your model, you can add it as a module to espnet2/enh/loss/criterions/.\",\"Check FrequencyDomainMSE in espnet2/enh/loss/criterions/tf_domain.py for reference.\",\"Then add your loss name to criterion_choices in espnet2/tasks/enh.py, so that you can configure it directly in a yaml file.\",\"Step 4 Create unit tests for the new model\",\"Finally, it would be nice to make some unit tests for your new model under test/espnet2/enh/test_espnet_model.py and test/espnet2/enh/encoder / test/espnet2/enh/decoder / test/espnet2/enh/separator.\"]},\"226\":{\"h\":\"Speech Recognition with Speech Enhancement\",\"t\":[\"This is the common recipe for ESPnet2 joint-task with speech enhancement frontend. Following are the directory structure of speech enhancement and joint-task recipes:\"]},\"227\":{\"h\":\"Table of Contents\",\"t\":[\"Recipe flow\",\"1. Data preparation\",\"2. Wav dump / Embedding preparation\",\"3. Removal of long / short data\",\"4. Codec statistics collection\",\"5. Codec training\",\"6. Codec decoding\",\"7. Codec Scoring\",\"8-9. (Optional) Pack results for upload\",\"How to run\",\"Basic training\",\"Scoring\",\"Supported Models\",\"FAQ\",\"egs2/ ├── chime4/ │ ├── enh1/ │ ├── enh_asr1/ │ └── asr1/ ├── l3das22/ │ └── enh1/ | │ ├── conf/ | │ ├── local/ | | │ ├── data.sh | | │ ├── metric.sh │ | │ └── ... | │ ├── enh.sh -> ../../TEMPLATE/enh1/enh.sh | │ ├── run.sh | │ └── ... ├── lt_slurp_spatialized/ │ └── enh1/ ├── slurp_spatialized/ │ ├── enh_asr1/ | │ ├── enh_asr.sh -> ../../TEMPLATE/enh_asr1/enh_asr.sh | │ ├── run.sh | │ └── ... │ └── asr1/ ├── ... └── TEMPLATE/ ├── enh1/ │ └── enh.sh ├── enh_asr1/ │ └── enh_asr.sh ├── enh_diar1/ │ └── enh_diar.sh ├── enh_st1/ │ └── enh_st.sh └── ...\"]},\"228\":{\"h\":\"Introduction to enh_asr.sh\",\"t\":[\"In egs2/TEMPLATE/enh_asr1/enh_asr.sh, 17 stages are included. Most of the stages are similar to asr.sh and enh.sh.\",\"stage 1 to stage 5: data preparation stages\",\"Stage 1: Data preparation\",\"Stage 2: Speech perturbation\",\"Stage 3: Format wav.scp\",\"Stage 4: Remove short data\",\"Stage 5: Generate token_list using BPE\",\"stage 6 to stage 9: language model training steps\",\"Stage 6: LM collect stats\",\"Stage 7: LM Training\",\"Stage 8: Calc perplexity\",\"Stage 9: Ngram Training\",\"stage 10 to stage 11: joint-task training steps\",\"Stage 10: Collect stats for the joint task.\",\"Stage 11: Joint task Training\",\"We have created EnhS2TTask in espnet2/tasks/enh_s2t_train.py, which is used to train the ESPnetEnhS2TModel(AbsESPnetModel) defined in espnet2/enh/espnet_enh_s2t_model.py. The ESPnetEnhS2TModel takes a front-end enh_model, and a back-end s2t_model (such as ASR, SLU, ST, and SD models) as inputs to build a joint-model.\",\"Related python files:\",\"espnet2/ ├── bin/ │ └── enh_s2t_train.py ├── enh/ │ └── espnet_enh_s2t_model.py ├── tasks/ │ └── enh_s2t.py └── ...\",\"stage 12 to stage 13: Inference stages: Decoding and enhancing\",\"Stage 12: downstream tasks (ASR) Decoding\",\"Stage 13: Enhance Speech\",\"Related python files:\",\"espnet2/ ├── bin/ │ ├── asr_inference.py │ ├── diar_inference.py │ ├── enh_inference.py │ └── st_inference.py └── ...\",\"stage 14 to stage 15: Scoring recognition and SSE results\",\"Stage 14: Scoring ASR\",\"Stage 15: Scoring Enhancement\",\"stage 16 to stage 17: model uploading steps\",\"Stage 16: Pack model\",\"Stage 17: Upload model to Hugging Face\"]},\"229\":{\"h\":\"Speaker Diarisation with Speech Enhancement\",\"t\":[\"This is a template of enh_diar1 recipe for ESPnet2.\"]},\"230\":{\"h\":\"Speech-to-Text Translation with Speech Enhancement\",\"t\":[\"This is a template of enh_st1 recipe for ESPnet2.\"]},\"231\":{\"h\":\"Self-supervised Learning\",\"t\":[\"This is a template of the hubert11 recipe for ESPnet2, designed for HuBERT-style SSL.\"]},\"232\":{\"h\":\"Differences from other recipes\",\"t\":[\"ESPnet2 serves two different recipes for Self-Supervised Learning (SSL): ssl1 and hubert1 (this one).\",\"hubert1 is the original implementation of SSL under the HuBERT pre-training framework. The recipe takes care of everything need for pre-training, such as K-means pseudo-labelling and discrete token evaluation. This is very important for reproducibility. However, it is quite complicated due to the multiple offline stages required for HuBERT and therefore difficult to hack/adapt to new training methods or other scenarios.\",\"We created the new ssl1 recipe to future-proof the codebase to accomodate other pre-training techniques that are purely end-to-end, such as DinoSR, SpeechFlow, or w2v-BERT. This recipe is designed to be easily customizable and more scalable to large-scale pre-training setups.\",\"Note: the ssl1 codebase also supports HuBERT pre-training, but the steps to create the pseudo-labels are not included in the recipe. Users will either need to run the hubert1 recipe to obtain the labels, or generate it themselves.\"]},\"233\":{\"h\":\"Language Identification\",\"t\":[\"This is a template of the lid1 recipe for ESPnet2. It follows a classification-based training/inference pipeline for spoken language identification. The model is trained as a closed-set classifier over a predefined set of language labels. Optionally, language embeddings can be extracted and used for downstream analysis, e.g., t-SNE visualization.\"]},\"234\":{\"h\":\"Table of Contents\",\"t\":[\"Language Identification\",\"Table of Contents\",\"Recipe flow\",\"1. Data preparation\",\"2. Speed perturbation (Optional)\",\"3. Wav format\",\"4. Statistics collection\",\"5. LID training\",\"6. Inference and embedding extraction\",\"7. Score calculation\",\"8. t-SNE visualization\",\"9-10. (Optional) Pack and upload results\",\"How to run\",\"Example: VoxLingua107 training\"]},\"235\":{\"h\":\"Recipe flow\",\"t\":[\"lid1 recipe consists of 10 stages.\",\"Data preparation\",\"Prepares Kaldi-style data directories using local/data.sh.\",\"Expected files include:\",\"wav.scp: path to raw audio\",\"utt2lang: utterance-to-language mapping\",\"lang2utt: language-to-utterance mapping (for sampling)\",\"segments (optional): used to extract segments from long recordings\",\"Speed perturbation (Optional)\",\"Applies offline speed perturbation to the training set using multiple speed factors, e.g., 0.9 1.0 1.1.\",\"Wav format\",\"Formats the audio to a consistent format (wav, flac, or Kaldi-ark) and copies necessary metadata to the working directory. Required for both training and evaluation sets.\",\"Statistics collection\",\"Collects input feature shape statistics and language information needed for batching and model configuration.\",\"LID training\",\"Trains the language identification model using the configuration provided via --lid_config and optional arguments in --lid_args. The model is trained to predict the correct language ID for each utterance.\",\"Inference and embedding extraction\",\"Performs inference on evaluation sets. This stage supports both:\",\"LID prediction (predicted utt2lang)\",\"Language embedding extraction (utterance-level or averaged per language)\",\"Optionally saves intermediate outputs\",\"Score calculation\",\"Computes standard classification metrics (Accuracy, Macro Accuracy) by comparing model predictions with reference utt2lang.\",\"t-SNE visualization\",\"Visualizes the per-language embeddings using t-SNE.\",\"9-10. (Optional) Pack and upload results\",\"Packs the trained model and metadata into a zip file and optionally uploads it to Hugging Face Hub for sharing and reproducibility.\"]},\"236\":{\"h\":\"How to run\",\"t\":[\"Example: VoxLingua107 training\",\"Move to the recipe directory:\",\"cd egs2/voxlingua107/lid1\",\"Edit the following files:\",\"vim db.sh # set path to VoxLingua107 dataset vim cmd.sh # job scheduling command if using a cluster vim conf/mms_ecapa_bs3min_baseline.yaml # model and training configuration (default training configuration)\",\"Then run the full pipeline:\",\"./run.sh\",\"This will go through all the stages from data preparation to scoring.\"]},\"237\":{\"h\":\"Language Modeling\",\"t\":[\"This is a template of lm1 recipe for ESPnet2.\"]},\"238\":{\"h\":\"Machine Translation\",\"t\":[\"This is a template of mt1 recipe for ESPnet2.\"]},\"239\":{\"h\":\"Speech-to-Speech Translation\",\"t\":[\"This is a template of s2st1 recipe for ESPnet2.\"]},\"240\":{\"h\":\"Weakly-supervised Learning (Speech-to-Text)\",\"t\":[\"This is a template of S2T1 recipe for ESPnet2. It is based on ASR1, but follows the style of OpenAI's Whisper to train a single encoder-decoder model for various speech processing tasks. Specifically, it uses special tokens as task specifiers (e.g., transcribe, translate) or prediction targets (e.g., language ID) so that a single model can perform multiple tasks for multiple languages. It further supports conditional generation where the condition is the previous sentence within the long talk.\",\"More details can be found in our OWSM paper (ASRU 2023).\",\"Table of Contents\",\"Recipe flow\",\"1. Data preparation\",\"ESPnet format:\",\"2. Speed perturbation\",\"3. Wav format\",\"4. Remove long or short data\",\"5. Generate token list\",\"6. LM statistics collection\",\"7. LM training\",\"8. LM perplexity\",\"9. Ngram LM training\",\"10. S2T statistics collection\",\"11. S2T training\",\"12. S2T inference\",\"13. S2T scoring\",\"14-15. (Optional) Pack results for upload\",\"How to run\",\"OWSM training\",\"How to fine-tune pre-trained OWSM\",\"1. Prepare s2t1 recipe\",\"2. Prepare data in OWSM format\",\"3. Fine-tune the model\",\"Related work\"]},\"241\":{\"h\":\"Table of Contents\"},\"242\":{\"h\":\"Recipe flow\",\"t\":[\"S2T1 recipe consists of 16 stages.\",\"Data preparation\",\"Data preparation stage.\",\"ESPnet format:\",\"It calls local/data.sh to creates Kaldi-style data directories in data/ for training and validation sets.\",\"The training data has the following format:\",\"&lt;sop&gt; prev&lt;sos&gt;&lt;category&gt;&lt;task&gt;&lt;starttime1&gt; utt1&lt;endtime1&gt;&lt;starttime2&gt; utt2&lt;endtime2&gt;&lt;eos&gt;\",\"where &lt;sop&gt; is a special token denoting the start of prev/prompt sentence. The timestamps are also treated as special tokens because the audio has a fixed length (30s) and resolution (20ms or 40ms). An example looks like:\",\"&lt;sop&gt; I'm going to talk today about energy and climate.&lt;sos&gt;&lt;en&gt;&lt;transcribe&gt;&lt;0.00&gt; And that might seem a bit surprising, because my full-time work at the foundation is mostly about vaccines and seeds, about the things that we need to invent and deliver to help the poorest two billion live better lives.&lt;14.12&gt;&lt;15.36&gt; But energy and climate are extremely important to these people; in fact, more important than to anyone else on the planet.&lt;24.26&gt;&lt;eos&gt;\",\"During data preparation, three text files should be generated:\",\"text contains the normal target sentence, i.e., the text between &lt;sos&gt; and &lt;eos&gt;. \",\"This must include all special tokens as described above.\",\"If your data does not have timestamps, you need to use &lt;notimestamps&gt; instead. Even so, you still need to make sure that your non-linguistic symbol list (nlsyms_txt) contains the range of all possible timestamps.\",\"&lt;sop&gt;, &lt;sos&gt;, and &lt;eos&gt; are actually inserted during preprocessing/dataloading. You only need &lt;language&gt;&lt;task&gt;&lt;starttime1&gt; utt1&lt;endtime1&gt;&lt;starttime2&gt; utt2&lt;endtime2&gt;\",\"For ST (speech translation), text should contain the translation in the target language.\",\"See https://github.com/espnet/espnet/tree/master/egs2/owsm_v1/s2t1/local for an example of data preparation and the expected format.\",\"text.prev contains the previous sentence, i.e., the text between &lt;sop&gt; and &lt;sos&gt;. This might be unavailable at the beginning of a talk. In such cases, a special token &lt;na&gt; will be used. \",\"This should not contain any special tokens except for &lt;na&gt;. In the example above, take the text between &lt;sop&gt; and &lt;sos&gt; and put it here.\",\"text.ctc contains the ASR transcript without any special token, which is used for the CTC loss. For ASR utterances, this can be derived from text, but for ST utterances, this is in a different language. If the ASR transcription is not available, &lt;na&gt; will be used. \",\"This should not contain any special tokens (e.g. timestamps). For ASR, just take the text between &lt;task&gt; and &lt;eos&gt; and put it here (timestamps removed). For ST, text.ctc should contain the ASR transcript of the source language and thus will be different from text (the target language translation).\",\"Further notes:\",\"The text.prev file should be called text.prev (with a period). Same with text.ctc\",\"In the training config, however, the text_ctc_name must be text_ctc (and text_ctc_name must be text_ctc)\",\"If you support multiple tasks (e.g. ASR and ST), all utterances are expected to be put into text, text.prev, and text.ctc. The task token (e.g. &lt;asr&gt;) will distinguish between different tasks.\",\"If the same utterance is used multiple times (e.g. once in ASR and once in ST), each copy of the utterance needs a unique ID. You can append the task to the utterance ID to make it unique. \",\"Use utils/fix_data_dir.sh --utt_extra_files \\\"utt2num_samples text.ctc text.prev\\\" SPLIT to ensure the file is still sorted.\",\"Speed perturbation\",\"Augment training data with speed perturbation. data/${train_set}_spXX would be generated (XX means the speed factor). This step is optional. Note that the timestamps need to be changed as well.\",\"Wav format\",\"Format the wave files in wav.scp to a single format (wav / flac / kaldi_ark).\",\"Remove long or short data\",\"Remove too long or too short data.\",\"Generate token list\",\"Generate token list from the training data. BPE tokens are used.\",\"LM statistics collection\",\"Neural-network (NN) based Language model (LM) is optional for S2T1 task. You can skip stage 6-9 by setting --use_lm false. Statistics calculation stage. It collects the shape information of LM texts and calculates statistics for LM training.\",\"LM training\",\"NN-based LM model training stage. You can change the training setting via --lm_config and --lm_args options.\",\"See also:\",\"Supported models.\",\"Change the configuration for training\",\"Distributed training\",\"LM perplexity\",\"NN-based LM evaluation stage. Perplexity (PPL) is computed against the trained model\",\"See also:\",\"Change the configuration for training\",\"Ngram LM training\",\"N-gram-based LM model training stage.\",\"S2T statistics collection\",\"Statistics calculation stage. It collects the shape information of input and output texts for S2T training.\",\"S2T training\",\"S2T model training stage. You can change the training setting via --s2t_config and --s2t_args options.\",\"See also:\",\"Supported models.\",\"Change the configuration for training\",\"Distributed training\",\"S2T inference\",\"S2T inference stage. We can perform ASR or ST using any prepared test data.\",\"S2T scoring\",\"Calculate ASR error rates (char / word / token).\",\"14-15. (Optional) Pack results for upload\",\"Packing stage. It packs the trained model files and uploads to Hugging Face.\",\"See also:\",\"ESPnet Model Zoo\",\"Upload the trained model to Hugging Face for sharing. Additional information at Docs.\"]},\"243\":{\"h\":\"How to run\",\"t\":[\"OWSM training\",\"We have created several recipes for OWSM training. Please check egs2/mixed_v1, egs2/mixed_v2, egs2/mixed_v3 for more information.\",\"How to fine-tune pre-trained OWSM\",\"Pre-trained OWSM can be fine-tuned on a specific dataset. Here, we use AISHELL-1 as an example.\",\"Prepare s2t1 recipe\",\"We use this s2t1 template to fine-tune OWSM. So we first create this directory under our custom dataset egs2/aishell.\",\"egs2/TEMPLATE/s2t1/setup.sh egs2/aishell/s2t1\",\"Then, we download a pre-trained model, e.g., espnet/owsm_v2_ebranchformer, using the following command:\",\"# go to the created dir cd egs2/aishell/s2t1 # source path.sh . ./path.sh # download model from hugging face using espnet_model_zoo_download # we use dummy names for required arguments and we do not run any actual stage (thus --stage 100) ./s2t.sh --download_model espnet/owsm_v2_ebranchformer --stage 100 --train_set dummy --valid_set dummy2 --test_sets dummy3\",\"The downloaded model will be saved in local cache and then uncompressed. An exp directory will be automatically created which contains symbolic links to the checkpoint and config files.\",\"To use a pre-trained model, we need the following important files:\",\"config: A yaml file containing all training arguments and the token list. The name is config.yaml.\",\"model checkpoint: The name is xxx.pth. In this example, it is valid.total_count.ave_5best.till25epoch.pth.\",\"stats: This is used to normalize the input speech features if feats_normalize is global_mvn.\",\"bpe model: This is the BPE model used by sentencepiece.\",\"The path to stats can be found in config.yaml, e.g.:\",\"grep stats_file exp/espnet/owsm_v2_ebranchformer/config.yaml\",\"The path to bpemodel can also be found in config.yaml, e.g.:\",\"grep bpemodel exp/espnet/owsm_v2_ebranchformer/config.yaml\",\"In the following sections, we will manually copy those two files to correct places.\",\"Prepare data in OWSM format\",\"The data should be prepared in the OWSM format. Please refer to 1. Data preparation for more information.\",\"Since AISHELL-1 has been included in OWSM v1, we can reuse those preparation scripts. For your own data, please write the scripts by yourself and make sure the special tokens such as the language codes are consistent with the pre-trained model. Note that we will NOT generate new vocabulary for fine-tuning. Instead, we will use the vocabulary from the pre-trained model.\",\"cd local/ ln -s ../../../mixed_v1/s2t1/local/utils.py ./ ln -s ../../../mixed_v1/s2t1/local/prepare_aishell.* ./ cd .. # modify data_dir and execute: ./local/prepare_aishell.sh\",\"The prepared data will be stored in a new directory data.\",\"Next, we execute various stages in s2t.sh. To make it easier, we create a run.sh shown below. It is mostly copied from the OWSM v2 recipe.\",\"#!/usr/bin/env bash # Set bash to 'debug' mode, it will exit on : # -e 'error', -u 'undefined variable', -o ... 'error in pipeline', -x 'print commands', set -e set -u set -o pipefail train_set=AISHELL-1/train valid_set=AISHELL-1/dev test_sets=\\\"AISHELL-1/dev\\\" nbpe=50000 # this should be consistent with the pre-trained model s2t_config=conf/train_s2t_ebf_conv2d_size1024_e12_d12.yaml inference_config=conf/decode_s2t.yaml # inference only args # --cleaner whisper_basic --hyp_cleaner whisper_basic ./s2t.sh \\\\ --stage 3 \\\\ --stop_stage 4 \\\\ --use_lm false \\\\ --num_nodes 1 \\\\ --ngpu 4 \\\\ --nj 32 \\\\ --gpu_inference true \\\\ --inference_nj 4 \\\\ --num_splits_s2t 1 \\\\ --feats_type raw \\\\ --audio_format flac.ark \\\\ --token_type bpe \\\\ --nbpe ${nbpe} \\\\ --bpe_input_sentence_size 10000000 \\\\ --s2t_config \\\"${s2t_config}\\\" \\\\ --inference_config \\\"${inference_config}\\\" \\\\ --train_set \\\"${train_set}\\\" \\\\ --valid_set \\\"${valid_set}\\\" \\\\ --test_sets \\\"${test_sets}\\\" \\\\ --bpe_train_text \\\"dump/raw/${train_set}/text\\\" \\\\ --bpe_nlsyms data/nlsyms.txt \\\\ --lm_train_text \\\"dump/raw/${train_set}/text\\\" \\\"$@\\\"\",\"We run Stage 3 and Stage 4 to format data:\",\"./run.sh --stage 3 --stop_stage 4\",\"We create the BPE token directory by ourselves. This is equivalent to Stage 5 but we do not generate a new token list.\",\"mkdir -p data/token_list/bpe_unigram50000 cp path_to_bpe_model data/token_list/bpe_unigram50000 # path_to_bpe_model is in config.yaml # we extract the token list python -c \\\"import yaml; config = yaml.safe_load(open('exp/espnet/owsm_v2_ebranchformer/config.yaml', 'r')); open('data/token_list/bpe_unigram50000/tokens.txt', 'w').write('\\\\n'.join(config['token_list']) )\\\"\",\"Fine-tune the model\",\"We create a training config file for fine-tuning. It is modified from the original config in config.yaml. Note that you may need to tune the training hyperparameters such as learning rate. The model might easily overfit to a small training set.\",\"preprocessor: s2t preprocessor_conf: text_prev_name: text_prev text_ctc_name: text_ctc fs: 16000 na_symbol: '&lt;na&gt;' speech_length: 30 speech_resolution: 0.02 speech_init_silence: 30 text_prev_apply_prob: 0.0 # we do not use previous prompt time_apply_prob: 0.0 # we do not use any timestamp for fine-tuning notime_symbol: '&lt;notimestamps&gt;' first_time_symbol: '&lt;0.00&gt;' last_time_symbol: '&lt;30.00&gt;' frontend_conf: n_fft: 512 win_length: 400 hop_length: 160 specaug: specaug specaug_conf: apply_time_warp: false time_warp_window: 5 time_warp_mode: bicubic apply_freq_mask: true freq_mask_width_range: - 0 - 27 num_freq_mask: 2 apply_time_mask: true time_mask_width_ratio_range: - 0. - 0.05 num_time_mask: 10 encoder: e_branchformer encoder_conf: output_size: 1024 attention_heads: 16 attention_layer_type: selfattn pos_enc_layer_type: abs_pos rel_pos_type: latest cgmlp_linear_units: 4096 cgmlp_conv_kernel: 31 use_linear_after_conv: false gate_activation: identity num_blocks: 12 dropout_rate: 0.1 positional_dropout_rate: 0.1 attention_dropout_rate: 0.1 input_layer: conv2d layer_drop_rate: 0.0 linear_units: 4096 positionwise_layer_type: linear use_ffn: true macaron_ffn: true merge_conv_kernel: 31 decoder: transformer decoder_conf: attention_heads: 16 linear_units: 4096 num_blocks: 12 dropout_rate: 0.1 positional_dropout_rate: 0.1 self_attention_dropout_rate: 0.1 src_attention_dropout_rate: 0.1 model_conf: ctc_weight: 0.3 lsm_weight: 0.1 length_normalized_loss: false sym_na: '&lt;na&gt;' # NOTE: you may need to tune these hyperparams optim: adamw optim_conf: lr: 1.0e-04 betas: - 0.9 - 0.98 eps: 1.0e-06 weight_decay: 0.0 scheduler: warmuplr scheduler_conf: warmup_steps: 5000 # NOTE: we are using 4 GPUs with 48GB memory batch_type: unsorted batch_size: 16 accum_grad: 4 max_epoch: 20 patience: none init: none best_model_criterion: - - valid - acc - max - - valid - total_count - max keep_nbest_models: 5 use_amp: true num_workers: 4 unused_parameters: false seed: 2023 num_att_plot: 1 # fine-tune init_param: - exp/espnet/owsm_v2_ebranchformer/valid.total_count.ave_5best.till25epoch.pth ignore_init_mismatch: false\",\"We need to collect the shapes of speech and text, but skip the mean and variance collection because we use a pre-trained model. We run Stage 10 with a smaller batch size:\",\"./run.sh --stage 10 --stop_stage 10 --feats_normalize utterance_mvn --s2t_args \\\"--model_conf extract_feats_in_collect_stats=false --batch_size 5\\\"\",\"Then, we copy the existing mean and variance to the correct place:\",\"cp path_to_train_stats exp/s2t_stats_raw_bpe50000/train/ # path_to_train_stats is in config.yaml\",\"Now, we can start training:\",\"./run.sh --stage 11 --stop_stage 11\"]},\"244\":{\"h\":\"Related work\",\"t\":[\"@article{peng2023reproducing, title={Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data}, author={Peng, Yifan and Tian, Jinchuan and Yan, Brian and Berrebbi, Dan and Chang, Xuankai and Li, Xinjian and Shi, Jiatong and Arora, Siddhant and Chen, William and Sharma, Roshan and others}, journal={arXiv preprint arXiv:2309.13876}, year={2023} }\"]},\"245\":{\"h\":\"ESPnet-SDS\",\"t\":[\"This is a template of ESPnet-SDS recipe. ESPnet-SDS is an open-source toolkit for building unified web interfaces for various cascaded and end-to-end (E2E) spoken dialogue systems, supporting real-time automated evaluation metrics, and human-in-the-loop feedback collection. This README describes features of ESPnet-SDS toolkit and provides comprehensive instructions for the users on navigating the demo interface.\"]},\"246\":{\"h\":\"Features\",\"t\":[\"Plug-and-Play Dialogue Systems\",\"Experiment with various ASR, LLM, and TTS systems by selecting from available options.\",\"Switch between: \",\"Cascaded Systems: Separate ASR, LLM, and TTS components.\",\"End-to-End Systems: Integrated processing pipelines.\",\"On-the-Fly Evaluation Metrics\",\"Evaluate system performance using: \",\"Latency: Response times.\",\"TTS Intelligibility: Intelligibility of synthesized speech.\",\"TTS Speech Quality: Clarity and quality of synthesized speech.\",\"ASR WER: Word Error Rate for transcription accuracy.\",\"Text Dialogue Metrics: Metrics like Perplexity and Diversity.\",\"Human Feedback Collection\",\"Rate system responses using buttons for: \",\"Naturalness: e.g., \\\"Very Natural\\\", \\\"Somewhat Awkward\\\".\",\"Relevance: e.g., \\\"Highly Relevant\\\", \\\"Slightly Irrelevant\\\".\",\"Optional integration with a remote HuggingFace dataset as a backend database, allowing researchers to store human relevance judgments and log user interaction data, including input recordings and system outputs such as ASR transcripts, text responses, and audio responses. \",\"upload_to_hub flag in app.py denotes the name of the remote HuggingFace dataset. If set to None, then this functionality is disabled.\"]},\"247\":{\"h\":\"How to Use\",\"t\":[\"Start the Demo\",\"Run run.sh in spoken_chatbot_arena/sds1 to run locally.\",\"You can optionally also visit our Voice Assistant Demo on HuggingFace Spaces.\",\"Wait for the interface to load.\",\"Choose a System Type\",\"Cascaded System:\",\"Combines separate Automatic Speech Recognition (ASR), Language Model (LLM), and Text-to-Speech (TTS) systems.\",\"E2E System:\",\"An integrated model where all tasks are handled jointly.\",\"Choose between Cascaded and E2E models.\",\"Configure the Components\",\"For a Cascaded System, configure the following: \",\"ASR: Select the model for speech-to-text conversion.\",\"LLM: Choose the language model for generating responses.\",\"TTS: Select the model for converting text responses to speech.\",\"For an E2E System, choose the integrated model\",\"Interact with the System\",\"Input Your Voice:\",\"Click the \\\"Microphone\\\" button and start speaking.\",\"The system will process your voice input and provide a synthesized speech response.\",\"View Outputs:\",\"ASR Output: Displays the text transcription of your speech.\",\"LLM Output: Shows the generated text response.\",\"Audio Output: Plays back the system's synthesized speech response.\",\"Evaluate Performance\",\"Select an evaluation metric from \\\"Choose Evaluation Metrics\\\" to analyze specific aspects of the system:\",\"Results will be displayed in the \\\"Evaluation Results\\\" box.\",\"Provide Feedback\",\"Use feedback buttons to rate the naturalness and relevance of the system's response. \",\"Naturalness: Rate the quality of the synthesized speech.\",\"Relevance: Rate how relevant the system's response was to your input.\"]},\"248\":{\"h\":\"Requirements\",\"t\":[\"Browser Compatibility\",\"Use Google Chrome for the best experience.\",\"Web interface might not be compatible with certain web browsers like Mozilla Firefox\",\"Internet Connection\",\"A stable internet connection is required to access the demo.\"]},\"249\":{\"h\":\"Troubleshooting\",\"t\":[\"When running locally, please follow these suggestions to debug:\",\"Gradio URL Issue: If audio doesn't work on the Gradio public URL, use the local URL for better performance.\",\"Docker Setup: Ensure your environment's package versions match those in the requirements file.\"]},\"250\":{\"h\":\"Spoken Language Understanding\",\"t\":[\"This is a template of slu1 recipe for ESPnet2.\"]},\"251\":{\"h\":\"Speech Language Model\",\"t\":[\"This is a template of speechlm1 recipe for ESPnet2.\"]},\"252\":{\"h\":\"Speaker Representation\",\"t\":[\"This is a template of Spk1 recipe for ESPnet2. It follows d-vector style training/inference for speaker verification. In other words, it trains a DNN as a closed set speaker classifier. After training the classification head is removed. The last hidden layer (or sometimes another layer) is used as a speaker representation (i.e., speaker embedding) to represent diverse open set speakers.\"]},\"253\":{\"h\":\"Table of Contents\",\"t\":[\"Recipe flow\",\"1. Data preparation\",\"2. Speed perturbation\",\"3. Wav format\",\"4. Spk statistics collection\",\"5. Spk training\",\"6. Speaker embedding extraction\",\"7. Score calculation\",\"8. Metric calculation\",\"9-10. (Optional) Pack results for upload\",\"How to run\",\"LibriSpeech training\",\"Related works\"]},\"254\":{\"h\":\"Recipe flow\",\"t\":[\"Spk1 recipe consists of 4 stages.\",\"Data preparation\",\"Data preparation stage.\",\"ESPnet format:\",\"It calls local/data.sh to create Kaldi-style data directories in data/ for training, validation, and evaluation sets. It's the same as asr1 tasks.\",\"See also:\",\"About Kaldi-style data directory\",\"Speed perturbation\",\"Generate train data with different speed offline, as a form of augmentation.\",\"Wav format\",\"Format the wave files in wav.scp to a single format (wav / flac / kaldi_ark).\",\"Spk statistics collection\",\"Statistics calculation stage. It collects the shape information of input and output texts for Spk training. Currently, it's close to a dummy because we set all utterances to have equal duration in the training phase.\",\"Spk training\",\"Spk model training stage. You can change the training setting via --spk_config and --spk_args options.\",\"See also:\",\"Change the configuration for training\",\"Distributed training\",\"Speaker embedding extraction\",\"Extracts speaker embeddings for inference. Speaker embeddings belonging to the evaluation set are extracted. If score_norm=true and/or qmf_func=true, cohort set(s) for score normalization and/or quality measure function is also extracted.\",\"Score calculation\",\"Calculates speaker similarity scores for an evaluation protocol (i.e., a set of trials). One scalar score is calcuated for each trial.\",\"This stage includes score normalization if set with --score_norm=true. This stage includes score normalization if set with --qmf_func=true.\",\"Metric calculation\",\"Calculates equal error rates (EERs) and minimum detection cost function (minDCF).\",\"9-10. (Optional) Pack results for upload\",\"Packing stage. It packs the trained model files and uploads to Huggingface. If you want to run this stage, you need to register your account in Huggingface.\"]},\"255\":{\"h\":\"How to run\",\"t\":[\"VoxCeleb Training\",\"Here, we show the procedure to run the recipe using egs2/voxceleb/spk1.\",\"Move to the recipe directory.\",\"$ cd egs2/voxceleb/spk1\",\"Modify VOXCELEB1, VOXCELEB2 variables in db.sh if you want to change the download directory.\",\"$ vim db.sh\",\"Modify cmd.sh and conf/*.conf if you want to use the job scheduler. See the detail in using job scheduling system.\",\"$ vim cmd.sh\",\"Run run.sh, which conducts all of the stages explained above.\",\"$ ./run.sh\"]},\"256\":{\"h\":\"Related works\",\"t\":[\"@INPROCEEDINGS{jung2022pushing, title={Pushing the limits of raw waveform speaker recognition}, author={Jung, Jee-weon and Kim, You Jin and Heo, Hee-Soo and Lee, Bong-Jin and Kwon, Youngki and Chung, Joon Son}, year={2022}, booktitle={Proc. INTERSPEECH} }\"]},\"257\":{\"h\":\"Self-supervised Learning\",\"t\":[\"This is a template of the ssl1 recipe for ESPnet2, designed for general purpose SSL.\"]},\"258\":{\"h\":\"Differences from other recipes\",\"t\":[\"ESPnet2 serves two different recipes for Self-Supervised Learning (SSL): ssl1 (this one) and hubert1.\",\"hubert1 is the original implementation of SSL under the HuBERT pre-training framework. The recipe takes care of everything need for pre-training, such as K-means pseudo-labelling and discrete token evaluation. This is very important for reproducibility. However, it is quite complicated due to the multiple offline stages required for HuBERT and therefore difficult to hack/adapt to new training methods or other scenarios.\",\"We created the new ssl1 recipe to future-proof the codebase to accomodate other pre-training techniques that are purely end-to-end, such as DinoSR, SpeechFlow, or w2v-BERT. This recipe is designed to be easily customizable and more scalable to large-scale pre-training setups.\"]},\"259\":{\"h\":\"HuBERT Pre-training in SSL1\",\"t\":[\"The ssl1 codebase also supports HuBERT pre-training, but the steps to create the pseudo-labels are not included in the recipe. Users will either need to run the hubert1 recipe to obtain the labels, or generate it themselves.\",\"To use the labels from hubert1, follow these steps\",\"Given a training set called train_ssl and a dev set called dev_ssl\",\"Run hubert1/hubert.sh from stages 1 to 5 for a single iteration. This will generate:\",\"A token vocabulary list. It will be called something like hubert1/data/en_token_list_kmeans_iter1_espnet_hubert_500clusters/word/tokens.txt. The name will depend on your exact hyperparameters.\",\"A pseudo-label text file for both sets. The exact path will depend on your hyperparameters, but will look something like hubert1/dump/&lt;feat_type&gt;/espnet_hubert/layer_&lt;x&gt;/&lt;data split name&gt;/pseudo_labels_km&lt;num&gt;.txt.\",\"Copy each pseudo_labels_km&lt;num&gt;.txt to the respective kaldi directly in ssl as text. For example: cp hubert1/dump/ssl_feats/espnet_hubert/layer_9/train_ssl/pseudo_labels_km500.txt ssl1/dump/train_ssl/text\",\"In ssl1/run.sh, add the following flags:\",\"--token_type word\",\"--token_list &lt;path to token list from step 1.1&gt;\",\"Update your training config with the used k-means size\",\"loss: - name: hubert conf: num_classes: &lt;update this&gt;\"]},\"260\":{\"h\":\"Speech-to-Text Translation\",\"t\":[\"This page is under construction\",\"Documentation for the ESPnet-ST-v2 project, to be presented at ACL 2023. This branch will be merged to the ESPnet master. More details can be found here.\",\"To use this development version, please clone this branch and then proceed with the normal ESPnet2 installation (Kaldi is not required):\",\"git clone https://github.com/brianyan918/espnet-ml.git\",\"git checkout md_pr\",\"Table of Contents\",\"Results\",\"Offline ST\",\"Simultaneous ST\",\"Offline ST Models\",\"Core Architectures\",\"Attentional Encoder-Decoder\",\"CTC/Attention\",\"Transducer\",\"Multi-Decoder\",\"Auxiliary Techniques\",\"ASR Pre-training\",\"SSL Front-end/Encoder\",\"LLM Decoder\",\"Hierarchical Encoding\",\"Code Components\",\"Tip on modifying code\"]},\"261\":{\"h\":\"Results\",\"t\":[\"The tables below provide a performance snapshot of different core architectures for offline and simultaneous ST. Links for downloading our example models or for building your own models from scratch are also provided.\",\"Here we're reporting MuST-C-v2 English-to-German results. These example models were trained using only the MuST-C-v2 ST corpus. No additional MT data was used. These models use ASR pre-training on the same data for faster convergence (ASR config).\",\"Offline ST\",\"Model\",\"BLEU\",\"Model Link\",\"Training Config\",\"Decoding Config\",\"Attentional Encoder-Decoder\",\"25.7\",\"link\",\"link\",\"Multi-Decoder Attn Enc-Dec\",\"27.6\",\"CTC/Attention\",\"28.6\",\"link\",\"link\",\"Multi-Decoder CTC/Attention\",\"28.8\",\"Transducer\",\"27.6\",\"Simultaneous ST\",\"Model\",\"BLEU\",\"AL\",\"Model Link\",\"Training Config\",\"Decoding Config\",\"Blockwise Attn Enc-Dec\",\"22.8\",\"3.23\",\"Label-Sync Blockwise CTC/Attn\",\"24.4\",\"3.23\",\"Time-Sync Blockwise CTC/Attn\",\"24.6\",\"2.34\",\"Blockwise Transducer\",\"22.9\",\"2.37\"]},\"262\":{\"h\":\"Offline ST Models\",\"t\":[\"Core Architectures\",\"Attentional Encoder-Decoder\",\"Attentional Encoder-Decoder\",\"Why choose this model?\",\"The attentional encoder-decoder is a commonly used model. Its relatively simple architecture makes it a solid base for experimenting with new auxiliary techniques and a reasonable choice for getting started with ST.\",\"Words of caution\",\"The most prominent weakness to be aware of is the end-detection problem: the auto-regressive decoder relies on a length penalty/bonus hyperparameter to stabilize output lengths. This hyperparameter reliance introduces risk of over-tuning.\",\"CTC/Attention\",\"CTC/Attention\",\"Why choose this model?\",\"The CTC/attention incorporates non-autoregressive hard alignment (CTC) and autoregressive soft alignment (attention) into a single model. CTC counteracts several weaknesses of its attentional counterpart via joint training/decoding (more details). Notably, the CTC/attention alleviates the end-detection problem of the pure attention approach. Compared to the attentional encoder-decoder, CTC/attention produces superior translation quality.\",\"Words of caution\",\"Joint training/decoding incurs an additional computational cost. Anecdotally, CTC/attention is 10-20% slower than pure attention.\",\"Transducer\",\"Transducer\",\"Why choose this model?\",\"The transducer is an autoregressive hard alignment model. Unlike models with an attentional decoder, transducer models typically use shallow LSTM decoders. Notably, the transducer's decoder avoids the quadratic computational complexity of its attentional counterpart -- inference is appreciably faster.\",\"Words of caution\",\"The translation quality lags behind that of CTC/attention due to its low capacity decoder. Further the transducer's loss function must marginalize over all possible alignment paths -- this makes training relatively slow. We also found that transducers are more difficult to train to convergence, likely due to the monotonic property of this framework. We solve this using a hierarchical encoding scheme (described in Section 4.1 of this paper) to encourage the encoder to take on the burden of re-ordering.\",\"Multi-Decoder\",\"Multi-Decoder CTC/Attention\",\"Why choose this model?\",\"The multi-decoder is an end-to-end differentiable cascade, consisting of an ASR subnet and an MT subnet. This approach inherits several strengths from cascaded approaches, the most prominent of which is the ability to perform search/retrieval over intermediate ASR representations (more details). The translation quality of the multi-decoder is greater than that of the attentional encoder decoder. The multi-decoder approach can also be applied to CTC/attention models -- this combination results in the strongest performance amongst our example models.\",\"Words of caution\",\"Multi-decoder inference involves two consecutive beam searches, one for the ASR subnet and one for the MT subnet. The model size is also greater than the single encoder-decoder (although trainable paramters are similar if using ASR multi-tasking for both approaches).\",\"Auxiliary Techniques\",\"ASR Pre-training\",\"Initializing encoder parameters from an ASR model is an easy way to speed up the convergence of ST training, allowing for more rapid experiment cycles. Models typically perform better with ASR pre-training as well.\",\"An example of ASR pre-training can be found in this config.\",\"SSL Front-end/Encoder\",\"We can leverage self-supervised learning representations as either front-end features or as an encoder initialization. The former method is typically less computationally intensive, as SSL models may be very large.\",\"An example of using an SSL front-end can be found in this config. More information about using SSL representations can be found here.\",\"LLM Decoder\",\"Initializing decoder parameters from a pre-trained large language model can greatly improve performance. This typically increases the model size drastically, but fewer iterations are required for convergence.\",\"An example of using a pre-trained LLM initialization can be found in this config.\",\"Hierarchical Encoding\",\"Building deeper, more sophisticated encoders can improve ST performance. We have found that hierarchical encoding, where initial layers are trained towards an ASR CTC objective and final layers are trained towards a ST CTC objective, encourages the encoder to take on more of the input-to-output re-ordering required for translation.\",\"An example of hierarchical encoding can be found in this config.\"]},\"263\":{\"h\":\"Code Components\",\"t\":[\"The three main code components can be found in espnet2/st/espnet_model.py, espnet2/tasks/st.py, and espnet2/bin/st_inference.py.\",\"espnet2/st/espnet_model.py defines the ST model initialization, forward pass, and loss functions.\",\"espnet2/tasks/st.py is the task wrapper which handles data loaders, training loops, and more.\",\"espnet2/bin/st_inference.py defines the Speech2Text API which handles inference.\",\"Tip on modifying code\",\"If you are developing new functions and wish to debug your new training/inference logic, you can use a Python debugger (e.g. pdb) and directly run Python commands (circumventing the recipe scripts).\",\"Once you have run the training or inference stage, ESPnet will have created a log file. At the top of these log files, you'll find the corresponding Python command. Note: for debugging you may want to set --multiprocessing_distributed False.\"]},\"264\":{\"h\":\"Singing Voice Synthesis\",\"t\":[\"This is a template of SVS recipe for ESPnet2.\"]},\"265\":{\"h\":\"Table of Contents\",\"t\":[\"Recipe flow\",\"1. Database-dependent data preparation\",\"2. Wav dump / Embedding preparation\",\"3. Filtering\",\"4. Token list generation\",\"5. SVS statistics collection\",\"6. SVS training\",\"7. SVS inference\",\"8. Objective evaluation\",\"9. Model packing\",\"How to run\",\"Naive_RNN training\",\"Naive_RNN_DP training\",\"XiaoiceSing training\",\"Diffsinger training\",\"VISinger (1+2) training\",\"VISinger 2 Plus training\",\"Singing Tacotron training\",\"Multi-speaker model with speaker ID embedding training\",\"Multi-language model with language ID embedding training\",\"Vocoder training\",\"Evaluation\",\"About data directory\",\"Score preparation - Case 1: phoneme annotation and standardized score - Case 2: phoneme annotation only\",\"Problems you might meet\",\"1. Wrong segmentation point\",\"2. Wrong lyric / midi annotation\",\"3. Different lyric-phoneme pairs against the given g2p\",\"4. Special marks in MusicXML\",\"Supported text cleaner\",\"Supported text frontend\",\"Supported Models\"]},\"266\":{\"h\":\"Recipe flow\",\"t\":[\"SVS recipe consists of 9 stages.\",\"Database-dependent data preparation\",\"Data preparation stage.\",\"It calls local/data.sh to creates Kaldi-style data directories but with additional score.scp and label in data/ for training, validation, and evaluation sets.\",\"See also:\",\"About data directory\",\"Score preparation\",\"Wav dump / Embedding preparation\",\"If you specify --feats_type raw option, this is a wav dumping stage which reformats wav.scp in data directories.\",\"Else, if you specify --feats_type fbank option or --feats_type stft option, this is a feature extracting stage (to be updated).\",\"Additionally, speaker ID embedding and language ID embedding preparation will be performed in this stage if you specify --use_sid true and --use_lid true options. Note that this processing assume that utt2spk or utt2lang are correctly created in stage 1, please be careful.\",\"Filtering\",\"Filtering stage.\",\"Processing stage to remove long and short utterances from the training and validation sets. You can change the threshold values via --min_wav_duration and --max_wav_duration.\",\"Empty text will also be removed.\",\"Token list generation\",\"Token list generation stage. It generates token list (dictionary) from srctexts. You can change the tokenization type via --token_type option. token_type=phn are supported. If --cleaner option is specified, the input text will be cleaned with the specified cleaner. If token_type=phn, the input text will be converted with G2P module specified by --g2p option.\",\"See also:\",\"Supported text cleaner.\",\"Supported text frontend.\",\"Data preparation will end in stage 4. You can skip data preparation (stage 1 ~ stage 4) via --skip_data_prep option.\",\"SVS statistics collection\",\"Statistics calculation stage. It collects the shape information of the input and output and calculates statistics for feature normalization (mean and variance over training and validation sets).\",\"In this stage, you can set --write_collected_feats true to store statistics of pitch and feats.\",\"SVS training\",\"SVS model training stage. You can change the training setting via --train_config and --train_args options.\",\"See also:\",\"Supported models.\",\"Change the configuration for training\",\"Distributed training\",\"Training process will end in stage 6. You can skip training process (stage 5 ~ stage 6) via --skip_train option.\",\"SVS inference\",\"SVS model decoding stage. You can change the decoding setting via --inference_config and --inference_args. Compatible vocoder can be trained and loaded.\",\"See also:\",\"Vocoder training\",\"Change the configuration for training\",\"Objective evaluation\",\"Evaluation stage. It conducts four objective evaluations. See also:\",\"Evaluation\",\"Model packing\",\"Packing stage. It packs the trained model files.\"]},\"267\":{\"h\":\"How to run\",\"t\":[\"Here, we show the procedure to run the recipe using egs2/ofuton_p_utagoe_db/svs1.\",\"Move on the recipe directory.\",\"$ cd egs2/ofuton_p_utagoe_db/svs1\",\"Modify OFUTON variable in db.sh if you want to change the download directory.\",\"$ vim db.sh\",\"Modify cmd.sh and conf/*.conf if you want to use job scheduler. See the detail in using job scheduling system.\",\"$ vim cmd.sh\",\"Run run.sh, which conducts all of the stages explained above.\",\"$ ./run.sh\",\"As a default, we train Naive_RNN (conf/train.yaml) with feats_type=raw + token_type=phn.\",\"Then, you can get the following directories in the recipe directory.\",\"├── data/ # Kaldi-style data directory │ ├── dev/ # validation set │ ├── eval/ # evaluation set │ ├── tr_no_dev/ # training set │ └── token_list/ # token list (directory) │ └── phn_none_jp/ # token list ├── dump/ # feature dump directory │ └── raw/ │ ├── org/ │ │ ├── tr_no_dev/ # training set before filtering │ │ └── dev/ # validation set before filtering │ ├── srctexts # text to create token list │ ├── eval/ # evaluation set │ ├── dev/ # validation set after filtering │ └── tr_no_dev/ # training set after filtering └── exp/ # experiment directory ├── svs_stats_raw_phn_none_jp # statistics │ ├── logdir/ # statistics calculation log directory │ ├── train/ # train statistics │ ├── valid/ # valid statistics └── svs_train_raw_phn_none_jp # model ├── tensorboard/ # tensorboard log ├── images/ # plot of training curves ├── valid/ # valid results ├── decode_train.loss.best/ # decoded results │ ├── dev/ # validation set │ └── eval/ # evaluation set │ ├── norm/ # generated features │ ├── denorm/ # generated denormalized features │ ├── MCD_res/ # mel-cepstral distortion │ ├── VUV_res/ # voiced/unvoiced error rate │ ├── SEMITONE_res/ # semitone accuracy │ ├── F0_res/ # log-F0 RMSE │ ├── wav/ # generated wav via vocoder │ ├── log/ # log directory │ ├── feats_type # feature type │ └── speech_shape # shape info of generated features ├── config.yaml # config used for the training ├── train.log # training log ├── *epoch.pth # model parameter file ├── checkpoint.pth # model + optimizer + scheduler parameter file ├── latest.pth # symlink to latest model parameter ├── *.ave_2best.pth # model averaged parameters └── *.best.pth # symlink to the best model parameter loss\",\"In decoding, you can see vocoder training to set vocoder.\",\"For the first time, we recommend performing each stage step-by-step via --stage and --stop_stage options.\",\"$ ./run.sh --stage 1 --stop_stage 1 $ ./run.sh --stage 2 --stop_stage 2 ... $ ./run.sh --stage 7 --stop_stage 7\",\"This might help you understand each stage's processing and directory structure.\",\"Naive_RNN training\",\"First, complete the data preparation:\",\"$ ./run.sh \\\\ --stage 1 \\\\ --stop_stage 4 \\\\ # for sample_rate 24000 hz $ ./run.sh \\\\ --fs 24000 \\\\ --n_shift 300 \\\\ --win_length 1200 \\\\ --stage 1 \\\\ --stop_stage 4 \\\\\",\"Warning: Please note that there is different setting in fs, n_shift and win_length in different model. The window shift n_shift and window length win_lenght are adapted to the sample rate fs.\",\"Second, check \\\"train_config\\\" (default conf/train.yaml), \\\"score_feats_extract\\\" (frame level in RNN) and modify \\\"vocoder_file\\\" with your own vocoder path.\",\"$ ./run.sh --stage 5 \\\\ --train_config conf/tuning/train_naive_rnn.yaml \\\\ --score_feats_extract frame_score_feats \\\\ --pitch_extract dio \\\\ --vocoder_file ${your vocoder path} \\\\\",\"Naive_RNN_DP training\",\"First, complete the data preparation:\",\"$ ./run.sh \\\\ --stage 1 \\\\ --stop_stage 4 \\\\ # for sample_rate 24000 hz $ ./run.sh \\\\ --fs 24000 \\\\ --n_shift 300 \\\\ --win_length 1200 \\\\ --stage 1 \\\\ --stop_stage 4 \\\\\",\"Warning: Please note that there is different setting in fs, n_shift and win_length in different model. The window shift n_shift and window length win_lenght are adapted to the sample rate fs.\",\"Second, check \\\"train_config\\\" (default conf/train.yaml), \\\"score_feats_extract\\\" (syllable level in RNN_DP) and modify \\\"vocoder_file\\\" with your own vocoder path.\",\"$ ./run.sh --stage 5 \\\\ --train_config conf/tuning/train_naive_rnn.yaml \\\\ --score_feats_extract syllable_score_feats \\\\ --pitch_extract dio \\\\ --vocoder_file ${your vocoder path} \\\\\",\"XiaoiceSing training\",\"First, complete the data preparation:\",\"$ ./run.sh \\\\ --stage 1 \\\\ --stop_stage 4 \\\\ # for sample_rate 24000 hz $ ./run.sh \\\\ --fs 24000 \\\\ --n_shift 300 \\\\ --win_length 1200 \\\\ --stage 1 \\\\ --stop_stage 4 \\\\\",\"Warning: Please note that there is different setting in fs, n_shift and win_length in different model. The window shift n_shift and window length win_lenght are adapted to the sample rate fs.\",\"Second, check \\\"train_config\\\" (default conf/train.yaml), \\\"score_feats_extract\\\" (syllable level in XiaoiceSing) and modify \\\"vocoder_file\\\" with your own vocoder path.\",\"$ ./run.sh --stage 5 \\\\ --train_config conf/tuning/train_naive_rnn.yaml \\\\ --score_feats_extract syllable_score_feats \\\\ --pitch_extract dio \\\\ --vocoder_file ${your vocoder path} \\\\\",\"Diffsinger training\",\"First, complete the data preparation:\",\"$ ./run.sh \\\\ --stage 1 \\\\ --stop_stage 4 \\\\ # for sample_rate 24000 hz $ ./run.sh \\\\ --fs 24000 \\\\ --n_shift 300 \\\\ --win_length 1200 \\\\ --stage 1 \\\\ --stop_stage 4 \\\\\",\"To train Diffsinger, you need to train a XiaoiceSing first(see XiaoiceSing training). And load pretrain model of XiaoiceSing as Diffsinger:FFTSinger.you can see details of --pretrained_modelhere.\",\"$ ./run.sh \\\\ --stage 5 \\\\ --train_config conf/tuning/train_diffsinger.yaml \\\\ --inference_config conf/tuning/decode_diffsinger.yaml \\\\ --score_feats_extract syllable_score_feats \\\\ --pitch_extract dio \\\\ --expdir exp/diffsinger \\\\ --inference_model latest.pth \\\\ --vocoder_file ${your vocoder path} \\\\ --pretrained_model ${your pretrained model path} \\\\ --use_feats_minmax true \\\\ # for example $ --pretrained_model /exp/xiaoice-2-24-250k/500epoch.pth:svs:svs.fftsinger \\\\\",\"VISinger (1+2) training\",\"The VISinger / VISinger 2 configs are hard coded for 22.05 khz or 44.1 khz and use different feature extraction method. (Note that you can use any feature extraction method but the default method is fbank.) If you want to use it with 24 khz or 16 khz dataset, please be careful about these points.\",\"First, check fs (Sampling Rate) and complete the data preparation:\",\"$ ./run.sh \\\\ --fs 44100 \\\\ --n_shift 512 \\\\ --win_length 2048 \\\\ --stage 1 \\\\ --stop_stage 4 \\\\\",\"Second, check \\\"train_config\\\" (default conf/train.yaml, you can also use --train_config ./conf/tuning/train_visinger2.yaml to train VISinger 2), \\\"score_feats_extract\\\" (syllable level in VISinger), \\\"svs_task\\\" (gan_svs in VISinger).\",\" # Single speaker 44100 hz case ./run.sh \\\\ --stage 5 \\\\ --fs 44100 \\\\ --n_fft 2048 \\\\ --n_shift 512 \\\\ --win_length 2048 \\\\ --svs_task gan_svs \\\\ --pitch_extract dio \\\\ --feats_extract fbank \\\\ --feats_normalize none \\\\ --score_feats_extract syllable_score_feats \\\\ --train_config ./conf/tuning/train_visinger.yaml \\\\ --inference_config conf/tuning/decode_vits.yaml \\\\ --inference_model latest.pth \\\\ --write_collected_feats true\",\"VISinger 2 Plus training\",\"VISinger 2 Plus has been accepted at SLT 2024. To use the pretrained model, you can download it from here. The model was trained using the OpenCpop dataset. Alternatively, you can choose the multi-singer ACE-OpenCpop dataset by downloading the model from here.\",\"To train the model, you can select multiple configurations to train on either the ACESinger or Opencpop dataset. The default setting in train_visinger2_plus_hubert.yaml uses hubert_large_ll60k as the pretrained SSL model. To switch the SSL model, you can modify the configuration by uncommenting the lines for MERT or Chinese Hubert.\",\"./run.sh \\\\ --stage 1 \\\\ --stop_stage 8 \\\\ --ngpu 1 \\\\ --fs 24000 \\\\ --n_fft 2048 \\\\ --n_shift 480 \\\\ --win_length 2048 \\\\ --dumpdir dump/24k \\\\ --expdir exp/24k \\\\ --svs_task gan_svs \\\\ --feats_extract fbank \\\\ --feats_normalize none \\\\ --train_config ./conf/tuning/train_visinger2_plus_hubert.yaml \\\\ --inference_config ./conf/tuning/decode_vits.yaml \\\\ --inference_model latest.pth \\\\ --write_collected_feats false\",\"Singing Tacotron training\",\"First, complete the data preparation:\",\"$ ./run.sh \\\\ --stage 1 \\\\ --stop_stage 4 \\\\ # for sample_rate 24000 hz $ ./run.sh \\\\ --fs 24000 \\\\ --n_shift 300 \\\\ --win_length 1200 \\\\ --stage 1 \\\\ --stop_stage 4 \\\\\",\"Warning: Please note that there is different setting in fs, n_shift and win_length in different model. The window shift n_shift and window length win_lenght are adapted to the sample rate fs.\",\"Second, check \\\"train_config\\\" (default conf/train.yaml), \\\"score_feats_extract\\\" (syllable level in Singing Tacotron) and modify \\\"vocoder_file\\\" with your own vocoder path.\",\"$ ./run.sh --stage 5 \\\\ --train_config conf/tuning/train_singing_tacotron.yaml \\\\ --inference_config conf/tuning/decode_singing_tacotron.yaml \\\\ --score_feats_extract syllable_score_feats \\\\ --vocoder_file ${your vocoder path} \\\\\",\"Multi-speaker model with speaker ID embedding training\",\"First, you need to run from the stage 2 and 3 with --use_sid true to extract speaker ID.\",\"$ ./run.sh --stage 2 --stop_stage 3 --use_sid true\",\"You can find the speaker ID file in dump/raw/*/utt2sid. Note that you need to correctly create utt2spk in data prep stage to generate utt2sid. Then, you can run the training with the config which has spks: #spks in svs_conf.\",\"# e.g. svs_conf: spks: 5 # Number of speakers\",\"Please run the training from stage 6.\",\"$ ./run.sh --stage 6 --use_sid true --train_config /path/to/your_multi_spk_config.yaml\",\"Multi-language model with language ID embedding training\",\"First, you need to run from the stage 2 and 3 with --use_lid true to extract speaker ID.\",\"$ ./run.sh --stage 2 --stop_stage 3 --use_lid true\",\"You can find the speaker ID file in dump/raw/*/utt2lid. Note that you need to additionally create utt2lang file in stage 1 to generate utt2lid. Then, you can run the training with the config which has langs: #langs in svs_conf.\",\"# e.g. svs_conf: langs: 4 # Number of languages\",\"Please run the training from stage 6.\",\"$ ./run.sh --stage 6 --use_lid true --train_config /path/to/your_multi_lang_config.yaml\",\"Of course you can further combine with speaker ID embedding. If you want to use both sid and lid, the process should be like this:\",\"$ ./run.sh --stage 2 --stop_stage 3 --use_lid true --use_sid true\",\"Make your config.\",\"# e.g. svs_conf: langs: 4 # Number of languages spks: 5 # Number of speakers\",\"Please run the training from stage 6.\",\"$ ./run.sh --stage 6 --use_lid true --use_sid true --train_config /path/to/your_multi_spk_multi_lang_config.yaml\",\"Vocoder training\",\"If your --vocoder_file is set to none, Griffin-Lim will be used. You can also train corresponding vocoder using kan-bayashi/ParallelWaveGAN..\",\"Pretrained vocoder is like follows:\",\"*_hifigan.v1 ├── checkpoint-xxxxxxsteps.pkl ├── config.yml └── stats.h5\",\"# Use the vocoder trained by `parallel_wavegan` repo manually $ ./run.sh --stage 7 --vocoder_file /path/to/checkpoint-xxxxxxsteps.pkl --inference_tag decode_with_my_vocoder\",\"Evaluation\",\"We provide four objective evaluation metrics:\",\"Mel-cepstral distortion (MCD)\",\"Logarithmic rooted mean square error of the fundamental frequency (log-F0 RMSE)\",\"Semitone accuracy (Semitone ACC)\",\"Voiced / unvoiced error rate (VUV_E)\",\"Word/character error rate (WER/CER, optional executated by users)\",\"For MCD, we apply dynamic time-warping (DTW) to match the length difference between ground-truth singing and generated singing.\",\"Here we show the example command to calculate objective metrics:\",\"cd egs2/&lt;recipe_name&gt;/svs1 . ./path.sh # Evaluate MCD & log-F0 RMSE & Semitone ACC & VUV Error Rate ./pyscripts/utils/evaluate_*.py \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval/wav/gen_wavdir_or_wavscp.scp \\\\ dump/raw/eval/gt_wavdir_or_wavscp.scp # Evaluate CER ./scripts/utils/evaluate_asr.sh \\\\ --model_tag &lt;asr_model_tag&gt; \\\\ --nj 1 \\\\ --inference_args \\\"--beam_size 10 --ctc_weight 0.4 --lm_weight 0.0\\\" \\\\ --gt_text \\\"dump/raw/eval1/text\\\" \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/asr_results # Since ASR model does not use punctuation, it is better to remove punctuations if it contains ./scripts/utils/remove_punctuation.pl &lt; dump/raw/eval1/text &gt; dump/raw/eval1/text.no_punc ./scripts/utils/evaluate_asr.sh \\\\ --model_tag &lt;asr_model_tag&gt; \\\\ --nj 1 \\\\ --inference_args \\\"--beam_size 10 --ctc_weight 0.4 --lm_weight 0.0\\\" \\\\ --gt_text \\\"dump/raw/eval1/text.no_punc\\\" \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/asr_results # You can also use openai whisper for evaluation ./scripts/utils/evaluate_asr.sh \\\\ --whisper_tag base \\\\ --nj 1 \\\\ --gt_text \\\"dump/raw/eval1/text\\\" \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/asr_results\",\"While these objective metrics can estimate the quality of synthesized singing, it is still difficult to fully determine human perceptual quality from these values, especially with high-fidelity generated singing. Therefore, we recommend performing the subjective evaluation (eg. MOS) if you want to check perceptual quality in detail.\",\"You can refer this page to launch web-based subjective evaluation system with webMUSHRA.\"]},\"268\":{\"h\":\"About data directory\",\"t\":[\"Each directory of training set, development set, and evaluation set, has same directory structure. See also https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE#about-kaldi-style-data-directory about Kaldi data structure. We recommend you running ofuton_p_utagoe_db recipe and checking the contents of data/ by yourself.\",\"cd egs/ofuton_p_utagoe_db/svs1 ./run.sh\",\"Directory structure\",\"data/ ├── tr_no_dev/ # Training set directory │ ├── text # The transcription │ ├── label # Specifying start and end time of the transcription │ ├── score.scp # Score file path │ ├── wav.scp # Wave file path │ ├── utt2spk # A file mapping utterance-id to speaker-id │ ├── spk2utt # A file mapping speaker-id to utterance-id │ ├── segments # [Option] Specifying start and end time of each utterance │ └── (utt2lang) # A file mapping utterance-id to language type (only for multilingual) ├── dev/ │ ... ├── eval/ │ ... └── token_list/ # token list directory ...\",\"text format\",\"uttidA &lt;transcription&gt; uttidB &lt;transcription&gt; ...\",\"label format\",\"uttidA (startA1, endA1, phA1) (startA2, endA2, phA1) ... uttidB (startB1, endB1, phB1) (startB2, endB2, phB2) ... ...\",\"score.scp format\",\"key1 /some/path/score.json key2 /some/path/score.json ...\",\"Note that for databases without explicit score or MusicXML, we will provide rule-based automatic music transcription to extract related music information in the future.\",\"wav.scp format\",\"uttidA /path/to/uttidA.wav uttidB /path/to/uttidB.wav ...\",\"utt2spk format\",\"uttidA speakerA uttidB speakerB uttidC speakerA uttidD speakerB ...\",\"spk2utt format\",\"speakerA uttidA uttidC ... speakerB uttidB uttidD ... ...\",\"Note that spk2utt file can be generated by utt2spk, and utt2spk can be generated by spk2utt, so it's enough to create either one of them.\",\"utils/utt2spk_to_spk2utt.pl data/tr_no_dev/utt2spk > data/tr_no_dev/spk2utt utils/spk2utt_to_utt2spk.pl data/tr_no_dev/spk2utt > data/tr_no_dev/utt2spk\",\"If your corpus doesn't include speaker information, give the same speaker id as the utterance id to satisfy the directory format, otherwise give the same speaker id for all utterances (Actually we don't use speaker information for asr recipe now).\",\"uttidA uttidA uttidB uttidB ...\",\"OR\",\"uttidA dummy uttidB dummy ...\",\"[Option] segments format\",\"If the audio data is originally long recording, about > ~1 hour, and each audio file includes multiple utterances in each section, you need to create segments file to specify the start time and end time of each utterance. The format is &lt;utterance_id&gt; &lt;wav_id&gt; &lt;start_time&gt; &lt;end_time&gt;.\",\"ofuton_0000000000000000hato_0007 ofuton_0000000000000000hato 33.470 38.013 ...\",\"Note that if using segments, wav.scp has &lt;wav_id&gt; which corresponds to the segments instead of utterance_id.\",\"ofuton_0000000000000000hato /path/to/ofuton_0000000000000000hato.wav ...\",\"utt2lang format\",\"uttidA languageA uttidB languageB uttidC languageA uttidD lagnuageB ...\",\"Note that utt2lang file is only generated for multilingual dataset (see in recipe egs/multilingual_four).\",\"Once you complete creating the data directory, it's better to check it by utils/validate_data_dir.sh.\",\"utils/validate_data_dir.sh --no-feats data/tr_no_dev utils/validate_data_dir.sh --no-feats data/dev utils/validate_data_dir.sh --no-feats data/test\"]},\"269\":{\"h\":\"Score preparation\",\"t\":[\"To prepare a new recipe, we first split songs into segments via --silence option if no official segmentation provided.\",\"Then, we transfer the raw data into score.json, where situations can be categorized into two cases depending on the annotation:\",\"Case 1: phoneme annotation and standardized score\",\"If the phonemes and notes are aligned in time domain, convert the raw data directly. (eg. Opencpop)\",\"If the phoneme annotation are misaligned with notes in time domain, align phonemes (from label) and note-lyric pairs (from musicXML) through g2p. (eg. Ofuton)\",\"We also offer some automatic fixes for missing silences in the dataset. During the stage1, when you encounter errors such as \\\"Lyrics are longer than phones\\\" or \\\"Phones are longer than lyrics\\\", the scripts will auto-generated the fixing code. You may need to put the code into the get_error_dict method in egs2/[dataset name]/svs1/local/prep_segments.py. Noted that depending on the suggested input_type, you may want to copy it into either the hts or xml's error_dict. (For more information, please check namine or natsume\",\"Specially, the note-lyric pairs can be rebuilt through other melody files, like MIDI, if there's something wrong with the note duration. (eg. Natsume)\",\"Case 2: phoneme annotation only\",\"To be updated.\",\"Problems you might meet\",\"During stage 1, which involves data preparation, you may encounter ValueError problems that typically indicate errors in the annotation. To address these issues, it is necessary to manually review the raw data in the corresponding sections and make the necessary corrections. While other toolkits and open-source codebases may not impose such requirements or checks, we have found that investing time to resolve these errors significantly enhances the quality of the singing voice synthesizer.\",\"Note that modifications can be made to the raw data locally or through the processing data flow at stage 1. For the convenience of open source, we recommend using the latter.\",\"To make changes to the raw data, you can use toolkits like music21, miditoolkit, or MuseScore.\",\"To process in the data flow, you can use score readers and writers provided. Examples can be found in functioin make_segment from egs2/{natsume, ameboshi, pjs}/svs1/local/{prep_segments.py, prep_segments_from_xml.py}/.\",\"Below are some common errors to watch out for:\",\"Wrong segmentation point\",\"Add pauses or directly split between adjacent lyrics.\",\"Remove pauses and assign the duration to correct phoneme.\",\"Wrong lyric / midi annotation\",\"Replace with correct one.\",\"Add missing one and reassign adjacent duration.\",\"Remove redundant one and reassign adjacent duration.\",\"Different lyric-phoneme pairs against the given g2p\",\"Use a customed_dic of syllable-phoneme pairs as following:\",\"# e.g. # In Japanese dataset ofuton, the output of \\\"ヴぁ\\\" from pyopenjtalk is different from raw data \\\"v a\\\" > pyopenjtalk.g2p(\\\"ヴぁ\\\") v u a # Add the following lyric-phoneme pair to customed_dic ヴぁ v_a\",\"Specify --g2p none and store the lyric-phoneme pairs into score.json, especially for polyphone problem in Mandarin.\",\"# e.g. # In Mandarin dataset Opencpop, the pronounce the second \\\"重\\\" should be \\\"chong\\\". > pypinyin.pinyin(\\\"情意深重爱恨两重\\\", style=Style.NORMAL) [['qing'], ['shen'], ['yi'], ['zhong'], ['ai'], ['hen'], ['liang'], ['zhong']]\",\"Special marks in MusicXML\",\"Breath: \",\"breath mark in note.articulations: usually appears at the end of the sentence. In some situations, breath mark doesn't take effect in its belonging note. Please handle them under local/.\",\"br in note.lyric. (solved in XMLReader)\",\"Special note with a fixed special pitch. (solved in XMLReader)\",\"Staccato: In some situations, there is a break when staccato occurs in note.articulations. We let users to decide whether to perform segmentation under local/.\"]},\"270\":{\"h\":\"Supported text cleaner\",\"t\":[\"You can change via --cleaner option in svs.sh.\",\"none: No text cleaner.\",\"You can see the code example from here.\"]},\"271\":{\"h\":\"Supported text frontend\",\"t\":[\"You can change via --g2p option in svs.sh.\",\"none: Just separate by space \",\"e.g.: HH AH0 L OW1 &lt;space&gt; W ER1 L D -> [HH, AH0, L, OW1, &lt;space&gt;, W, ER1, L D]\",\"pyopenjtalk: r9y9/pyopenjtalk\",\"e.g. こ、こんにちは -> [k, o, pau, k, o, N, n, i, ch, i, w, a]\",\"You can see the code example from here.\"]},\"272\":{\"h\":\"Supported Models\",\"t\":[\"You can train the following models by changing *.yaml config for --train_config option in run.sh.\",\"Naive-RNN\",\"XiaoiceSing\",\"VISinger\",\"VISinger 2\",\"Singing Tacotron\",\"You can find example configs of the above models in egs/ofuton_p_utagoe_db/svs1/conf/tuning.\"]},\"273\":{\"h\":\"ESPnet2 SVS2 Recipe TEMPLATE\",\"t\":[\"This is a template of SVS2 recipe for ESPnet2.\",\"SVS2 replace intermediate features in SVS1 (e.g. mel-spectrograms or continuous features) with discrete tokens, which are usually extracted from pretrained self-supervised learning (SSL) models or codec based models.\"]},\"274\":{\"h\":\"Table of Contents\",\"t\":[\"ESPnet2 SVS2 Recipe TEMPLATE\",\"Table of Contents\",\"Recipe flow\",\"1. Database-dependent data preparation\",\"2. Wav dump / Embedding preparation\",\"3. Filtering\",\"4. Discrete token extraction using pretrained model\",\"5. Token list generation\",\"6. SVS statistics collection\",\"7. SVS training\",\"8. SVS inference\",\"9. Objective evaluation\",\"10. Model packing\",\"How to run\",\"Naive_RNN_DP training\",\"XiaoiceSing training\",\"TokSing training\",\"Multi-speaker model with speaker ID embedding training\",\"Multi-language model with language ID embedding training\",\"Vocoder training\",\"Evaluation\",\"About data directory\",\"Score preparation\",\"Case 1: phoneme annotation and standardized score\",\"Case 2: phoneme annotation only\",\"Problems you might meet\",\"1. Wrong segmentation point\",\"2. Wrong lyric / midi annotation\",\"3. Different lyric-phoneme pairs against the given g2p\",\"4. Special marks in MusicXML\",\"Supported text cleaner\",\"Supported text frontend\",\"Supported Pretrained Model\",\"Supported Models\"]},\"275\":{\"h\":\"Recipe flow\",\"t\":[\"SVS recipe consists of 9 stages.\",\"Database-dependent data preparation\",\"Data preparation stage.\",\"It calls local/data.sh to creates Kaldi-style data directories but with additional score.scp and label in data/ for training, validation, and evaluation sets.\",\"See also:\",\"About data directory\",\"Score preparation\",\"Wav dump / Embedding preparation\",\"If you specify --feats_type raw option, this is a wav dumping stage which reformats wav.scp in data directories.\",\"Else, if you specify --feats_type fbank option or --feats_type stft option, this is a feature extracting stage (to be updated).\",\"Additionally, speaker ID embedding and language ID embedding preparation will be performed in this stage if you specify --use_sid true and --use_lid true options. Note that this processing assume that utt2spk or utt2lang are correctly created in stage 1, please be careful.\",\"Filtering\",\"Filtering stage.\",\"Processing stage to remove long and short utterances from the training and validation sets. You can change the threshold values via --min_wav_duration and --max_wav_duration.\",\"Empty text will also be removed.\",\"Discrete token extraction using pretrained model\",\"Discrete token extraction stage.\",\"Two types of discrete token are supported in SVS2: Single layer token and Multi layer token.\",\"Single layer token will only extract discrete token from single layer of the pretrained model. The RVQ extraction is also available and the follwing application of RVQ discrete tokens will be same as Multi layer token.\",\"Multi layer token will combine different layer discrete token from Single layer token or RVQ together in frame or sequence ways.\",\"More details can ben found in Data Preparation.\",\"See also:\",\"Supported pretrained model\",\"Token list generation\",\"Token list generation stage.\",\"It generates token list (dictionary) from srctexts. You can change the tokenization type via --token_type option. token_type=phn are supported. If --cleaner option is specified, the input text will be cleaned with the specified cleaner. If token_type=phn, the input text will be converted with G2P module specified by --g2p option.\",\"See also:\",\"Supported text cleaner.\",\"Supported text frontend.\",\"Data preparation will end in stage 4. You can skip data preparation (stage 1 ~ stage 4) via --skip_data_prep option.\",\"SVS statistics collection\",\"Statistics calculation stage. It collects the shape information of the input and output and calculates statistics for feature normalization (mean and variance over training and validation sets).\",\"In this stage, you can set --write_collected_feats true to store statistics of pitch and feats.\",\"SVS training\",\"SVS model training stage. You can change the training setting via --train_config and --train_args options.\",\"See also:\",\"Supported models.\",\"Change the configuration for training\",\"Distributed training\",\"Training process will end in stage 6. You can skip training process (stage 5 ~ stage 6) via --skip_train option.\",\"SVS inference\",\"SVS model decoding stage. You can change the decoding setting via --inference_config and --inference_args. Compatible vocoder can be trained and loaded.\",\"See also:\",\"Vocoder training\",\"Change the configuration for training\",\"Objective evaluation\",\"Evaluation stage. It conducts four objective evaluations. See also:\",\"Evaluation\",\"Model packing\",\"Packing stage. It packs the trained model files.\"]},\"276\":{\"h\":\"How to run\",\"t\":[\"Here, we show the procedure to run the recipe using egs2/opencpop/svs2.\",\"Move on the recipe directory.\",\"$ cd egs2/opencpop/svs2\",\"Modify OPENCPOP variable in db.sh if you want to change the download directory.\",\"$ vim db.sh\",\"Modify cmd.sh and conf/*.conf if you want to use job scheduler. See the detail in using job scheduling system.\",\"$ vim cmd.sh\",\"Run run.sh, which conducts all of the stages explained above.\",\"$ ./run.sh\",\"As a default, we train Naive_RNN (conf/train.yaml) with feats_type=raw + token_type=phn.\",\"Then, you can get the following directories in the recipe directory.\",\"├── data/ # Kaldi-style data directory │ ├── dev/ # validation set │ ├── eval/ # evaluation set │ ├── tr_no_dev/ # training set │ └── token_list/ # token list (directory) │ └── phn_none_zh/ # token list ├── dump/ # feature dump directory │ ├── raw/ │ │ ├── org/ │ │ │ ├── tr_no_dev/ # training set before filtering │ │ │ └── dev/ # validation set before filtering │ │ ├── srctexts # text to create token list │ │ ├── eval/ # evaluation set │ │ ├── dev/ # validation set after filtering │ │ └── tr_no_dev/ # training set after filtering │ └── extracted/ # log direcotry for feature extraction │ └── [model_name]/ # pretrained model workspace │ └── [layer_index]/ # specific layer for pretrained model │ ├── eval/ │ ├── dev/ │ ├── tr_no_dev/ │ └── tr_no_dev_subset[portion]/ # subset of train set └── exp/ # experiment directory ├── svs_stats_raw_phn_none_jp # statistics │ ├── logdir/ # statistics calculation log directory │ ├── train/ # train statistics │ ├── valid/ # valid statistics └── svs_train_raw_phn_none_jp # model ├── tensorboard/ # tensorboard log ├── images/ # plot of training curves ├── valid/ # valid results ├── decode_train.loss.best/ # decoded results │ ├── dev/ # validation set │ └── eval/ # evaluation set │ ├── norm/ # generated features │ ├── denorm/ # generated denormalized features │ ├── MCD_res/ # mel-cepstral distortion │ ├── VUV_res/ # voiced/unvoiced error rate │ ├── SEMITONE_res/ # semitone accuracy │ ├── F0_res/ # log-F0 RMSE │ ├── wav/ # generated wav via vocoder │ ├── log/ # log directory │ ├── feats_type # feature type │ └── speech_shape # shape info of generated features ├── config.yaml # config used for the training ├── train.log # training log ├── *epoch.pth # model parameter file ├── checkpoint.pth # model + optimizer + scheduler parameter file ├── latest.pth # symlink to latest model parameter ├── *.ave_2best.pth # model averaged parameters └── *.best.pth # symlink to the best model parameter loss\",\"In decoding, you can see vocoder training to set vocoder.\",\"For the first time, we recommend performing each stage step-by-step via --stage and --stop_stage options.\",\"$ ./run.sh --stage 1 --stop_stage 1 $ ./run.sh --stage 2 --stop_stage 2 ... $ ./run.sh --stage 8 --stop_stage 8\",\"This might help you understand each stage's processing and directory structure.\",\"Data Preparation\",\"First, ensure that the settings in run.sh such as fs and n_shift match those in pretrained models (use 6th layer of model wavlm_large as the exmpale).\",\"Complete the dataset preprocess:\",\"$ ./run.sh \\\\ --stage 1 \\\\ --stop_stage 3 \\\\ --fs 16000 \\\\ --n_shift 320\",\"NOTICE: This setting is mainly for self-supervised learning (SSL) model.\",\"Then, set the configuration for discrete token, including nclusters, RVQ_layers and kmeans_feature. If multi layer token is chosen or RVQ_layer is greater than 1, configurations multi_token and mix_type should also be specified.\",\"Single layer token\",\"In Single layer token, it extracts discrete tokens from single layer feature of pretrained model and RVQ exctraction is also available.\",\"In this setting, kmeans_feature is in the format [model]/[layer index], where [model] shows the pretrained model and [layer index] represents the layer number in pretrained model.\",\"For exmpale, if you want to get discrete tokens from 3rd layer of HuBERT base model, you can set kmeans_feature=\\\"hubert_base/3\\\" and RVQ_layers=1. nclusters is usually set at 128 or 1024.\",\"If RVQ_layers is greater than 1, discret token are extracted in residual vector quantization way, which will generate RVQ_layers layers of discrete tokens finally. The following application of RVQ discrete tokens is used in Multi layer token.\",\"Multi layer token\",\"Multi layer token will combine multi layers of discrete tokens extracted in Single layer token or RVQ. Therefore, to utilize this type of tokens, you should get corresponding discrete tokens in Single layer token at first.\",\"In this setting, kmeans_feature should be specified in the format multi/[tag] where [tag] is the new name for combined tokens. Discrete tokens in all layers should be listed in multi_token, seprated by a blank space.\",\"For exmpale, if you want to combine discrete tokens from 6th layer of hubert large, 6th layer and 23rd layer of wavlm large, you should set multi_token=\\\"hubert_large_ll60k_128_6_RVQ_0 wavlm_large_128_6_RVQ_0 wavlm_large_128_23_RVQ_0\\\" with the setting RVQ_layers=2. If the setting is RVQ_layers=1, the configuration will be multi_token=\\\"hubert_large_ll60k_128_6 wavlm_large_128_6 wavlm_large_128_23\\\".\",\"When input these token into SVS2 model, it should be mix in a single token sequence. You can set mix_type=\\\"frame\\\" or mix_type=\\\"sequence\\\" to get the single token sequence.\",\"Exmpale for mix_type:\",\"l_A=[1, 2, 3] l_B=[a, b, c]. # mix l_A and l_B \\\"frame\\\": [1, a, 2, b, 3, c] # frame by frame \\\"sequence\\\": [1, 2, 3, a, b, c] # sequence by sequence\",\"Extract discrete token:\",\"# Single layer token (RVQ_layers=1) $ ./run.sh \\\\ --stage 4 \\\\ --stop_stage 4 \\\\ --fs 16000 \\\\ --n_shift 320 \\\\ --nclusters 128 \\\\ --RVQ_layers 1 \\\\ --kmeans_feature wavlm_large/6 $ ./run.sh \\\\ --stage 4 \\\\ --stop_stage 4 \\\\ --fs 16000 \\\\ --n_shift 320 \\\\ --nclusters 128 \\\\ --RVQ_layers 1 \\\\ --kmeans_feature wavlm_large/23 # Multi layer token(RVQ_layers=1) $ ./run.sh \\\\ --stage 4 \\\\ --stop_stage 4 \\\\ --fs 16000 \\\\ --n_shift 320 \\\\ --nclusters 128 \\\\ --RVQ_layers 1 \\\\ --mix_type frame \\\\ --kmeans_feature multi/wavlm_large_6+wavlm_large_23 \\\\ --multi_token \\\"wavlm_large_128_6 wavlm_large_128_23\\\" # Single layer token (RVQ_layers>1) $ ./run.sh \\\\ --stage 4 \\\\ --stop_stage 4 \\\\ --fs 16000 \\\\ --n_shift 320 \\\\ --nclusters 128 \\\\ --RVQ_layers 3 \\\\ --kmeans_feature wavlm_large/6 # Multi layer token(RVQ_layers>1) $ ./run.sh \\\\ --stage 4 \\\\ --stop_stage 4 \\\\ --fs 16000 \\\\ --n_shift 320 \\\\ --nclusters 128 \\\\ --RVQ_layers 3 \\\\ --mix_type frame \\\\ --kmeans_feature multi/wavlm_large_6_RVQ_0+1+2 \\\\ --multi_token \\\"wavlm_large_128_6_RVQ_0 wavlm_large_128_6_RVQ_1 wavlm_large_128_6_RVQ_2\\\"\",\"Naive_RNN_DP training\",\"First, complete the data preparation.\",\"Second, check \\\"train_config\\\" (default conf/train.yaml), \\\"score_feats_extract\\\" (syllable level in RNN_DP) and modify \\\"vocoder_file\\\" with your own vocoder path.\",\"$ ./run.sh --stage 6 \\\\ --train_config conf/tuning/train_naive_rnn_dp.yaml \\\\ --score_feats_extract syllable_score_feats \\\\ --pitch_extract dio \\\\ --vocoder_file ${your vocoder path} \\\\ ${command in data preparation}\",\"XiaoiceSing training\",\"First, complete the data preparation.\",\"Second, check \\\"train_config\\\" (default conf/train.yaml), \\\"score_feats_extract\\\" (syllable level in XiaoiceSing) and modify \\\"vocoder_file\\\" with your own vocoder path.\",\"$ ./run.sh --stage 6 \\\\ --train_config conf/tuning/train_xiaoice.yaml \\\\ --score_feats_extract syllable_score_feats \\\\ --pitch_extract dio \\\\ --vocoder_file ${your vocoder path} \\\\ ${command in data preparation}\",\"TokSing training\",\"First, complete the data preparation.\",\"Second, check \\\"train_config\\\" (default conf/train.yaml), \\\"score_feats_extract\\\" (syllable level in XiaoiceSing) and modify \\\"vocoder_file\\\" with your own vocoder path.\",\"$ ./run.sh --stage 6 \\\\ --train_config conf/tuning/train_discrete_acoustic.yaml \\\\ --score_feats_extract syllable_score_feats \\\\ --pitch_extract dio \\\\ --vocoder_file ${your vocoder path} \\\\ ${command in data preparation}\",\"Multi-speaker model with speaker ID embedding training\",\"First, you need to run from the stage 2 and 3 with --use_sid true to extract speaker ID.\",\"$ ./run.sh --stage 2 --stop_stage 3 --use_sid true\",\"You can find the speaker ID file in dump/raw/*/utt2sid. Note that you need to correctly create utt2spk in data prep stage to generate utt2sid. Then, you can run the training with the config which has spks: #spks in svs_conf.\",\"# e.g. svs_conf: spks: 5 # Number of speakers\",\"Please run the training from stage 6.\",\"$ ./run.sh --stage 6 --use_sid true --train_config /path/to/your_multi_spk_config.yaml\",\"Multi-language model with language ID embedding training\",\"First, you need to run from the stage 2 and 3 with --use_lid true to extract speaker ID.\",\"$ ./run.sh --stage 2 --stop_stage 3 --use_lid true\",\"You can find the speaker ID file in dump/raw/*/utt2lid. Note that you need to additionally create utt2lang file in stage 1 to generate utt2lid. Then, you can run the training with the config which has langs: #langs in svs_conf.\",\"# e.g. svs_conf: langs: 4 # Number of languages\",\"Please run the training from stage 6.\",\"$ ./run.sh --stage 6 --use_lid true --train_config /path/to/your_multi_lang_config.yaml\",\"Of course you can further combine with speaker ID embedding. If you want to use both sid and lid, the process should be like this:\",\"$ ./run.sh --stage 2 --stop_stage 3 --use_lid true --use_sid true\",\"Make your config.\",\"# e.g. svs_conf: langs: 4 # Number of languages spks: 5 # Number of speakers\",\"Please run the training from stage 6.\",\"$ ./run.sh --stage 6 --use_lid true --use_sid true --train_config /path/to/your_multi_spk_multi_lang_config.yaml\",\"Vocoder training\",\"If your --vocoder_file is set to none, Griffin-Lim will be used. You can also train corresponding vocoder using kan-bayashi/ParallelWaveGAN.\",\"Pretrained vocoder is like follows:\",\"*_hifigan.v1 ├── checkpoint-xxxxxxsteps.pkl ├── config.yml └── stats.h5\",\"# Use the vocoder trained by `parallel_wavegan` repo manually $ ./run.sh --stage 8 --vocoder_file /path/to/checkpoint-xxxxxxsteps.pkl --inference_tag decode_with_my_vocoder\",\"Evaluation\",\"We provide four objective evaluation metrics:\",\"Mel-cepstral distortion (MCD)\",\"Logarithmic rooted mean square error of the fundamental frequency (log-F0 RMSE)\",\"Semitone accuracy (Semitone ACC)\",\"Voiced / unvoiced error rate (VUV_E)\",\"Word/character error rate (WER/CER, optional executated by users)\",\"pseudo MOS (coming soon)\",\"For MCD, we apply dynamic time-warping (DTW) to match the length difference between ground-truth singing and generated singing.\",\"Here we show the example command to calculate objective metrics:\",\"cd egs2/&lt;recipe_name&gt;/svs1 . ./path.sh # Evaluate MCD & log-F0 RMSE & Semitone ACC & VUV Error Rate ./pyscripts/utils/evaluate_*.py \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval/wav/gen_wavdir_or_wavscp.scp \\\\ dump/raw/eval/gt_wavdir_or_wavscp.scp # Evaluate CER ./scripts/utils/evaluate_asr.sh \\\\ --model_tag &lt;asr_model_tag&gt; \\\\ --nj 1 \\\\ --inference_args \\\"--beam_size 10 --ctc_weight 0.4 --lm_weight 0.0\\\" \\\\ --gt_text \\\"dump/raw/eval1/text\\\" \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/asr_results # Since ASR model does not use punctuation, it is better to remove punctuations if it contains ./scripts/utils/remove_punctuation.pl &lt; dump/raw/eval1/text &gt; dump/raw/eval1/text.no_punc ./scripts/utils/evaluate_asr.sh \\\\ --model_tag &lt;asr_model_tag&gt; \\\\ --nj 1 \\\\ --inference_args \\\"--beam_size 10 --ctc_weight 0.4 --lm_weight 0.0\\\" \\\\ --gt_text \\\"dump/raw/eval1/text.no_punc\\\" \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/asr_results # You can also use openai whisper for evaluation ./scripts/utils/evaluate_asr.sh \\\\ --whisper_tag base \\\\ --nj 1 \\\\ --gt_text \\\"dump/raw/eval1/text\\\" \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/asr_results\",\"While these objective metrics can estimate the quality of synthesized singing, it is still difficult to fully determine human perceptual quality from these values, especially with high-fidelity generated singing. Therefore, we recommend performing the subjective evaluation (eg. MOS) if you want to check perceptual quality in detail.\",\"You can refer this page to launch web-based subjective evaluation system with webMUSHRA.\"]},\"277\":{\"h\":\"About data directory\",\"t\":[\"Each directory of training set, development set, and evaluation set, has same directory structure. See also https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE#about-kaldi-style-data-directory about Kaldi data structure. We recommend you running opencpop recipe and checking the contents of data/ by yourself.\",\"cd egs/opencpop/svs2 ./run.sh\",\"Directory structure\",\"data/ ├── tr_no_dev/ # Training set directory │ ├── text # The transcription │ ├── label # Specifying start and end time of the transcription │ ├── score.scp # Score file path │ ├── wav.scp # Wave file path │ ├── utt2spk # A file mapping utterance-id to speaker-id │ ├── spk2utt # A file mapping speaker-id to utterance-id │ ├── segments # [Option] Specifying start and end time of each utterance │ └── (utt2lang) # A file mapping utterance-id to language type (only for multilingual) ├── dev/ │ ... ├── eval/ │ ... └── token_list/ # token list directory ...\",\"text format\",\"uttidA &lt;transcription&gt; uttidB &lt;transcription&gt; ...\",\"label format\",\"uttidA (startA1, endA1, phA1) (startA2, endA2, phA1) ... uttidB (startB1, endB1, phB1) (startB2, endB2, phB2) ... ...\",\"score.scp format\",\"key1 /some/path/score.json key2 /some/path/score.json ...\",\"Note that for databases without explicit score or MusicXML, we will provide rule-based automatic music transcription to extract related music information in the future.\",\"wav.scp format\",\"uttidA /path/to/uttidA.wav uttidB /path/to/uttidB.wav ...\",\"utt2spk format\",\"uttidA speakerA uttidB speakerB uttidC speakerA uttidD speakerB ...\",\"spk2utt format\",\"speakerA uttidA uttidC ... speakerB uttidB uttidD ... ...\",\"Note that spk2utt file can be generated by utt2spk, and utt2spk can be generated by spk2utt, so it's enough to create either one of them.\",\"utils/utt2spk_to_spk2utt.pl data/tr_no_dev/utt2spk > data/tr_no_dev/spk2utt utils/spk2utt_to_utt2spk.pl data/tr_no_dev/spk2utt > data/tr_no_dev/utt2spk\",\"If your corpus doesn't include speaker information, give the same speaker id as the utterance id to satisfy the directory format, otherwise give the same speaker id for all utterances (Actually we don't use speaker information for asr recipe now).\",\"uttidA uttidA uttidB uttidB ...\",\"OR\",\"uttidA dummy uttidB dummy ...\",\"[Option] segments format\",\"If the audio data is originally long recording, about > ~1 hour, and each audio file includes multiple utterances in each section, you need to create segments file to specify the start time and end time of each utterance. The format is &lt;utterance_id&gt; &lt;wav_id&gt; &lt;start_time&gt; &lt;end_time&gt;.\",\"ofuton_0000000000000000hato_0007 ofuton_0000000000000000hato 33.470 38.013 ...\",\"Note that if using segments, wav.scp has &lt;wav_id&gt; which corresponds to the segments instead of utterance_id.\",\"ofuton_0000000000000000hato /path/to/ofuton_0000000000000000hato.wav ...\",\"utt2lang format\",\"uttidA languageA uttidB languageB uttidC languageA uttidD lagnuageB ...\",\"Note that utt2lang file is only generated for multilingual dataset (see in recipe egs/multilingual_four).\",\"Once you complete creating the data directory, it's better to check it by utils/validate_data_dir.sh.\",\"utils/validate_data_dir.sh --no-feats data/tr_no_dev utils/validate_data_dir.sh --no-feats data/dev utils/validate_data_dir.sh --no-feats data/test\"]},\"278\":{\"h\":\"Score preparation\",\"t\":[\"To prepare a new recipe, we first split songs into segments via --silence option if no official segmentation provided.\",\"Then, we transfer the raw data into score.json, where situations can be categorized into two cases depending on the annotation:\",\"Case 1: phoneme annotation and standardized score\",\"If the phonemes and notes are aligned in time domain, convert the raw data directly. (eg. Opencpop)\",\"If the phoneme annotation are misaligned with notes in time domain, align phonemes (from label) and note-lyric pairs (from musicXML) through g2p. (eg. Ofuton)\",\"We also offer some automatic fixes for missing silences in the dataset. During the stage1, when you encounter errors such as \\\"Lyrics are longer than phones\\\" or \\\"Phones are longer than lyrics\\\", the scripts will auto-generated the fixing code. You may need to put the code into the get_error_dict method in egs2/[dataset name]/svs1/local/prep_segments.py. Noted that depending on the suggested input_type, you may want to copy it into either the hts or xml's error_dict. (For more information, please check namine or natsume\",\"Specially, the note-lyric pairs can be rebuilt through other melody files, like MIDI, if there's something wrong with the note duration. (eg. Natsume)\",\"Case 2: phoneme annotation only\",\"To be updated.\",\"Problems you might meet\",\"During stage 1, which involves data preparation, you may encounter ValueError problems that typically indicate errors in the annotation. To address these issues, it is necessary to manually review the raw data in the corresponding sections and make the necessary corrections. While other toolkits and open-source codebases may not impose such requirements or checks, we have found that investing time to resolve these errors significantly enhances the quality of the singing voice synthesizer.\",\"Note that modifications can be made to the raw data locally or through the processing data flow at stage 1. For the convenience of open source, we recommend using the latter.\",\"To make changes to the raw data, you can use toolkits like music21, miditoolkit, or MuseScore.\",\"To process in the data flow, you can use score readers and writers provided. Examples can be found in functioin make_segment from egs2/{natsume, ameboshi, pjs}/svs1/local/{prep_segments.py, prep_segments_from_xml.py}/.\",\"Below are some common errors to watch out for:\",\"Wrong segmentation point\",\"Add pauses or directly split between adjacent lyrics.\",\"Remove pauses and assign the duration to correct phoneme.\",\"Wrong lyric / midi annotation\",\"Replace with correct one.\",\"Add missing one and reassign adjacent duration.\",\"Remove redundant one and reassign adjacent duration.\",\"Different lyric-phoneme pairs against the given g2p\",\"Use a customed_dic of syllable-phoneme pairs as following:\",\"# e.g. # In Japanese dataset ofuton, the output of \\\"ヴぁ\\\" from pyopenjtalk is different from raw data \\\"v a\\\" > pyopenjtalk.g2p(\\\"ヴぁ\\\") v u a # Add the following lyric-phoneme pair to customed_dic ヴぁ v_a\",\"Specify --g2p none and store the lyric-phoneme pairs into score.json, especially for polyphone problem in Mandarin.\",\"# e.g. # In Mandarin dataset Opencpop, the pronounce the second \\\"重\\\" should be \\\"chong\\\". > pypinyin.pinyin(\\\"情意深重爱恨两重\\\", style=Style.NORMAL) [['qing'], ['shen'], ['yi'], ['zhong'], ['ai'], ['hen'], ['liang'], ['zhong']]\",\"Special marks in MusicXML\",\"Breath: \",\"breath mark in note.articulations: usually appears at the end of the sentence. In some situations, breath mark doesn't take effect in its belonging note. Please handle them under local/.\",\"br in note.lyric. (solved in XMLReader)\",\"Special note with a fixed special pitch. (solved in XMLReader)\",\"Staccato: In some situations, there is a break when staccato occurs in note.articulations. We let users to decide whether to perform segmentation under local/.\"]},\"279\":{\"h\":\"Supported text cleaner\",\"t\":[\"You can change via --cleaner option in svs.sh.\",\"none: No text cleaner.\",\"You can see the code example from here.\"]},\"280\":{\"h\":\"Supported text frontend\",\"t\":[\"You can change via --g2p option in svs.sh.\",\"none: Just separate by space \",\"e.g.: HH AH0 L OW1 &lt;space&gt; W ER1 L D -> [HH, AH0, L, OW1, &lt;space&gt;, W, ER1, L D]\",\"pyopenjtalk: r9y9/pyopenjtalk\",\"e.g. こ、こんにちは -> [k, o, pau, k, o, N, n, i, ch, i, w, a]\",\"You can see the code example from here.\"]},\"281\":{\"h\":\"Supported pretrained model\",\"t\":[\"Self supervised learning model: HuBERT, WavLM, wav2vec2.0, ...\",\"Codec model\"]},\"282\":{\"h\":\"Supported Models\",\"t\":[\"You can train the following models by changing *.yaml config for --train_config option in run.sh.\",\"Naive-RNN\",\"XiaoiceSing\",\"TokSing\",\"You can find example configs of the above models in egs/opencpop/svs2/conf/tuning.\"]},\"283\":{\"h\":\"Text-to-Speech\",\"t\":[\"This is a template of TTS recipe for ESPnet2.\"]},\"284\":{\"h\":\"Table of Contents\",\"t\":[\"Text-to-Speech\",\"Table of Contents\",\"Recipe flow\",\"1. Data preparation\",\"ESPnet format:\",\"(New) MFA Aligments generation\",\"2. Wav dump / Embedding preparation\",\"3. Extract speaker embeddings\",\"4. Removal of long / short data\",\"5. Token list generation\",\"6. TTS statistics collection\",\"7. TTS training\",\"8. TTS decoding\",\"9. TTS eval using versa\",\"10. (Optional) Pack results for upload\",\"11. (Optional) Upload model to Hugging Face\",\"How to run\",\"FastSpeech training\",\"FastSpeech2 training\",\"Multi-speaker model with speaker embedding training\",\"(Optional) Train on speaker-averaged speaker embeddings\",\"Multi-speaker model with speaker ID embedding training\",\"Multi-language model with language ID embedding training\",\"VITS training\",\"Joint text2wav training\",\"Evaluation\",\"Supported text frontend\",\"Supported text cleaner\",\"Supported Models\",\"Single speaker model\",\"Multi speaker model extension\",\"FAQ\",\"ESPnet1 model is compatible with ESPnet2?\",\"How to change minibatch size in training?\",\"How to make a new recipe for my own dataset?\",\"How to add a new g2p module?\",\"How to add a new cleaner module?\",\"How to use trained model in python?\",\"How to get pretrained models?\",\"How to load the pretrained parameters?\",\"How to finetune the pretrained model?\",\"How to add a new model?\",\"How to test my model with an arbitrary given text?\",\"How to train vocoder?\",\"How to train vocoder with text2mel GTA outputs?\",\"How to handle the errors in validate_data_dir.sh?\",\"Why the model generate meaningless speech at the end?\",\"Why the model cannot be trained well with my own dataset?\",\"Why the outputs contains metallic noise when combining neural vocoder?\",\"How is the duration for FastSpeech2 generated?\",\"Why the output of Tacotron2 or Transformer is non-deterministic?\"]},\"285\":{\"h\":\"Recipe flow\",\"t\":[\"TTS recipe consists of 9 stages.\",\"Data preparation\",\"Data preparation stage. You have two methods to generate the data:\",\"ESPnet format:\",\"It calls local/data.sh to creates Kaldi-style data directories in data/ for training, validation, and evaluation sets.\",\"See also:\",\"About Kaldi-style data directory\",\"(New) MFA Aligments generation\",\"You can generate aligments using the Montreal-Forced-Aligner tool Use the script scripts/mfa.sh to generate the required mfa aligments and train a model that employs these alignments.\",\"Because the script scripts/mfa.sh prepares the data, it is not required to execute local/data.sh previously. However, you will need to set some additional flags, such as --split_sets, --samplerate, or --acoustic_model:\",\"./scripts/mfa.sh --split_sets \\\"train_set dev_set test_set\\\" \\\\ --stage 1 \\\\ --stop-stage 2 \\\\ --train true --nj 36 --g2p_model espeak_ng_english_vits\",\"You can find a reference at egs2/ljspeech/tts1/local/run_mfa.sh.\",\"The script scripts/mfa.sh will generate the aligments using a given g2p_model & acoustic_model and store it in the &lt;split_sets&gt;_phn directory. This script download a pretrained model (if --train false) or trains the mfa g2p and acoustic model (if --train true), for then generate the aligments.\",\"Then, you can continue the training on the main script:\",\"./run.sh --train-set train_set_phn \\\\ --dev-set dev_set_phn \\\\ --test_sets \\\"dev_set_phn test_set_phn\\\" \\\\ --stage 2 \\\\ --g2p none \\\\ --cleaner none \\\\ --teacher_dumpdir \\\"data\\\"\",\"Wav dump / Embedding preparation\",\"Wav dumping stage. This stage reformats wav.scp in data directories.\",\"Additionally, We support speaker embedding extraction in this stage as you can use in ESPnet1. If you specify --use_spk_embed true (Default: use_spk_embed=false), we extract speaker embeddings. You can select the type of toolkit to use (kaldi, speechbrain, or espnet) when you specify --spk_embed_tool &lt;option&gt; (Default: spk_embed_tool=espnet). If you specify kaldi, then we additionally extract mfcc features and vad decision. In that case, spk_embed_tag will be set to xvector automatically. This processing requires the compiled kaldi, please be careful.\",\"Also, speaker ID embedding and language ID embedding preparation will be performed in this stage if you specify --use_sid true and --use_lid true options. Note that this processing assume that utt2spk or utt2lang are correctly created in stage 1, please be careful.\",\"Extract speaker embeddings\",\"Extract speaker embeddings.\",\"Removal of long / short data\",\"Processing stage to remove long and short utterances from the training and validation data. You can change the threshold values via --min_wav_duration and --max_wav_duration.\",\"Token list generation\",\"Token list generation stage. It generates token list (dictionary) from srctexts. You can change the tokenization type via --token_type option. token_type=char and token_type=phn are supported. If --cleaner option is specified, the input text will be cleaned with the specified cleaner. If token_type=phn, the input text will be converted with G2P module specified by --g2p option.\",\"See also:\",\"Supported text cleaner.\",\"Supported text frontend.\",\"TTS statistics collection\",\"Statistics calculation stage. It collects the shape information of the input and output and calculates statistics for feature normalization (mean and variance over training data).\",\"TTS training\",\"TTS model training stage. You can change the training setting via --train_config and --train_args options.\",\"See also:\",\"Supported models.\",\"Change the configuration for training\",\"Distributed training\",\"TTS decoding\",\"TTS model decoding stage. You can change the decoding setting via --inference_config and --inference_args.\",\"See also:\",\"Change the configuration for training\",\"TTS eval using versa\",\"TTS model eval stage using versa.\",\"The default metrics is below in conf/versa.yaml:\",\"| 1 | Perceptual Evaluation of Speech Quality (PESQ) | pesq | pesq | | 2 | Short-Time Objective Intelligibility (STOI) | stoi | stoi | | 3 | Mel Cepstral Distortion (MCD) | mcd_f0 | mcd | | 4 | F0 Correlation | mcd_f0 | f0_corr | | 5 | F0 Root Mean Square Error | mcd_f0 | f0_rmse | | 6 | UTokyo-SaruLab System for VoiceMOS Challenge 2022 (UTMOS) | pseudo_mos | utmos | | 7 | Deep Noise Suppression MOS Score of P.835 (DNSMOS) | pseudo_mos | dnsmos_overall | | 8 | Deep Noise Suppression MOS Score of P.808 (DNSMOS) | pseudo_mos | dnsmos_p808 | | 9 | Packet Loss Concealment-related MOS Score (PLCMOS) | pseudo_mos | plcmos | | 10 | Speaker Embedding Similarity | speaker | spk_similarity |\",\"You can find more detail and more metrics on VERSA README.\",\"(Optional) Pack results for upload\",\"Packing stage. It packs the trained model files as a preparation for uploading to Hugging Face.\",\"(Optional) Upload model to Hugging Face\",\"Upload the trained model to Hugging Face for sharing. Additional information at Docs.\"]},\"286\":{\"h\":\"How to run\",\"t\":[\"Here, we show the procedure to run the recipe using egs2/ljspeech/tts1.\",\"Move on the recipe directory.\",\"$ cd egs2/ljspeech/tts1\",\"Modify LJSPEECH variable in db.sh if you want to change the download directory.\",\"$ vim db.sh\",\"Modify cmd.sh and conf/*.conf if you want to use job scheduler. See the detail in using job scheduling system.\",\"$ vim cmd.sh\",\"Run run.sh, which conducts all of the stages explained above.\",\"$ ./run.sh\",\"As a default, we train Tacotron2 (conf/train.yaml) with feats_type=raw + token_type=phn.\",\"Then, you can get the following directories in the recipe directory.\",\"├── data/ # Kaldi-style data directory │ ├── dev/ # validation set │ ├── eval1/ # evaluation set │ └── tr_no_dev/ # training set ├── dump/ # feature dump directory │ ├── token_list/ # token list (dictionary) │ └── raw/ │ ├── org/ │ │ ├── tr_no_dev/ # training set before filtering │ │ └── dev/ # validation set before filtering │ ├── srctexts # text to create token list │ ├── eval1/ # evaluation set │ ├── dev/ # validation set after filtering │ └── tr_no_dev/ # training set after filtering └── exp/ # experiment directory ├── tts_stats_raw_phn_tacotron_g2p_en_no_space # statistics └── tts_train_raw_phn_tacotron_g2p_en_no_space # model ├── att_ws/ # attention plot during training ├── tensorboard/ # tensorboard log ├── images/ # plot of training curves ├── decode_train.loss.ave/ # decoded results │ ├── dev/ # validation set │ └── eval1/ # evaluation set │ ├── att_ws/ # attention plot in decoding │ ├── probs/ # stop probability plot in decoding │ ├── norm/ # generated features │ ├── denorm/ # generated denormalized features │ ├── wav/ # generated wav via Griffin-Lim │ ├── log/ # log directory │ ├── durations # duration of each input tokens │ ├── feats_type # feature type │ ├── focus_rates # focus rate │ └── speech_shape # shape info of generated features ├── config.yaml # config used for the training ├── train.log # training log ├── *epoch.pth # model parameter file ├── checkpoint.pth # model + optimizer + scheduler parameter file ├── latest.pth # symlink to latest model parameter ├── *.ave_5best.pth # model averaged parameters └── *.best.pth # symlink to the best model parameter loss\",\"In decoding, we use Griffin-Lim for waveform generation as a default (End-to-end text-to-wav model can generate waveform directly such as VITS and Joint training model). If you want to combine with neural vocoders, you can combine with kan-bayashi/ParallelWaveGAN.\",\"# Make sure you already install parallel_wavegan repo $ . ./path.sh && pip install -U parallel_wavegan # Use parallel_wavegan provided pretrained ljspeech style melgan as a vocoder $ ./run.sh --stage 8 --inference_args \\\"--vocoder_tag parallel_wavegan/ljspeech_style_melgan.v1\\\" --inference_tag decode_with_ljspeech_style_melgan.v1 # Use the vocoder trained by `parallel_wavegan` repo manually $ ./run.sh --stage 8 --vocoder_file /path/to/checkpoint-xxxxxxsteps.pkl --inference_tag decode_with_my_vocoder\",\"If you want to generate waveform from dumped features, please check decoding with ESPnet-TTS model's feature.\",\"For the first time, we recommend performing each stage step-by-step via --stage and --stop-stage options.\",\"$ ./run.sh --stage 1 --stop-stage 1 $ ./run.sh --stage 2 --stop-stage 2 ... $ ./run.sh --stage 8 --stop-stage 8\",\"This might helps you to understand each stage's processing and directory structure.\",\"FastSpeech training\",\"If you want to train FastSpeech, additional steps with the teacher model are needed. Please make sure you already finished the training of the teacher model (Tacotron2 or Transformer-TTS).\",\"First, decode all of data including training, validation, and evaluation set.\",\"# specify teacher model directory via --tts_exp option $ ./run.sh --stage 8 \\\\ --tts_exp exp/tts_train_raw_phn_tacotron_g2p_en_no_space \\\\ --test_sets \\\"tr_no_dev dev eval1\\\"\",\"This will generate durations for training, validation, and evaluation sets in exp/tts_train_raw_phn_tacotron_g2p_en_no_space/decode_train.loss.ave.\",\"Then, you can train FastSpeech by specifying the directory including durations via --teacher_dumpdir option.\",\"$ ./run.sh --stage 7 \\\\ --train_config conf/tuning/train_fastspeech.yaml \\\\ --teacher_dumpdir exp/tts_train_raw_phn_tacotron_g2p_en_no_space/decode_train.loss.ave\",\"In the above example, we use generated mel-spectrogram as the target, which is known as knowledge distillation training. If you want to use groundtruth mel-spectrogram as the target, we need to use teacher forcing in decoding.\",\"$ ./run.sh --stage 8 \\\\ --tts_exp exp/tts_train_raw_phn_tacotron_g2p_en_no_space \\\\ --inference_args \\\"--use_teacher_forcing true\\\" \\\\ --test_sets \\\"tr_no_dev dev eval1\\\"\",\"You can get the groundtruth aligned durations in exp/tts_train_raw_phn_tacotron_g2p_en_no_space/decode_use_teacher_forcingtrue_train.loss.ave.\",\"Then, you can train FastSpeech without knowledge distillation.\",\"$ ./run.sh --stage 7 \\\\ --train_config conf/tuning/train_fastspeech.yaml \\\\ --teacher_dumpdir exp/tts_train_raw_phn_tacotron_g2p_en_no_space/decode_use_teacher_forcingtrue_train.loss.ave\",\"FastSpeech2 training\",\"The procedure is almost the same as FastSpeech but we MUST use teacher forcing in decoding.\",\"$ ./run.sh --stage 8 \\\\ --tts_exp exp/tts_train_raw_phn_tacotron_g2p_en_no_space \\\\ --inference_args \\\"--use_teacher_forcing true\\\" \\\\ --test_sets \\\"tr_no_dev dev eval1\\\"\",\"To train FastSpeech2, we use additional feature (F0 and energy). Therefore, we need to start from stage 5 to calculate additional statistics.\",\"$ ./run.sh --stage 6 \\\\ --train_config conf/tuning/train_fastspeech2.yaml \\\\ --teacher_dumpdir exp/tts_train_raw_phn_tacotron_g2p_en_no_space/decode_use_teacher_forcingtrue_train.loss.ave \\\\ --tts_stats_dir exp/tts_train_raw_phn_tacotron_g2p_en_no_space/decode_use_teacher_forcingtrue_train.loss.ave/stats \\\\ --write_collected_feats true\",\"where --tts_stats_dir is the option to specify the directory to dump Statistics, and --write_collected_feats is the option to dump features in statistics calculation. The use of --write_collected_feats is optional but it helps to accelerate the training.\",\"Multi-speaker model with speaker embedding training\",\"First, you need to run from the stage 2 and 3 with --use_spk_embed true to extract speaker embedding.\",\"$ ./run.sh --stage 3 --stop-stage 4 --use_spk_embed true\",\"You can find the extracted speaker embedding in dump/${spk_embed_tag}/*/${spk_embed_tag}.{ark,scp}. Then, you can run the training with the config which has spk_embed_dim: 512 in tts_conf.\",\"# e.g. tts_conf: spk_embed_dim: 512 # dimension of speaker embedding spk_embed_integration_type: add # how to integrate speaker embedding\",\"(Optional) Train on speaker-averaged speaker embeddings\",\"Models trained using speaker-averaged speaker embeddings may generalise better to inference tasks where the utterance-specific speaker embedding is unknown, compared to models trained using embeddings derived from individual training utterances. After you perform the above extraction step, if you want to train and evaluate using speaker-averaged speaker embeddings, you can use the following command to replace utterance-level speaker embeddings with speaker-averaged values. Make sure to set your train_setdev_set and test_set variables beforehand:\",\"for dset in \\\"${train_set}\\\" \\\"${dev_set}\\\" \\\"${test_set}\\\" do ./pyscripts/utils/convert_to_avg_spk_embed.py \\\\ --utt-embed-path dump/${spk_embed_tag}/${dset}/${spk_embed_tag}.scp \\\\ --utt2spk data/${dset}/utt2spk \\\\ --spk-embed-path dump/${spk_embed_tag}/${dset}/spk_${spk_embed_tag}.scp done\",\"The original ${spk_embed_tag}.scp files are renamed to xvector.scp.bak in case you wish to revert the changes.\",\"Once you've performed extraction and optionally the speaker-averaged replacement step, please run the training from stage 6.\",\"$ ./run.sh --stage 7 --use_xvector true --train_config /path/to/your_xvector_config.yaml\",\"You can find the example config in egs2/vctk/tts1/conf/tuning.\",\"Multi-speaker model with speaker ID embedding training\",\"First, you need to run from the stage 2 and 3 with --use_sid true to extract speaker ID.\",\"$ ./run.sh --stage 3 --stop-stage 4 --use_sid true\",\"You can find the speaker ID file in dump/raw/*/utt2sid. Note that you need to correctly create utt2spk in data prep stage to generate utt2sid. Then, you can run the training with the config which has spks: #spks in tts_conf.\",\"# e.g. tts_conf: spks: 128 # Number of speakers\",\"Please run the training from stage 6.\",\"$ ./run.sh --stage 7 --use_sid true --train_config /path/to/your_multi_spk_config.yaml\",\"Multi-language model with language ID embedding training\",\"First, you need to run from the stage 2 and 3 with --use_lid true to extract speaker ID.\",\"$ ./run.sh --stage 3 --stop-stage 4 --use_lid true\",\"You can find the speaker ID file in dump/raw/*/utt2lid. Note that you need to additionally create utt2lang file in data prep stage to generate utt2lid. Then, you can run the training with the config which has langs: #langs in tts_conf.\",\"# e.g. tts_conf: langs: 4 # Number of languages\",\"Please run the training from stage 6.\",\"$ ./run.sh --stage 7 --use_lid true --train_config /path/to/your_multi_lang_config.yaml\",\"Of course you can further combine with x-vector or speaker ID embedding. If you want to use both sid and lid, the process should be like this:\",\"$ ./run.sh --stage 3 --stop-stage 4 --use_lid true --use_sid true\",\"Make your config.\",\"# e.g. tts_conf: langs: 4 # Number of languages spks: 128 # Number of speakers\",\"Please run the training from stage 6.\",\"$ ./run.sh --stage 7 --use_lid true --use_sid true --train_config /path/to/your_multi_spk_multi_lang_config.yaml\",\"VITS training\",\"First, the VITS config is hard coded for 22.05 khz or 44.1 khz and use different feature extraction method. (Note that you can use any feature extraction method but the default method is linear_spectrogram.) If you want to use it with 24 khz or 16 khz dataset, please be careful about these point.\",\"# Assume that data prep stage (stage 1) is finished $ ./run.sh --stage 1 --stop-stage 1 # Single speaker 22.05 khz case $ ./run.sh \\\\ --stage 2 \\\\ --ngpu 4 \\\\ --fs 22050 \\\\ --n_fft 1024 \\\\ --n_shift 256 \\\\ --win_length null \\\\ --dumpdir dump/22k \\\\ --expdir exp/22k \\\\ --tts_task gan_tts \\\\ --feats_extract linear_spectrogram \\\\ --feats_normalize none \\\\ --train_config ./conf/tuning/train_vits.yaml \\\\ --inference_config ./conf/tuning/decode_vits.yaml \\\\ --inference_model latest.pth # Single speaker 44.1 khz case $ ./run.sh \\\\ --stage 2 \\\\ --ngpu 4 \\\\ --fs 44100 \\\\ --n_fft 2048 \\\\ --n_shift 512 \\\\ --win_length null \\\\ --dumpdir dump/44k \\\\ --expdir exp/44k \\\\ --tts_task gan_tts \\\\ --feats_extract linear_spectrogram \\\\ --feats_normalize none \\\\ --train_config ./conf/tuning/train_full_band_vits.yaml \\\\ --inference_config ./conf/tuning/decode_vits.yaml \\\\ --inference_model latest.pth # Multi speaker with SID 22.05 khz case $ ./run.sh \\\\ --stage 2 \\\\ --use_sid true \\\\ --ngpu 4 \\\\ --fs 22050 \\\\ --n_fft 1024 \\\\ --n_shift 256 \\\\ --win_length null \\\\ --dumpdir dump/22k \\\\ --expdir exp/22k \\\\ --tts_task gan_tts \\\\ --feats_extract linear_spectrogram \\\\ --feats_normalize none \\\\ --train_config ./conf/tuning/train_multi_spk_vits.yaml \\\\ --inference_config ./conf/tuning/decode_vits.yaml \\\\ --inference_model latest.pth # Multi speaker with SID 44.1 khz case $ ./run.sh \\\\ --stage 2 \\\\ --use_sid true \\\\ --ngpu 4 \\\\ --fs 44100 \\\\ --n_fft 2048 \\\\ --n_shift 512 \\\\ --win_length null \\\\ --dumpdir dump/44k \\\\ --expdir exp/44k \\\\ --tts_task gan_tts \\\\ --feats_extract linear_spectrogram \\\\ --feats_normalize none \\\\ --train_config ./conf/tuning/train_full_band_multi_spk_vits.yaml \\\\ --inference_config ./conf/tuning/decode_vits.yaml \\\\ --inference_model latest.pth # Multi speaker with speaker embedding 22.05 khz case (need compiled kaldi to run if use Kaldi toolkit) $ ./run.sh \\\\ --stage 2 \\\\ --use_spk_embed true \\\\ --ngpu 4 \\\\ --fs 22050 \\\\ --n_fft 1024 \\\\ --n_shift 256 \\\\ --win_length null \\\\ --dumpdir dump/22k \\\\ --expdir exp/22k \\\\ --tts_task gan_tts \\\\ --feats_extract linear_spectrogram \\\\ --feats_normalize none \\\\ --train_config ./conf/tuning/train_xvector_vits.yaml \\\\ --inference_config ./conf/tuning/decode_vits.yaml \\\\ --inference_model latest.pth\",\"The training time requires long times (around several weeks) but around 100k samples can generate a reasonable sounds.\",\"You can find the example configs in:\",\"egs2/ljspeech/tts1/conf/tuning/train_vits.yaml: Single speaker 22.05 khz config.\",\"egs2/jsut/tts1/conf/tuning/train_full_band_vits.yaml: Single speaker 44.1 khz config.\",\"egs2/vctk/tts1/conf/tuning/train_multi_spk_vits.yaml: Multi speaker with SID 22.05 khz config.\",\"egs2/vctk/tts1/conf/tuning/train_full_band_multi_spk_vits.yaml: Multi speaker with SID 44.1 khz config.\",\"egs2/libritts/tts1/conf/tuning/train_xvector_vits.yaml: Multi speaker with X-vector 22.05 khz config.\",\"During VITS and JETS training, you can monitor pseudo MOS values predicted by a MOS prediction model. You can enable it by setting tts_conf.plot_pred_mos: true in training configs. Take a look at egs2/ljspeech/tts1/conf/tuning/train_vits.yaml to see how to set the flag.\",\"Joint text2wav training\",\"Joint training enables us to train both text2mel and vocoder model jointly with GAN-based training. Currently, we tested on only for non-autoregressive text2mel models with ljspeech dataset but the following models and vocoders are supported.\",\"Text2mel\",\"Tacotron2\",\"Transformer\",\"FastSpeech\",\"FastSpeech2\",\"Vocoder\",\"ParallelWaveGAN G / D\",\"(Multi-band) MelGAN G / D\",\"HiFiGAN G / D\",\"StyleMelGAN G / D\",\"Here, we show the example procedure to train conformer fastspeech2 + hifigan jointly with two training strategy (training from scratch and fine-tuning of pretrained text2mel and vocoder).\",\"# Make sure you are ready to train fastspeech2 (already prepared durations file with teacher model) $ ... # Case 1: Train conformer fastspeech2 + hifigan G + hifigan D from scratch $ ./run.sh \\\\ --stage 7 \\\\ --tts_task gan_tts \\\\ --train_config ./conf/tuning/train_joint_conformer_fastspeech2_hifigan.yaml # Case 2: Fine-tuning of pretrained conformer fastspeech2 + hifigan G + hifigan D # (a) Prepare pretrained models as follows $ tree -L 2 exp exp ... ├── ljspeech_hifigan.v1 # pretrained vocoder │ ├── checkpoint-2500000steps.pkl │ ├── config.yml │ └── stats.h5 ├── tts_train_conformer_fastspeech2_raw_phn_tacotron_g2p_en_no_space # pretrained text2mel │ ├── config.yaml │ ├── images │ └── train.loss.ave_5best.pth ... # If you want to use the same files of this example $ ipython # Download text2mel model [ins] In [1]: from espnet_model_zoo.downloader import ModelDownloader [ins] In [2]: d = ModelDownloader(\\\"./downloads\\\") [ins] In [3]: d.download_and_unpack(\\\"kan-bayashi/ljspeech_conformer_fastspeech2\\\") # Download vocoder [ins] In [4]: from parallel_wavegan.utils import download_pretrained_model [ins] In [5]: download_pretrained_model(\\\"ljspeech_hifigan.v1\\\", \\\"downloads\\\") # Move them to exp directory $ mv download/59c43ac0d40b121060bd71dd418f5ece/exp/tts_train_conformer_fastspeech2_raw_phn_tacotron_g2p_en_no_space exp $ mv downloads/ljspeech_hifigan.v1 exp # (b) Convert .pkl checkpoint to espnet loadable format $ ipython [ins] In [1]: import torch [ins] In [2]: d = torch.load(\\\"./exp/ljspeech_hifigan.v1/checkpoint-2500000steps.pkl\\\") [ins] In [3]: torch.save(d[\\\"model\\\"][\\\"generator\\\"], \\\"generator.pth\\\") [ins] In [4]: torch.save(d[\\\"model\\\"][\\\"discriminator\\\"], \\\"discriminator.pth\\\") # (c) Prepare configuration $ vim conf/tuning/finetune_joint_conformer_fastspeech2_hifigan.yaml # edit text2mel_params / generator_params / discriminator_params to be the same as the pretrained model # edit init_param part to specify the correct path of the pretrained model # (d) Run training $ ./run.sh \\\\ --stage 7 \\\\ --tts_task gan_tts \\\\ --train_config ./conf/tuning/finetune_joint_conformer_fastspeech2_hifigan.yaml\",\"You can find the example configs in:\",\"egs2/ljspeech/tts1/conf/tuning/train_joint_conformer_fastspeech2_hifigan.yaml: Joint training of conformer fastspeech2 + hifigan.\",\"egs2/ljspeech/tts1/conf/tuning/finetune_joint_conformer_fastspeech2_hifigan.yaml: Joint fine-tuning of conformer fastspeech2 + hifigan.\",\"Evaluation\",\"We provide five objective evaluation metrics:\",\"Mel-cepstral distortion (MCD)\",\"Log-F0 root mean square error (log-F0 RMSE)\",\"Character error rate (CER)\",\"Conditional Fréchet Speech Distance (CFSD)\",\"Speaker Embedding Cosine Similarity (SECS)\",\"Discrete speech metrics\",\"MCD and log-F0 RMSE reflect speaker, prosody, and phonetic content similarities, and CER can reflect the intelligibility. For MCD and log-F0 RMSE, we apply dynamic time-warping (DTW) to match the length difference between ground-truth speech and generated speech. Discrete speech metrics better correlate with human subjective judgements than MCD.\",\"Here we show the example command to calculate objective metrics:\",\"cd egs2/&lt;recipe_name&gt;/tts1 . ./path.sh # Evaluate MCD ./pyscripts/utils/evaluate_mcd.py \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ dump/raw/eval1/wav.scp # Evaluate log-F0 RMSE ./pyscripts/utils/evaluate_f0.py \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ dump/raw/eval1/wav.scp # If you want to calculate more precisely, limit the F0 range ./pyscripts/utils/evaluate_f0.py \\\\ --f0min xxx \\\\ --f0max yyy \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ dump/raw/eval1/wav.scp # Evaluate with automatic MOS prediction models. ./pyscripts/utils/evaluate_pseudomos.py \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ dump/raw/eval1/wav.scp # Evaluate CER ./scripts/utils/evaluate_asr.sh \\\\ --model_tag &lt;asr_model_tag&gt; \\\\ --nj 1 \\\\ --inference_args \\\"--beam_size 10 --ctc_weight 0.4 --lm_weight 0.0\\\" \\\\ --gt_text \\\"dump/raw/eval1/text\\\" \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/asr_results # You can also use openai whisper for evaluation ./scripts/utils/evaluate_asr.sh \\\\ --whisper_tag base \\\\ --nj 1 \\\\ --gt_text \\\"dump/raw/eval1/text\\\" \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/asr_results # Since ASR model does not use punctuation, it is better to remove punctuations if it contains ./scripts/utils/remove_punctuation.pl &lt; dump/raw/eval1/text &gt; dump/raw/eval1/text.no_punc ./scripts/utils/evaluate_asr.sh \\\\ --model_tag &lt;asr_model_tag&gt; \\\\ --nj 1 \\\\ --inference_args \\\"--beam_size 10 --ctc_weight 0.4 --lm_weight 0.0\\\" \\\\ --gt_text \\\"dump/raw/eval1/text.no_punc\\\" \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/asr_results # Some ASR models assume the existence of silence at the beginning and the end of audio # Then, you can perform silence padding with sox to get more reasonable ASR results awk &lt; \\\"exp/<model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp\\\" \\\\ '{print $1 \\\" sox \\\" $2 \\\" -t wav - pad 0.25 0.25 |\\\"}' \\\\ > exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav_pad.scp ./scripts/utils/evaluate_asr.sh \\\\ --model_tag &lt;asr_model_tag&gt; \\\\ --nj 1 \\\\ --inference_args \\\"--beam_size 10 --ctc_weight 0.4 --lm_weight 0.0\\\" \\\\ --gt_text \\\"dump/raw/eval1/text.no_punc\\\" \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav_pad.scp \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/asr_results # Evaluate CFSD ./pyscripts/utils/evaluate_cfsd.py \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ dump/raw/eval1/wav.scp # Evaluate SECS ./pyscripts/utils/evaluate_secs.py \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ dump/raw/eval1/wav.scp # Evaluate SpeechBERTScore ./pyscripts/utils/evaluate_speechbertscore.py \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ dump/raw/eval1/wav.scp # Evaluate SpeechBLEU ./pyscripts/utils/evaluate_speechbleu.py \\\\ exp/&lt;model_dir_name&gt;/&lt;decode_dir_name&gt;/eval1/wav/wav.scp \\\\ dump/raw/eval1/wav.scp\",\"While these objective metrics can estimate the quality of synthesized speech, it is still difficult to fully determine human perceptual quality from these values, especially with high-fidelity generated speech. Therefore, we recommend performing the subjective evaluation if you want to check perceptual quality in detail.\",\"You can refer this page to launch web-based subjective evaluation system with webMUSHRA.\"]},\"287\":{\"h\":\"Supported text frontend\",\"t\":[\"You can change via --g2p option in tts.sh.\",\"none: Just separate by space \",\"e.g.: HH AH0 L OW1 &lt;space&gt; W ER1 L D -> [HH, AH0, L, OW1, &lt;space&gt;, W, ER1, L D]\",\"g2p_en: espnet/g2p\",\"e.g. Hello World -> [HH, AH0, L, OW1, &lt;space&gt;, W, ER1, L D]\",\"g2p_en_no_space: espnet/g2p\",\"Same G2P but do not use word separator\",\"e.g. Hello World -> [HH, AH0, L, OW1, W, ER1, L, D]\",\"pyopenjtalk: r9y9/pyopenjtalk\",\"e.g. こ、こんにちは -> [k, o, pau, k, o, N, n, i, ch, i, w, a]\",\"pyopenjtalk_kana: r9y9/pyopenjtalk\",\"Use kana instead of phoneme\",\"e.g. こ、こんにちは -> [コ, 、, コ, ン, ニ, チ, ワ]\",\"pyopenjtalk_accent: r9y9/pyopenjtalk\",\"Add accent labels in addition to phoneme labels\",\"Based on Developing a Japanese End-to-End Speech Synthesis Server Considering Accent Phrases\",\"e.g. こ、こんにちは -> [k, 1, 0, o, 1, 0, k, 5, -4, o, 5, -4, N, 5, -3, n, 5, -2, i, 5, -2, ch, 5, -1, i, 5, -1, w, 5, 0, a, 5, 0]\",\"pyopenjtalk_accent_with_pause: r9y9/pyopenjtalk\",\"Add a pause label in addition to phoneme and accent labels\",\"Based on Developing a Japanese End-to-End Speech Synthesis Server Considering Accent Phrases\",\"e.g. こ、こんにちは -> [k, 1, 0, o, 1, 0, pau, k, 5, -4, o, 5, -4, N, 5, -3, n, 5, -2, i, 5, -2, ch, 5, -1, i, 5, -1, w, 5, 0, a, 5, 0]\",\"pyopenjtalk_prosody: r9y9/pyopenjtalk\",\"Use special symbols for prosody control\",\"Based on Prosodic features control by symbols as input of sequence-to-sequence acoustic modeling for neural TTS\",\"e.g. こ、こんにちは -> [^, k, #, o, _, k, o, [, N, n, i, ch, i, w, a, $]\",\"pypinyin: mozillanzg/python-pinyin\",\"e.g. 卡尔普陪外孙玩滑梯。 -> [ka3, er3, pu3, pei2, wai4, sun1, wan2, hua2, ti1, 。]\",\"pypinyin_phone: mozillanzg/python-pinyin\",\"Separate into first and last parts\",\"e.g. 卡尔普陪外孙玩滑梯。 -> [k, a3, er3, p, u3, p, ei2, wai4, s, un1, uan2, h, ua2, t, i1, 。]\",\"espeak_ng_arabic: espeak-ng/espeak-ng\",\"e.g. السلام عليكم -> [ʔ, a, s, s, ˈa, l, aː, m, ʕ, l, ˈiː, k, m]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"espeak_ng_german: espeak-ng/espeak-ng\",\"e.g. Das hört sich gut an. -> [d, a, s, h, ˈœ, ɾ, t, z, ɪ, ç, ɡ, ˈuː, t, ˈa, n, .]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"espeak_ng_french: espeak-ng/espeak-ng\",\"e.g. Bonjour le monde. -> [b, ɔ̃, ʒ, ˈu, ʁ, l, ə-, m, ˈɔ̃, d, .]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"espeak_ng_spanish: espeak-ng/espeak-ng\",\"e.g. Hola Mundo. -> [ˈo, l, a, m, ˈu, n, d, o, .]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"espeak_ng_russian: espeak-ng/espeak-ng\",\"e.g. Привет мир. -> [p, rʲ, i, vʲ, ˈe, t, mʲ, ˈi, r, .]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"espeak_ng_greek: espeak-ng/espeak-ng\",\"e.g. Γειά σου Κόσμε. -> [j, ˈa, s, u, k, ˈo, s, m, e, .]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"espeak_ng_finnish: espeak-ng/espeak-ng\",\"e.g. Hei maailma. -> [h, ˈei, m, ˈaː, ɪ, l, m, a, .]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"espeak_ng_hungarian: espeak-ng/espeak-ng\",\"e.g. Helló Világ. -> [h, ˈɛ, l, l, oː, v, ˈi, l, aː, ɡ, .]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"espeak_ng_dutch: espeak-ng/espeak-ng\",\"e.g. Hallo Wereld. -> [h, ˈɑ, l, oː, ʋ, ˈɪː, r, ə, l, t, .]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"espeak_ng_hindi: espeak-ng/espeak-ng\",\"e.g. नमस्ते दुनिया -> [n, ə, m, ˈʌ, s, t, eː, d, ˈʊ, n, ɪ, j, ˌaː]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"espeak_ng_italian: espeak-ng/espeak-ng\",\"e.g. Ciao mondo. -> [tʃ, ˈa, o, m, ˈo, n, d, o, .]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"espeak_ng_polish: espeak-ng/espeak-ng\",\"e.g. Witaj świecie. -> [v, ˈi, t, a, j, ɕ, fʲ, ˈɛ, tɕ, ɛ, .]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"espeak_ng_english_us_vits: espeak-ng/espeak-ng\",\"VITS official implementation-like processing (https://github.com/jaywalnut310/vits)\",\"e.g. Hello World. -> [h, ə, l, ˈ, o, ʊ, , &lt;space&gt;, w, ˈ, ɜ, ː, l, d, .]\",\"This result provided by the wrapper library bootphon/phonemizer\",\"g2pk: Kyubyong/g2pK\",\"e.g. 안녕하세요 세계입니다. -> [ᄋ, ᅡ, ᆫ, ᄂ, ᅧ, ᆼ, ᄒ, ᅡ, ᄉ, ᅦ, ᄋ, ᅭ, , ᄉ, ᅦ, ᄀ, ᅨ, ᄋ, ᅵ, ᆷ, ᄂ, ᅵ, ᄃ, ᅡ, .]\",\"g2pk_no_space: Kyubyong/g2pK\",\"Same G2P but do not use word separator\",\"e.g. 안녕하세요 세계입니다. -> [ᄋ, ᅡ, ᆫ, ᄂ, ᅧ, ᆼ, ᄒ, ᅡ, ᄉ, ᅦ, ᄋ, ᅭ, ᄉ, ᅦ, ᄀ, ᅨ, ᄋ, ᅵ, ᆷ, ᄂ, ᅵ, ᄃ, ᅡ, .]\",\"g2pk_explicit_space: Kyubyong/g2pK\",\"Same G2P but use explicit word separator\",\"e.g. 안녕하세요 세계입니다. -> [ᄋ, ᅡ, ᆫ, ᄂ, ᅧ, ᆼ, ᄒ, ᅡ, ᄉ, ᅦ, ᄋ, ᅭ, &lt;space&gt;, ᄉ, ᅦ, ᄀ, ᅨ, ᄋ, ᅵ, ᆷ, ᄂ, ᅵ, ᄃ, ᅡ, .]\",\"korean_jaso: jdongian/python-jamo\",\"e.g. 나는 학교에 갑니다. -> [ᄂ, ᅡ, ᄂ, ᅳ, ᆫ, &lt;space&gt;, ᄒ, ᅡ, ᆨ, ᄀ, ᅭ, ᄋ, ᅦ, &lt;space&gt;, ᄀ, ᅡ, ᆸ, ᄂ, ᅵ, ᄃ, ᅡ, .]\",\"korean_jaso_no_space: jdongian/python-jamo\",\"e.g. 나는 학교에 갑니다. -> [ᄂ, ᅡ, ᄂ, ᅳ, ᆫ, ᄒ, ᅡ, ᆨ, ᄀ, ᅭ, ᄋ, ᅦ, ᄀ, ᅡ, ᆸ, ᄂ, ᅵ, ᄃ, ᅡ, .]\",\"You can see the code example from here.\"]},\"288\":{\"h\":\"Supported text cleaner\",\"t\":[\"You can change via --cleaner option in tts.sh.\",\"none: No text cleaner.\",\"tacotron: keithito/tacotron\",\"e.g.\\\"(Hello-World); & jr. & dr.\\\" ->HELLO WORLD, AND JUNIOR AND DOCTOR\",\"jaconv: kazuhikoarase/jaconv\",\"e.g. ”あらゆる” 現実を 〜 ’すべて’ 自分の ほうへ ねじ曲げたのだ。\\\" -> \\\"あらゆる\\\" 現実を ー \\\\'すべて\\\\' 自分の ほうへ ねじ曲げたのだ。\",\"You can see the code example from here.\"]},\"289\":{\"h\":\"Supported Models\",\"t\":[\"You can train the following models by changing *.yaml config for --train_config option in tts.sh.\",\"Single speaker model\",\"Tacotron 2\",\"Transformer-TTS\",\"FastSpeech\",\"FastSpeech2 (FastPitch)\",\"Conformer-based FastSpeech / FastSpeech2\",\"VITS\",\"JETS\",\"You can find example configs of the above models in egs2/ljspeech/tts1/conf/tuning.\",\"Multi speaker model extension\",\"You can use / combine the following embedding to build multi-speaker model:\",\"X-Vector\",\"GST\",\"Speaker ID embedding (One-hot vector -> Continuous embedding)\",\"Language ID embedding (One-hot vector -> Continuous embedding)\",\"X-Vector is provided by kaldi and pretrained with VoxCeleb corpus. You can find example configs of the above models in:\",\"egs2/vctk/tts1/conf/tuning.\",\"egs2/libritts/tts1/conf/tuning.\",\"And now we support other toolkit's speaker embeddings: Please check the following options.\",\"https://github.com/espnet/espnet/blob/df053b8c13c26fe289fc882751801fd781e9d43e/egs2/TEMPLATE/tts1/tts.sh#L69-L71\"]},\"290\":{\"h\":\"FAQ\",\"t\":[\"ESPnet1 model is compatible with ESPnet2?\",\"No. We cannot use the ESPnet1 model in ESPnet2.\",\"How to change minibatch size in training?\",\"See change mini-batch type. As a default, we use batch_type=numel and batch_bins instead of batch_size to enable us to use dynamic batch size. See the following config as an example. https://github.com/espnet/espnet/blob/96b2fd08d4fd9276aabd7ad41ec5e02a88b30958/egs2/ljspeech/tts1/conf/tuning/train_tacotron2.yaml#L61-L62\",\"How to make a new recipe for my own dataset?\",\"See how to make/port new recipe.\",\"How to add a new g2p module?\",\"Add a new module in espnet2/text/phoneme_tokenizer.py and add it to g2p_choices in espnet2/text/phoneme_tokenizer.py.\",\"We have the wrapper module of bootphon/phonemizer. You can find the module espnet2/text/phoneme_tokenizer.py. If the g2p you wanted is implemented in bootphon/phonemizer, we can easily add it like this (Note that you need to update the choice as I mentioned the above).\",\"Example PRs may help you:\",\"#3382 Support Korean G2P\",\"#3463 Support G2P functions for various languages \",\"How to add a new cleaner module?\",\"Update espnet2/text/cleaner.py to add new module. Then, add new choice in the argument parser of espnet2/bin/tokenize_text.py and espnet2/tasks/tts.py.\",\"How to use trained model in python?\",\"from espnet2.bin.tts_inference import Text2Speech # without vocoder tts = Text2Speech.from_pretrained(model_file=\\\"/path/to/model.pth\\\") wav = tts(\\\"Hello, world\\\")[\\\"wav\\\"] # with local vocoder tts = Text2Speech.from_pretrained(model_file=\\\"/path/to/model.pth\\\", vocoder_file=\\\"/path/to/vocoder.pkl\\\") wav = tts(\\\"Hello, world\\\")[\\\"wav\\\"] # with pretrained vocoder (use ljseepch style melgan as an example) tts = Text2Speech.from_pretrained(model_file=\\\"/path/to/model.pth\\\", vocoder_tag=\\\"parallel_wavegan/ljspeech_style_melgan.v1\\\") wav = tts(\\\"Hello, world\\\")[\\\"wav\\\"]\",\"See use a pretrained model for inference.\",\"How to get pretrained models?\",\"Use ESPnet model zoo. You can find the all of the pretrained model list from here or search for pretrained models at Hugging Face.\",\"If you want to use pretrained models written in egs2/hogehoge/tts1/README.md, go to Zenodo URL and copy the URL of download in the below of the page. Then, you can use as follows:\",\"from espnet2.bin.tts_inference import Text2Speech # provide copied URL directly tts = Text2Speech.from_pretrained( \\\"https://zenodo.org/record/5414980/files/tts_train_vits_raw_phn_jaconv_pyopenjtalk_accent_with_pause_train.total_count.ave.zip?download=1\\\", ) wav = tts(\\\"こんにちは、世界。\\\")[\\\"wav\\\"]\",\"How to load the pretrained parameters?\",\"Please use --init_param option or add it in training config (*.yaml).\",\"# Usage --init_param &lt;file_path&gt;:&lt;src_key&gt;:&lt;dst_key&gt;:&lt;exclude_keys&gt; # Load all parameters python -m espnet2.bin.tts_train --init_param model.pth # Load only the parameters starting with \\\"decoder\\\" python -m espnet2.bin.tts_train --init_param model.pth:tts.dec # Load only the parameters starting with \\\"decoder\\\" and set it to model.tts.dec python -m espnet2.bin.tts_train --init_param model.pth:decoder:tts.dec # Set parameters to model.tts.dec python -m espnet2.bin.tts_train --init_param decoder.pth::tts.dec # Load all parameters excluding \\\"tts.enc.embed\\\" python -m espnet2.bin.tts_train --init_param model.pth:::tts.enc.embed # Load all parameters excluding \\\"tts.enc.embed\\\" and \\\"tts.dec\\\" python -m espnet2.bin.tts_train --init_param model.pth:::tts.enc.embed,tts.dec\",\"How to finetune the pretrained model?\",\"See jvs recipe example.\",\"How to add a new model?\",\"Under construction.\",\"How to test my model with an arbitrary given text?\",\"See Google Colab demo notebook: \",\"If you want to try in local:\",\"from espnet2.bin.tts_inference import Text2Speech # with local model tts = Text2Speech.from_pretrained(model_file=\\\"/path/to/model.pth\\\") wav = tts(\\\"Hello, world\\\")[\\\"wav\\\"] # with local model and local vocoder tts = Text2Speech.from_pretrained(model_file=\\\"/path/to/model.pth\\\", vocoder_file=\\\"/path/to/vocoder.pkl\\\") wav = tts(\\\"Hello, world\\\")[\\\"wav\\\"] # with local model and pretrained vocoder (use ljseepch as an example) tts = Text2Speech.from_pretrained(model_file=\\\"/path/to/model.pth\\\", vocoder_tag=\\\"parallel_wavegan/ljspeech_style_melgan.v1\\\") wav = tts(\\\"Hello, world\\\")[\\\"wav\\\"] # with pretrained model and pretrained vocoder (use ljseepch as an example) tts = Text2Speech.from_pretrained(model_tag=\\\"kan-bayashi/ljspeech_conformer_fastspeech2\\\", vocoder_tag=\\\"parallel_wavegan/ljspeech_style_melgan.v1\\\") wav = tts(\\\"Hello, world\\\")[\\\"wav\\\"]\",\"How to train vocoder?\",\"Please use kan-bayashi/ParallelWaveGAN, which provides the recipes to train various GAN-based vocoders. If the recipe is not prepared, you can quickly start the training with espnet2 TTS recipe. See Run training using ESPnet2-TTS recipe within 5 minutes.\",\"Or you can try joint training of text2mel & vocoder.\",\"The trained vocoder can be used as follows:\",\"With python\",\"from espnet2.bin.tts_inference import Text2Speech tts = Text2Speech.from_pretrained(model_file=\\\"/path/to/model.pth\\\", vocoder_file=\\\"/path/to/your_trained_vocoder_checkpoint.pkl\\\") wav = tts(\\\"Hello, world\\\")[\\\"wav\\\"]\",\"With TTS recipe\",\"$ ./run.sh --stage 8 --vocoder_file /path/to/your_trained_vocoder_checkpoint.pkl --inference_tag decode_with_my_vocoder\",\"With command line\",\"How to train vocoder with text2mel GTA outputs?\",\"Sometimes, we want to finetune the vocoder with text2mel groundtruth aligned (GTA) outputs. See Run finetuning using ESPnet2-TTS GTA outputs.\",\"How to handle the errors in validate_data_dir.sh?\",\"utils/validate_data_dir.sh: text contains N lines with non-printable characters which occurs at this line\",\"This is caused by the recent change in kaldi. We recommend modifying the following part in utils/validate_data_dir.sh to be non_print=true.\",\"https://github.com/kaldi-asr/kaldi/blob/40c71c5ee3ee5dffa1ad2c53b1b089e16d967bb5/egs/wsj/s5/utils/validate_data_dir.sh#L9\",\"utils/validate_text.pl: The line for utterance xxx contains disallowed Unicode whitespaces > utils/validate_text.pl: ERROR: text file 'data/xxx' contains disallowed UTF-8 whitespace character(s)\",\"The use of zenkaku whitespace in text is not allowed. Please changes it to hankaku whitespace or the other symbol.\",\"Why the model generate meaningless speech at the end?\",\"This is because the model failed to predict the stop token. There are several solutions to solve this issue:\",\"Use attention constraint in the inference (use_attention_constraint=True in inference config, only for Tacotron 2).\",\"Train the model with a large bce_pos_weight (e.g., bce_pos_weight=10.0).\",\"Use non-autoregressive models (FastSpeech or FastSpeech2)\",\"Why the model cannot be trained well with my own dataset?\",\"The most of the problems are caused by the bad cleaning of the dataset. Please check the following items carefully:\",\"Check the attention plot during the training. Loss value is not so meaningful in TTS. \",\"You can check this PR as an example.\",\"Remove the silence at the beginning and end of the speech. \",\"You can use silence trimming scripts in this example.\",\"Separate speech if it contains a long silence at the middle of speech.\",\"Use phonemes instead of characters if G2P is available.\",\"Clean the text as possible as you can (abbreviation, number, etc...)\",\"Add the pose symbol in text if the speech contains the silence.\",\"If the dataset is small, please consider the use of adaptation with pretrained model.\",\"If the dataset is small, please consider the use of large reduction factor, which helps the attention learning.\",\"Why the outputs contains metallic noise when combining neural vocoder?\",\"This will be happened especially when the neural vocoders did not use noise as the input (e.g., MelGAN, HiFiGAN), which are less robust to the mismatch of acoustic features. The metallic sound can reduce by performing vocoder finetuning with text2mel GTA outputs or joint training / finetuning of text2mel and vocoder.\",\"How is the duration for FastSpeech2 generated?\",\"We use the teacher model attention weight to calculate the duration as the same as FastSpeech. See more info in FastSpeech paper.\",\"Why the output of Tacotron2 or Transformer is non-deterministic?\",\"This is because we use prenet in the decoder, which always applies dropout. See more info in Tacotron2 paper.\",\"If you want to fix the results, you can use --always_fix_seed option.\"]},\"291\":{\"h\":\"Text-to-Speech with Discrete Units\",\"t\":[\"This is a template of tts2 recipe for ESPnet2.\"]},\"292\":{\"h\":\"Unsupervised Automatic Speech Recognition\",\"t\":[\"This is a template of uasr1 recipe for ESPnet2.\"]},\"293\":{\"h\":\"aggregate_stats_dirs.py\",\"t\":[\"source\",\"Aggregate statistics directories into one directory\",\"usage: aggregate_stats_dirs.py [-h] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] [--skip_sum_stats] [--input_dir INPUT_DIR] --output_dir OUTPUT_DIR\"]},\"294\":{\"h\":\"Named Arguments\"},\"295\":{\"h\":\"asr_align.py\",\"t\":[\"source\",\"ASR Decoding\",\"usage: asr_align.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] [--ngpu NGPU] [--dtype {float16,float32,float64}] --asr_train_config ASR_TRAIN_CONFIG --asr_model_file ASR_MODEL_FILE [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--fs FS] [--min_window_size MIN_WINDOW_SIZE] [--max_window_size MAX_WINDOW_SIZE] [--set_blank SET_BLANK] [--gratis_blank GRATIS_BLANK] [--replace_spaces_with_blanks REPLACE_SPACES_WITH_BLANKS] [--scoring_length SCORING_LENGTH] [--time_stamps {auto,fixed}] [--text_converter {tokenize,classic}] [--kaldi_style_text KALDI_STYLE_TEXT] [--print_utt_text PRINT_UTT_TEXT] [--print_utt_score PRINT_UTT_SCORE] -a AUDIO -t TEXT [-o OUTPUT]\"]},\"296\":{\"h\":\"Named Arguments\"},\"297\":{\"h\":\"Model configuration related\"},\"298\":{\"h\":\"Text converter related\"},\"299\":{\"h\":\"CTC segmentation related\"},\"300\":{\"h\":\"Input/output arguments\"},\"301\":{\"h\":\"asr_inference.py\",\"t\":[\"source\",\"ASR Decoding\",\"usage: asr_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--asr_train_config ASR_TRAIN_CONFIG] [--asr_model_file ASR_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--enh_s2t_task ENH_S2T_TASK] [--multi_asr MULTI_ASR] [--quantize_asr_model QUANTIZE_ASR_MODEL] [--quantize_lm QUANTIZE_LM] [--quantize_modules [QUANTIZE_MODULES ...]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--streaming STREAMING] [--hugging_face_decoder HUGGING_FACE_DECODER] [--hugging_face_decoder_conf HUGGING_FACE_DECODER_CONF] [--transducer_conf TRANSDUCER_CONF] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--time_sync TIME_SYNC] [--lang_prompt_token LANG_PROMPT_TOKEN] [--nlp_prompt_token NLP_PROMPT_TOKEN] [--prompt_token_file PROMPT_TOKEN_FILE] [--normalize_length NORMALIZE_LENGTH] [--partial_ar PARTIAL_AR] [--threshold_probability THRESHOLD_PROBABILITY] [--max_seq_len MAX_SEQ_LEN] [--max_mask_parallel MAX_MASK_PARALLEL]\"]},\"302\":{\"h\":\"Named Arguments\"},\"303\":{\"h\":\"Input data related\"},\"304\":{\"h\":\"The model configuration related\"},\"305\":{\"h\":\"Quantization related\"},\"306\":{\"h\":\"Beam-search related\"},\"307\":{\"h\":\"Text converter related\"},\"308\":{\"h\":\"Partially AR related\"},\"309\":{\"h\":\"asr_inference_maskctc.py\",\"t\":[\"source\",\"ASR Decoding\",\"usage: asr_inference_maskctc.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] --asr_train_config ASR_TRAIN_CONFIG --asr_model_file ASR_MODEL_FILE [--model_tag MODEL_TAG] [--batch_size BATCH_SIZE] [--maskctc_n_iterations MASKCTC_N_ITERATIONS] [--maskctc_threshold_probability MASKCTC_THRESHOLD_PROBABILITY] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL]\"]},\"310\":{\"h\":\"Named Arguments\"},\"311\":{\"h\":\"Input data related\"},\"312\":{\"h\":\"The model configuration related\"},\"313\":{\"h\":\"Decoding related\"},\"314\":{\"h\":\"Text converter related\"},\"315\":{\"h\":\"asr_inference_streaming.py\",\"t\":[\"source\",\"ASR Decoding\",\"usage: asr_inference_streaming.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--sim_chunk_length SIM_CHUNK_LENGTH] --asr_train_config ASR_TRAIN_CONFIG --asr_model_file ASR_MODEL_FILE [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--disable_repetition_detection DISABLE_REPETITION_DETECTION] [--encoded_feat_length_limit ENCODED_FEAT_LENGTH_LIMIT] [--decoder_text_length_limit DECODER_TEXT_LENGTH_LIMIT] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--normalize_length NORMALIZE_LENGTH]\"]},\"316\":{\"h\":\"Named Arguments\"},\"317\":{\"h\":\"Input data related\"},\"318\":{\"h\":\"The model configuration related\"},\"319\":{\"h\":\"Beam-search related\"},\"320\":{\"h\":\"Text converter related\"},\"321\":{\"h\":\"asr_transducer_inference.py\",\"t\":[\"source\",\"ASR Transducer Decoding\",\"usage: asr_transducer_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--asr_train_config ASR_TRAIN_CONFIG] [--asr_model_file ASR_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--model_tag MODEL_TAG] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--lm_weight LM_WEIGHT] [--beam_search_config BEAM_SEARCH_CONFIG] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--quantize_asr_model QUANTIZE_ASR_MODEL] [--quantize_modules [QUANTIZE_MODULES ...]] [--quantize_dtype {float16,qint8}] [--streaming STREAMING] [--decoding_window DECODING_WINDOW] [--left_context LEFT_CONTEXT] [--display_hypotheses DISPLAY_HYPOTHESES]\"]},\"322\":{\"h\":\"Named Arguments\"},\"323\":{\"h\":\"Input data related\"},\"324\":{\"h\":\"The model configuration related\"},\"325\":{\"h\":\"Beam-search related\"},\"326\":{\"h\":\"Text converter related\"},\"327\":{\"h\":\"asvspoof_inference.py\",\"t\":[\"source\",\"ASVSpoof Decoding\",\"usage: asvspoof_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] [--batch_size BATCH_SIZE] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--asvspoof_train_config ASVSPOOF_TRAIN_CONFIG] [--asvspoof_model_file ASVSPOOF_MODEL_FILE]\"]},\"328\":{\"h\":\"Named Arguments\"},\"329\":{\"h\":\"Input data related\"},\"330\":{\"h\":\"The model configuration related\"},\"331\":{\"h\":\"cls_inference.py\",\"t\":[\"source\",\"CLS Decoding\",\"usage: cls_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] [--batch_size BATCH_SIZE] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--classification_train_config CLASSIFICATION_TRAIN_CONFIG] [--classification_model_file CLASSIFICATION_MODEL_FILE] [--output_all_probabilities OUTPUT_ALL_PROBABILITIES]\"]},\"332\":{\"h\":\"Named Arguments\"},\"333\":{\"h\":\"Input data related\"},\"334\":{\"h\":\"The model configuration related\"},\"335\":{\"h\":\"diar_inference.py\",\"t\":[\"source\",\"Speaker Diarization inference\",\"usage: diar_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--fs FS] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--batch_size BATCH_SIZE] [--segment_size SEGMENT_SIZE] [--hop_size HOP_SIZE] [--show_progressbar SHOW_PROGRESSBAR] [--num_spk NUM_SPK] [--enh_s2t_task ENH_S2T_TASK] [--normalize_segment_scale NORMALIZE_SEGMENT_SCALE] [--normalize_output_wav NORMALIZE_OUTPUT_WAV] [--multiply_diar_result MULTIPLY_DIAR_RESULT]\"]},\"336\":{\"h\":\"Named Arguments\"},\"337\":{\"h\":\"Input data related\"},\"338\":{\"h\":\"The model configuration related\"},\"339\":{\"h\":\"Data loading related\"},\"340\":{\"h\":\"Diarize speech related\"},\"341\":{\"h\":\"Enh + Diar related\"},\"342\":{\"h\":\"enh_inference.py\",\"t\":[\"source\",\"Frontend inference\",\"usage: enh_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--fs FS] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--output_format OUTPUT_FORMAT] [--normalize_output_wav NORMALIZE_OUTPUT_WAV] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--inference_config INFERENCE_CONFIG] [--enh_s2t_task ENH_S2T_TASK] [--batch_size BATCH_SIZE] [--segment_size SEGMENT_SIZE] [--hop_size HOP_SIZE] [--normalize_segment_scale NORMALIZE_SEGMENT_SCALE] [--show_progressbar SHOW_PROGRESSBAR] [--ref_channel REF_CHANNEL]\"]},\"343\":{\"h\":\"Named Arguments\"},\"344\":{\"h\":\"Input data related\"},\"345\":{\"h\":\"Output data related\"},\"346\":{\"h\":\"The model configuration related\"},\"347\":{\"h\":\"Data loading related\"},\"348\":{\"h\":\"SeparateSpeech related\"},\"349\":{\"h\":\"enh_inference_streaming.py\",\"t\":[\"source\",\"Frontend inference\",\"usage: enh_inference_streaming.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--fs FS] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--output_format OUTPUT_FORMAT] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--inference_config INFERENCE_CONFIG] [--enh_s2t_task ENH_S2T_TASK] [--batch_size BATCH_SIZE] [--ref_channel REF_CHANNEL]\"]},\"350\":{\"h\":\"Named Arguments\"},\"351\":{\"h\":\"Input data related\"},\"352\":{\"h\":\"Output data related\"},\"353\":{\"h\":\"The model configuration related\"},\"354\":{\"h\":\"Data loading related\"},\"355\":{\"h\":\"SeparateSpeech related\"},\"356\":{\"h\":\"enh_scoring.py\",\"t\":[\"source\",\"Frontend inference\",\"usage: enh_scoring.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--dtype {float16,float32,float64}] --ref_scp REF_SCP --inf_scp INF_SCP [--key_file KEY_FILE] [--ref_channel REF_CHANNEL] [--flexible_numspk FLEXIBLE_NUMSPK] [--is_tse IS_TSE] [--use_dnsmos USE_DNSMOS] [--dnsmos_mode {local,web}] [--dnsmos_auth_key DNSMOS_AUTH_KEY] [--dnsmos_use_gpu DNSMOS_USE_GPU] [--dnsmos_convert_to_torch DNSMOS_CONVERT_TO_TORCH] [--dnsmos_primary_model DNSMOS_PRIMARY_MODEL] [--dnsmos_p808_model DNSMOS_P808_MODEL] [--use_pesq USE_PESQ]\"]},\"357\":{\"h\":\"Named Arguments\"},\"358\":{\"h\":\"Input data related\"},\"359\":{\"h\":\"DNSMOS related\"},\"360\":{\"h\":\"PESQ related\"},\"361\":{\"h\":\"enh_tse_inference.py\",\"t\":[\"source\",\"Frontend inference\",\"usage: enh_tse_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--fs FS] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--normalize_output_wav NORMALIZE_OUTPUT_WAV] [--output_format OUTPUT_FORMAT] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--inference_config INFERENCE_CONFIG] [--batch_size BATCH_SIZE] [--segment_size SEGMENT_SIZE] [--hop_size HOP_SIZE] [--normalize_segment_scale NORMALIZE_SEGMENT_SCALE] [--show_progressbar SHOW_PROGRESSBAR] [--ref_channel REF_CHANNEL]\"]},\"362\":{\"h\":\"Named Arguments\"},\"363\":{\"h\":\"Input data related\"},\"364\":{\"h\":\"Output data related\"},\"365\":{\"h\":\"The model configuration related\"},\"366\":{\"h\":\"Data loading related\"},\"367\":{\"h\":\"SeparateSpeech related\"},\"368\":{\"h\":\"gan_codec_inference.py\",\"t\":[\"source\",\"Codec inference\",\"usage: gan_codec_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--target_bandwidth TARGET_BANDWIDTH] [--encode_only ENCODE_ONLY] [--always_fix_seed ALWAYS_FIX_SEED]\"]},\"369\":{\"h\":\"Named Arguments\"},\"370\":{\"h\":\"Input data related\"},\"371\":{\"h\":\"The model configuration related\"},\"372\":{\"h\":\"hugging_face_export_vocabulary.py\",\"t\":[\"source\",\"Export Hugging Face vocabulary\",\"usage: hugging_face_export_vocabulary.py [-h] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output OUTPUT --model_name_or_path MODEL_NAME_OR_PATH [--add_symbol ADD_SYMBOL]\"]},\"373\":{\"h\":\"Named Arguments\"},\"374\":{\"h\":\"launch.py\",\"t\":[\"source\",\"Launch distributed process with appropriate options.\",\"usage: launch.py [-h] [--cmd CMD] [--log LOG] [--max_num_log_files MAX_NUM_LOG_FILES] [--ngpu NGPU] [--num_nodes NUM_NODES | --host HOST] [--envfile ENVFILE] [--multiprocessing_distributed MULTIPROCESSING_DISTRIBUTED] [--master_port MASTER_PORT] [--master_addr MASTER_ADDR] [--init_file_prefix INIT_FILE_PREFIX] args [args ...]\"]},\"375\":{\"h\":\"Positional Arguments\"},\"376\":{\"h\":\"Named Arguments\"},\"377\":{\"h\":\"lid_inference.py\",\"t\":[\"source\",\"speaker embedding extraction\",\"usage: lid_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--shape_file SHAPE_FILE] [--train_dtype {float16,float32,float64}] [--lid_train_config LID_TRAIN_CONFIG] [--lid_model_file LID_MODEL_FILE] [--extract_embd EXTRACT_EMBD] [--save_embd_per_utt SAVE_EMBD_PER_UTT] [--save_embd_avg_lang SAVE_EMBD_AVG_LANG] [--save_tsne_plot SAVE_TSNE_PLOT] [--dist_backend DIST_BACKEND] [--dist_init_method DIST_INIT_METHOD] [--dist_world_size DIST_WORLD_SIZE] [--dist_rank DIST_RANK] [--local_rank LOCAL_RANK] [--dist_master_addr DIST_MASTER_ADDR] [--dist_master_port DIST_MASTER_PORT] [--dist_launcher {slurm,mpi,None}] [--init_param INIT_PARAM] [--multiprocessing_distributed MULTIPROCESSING_DISTRIBUTED] [--unused_parameters UNUSED_PARAMETERS] [--sharded_ddp SHARDED_DDP] [--use_matplotlib USE_MATPLOTLIB] [--use_tensorboard USE_TENSORBOARD] [--create_graph_in_tensorboard CREATE_GRAPH_IN_TENSORBOARD] [--use_wandb USE_WANDB] [--wandb_project WANDB_PROJECT] [--wandb_id WANDB_ID] [--wandb_entity WANDB_ENTITY] [--wandb_name WANDB_NAME] [--wandb_model_log_interval WANDB_MODEL_LOG_INTERVAL] [--detect_anomaly DETECT_ANOMALY] [--use_lora USE_LORA] [--save_lora_only SAVE_LORA_ONLY] [--lora_conf LORA_CONF] [--cudnn_enabled CUDNN_ENABLED] [--cudnn_benchmark CUDNN_BENCHMARK] [--cudnn_deterministic CUDNN_DETERMINISTIC] [--valid_batch_size VALID_BATCH_SIZE] [--fix_duration FIX_DURATION] [--target_duration TARGET_DURATION] [--fold_length FOLD_LENGTH] [--use_preprocessor USE_PREPROCESSOR] [--checkpoint_interval CHECKPOINT_INTERVAL] [--resume RESUME] [--max_utt_per_lang_for_tsne MAX_UTT_PER_LANG_FOR_TSNE] [--perplexity PERPLEXITY] [--max_iter MAX_ITER]\"]},\"378\":{\"h\":\"Named Arguments\"},\"379\":{\"h\":\"Input data related\"},\"380\":{\"h\":\"The model configuration related\"},\"381\":{\"h\":\"distributed training related\"},\"382\":{\"h\":\"trainer initialization related\"},\"383\":{\"h\":\"cudnn mode related\"},\"384\":{\"h\":\"The inference hyperparameter related\"},\"385\":{\"h\":\"lm_calc_perplexity.py\",\"t\":[\"source\",\"Calc perplexity\",\"usage: lm_calc_perplexity.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] [--log_base LOG_BASE] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE]\"]},\"386\":{\"h\":\"Named Arguments\"},\"387\":{\"h\":\"Input data related\"},\"388\":{\"h\":\"The model configuration related\"},\"389\":{\"h\":\"lm_inference.py\",\"t\":[\"source\",\"LM Decoding (conditional generation)\",\"usage: lm_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--quantize_lm QUANTIZE_LM] [--quantize_modules [QUANTIZE_MODULES ...]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlen MAXLEN] [--minlen MINLEN] [--ngram_weight NGRAM_WEIGHT] [--token_type {char,word,bpe,None}] [--bpemodel BPEMODEL]\"]},\"390\":{\"h\":\"Named Arguments\"},\"391\":{\"h\":\"Input data related\"},\"392\":{\"h\":\"The model configuration related\"},\"393\":{\"h\":\"Quantization related\"},\"394\":{\"h\":\"Beam-search related\"},\"395\":{\"h\":\"Text converter related\"},\"396\":{\"h\":\"mt_inference.py\",\"t\":[\"source\",\"MT Decoding\",\"usage: mt_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--mt_train_config MT_TRAIN_CONFIG] [--mt_model_file MT_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--normalize_length NORMALIZE_LENGTH]\"]},\"397\":{\"h\":\"Named Arguments\"},\"398\":{\"h\":\"Input data related\"},\"399\":{\"h\":\"The model configuration related\"},\"400\":{\"h\":\"Beam-search related\"},\"401\":{\"h\":\"Text converter related\"},\"402\":{\"h\":\"pack.py\",\"t\":[\"source\",\"Pack input files to archive format\",\"usage: pack.py [-h] {asr,st,tts,enh,diar,svs,enh_s2t,ssl,s2st,s2t,spk,lid,codec,cls} ...\"]},\"403\":{\"h\":\"Sub-commands\",\"t\":[\"asr\",\"Undocumented\",\"pack.py asr [-h] --outpath OUTPATH [--asr_train_config ASR_TRAIN_CONFIG] [--lm_train_config LM_TRAIN_CONFIG] [--asr_model_file ASR_MODEL_FILE] [--lm_file LM_FILE] [--option OPTION]\",\"Named Arguments\",\"st\",\"Undocumented\",\"pack.py st [-h] --outpath OUTPATH [--st_train_config ST_TRAIN_CONFIG] [--st_model_file ST_MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"tts\",\"Undocumented\",\"pack.py tts [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"enh\",\"Undocumented\",\"pack.py enh [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"diar\",\"Undocumented\",\"pack.py diar [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"svs\",\"Undocumented\",\"pack.py svs [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"enh _s2t \",\"Undocumented\",\"pack.py enh_s2t [-h] --outpath OUTPATH [--enh_s2t_train_config ENH_S2T_TRAIN_CONFIG] [--lm_train_config LM_TRAIN_CONFIG] [--enh_s2t_model_file ENH_S2T_MODEL_FILE] [--lm_file LM_FILE] [--option OPTION]\",\"Named Arguments\",\"ssl\",\"Undocumented\",\"pack.py ssl [-h] --outpath OUTPATH [--ssl_train_config SSL_TRAIN_CONFIG] [--ssl_model_file SSL_MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"s2st\",\"Undocumented\",\"pack.py s2st [-h] --outpath OUTPATH [--s2st_train_config S2ST_TRAIN_CONFIG] [--s2st_model_file S2ST_MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"s2t\",\"Undocumented\",\"pack.py s2t [-h] --outpath OUTPATH [--s2t_train_config S2T_TRAIN_CONFIG] [--lm_train_config LM_TRAIN_CONFIG] [--s2t_model_file S2T_MODEL_FILE] [--lm_file LM_FILE] [--option OPTION]\",\"Named Arguments\",\"spk\",\"Undocumented\",\"pack.py spk [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"lid\",\"Undocumented\",\"pack.py lid [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"codec\",\"Undocumented\",\"pack.py codec [-h] --outpath OUTPATH [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--option OPTION]\",\"Named Arguments\",\"cls\",\"Undocumented\",\"pack.py cls [-h] --outpath OUTPATH [--classification_train_config CLASSIFICATION_TRAIN_CONFIG] [--classification_model_file CLASSIFICATION_MODEL_FILE] [--option OPTION]\",\"Named Arguments\"]},\"404\":{\"h\":\"ps2st_inference.py\",\"t\":[\"source\",\"Qwen2-Audio inference using ESPnet2 framework\",\"usage: ps2st_inference.py [-h] [--config CONFIG] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--decode_config_path DECODE_CONFIG_PATH] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}]\"]},\"405\":{\"h\":\"Named Arguments\"},\"406\":{\"h\":\"s2st_inference.py\",\"t\":[\"source\",\"S2ST inference\",\"usage: s2st_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--st_subtask_maxlenratio ST_SUBTASK_MAXLENRATIO] [--st_subtask_minlenratio ST_SUBTASK_MINLENRATIO] [--threshold THRESHOLD] [--use_att_constraint USE_ATT_CONSTRAINT] [--backward_window BACKWARD_WINDOW] [--forward_window FORWARD_WINDOW] [--use_teacher_forcing USE_TEACHER_FORCING] [--always_fix_seed ALWAYS_FIX_SEED] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--st_subtask_nbest ST_SUBTASK_NBEST] [--st_subtask_beam_size ST_SUBTASK_BEAM_SIZE] [--st_subtask_penalty ST_SUBTASK_PENALTY] [--vocoder_config VOCODER_CONFIG] [--vocoder_file VOCODER_FILE] [--vocoder_tag VOCODER_TAG] [--st_subtask_token_type {char,bpe,None}] [--st_subtask_bpemodel ST_SUBTASK_BPEMODEL] [--normalize_length NORMALIZE_LENGTH]\"]},\"407\":{\"h\":\"Named Arguments\"},\"408\":{\"h\":\"Input data related\"},\"409\":{\"h\":\"The model configuration related\"},\"410\":{\"h\":\"Decoding related\"},\"411\":{\"h\":\"Spectrogram-based generation related\"},\"412\":{\"h\":\"Beam-search (discrete unit/multi-pass) related\"},\"413\":{\"h\":\"Vocoder related\"},\"414\":{\"h\":\"Text converter related\"},\"415\":{\"h\":\"s2t_ctc_align.py\",\"t\":[\"source\",\"CTC alignment\",\"usage: s2t_ctc_align.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] [--ngpu NGPU] [--dtype {float16,float32,float64}] --s2t_train_config S2T_TRAIN_CONFIG --s2t_model_file S2T_MODEL_FILE [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--fs FS] [--min_window_size MIN_WINDOW_SIZE] [--max_window_size MAX_WINDOW_SIZE] [--set_blank SET_BLANK] [--gratis_blank GRATIS_BLANK] [--replace_spaces_with_blanks REPLACE_SPACES_WITH_BLANKS] [--scoring_length SCORING_LENGTH] [--time_stamps {auto,fixed}] [--text_converter {tokenize,classic}] [--kaldi_style_text KALDI_STYLE_TEXT] [--print_utt_text PRINT_UTT_TEXT] [--print_utt_score PRINT_UTT_SCORE] -a AUDIO -t TEXT [-o OUTPUT]\"]},\"416\":{\"h\":\"Named Arguments\"},\"417\":{\"h\":\"Model configuration related\"},\"418\":{\"h\":\"Text converter related\"},\"419\":{\"h\":\"CTC segmentation related\"},\"420\":{\"h\":\"Input/output arguments\"},\"421\":{\"h\":\"s2t_inference.py\",\"t\":[\"source\",\"S2T Decoding\",\"usage: s2t_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--s2t_train_config S2T_TRAIN_CONFIG] [--s2t_model_file S2T_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--lang_sym LANG_SYM] [--task_sym TASK_SYM] [--predict_time PREDICT_TIME] [--quantize_s2t_model QUANTIZE_S2T_MODEL] [--quantize_lm QUANTIZE_LM] [--quantize_modules [QUANTIZE_MODULES ...]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--normalize_length NORMALIZE_LENGTH] [--token_type {char,bpe,word,None}] [--bpemodel BPEMODEL] [--partial_ar PARTIAL_AR] [--threshold_probability THRESHOLD_PROBABILITY] [--max_seq_len MAX_SEQ_LEN] [--max_mask_parallel MAX_MASK_PARALLEL]\"]},\"422\":{\"h\":\"Named Arguments\"},\"423\":{\"h\":\"Input data related\"},\"424\":{\"h\":\"Model configuration related\"},\"425\":{\"h\":\"Quantization related\"},\"426\":{\"h\":\"Beam-search related\"},\"427\":{\"h\":\"Text converter related\"},\"428\":{\"h\":\"Partially AR related\"},\"429\":{\"h\":\"s2t_inference_ctc.py\",\"t\":[\"source\",\"S2T Decoding\",\"usage: s2t_inference_ctc.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--s2t_train_config S2T_TRAIN_CONFIG] [--s2t_model_file S2T_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--quantize_s2t_model QUANTIZE_S2T_MODEL] [--quantize_lm QUANTIZE_LM] [--quantize_modules [QUANTIZE_MODULES ...]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--lm_weight LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--streaming STREAMING] [--lang_sym LANG_SYM] [--task_sym TASK_SYM] [--generate_interctc_outputs GENERATE_INTERCTC_OUTPUTS] [--token_type {char,bpe,word,None}] [--bpemodel BPEMODEL]\"]},\"430\":{\"h\":\"Named Arguments\"},\"431\":{\"h\":\"Input data related\"},\"432\":{\"h\":\"The model configuration related\"},\"433\":{\"h\":\"Quantization related\"},\"434\":{\"h\":\"Beam-search related\"},\"435\":{\"h\":\"Text converter related\"},\"436\":{\"h\":\"s2t_inference_language.py\",\"t\":[\"source\",\"S2T Decoding\",\"usage: s2t_inference_language.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--s2t_train_config S2T_TRAIN_CONFIG] [--s2t_model_file S2T_MODEL_FILE] [--model_tag MODEL_TAG] [--first_lang_sym FIRST_LANG_SYM] [--last_lang_sym LAST_LANG_SYM] [--quantize_s2t_model QUANTIZE_S2T_MODEL] [--quantize_modules [QUANTIZE_MODULES ...]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST]\"]},\"437\":{\"h\":\"Named Arguments\"},\"438\":{\"h\":\"Input data related\"},\"439\":{\"h\":\"Model configuration related\"},\"440\":{\"h\":\"Quantization related\"},\"441\":{\"h\":\"Beam-search related\"},\"442\":{\"h\":\"slu_inference.py\",\"t\":[\"source\",\"ASR Decoding\",\"usage: slu_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--slu_train_config SLU_TRAIN_CONFIG] [--slu_model_file SLU_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--quantize_asr_model QUANTIZE_ASR_MODEL] [--quantize_lm QUANTIZE_LM] [--quantize_modules [QUANTIZE_MODULES ...]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--streaming STREAMING] [--transducer_conf TRANSDUCER_CONF] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--normalize_length NORMALIZE_LENGTH] [--run_chunk RUN_CHUNK]\"]},\"443\":{\"h\":\"Named Arguments\"},\"444\":{\"h\":\"Input data related\"},\"445\":{\"h\":\"The model configuration related\"},\"446\":{\"h\":\"Quantization related\"},\"447\":{\"h\":\"Beam-search related\"},\"448\":{\"h\":\"Text converter related\"},\"449\":{\"h\":\"spk_embed_extract.py\",\"t\":[\"source\",\"speaker embedding extraction\",\"usage: spk_embed_extract.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--batch_type {unsorted,sorted,folded,length,numel}] [--batch_bins BATCH_BINS] [--valid_batch_bins VALID_BATCH_BINS] [--valid_batch_type {unsorted,sorted,folded,length,numel,None}] [--max_cache_size MAX_CACHE_SIZE] [--max_cache_fd MAX_CACHE_FD] [--allow_multi_rates ALLOW_MULTI_RATES] [--valid_max_cache_size VALID_MAX_CACHE_SIZE] [--shape_file SHAPE_FILE] [--input_size INPUT_SIZE] [--num_cohort_spk NUM_COHORT_SPK] [--num_utt_per_spk NUM_UTT_PER_SPK] [--utt_select_sec UTT_SELECT_SEC] [--average_spk AVERAGE_SPK] [--adaptive_cohort_size ADAPTIVE_COHORT_SIZE] [--qmf_dur_thresh QMF_DUR_THRESH] [--qmf_num_trial_per_condition QMF_NUM_TRIAL_PER_CONDITION] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--average_embd AVERAGE_EMBD] [--train_dtype {float16,float32,float64}] [--use_amp USE_AMP] [--no_forward_run NO_FORWARD_RUN] [--sort_in_batch {descending,ascending}] [--sort_batch {descending,ascending}] [--drop_last_iter DROP_LAST_ITER] [--spk_train_config SPK_TRAIN_CONFIG] [--spk_model_file SPK_MODEL_FILE] [--model_tag MODEL_TAG] [--dist_backend DIST_BACKEND] [--dist_init_method DIST_INIT_METHOD] [--dist_world_size DIST_WORLD_SIZE] [--dist_rank DIST_RANK] [--local_rank LOCAL_RANK] [--dist_master_addr DIST_MASTER_ADDR] [--dist_master_port DIST_MASTER_PORT] [--dist_launcher {slurm,mpi,None}] [--multiprocessing_distributed MULTIPROCESSING_DISTRIBUTED] [--unused_parameters UNUSED_PARAMETERS] [--sharded_ddp SHARDED_DDP] [--use_matplotlib USE_MATPLOTLIB] [--use_tensorboard USE_TENSORBOARD] [--create_graph_in_tensorboard CREATE_GRAPH_IN_TENSORBOARD] [--use_wandb USE_WANDB] [--wandb_project WANDB_PROJECT] [--wandb_id WANDB_ID] [--wandb_entity WANDB_ENTITY] [--wandb_name WANDB_NAME] [--wandb_model_log_interval WANDB_MODEL_LOG_INTERVAL] [--detect_anomaly DETECT_ANOMALY] [--use_lora USE_LORA] [--save_lora_only SAVE_LORA_ONLY] [--lora_conf LORA_CONF] [--cudnn_enabled CUDNN_ENABLED] [--cudnn_benchmark CUDNN_BENCHMARK] [--cudnn_deterministic CUDNN_DETERMINISTIC] [--valid_batch_size VALID_BATCH_SIZE] [--target_duration TARGET_DURATION] [--num_eval NUM_EVAL] [--fold_length FOLD_LENGTH] [--use_preprocessor USE_PREPROCESSOR]\"]},\"450\":{\"h\":\"Named Arguments\"},\"451\":{\"h\":\"Input data related\"},\"452\":{\"h\":\"The model configuration related\"},\"453\":{\"h\":\"distributed training related\"},\"454\":{\"h\":\"trainer initialization related\"},\"455\":{\"h\":\"cudnn mode related\"},\"456\":{\"h\":\"The inference hyperparameter related\"},\"457\":{\"h\":\"spk_inference.py\",\"t\":[\"source\",\"Speaker Embedding Extraction\",\"usage: spk_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--batch_size BATCH_SIZE] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG]\"]},\"458\":{\"h\":\"Named Arguments\"},\"459\":{\"h\":\"Input data related\"},\"460\":{\"h\":\"The model configuration related\"},\"461\":{\"h\":\"split_scps.py\",\"t\":[\"source\",\"Split scp files\",\"usage: split_scps.py [-h] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --scps SCPS [SCPS ...] [--names NAMES [NAMES ...]] [--num_splits NUM_SPLITS] --output_dir OUTPUT_DIR\"]},\"462\":{\"h\":\"Named Arguments\"},\"463\":{\"h\":\"st_inference.py\",\"t\":[\"source\",\"ST Decoding\",\"usage: st_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--st_train_config ST_TRAIN_CONFIG] [--st_model_file ST_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--src_lm_train_config SRC_LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--src_lm_file SRC_LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--src_word_lm_train_config SRC_WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--src_word_lm_file SRC_WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--src_ngram_file SRC_NGRAM_FILE] [--model_tag MODEL_TAG] [--enh_s2t_task ENH_S2T_TASK] [--batch_size BATCH_SIZE] [--nbest NBEST] [--asr_nbest ASR_NBEST] [--beam_size BEAM_SIZE] [--asr_beam_size ASR_BEAM_SIZE] [--penalty PENALTY] [--asr_penalty ASR_PENALTY] [--maxlenratio MAXLENRATIO] [--asr_maxlenratio ASR_MAXLENRATIO] [--minlenratio MINLENRATIO] [--asr_minlenratio ASR_MINLENRATIO] [--lm_weight LM_WEIGHT] [--asr_lm_weight ASR_LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--asr_ngram_weight ASR_NGRAM_WEIGHT] [--ctc_weight CTC_WEIGHT] [--asr_ctc_weight ASR_CTC_WEIGHT] [--transducer_conf TRANSDUCER_CONF] [--token_type {char,bpe,None}] [--src_token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--src_bpemodel SRC_BPEMODEL] [--ctc_greedy CTC_GREEDY] [--hugging_face_decoder HUGGING_FACE_DECODER] [--hugging_face_decoder_max_length HUGGING_FACE_DECODER_MAX_LENGTH] [--normalize_length NORMALIZE_LENGTH]\"]},\"464\":{\"h\":\"Named Arguments\"},\"465\":{\"h\":\"Input data related\"},\"466\":{\"h\":\"The model configuration related\"},\"467\":{\"h\":\"Beam-search related\"},\"468\":{\"h\":\"Text converter related\"},\"469\":{\"h\":\"st_inference_streaming.py\",\"t\":[\"source\",\"ST Decoding\",\"usage: st_inference_streaming.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--sim_chunk_length SIM_CHUNK_LENGTH] --st_train_config ST_TRAIN_CONFIG --st_model_file ST_MODEL_FILE [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--disable_repetition_detection DISABLE_REPETITION_DETECTION] [--encoded_feat_length_limit ENCODED_FEAT_LENGTH_LIMIT] [--decoder_text_length_limit DECODER_TEXT_LENGTH_LIMIT] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL] [--time_sync TIME_SYNC] [--incremental_decode INCREMENTAL_DECODE] [--blank_penalty BLANK_PENALTY] [--hold_n HOLD_N] [--transducer_conf TRANSDUCER_CONF] [--hugging_face_decoder HUGGING_FACE_DECODER] [--normalize_length NORMALIZE_LENGTH]\"]},\"470\":{\"h\":\"Named Arguments\"},\"471\":{\"h\":\"Input data related\"},\"472\":{\"h\":\"The model configuration related\"},\"473\":{\"h\":\"Beam-search related\"},\"474\":{\"h\":\"Text converter related\"},\"475\":{\"h\":\"svs_inference.py\",\"t\":[\"source\",\"SVS Decode\",\"usage: svs_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--use_teacher_forcing USE_TEACHER_FORCING] [--noise_scale NOISE_SCALE] [--noise_scale_dur NOISE_SCALE_DUR] [--vocoder_checkpoint VOCODER_CHECKPOINT] [--vocoder_config VOCODER_CONFIG] [--discrete_token_layers DISCRETE_TOKEN_LAYERS] [--mix_type MIX_TYPE] [--svs_task SVS_TASK] [--use_singomd USE_SINGOMD]\"]},\"476\":{\"h\":\"Named Arguments\"},\"477\":{\"h\":\"Input data related\"},\"478\":{\"h\":\"The model configuration related\"},\"479\":{\"h\":\"Decoding related\"},\"480\":{\"h\":\"Vocoder related\"},\"481\":{\"h\":\"tokenize_text.py\",\"t\":[\"source\",\"Tokenize texts\",\"usage: tokenize_text.py [-h] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --input INPUT --output OUTPUT [--field FIELD] [--token_type {char,bpe,word,phn}] [--delimiter DELIMITER] [--space_symbol SPACE_SYMBOL] [--bpemodel BPEMODEL] [--non_linguistic_symbols NON_LINGUISTIC_SYMBOLS] [--remove_non_linguistic_symbols REMOVE_NON_LINGUISTIC_SYMBOLS] [--cleaner {None,tacotron,jaconv,vietnamese,korean_cleaner,whisper_en,whisper_basic}] [--g2p {None,g2p_en,g2p_en_no_space,pyopenjtalk,pyopenjtalk_kana,pyopenjtalk_accent,pyopenjtalk_accent_with_pause,pyopenjtalk_prosody,pypinyin_g2p,pypinyin_g2p_phone,pypinyin_g2p_phone_without_prosody,espeak_ng_arabic,espeak_ng_german,espeak_ng_french,espeak_ng_spanish,espeak_ng_russian,espeak_ng_greek,espeak_ng_finnish,espeak_ng_hungarian,espeak_ng_dutch,espeak_ng_english_us_vits,espeak_ng_hindi,espeak_ng_italian,espeak_ng_ukrainian,espeak_ng_polish,g2pk,g2pk_no_space,g2pk_explicit_space,korean_jaso,korean_jaso_no_space,g2p_is}] [--write_vocabulary WRITE_VOCABULARY] [--vocabulary_size VOCABULARY_SIZE] [--cutoff CUTOFF] [--add_symbol ADD_SYMBOL] [--add_nonsplit_symbol ADD_NONSPLIT_SYMBOL]\"]},\"482\":{\"h\":\"Named Arguments\"},\"483\":{\"h\":\"write_vocabulary mode related\"},\"484\":{\"h\":\"tts2_inference.py\",\"t\":[\"source\",\"TTS inference\",\"usage: tts2_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--threshold THRESHOLD] [--use_att_constraint USE_ATT_CONSTRAINT] [--backward_window BACKWARD_WINDOW] [--forward_window FORWARD_WINDOW] [--use_teacher_forcing USE_TEACHER_FORCING] [--speed_control_alpha SPEED_CONTROL_ALPHA] [--noise_scale NOISE_SCALE] [--noise_scale_dur NOISE_SCALE_DUR] [--always_fix_seed ALWAYS_FIX_SEED] [--vocoder_config VOCODER_CONFIG] [--vocoder_file VOCODER_FILE] [--vocoder_tag VOCODER_TAG]\"]},\"485\":{\"h\":\"Named Arguments\"},\"486\":{\"h\":\"Input data related\"},\"487\":{\"h\":\"The model configuration related\"},\"488\":{\"h\":\"Decoding related\"},\"489\":{\"h\":\"Vocoder related\"},\"490\":{\"h\":\"tts_inference.py\",\"t\":[\"source\",\"TTS inference\",\"usage: tts_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--train_config TRAIN_CONFIG] [--model_file MODEL_FILE] [--model_tag MODEL_TAG] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--threshold THRESHOLD] [--use_att_constraint USE_ATT_CONSTRAINT] [--backward_window BACKWARD_WINDOW] [--forward_window FORWARD_WINDOW] [--use_teacher_forcing USE_TEACHER_FORCING] [--speed_control_alpha SPEED_CONTROL_ALPHA] [--noise_scale NOISE_SCALE] [--noise_scale_dur NOISE_SCALE_DUR] [--always_fix_seed ALWAYS_FIX_SEED] [--vocoder_config VOCODER_CONFIG] [--vocoder_file VOCODER_FILE] [--vocoder_tag VOCODER_TAG]\"]},\"491\":{\"h\":\"Named Arguments\"},\"492\":{\"h\":\"Input data related\"},\"493\":{\"h\":\"The model configuration related\"},\"494\":{\"h\":\"Decoding related\"},\"495\":{\"h\":\"Vocoder related\"},\"496\":{\"h\":\"uasr_extract_feature.py\",\"t\":[\"source\",\"UASR Decoding\",\"usage: uasr_extract_feature.py [-h] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--uasr_train_config UASR_TRAIN_CONFIG] [--uasr_model_file UASR_MODEL_FILE] [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--ngpu NGPU] [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE] [--dtype {float16,float32,float64}] [--dset DSET] [--output_dir OUTPUT_DIR] [--log_level {ERROR,WARNING,INFO,DEBUG,NOTSET}]\"]},\"497\":{\"h\":\"Named Arguments\"},\"498\":{\"h\":\"uasr_inference.py\",\"t\":[\"source\",\"UASR Decoding\",\"usage: uasr_inference.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--uasr_train_config UASR_TRAIN_CONFIG] [--uasr_model_file UASR_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--ngram_file NGRAM_FILE] [--model_tag MODEL_TAG] [--quantize_uasr_model QUANTIZE_UASR_MODEL] [--quantize_lm QUANTIZE_LM] [--quantize_modules [QUANTIZE_MODULES ...]] [--quantize_dtype {float16,qint8}] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--lm_weight LM_WEIGHT] [--ngram_weight NGRAM_WEIGHT] [--token_type {char,bpe,None}] [--bpemodel BPEMODEL]\"]},\"499\":{\"h\":\"Named Arguments\"},\"500\":{\"h\":\"Input data related\"},\"501\":{\"h\":\"The model configuration related\"},\"502\":{\"h\":\"Quantization related\"},\"503\":{\"h\":\"Beam-search related\"},\"504\":{\"h\":\"Text converter related\"},\"505\":{\"h\":\"uasr_inference_k2.py\",\"t\":[\"source\",\"UASR Decoding\",\"usage: uasr_inference_k2.py [-h] [--config CONFIG] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output_dir OUTPUT_DIR [--ngpu NGPU] [--seed SEED] [--dtype {float16,float32,float64}] [--num_workers NUM_WORKERS] --data_path_and_name_and_type DATA_PATH_AND_NAME_AND_TYPE [--key_file KEY_FILE] [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS] [--uasr_train_config UASR_TRAIN_CONFIG] [--uasr_model_file UASR_MODEL_FILE] [--lm_train_config LM_TRAIN_CONFIG] [--lm_file LM_FILE] [--word_lm_train_config WORD_LM_TRAIN_CONFIG] [--word_lm_file WORD_LM_FILE] [--model_tag MODEL_TAG] [--batch_size BATCH_SIZE] [--nbest NBEST] [--beam_size BEAM_SIZE] [--penalty PENALTY] [--maxlenratio MAXLENRATIO] [--minlenratio MINLENRATIO] [--ctc_weight CTC_WEIGHT] [--lm_weight LM_WEIGHT] [--streaming STREAMING] [--token_type {phn,word}] [--bpemodel BPEMODEL] [--is_ctc_decoding IS_CTC_DECODING] [--use_nbest_rescoring USE_NBEST_RESCORING] [--num_paths NUM_PATHS] [--nbest_batch_size NBEST_BATCH_SIZE] [--nll_batch_size NLL_BATCH_SIZE] [--decoding_graph DECODING_GRAPH] [--word_token_list WORD_TOKEN_LIST] [--k2_config K2_CONFIG]\"]},\"506\":{\"h\":\"Named Arguments\"},\"507\":{\"h\":\"Input data related\"},\"508\":{\"h\":\"The model configuration related\"},\"509\":{\"h\":\"Beam-search related\"},\"510\":{\"h\":\"Text converter related\"},\"511\":{\"h\":\"whisper_export_vocabulary.py\",\"t\":[\"source\",\"Export Whisper vocabulary\",\"usage: whisper_export_vocabulary.py [-h] [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}] --output OUTPUT --whisper_model WHISPER_MODEL [--add_token_file_name ADD_TOKEN_FILE_NAME] [--whisper_language WHISPER_LANGUAGE] [--whisper_task WHISPER_TASK] [--sot_asr SOT_ASR] [--speaker_change_symbol SPEAKER_CHANGE_SYMBOL]\"]},\"512\":{\"h\":\"Named Arguments\"},\"513\":{\"h\":\"spm_decode\",\"t\":[\"source\",\"usage: spm_decode [-h] --model MODEL [--input INPUT] [--input_format {piece,id}] options: --model MODEL sentencepiece model to use for decoding --input INPUT input file to decode --input_format {piece,id}\"]},\"514\":{\"h\":\"spm_encode\",\"t\":[\"source\",\"usage: spm_encode [-h] --model MODEL [--inputs INPUTS [INPUTS ...]] [--outputs OUTPUTS [OUTPUTS ...]] [--output_format {piece,id}] [--min-len N] [--max-len N] options: --model MODEL sentencepiece model to use for encoding --inputs INPUTS [INPUTS ...] input files to filter/encode --outputs OUTPUTS [OUTPUTS ...] path to save encoded outputs --output_format {piece,id} --min-len N filter sentence pairs with fewer than N tokens --max-len N filter sentence pairs with more than N tokens\"]},\"515\":{\"h\":\"clean_corpus.sh\",\"t\":[\"source\",\"Usage: clean_corpus.sh [options] &lt;data-dir&gt; &lt;langs&gt; e.g.: clean_corpus.sh data/train \\\"en de\\\" Options: --maxframes # number of maximum input frame length --maxchars # number of maximum character length --utt_extra_files # extra text files for target sequence --no_feat # set to True for MT recipe\"]},\"516\":{\"h\":\"convert_fbank.sh\",\"t\":[\"source\",\"Usage: convert_fbank.sh [options] &lt;data-dir&gt; [&lt;log-dir&gt; [&lt;fbank-dir&gt;] ] e.g.: convert_fbank.sh data/train exp/griffin_lim/train wav Note: &lt;log-dir&gt; defaults to &lt;data-dir&gt;/log, and &lt;fbank-dir&gt; defaults to &lt;data-dir&gt;/data Options: --nj &lt;nj&gt; # number of parallel jobs --fs &lt;fs&gt; # sampling rate --fmax &lt;fmax&gt; # maximum frequency --fmin &lt;fmin&gt; # minimum frequency --n_fft &lt;n_fft&gt; # number of FFT points (default=1024) --n_shift &lt;n_shift&gt; # shift size in point (default=256) --win_length &lt;win_length&gt; # window length in point (default=) --n_mels &lt;n_mels&gt; # number of mel basis (default=80) --iters &lt;iters&gt; # number of Griffin-lim iterations (default=64) --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs.\"]},\"517\":{\"h\":\"divide_lang.sh\",\"t\":[\"source\",\"Usage: divide_lang.sh &lt;set&gt; &lt;langs divided by space&gt; e.g.: divide_lang.sh dev\"]},\"518\":{\"h\":\"download_from_google_drive.sh\",\"t\":[\"source\",\"Usage: download_from_google_drive.sh &lt;share-url&gt; [&lt;download_dir&gt; &lt;file_ext&gt;] e.g.: download_from_google_drive.sh https://drive.google.com/open?id=1zF88bRNbJhw9hNBq3NrDg8vnGGibREmg downloads zip Options: &lt;download_dir&gt;: directory to save downloaded file. (Default=downloads) &lt;file_ext&gt;: file extension of the file to be downloaded. (Default=zip)\"]},\"519\":{\"h\":\"dump.sh\",\"t\":[\"source\",\"Usage: dump.sh &lt;scp&gt; &lt;cmvnark&gt; &lt;logdir&gt; &lt;dumpdir&gt;\"]},\"520\":{\"h\":\"eval_source_separation.sh\",\"t\":[\"source\",\"Usage: eval_source_separation.sh reffiles enffiles &lt;dir&gt; e.g. eval_source_separation.sh reference.scp enhanced.scp outdir And also supporting multiple sources: e.g. eval_source_separation.sh \\\"ref1.scp,ref2.scp\\\" \\\"enh1.scp,enh2.scp\\\" outdir Options: --nj &lt;nj&gt; # number of parallel jobs --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs.\"]},\"521\":{\"h\":\"feat_to_shape.sh\",\"t\":[\"source\",\"Usage: feat_to_shape.sh [options] &lt;input-scp&gt; &lt;output-scp&gt; [&lt;log-dir&gt;] e.g.: feat_to_shape.sh data/train/feats.scp data/train/shape.scp data/train/log Options: --nj &lt;nj&gt; # number of parallel jobs --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs. --filetype &lt;mat|hdf5|sound.hdf5&gt; # Specify the format of feats file --preprocess-conf &lt;json&gt; # Apply preprocess to feats when creating shape.scp --verbose &lt;num&gt; # Default: 0\"]},\"522\":{\"h\":\"free-gpu.sh\",\"t\":[\"source\",\"Usage: free-gpu.sh -n &lt;Number of free GPUs on a machine&gt; e.g.: free-gpu.sh -n 2 1, 2\"]},\"523\":{\"h\":\"generate_wav.sh\",\"t\":[\"source\",\"Usage: generate_wav.sh [options] &lt;model-path&gt; &lt;data-dir&gt; [&lt;log-dir&gt; [&lt;fbank-dir&gt;] ] Example: generate_wav.sh ljspeech.wavenet.ns.v1/checkpoint-1000000.pkl data/train exp/wavenet_vocoder/train wav Note: &lt;log-dir&gt; defaults to &lt;data-dir&gt;/log, and &lt;fbank-dir&gt; defaults to &lt;data-dir&gt;/data Options: --nj &lt;nj&gt; # number of parallel jobs --fs &lt;fs&gt; # sampling rate (default=22050) --n_fft &lt;n_fft&gt; # number of FFT points (default=1024) --n_shift &lt;n_shift&gt; # shift size in point (default=256) --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs.\"]},\"524\":{\"h\":\"make_fbank.sh\",\"t\":[\"source\",\"Usage: make_fbank.sh [options] &lt;data-dir&gt; [&lt;log-dir&gt; [&lt;fbank-dir&gt;] ] e.g.: make_fbank.sh data/train exp/make_fbank/train mfcc Note: &lt;log-dir&gt; defaults to &lt;data-dir&gt;/log, and &lt;fbank-dir&gt; defaults to &lt;data-dir&gt;/data Options: --nj &lt;nj&gt; # number of parallel jobs --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs. --filetype &lt;mat|hdf5|sound.hdf5&gt; # Specify the format of feats file\"]},\"525\":{\"h\":\"make_stft.sh\",\"t\":[\"source\",\"Usage: make_stft.sh [options] &lt;data-dir&gt; [&lt;log-dir&gt; [&lt;stft-dir&gt;] ] e.g.: make_stft.sh data/train exp/make_stft/train stft Note: &lt;log-dir&gt; defaults to &lt;data-dir&gt;/log, and &lt;stft-dir&gt; defaults to &lt;data-dir&gt;/data Options: --nj &lt;nj&gt; # number of parallel jobs --cmd (utils/run.pl|utils/queue.pl &lt;queue opts&gt;) # how to run jobs. --filetype &lt;mat|hdf5|sound.hdf5&gt; # Specify the format of feats file\"]},\"526\":{\"h\":\"pack_model.sh\",\"t\":[\"source\",\"Usage: pack_model.sh --lm &lt;lm&gt; --dict &lt;dict&gt; &lt;tr_conf&gt; &lt;dec_conf&gt; &lt;cmvn&gt; &lt;e2e&gt;, for example: &lt;lm&gt;: exp/train_rnnlm/rnnlm.model.best &lt;dict&gt;: data/lang_char &lt;tr_conf&gt;: conf/train.yaml &lt;dec_conf&gt;: conf/decode.yaml &lt;cmvn&gt;: data/tr_it/cmvn.ark &lt;e2e&gt;: exp/tr_it_pytorch_train/results/model.last10.avg.best\"]},\"527\":{\"h\":\"recog_wav.sh\",\"t\":[\"source\",\"Usage: recog_wav.sh [options] &lt;wav_file&gt; Options: --backend &lt;chainer|pytorch&gt; # chainer or pytorch (Default: pytorch) --ngpu &lt;ngpu&gt; # Number of GPUs (Default: 0) --decode_dir &lt;directory_name&gt; # Name of directory to store decoding temporary data --models &lt;model_name&gt; # Model name (e.g. tedlium2.transformer.v1) --cmvn &lt;path&gt; # Location of cmvn.ark --lang_model &lt;path&gt; # Location of language model --recog_model &lt;path&gt; # Location of E2E model --decode_config &lt;path&gt; # Location of configuration file --api &lt;api_version&gt; # API version (v1 or v2, available in only pytorch backend) Example: # Record audio from microphone input as example.wav rec -c 1 -r 16000 example.wav trim 0 5 # Decode using model name recog_wav.sh --models tedlium2.transformer.v1 example.wav # Decode with streaming mode (only RNN with API v1 is supported) recog_wav.sh --models tedlium2.rnn.v2 --api v1 example.wav # Decode using model file recog_wav.sh --cmvn cmvn.ark --lang_model rnnlm.model.best --recog_model model.acc.best --decode_config conf/decode.yaml example.wav # Decode with GPU (require batchsize > 0 in configuration file) recog_wav.sh --ngpu 1 example.wav Available models: - tedlium2.rnn.v1 - tedlium2.rnn.v2 - tedlium2.transformer.v1 - tedlium3.transformer.v1 - librispeech.transformer.v1 - librispeech.transformer.v1.transformerlm.v1 - commonvoice.transformer.v1 - csj.transformer.v1\"]},\"528\":{\"h\":\"reduce_data_dir.sh\",\"t\":[\"source\",\"usage: reduce_data_dir.sh srcdir turnlist destdir\"]},\"529\":{\"h\":\"remove_longshortdata.sh\",\"t\":[\"source\",\"usage: remove_longshortdata.sh olddatadir newdatadir\"]},\"530\":{\"h\":\"score_bleu.sh\",\"t\":[\"source\",\"No help found.\"]},\"531\":{\"h\":\"score_sclite.sh\",\"t\":[\"source\",\"Usage: score_sclite.sh &lt;data-dir&gt; &lt;dict&gt;\"]},\"532\":{\"h\":\"score_sclite_case.sh\",\"t\":[\"source\",\"No help found.\"]},\"533\":{\"h\":\"score_sclite_wo_dict.sh\",\"t\":[\"source\",\"Usage: score_sclite_wo_dict.sh &lt;data-dir&gt;\"]},\"534\":{\"h\":\"show_result.sh\",\"t\":[\"source\",\"No help found.\"]},\"535\":{\"h\":\"speed_perturb.sh\",\"t\":[\"source\",\"Usage: speed_perturb.sh [options] &lt;data-dir&gt; &lt;destination-dir&gt; &lt;fbankdir&gt; e.g.: speed_perturb.sh data/train en de Options: --cases # target case information (e.g., lc.rm, lc, tc) --speeds # speed used in speed perturbation (e.g., 0.9. 1.0, 1.1) --langs # all languages (source + target) --write_utt2num_frames # write utt2num_frames in steps/make_fbank_pitch.sh --cmd &lt;run.pl|queue.pl <queue opts&gt;> # how to run jobs --nj &lt;nj&gt; # number of parallel jobs\"]},\"536\":{\"h\":\"synth_wav.sh\",\"t\":[\"source\",\"Usage: $ synth_wav.sh &lt;text&gt; Note: This code does not include text frontend part. Please clean the input text manually. Also, you need to modify feature configuration according to the model. Default setting is for ljspeech models, so if you want to use other pretrained models, please modify the parameters by yourself. For our provided models, you can find them in the tables at https://github.com/espnet/espnet#tts-demo. If you are beginner, instead of this script, I strongly recommend trying the following colab notebook at first, which includes all of the procedure from text frontend, feature generation, and waveform generation. https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb Example: # make text file and then generate it # (for the default model, ljspeech, we use upper-case char sequence as the input) echo \\\"THIS IS A DEMONSTRATION OF TEXT TO SPEECH.\\\" > example.txt synth_wav.sh example.txt # also you can use multiple text echo \\\"THIS IS A DEMONSTRATION OF TEXT TO SPEECH.\\\" > example.txt echo \\\"TEXT TO SPEECH IS A TECHQNIQUE TO CONVERT TEXT INTO SPEECH.\\\" >> example.txt synth_wav.sh example.txt # you can specify the pretrained models synth_wav.sh --models ljspeech.transformer.v3 example.txt # also you can specify vocoder model synth_wav.sh --vocoder_models ljspeech.wavenet.mol.v2 example.txt Available models: - ljspeech.tacotron2.v1 - ljspeech.tacotron2.v2 - ljspeech.tacotron2.v3 - ljspeech.transformer.v1 - ljspeech.transformer.v2 - ljspeech.transformer.v3 - ljspeech.fastspeech.v1 - ljspeech.fastspeech.v2 - ljspeech.fastspeech.v3 - libritts.tacotron2.v1 - libritts.transformer.v1 - jsut.transformer.v1 - jsut.tacotron2.v1 - csmsc.transformer.v1 - csmsc.fastspeech.v3 Available vocoder models: - ljspeech.wavenet.softmax.ns.v1 - ljspeech.wavenet.mol.v1 - ljspeech.parallel_wavegan.v1 - libritts.wavenet.mol.v1 - jsut.wavenet.mol.v1 - jsut.parallel_wavegan.v1 - csmsc.wavenet.mol.v1 - csmsc.parallel_wavegan.v1 Model details: | Model name | Lang | Fs [Hz] | Mel range [Hz] | FFT / Shift / Win [pt] | Input type | | ----------------------- | ---- | ------- | -------------- | ---------------------- | ---------- | | ljspeech.tacotron2.v1 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.tacotron2.v2 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.tacotron2.v3 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.transformer.v1 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.transformer.v2 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.transformer.v3 | EN | 22.05k | None | 1024 / 256 / None | phn | | ljspeech.fastspeech.v1 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.fastspeech.v2 | EN | 22.05k | None | 1024 / 256 / None | char | | ljspeech.fastspeech.v3 | EN | 22.05k | None | 1024 / 256 / None | phn | | libritts.tacotron2.v1 | EN | 24k | 80-7600 | 1024 / 256 / None | char | | libritts.transformer.v1 | EN | 24k | 80-7600 | 1024 / 256 / None | char | | jsut.tacotron2 | JP | 24k | 80-7600 | 2048 / 300 / 1200 | phn | | jsut.transformer | JP | 24k | 80-7600 | 2048 / 300 / 1200 | phn | | csmsc.transformer.v1 | ZH | 24k | 80-7600 | 2048 / 300 / 1200 | pinyin | | csmsc.fastspeech.v3 | ZH | 24k | 80-7600 | 2048 / 300 / 1200 | pinyin | Vocoder model details: | Model name | Lang | Fs [Hz] | Mel range [Hz] | FFT / Shift / Win [pt] | Model type | | ------------------------------ | ---- | ------- | -------------- | ---------------------- | ---------------- | | ljspeech.wavenet.softmax.ns.v1 | EN | 22.05k | None | 1024 / 256 / None | Softmax WaveNet | | ljspeech.wavenet.mol.v1 | EN | 22.05k | None | 1024 / 256 / None | MoL WaveNet | | ljspeech.parallel_wavegan.v1 | EN | 22.05k | None | 1024 / 256 / None | Parallel WaveGAN | | libritts.wavenet.mol.v1 | EN | 24k | None | 1024 / 256 / None | MoL WaveNet | | jsut.wavenet.mol.v1 | JP | 24k | 80-7600 | 2048 / 300 / 1200 | MoL WaveNet | | jsut.parallel_wavegan.v1 | JP | 24k | 80-7600 | 2048 / 300 / 1200 | Parallel WaveGAN | | csmsc.wavenet.mol.v1 | ZH | 24k | 80-7600 | 2048 / 300 / 1200 | MoL WaveNet | | csmsc.parallel_wavegan.v1 | ZH | 24k | 80-7600 | 2048 / 300 / 1200 | Parallel WaveGAN |\"]},\"537\":{\"h\":\"trim_silence.sh\",\"t\":[\"source\",\"Usage: trim_silence.sh [options] &lt;data-dir&gt; &lt;log-dir&gt; e.g.: trim_silence.sh data/train exp/trim_silence/train Options: --fs &lt;fs&gt; # sampling frequency (default=16000) --win_length &lt;win_length&gt; # window length in point (default=1024) --shift_length &lt;shift_length&gt; # shift length in point (default=256) --threshold &lt;threshold&gt; # power threshold in db (default=60) --min_silence &lt;sec&gt; # minimum silence length in sec (default=0.01) --normalize &lt;bit&gt; # audio bit (default=16) --cmd &lt;cmd&gt; # how to run jobs (default=run.pl) --nj &lt;nj&gt; # number of parallel jobs (default=32)\"]},\"538\":{\"h\":\"addjson.py\",\"t\":[\"source\",\"add multiple json values to an input or output value\",\"usage: addjson.py [-h] [-i IS_INPUT] [--verbose VERBOSE] jsons [jsons ...]\"]},\"539\":{\"h\":\"Positional Arguments\"},\"540\":{\"h\":\"Named Arguments\"},\"541\":{\"h\":\"average_checkpoints.py\",\"t\":[\"source\",\"average models from snapshot\",\"usage: average_checkpoints.py [-h] --snapshots SNAPSHOTS [SNAPSHOTS ...] --out OUT [--num NUM] [--backend BACKEND] [--log [LOG]] [--metric [{acc,bleu,cer,cer_ctc,loss,perplexity}]] [--max-epoch [MAX_EPOCH]]\"]},\"542\":{\"h\":\"Named Arguments\"},\"543\":{\"h\":\"calculate_rtf.py\",\"t\":[\"source\",\"calculate real time factor (RTF)\",\"usage: calculate_rtf.py [-h] [--log-dir LOG_DIR] [--log-name {decode,asr_inference}] [--input-shift INPUT_SHIFT] [--start-times-marker {input lengths,speech length}] [--end-times-marker {prediction,best hypo}] [--inf-num INF_NUM]\"]},\"544\":{\"h\":\"Named Arguments\"},\"545\":{\"h\":\"change_yaml.py\",\"t\":[\"source\",\"change specified attributes of a YAML file\",\"usage: change_yaml.py [-h] [-o OUTYAML | --outdir OUTDIR] [-a ARG] [-d DELETE] [inyaml]\"]},\"546\":{\"h\":\"Positional Arguments\"},\"547\":{\"h\":\"Named Arguments\"},\"548\":{\"h\":\"compute-fbank-feats.py\",\"t\":[\"source\",\"compute FBANK feature from WAV\",\"usage: compute-fbank-feats.py [-h] [--fs FS] [--fmax [FMAX]] [--fmin [FMIN]] [--n_mels N_MELS] [--n_fft N_FFT] [--n_shift N_SHIFT] [--win_length [WIN_LENGTH]] [--window {hann,hamming}] [--write-num-frames WRITE_NUM_FRAMES] [--filetype {mat,hdf5}] [--compress COMPRESS] [--compression-method COMPRESSION_METHOD] [--verbose VERBOSE] [--normalize {1,16,24,32}] [--segments SEGMENTS] rspecifier wspecifier\"]},\"549\":{\"h\":\"Positional Arguments\"},\"550\":{\"h\":\"Named Arguments\"},\"551\":{\"h\":\"compute-stft-feats.py\",\"t\":[\"source\",\"compute STFT feature from WAV\",\"usage: compute-stft-feats.py [-h] [--fs FS] [--n_fft N_FFT] [--n_shift N_SHIFT] [--win_length [WIN_LENGTH]] [--window {hann,hamming}] [--write-num-frames WRITE_NUM_FRAMES] [--filetype {mat,hdf5}] [--compress COMPRESS] [--compression-method COMPRESSION_METHOD] [--verbose VERBOSE] [--normalize {1,16,24,32}] [--segments SEGMENTS] rspecifier wspecifier\"]},\"552\":{\"h\":\"Positional Arguments\"},\"553\":{\"h\":\"Named Arguments\"},\"554\":{\"h\":\"concat_json_multiref.py\",\"t\":[\"source\",\"concatenate multiple json files for data augmentation\",\"usage: concat_json_multiref.py [-h] jsons [jsons ...]\"]},\"555\":{\"h\":\"Positional Arguments\"},\"556\":{\"h\":\"concatjson.py\",\"t\":[\"source\",\"concatenate json files\",\"usage: concatjson.py [-h] jsons [jsons ...]\"]},\"557\":{\"h\":\"Positional Arguments\"},\"558\":{\"h\":\"convert_fbank_to_wav.py\",\"t\":[\"source\",\"convert FBANK to WAV using Griffin-Lim algorithm\",\"usage: convert_fbank_to_wav.py [-h] [--fs FS] [--fmax [FMAX]] [--fmin [FMIN]] [--n_fft N_FFT] [--n_shift N_SHIFT] [--win_length [WIN_LENGTH]] [--n_mels [N_MELS]] [--window {hann,hamming}] [--iters ITERS] [--filetype {mat,hdf5}] rspecifier outdir\"]},\"559\":{\"h\":\"Positional Arguments\"},\"560\":{\"h\":\"Named Arguments\"},\"561\":{\"h\":\"copy-feats.py\",\"t\":[\"source\",\"copy feature with preprocessing\",\"usage: copy-feats.py [-h] [--verbose VERBOSE] [--in-filetype {mat,hdf5,sound.hdf5,sound}] [--out-filetype {mat,hdf5,sound.hdf5,sound}] [--write-num-frames WRITE_NUM_FRAMES] [--compress COMPRESS] [--compression-method COMPRESSION_METHOD] [--preprocess-conf PREPROCESS_CONF] rspecifier wspecifier\"]},\"562\":{\"h\":\"Positional Arguments\"},\"563\":{\"h\":\"Named Arguments\"},\"564\":{\"h\":\"eval_perm_free_error.py\",\"t\":[\"source\",\"evaluate permutation-free error\",\"usage: eval_perm_free_error.py [-h] [--num-spkrs NUM_SPKRS] results [results ...]\"]},\"565\":{\"h\":\"Positional Arguments\"},\"566\":{\"h\":\"Named Arguments\"},\"567\":{\"h\":\"feat-to-shape.py\",\"t\":[\"source\",\"convert feature to its shape\",\"usage: feat-to-shape.py [-h] [--verbose VERBOSE] [--filetype {mat,hdf5,sound.hdf5,sound}] [--preprocess-conf PREPROCESS_CONF] rspecifier [out]\"]},\"568\":{\"h\":\"Positional Arguments\"},\"569\":{\"h\":\"Named Arguments\"},\"570\":{\"h\":\"feats2npy.py\",\"t\":[\"source\",\"Convet kaldi-style features to numpy arrays\",\"usage: feats2npy.py [-h] scp_file out_dir\"]},\"571\":{\"h\":\"Positional Arguments\"},\"572\":{\"h\":\"filt.py\",\"t\":[\"source\",\"filter words in a text file\",\"usage: filt.py [-h] [--exclude] filt infile\"]},\"573\":{\"h\":\"Positional Arguments\"},\"574\":{\"h\":\"Named Arguments\"},\"575\":{\"h\":\"generate_wav_from_fbank.py\",\"t\":[\"source\",\"generate wav from FBANK using wavenet vocoder\",\"usage: generate_wav_from_fbank.py [-h] [--fs FS] [--n_fft N_FFT] [--n_shift N_SHIFT] [--model MODEL] [--filetype {mat,hdf5}] rspecifier outdir\"]},\"576\":{\"h\":\"Positional Arguments\"},\"577\":{\"h\":\"Named Arguments\"},\"578\":{\"h\":\"json2sctm.py\",\"t\":[\"source\",\"convert json to sctm\",\"usage: json2sctm.py [-h] [--num-spkrs [NUM_SPKRS]] [--refs [REFS ...]] [--hyps [HYPS ...]] [--orig-stm [ORIG_STM]] [--stm STM [STM ...]] [--ctm CTM [CTM ...]] [--bpe [BPE]] [json] dict\"]},\"579\":{\"h\":\"Positional Arguments\"},\"580\":{\"h\":\"Named Arguments\"},\"581\":{\"h\":\"json2text.py\",\"t\":[\"source\",\"convert ASR recognized json to text\",\"usage: json2text.py [-h] json dict ref hyp\"]},\"582\":{\"h\":\"Positional Arguments\"},\"583\":{\"h\":\"json2trn.py\",\"t\":[\"source\",\"convert a json to a transcription file with a token dictionary\",\"usage: json2trn.py [-h] [--num-spkrs NUM_SPKRS] [--refs REFS [REFS ...]] [--hyps HYPS [HYPS ...]] json dict\"]},\"584\":{\"h\":\"Positional Arguments\"},\"585\":{\"h\":\"Named Arguments\"},\"586\":{\"h\":\"json2trn_mt.py\",\"t\":[\"source\",\"convert json to machine translation transcription\",\"usage: json2trn_mt.py [-h] [--refs REFS [REFS ...]] [--hyps HYPS [HYPS ...]] [--srcs SRCS [SRCS ...]] [--dict-src [DICT_SRC]] json dict\"]},\"587\":{\"h\":\"Positional Arguments\"},\"588\":{\"h\":\"Named Arguments\"},\"589\":{\"h\":\"json2trn_wo_dict.py\",\"t\":[\"source\",\"convert a json to a transcription file with a token dictionary\",\"usage: json2trn_wo_dict.py [-h] [--num-spkrs NUM_SPKRS] [--refs REFS [REFS ...]] [--hyps HYPS [HYPS ...]] json\"]},\"590\":{\"h\":\"Positional Arguments\"},\"591\":{\"h\":\"Named Arguments\"},\"592\":{\"h\":\"mix-mono-wav-scp.py\",\"t\":[\"source\",\"Mixing wav.scp files into a multi-channel wav.scp using sox.\",\"usage: mix-mono-wav-scp.py [-h] scp [scp ...] [out]\"]},\"593\":{\"h\":\"Positional Arguments\"},\"594\":{\"h\":\"result2json.py\",\"t\":[\"source\",\"convert sclite’s result.txt file to json\",\"usage: result2json.py [-h] [--key KEY]\"]},\"595\":{\"h\":\"Named Arguments\"},\"596\":{\"h\":\"score_lang_id.py\",\"t\":[\"source\",\"language identification scoring\",\"usage: score_lang_id.py [-h] --ref REF --hyp HYP [--out OUT]\"]},\"597\":{\"h\":\"Named Arguments\"},\"598\":{\"h\":\"scp2json.py\",\"t\":[\"source\",\"convert scp to json\",\"usage: scp2json.py [-h] [--key KEY]\"]},\"599\":{\"h\":\"Named Arguments\"},\"600\":{\"h\":\"splitjson.py\",\"t\":[\"source\",\"split a json file for parallel processing\",\"usage: splitjson.py [-h] [--parts PARTS] json\"]},\"601\":{\"h\":\"Positional Arguments\"},\"602\":{\"h\":\"Named Arguments\"},\"603\":{\"h\":\"text2token.py\",\"t\":[\"source\",\"convert raw text to tokenized text\",\"usage: text2token.py [-h] [--nchar NCHAR] [--skip-ncols SKIP_NCOLS] [--space SPACE] [--non-lang-syms NON_LANG_SYMS] [--trans_type {char,phn}] [text]\"]},\"604\":{\"h\":\"Positional Arguments\"},\"605\":{\"h\":\"Named Arguments\"},\"606\":{\"h\":\"trim_silence.py\",\"t\":[\"source\",\"Trim slience with simple power thresholding and make segments file.\",\"usage: trim_silence.py [-h] [--fs FS] [--threshold THRESHOLD] [--win_length WIN_LENGTH] [--shift_length SHIFT_LENGTH] [--min_silence MIN_SILENCE] [--figdir FIGDIR] [--verbose VERBOSE] [--normalize {1,16,24,32}] rspecifier wspecifier\"]},\"607\":{\"h\":\"Positional Arguments\"},\"608\":{\"h\":\"Named Arguments\"},\"609\":{\"h\":\"trn2ctm.py\",\"t\":[\"source\",\"convert trn to ctm\",\"usage: trn2ctm.py [-h] [trn] [ctm]\"]},\"610\":{\"h\":\"Positional Arguments\"},\"611\":{\"h\":\"trn2stm.py\",\"t\":[\"source\",\"convert trn to stm\",\"usage: trn2stm.py [-h] [--orig-stm [ORIG_STM]] [trn] [stm]\"]},\"612\":{\"h\":\"Positional Arguments\"},\"613\":{\"h\":\"Named Arguments\"},\"614\":{\"h\":\"espnet2.asr_transducer.decoder.abs_decoder.AbsDecoder\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.abs_decoder.AbsDecoder(*args, **kwargs)\",\"Bases: Module, ABC\",\"Abstract decoder module.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract batch_score(hyps: List[Any]) → Tuple[Tensor, List[Dict[str, Tensor]] | List[Tensor] | Tuple[Tensor, Tensor | None]]\",\"One-step forward hypotheses.\",\"Parameters:hyps – Hypotheses.\",\"Returns: Decoder output sequences. states: Decoder hidden states.\",\"Return type: out\",\"abstract create_batch_states(new_states: List[List[Dict[str, Tensor | None]] | List[List[Tensor]] | Tuple[Tensor, Tensor | None]]) → List[Dict[str, Tensor]] | List[Tensor] | Tuple[Tensor, Tensor | None]\",\"Create batch of decoder hidden states given a list of new states.\",\"Parameters:new_states – Decoder hidden states.\",\"Returns: Decoder hidden states.\",\"abstract forward(labels: Tensor) → Tensor\",\"Encode source label sequences.\",\"Parameters:labels – Label ID sequences.\",\"Returns: Decoder output sequences.\",\"abstract init_state(batch_size: int) → List[Dict[str, Tensor]] | List[Tensor] | Tuple[Tensor, tensor | None]\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Decoder hidden states.\",\"abstract score(label_sequence: List[int], states: List[Dict[str, Tensor]] | List[Tensor] | Tuple[Tensor, Tensor | None]) → Tuple[Tensor, List[Dict[str, Tensor]] | List[Tensor] | Tuple[Tensor, Tensor | None]]\",\"One-step forward hypothesis.\",\"Parameters:\",\"label_sequence – Current label sequence.\",\"state – Decoder hidden states.\",\"Returns: Decoder output sequence. state: Decoder hidden states.\",\"Return type: out\",\"abstract select_state(states: List[Dict[str, Tensor]] | List[Tensor] | Tuple[Tensor, Tensor | None], idx: int = 0) → List[Dict[str, Tensor]] | List[Tensor] | Tuple[Tensor, Tensor | None]\",\"Get specified ID state from batch of states, if provided.\",\"Parameters:\",\"states – Decoder hidden states.\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID.\",\"abstract set_device(device: Tensor) → None\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\"]},\"615\":{\"h\":\"espnet2.asr_transducer.normalization.BasicNorm\",\"t\":[\"source\",\"class espnet2.asr_transducer.normalization.BasicNorm(normalized_shape: int, eps: float = 0.25)\",\"Bases: Module\",\"BasicNorm module definition.\",\"Reference: https://github.com/k2-fsa/icefall/pull/288\",\"Parameters:\",\"normalized_shape – Expected size.\",\"eps – Value added to the denominator for numerical stability.\",\"Construct a BasicNorm object.\",\"forward(x: Tensor) → Tensor\",\"Compute basic normalization.\",\"Parameters:x – Input sequences. (B, T, D_hidden)\",\"Returns: Output sequences. (B, T, D_hidden)\"]},\"616\":{\"h\":\"espnet2.asr_transducer.beam_search_transducer.BeamSearchTransducer\",\"t\":[\"source\",\"class espnet2.asr_transducer.beam_search_transducer.BeamSearchTransducer(decoder: AbsDecoder, joint_network: JointNetwork, beam_size: int, lm: Module | None = None, lm_weight: float = 0.1, search_type: str = 'default', max_sym_exp: int = 3, u_max: int = 50, nstep: int = 2, expansion_gamma: float = 2.3, expansion_beta: int = 2, score_norm: bool = False, nbest: int = 1, streaming: bool = False)\",\"Bases: object\",\"Beam search implementation for Transducer.\",\"Parameters:\",\"decoder – Decoder module.\",\"joint_network – Joint network module.\",\"beam_size – Size of the beam.\",\"lm – LM module.\",\"lm_weight – LM weight for soft fusion.\",\"search_type – Search algorithm to use during inference.\",\"max_sym_exp – Number of maximum symbol expansions at each time step. (TSD)\",\"u_max – Maximum expected target sequence length. (ALSD)\",\"nstep – Number of maximum expansion steps at each time step. (mAES)\",\"expansion_gamma – Allowed logp difference for prune-by-value method. (mAES)\",\"expansion_beta – Number of additional candidates for expanded hypotheses selection. (mAES)\",\"score_norm – Normalize final scores by length.\",\"nbest – Number of final hypothesis.\",\"streaming – Whether to perform chunk-by-chunk beam search.\",\"Construct a BeamSearchTransducer object.\",\"align_length_sync_decoding(enc_out: Tensor) → List[Hypothesis]\",\"Alignment-length synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:h – Encoder output sequences. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"create_lm_batch_inputs(hyps_seq: List[List[int]]) → Tensor\",\"Make batch of inputs with left padding for LM scoring.\",\"Parameters:hyps_seq – Hypothesis sequences.\",\"Returns: Padded batch of sequences.\",\"default_beam_search(enc_out: Tensor) → List[Hypothesis]\",\"Beam search implementation without prefix search.\",\"Modified from https://arxiv.org/pdf/1211.3711.pdf\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"modified_adaptive_expansion_search(enc_out: Tensor) → List[ExtendedHypothesis]\",\"Modified version of Adaptive Expansion Search (mAES).\",\"Based on AES (https://ieeexplore.ieee.org/document/9250505) and : NSC (https://arxiv.org/abs/2201.05420).\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"recombine_hyps(hyps: List[Hypothesis]) → List[Hypothesis]\",\"Recombine hypotheses with same label ID sequence.\",\"Parameters:hyps – Hypotheses.\",\"Returns: Recombined hypotheses.\",\"Return type: final\",\"reset_cache() → None\",\"Reset cache for streaming decoding.\",\"select_k_expansions(hyps: List[ExtendedHypothesis], topk_idx: Tensor, topk_logp: Tensor) → List[ExtendedHypothesis]\",\"Return K hypotheses candidates for expansion from a list of hypothesis.\",\"K candidates are selected according to the extended hypotheses probabilities and a prune-by-value method. Where K is equal to beam_size + beta.\",\"Parameters:\",\"hyps – Hypotheses.\",\"topk_idx – Indices of candidates hypothesis.\",\"topk_logp – Log-probabilities of candidates hypothesis.\",\"Returns: Best K expansion hypotheses candidates.\",\"Return type: k_expansions\",\"sort_nbest(hyps: List[Hypothesis]) → List[Hypothesis]\",\"Sort in-place hypotheses by score or score given sequence length.\",\"Parameters:hyps – Hypothesis.\",\"Returns: Sorted hypothesis.\",\"Return type: hyps\",\"time_sync_decoding(enc_out: Tensor) → List[Hypothesis]\",\"Time synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\"]},\"617\":{\"h\":\"espnet2.asr_transducer.encoder.blocks.branchformer.Branchformer\",\"t\":[\"source\",\"class espnet2.asr_transducer.encoder.blocks.branchformer.Branchformer(block_size: int, linear_size: int, self_att: ~torch.nn.modules.module.Module, conv_mod: ~torch.nn.modules.module.Module, norm_class: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, norm_args: ~typing.Dict = {}, dropout_rate: float = 0.0)\",\"Bases: Module\",\"Branchformer module definition.\",\"Reference: https://arxiv.org/pdf/2207.02971.pdf\",\"Parameters:\",\"block_size – Input/output size.\",\"linear_size – Linear layers’ hidden size.\",\"self_att – Self-attention module instance.\",\"conv_mod – Convolution module instance.\",\"norm_class – Normalization class.\",\"norm_args – Normalization module arguments.\",\"dropout_rate – Dropout rate.\",\"Construct a Branchformer object.\",\"chunk_forward(x: Tensor, pos_enc: Tensor, mask: Tensor, left_context: int = 0) → Tuple[Tensor, Tensor]\",\"Encode chunk of input sequence.\",\"Parameters:\",\"x – Branchformer input sequences. (B, T, D_block)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"mask – Source mask. (B, T_2)\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"Returns: Branchformer output sequences. (B, T, D_block) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"Return type: x\",\"forward(x: Tensor, pos_enc: Tensor, mask: Tensor, chunk_mask: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor]\",\"Encode input sequences.\",\"Parameters:\",\"x – Branchformer input sequences. (B, T, D_block)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"mask – Source mask. (B, T)\",\"chunk_mask – Chunk mask. (T_2, T_2)\",\"Returns: Branchformer output sequences. (B, T, D_block) mask: Source mask. (B, T) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"Return type: x\",\"reset_streaming_cache(left_context: int, device: device) → None\",\"Initialize/Reset self-attention and convolution modules cache for streaming.\",\"Parameters:\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"device – Device to use for cache tensor.\"]},\"618\":{\"h\":\"espnet2.asr_transducer.encoder.blocks.conformer.Conformer\",\"t\":[\"source\",\"class espnet2.asr_transducer.encoder.blocks.conformer.Conformer(block_size: int, self_att: ~torch.nn.modules.module.Module, feed_forward: ~torch.nn.modules.module.Module, feed_forward_macaron: ~torch.nn.modules.module.Module, conv_mod: ~torch.nn.modules.module.Module, norm_class: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, norm_args: ~typing.Dict = {}, dropout_rate: float = 0.0)\",\"Bases: Module\",\"Conformer module definition.\",\"Parameters:\",\"block_size – Input/output size.\",\"self_att – Self-attention module instance.\",\"feed_forward – Feed-forward module instance.\",\"feed_forward_macaron – Feed-forward module instance for macaron network.\",\"conv_mod – Convolution module instance.\",\"norm_class – Normalization module class.\",\"norm_args – Normalization module arguments.\",\"dropout_rate – Dropout rate.\",\"Construct a Conformer object.\",\"chunk_forward(x: Tensor, pos_enc: Tensor, mask: Tensor, left_context: int = 0) → Tuple[Tensor, Tensor]\",\"Encode chunk of input sequence.\",\"Parameters:\",\"x – Conformer input sequences. (B, T, D_block)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"mask – Source mask. (B, T_2)\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"Returns: Conformer output sequences. (B, T, D_block) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"Return type: x\",\"forward(x: Tensor, pos_enc: Tensor, mask: Tensor, chunk_mask: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor]\",\"Encode input sequences.\",\"Parameters:\",\"x – Conformer input sequences. (B, T, D_block)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"mask – Source mask. (B, T)\",\"chunk_mask – Chunk mask. (T_2, T_2)\",\"Returns: Conformer output sequences. (B, T, D_block) mask: Source mask. (B, T) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"Return type: x\",\"reset_streaming_cache(left_context: int, device: device) → None\",\"Initialize/Reset self-attention and convolution modules cache for streaming.\",\"Parameters:\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"device – Device to use for cache tensor.\"]},\"619\":{\"h\":\"espnet2.asr_transducer.encoder.modules.convolution.ConformerConvolution\",\"t\":[\"source\",\"class espnet2.asr_transducer.encoder.modules.convolution.ConformerConvolution(channels: int, kernel_size: int, activation: Module = ReLU(), norm_args: Dict = {}, causal: bool = False)\",\"Bases: Module\",\"ConformerConvolution module definition.\",\"Parameters:\",\"channels – The number of channels.\",\"kernel_size – Size of the convolving kernel.\",\"activation – Activation function.\",\"norm_args – Normalization module arguments.\",\"causal – Whether to use causal convolution (set to True if streaming).\",\"Construct an ConformerConvolution object.\",\"forward(x: Tensor, mask: Tensor | None = None, cache: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Compute convolution module.\",\"Parameters:\",\"x – ConformerConvolution input sequences. (B, T, D_hidden)\",\"mask – Source mask. (B, T_2)\",\"cache – ConformerConvolution input cache. (1, D_hidden, conv_kernel)\",\"Returns: ConformerConvolution output sequences. (B, ?, D_hidden) cache: ConformerConvolution output cache. (1, D_hidden, conv_kernel)\",\"Return type: x\"]},\"620\":{\"h\":\"espnet2.asr_transducer.encoder.blocks.conv1d.Conv1d\",\"t\":[\"source\",\"class espnet2.asr_transducer.encoder.blocks.conv1d.Conv1d(input_size: int, output_size: int, kernel_size: int | Tuple, stride: int | Tuple = 1, dilation: int | Tuple = 1, groups: int | Tuple = 1, bias: bool = True, batch_norm: bool = False, relu: bool = True, causal: bool = False, dropout_rate: float = 0.0)\",\"Bases: Module\",\"Conv1d module definition.\",\"Parameters:\",\"input_size – Input dimension.\",\"output_size – Output dimension.\",\"kernel_size – Size of the convolving kernel.\",\"stride – Stride of the convolution.\",\"dilation – Spacing between the kernel points.\",\"groups – Number of blocked connections from input channels to output channels.\",\"bias – Whether to add a learnable bias to the output.\",\"batch_norm – Whether to use batch normalization after convolution.\",\"relu – Whether to use a ReLU activation after convolution.\",\"causal – Whether to use causal convolution (set to True if streaming).\",\"dropout_rate – Dropout rate.\",\"Construct a Conv1d object.\",\"chunk_forward(x: Tensor, pos_enc: Tensor, mask: Tensor, left_context: int = 0) → Tuple[Tensor, Tensor]\",\"Encode chunk of input sequence.\",\"Parameters:\",\"x – Conv1d input sequences. (B, T, D_in)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_in)\",\"mask – Source mask. (B, T)\",\"left_context – Number of previous frames the attention module can see in current chunk (not used here).\",\"Returns: Conv1d output sequences. (B, T, D_out) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_out)\",\"Return type: x\",\"create_new_mask(mask: Tensor) → Tensor\",\"Create new mask for output sequences.\",\"Parameters:mask – Mask of input sequences. (B, T)\",\"Returns: Mask of output sequences. (B, sub(T))\",\"Return type: mask\",\"create_new_pos_enc(pos_enc: Tensor) → Tensor\",\"Create new positional embedding vector.\",\"Parameters:pos_enc – Input sequences positional embedding. (B, 2 * (T - 1), D_in)\",\"Returns: Output sequences positional embedding. : (B, 2 * (sub(T) - 1), D_in)\",\"Return type: pos_enc\",\"forward(x: Tensor, pos_enc: Tensor, mask: Tensor | None = None, chunk_mask: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor]\",\"Encode input sequences.\",\"Parameters:\",\"x – Conv1d input sequences. (B, T, D_in)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_in)\",\"mask – Source mask. (B, T)\",\"chunk_mask – Chunk mask. (T_2, T_2)\",\"Returns: Conv1d output sequences. (B, sub(T), D_out) mask: Source mask. (B, T) or (B, sub(T)) pos_enc: Positional embedding sequences. \",\"(B, 2 * (T - 1), D_att) or (B, 2 * (sub(T) - 1), D_out)\",\"Return type: x\",\"reset_streaming_cache(left_context: int, device: device) → None\",\"Initialize/Reset Conv1d cache for streaming.\",\"Parameters:\",\"left_context – Number of previous frames the attention module can see in current chunk (not used here).\",\"device – Device to use for cache tensor.\"]},\"621\":{\"h\":\"espnet2.asr_transducer.encoder.blocks.conv_input.ConvInput\",\"t\":[\"source\",\"class espnet2.asr_transducer.encoder.blocks.conv_input.ConvInput(input_size: int, conv_size: int | Tuple, subsampling_factor: int = 4, vgg_like: bool = True, output_size: int | None = None)\",\"Bases: Module\",\"ConvInput module definition.\",\"Parameters:\",\"input_size – Input size.\",\"conv_size – Convolution size.\",\"subsampling_factor – Subsampling factor.\",\"vgg_like – Whether to use a VGG-like network.\",\"output_size – Block output dimension.\",\"Construct a ConvInput object.\",\"forward(x: Tensor, mask: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Encode input sequences.\",\"Parameters:\",\"x – ConvInput input sequences. (B, T, D_feats)\",\"mask – Mask of input sequences. (B, 1, T)\",\"Returns: ConvInput output sequences. (B, sub(T), D_out) mask: Mask of output sequences. (B, 1, sub(T))\",\"Return type: x\"]},\"622\":{\"h\":\"espnet2.asr_transducer.encoder.modules.convolution.ConvolutionalSpatialGatingUnit\",\"t\":[\"source\",\"class espnet2.asr_transducer.encoder.modules.convolution.ConvolutionalSpatialGatingUnit(size: int, kernel_size: int, norm_class: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, norm_args: ~typing.Dict = {}, dropout_rate: float = 0.0, causal: bool = False)\",\"Bases: Module\",\"Convolutional Spatial Gating Unit module definition.\",\"Parameters:\",\"size – Initial size to determine the number of channels.\",\"kernel_size – Size of the convolving kernel.\",\"norm_class – Normalization module class.\",\"norm_args – Normalization module arguments.\",\"dropout_rate – Dropout rate.\",\"causal – Whether to use causal convolution (set to True if streaming).\",\"Construct a ConvolutionalSpatialGatingUnit object.\",\"forward(x: Tensor, mask: Tensor | None = None, cache: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Compute convolution module.\",\"Parameters:\",\"x – ConvolutionalSpatialGatingUnit input sequences. (B, T, D_hidden)\",\"mask – Source mask. (B, T_2)\",\"cache – ConvolutionalSpationGatingUnit input cache. (1, D_hidden, conv_kernel)\",\"Returns: ConvolutionalSpatialGatingUnit output sequences. (B, ?, D_hidden)\",\"Return type: x\"]},\"623\":{\"h\":\"espnet2.asr_transducer.encoder.modules.convolution.DepthwiseConvolution\",\"t\":[\"source\",\"class espnet2.asr_transducer.encoder.modules.convolution.DepthwiseConvolution(size: int, kernel_size: int, causal: bool = False)\",\"Bases: Module\",\"Depth-wise Convolution module definition.\",\"Parameters:\",\"size – Initial size to determine the number of channels.\",\"kernel_size – Size of the convolving kernel.\",\"causal – Whether to use causal convolution (set to True if streaming).\",\"Construct a DepthwiseConvolution object.\",\"forward(x: Tensor, mask: Tensor | None = None, cache: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Compute convolution module.\",\"Parameters:\",\"x – DepthwiseConvolution input sequences. (B, T, D_hidden)\",\"mask – Source mask. (B, T_2)\",\"cache – DepthwiseConvolution input cache. (1, conv_kernel, D_hidden)\",\"Returns: DepthwiseConvolution output sequences. (B, ?, D_hidden)\",\"Return type: x\"]},\"624\":{\"h\":\"espnet2.asr_transducer.encoder.blocks.ebranchformer.EBranchformer\",\"t\":[\"source\",\"class espnet2.asr_transducer.encoder.blocks.ebranchformer.EBranchformer(block_size: int, linear_size: int, self_att: ~torch.nn.modules.module.Module, feed_forward: ~torch.nn.modules.module.Module, feed_forward_macaron: ~torch.nn.modules.module.Module, conv_mod: ~torch.nn.modules.module.Module, depthwise_conv_mod: ~torch.nn.modules.module.Module, norm_class: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, norm_args: ~typing.Dict = {}, dropout_rate: float = 0.0)\",\"Bases: Module\",\"E-Branchformer module definition.\",\"Reference: https://arxiv.org/pdf/2210.00077.pdf\",\"Parameters:\",\"block_size – Input/output size.\",\"linear_size – Linear layers’ hidden size.\",\"self_att – Self-attention module instance.\",\"feed_forward – Feed-forward module instance.\",\"feed_forward_macaron – Feed-forward module instance for macaron network.\",\"conv_mod – ConvolutionalSpatialGatingUnit module instance.\",\"depthwise_conv_mod – DepthwiseConvolution module instance.\",\"norm_class – Normalization class.\",\"norm_args – Normalization module arguments.\",\"dropout_rate – Dropout rate.\",\"Construct a E-Branchformer object.\",\"chunk_forward(x: Tensor, pos_enc: Tensor, mask: Tensor, left_context: int = 0) → Tuple[Tensor, Tensor]\",\"Encode chunk of input sequence.\",\"Parameters:\",\"x – E-Branchformer input sequences. (B, T, D_block)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"mask – Source mask. (B, T_2)\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"Returns: E-Branchformer output sequences. (B, T, D_block) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"Return type: x\",\"forward(x: Tensor, pos_enc: Tensor, mask: Tensor, chunk_mask: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor]\",\"Encode input sequences.\",\"Parameters:\",\"x – E-Branchformer input sequences. (B, T, D_block)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"mask – Source mask. (B, T)\",\"chunk_mask – Chunk mask. (T_2, T_2)\",\"Returns: E-Branchformer output sequences. (B, T, D_block) mask: Source mask. (B, T) pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)\",\"Return type: x\",\"reset_streaming_cache(left_context: int, device: device) → None\",\"Initialize/Reset self-attention and convolution modules cache for streaming.\",\"Parameters:\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"device – Device to use for cache tensor.\"]},\"625\":{\"h\":\"espnet2.asr_transducer.espnet_transducer_model.ESPnetASRTransducerModel\",\"t\":[\"source\",\"class espnet2.asr_transducer.espnet_transducer_model.ESPnetASRTransducerModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, encoder: Encoder, decoder: AbsDecoder, joint_network: JointNetwork, transducer_weight: float = 1.0, use_k2_pruned_loss: bool = False, k2_pruned_loss_args: Dict = {}, warmup_steps: int = 25000, validation_nstep: int = 2, fastemit_lambda: float = 0.0, auxiliary_ctc_weight: float = 0.0, auxiliary_ctc_dropout_rate: float = 0.0, auxiliary_lm_loss_weight: float = 0.0, auxiliary_lm_loss_smoothing: float = 0.05, ignore_id: int = -1, sym_space: str = '<space>', sym_blank: str = '<blank>', report_cer: bool = False, report_wer: bool = False, extract_feats_in_collect_stats: bool = True)\",\"Bases: AbsESPnetModel\",\"ESPnet2ASRTransducerModel module definition.\",\"Parameters:\",\"vocab_size – Size of complete vocabulary (w/ SOS/EOS and blank included).\",\"token_list – List of tokens in vocabulary (minus reserved tokens).\",\"frontend – Frontend module.\",\"specaug – SpecAugment module.\",\"normalize – Normalization module.\",\"encoder – Encoder module.\",\"decoder – Decoder module.\",\"joint_network – Joint Network module.\",\"transducer_weight – Weight of the Transducer loss.\",\"use_k2_pruned_loss – Whether to use k2 pruned Transducer loss.\",\"k2_pruned_loss_args – Arguments of the k2 loss pruned Transducer loss.\",\"warmup_steps – Number of steps in warmup, used for pruned loss scaling.\",\"validation_nstep – Maximum number of symbol expansions at each time step when reporting CER or/and WER using mAES.\",\"fastemit_lambda – FastEmit lambda value.\",\"auxiliary_ctc_weight – Weight of auxiliary CTC loss.\",\"auxiliary_ctc_dropout_rate – Dropout rate for auxiliary CTC loss inputs.\",\"auxiliary_lm_loss_weight – Weight of auxiliary LM loss.\",\"auxiliary_lm_loss_smoothing – Smoothing rate for LM loss’ label smoothing.\",\"ignore_id – Initial padding ID.\",\"sym_space – Space symbol.\",\"sym_blank – Blank Symbol.\",\"report_cer – Whether to report Character Error Rate during validation.\",\"report_wer – Whether to report Word Error Rate during validation.\",\"extract_feats_in_collect_stats – Whether to use extract_feats stats collection.\",\"Construct an ESPnetASRTransducerModel object.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"Collect features sequences and features lengths sequences.\",\"Parameters:\",\"speech – Speech sequences. (B, S)\",\"speech_lengths – Speech sequences lengths. (B,)\",\"text – Label ID sequences. (B, L)\",\"text_lengths – Label ID sequences lengths. (B,)\",\"kwargs – Contains “utts_id”.\",\"Returns: “feats”: Features sequences. (B, T, D_feats), : ”feats_lengths”: Features sequences lengths. (B,)\",\"Return type: {}\",\"encode(speech: Tensor, speech_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Encoder speech sequences.\",\"Parameters:\",\"speech – Speech sequences. (B, S)\",\"speech_lengths – Speech sequences lengths. (B,)\",\"Returns: Encoder outputs. (B, T, D_enc) encoder_out_lens: Encoder outputs lengths. (B,)\",\"Return type: encoder_out\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Forward architecture and compute loss(es).\",\"Parameters:\",\"speech – Speech sequences. (B, S)\",\"speech_lengths – Speech sequences lengths. (B,)\",\"text – Label ID sequences. (B, L)\",\"text_lengths – Label ID sequences lengths. (B,)\",\"kwargs – Contains “utts_id”.\",\"Returns: Main loss value. stats: Task statistics. weight: Task weights.\",\"Return type: loss\"]},\"626\":{\"h\":\"espnet2.asr_transducer.encoder.encoder.Encoder\",\"t\":[\"source\",\"class espnet2.asr_transducer.encoder.encoder.Encoder(input_size: int, body_conf: List[Dict[str, Any]], input_conf: Dict[str, Any] = {}, main_conf: Dict[str, Any] = {})\",\"Bases: Module\",\"Encoder module definition.\",\"Parameters:\",\"input_size – Input size.\",\"body_conf – Encoder body configuration.\",\"input_conf – Encoder input configuration.\",\"main_conf – Encoder main configuration.\",\"Construct an Encoder object.\",\"chunk_forward(x: Tensor, x_len: Tensor, processed_frames: tensor, left_context: int = 32) → Tensor\",\"Encode input sequences as chunks.\",\"Parameters:\",\"x – Encoder input features. (1, T_in, F)\",\"x_len – Encoder input features lengths. (1,)\",\"processed_frames – Number of frames already seen.\",\"left_context – Number of previous frames (AFTER subsampling) the attention module can see in current chunk.\",\"Returns: Encoder outputs. (B, T_out, D_enc)\",\"Return type: x\",\"forward(x: Tensor, x_len: Tensor) → Tuple[Tensor, Tensor]\",\"Encode input sequences.\",\"Parameters:\",\"x – Encoder input features. (B, T_in, F)\",\"x_len – Encoder input features lengths. (B,)\",\"Returns: Encoder outputs. (B, T_out, D_enc) x_len: Encoder outputs lenghts. (B,)\",\"Return type: x\",\"reset_cache(left_context: int, device: device) → None\",\"Initialize/Reset encoder cache for streaming.\",\"Parameters:\",\"left_context – Number of previous frames (AFTER subsampling) the attention module can see in current chunk.\",\"device – Device ID.\"]},\"627\":{\"h\":\"espnet2.asr_transducer.error_calculator.ErrorCalculator\",\"t\":[\"source\",\"class espnet2.asr_transducer.error_calculator.ErrorCalculator(decoder: AbsDecoder, joint_network: JointNetwork, token_list: List[int], sym_space: str, sym_blank: str, nstep: int = 2, report_cer: bool = False, report_wer: bool = False)\",\"Bases: object\",\"Calculate CER and WER for transducer models.\",\"Parameters:\",\"decoder – Decoder module.\",\"joint_network – Joint Network module.\",\"token_list – List of token units.\",\"sym_space – Space symbol.\",\"sym_blank – Blank symbol.\",\"nstep – Maximum number of symbol expansions at each time step w/ mAES.\",\"report_cer – Whether to compute CER.\",\"report_wer – Whether to compute WER.\",\"Construct an ErrorCalculatorTransducer object.\",\"calculate_cer(char_pred: Tensor, char_target: Tensor) → float\",\"Calculate sentence-level CER score.\",\"Parameters:\",\"char_pred – Prediction character sequences. (B, ?)\",\"char_target – Target character sequences. (B, ?)\",\"Returns: Average sentence-level CER score.\",\"calculate_wer(char_pred: Tensor, char_target: Tensor) → float\",\"Calculate sentence-level WER score.\",\"Parameters:\",\"char_pred – Prediction character sequences. (B, ?)\",\"char_target – Target character sequences. (B, ?)\",\"Returns: Average sentence-level WER score\",\"convert_to_char(pred: Tensor, target: Tensor) → Tuple[List, List]\",\"Convert label ID sequences to character sequences.\",\"Parameters:\",\"pred – Prediction label ID sequences. (B, U)\",\"target – Target label ID sequences. (B, L)\",\"Returns: Prediction character sequences. (B, ?) char_target: Target character sequences. (B, ?)\",\"Return type: char_pred\"]},\"628\":{\"h\":\"espnet2.asr_transducer.beam_search_transducer.ExtendedHypothesis\",\"t\":[\"source\",\"class espnet2.asr_transducer.beam_search_transducer.ExtendedHypothesis(score: float, yseq: List[int], dec_state: Tuple[Tensor, Tensor | None] | None = None, lm_state: Dict[str, Any] | List[Any] | None = None, dec_out: Tensor | None = None, lm_score: Tensor | None = None)\",\"Bases: Hypothesis\",\"Extended hypothesis definition for NSC beam search and mAES.\",\":param : Hypothesis dataclass arguments. :param dec_out: Decoder output sequence. (B, D_dec) :param lm_score: Log-probabilities of the LM for given label. (vocab_size)\",\"dec_out : Tensor= None\",\"lm_score : Tensor= None\"]},\"629\":{\"h\":\"espnet2.asr_transducer.activation.FTSwish\",\"t\":[\"source\",\"class espnet2.asr_transducer.activation.FTSwish(threshold: float = -0.2, mean_shift: float = 0)\",\"Bases: Module\",\"Flatten-T Swish activation definition.\",\"FTSwish(x) = x * sigmoid(x) + threshold : where FTSwish(x) < 0 = threshold\",\"Reference: https://arxiv.org/abs/1812.06247\",\"Parameters:\",\"threshold – Threshold value for FTSwish activation formulation. (threshold < 0)\",\"mean_shift – Mean shifting value for FTSwish activation formulation. (applied only if != 0, disabled by default)\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor) → Tensor\",\"Forward computation.\"]},\"630\":{\"h\":\"espnet2.asr_transducer.decoder.modules.rwkv.feed_forward.FeedForward\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.modules.rwkv.feed_forward.FeedForward(size: int, hidden_size: int, block_id: int, num_blocks: int)\",\"Bases: Module\",\"FeedForward module definition.\",\"Parameters:\",\"size – Input/Output size.\",\"hidden_size – Hidden size.\",\"block_id – Block index.\",\"num_blocks – Number of blocks in the architecture.\",\"Construct a FeedForward object.\",\"forward(x: Tensor, state: List[Tensor] | None = None) → Tuple[Tensor, List[Tensor] | None]\",\"Compute channel mixing.\",\"Parameters:\",\"x – FeedForward input sequences. (B, U, size)\",\"state – Decoder hidden state. [5 x (B, 1, size, N)]\",\"Returns: FeedForward output sequences. (B, U, size) state: Decoder hidden state. [5 x (B, 1, size, N)]\",\"Return type: x\",\"reset_parameters(size: int, block_id: int, num_blocks: int) → None\",\"Reset module parameters.\",\"Parameters:\",\"size – Block size.\",\"block_id – Block index.\",\"num_blocks – Number of blocks in the architecture.\"]},\"631\":{\"h\":\"espnet2.asr_transducer.beam_search_transducer.Hypothesis\",\"t\":[\"source\",\"class espnet2.asr_transducer.beam_search_transducer.Hypothesis(score: float, yseq: List[int], dec_state: Tuple[Tensor, Tensor | None] | None = None, lm_state: Dict[str, Any] | List[Any] | None = None)\",\"Bases: object\",\"Default hypothesis definition for Transducer search algorithms.\",\"Parameters:\",\"score – Total log-probability.\",\"yseq – Label sequence as integer ID sequence.\",\"dec_state – RNN/MEGA Decoder state (None if Stateless).\",\"lm_state – RNNLM state. ((N, D_lm), (N, D_lm)) or None\",\"dec_state : Tuple[Tensor, Tensor | None] | None= None\",\"lm_state : Dict[str, Any] | List[Any] | None= None\",\"score : float\",\"yseq : List[int]\"]},\"632\":{\"h\":\"espnet2.asr_transducer.joint_network.JointNetwork\",\"t\":[\"source\",\"class espnet2.asr_transducer.joint_network.JointNetwork(output_size: int, encoder_size: int, decoder_size: int, joint_space_size: int = 256, joint_activation_type: str = 'tanh', lin_dec_bias: bool = True, **activation_parameters)\",\"Bases: Module\",\"Transducer joint network module.\",\"Parameters:\",\"output_size – Output size.\",\"encoder_size – Encoder output size.\",\"decoder_size – Decoder output size.\",\"joint_space_size – Joint space size.\",\"joint_act_type – Type of activation for joint network.\",\"**activation_parameters – Parameters for the activation function.\",\"Construct a JointNetwork object.\",\"forward(enc_out: Tensor, dec_out: Tensor, no_projection: bool = False) → Tensor\",\"Joint computation of encoder and decoder hidden state sequences.\",\"Parameters:\",\"enc_out – Expanded encoder output state sequences. (B, T, s_range, D_enc) or (B, T, 1, D_enc)\",\"dec_out – Expanded decoder output state sequences. (B, T, s_range, D_dec) or (B, 1, U, D_dec)\",\"Returns: Joint output state sequences. : (B, T, U, D_out) or (B, T, s_range, D_out)\",\"Return type: joint_out\"]},\"633\":{\"h\":\"espnet2.asr_transducer.decoder.blocks.mega.MEGA\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.blocks.mega.MEGA(size: int = 512, num_heads: int = 4, qk_size: int = 128, v_size: int = 1024, activation: ~torch.nn.modules.module.Module = ReLU(), normalization: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, rel_pos_bias_type: str = 'simple', max_positions: int = 2048, truncation_length: int | None = None, chunk_size: int = -1, dropout_rate: float = 0.0, att_dropout_rate: float = 0.0, ema_dropout_rate: float = 0.0)\",\"Bases: Module\",\"MEGA module.\",\"Parameters:\",\"size – Input/Output size.\",\"num_heads – Number of EMA heads.\",\"qk_size – Shared query and key size for attention module.\",\"v_size – Value size for attention module.\",\"qk_v_size – (QK, V) sizes for attention module.\",\"activation – Activation function type.\",\"normalization – Normalization module.\",\"rel_pos_bias_type – Type of relative position bias in attention module.\",\"max_positions – Maximum number of position for RelativePositionBias.\",\"truncation_length – Maximum length for truncation in EMA module.\",\"chunk_size – Chunk size for attention computation (-1 = full context).\",\"dropout_rate – Dropout rate for inner modules.\",\"att_dropout_rate – Dropout rate for the attention module.\",\"ema_dropout_rate – Dropout rate for the EMA module.\",\"Construct a MEGA object.\",\"forward(x: Tensor, mask: Tensor | None = None, attn_mask: Tensor | None = None, state: Dict[str, Tensor | None] | None = None) → Tuple[Tensor, Dict[str, Tensor | None] | None]\",\"Compute moving average equiped gated attention.\",\"Parameters:\",\"x – MEGA input sequences. (L, B, size)\",\"mask – MEGA input sequence masks. (B, 1, L)\",\"attn_mask – MEGA attention mask. (1, L, L)\",\"state – Decoder hidden states.\",\"Returns: MEGA output sequences. (B, L, size) state: Decoder hidden states.\",\"Return type: x\",\"reset_parameters(val: int = 0.0, std: int = 0.02) → None\",\"Reset module parameters.\",\"Parameters:\",\"val – Initialization value.\",\"std – Standard deviation.\",\"softmax_attention(query: Tensor, key: Tensor, mask: Tensor | None = None, attn_mask: Tensor | None = None) → Tensor\",\"Compute attention weights with softmax.\",\"Parameters:\",\"query – Query tensor. (B, 1, L, D)\",\"key – Key tensor. (B, 1, L, D)\",\"mask – Sequence mask. (B, 1, L)\",\"attn_mask – Attention mask. (1, L, L)\",\"Returns: Attention weights. (B, 1, L, L)\",\"Return type: attn_weights\"]},\"634\":{\"h\":\"espnet2.asr_transducer.decoder.mega_decoder.MEGADecoder\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.mega_decoder.MEGADecoder(vocab_size: int, block_size: int = 512, linear_size: int = 1024, qk_size: int = 128, v_size: int = 1024, num_heads: int = 4, rel_pos_bias_type: str = 'simple', max_positions: int = 2048, truncation_length: int | None = None, normalization_type: str = 'layer_norm', normalization_args: Dict = {}, activation_type: str = 'swish', activation_args: Dict = {}, chunk_size: int = -1, num_blocks: int = 4, dropout_rate: float = 0.0, embed_dropout_rate: float = 0.0, att_dropout_rate: float = 0.0, ema_dropout_rate: float = 0.0, ffn_dropout_rate: float = 0.0, embed_pad: int = 0)\",\"Bases: AbsDecoder\",\"MEGA decoder module.\",\"Based on https://arxiv.org/pdf/2209.10655.pdf.\",\"Parameters:\",\"vocab_size – Vocabulary size.\",\"block_size – Input/Output size.\",\"linear_size – NormalizedPositionwiseFeedForward hidden size.\",\"qk_size – Shared query and key size for attention module.\",\"v_size – Value size for attention module.\",\"num_heads – Number of EMA heads.\",\"rel_pos_bias – Type of relative position bias in attention module.\",\"max_positions – Maximum number of position for RelativePositionBias.\",\"truncation_length – Maximum length for truncation in EMA module.\",\"normalization_type – Normalization layer type.\",\"normalization_args – Normalization layer arguments.\",\"activation_type – Activation function type.\",\"activation_args – Activation function arguments.\",\"chunk_size – Chunk size for attention computation (-1 = full context).\",\"num_blocks – Number of MEGA blocks.\",\"dropout_rate – Dropout rate for MEGA internal modules.\",\"embed_dropout_rate – Dropout rate for embedding layer.\",\"att_dropout_rate – Dropout rate for the attention module.\",\"ema_dropout_rate – Dropout rate for the EMA module.\",\"ffn_dropout_rate – Dropout rate for the feed-forward module.\",\"embed_pad – Embedding padding symbol ID.\",\"Construct a MEGADecoder object.\",\"batch_score(hyps: List[Hypothesis]) → Tuple[Tensor, List[Dict[str, Tensor]]]\",\"One-step forward hypotheses.\",\"Parameters:hyps – Hypotheses.\",\"Returns: states:\",\"Return type: out\",\"create_batch_states(new_states: List[List[Dict[str, Tensor]]]) → List[Dict[str, Tensor]]\",\"Create batch of decoder hidden states given a list of new states.\",\"Parameters:new_states – Decoder hidden states. [B x [N x Dict]]\",\"Returns: Decoder hidden states. [N x Dict]\",\"forward(labels: Tensor) → Tensor\",\"Encode source label sequences.\",\"Parameters:labels – Decoder input sequences. (B, L)\",\"Returns: Decoder output sequences. (B, U, D_dec)\",\"Return type: out\",\"inference(labels: Tensor, states: List[Dict[str, Tensor]]) → Tuple[Tensor, List[Dict[str, Tensor]]]\",\"Encode source label sequences.\",\"Parameters:\",\"labels – Decoder input sequences. (B, L)\",\"states – Decoder hidden states. [B x Dict]\",\"Returns: Decoder output sequences. (B, U, D_dec) new_states: Decoder hidden states. [B x Dict]\",\"Return type: out\",\"init_state(batch_size: int = 0) → List[Dict[str, Tensor]]\",\"Initialize MEGADecoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Decoder hidden states. [N x Dict]\",\"Return type: states\",\"score(label_sequence: List[int], states: List[Dict[str, Tensor]]) → Tuple[Tensor, List[Dict[str, Tensor]]]\",\"One-step forward hypothesis.\",\"Parameters:\",\"label_sequence – Current label sequence.\",\"states – Decoder hidden states. (??)\",\"Returns: Decoder output sequence. (D_dec) states: Decoder hidden states. (??)\",\"select_state(states: List[Dict[str, Tensor]], idx: int) → List[Dict[str, Tensor]]\",\"Select ID state from batch of decoder hidden states.\",\"Parameters:states – Decoder hidden states. [N x Dict]\",\"Returns: Decoder hidden states for given ID. [N x Dict]\",\"set_device(device: device) → None\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\",\"stack_qk_states(state_list: List[Tensor], dim: int) → List[Tensor]\",\"Stack query or key states with different lengths.\",\"Parameters:state_list – List of query or key states.\",\"Returns: Query/Key state.\",\"Return type: new_state\"]},\"635\":{\"h\":\"espnet2.asr_transducer.activation.Mish\",\"t\":[\"source\",\"class espnet2.asr_transducer.activation.Mish(softplus_beta: float = 1.0, softplus_threshold: int = 20, use_builtin: bool = False)\",\"Bases: Module\",\"Mish activation definition.\",\"Mish(x) = x * tanh(softplus(x))\",\"Reference: https://arxiv.org/abs/1908.08681.\",\"Parameters:\",\"softplus_beta – Beta value for softplus activation formulation. (Usually 0 > softplus_beta >= 2)\",\"softplus_threshold – Values above this revert to a linear function. (Usually 10 > softplus_threshold >= 20)\",\"use_builtin – Whether to use PyTorch activation function if available.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor) → Tensor\",\"Forward computation.\"]},\"636\":{\"h\":\"espnet2.asr_transducer.encoder.modules.multi_blocks.MultiBlocks\",\"t\":[\"source\",\"class espnet2.asr_transducer.encoder.modules.multi_blocks.MultiBlocks(block_list: ~typing.List[~torch.nn.modules.module.Module], output_size: int, norm_class: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, norm_args: ~typing.Dict | None = None, blockdrop_rate: int = 0.0)\",\"Bases: Module\",\"MultiBlocks definition.\",\"Parameters:\",\"block_list – Individual blocks of the encoder architecture.\",\"output_size – Architecture output size.\",\"norm_class – Normalization module class.\",\"norm_args – Normalization module arguments.\",\"blockdrop_rate – Probability threshold of dropping out each block.\",\"Construct a MultiBlocks object.\",\"chunk_forward(x: Tensor, pos_enc: Tensor, mask: Tensor, left_context: int = 0) → Tensor\",\"Forward each block of the encoder architecture.\",\"Parameters:\",\"x – MultiBlocks input sequences. (B, T, D_block_1)\",\"pos_enc – Positional embedding sequences. (B, 2 * (T - 1), D_att)\",\"mask – Source mask. (B, T_2)\",\"left_context – Number of previous frames the attention module can see in current chunk (used by Conformer and Branchformer block).\",\"Returns: MultiBlocks output sequences. (B, T, D_block_N)\",\"Return type: x\",\"forward(x: Tensor, pos_enc: Tensor, mask: Tensor, chunk_mask: Tensor | None = None) → Tensor\",\"Forward each block of the encoder architecture.\",\"Parameters:\",\"x – MultiBlocks input sequences. (B, T, D_block_1)\",\"pos_enc – Positional embedding sequences.\",\"mask – Source mask. (B, T)\",\"chunk_mask – Chunk mask. (T_2, T_2)\",\"Returns: Output sequences. (B, T, D_block_N)\",\"Return type: x\",\"reset_streaming_cache(left_context: int, device: device) → None\",\"Initialize/Reset encoder streaming cache.\",\"Parameters:\",\"left_context – Number of previous frames the attention module can see in current chunk (used by Conformer and Branchformer block).\",\"device – Device to use for cache tensor.\"]},\"637\":{\"h\":\"espnet2.asr_transducer.decoder.modules.mega.multi_head_damped_ema.MultiHeadDampedEMA\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.modules.mega.multi_head_damped_ema.MultiHeadDampedEMA(size: int, num_heads: int = 4, activation: Module = ReLU(), truncation_length: int | None = None)\",\"Bases: Module\",\"MultiHeadDampedEMA module definition.\",\"Parameters:\",\"size – Module size.\",\"num_heads – Number of attention heads.\",\"activation – Activation function type.\",\"truncation_length – Maximum length for truncation.\",\"Construct an MultiHeadDampedEMA object.\",\"compute_ema_coefficients() → Tuple[Tensor, Tensor]\",\"Compute EMA coefficients.\",\"Parameters:None\",\"Returns: Damping factor / P-th order coefficient. : (size, num_heads, 1)\",\"prev_timestep_weight: Previous timestep weight / Q-th order coefficient. : (size, num_heads, 1)\",\"Return type: damping_factor\",\"compute_ema_kernel(length: int) → Tensor\",\"Compute EMA kernel / vandermonde product.\",\"Parameters:length – Sequence length.\",\"Returns: EMA kernel / Vandermonde product. (size, L)\",\"ema_one_step(x: Tensor, state: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Perform exponential moving average for a single step.\",\"Parameters:\",\"x – MultiHeadDampedEMA input sequences. (B, D, 1)\",\"state – MultiHeadDampedEMA state. (B, D, num_heads)\",\"Returns: MultiHeadDamped output sequences. (B, 1, D) new_state: MultiHeadDampedEMA state. (B, D, num_heads)\",\"Return type: out\",\"forward(x: Tensor, mask: Tensor | None = None, state: Dict[str, Tensor] | None = None) → Tensor | None\",\"Compute multi-dimensional damped EMA.\",\"Parameters:\",\"x – MultiHeadDampedEMA input sequence. (L, B, D)\",\"mask – Sequence mask. (B, 1, L)\",\"state – MultiHeadDampedEMA state. (B, D, num_heads)\",\"Returns: MultiHeadDampedEMA output sequence. (B, L, D) new_state: MultiHeadDampedEMA state. (B, D, num_heads)\",\"Return type: x\",\"get_ema_coefficients() → Tuple[Tensor, Tensor]\",\"Get EMA coefficients.\",\"Parameters:None\",\"Returns: Damping factor / P-th order coefficient. (size, num_heads, 1) : Previous timestep weight / Q-th order coefficient. (size, num_heads, 1)\",\"reset_parameters(val: float = 0.0, std1: float = 0.2, std2: float = 1.0) → None\",\"Reset module parameters.\",\"Parameters:\",\"val – Initialization value.\",\"std1 – Main standard deviation.\",\"std2 – Secondary standard deviation.\"]},\"638\":{\"h\":\"espnet2.asr_transducer.decoder.modules.mega.feed_forward.NormalizedPositionwiseFeedForward\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.modules.mega.feed_forward.NormalizedPositionwiseFeedForward(size: int, hidden_size: int, normalization: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, activation: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.activation.ReLU'>, dropout_rate: float = 0.0)\",\"Bases: Module\",\"NormalizedPositionFeedForward module definition.\",\"Parameters:\",\"size – Input/Output size.\",\"hidden_size – Hidden size.\",\"normalization – Normalization module.\",\"activation – Activation function.\",\"dropout_rate – Dropout rate.\",\"Construct an NormalizedPositionwiseFeedForward object.\",\"forward(x: Tensor) → Tensor\",\"Compute feed-forward module.\",\"Parameters:x – NormalizedPositionwiseFeedForward input sequences. (B, L, size)\",\"Returns: NormalizedPositionwiseFeedForward output sequences. (B, L, size)\",\"Return type: x\",\"reset_parameters(val: float = 0.0, std: float = 0.02) → None\",\"Reset module parameters.\",\"Parameters:\",\"val – Initialization value.\",\"std – Standard deviation.\"]},\"639\":{\"h\":\"espnet2.asr_transducer.frontend.online_audio_processor.OnlineAudioProcessor\",\"t\":[\"source\",\"class espnet2.asr_transducer.frontend.online_audio_processor.OnlineAudioProcessor(feature_extractor: Module, normalization_module: Module, decoding_window: int, encoder_sub_factor: int, frontend_conf: Dict, device: device, audio_sampling_rate: int = 16000)\",\"Bases: object\",\"OnlineProcessor module definition.\",\"Parameters:\",\"feature_extractor – Feature extractor module.\",\"normalization_module – Normalization module.\",\"decoding_window – Size of the decoding window (in ms).\",\"encoder_sub_factor – Encoder subsampling factor.\",\"frontend_conf – Frontend configuration.\",\"device – Device to pin module tensors on.\",\"audio_sampling_rate – Input sampling rate.\",\"Construct an OnlineAudioProcessor.\",\"compute_features(samples: Tensor, is_final: bool) → None\",\"Compute features from input samples.\",\"Parameters:\",\"samples – Speech data. (S)\",\"is_final – Whether speech corresponds to the final chunk of data.\",\"Returns: Features sequence. (1, chunk_sz_bs, D_feats) feats_length: Features length sequence. (1,)\",\"Return type: feats\",\"get_current_feats(feats: Tensor, feats_length: Tensor, is_final: bool) → Tuple[Tensor, Tensor]\",\"Get features for current decoding window.\",\"Parameters:\",\"feats – Computed features sequence. (1, F, D_feats)\",\"feats_length – Computed features sequence length. (1,)\",\"is_final – Whether feats corresponds to the final chunk of data.\",\"Returns: Decoding window features sequence. (1, chunk_sz_bs, D_feats) feats_length: Decoding window features length sequence. (1,)\",\"Return type: feats\",\"get_current_samples(samples: Tensor, is_final: bool) → Tensor\",\"Get samples for feature computation.\",\"Parameters:\",\"samples – Speech data. (S)\",\"is_final – Whether speech corresponds to the final chunk of data.\",\"Returns: New speech data. (1, decoding_samples)\",\"Return type: samples\",\"reset_cache() → None\",\"Reset cache parameters.\",\"Parameters:None\",\"Returns: None\"]},\"640\":{\"h\":\"espnet2.asr_transducer.normalization.RMSNorm\",\"t\":[\"source\",\"class espnet2.asr_transducer.normalization.RMSNorm(normalized_shape: int, eps: float = 1e-05, partial: float = 0.0)\",\"Bases: Module\",\"RMSNorm module definition.\",\"Reference: https://arxiv.org/pdf/1910.07467.pdf\",\"Parameters:\",\"normalized_shape – Expected size.\",\"eps – Value added to the denominator for numerical stability.\",\"partial – Value defining the part of the input used for RMS stats.\",\"Construct a RMSNorm object.\",\"forward(x: Tensor) → Tensor\",\"Compute RMS normalization.\",\"Parameters:x – Input sequences. (B, T, D_hidden)\",\"Returns: Output sequences. (B, T, D_hidden)\",\"Return type: x\"]},\"641\":{\"h\":\"espnet2.asr_transducer.decoder.rnn_decoder.RNNDecoder\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.rnn_decoder.RNNDecoder(vocab_size: int, embed_size: int = 256, hidden_size: int = 256, rnn_type: str = 'lstm', num_layers: int = 1, dropout_rate: float = 0.0, embed_dropout_rate: float = 0.0, embed_pad: int = 0)\",\"Bases: AbsDecoder\",\"RNN decoder module.\",\"Parameters:\",\"vocab_size – Vocabulary size.\",\"embed_size – Embedding size.\",\"hidden_size – Hidden size..\",\"rnn_type – Decoder layers type.\",\"num_layers – Number of decoder layers.\",\"dropout_rate – Dropout rate for decoder layers.\",\"embed_dropout_rate – Dropout rate for embedding layer.\",\"embed_pad – Embedding padding symbol ID.\",\"Construct a RNNDecoder object.\",\"batch_score(hyps: List[Hypothesis]) → Tuple[Tensor, Tuple[Tensor, Tensor | None]]\",\"One-step forward hypotheses.\",\"Parameters:hyps – Hypotheses.\",\"Returns: Decoder output sequences. (B, D_dec) states: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec) or None)\",\"Return type: out\",\"create_batch_states(new_states: List[Tuple[Tensor, Tensor | None]]) → Tuple[Tensor, Tensor | None]\",\"Create decoder hidden states.\",\"Parameters:new_states – Decoder hidden states. [B x ((N, 1, D_dec), (N, 1, D_dec) or None)]\",\"Returns: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec) or None)\",\"Return type: states\",\"forward(labels: Tensor) → Tensor\",\"Encode source label sequences.\",\"Parameters:labels – Label ID sequences. (B, L)\",\"Returns: Decoder output sequences. (B, U, D_dec)\",\"Return type: out\",\"init_state(batch_size: int) → Tuple[Tensor, tensor | None]\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Initial decoder hidden states. ((N, B, D_dec), (N, B, D_dec) or None)\",\"rnn_forward(x: Tensor, state: Tuple[Tensor, Tensor | None]) → Tuple[Tensor, Tuple[Tensor, Tensor | None]]\",\"Encode source label sequences.\",\"Parameters:\",\"x – RNN input sequences. (B, D_emb)\",\"state – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec) or None)\",\"Returns: RNN output sequences. (B, D_dec) (h_next, c_next): Decoder hidden states. \",\"(N, B, D_dec), (N, B, D_dec) or None)\",\"Return type: x\",\"score(label_sequence: List[int], states: Tuple[Tensor, Tensor | None]) → Tuple[Tensor, Tuple[Tensor, Tensor | None]]\",\"One-step forward hypothesis.\",\"Parameters:\",\"label_sequence – Current label sequence.\",\"states – Decoder hidden states. ((N, 1, D_dec), (N, 1, D_dec) or None)\",\"Returns: Decoder output sequence. (1, D_dec) states: Decoder hidden states. \",\"((N, 1, D_dec), (N, 1, D_dec) or None)\",\"Return type: out\",\"select_state(states: Tuple[Tensor, Tensor | None], idx: int) → Tuple[Tensor, Tensor | None]\",\"Get specified ID state from decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec) or None)\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID. ((N, 1, D_dec), (N, 1, D_dec) or None)\",\"set_device(device: device) → None\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\"]},\"642\":{\"h\":\"espnet2.asr_transducer.decoder.blocks.rwkv.RWKV\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.blocks.rwkv.RWKV(size: int, linear_size: int, attention_size: int, context_size: int, block_id: int, num_blocks: int, normalization_class: ~torch.nn.modules.module.Module = <class 'torch.nn.modules.normalization.LayerNorm'>, normalization_args: ~typing.Dict = {}, att_dropout_rate: float = 0.0, ffn_dropout_rate: float = 0.0)\",\"Bases: Module\",\"RWKV module.\",\"Parameters:\",\"size – Input/Output size.\",\"linear_size – Feed-forward hidden size.\",\"attention_size – SelfAttention hidden size.\",\"context_size – Context size for WKV computation.\",\"block_id – Block index.\",\"num_blocks – Number of blocks in the architecture.\",\"normalization_class – Normalization layer class.\",\"normalization_args – Normalization layer arguments.\",\"att_dropout_rate – Dropout rate for the attention module.\",\"ffn_dropout_rate – Dropout rate for the feed-forward module.\",\"Construct a RWKV object.\",\"forward(x: Tensor, state: Tensor | None = None) → Tuple[Tensor, Tensor | None]\",\"Compute receptance weighted key value.\",\"Parameters:\",\"x – RWKV input sequences. (B, L, size)\",\"state – Decoder hidden states. [5 x (B, D_att/size, N)]\",\"Returns: RWKV output sequences. (B, L, size) x: Decoder hidden states. [5 x (B, D_att/size, N)]\",\"Return type: x\"]},\"643\":{\"h\":\"espnet2.asr_transducer.decoder.rwkv_decoder.RWKVDecoder\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.rwkv_decoder.RWKVDecoder(vocab_size: int, block_size: int = 512, context_size: int = 1024, linear_size: int | None = None, attention_size: int | None = None, normalization_type: str = 'layer_norm', normalization_args: Dict = {}, num_blocks: int = 4, rescale_every: int = 0, embed_dropout_rate: float = 0.0, att_dropout_rate: float = 0.0, ffn_dropout_rate: float = 0.0, embed_pad: int = 0)\",\"Bases: AbsDecoder\",\"RWKV decoder module.\",\"Based on https://arxiv.org/pdf/2305.13048.pdf.\",\"Parameters:\",\"vocab_size – Vocabulary size.\",\"block_size – Input/Output size.\",\"context_size – Context size for WKV computation.\",\"linear_size – FeedForward hidden size.\",\"attention_size – SelfAttention hidden size.\",\"normalization_type – Normalization layer type.\",\"normalization_args – Normalization layer arguments.\",\"num_blocks – Number of RWKV blocks.\",\"rescale_every – Whether to rescale input every N blocks (inference only).\",\"embed_dropout_rate – Dropout rate for embedding layer.\",\"att_dropout_rate – Dropout rate for the attention module.\",\"ffn_dropout_rate – Dropout rate for the feed-forward module.\",\"embed_pad – Embedding padding symbol ID.\",\"Construct a RWKVDecoder object.\",\"batch_score(hyps: List[Hypothesis]) → Tuple[Tensor, List[Tensor]]\",\"One-step forward hypotheses.\",\"Parameters:hyps – Hypotheses.\",\"Returns: Decoder output sequence. (B, D_dec) states: Decoder hidden states. [5 x (B, 1, D_att/D_dec, N)]\",\"Return type: out\",\"create_batch_states(new_states: List[List[Dict[str, Tensor]]]) → List[Tensor]\",\"Create batch of decoder hidden states given a list of new states.\",\"Parameters:new_states – Decoder hidden states. [B x [5 x (1, 1, D_att/D_dec, N)]\",\"Returns: Decoder hidden states. [5 x (B, 1, D_att/D_dec, N)]\",\"forward(labels: Tensor) → Tensor\",\"Encode source label sequences.\",\"Parameters:labels – Decoder input sequences. (B, L)\",\"Returns: Decoder output sequences. (B, U, D_dec)\",\"Return type: out\",\"inference(labels: Tensor, states: Tensor) → Tuple[Tensor, List[Tensor]]\",\"Encode source label sequences.\",\"Parameters:\",\"labels – Decoder input sequences. (B, L)\",\"states – Decoder hidden states. [5 x (B, D_att/D_dec, N)]\",\"Returns: Decoder output sequences. (B, U, D_dec) states: Decoder hidden states. [5 x (B, D_att/D_dec, N)]\",\"Return type: out\",\"init_state(batch_size: int = 1) → List[Tensor]\",\"Initialize RWKVDecoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Decoder hidden states. [5 x (B, 1, D_att/D_dec, N)]\",\"Return type: states\",\"score(label_sequence: List[int], states: List[Tensor]) → Tuple[Tensor, List[Tensor]]\",\"One-step forward hypothesis.\",\"Parameters:\",\"label_sequence – Current label sequence.\",\"states – Decoder hidden states. [5 x (1, 1, D_att/D_dec, N)]\",\"Returns: Decoder output sequence. (D_dec) states: Decoder hidden states. [5 x (1, 1, D_att/D_dec, N)]\",\"select_state(states: List[Tensor], idx: int) → List[Tensor]\",\"Select ID state from batch of decoder hidden states.\",\"Parameters:states – Decoder hidden states. [5 x (B, 1, D_att/D_dec, N)]\",\"Returns: Decoder hidden states for given ID. [5 x (1, 1, D_att/D_dec, N)]\",\"set_device(device: device) → None\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\"]},\"644\":{\"h\":\"espnet2.asr_transducer.encoder.modules.attention.RelPositionMultiHeadedAttention\",\"t\":[\"source\",\"class espnet2.asr_transducer.encoder.modules.attention.RelPositionMultiHeadedAttention(num_heads: int, embed_size: int, dropout_rate: float = 0.0, simplified_attention_score: bool = False)\",\"Bases: Module\",\"RelPositionMultiHeadedAttention definition.\",\"Parameters:\",\"num_heads – Number of attention heads.\",\"embed_size – Embedding size.\",\"dropout_rate – Dropout rate.\",\"Construct an MultiHeadedAttention object.\",\"compute_attention_score(query: Tensor, key: Tensor, pos_enc: Tensor, left_context: int = 0) → Tensor\",\"Attention score computation.\",\"Parameters:\",\"query – Transformed query tensor. (B, H, T_1, d_k)\",\"key – Transformed key tensor. (B, H, T_2, d_k)\",\"pos_enc – Positional embedding tensor. (B, 2 * T_1 - 1, size)\",\"left_context – Number of previous frames to use for current chunk attention computation.\",\"Returns: Attention score. (B, H, T_1, T_2)\",\"compute_simplified_attention_score(query: Tensor, key: Tensor, pos_enc: Tensor, left_context: int = 0) → Tensor\",\"Simplified attention score computation.\",\"Reference: https://github.com/k2-fsa/icefall/pull/458\",\"Parameters:\",\"query – Transformed query tensor. (B, H, T_1, d_k)\",\"key – Transformed key tensor. (B, H, T_2, d_k)\",\"pos_enc – Positional embedding tensor. (B, 2 * T_1 - 1, size)\",\"left_context – Number of previous frames to use for current chunk attention computation.\",\"Returns: Attention score. (B, H, T_1, T_2)\",\"forward(query: Tensor, key: Tensor, value: Tensor, pos_enc: Tensor, mask: Tensor, chunk_mask: Tensor | None = None, left_context: int = 0) → Tensor\",\"Compute scaled dot product attention with rel. positional encoding.\",\"Parameters:\",\"query – Query tensor. (B, T_1, size)\",\"key – Key tensor. (B, T_2, size)\",\"value – Value tensor. (B, T_2, size)\",\"pos_enc – Positional embedding tensor. (B, 2 * T_1 - 1, size)\",\"mask – Source mask. (B, T_2)\",\"chunk_mask – Chunk mask. (T_1, T_1)\",\"left_context – Number of previous frames to use for current chunk attention computation.\",\"Returns: Output tensor. (B, T_1, H * d_k)\",\"forward_attention(value: Tensor, scores: Tensor, mask: Tensor, chunk_mask: Tensor | None = None) → Tensor\",\"Compute attention context vector.\",\"Parameters:\",\"value – Transformed value. (B, H, T_2, d_k)\",\"scores – Attention score. (B, H, T_1, T_2)\",\"mask – Source mask. (B, T_2)\",\"chunk_mask – Chunk mask. (T_1, T_1)\",\"Returns: Transformed value weighted by attention score. (B, T_1, H * d_k)\",\"Return type: attn_output\",\"forward_qkv(query: Tensor, key: Tensor, value: Tensor) → Tuple[Tensor, Tensor, Tensor]\",\"Transform query, key and value.\",\"Parameters:\",\"query – Query tensor. (B, T_1, size)\",\"key – Key tensor. (B, T_2, size)\",\"v – Value tensor. (B, T_2, size)\",\"Returns: Transformed query tensor. (B, H, T_1, d_k) k: Transformed key tensor. (B, H, T_2, d_k) v: Transformed value tensor. (B, H, T_2, d_k)\",\"Return type: q\",\"rel_shift(x: Tensor, left_context: int = 0) → Tensor\",\"Compute relative positional encoding.\",\"Parameters:\",\"x – Input sequence. (B, H, T_1, 2 * T_1 - 1)\",\"left_context – Number of previous frames to use for current chunk attention computation.\",\"Returns: Output sequence. (B, H, T_1, T_2)\",\"Return type: x\"]},\"645\":{\"h\":\"espnet2.asr_transducer.encoder.modules.positional_encoding.RelPositionalEncoding\",\"t\":[\"source\",\"class espnet2.asr_transducer.encoder.modules.positional_encoding.RelPositionalEncoding(size: int, dropout_rate: float = 0.0, max_len: int = 5000)\",\"Bases: Module\",\"Relative positional encoding.\",\"Parameters:\",\"size – Module size.\",\"max_len – Maximum input length.\",\"dropout_rate – Dropout rate.\",\"Construct a RelativePositionalEncoding object.\",\"extend_pe(x: Tensor, left_context: int = 0) → None\",\"Reset positional encoding.\",\"Parameters:\",\"x – Input sequences. (B, T, ?)\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"forward(x: Tensor, left_context: int = 0) → Tensor\",\"Compute positional encoding.\",\"Parameters:\",\"x – Input sequences. (B, T, ?)\",\"left_context – Number of previous frames the attention module can see in current chunk.\",\"Returns: Positional embedding sequences. (B, 2 * (T - 1), ?)\",\"Return type: pos_enc\"]},\"646\":{\"h\":\"espnet2.asr_transducer.decoder.modules.mega.positional_bias.RelativePositionBias\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.modules.mega.positional_bias.RelativePositionBias(max_positions: int)\",\"Bases: Module\",\"RelativePositionBias module definition.\",\"Parameters:max_positions – Maximum number of relative positions.\",\"Construct a RelativePositionBias object.\",\"forward(length: int) → Tensor\",\"Compute relative position bias.\",\"Parameters:length – Sequence length.\",\"Returns: Relative position bias. (L, L)\",\"Return type: tile\",\"reset_parameters(val: float = 0.0, std: float = 0.02) → None\",\"Reset module parameters.\",\"Parameters:\",\"val – Initialization value.\",\"std – Standard deviation.\"]},\"647\":{\"h\":\"espnet2.asr_transducer.decoder.modules.mega.positional_bias.RotaryRelativePositionBias\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.modules.mega.positional_bias.RotaryRelativePositionBias(size: int, max_positions: int = 2048)\",\"Bases: Module\",\"RotaryRelativePositionBias module definition.\",\"Parameters:\",\"size – Module embedding size.\",\"max_positions – Maximum number of relative positions.\",\"Construct a RotaryRelativePositionBias object.\",\"forward(length: int) → Tensor\",\"Compute rotary relative position bias.\",\"Parameters:length – Sequence length.\",\"Returns: Rotary relative position bias. (L, L)\",\"Return type: bias\",\"static get_sinusoid_embeddings(max_positions: int, size: int) → Tuple[Tensor, Tensor]\",\"Compute sinusoidal positional embeddings.\",\"Parameters:\",\"max_positions – Maximum number of positions.\",\"size – Input size.\",\"Returns: Sine elements. (max_positions, size // 2) : Cos elements. (max_positions, size // 2)\",\"reset_parameters(val: float = 0.0, std: float = 0.02) → None\",\"Reset module parameters.\",\"Parameters:\",\"val – Initialization value.\",\"std – Standard deviation.\",\"rotary(x: Tensor) → Tensor\",\"Compute rotary positional embeddings.\",\"Parameters:x – Input sequence. (L, size)\",\"Returns: Rotary positional embeddings. (L, size)\",\"Return type: x\"]},\"648\":{\"h\":\"espnet2.asr_transducer.normalization.ScaleNorm\",\"t\":[\"source\",\"class espnet2.asr_transducer.normalization.ScaleNorm(normalized_shape: int, eps: float = 1e-05)\",\"Bases: Module\",\"ScaleNorm module definition.\",\"Reference: https://arxiv.org/pdf/1910.05895.pdf\",\"Parameters:\",\"normalized_shape – Expected size.\",\"eps – Value added to the denominator for numerical stability.\",\"Construct a ScaleNorm object.\",\"forward(x: Tensor) → Tensor\",\"Compute scale normalization.\",\"Parameters:x – Input sequences. (B, T, D_hidden)\",\"Returns: Output sequences. (B, T, D_hidden)\"]},\"649\":{\"h\":\"espnet2.asr_transducer.decoder.modules.rwkv.attention.SelfAttention\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.modules.rwkv.attention.SelfAttention(size: int, attention_size: int, context_size: int, block_id: int, num_blocks: int)\",\"Bases: Module\",\"SelfAttention module definition.\",\"Parameters:\",\"size – Input/Output size.\",\"attention_size – Attention hidden size.\",\"context_size – Context size for WKV kernel.\",\"block_id – Block index.\",\"num_blocks – Number of blocks in the architecture.\",\"Construct a SelfAttention object.\",\"forward(x: Tensor, state: List[Tensor] | None = None) → Tuple[Tensor, List[Tensor] | None]\",\"Compute time mixing.\",\"Parameters:\",\"x – SelfAttention input sequences. (B, U, size)\",\"state – Decoder hidden states. [5 x (B, 1, D_att, N)]\",\"Returns: SelfAttention output sequences. (B, U, size)\",\"Return type: x\",\"reset_parameters(size: int, attention_size: int, block_id: int, num_blocks: int) → None\",\"Reset module parameters.\",\"Parameters:\",\"size – Block size.\",\"attention_size – Attention hidden size.\",\"block_id – Block index.\",\"num_blocks – Number of blocks in the architecture.\",\"wkv_linear_attention(time_decay: Tensor, time_first: Tensor, key: Tensor, value: Tensor, state: Tuple[Tensor, Tensor, Tensor]) → Tuple[Tensor, Tuple[Tensor, Tensor, Tensor]]\",\"Compute WKV with state (i.e.: for inference).\",\"Parameters:\",\"time_decay – Channel-wise time decay vector. (D_att)\",\"time_first – Channel-wise time first vector. (D_att)\",\"key – Key tensor. (B, 1, D_att)\",\"value – Value tensor. (B, 1, D_att)\",\"state – Decoder hidden states. [3 x (B, D_att)]\",\"Returns: Weighted Key-Value. (B, 1, D_att) state: Decoder hidden states. [3 x (B, 1, D_att)]\",\"Return type: output\"]},\"650\":{\"h\":\"espnet2.asr_transducer.activation.Smish\",\"t\":[\"source\",\"class espnet2.asr_transducer.activation.Smish(alpha: float = 1.0, beta: float = 1.0)\",\"Bases: Module\",\"Smish activation definition.\",\"Smish(x) = (alpha * x) * tanh(log(1 + sigmoid(beta * x))) : where alpha > 0 and beta > 0\",\"Reference: https://www.mdpi.com/2079-9292/11/4/540/htm.\",\"Parameters:\",\"alpha – Alpha value for Smish activation fomulation. (Usually, alpha = 1. If alpha <= 0, set value to 1).\",\"beta – Beta value for Smish activation formulation. (Usually, beta = 1. If beta <= 0, set value to 1).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor) → Tensor\",\"Forward computation.\"]},\"651\":{\"h\":\"espnet2.asr_transducer.decoder.stateless_decoder.StatelessDecoder\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.stateless_decoder.StatelessDecoder(vocab_size: int, embed_size: int = 256, embed_dropout_rate: float = 0.0, embed_pad: int = 0)\",\"Bases: AbsDecoder\",\"Stateless Transducer decoder module.\",\"Parameters:\",\"vocab_size – Output size.\",\"embed_size – Embedding size.\",\"embed_dropout_rate – Dropout rate for embedding layer.\",\"embed_pad – Embed/Blank symbol ID.\",\"Construct a StatelessDecoder object.\",\"batch_score(hyps: List[Hypothesis]) → Tuple[Tensor, None]\",\"One-step forward hypotheses.\",\"Parameters:hyps – Hypotheses.\",\"Returns: Decoder output sequences. (B, D_dec) states: Decoder hidden states. None\",\"Return type: out\",\"create_batch_states(new_states: List[Tensor | None]) → None\",\"Create decoder hidden states.\",\"Parameters:new_states – Decoder hidden states. [N x None]\",\"Returns: Decoder hidden states. None\",\"Return type: states\",\"forward(labels: Tensor, states: Any | None = None) → Tensor\",\"Encode source label sequences.\",\"Parameters:\",\"labels – Label ID sequences. (B, L)\",\"states – Decoder hidden states. None\",\"Returns: Decoder output sequences. (B, U, D_emb)\",\"Return type: embed\",\"init_state(batch_size: int) → None\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Initial decoder hidden states. None\",\"score(label_sequence: List[int], states: Any | None = None) → Tuple[Tensor, None]\",\"One-step forward hypothesis.\",\"Parameters:\",\"label_sequence – Current label sequence.\",\"states – Decoder hidden states. None\",\"Returns: Decoder output sequence. (1, D_emb) state: Decoder hidden states. None\",\"select_state(states: Tensor | None, idx: int) → None\",\"Get specified ID state from decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. None\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID. None\",\"set_device(device: device) → None\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\"]},\"652\":{\"h\":\"espnet2.asr_transducer.activation.Swish\",\"t\":[\"source\",\"class espnet2.asr_transducer.activation.Swish(beta: float = 1.0, use_builtin: bool = False)\",\"Bases: Module\",\"Swish activation definition.\",\"Swish(x) = (beta * x) * sigmoid(x) : where beta = 1 defines standard Swish activation.\",\"References\",\"https://arxiv.org/abs/2108.12943 / https://arxiv.org/abs/1710.05941v1. E-swish variant: https://arxiv.org/abs/1801.07145.\",\"Parameters:\",\"beta – Beta parameter for E-Swish. (beta >= 1. If beta < 1, use standard Swish).\",\"use_builtin – Whether to use PyTorch function if available.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor) → Tensor\",\"Forward computation.\"]},\"653\":{\"h\":\"espnet2.asr_transducer.utils.TooShortUttError\",\"t\":[\"source\",\"class espnet2.asr_transducer.utils.TooShortUttError(message: str, actual_size: int, limit: int)\",\"Bases: Exception\",\"Raised when the utt is too short for subsampling.\",\"Parameters:\",\"message – Error message to display.\",\"actual_size – The size that cannot pass the subsampling.\",\"limit – The size limit for subsampling.\",\"Construct a TooShortUttError module.\"]},\"654\":{\"h\":\"espnet2.asr_transducer.decoder.modules.rwkv.attention.WKVLinearAttention\",\"t\":[\"source\",\"class espnet2.asr_transducer.decoder.modules.rwkv.attention.WKVLinearAttention(*args, **kwargs)\",\"Bases: Function\",\"WKVLinearAttention function definition.\",\"static backward(ctx, grad_output: Tensor) → Tuple[Tensor, Tensor, Tensor, Tensor]\",\"WKVLinearAttention function backward pass.\",\"Parameters:grad_output – Output gradient. (B, U, D_att)\",\"Returns: Gradient for channel-wise time decay vector. (D_att) grad_time_first: Gradient for channel-wise time first vector. (D_att) grad_key: Gradient for key tensor. (B, U, D_att) grad_value: Gradient for value tensor. (B, U, D_att)\",\"Return type: grad_time_decay\",\"static forward(ctx, time_decay: Tensor, time_first: Tensor, key: Tensor, value: tensor) → Tensor\",\"WKVLinearAttention function forward pass.\",\"Parameters:\",\"time_decay – Channel-wise time decay vector. (D_att)\",\"time_first – Channel-wise time first vector. (D_att)\",\"key – Key tensor. (B, U, D_att)\",\"value – Value tensor. (B, U, D_att)\",\"Returns: Weighted Key-Value tensor. (B, U, D_att)\",\"Return type: out\"]},\"655\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_body_blocks\",\"t\":[\"source\",\"espnet2.asr_transducer.encoder.building.build_body_blocks(configuration: List[Dict[str, Any]], main_params: Dict[str, Any], output_size: int) → MultiBlocks\",\"Build encoder body blocks.\",\"Parameters:\",\"configuration – Body blocks configuration.\",\"main_params – Encoder main parameters.\",\"output_size – Architecture output size.\",\"Returns: MultiBlocks function encapsulation all encoder blocks.\"]},\"656\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_branchformer_block\",\"t\":[\"source\",\"espnet2.asr_transducer.encoder.building.build_branchformer_block(configuration: List[Dict[str, Any]], main_params: Dict[str, Any]) → Branchformer\",\"Build Branchformer block.\",\"Parameters:\",\"configuration – Branchformer block configuration.\",\"main_params – Encoder main parameters.\",\"Returns: Branchformer block function.\"]},\"657\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_conformer_block\",\"t\":[\"source\",\"espnet2.asr_transducer.encoder.building.build_conformer_block(configuration: List[Dict[str, Any]], main_params: Dict[str, Any]) → Conformer\",\"Build Conformer block.\",\"Parameters:\",\"configuration – Conformer block configuration.\",\"main_params – Encoder main parameters.\",\"Returns: Conformer block function.\"]},\"658\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_conv1d_block\",\"t\":[\"source\",\"espnet2.asr_transducer.encoder.building.build_conv1d_block(configuration: List[Dict[str, Any]], causal: bool) → Conv1d\",\"Build Conv1d block.\",\"Parameters:configuration – Conv1d block configuration.\",\"Returns: Conv1d block function.\"]},\"659\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_ebranchformer_block\",\"t\":[\"source\",\"espnet2.asr_transducer.encoder.building.build_ebranchformer_block(configuration: List[Dict[str, Any]], main_params: Dict[str, Any]) → EBranchformer\",\"Build E-Branchformer block.\",\"Parameters:\",\"configuration – E-Branchformer block configuration.\",\"main_params – Encoder main parameters.\",\"Returns: E-Branchformer block function.\"]},\"660\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_input_block\",\"t\":[\"source\",\"espnet2.asr_transducer.encoder.building.build_input_block(input_size: int, configuration: Dict[str, str | int]) → ConvInput\",\"Build encoder input block.\",\"Parameters:\",\"input_size – Input size.\",\"configuration – Input block configuration.\",\"Returns: ConvInput block function.\"]},\"661\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_main_parameters\",\"t\":[\"source\",\"espnet2.asr_transducer.encoder.building.build_main_parameters(pos_wise_act_type: str = 'swish', conv_mod_act_type: str = 'swish', pos_enc_dropout_rate: float = 0.0, pos_enc_max_len: int = 5000, simplified_att_score: bool = False, norm_type: str = 'layer_norm', conv_mod_norm_type: str = 'layer_norm', after_norm_eps: float | None = None, after_norm_partial: float | None = None, blockdrop_rate: float = 0.0, dynamic_chunk_training: bool = False, short_chunk_threshold: float = 0.75, short_chunk_size: int = 25, num_left_chunks: int = 0, **activation_parameters) → Dict[str, Any]\",\"Build encoder main parameters.\",\"Parameters:\",\"pos_wise_act_type – X-former position-wise feed-forward activation type.\",\"conv_mod_act_type – X-former convolution module activation type.\",\"pos_enc_dropout_rate – Positional encoding dropout rate.\",\"pos_enc_max_len – Positional encoding maximum length.\",\"simplified_att_score – Whether to use simplified attention score computation.\",\"norm_type – X-former normalization module type.\",\"conv_mod_norm_type – Conformer convolution module normalization type.\",\"after_norm_eps – Epsilon value for the final normalization.\",\"after_norm_partial – Value for the final normalization with RMSNorm.\",\"blockdrop_rate – Probability threshold of dropping out each encoder block.\",\"dynamic_chunk_training – Whether to use dynamic chunk training.\",\"short_chunk_threshold – Threshold for dynamic chunk selection.\",\"short_chunk_size – Minimum number of frames during dynamic chunk training.\",\"num_left_chunks – Number of left chunks the attention module can see. (null or negative value means full context)\",\"**activation_parameters – Parameters of the activation functions. (See espnet2/asr_transducer/activation.py)\",\"Returns: Main encoder parameters\"]},\"662\":{\"h\":\"espnet2.asr_transducer.encoder.building.build_positional_encoding\",\"t\":[\"source\",\"espnet2.asr_transducer.encoder.building.build_positional_encoding(block_size: int, configuration: Dict[str, Any]) → RelPositionalEncoding\",\"Build positional encoding block.\",\"Parameters:\",\"block_size – Input/output size.\",\"configuration – Positional encoding configuration.\",\"Returns: Positional encoding module.\"]},\"663\":{\"h\":\"espnet2.asr_transducer.utils.check_short_utt\",\"t\":[\"source\",\"espnet2.asr_transducer.utils.check_short_utt(sub_factor: int, size: int) → Tuple[bool, int]\",\"Check if the input is too short for subsampling.\",\"Parameters:\",\"sub_factor – Subsampling factor for Conv2DSubsampling.\",\"size – Input size.\",\"Returns: Whether an error should be sent. : Size limit for specified subsampling factor.\"]},\"664\":{\"h\":\"espnet2.asr_transducer.activation.get_activation\",\"t\":[\"source\",\"espnet2.asr_transducer.activation.get_activation(activation_type: str, ftswish_threshold: float = -0.2, ftswish_mean_shift: float = 0.0, hardtanh_min_val: int = -1.0, hardtanh_max_val: int = 1.0, leakyrelu_neg_slope: float = 0.01, smish_alpha: float = 1.0, smish_beta: float = 1.0, softplus_beta: float = 1.0, softplus_threshold: int = 20, swish_beta: float = 1.0) → Module\",\"Return activation function.\",\"Parameters:\",\"activation_type – Activation function type.\",\"ftswish_threshold – Threshold value for FTSwish activation formulation.\",\"ftswish_mean_shift – Mean shifting value for FTSwish activation formulation.\",\"hardtanh_min_val – Minimum value of the linear region range for HardTanh.\",\"hardtanh_max_val – Maximum value of the linear region range for HardTanh.\",\"leakyrelu_neg_slope – Negative slope value for LeakyReLU activation formulation.\",\"smish_alpha – Alpha value for Smish activation fomulation.\",\"smish_beta – Beta value for Smish activation formulation.\",\"softplus_beta – Beta value for softplus activation formulation in Mish.\",\"softplus_threshold – Values above this revert to a linear function in Mish.\",\"swish_beta – Beta value for Swish variant formulation.\",\"Returns: Activation function.\"]},\"665\":{\"h\":\"espnet2.asr_transducer.utils.get_convinput_module_parameters\",\"t\":[\"source\",\"espnet2.asr_transducer.utils.get_convinput_module_parameters(input_size: int, last_conv_size, subsampling_factor: int, is_vgg: bool = True) → Tuple[Tuple[int, int] | int, int]\",\"Return the convolution module parameters.\",\"Parameters:\",\"input_size – Module input size.\",\"last_conv_size – Last convolution size for module output size computation.\",\"subsampling_factor – Total subsampling factor.\",\"is_vgg – Whether the module type is VGG-like.\",\"Returns: First MaxPool2D kernel size or second Conv2d kernel size and stride. output_size: Convolution module output size.\"]},\"666\":{\"h\":\"espnet2.asr_transducer.normalization.get_normalization\",\"t\":[\"source\",\"espnet2.asr_transducer.normalization.get_normalization(normalization_type: str, eps: float | None = None, partial: float | None = None) → Tuple[Module, Dict]\",\"Get normalization module and arguments given parameters.\",\"Parameters:\",\"normalization_type – Normalization module type.\",\"eps – Value added to the denominator.\",\"partial – Value defining the part of the input used for RMS stats (RMSNorm).\",\"Returns: Normalization module class : Normalization module arguments\"]},\"667\":{\"h\":\"espnet2.asr_transducer.utils.get_transducer_task_io\",\"t\":[\"source\",\"espnet2.asr_transducer.utils.get_transducer_task_io(labels: Tensor, encoder_out_lens: Tensor, ignore_id: int = -1, blank_id: int = 0) → Tuple[Tensor, Tensor, Tensor, Tensor]\",\"Get Transducer loss I/O.\",\"Parameters:\",\"labels – Label ID sequences. (B, L)\",\"encoder_out_lens – Encoder output lengths. (B,)\",\"ignore_id – Padding symbol ID.\",\"blank_id – Blank symbol ID.\",\"Returns: Decoder inputs. (B, U) target: Target label ID sequences. (B, U) t_len: Time lengths. (B,) u_len: Label lengths. (B,)\",\"Return type: decoder_in\"]},\"668\":{\"h\":\"espnet2.asr_transducer.decoder.modules.rwkv.attention.load_wkv_kernel\",\"t\":[\"source\",\"espnet2.asr_transducer.decoder.modules.rwkv.attention.load_wkv_kernel(context_size: int) → None\",\"Load WKV CUDA kernel.\",\"Parameters:context_size – Context size.\"]},\"669\":{\"h\":\"espnet2.asr_transducer.utils.make_chunk_mask\",\"t\":[\"source\",\"espnet2.asr_transducer.utils.make_chunk_mask(size: int, chunk_size: int, num_left_chunks: int = 0, device: device | None = None) → Tensor\",\"Create chunk mask for the subsequent steps (size, size).\",\"Reference: https://github.com/k2-fsa/icefall/blob/master/icefall/utils.py\",\"Parameters:\",\"size – Size of the source mask.\",\"chunk_size – Number of frames in chunk.\",\"num_left_chunks – Number of left chunks the attention module can see. (null or negative value means full context)\",\"device – Device for the mask tensor.\",\"Returns: Chunk mask. (size, size)\",\"Return type: mask\"]},\"670\":{\"h\":\"espnet2.asr_transducer.utils.make_source_mask\",\"t\":[\"source\",\"espnet2.asr_transducer.utils.make_source_mask(lengths: Tensor) → Tensor\",\"Create source mask for given lengths.\",\"Reference: https://github.com/k2-fsa/icefall/blob/master/icefall/utils.py\",\"Parameters:lengths – Sequence lengths. (B,)\",\"Returns: Mask for the sequence lengths. (B, max_len)\"]},\"671\":{\"h\":\"espnet2.asr_transducer.encoder.validation.validate_architecture\",\"t\":[\"source\",\"espnet2.asr_transducer.encoder.validation.validate_architecture(input_conf: Dict[str, Any], body_conf: List[Dict[str, Any]], input_size: int) → Tuple[int, int]\",\"Validate specified architecture is valid.\",\"Parameters:\",\"input_conf – Encoder input block configuration.\",\"body_conf – Encoder body blocks configuration.\",\"input_size – Encoder input size.\",\"Returns: Encoder input block output size. : Encoder body block output size.\",\"Return type: input_block_osize\"]},\"672\":{\"h\":\"espnet2.asr_transducer.encoder.validation.validate_block_arguments\",\"t\":[\"source\",\"espnet2.asr_transducer.encoder.validation.validate_block_arguments(configuration: Dict[str, Any], block_id: int, previous_block_output: int) → Tuple[int, int]\",\"Validate block arguments.\",\"Parameters:\",\"configuration – Architecture configuration.\",\"block_id – Block ID.\",\"previous_block_output – Previous block output size.\",\"Returns: Block input size. output_size: Block output size.\",\"Return type: input_size\"]},\"673\":{\"h\":\"espnet2.asr_transducer.encoder.validation.validate_input_block\",\"t\":[\"source\",\"espnet2.asr_transducer.encoder.validation.validate_input_block(configuration: Dict[str, Any], body_first_conf: Dict[str, Any], input_size: int) → int\",\"Validate input block.\",\"Parameters:\",\"configuration – Encoder input block configuration.\",\"body_first_conf – Encoder first body block configuration.\",\"input_size – Encoder input block input size.\",\"Returns: Encoder input block output size.\",\"Return type: output_size\"]},\"674\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.AVHubertConfig\",\"t\":[\"source\",\"class espnet2.asr.encoder.avhubert_encoder.AVHubertConfig(sample_rate: int = 16000, label_rate: int = -1, encoder_layers: int = 12, encoder_embed_dim: int = 768, encoder_ffn_embed_dim: int = 3072, encoder_attention_heads: int = 12, activation_fn: str = 'gelu', dropout: float = 0.1, attention_dropout: float = 0.1, activation_dropout: float = 0.0, encoder_layerdrop: float = 0.0, dropout_input: float = 0.0, dropout_features: float = 0.0, final_dim: int = 0, untie_final_proj: bool = False, layer_norm_first: bool = False, conv_feature_layers: str = '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', conv_bias: bool = False, logit_temp: float = 0.1, target_glu: bool = False, feature_grad_mult: float = 1.0, mask_length_audio: int = 10, mask_prob_audio: float = 0.65, mask_length_image: int = 10, mask_prob_image: float = 0.65, mask_selection: str = 'static', mask_other: float = 0, no_mask_overlap: bool = False, mask_min_space: int = 1, mask_channel_length: int = 10, mask_channel_prob: float = 0.0, mask_channel_selection: str = 'static', mask_channel_other: float = 0, no_mask_channel_overlap: bool = False, mask_channel_min_space: int = 1, conv_pos: int = 128, conv_pos_groups: int = 16, latent_temp: Tuple[float, float, float] = (2, 0.5, 0.999995), skip_masked: bool = False, skip_nomask: bool = False, resnet_relu_type: str = 'prelu', resnet_weights: str | None = None, sim_type: str = 'cosine', sub_encoder_layers: int = 0, audio_feat_dim: int = -1, modality_dropout: float = 0, audio_dropout: float = 0, modality_fuse: str = 'concat', selection_type: str = 'same_other_seq', masking_type: str = 'input', decoder_embed_dim: int = 768, decoder_ffn_embed_dim: int = 3072, decoder_layers: int = 6, decoder_layerdrop: float = 0.0, decoder_attention_heads: int = 4, decoder_learned_pos: bool = False, decoder_normalize_before: bool = False, no_token_positional_embeddings: bool = False, decoder_dropout: float = 0.1, decoder_attention_dropout: float = 0.1, decoder_activation_dropout: float = 0.0, max_target_positions: int = 2048, share_decoder_input_output_embed: bool = False, audio_only: bool = False, no_scale_embedding: bool = True)\",\"Bases: object\",\"Configuration from original AVHubert Github\",\"activation_dropout : float= 0.0\",\"activation_fn : str= 'gelu'\",\"attention_dropout : float= 0.1\",\"audio_dropout : float= 0\",\"audio_feat_dim : int= -1\",\"audio_only : bool= False\",\"conv_bias : bool= False\",\"conv_feature_layers : str= '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2'\",\"conv_pos : int= 128\",\"conv_pos_groups : int= 16\",\"decoder_activation_dropout : float= 0.0\",\"decoder_attention_dropout : float= 0.1\",\"decoder_attention_heads : int= 4\",\"decoder_dropout : float= 0.1\",\"decoder_embed_dim : int= 768\",\"decoder_ffn_embed_dim : int= 3072\",\"decoder_layerdrop : float= 0.0\",\"decoder_layers : int= 6\",\"decoder_learned_pos : bool= False\",\"decoder_normalize_before : bool= False\",\"dropout : float= 0.1\",\"dropout_features : float= 0.0\",\"dropout_input : float= 0.0\",\"encoder_attention_heads : int= 12\",\"encoder_embed_dim : int= 768\",\"encoder_ffn_embed_dim : int= 3072\",\"encoder_layerdrop : float= 0.0\",\"encoder_layers : int= 12\",\"feature_grad_mult : float= 1.0\",\"final_dim : int= 0\",\"label_rate : int= -1\",\"latent_temp : Tuple[float, float, float]= (2, 0.5, 0.999995)\",\"layer_norm_first : bool= False\",\"logit_temp : float= 0.1\",\"mask_channel_length : int= 10\",\"mask_channel_min_space : int= 1\",\"mask_channel_other : float= 0\",\"mask_channel_prob : float= 0.0\",\"mask_channel_selection : str= 'static'\",\"mask_length_audio : int= 10\",\"mask_length_image : int= 10\",\"mask_min_space : int= 1\",\"mask_other : float= 0\",\"mask_prob_audio : float= 0.65\",\"mask_prob_image : float= 0.65\",\"mask_selection : str= 'static'\",\"masking_type : str= 'input'\",\"max_target_positions : int= 2048\",\"modality_dropout : float= 0\",\"modality_fuse : str= 'concat'\",\"no_mask_channel_overlap : bool= False\",\"no_mask_overlap : bool= False\",\"no_scale_embedding : bool= True\",\"no_token_positional_embeddings : bool= False\",\"resnet_relu_type : str= 'prelu'\",\"resnet_weights : str | None= None\",\"sample_rate : int= 16000\",\"selection_type : str= 'same_other_seq'\",\"share_decoder_input_output_embed : bool= False\",\"sim_type : str= 'cosine'\",\"skip_masked : bool= False\",\"skip_nomask : bool= False\",\"sub_encoder_layers : int= 0\",\"target_glu : bool= False\",\"untie_final_proj : bool= False\"]},\"675\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.AVHubertModel\",\"t\":[\"source\",\"class espnet2.asr.encoder.avhubert_encoder.AVHubertModel(cfg: AVHubertConfig, **kwargs)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"classmethod build_model(cfg: AVHubertConfig)\",\"Build a new model instance.\",\"extract_finetune(source, padding_mask=None, mask=False, ret_conv=False, output_layer=None)\",\"Forward AVHubert Pretrain Encoder.\",\"Parameters:\",\"source**['video']** – input tensor (B, 1, L, H, W)\",\"source**['audio']** – input tensor (B, F, L)\",\"padding_mask – input tensor (B, L)\",\"Returns: encoded tensor and mask\",\"forward_audio(source_audio)\",\"forward_features(source: Tensor, modality: str) → Tensor\",\"forward_padding_mask(features: Tensor, padding_mask: Tensor) → Tensor\",\"forward_transformer(source, padding_mask=None, output_layer=None)\",\"Forward AVHubert Pretrain Encoder (without frontend).\",\"Assume the source is already fused feature. :param source: input tensor (B, L, D*2) :param padding_mask: input tensor (B, L)\",\"Returns: encoded tensor and mask\",\"forward_video(source_video)\",\"modality_fusion(features_audio, features_video)\"]},\"676\":{\"h\":\"espnet2.asr.decoder.abs_decoder.AbsDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.abs_decoder.AbsDecoder(*args, **kwargs)\",\"Bases: Module, ScorerInterface, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"677\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"678\":{\"h\":\"espnet2.asr.encoder.abs_encoder.AbsEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.abs_encoder.AbsEncoder(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"679\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size() → int\"]},\"680\":{\"h\":\"espnet2.asr.frontend.abs_frontend.AbsFrontend\",\"t\":[\"source\",\"class espnet2.asr.frontend.abs_frontend.AbsFrontend(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"681\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size() → int\"]},\"682\":{\"h\":\"espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder\",\"t\":[\"source\",\"class espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"683\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size() → int\"]},\"684\":{\"h\":\"espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder\",\"t\":[\"source\",\"class espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"685\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size() → int\"]},\"686\":{\"h\":\"espnet2.asr.specaug.abs_specaug.AbsSpecAug\",\"t\":[\"source\",\"class espnet2.asr.specaug.abs_specaug.AbsSpecAug(*args, **kwargs)\",\"Bases: Module\",\"Abstract class for the augmentation of spectrogram\",\"The process-flow:\",\"Frontend -> SpecAug -> Normalization -> Encoder -> Decoder\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, x_lengths: Tensor | None = None) → Tuple[Tensor, Tensor | None]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"687\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"688\":{\"h\":\"espnet2.asr.state_spaces.components.Activation\",\"t\":[\"source\",\"espnet2.asr.state_spaces.components.Activation(activation=None, size=None, dim=-1)\"]},\"689\":{\"h\":\"espnet2.asr.state_spaces.residual.Affine\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.residual.Affine(*args, scalar=True, gamma=0.0, **kwargs)\",\"Bases: Residual\",\"Residual connection with learnable scalar multipliers on the main branch.\",\"scalar: Single scalar multiplier, or one per dimension scale, power: Initialize to scale * layer_num**(-power)\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y, transposed)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"690\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"691\":{\"h\":\"espnet2.asr.frontend.asteroid_frontend.AsteroidFrontend\",\"t\":[\"source\",\"class espnet2.asr.frontend.asteroid_frontend.AsteroidFrontend(sinc_filters: int = 256, sinc_kernel_size: int = 251, sinc_stride: int = 16, preemph_coef: float = 0.97, log_term: float = 1e-06)\",\"Bases: AbsFrontend\",\"Asteroid Filterbank Frontend.\",\"Provides a Sinc-convolutional-based audio feature extractor. The same function can be achieved by using sliding_winodw frontend + sinc preencoder.\",\"NOTE(jiatong): this function is used in sentence-level classification tasks (e.g., spk). Other usages are not fully investigated.\",\"NOTE(jeeweon): this function implements the parameterized analytic filterbank layer in M. Pariente, S. Cornell, A. Deleforge and E. Vincent, “Filterbank design for end-to-end speech separation,” in Proc. ICASSP, 2020\",\"Initialize.\",\"Parameters:\",\"sinc_filters – the filter numbers for sinc.\",\"sinc_kernel_size – the kernel size for sinc.\",\"sinc_stride – the sincstride size of the first sinc-conv layer where it decides the compression rate (Hz).\",\"preemph_coef – the coeifficient for preempahsis.\",\"log_term – the log term to prevent infinity.\",\"forward(input: Tensor, input_length: Tensor) → Tuple[Tensor, Tensor]\",\"Apply the Asteroid filterbank frontend to the input.\",\"Parameters:\",\"input – Input (B, T).\",\"input_length – Input length (B,).\",\"Returns: Frame-wise output (B, T’, D).\",\"Return type: Tensor\",\"output_size() → int\",\"Return output length of feature dimension D.\"]},\"692\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder(vocab_size: int, encoder_output_size: int, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, gradient_checkpoint_layers: ~typing.List[int] = [])\",\"Bases: AbsDecoder, BatchScorerInterface, MaskParallelScorerInterface\",\"Base class of Transfomer decoder module.\",\"Parameters:\",\"vocab_size – output dim\",\"encoder_output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of decoder blocks\",\"dropout_rate – dropout rate\",\"self_attention_dropout_rate – dropout rate for attention\",\"input_layer – input layer type\",\"use_output_layer – whether to use output layer\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor, return_hs: bool = False) → Tuple[Tensor, List[Any]]\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"batch_score_partially_AR(ys: Tensor, states: List[Any], xs: Tensor, yseq_lengths: Tensor) → Tuple[Tensor, List[Any]]\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor, return_hs: bool = False, return_all_hs: bool = False) → Tuple[Tensor, Tensor]\",\"Forward decoder.\",\"Parameters:\",\"hs_pad – encoded memory, float32 (batch, maxlen_in, feat)\",\"hlens – (batch)\",\"ys_in_pad – input token ids, int64 (batch, maxlen_out) if input_layer == “embed” input tensor (batch, maxlen_out, #mels) in the other cases\",\"ys_in_lens – (batch)\",\"return_hs – (bool) whether to return the last hidden output before output layer\",\"return_all_hs – (bool) whether to return all the hidden intermediates\",\"Returns: tuple containing:\",\"x: decoded token score before softmax (batch, maxlen_out, token) : if use_output_layer is True,\",\"olens: (batch, )\",\"Return type: (tuple)\",\"forward_one_step(tgt: Tensor, tgt_mask: Tensor, memory: Tensor, memory_mask: Tensor | None = None, *, cache: List[Tensor] | None = None, return_hs: bool = False) → Tuple[Tensor, List[Tensor]]\",\"Forward one step.\",\"Parameters:\",\"tgt – input token ids, int64 (batch, maxlen_out)\",\"tgt_mask – input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2)\",\"memory – encoded memory, float32 (batch, maxlen_in, feat)\",\"memory_mask – encoded memory mask (batch, 1, maxlen_in)\",\"cache – cached output list of (batch, max_time_out-1, size)\",\"return_hs – dec hidden state corresponding to ys, used for searchable hidden ints\",\"Returns: NN output value and cache per self.decoders. y.shape` is (batch, maxlen_out, token)\",\"Return type: y, cache\",\"forward_partially_AR(tgt: Tensor, tgt_mask: Tensor, tgt_lengths: Tensor, memory: Tensor, cache: List[Tensor] | None = None) → Tuple[Tensor, List[Tensor]]\",\"Forward one step.\",\"Parameters:\",\"tgt – input token ids, int64 (n_mask * n_beam, maxlen_out)\",\"tgt_mask – input token mask, (n_mask * n_beam, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2)\",\"tgt_lengths – (n_mask * n_beam, )\",\"memory – encoded memory, float32 (batch, maxlen_in, feat)\",\"cache – cached output list of (batch, max_time_out-1, size)\",\"Returns: NN output value and cache per self.decoders. y.shape` is (batch, maxlen_out, token)\",\"Return type: y, cache\",\"score(ys, state, x, return_hs=False)\",\"Score.\"]},\"693\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.BasicBlock\",\"t\":[\"source\",\"class espnet2.asr.encoder.avhubert_encoder.BasicBlock(inplanes, planes, stride=1, downsample=None, relu_type='relu')\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"expansion = 1\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"694\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"695\":{\"h\":\"espnet2.asr.bayes_risk_ctc.BayesRiskCTC\",\"t\":[\"source\"]},\"696\":{\"h\":\"espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer\",\"t\":[\"source\",\"class espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer(decoder: AbsDecoder, joint_network: JointNetwork, beam_size: int, lm: Module | None = None, lm_weight: float = 0.1, search_type: str = 'default', max_sym_exp: int = 2, u_max: int = 50, nstep: int = 1, prefix_alpha: int = 1, expansion_gamma: int = 2.3, expansion_beta: int = 2, multi_blank_durations: List[int] = [], multi_blank_indices: List[int] = [], score_norm: bool = True, score_norm_during: bool = False, nbest: int = 1, token_list: List[str] | None = None)\",\"Bases: object\",\"Beam search implementation for Transducer.\",\"Initialize Transducer search module.\",\"Parameters:\",\"decoder – Decoder module.\",\"joint_network – Joint network module.\",\"beam_size – Beam size.\",\"lm – LM class.\",\"lm_weight – LM weight for soft fusion.\",\"search_type – Search algorithm to use during inference.\",\"max_sym_exp – Number of maximum symbol expansions at each time step. (TSD)\",\"u_max – Maximum output sequence length. (ALSD)\",\"nstep – Number of maximum expansion steps at each time step. (NSC/mAES)\",\"prefix_alpha – Maximum prefix length in prefix search. (NSC/mAES)\",\"expansion_beta – Number of additional candidates for expanded hypotheses selection. (mAES)\",\"expansion_gamma – Allowed logp difference for prune-by-value method. (mAES)\",\"multi_blank_durations – The duration of each blank token. (MBG)\",\"multi_blank_indices – The index of each blank token in token_list. (MBG)\",\"score_norm – Normalize final scores by length. (“default”)\",\"score_norm_during – Normalize scores by length during search. (default, TSD, ALSD)\",\"nbest – Number of final hypothesis.\",\"align_length_sync_decoding(enc_out: Tensor) → List[Hypothesis]\",\"Alignment-length synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:h – Encoder output sequences. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"default_beam_search(enc_out: Tensor) → List[Hypothesis]\",\"Beam search implementation.\",\"Modified from https://arxiv.org/pdf/1211.3711.pdf\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"greedy_search(enc_out: Tensor) → List[Hypothesis]\",\"Greedy search implementation.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: 1-best hypotheses.\",\"Return type: hyp\",\"modified_adaptive_expansion_search(enc_out: Tensor) → List[ExtendedHypothesis]\",\"It’s the modified Adaptive Expansion Search (mAES) implementation.\",\"Based on/modified from https://ieeexplore.ieee.org/document/9250505 and NSC.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"multi_blank_greedy_search(enc_out: Tensor) → List[Hypothesis]\",\"Greedy Search for Multi-Blank Transducer (Multi-Blank Greedy, MBG).\",\"In this implementation, we assume:\",\"the index of standard blank is the last entry of self.multi_blank_indices\",\"rather than self.blank_id (to avoid too much change on original transducer)\",\"other entries in self.multi_blank_indices are big blanks that account for multiple frames.\",\"Based on https://arxiv.org/abs/2211.03541\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: 1-best hypothesis.\",\"Return type: hyp\",\"nsc_beam_search(enc_out: Tensor) → List[ExtendedHypothesis]\",\"N-step constrained beam search implementation.\",\"Based on/Modified from https://arxiv.org/pdf/2002.03577.pdf. Please reference ESPnet (b-flo, PR #2444) for any usage outside ESPnet until further modifications.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"prefix_search(hyps: List[ExtendedHypothesis], enc_out_t: Tensor) → List[ExtendedHypothesis]\",\"Prefix search for NSC and mAES strategies.\",\"Based on https://arxiv.org/pdf/1211.3711.pdf\",\"sort_nbest(hyps: List[Hypothesis] | List[ExtendedHypothesis]) → List[Hypothesis] | List[ExtendedHypothesis]\",\"Sort hypotheses by score or score given sequence length.\",\"Parameters:hyps – Hypothesis.\",\"Returns: Sorted hypothesis.\",\"Return type: hyps\",\"time_sync_decoding(enc_out: Tensor) → List[Hypothesis]\",\"Time synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\"]},\"697\":{\"h\":\"espnet2.asr.transducer.beam_search_transducer_streaming.BeamSearchTransducerStreaming\",\"t\":[\"source\",\"class espnet2.asr.transducer.beam_search_transducer_streaming.BeamSearchTransducerStreaming(decoder: AbsDecoder, joint_network: JointNetwork, beam_size: int, lm: Module | None = None, lm_weight: float = 0.1, search_type: str = 'default', max_sym_exp: int = 2, u_max: int = 50, nstep: int = 1, prefix_alpha: int = 1, expansion_gamma: int = 2.3, expansion_beta: int = 2, score_norm: bool = True, score_norm_during: bool = False, nbest: int = 1, penalty: float = 0.0, token_list: List[str] | None = None, hold_n: int = 0)\",\"Bases: object\",\"Beam search implementation for Transducer.\",\"Initialize Transducer search module.\",\"Parameters:\",\"decoder – Decoder module.\",\"joint_network – Joint network module.\",\"beam_size – Beam size.\",\"lm – LM class.\",\"lm_weight – LM weight for soft fusion.\",\"search_type – Search algorithm to use during inference.\",\"max_sym_exp – Number of maximum symbol expansions at each time step. (TSD)\",\"u_max – Maximum output sequence length. (ALSD)\",\"nstep – Number of maximum expansion steps at each time step. (NSC/mAES)\",\"prefix_alpha – Maximum prefix length in prefix search. (NSC/mAES)\",\"expansion_beta – Number of additional candidates for expanded hypotheses selection. (mAES)\",\"expansion_gamma – Allowed logp difference for prune-by-value method. (mAES)\",\"score_norm – Normalize final scores by length. (“default”)\",\"score_norm_during – Normalize scores by length during search. (default, TSD, ALSD)\",\"nbest – Number of final hypothesis.\",\"align_length_sync_decoding(enc_out: Tensor) → List[Hypothesis]\",\"Alignment-length synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:h – Encoder output sequences. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"default_beam_search(enc_out: Tensor) → List[Hypothesis]\",\"Beam search implementation.\",\"Modified from https://arxiv.org/pdf/1211.3711.pdf\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"greedy_search(enc_out: Tensor) → List[Hypothesis]\",\"Greedy search implementation.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: 1-best hypotheses.\",\"Return type: hyp\",\"modified_adaptive_expansion_search(enc_out: Tensor) → List[ExtendedHypothesis]\",\"It’s the modified Adaptive Expansion Search (mAES) implementation.\",\"Based on/modified from https://ieeexplore.ieee.org/document/9250505 and NSC.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"nsc_beam_search(enc_out: Tensor) → List[ExtendedHypothesis]\",\"N-step constrained beam search implementation.\",\"Based on/Modified from https://arxiv.org/pdf/2002.03577.pdf. Please reference ESPnet (b-flo, PR #2444) for any usage outside ESPnet until further modifications.\",\"Parameters:enc_out – Encoder output sequence. (T, D_enc)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\",\"prefix_search(hyps: List[ExtendedHypothesis], enc_out_t: Tensor) → List[ExtendedHypothesis]\",\"Prefix search for NSC and mAES strategies.\",\"Based on https://arxiv.org/pdf/1211.3711.pdf\",\"reset()\",\"sort_nbest(hyps: List[Hypothesis] | List[ExtendedHypothesis]) → List[Hypothesis] | List[ExtendedHypothesis]\",\"Sort hypotheses by score or score given sequence length.\",\"Parameters:hyps – Hypothesis.\",\"Returns: Sorted hypothesis.\",\"Return type: hyps\",\"time_sync_decoding(enc_out: Tensor) → List[Hypothesis]\",\"Time synchronous beam search implementation.\",\"Based on https://ieeexplore.ieee.org/document/9053040\",\"Parameters:enc_out – Encoder output sequence. (T, D)\",\"Returns: N-best hypothesis.\",\"Return type: nbest_hyps\"]},\"698\":{\"h\":\"espnet2.asr.encoder.beats_encoder.BeatsConfig\",\"t\":[\"source\",\"class espnet2.asr.encoder.beats_encoder.BeatsConfig(cfg=None)\",\"Bases: object\",\"update(cfg: dict)\"]},\"699\":{\"h\":\"espnet2.asr.encoder.beats_encoder.BeatsEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.beats_encoder.BeatsEncoder(input_size: int, beats_ckpt_path: str | None = None, max_layer: int | None = None, downsampling_rate: int = 1, adapter_config: str = '', use_weighted_representation: bool = False, beats_config: Dict | None = None, specaug_config: Dict | None = None, add_positional_information: bool = False, max_positions: int | None = None, fbank_mean: float = 15.41663, fbank_std: float = 6.55582, roll_augment: bool = False, roll_interval: int = 1600)\",\"Bases: AbsEncoder\",\"BEATs: Audio Pre-Training with Acoustic Tokenizers.\",\"(https://arxiv.org/abs/2212.09058) :param beats_ckpt_path: Path to a pretrained Beats checkpoint. If\",\"beats_config is provided and it does not match the config in the checkpoint, code might throw an error.\",\"Parameters:\",\"max_layer – Propagate input through all layers for encoding if None. Otherwise use upto max_layer.\",\"downsampling_rate – Downsampling rate for the encoder. Applied if > 1.\",\"adapter_config – Path to a config file for the wav2vec2 adapter.\",\"use_weighted_representation – Use weighted representations from max_layer if True. Weights are randomly initialized.\",\"beats_config – BeatsConfig object. If provided, we will try to override the config in the checkpoint. This can be used to change dropouts etc for fine-tuning the model while starting from a pretrained checkpoint.\",\"specaug_config – Dictionary containing parameters for SpecAugment. If provided, SpecAugment will be applied.\",\"add_positional_information – Add learned positional embeddings.\",\"max_positions – Maximum number of positions for positional embeddings. Required if add_positional_information is True.\",\"roll_augment – Apply roll augmentation to the input.\",\"roll_interval – Interval for roll augmentation. All rolling is quantized to this interval.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extract_features(source: Tensor, padding_mask: Tensor | None = None, max_layer: int | None = None)\",\"Extract features from raw audio.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Wrapper for compatibility with ESPnets’ AbsEncoder Interface.\",\"Parameters:\",\"xs_pad – (B, T)\",\"ilens – (B,)\",\"prev_states – None\",\"Returns: (B, T, D) output_lens: (B,) masks: None\",\"Return type: audio_representation\",\"forward_padding_mask(features: Tensor, padding_mask: Tensor) → Tensor\",\"Forward padding mask. Featuires: BTC, padding_mask: BT.\",\"output_size() → int\",\"preprocess(source: Tensor) → Tensor\",\"Preprocess raw audio.\",\"reload_pretrained_parameters()\",\"Initialization function for Beats.\",\"This must be called last in the initialization procedure. The initialization occurs in three steps:\",\"ESPnet initializes all modules.\",\"This function initializes Beats encoder overriding 1.\",\"Optionally, if we have the pretrained checkpoint, we load the\",\"weights from the checkpoint overriding 2 and 1.\"]},\"700\":{\"h\":\"espnet2.asr.encoder.branchformer_encoder.BranchformerEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.branchformer_encoder.BranchformerEncoder(input_size: int, output_size: int = 256, use_attn: bool = True, attention_heads: int = 4, attention_layer_type: str = 'rel_selfattn', pos_enc_layer_type: str = 'rel_pos', rel_pos_type: str = 'latest', use_cgmlp: bool = True, cgmlp_linear_units: int = 2048, cgmlp_conv_kernel: int = 31, use_linear_after_conv: bool = False, gate_activation: str = 'identity', merge_method: str = 'concat', cgmlp_weight: float | List[float] = 0.5, attn_branch_drop_rate: float | List[float] = 0.0, num_blocks: int = 12, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', zero_triu: bool = False, padding_idx: int = -1, stochastic_depth_rate: float | List[float] = 0.0, qk_norm: bool = False, use_flash_attn: bool = True)\",\"Bases: AbsEncoder\",\"Branchformer encoder module.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Calculate forward propagation.\",\"Parameters:\",\"xs_pad (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"ilens (torch.Tensor) – Input length (#batch).\",\"prev_states (torch.Tensor) – Not to be used now.\",\"Returns: Output tensor (#batch, L, output_size). torch.Tensor: Output length (#batch). torch.Tensor: Not to be used now.\",\"Return type: torch.Tensor\",\"output_size() → int\"]},\"701\":{\"h\":\"espnet2.asr.encoder.branchformer_encoder.BranchformerEncoderLayer\",\"t\":[\"source\",\"class espnet2.asr.encoder.branchformer_encoder.BranchformerEncoderLayer(size: int, attn: Module | None, cgmlp: Module | None, dropout_rate: float, merge_method: str, cgmlp_weight: float = 0.5, attn_branch_drop_rate: float = 0.0, stochastic_depth_rate: float = 0.0)\",\"Bases: Module\",\"Branchformer encoder layer module.\",\"Parameters:\",\"size (int) – model dimension\",\"attn – standard self-attention or efficient attention, optional\",\"cgmlp – ConvolutionalGatingMLP, optional\",\"dropout_rate (float) – dropout probability\",\"merge_method (str) – concat, learned_ave, fixed_ave\",\"cgmlp_weight (float) – weight of the cgmlp branch, between 0 and 1, used if merge_method is fixed_ave\",\"attn_branch_drop_rate (float) – probability of dropping the attn branch, used if merge_method is learned_ave\",\"stochastic_depth_rate (float) – stochastic depth probability\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x_input, mask, cache=None)\",\"Compute encoded features.\",\"Parameters:\",\"x_input (Union *[*Tuple,torch.Tensor]) – Input tensor w/ or w/o pos emb. \",\"w/ pos emb: Tuple of tensors [(#batch, time, size), (1, time, size)].\",\"w/o pos emb: Tensor (#batch, time, size).\",\"mask (torch.Tensor) – Mask tensor for the input (#batch, 1, time).\",\"cache (torch.Tensor) – Cache tensor of the input (#batch, time - 1, size).\",\"Returns: Output tensor (#batch, time, size). torch.Tensor: Mask tensor (#batch, time).\",\"Return type: torch.Tensor\"]},\"702\":{\"h\":\"espnet2.asr.frontend.cnn.CNNFrontend\",\"t\":[\"source\",\"class espnet2.asr.frontend.cnn.CNNFrontend(norm_mode: str, conv_mode: str, bias: bool, shapes: List[Tuple[int, int, int]] = [(512, 10, 5), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 2, 2), (512, 2, 2)], fs: int | str = 16000, normalize_audio: bool = False, normalize_output: bool = False, layer_norm_cls: Literal['transposed', 'dim1'] = 'transposed')\",\"Bases: AbsFrontend\",\"Convolutional feature extractor.\",\"Typically used in SSL models. Uses raw waveforms as input.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, length: Tensor | None) → Tuple[Tensor, Tensor | None]\",\"CNNFrontend Forward.\",\"Parameters:\",\"x (Tensor) – Input Tensor representing a batch of audio, shape: [batch, time].\",\"length (TensororNone,optional) – Valid length of each input sample. shape: [batch, ].\",\"Returns: The resulting feature, shape: [batch, frame, feature] Optional[Tensor]: \",\"Valid length of each output sample. shape: [batch, ].\",\"Return type: Tensor\",\"output_size() → int\"]},\"703\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.CPURNNT\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.CPURNNT(minibatch: int, maxT: int, maxU: int, alphabet_size: int, workspace: Tensor, blank: int, fastemit_lambda: float, clamp: float, num_threads: int, batch_first: bool)\",\"Bases: object\",\"Helper class to compute the Transducer Loss on CPU.\",\"Parameters:\",\"minibatch – Size of the minibatch b.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"workspace – An allocated chunk of memory that will be sliced off and reshaped into required blocks used as working memory.\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"num_threads – Number of OMP threads to launch.\",\"batch_first – Bool that decides if batch dimension is first or third.\",\"compute_alphas(log_probs: Tensor, T: int, U: int, alphas: Tensor)\",\"Compute the probability of the forward variable alpha.\",\"Parameters:\",\"log_probs – Flattened tensor [B, T, U, V+1]\",\"T – Length of the acoustic sequence T (not padded).\",\"U – Length of the target sequence U (not padded).\",\"alphas – Working space memory for alpha of shape [B, T, U].\",\"Returns: Loglikelihood of the forward variable alpha.\",\"compute_betas_and_grads(grad: Tensor, log_probs: Tensor, T: int, U: int, alphas: Tensor, betas: Tensor, labels: Tensor, logll: Tensor)\",\"Compute backward variable beta as well as gradients of the activation\",\"matrix wrt loglikelihood of forward variable.\",\"Parameters:\",\"grad – Working space memory of flattened shape [B, T, U, V+1]\",\"log_probs – Activatio tensor of flattented shape [B, T, U, V+1]\",\"T – Length of the acoustic sequence T (not padded).\",\"U – Length of the target sequence U (not padded).\",\"alphas – Working space memory for alpha of shape [B, T, U].\",\"betas – Working space memory for alpha of shape [B, T, U].\",\"labels – Ground truth label of shape [B, U]\",\"logll – Loglikelihood of the forward variable.\",\"Returns: Loglikelihood of the forward variable and inplace updates the grad tensor.\",\"cost_and_grad(log_probs: Tensor, grads: Tensor, costs: Tensor, flat_labels: Tensor, label_lengths: Tensor, input_lengths: Tensor) → RNNTStatus\",\"cost_and_grad_kernel(log_probs: Tensor, grad: Tensor, labels: Tensor, mb: int, T: int, U: int, bytes_used: int)\",\"score_forward(log_probs: Tensor, costs: Tensor, flat_labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\"]},\"704\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.CTAReduce\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.CTAReduce(tid: int, x, storage, count: int, R_opid: int)\",\"CUDA Warp reduction kernel.\",\"It is a device kernel to be called by other kernels.\",\"The data will be read from the right segement recursively, and reduced (ROP) onto the left half. Operation continues while warp size is larger than a given offset. Beyond this offset, warp reduction is performed via shfl_down_sync, which halves the reduction space and sums the two halves at each call.\"]},\"705\":{\"h\":\"NOTE\",\"t\":[\"Efficient warp occurs at input shapes of 2 ^ K.\",\"References\",\"Warp Primitives [https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/]\",\"Parameters:\",\"tid – CUDA thread index\",\"x – activation. Single float.\",\"storage – shared memory of size CTA_REDUCE_SIZE used for reduction in parallel threads.\",\"count – equivalent to num_rows, which is equivalent to alphabet_size (V+1)\",\"R_opid – Operator ID for reduction. See R_Op for more information.\"]},\"706\":{\"h\":\"espnet2.asr.ctc.CTC\",\"t\":[\"source\",\"class espnet2.asr.ctc.CTC(odim: int, encoder_output_size: int, dropout_rate: float = 0.0, ctc_type: str = 'builtin', reduce: bool = True, ignore_nan_grad: bool | None = None, zero_infinity: bool = True, brctc_risk_strategy: str = 'exp', brctc_group_strategy: str = 'end', brctc_risk_factor: float = 0.0)\",\"Bases: Module\",\"CTC module.\",\"Parameters:\",\"odim – dimension of outputs\",\"encoder_output_size – number of encoder projection units\",\"dropout_rate – dropout rate (0.0 ~ 1.0)\",\"ctc_type – builtin or gtnctc\",\"reduce – reduce the CTC loss into a scalar\",\"ignore_nan_grad – Same as zero_infinity (keeping for backward compatiblity)\",\"zero_infinity – Whether to zero infinite losses and the associated gradients.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"argmax(hs_pad)\",\"argmax of frame activations\",\"Parameters:hs_pad (torch.Tensor) – 3d tensor (B, Tmax, eprojs)\",\"Returns: argmax applied 2d tensor (B, Tmax)\",\"Return type: torch.Tensor\",\"forced_align(hs_pad, hlens, ys_pad, ys_lens, blank_idx=0)\",\"Force alignment between input and target sequences (Viterbi path).\",\"Parameters:\",\"hs_pad – batch of padded hidden state sequences (B, Tmax, D)\",\"hlens – batch of lengths of hidden state sequences (B)\",\"ys_pad – batch of padded character id sequence tensor (B, Lmax)\",\"ys_lens – batch of lengths of character sequence (B)\",\"blank_idx – index of blank symbol\",\"Note – B must be 1.\",\"Returns: Tuple(tensor, tensor): : - Label for each time step in the alignment path computed <br/> using forced alignment. \",\"Log probability scores of the labels for each time step.\",\"Return type: alignments\",\"forward(hs_pad, hlens, ys_pad, ys_lens)\",\"Calculate CTC loss.\",\"Parameters:\",\"hs_pad – batch of padded hidden state sequences (B, Tmax, D)\",\"hlens – batch of lengths of hidden state sequences (B)\",\"ys_pad – batch of padded character id sequence tensor (B, Lmax)\",\"ys_lens – batch of lengths of character sequence (B)\",\"log_softmax(hs_pad)\",\"log_softmax of frame activations\",\"Parameters:hs_pad (Tensor) – 3d tensor (B, Tmax, eprojs)\",\"Returns: log softmax applied 3d tensor (B, Tmax, odim)\",\"Return type: torch.Tensor\",\"loss_fn(th_pred, th_target, th_ilen, th_olen) → Tensor\",\"softmax(hs_pad)\",\"softmax of frame activations\",\"Parameters:hs_pad (Tensor) – 3d tensor (B, Tmax, eprojs)\",\"Returns: softmax applied 3d tensor (B, Tmax, odim)\",\"Return type: torch.Tensor\"]},\"707\":{\"h\":\"espnet2.asr.state_spaces.cauchy.CauchyMultiply\",\"t\":[\"source\"]},\"708\":{\"h\":\"espnet2.asr.state_spaces.cauchy.CauchyMultiplySymmetric\",\"t\":[\"source\"]},\"709\":{\"h\":\"espnet2.asr.encoder.conformer_encoder.ConformerEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.conformer_encoder.ConformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'rel_pos', selfattention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, padding_idx: int = -1, interctc_layer_idx: List[int] = [], interctc_use_conditioning: bool = False, ctc_trim: bool = False, stochastic_depth_rate: float | List[float] = 0.0, layer_drop_rate: float = 0.0, max_pos_emb_len: int = 5000, qk_norm: bool = False, use_flash_attn: bool = True)\",\"Bases: AbsEncoder\",\"Conformer encoder module.\",\"Parameters:\",\"input_size (int) – Input dimension.\",\"output_size (int) – Dimension of attention.\",\"attention_heads (int) – The number of heads of multi head attention.\",\"linear_units (int) – The number of units of position-wise feed forward.\",\"num_blocks (int) – The number of decoder blocks.\",\"dropout_rate (float) – Dropout rate.\",\"attention_dropout_rate (float) – Dropout rate in attention.\",\"positional_dropout_rate (float) – Dropout rate after adding positional encoding.\",\"input_layer (Union *[*str,torch.nn.Module]) – Input layer type.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. If True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) If False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type (str) – “linear”, “conv1d”, or “conv1d-linear”.\",\"positionwise_conv_kernel_size (int) – Kernel size of positionwise conv1d layer.\",\"rel_pos_type (str) – Whether to use the latest relative positional encoding or the legacy one. The legacy relative positional encoding will be deprecated in the future. More Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"encoder_pos_enc_layer_type (str) – Encoder positional encoding layer type.\",\"encoder_attn_layer_type (str) – Encoder attention layer type.\",\"activation_type (str) – Encoder activation function type.\",\"macaron_style (bool) – Whether to use macaron style for positionwise layer.\",\"use_cnn_module (bool) – Whether to use convolution module.\",\"zero_triu (bool) – Whether to zero the upper triangular part of attention matrix.\",\"cnn_module_kernel (int) – Kernerl size of convolution module.\",\"padding_idx (int) – Padding idx for input_layer=embed.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, masks: Tensor | None = None, ctc: CTC | None = None, return_all_hs: bool = False) → Tuple[Tensor, Tensor, Tensor | None]\",\"Calculate forward propagation.\",\"Parameters:\",\"xs_pad (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"ilens (torch.Tensor) – Input length (#batch).\",\"prev_states (torch.Tensor) – Not to be used now.\",\"ctc (CTC) – ctc module for intermediate CTC loss\",\"return_all_hs (bool) – whether to return all hidden states\",\"Returns: Output tensor (#batch, L, output_size). torch.Tensor: Output length (#batch). torch.Tensor: Not to be used now.\",\"Return type: torch.Tensor\",\"output_size() → int\"]},\"710\":{\"h\":\"espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding'>, selfattention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, cnn_module_kernel: int = 31, padding_idx: int = -1, block_size: int = 40, hop_size: int = 16, look_ahead: int = 16, init_average: bool = True, ctx_pos_enc: bool = True)\",\"Bases: AbsEncoder\",\"Contextual Block Conformer encoder module.\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of decoder blocks\",\"dropout_rate – dropout rate\",\"attention_dropout_rate – dropout rate in attention\",\"positional_dropout_rate – dropout rate after adding positional encoding\",\"input_layer – input layer type\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type – linear of conv1d\",\"positionwise_conv_kernel_size – kernel size of positionwise conv1d layer\",\"padding_idx – padding_idx for input_layer=embed\",\"block_size – block size for contextual block processing\",\"hop_Size – hop size for block processing\",\"look_ahead – look-ahead size for block_processing\",\"init_average – whether to use average as initial context (otherwise max values)\",\"ctx_pos_enc – whether to use positional encoding to the context vectors\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, is_final=True, infer_mode=False) → Tuple[Tensor, Tensor, Tensor | None]\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"infer_mode – whether to be used for inference. This is used to distinguish between forward_train (train and validate) and forward_infer (decode).\",\"Returns: position embedded tensor and mask\",\"forward_infer(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, is_final: bool = True) → Tuple[Tensor, Tensor, Tensor | None]\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"forward_train(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size() → int\"]},\"711\":{\"h\":\"espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, padding_idx: int = -1, block_size: int = 40, hop_size: int = 16, look_ahead: int = 16, init_average: bool = True, ctx_pos_enc: bool = True)\",\"Bases: AbsEncoder\",\"Contextual Block Transformer encoder module.\",\"Details in Tsunoo et al. “Transformer ASR with contextual block processing” (https://arxiv.org/abs/1910.07204)\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of encoder blocks\",\"dropout_rate – dropout rate\",\"attention_dropout_rate – dropout rate in attention\",\"positional_dropout_rate – dropout rate after adding positional encoding\",\"input_layer – input layer type\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type – linear of conv1d\",\"positionwise_conv_kernel_size – kernel size of positionwise conv1d layer\",\"padding_idx – padding_idx for input_layer=embed\",\"block_size – block size for contextual block processing\",\"hop_Size – hop size for block processing\",\"look_ahead – look-ahead size for block_processing\",\"init_average – whether to use average as initial context (otherwise max values)\",\"ctx_pos_enc – whether to use positional encoding to the context vectors\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, is_final=True, infer_mode=False) → Tuple[Tensor, Tensor, Tensor | None]\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"infer_mode – whether to be used for inference. This is used to distinguish between forward_train (train and validate) and forward_infer (decode).\",\"Returns: position embedded tensor and mask\",\"forward_infer(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, is_final: bool = True) → Tuple[Tensor, Tensor, Tensor | None]\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"forward_train(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size() → int\"]},\"712\":{\"h\":\"espnet2.asr.frontend.cnn.ConvLayerBlock\",\"t\":[\"source\",\"class espnet2.asr.frontend.cnn.ConvLayerBlock(in_channels: int, out_channels: int, kernel_size: int, stride: int, bias: bool, layer_norm: Module | None, conv_mode: str)\",\"Bases: Module\",\"Convolution unit of FeatureExtractor\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, length: Tensor | None) → Tuple[Tensor, Tensor | None]\",\"ConvLayerBlock Forward.\",\"Parameters:\",\"x (Tensor) – Shape: [batch, in_channels, in_frame].\",\"length (TensororNone,optional) – Shape [batch, ].\",\"Returns: Shape [batch, out_channels, out_frames]. Optional[Tensor]: Shape [batch, ].\",\"Return type: Tensor\"]},\"713\":{\"h\":\"espnet2.asr.layers.cgmlp.ConvolutionalGatingMLP\",\"t\":[\"source\",\"class espnet2.asr.layers.cgmlp.ConvolutionalGatingMLP(size: int, linear_units: int, kernel_size: int, dropout_rate: float, use_linear_after_conv: bool, gate_activation: str)\",\"Bases: Module\",\"Convolutional Gating MLP (cgMLP).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, mask)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"714\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"715\":{\"h\":\"espnet2.asr.layers.cgmlp.ConvolutionalSpatialGatingUnit\",\"t\":[\"source\",\"class espnet2.asr.layers.cgmlp.ConvolutionalSpatialGatingUnit(size: int, kernel_size: int, dropout_rate: float, use_linear_after_conv: bool, gate_activation: str)\",\"Bases: Module\",\"Convolutional Spatial Gating Unit (CSGU).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"espnet_initialization_fn()\",\"forward(x, gate_add=None)\",\"Forward method\",\"Parameters:\",\"x (torch.Tensor) – (N, T, D)\",\"gate_add (torch.Tensor) – (N, T, D/2)\",\"Returns: (N, T, D/2)\",\"Return type: out (torch.Tensor)\"]},\"716\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.CpuRNNT_index\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.CpuRNNT_index(U: int, maxU: int, minibatch: int, alphabet_size: int, batch_first: bool)\",\"Bases: object\",\"A placeholder Index computation class that emits the resolved index in a\",\"flattened tensor, mimicing pointer indexing in CUDA kernels on the CPU.\",\"Parameters:\",\"U – Length of the current target sample (without padding).\",\"maxU – Max Length of the padded target samples.\",\"minibatch – Minibatch index\",\"alphabet_size – Size of the vocabulary including RNNT blank - V+1.\",\"batch_first – Bool flag determining if batch index is first or third.\"]},\"717\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.CpuRNNT_metadata\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.CpuRNNT_metadata(T: int, U: int, workspace: Tensor, bytes_used: int, blank: int, labels: Tensor, log_probs: Tensor, idx: CpuRNNT_index)\",\"Bases: object\",\"Metadata for CPU based RNNT loss calculation. Holds the working space memory.\",\"Parameters:\",\"T – Length of the acoustic sequence (without padding).\",\"U – Length of the target sequence (without padding).\",\"workspace – Working space memory for the CPU.\",\"bytes_used – Number of bytes currently used for indexing the working space memory. Generally 0.\",\"blank – Index of the blank token in the vocabulary.\",\"labels – Ground truth padded labels matrix of shape [B, U]\",\"log_probs – Log probs / activation matrix of flattented shape [B, T, U, V+1]\",\"idx\",\"setup_probs(T: int, U: int, labels: Tensor, blank: int, log_probs: Tensor, idx: CpuRNNT_index)\"]},\"718\":{\"h\":\"espnet2.asr.state_spaces.residual.DecayResidual\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.residual.DecayResidual(*args, power=0.5, l2=True)\",\"Bases: Residual\",\"Residual connection that can decay the linear combination depending on depth.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y, transposed)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"719\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"720\":{\"h\":\"espnet2.asr.frontend.default.DefaultFrontend\",\"t\":[\"source\",\"class espnet2.asr.frontend.default.DefaultFrontend(fs: int | str = 16000, n_fft: int = 512, win_length: int | None = None, hop_length: int = 128, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True, n_mels: int = 80, fmin: int | None = None, fmax: int | None = None, htk: bool = False, frontend_conf: dict | None = {'badim': 320, 'bdropout_rate': 0.0, 'blayers': 3, 'bnmask': 2, 'bprojs': 320, 'btype': 'blstmp', 'bunits': 300, 'delay': 3, 'ref_channel': -1, 'taps': 5, 'use_beamformer': False, 'use_dnn_mask_for_wpe': True, 'use_wpe': False, 'wdropout_rate': 0.0, 'wlayers': 3, 'wprojs': 320, 'wtype': 'blstmp', 'wunits': 300}, apply_stft: bool = True)\",\"Bases: AbsFrontend\",\"Conventional frontend structure for ASR.\",\"Stft -> WPE -> MVDR-Beamformer -> Power-spec -> Log-Mel-Fbank\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"721\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\"]},\"722\":{\"h\":\"espnet2.asr.frontend.cnn.Dim1LayerNorm\",\"t\":[\"source\",\"class espnet2.asr.frontend.cnn.Dim1LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, bias=True)\",\"Bases: Module\",\"LayerNorm on middle dim.\",\"It assumes the input is shape B, D, T to avoid transposing. Faster than TransposedLayerNorm, but may lead to minor numerical differences.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"723\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"724\":{\"h\":\"espnet2.asr.state_spaces.pool.DownAvgPool\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.pool.DownAvgPool(d_input, stride=1, expand=1, transposed=True)\",\"Bases: SequenceModule\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"forward(x)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\"]},\"725\":{\"h\":\"espnet2.asr.state_spaces.pool.DownLinearPool\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.pool.DownLinearPool(d_input, stride=1, expand=1, transposed=True)\",\"Bases: SequenceModule\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"forward(x)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\"]},\"726\":{\"h\":\"espnet2.asr.state_spaces.pool.DownPool\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.pool.DownPool(d_input, d_output=None, expand=None, stride=1, transposed=True, weight_norm=True, initializer=None, activation=None)\",\"Bases: SequenceModule\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"default_state(*batch_shape, device=None)\",\"Create initial state for a batch of inputs.\",\"forward(x)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"step(x, state, **kwargs)\",\"Step one time step as a recurrent model.\",\"x: (…, H)\"]},\"727\":{\"h\":\"espnet2.asr.state_spaces.pool.DownPool2d\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.pool.DownPool2d(d_input, d_output, stride=1, transposed=True, weight_norm=True)\",\"Bases: SequenceModule\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\"]},\"728\":{\"h\":\"espnet2.asr.state_spaces.pool.DownSample\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.pool.DownSample(d_input, stride=1, expand=1, transposed=True)\",\"Bases: SequenceModule\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"forward(x)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\"]},\"729\":{\"h\":\"espnet2.asr.state_spaces.pool.DownSpectralPool\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.pool.DownSpectralPool(d_input, stride=1, expand=1, transposed=True)\",\"Bases: SequenceModule\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"forward(x)\",\"Forward pass.\",\"x: (B, L…, D)\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\"]},\"730\":{\"h\":\"espnet2.asr.state_spaces.components.DropoutNd\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.components.DropoutNd(p: float = 0.5, tie=True, transposed=True)\",\"Bases: Module\",\"Initialize dropout module.\",\"tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)\",\"forward(X)\",\"Forward pass.\",\"X: (batch, dim, lengths…)\"]},\"731\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.DynamicConvolution2DTransformerDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.transformer_decoder.DynamicConvolution2DTransformerDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, conv_wshare: int = 4, conv_kernel_length: ~typing.Sequence[int] = (11, 11, 11, 11, 11, 11), conv_usebias: int = False)\",\"Bases: BaseTransformerDecoder\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"]},\"732\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.DynamicConvolutionTransformerDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.transformer_decoder.DynamicConvolutionTransformerDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, conv_wshare: int = 4, conv_kernel_length: ~typing.Sequence[int] = (11, 11, 11, 11, 11, 11), conv_usebias: int = False)\",\"Bases: BaseTransformerDecoder\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"]},\"733\":{\"h\":\"espnet2.asr.encoder.e_branchformer_ctc_encoder.EBranchformerCTCEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.e_branchformer_ctc_encoder.EBranchformerCTCEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, attention_layer_type: str = 'rel_selfattn', pos_enc_layer_type: str = 'rel_pos', rel_pos_type: str = 'latest', cgmlp_linear_units: int = 2048, cgmlp_conv_kernel: int = 31, use_linear_after_conv: bool = False, gate_activation: str = 'identity', num_blocks: int = 12, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d8', zero_triu: bool = False, padding_idx: int = -1, layer_drop_rate: float = 0.0, max_pos_emb_len: int = 5000, use_ffn: bool = False, macaron_ffn: bool = False, ffn_activation_type: str = 'swish', linear_units: int = 2048, positionwise_layer_type: str = 'linear', merge_conv_kernel: int = 3, interctc_layer_idx=None, interctc_use_conditioning: bool = False, use_cross_attention=True, use_flash_attn: bool = False)\",\"Bases: AbsEncoder\",\"E-Branchformer encoder module.\",\"Compared to the original encoder in e_branchformer_encoder.py, this variant supports additional cross-attention modules. Additionally, it supports extra prefix tokens for the input. This is useful for language and task conditioning.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, ctc: CTC | None = None, max_layer: int | None = None, prefix_embeds: tensor | None = None, memory=None, memory_mask=None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Calculate forward propagation.\",\"Parameters:\",\"xs_pad (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"ilens (torch.Tensor) – Input length (#batch).\",\"prev_states (torch.Tensor) – Not to be used now.\",\"ctc (CTC) – Intermediate CTC module.\",\"max_layer (int) – Layer depth below which InterCTC is applied.\",\"Returns: Output tensor (#batch, L, output_size). torch.Tensor: Output length (#batch). torch.Tensor: Not to be used now.\",\"Return type: torch.Tensor\",\"output_size() → int\"]},\"734\":{\"h\":\"espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, attention_layer_type: str = 'rel_selfattn', pos_enc_layer_type: str = 'rel_pos', rel_pos_type: str = 'latest', cgmlp_linear_units: int = 2048, cgmlp_conv_kernel: int = 31, use_linear_after_conv: bool = False, gate_activation: str = 'identity', num_blocks: int = 12, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', zero_triu: bool = False, padding_idx: int = -1, layer_drop_rate: float = 0.0, max_pos_emb_len: int = 5000, use_ffn: bool = False, macaron_ffn: bool = False, ffn_activation_type: str = 'swish', linear_units: int = 2048, positionwise_layer_type: str = 'linear', merge_conv_kernel: int = 3, interctc_layer_idx=None, interctc_use_conditioning: bool = False, qk_norm: bool = False, use_flash_attn: bool = True, gradient_checkpoint_layers: List[int] = [])\",\"Bases: AbsEncoder\",\"E-Branchformer encoder module.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, masks: Tensor | None = None, ctc: CTC | None = None, max_layer: int | None = None, return_all_hs: bool = False) → Tuple[Tensor, Tensor, Tensor | None]\",\"Calculate forward propagation.\",\"Parameters:\",\"xs_pad (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"ilens (torch.Tensor) – Input length (#batch).\",\"prev_states (torch.Tensor) – Not to be used now.\",\"ctc (CTC) – Intermediate CTC module.\",\"max_layer (int) – Layer depth below which InterCTC is applied.\",\"Returns: Output tensor (#batch, L, output_size). torch.Tensor: Output length (#batch). torch.Tensor: Not to be used now.\",\"Return type: torch.Tensor\",\"output_size() → int\"]},\"735\":{\"h\":\"espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoderLayer\",\"t\":[\"source\",\"class espnet2.asr.encoder.e_branchformer_encoder.EBranchformerEncoderLayer(size: int, attn: Module, cgmlp: Module, feed_forward: Module | None, feed_forward_macaron: Module | None, dropout_rate: float, merge_conv_kernel: int = 3)\",\"Bases: Module\",\"E-Branchformer encoder layer module.\",\"Parameters:\",\"size (int) – model dimension\",\"attn – standard self-attention or efficient attention\",\"cgmlp – ConvolutionalGatingMLP\",\"feed_forward – feed-forward module, optional\",\"feed_forward – macaron-style feed-forward module, optional\",\"dropout_rate (float) – dropout probability\",\"merge_conv_kernel (int) – kernel size of the depth-wise conv in merge module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x_input, mask, cache=None)\",\"Compute encoded features.\",\"Parameters:\",\"x_input (Union *[*Tuple,torch.Tensor]) – Input tensor w/ or w/o pos emb. \",\"w/ pos emb: Tuple of tensors [(#batch, time, size), (1, time, size)].\",\"w/o pos emb: Tensor (#batch, time, size).\",\"mask (torch.Tensor) – Mask tensor for the input (#batch, 1, time).\",\"cache (torch.Tensor) – Cache tensor of the input (#batch, time - 1, size).\",\"Returns: Output tensor (#batch, time, size). torch.Tensor: Mask tensor (#batch, time).\",\"Return type: torch.Tensor\"]},\"736\":{\"h\":\"espnet2.asr.pit_espnet_model.ESPnetASRModel\",\"t\":[\"source\",\"class espnet2.asr.pit_espnet_model.ESPnetASRModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, decoder: AbsDecoder | None, ctc: CTC, joint_network: Module | None, ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', sym_sos: str = '<sos/eos>', sym_eos: str = '<sos/eos>', extract_feats_in_collect_stats: bool = True, lang_token_id: int = -1, num_inf: int = 1, num_ref: int = 1)\",\"Bases: ESPnetASRModel\",\"CTC-attention hybrid Encoder-Decoder model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\"]},\"737\":{\"h\":\"espnet2.asr.discrete_asr_espnet_model.ESPnetDiscreteASRModel\",\"t\":[\"source\",\"class espnet2.asr.discrete_asr_espnet_model.ESPnetDiscreteASRModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, decoder: AbsDecoder, ctc: CTC | None, ctc_weight: float = 0.5, interctc_weight: float = 0.0, src_vocab_size: int = 0, src_token_list: Tuple[str, ...] | List[str] = [], ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_bleu: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', patch_size: int = 1, extract_feats_in_collect_stats: bool = True, share_decoder_input_output_embed: bool = False, share_encoder_decoder_input_embed: bool = False)\",\"Bases: ESPnetMTModel\",\"Encoder-Decoder model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"encode(src_text: Tensor, src_text_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Frontend + Encoder. Note that this method is used by mt_inference.py\",\"Parameters:\",\"src_text – (Batch, Length, …)\",\"src_text_lengths – (Batch, )\",\"forward(text: Tensor, text_lengths: Tensor, src_text: Tensor, src_text_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"src_text – (Batch, length)\",\"src_text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\"]},\"738\":{\"h\":\"espnet2.asr.frontend.espnet_ssl.ESPnetSSLFrontend\",\"t\":[\"source\",\"class espnet2.asr.frontend.espnet_ssl.ESPnetSSLFrontend(fs: int | str = 16000, frontend_conf: dict | None = {'badim': 320, 'bdropout_rate': 0.0, 'blayers': 3, 'bnmask': 2, 'bprojs': 320, 'btype': 'blstmp', 'bunits': 300, 'delay': 3, 'ref_channel': -1, 'taps': 5, 'use_beamformer': False, 'use_dnn_mask_for_wpe': True, 'use_wpe': False, 'wdropout_rate': 0.0, 'wlayers': 3, 'wprojs': 320, 'wtype': 'blstmp', 'wunits': 300}, masking_conf: dict | None = {}, multilayer_feature: bool = False, layer: int = -1, freeze_encoder_steps: int = 0, mask_feats: bool = True, use_final_output: bool = True)\",\"Bases: AbsFrontend\",\"Frontend wrapper for SSL models trained in ESPnet.\",\"Parameters:\",\"fs (int) – unused.\",\"frontend_conf (dict) – make sure path_or_url has a value\",\"multilayer_feature (bool) – whether to use weighted sum\",\"layer (int) – 0-indexed layer to use if not using weighted sum\",\"mask_feats (int) – whether to mask input feats to encoder\",\"use_final_output (bool) – use final normalized output instead of last layer output. For post-LN model archs.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"739\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\",\"reload_pretrained_parameters()\"]},\"740\":{\"h\":\"espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer\",\"t\":[\"source\",\"class espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer(decoder: AbsDecoder, joint_network: Module, token_list: List[int], sym_space: str, sym_blank: str, report_cer: bool = False, report_wer: bool = False)\",\"Bases: object\",\"Calculate CER and WER for transducer models.\",\"Parameters:\",\"decoder – Decoder module.\",\"token_list – List of tokens.\",\"sym_space – Space symbol.\",\"sym_blank – Blank symbol.\",\"report_cer – Whether to compute CER.\",\"report_wer – Whether to compute WER.\",\"Construct an ErrorCalculatorTransducer.\",\"calculate_cer(char_pred: Tensor, char_target: Tensor) → float\",\"Calculate sentence-level CER score.\",\"Parameters:\",\"char_pred – Prediction character sequences. (B, ?)\",\"char_target – Target character sequences. (B, ?)\",\"Returns: Average sentence-level CER score.\",\"calculate_wer(char_pred: Tensor, char_target: Tensor) → float\",\"Calculate sentence-level WER score.\",\"Parameters:\",\"char_pred – Prediction character sequences. (B, ?)\",\"char_target – Target character sequences. (B, ?)\",\"Returns: Average sentence-level WER score\",\"convert_to_char(pred: Tensor, target: Tensor) → Tuple[List, List]\",\"Convert label ID sequences to character sequences.\",\"Parameters:\",\"pred – Prediction label ID sequences. (B, U)\",\"target – Target label ID sequences. (B, L)\",\"Returns: Prediction character sequences. (B, ?) char_target: Target character sequences. (B, ?)\",\"Return type: char_pred\"]},\"741\":{\"h\":\"espnet2.asr.decoder.whisper_decoder.ExpandedTokenEmbedding\",\"t\":[\"source\",\"class espnet2.asr.decoder.whisper_decoder.ExpandedTokenEmbedding(ori_emebedding, additional_size)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"742\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"property weight\"]},\"743\":{\"h\":\"espnet2.asr.transducer.beam_search_transducer_streaming.ExtendedHypothesis\",\"t\":[\"source\",\"class espnet2.asr.transducer.beam_search_transducer_streaming.ExtendedHypothesis(score: float, yseq: List[int], dec_state: Tuple[Tensor, Tensor | None] | List[Tensor | None] | Tensor, lm_state: Dict[str, Any] | List[Any] | None = None, dec_out: List[Tensor] | None = None, lm_scores: Tensor | None = None)\",\"Bases: Hypothesis\",\"Extended hypothesis definition for NSC beam search and mAES.\",\"dec_out : List[Tensor]= None\",\"lm_scores : Tensor= None\"]},\"744\":{\"h\":\"espnet2.asr.state_spaces.ff.FF\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.ff.FF(d_input, expand=2, d_output=None, transposed=False, activation='gelu', initializer=None, dropout=0.0, tie_dropout=False)\",\"Bases: SequenceModule\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, *args, **kwargs)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\"]},\"745\":{\"h\":\"espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder(input_size: int, w2v_url: str, w2v_dir_path: str = './', output_size: int = 256, normalize_before: bool = False, freeze_finetune_updates: int = 0)\",\"Bases: AbsEncoder\",\"FairSeq Wav2Vec2 encoder module.\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"w2v_url – url to Wav2Vec2.0 pretrained model\",\"w2v_dir_path – directory to download the Wav2Vec2.0 pretrained model.\",\"normalize_before – whether to use layer_norm before the first block\",\"finetune_last_n_layers – last n layers to be finetuned in Wav2Vec2.0 0 means to finetune every layer if freeze_w2v=False.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Forward FairSeqWav2Vec2 Encoder.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size() → int\",\"reload_pretrained_parameters()\"]},\"746\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.FairseqAVHubertEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.avhubert_encoder.FairseqAVHubertEncoder(input_size: int = 1, avhubert_url: str = './', avhubert_dir_path: str = './', freeze_finetune_updates: int = 0, encoder_embed_dim: int = 1024, encoder_layerdrop: float = 0.05, dropout_input: float = 0.1, dropout_features: float = 0.1, dropout: float = 0.1, attention_dropout: float = 0.1, feature_grad_mult: float = 0.1, activation_dropout: float = 0.0, wav_input: bool = False, layer_norm_first: bool = True, audio_feat_dim: int = 104, encoder_layers: int = 24, encoder_ffn_embed_dim: int = 4096, encoder_attention_heads: int = 16, extracted: bool = False, pretrain: bool = True, modality_dropout: float = 0.0, audio_dropout: float = 0.0, noise_augmentation: bool = False, noise_path: str = './data/babble_noise.pt', max_noise_weight: float = 0.5, audio_only: bool = False)\",\"Bases: AbsEncoder\",\"FairSeq AVHubert pretrained encoder module\",\"Parameters:\",\"input_size – input dim\",\"avhubert_url – download link for pre-trained avhubert model\",\"avhubert_dir_path – dir_path for downloading pre-trained avhubert model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Dict[str, Tensor], ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Forward AVHubert Encoder.\",\"Parameters:\",\"xs_pad**[video]** – input tensor (B, 1, L, H, W)\",\"xs_pad**[audio]** – input tensor (B, D, L)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"forward_fusion(xs_pad: Dict[str, Tensor]) → Tensor\",\"output_size() → int\",\"reload_pretrained_parameters()\"]},\"747\":{\"h\":\"espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder(input_size: int, hubert_url: str = './', hubert_dir_path: str = './', output_size: int = 256, normalize_before: bool = False, freeze_finetune_updates: int = 0, dropout_rate: float = 0.0, activation_dropout: float = 0.1, attention_dropout: float = 0.0, mask_length: int = 10, mask_prob: float = 0.75, mask_selection: str = 'static', mask_other: int = 0, apply_mask: bool = True, mask_channel_length: int = 64, mask_channel_prob: float = 0.5, mask_channel_other: int = 0, mask_channel_selection: str = 'static', layerdrop: float = 0.1, feature_grad_mult: float = 0.0)\",\"Bases: AbsEncoder\",\"FairSeq Hubert encoder module, used for loading pretrained weight and finetuning\",\"Parameters:\",\"input_size – input dim\",\"hubert_url – url to Hubert pretrained model\",\"hubert_dir_path – directory to download the Wav2Vec2.0 pretrained model.\",\"output_size – dimension of attention\",\"normalize_before – whether to use layer_norm before the first block\",\"freeze_finetune_updates – steps that freeze all layers except output layer before tuning the whole model (nessasary to prevent overfit).\",\"dropout_rate – dropout rate\",\"activation_dropout – dropout rate in activation function\",\"attention_dropout – dropout rate in attention\",\"Hubert specific Args: : Please refer to: https://github.com/pytorch/fairseq/blob/master/fairseq/models/hubert/hubert.py\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Forward Hubert ASR Encoder.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size() → int\",\"reload_pretrained_parameters()\"]},\"748\":{\"h\":\"espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder(input_size: int = 1, output_size: int = 1024, linear_units: int = 1024, attention_heads: int = 12, num_blocks: int = 12, dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0, activation_dropout_rate: float = 0.0, hubert_dict: str = './dict.txt', label_rate: int = 100, checkpoint_activations: bool = False, sample_rate: int = 16000, use_amp: bool = False, **kwargs)\",\"Bases: AbsEncoder\",\"FairSeq Hubert pretrain encoder module, only used for pretraining stage\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"linear_units – dimension of feedforward layers\",\"attention_heads – the number of heads of multi head attention\",\"num_blocks – the number of encoder blocks\",\"dropout_rate – dropout rate\",\"attention_dropout_rate – dropout rate in attention\",\"hubert_dict – target dictionary for Hubert pretraining\",\"label_rate – label frame rate. -1 for sequence label\",\"sample_rate – target sample rate.\",\"use_amp – whether to use automatic mixed precision\",\"normalize_before – whether to use layer_norm before the first block\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"cast_mask_emb()\",\"forward(xs_pad: Tensor, ilens: Tensor, ys_pad: Tensor, ys_pad_length: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Forward Hubert Pretrain Encoder.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size() → int\",\"reload_pretrained_parameters()\"]},\"749\":{\"h\":\"espnet2.asr.layers.fastformer.FastSelfAttention\",\"t\":[\"source\",\"class espnet2.asr.layers.fastformer.FastSelfAttention(size, attention_heads, dropout_rate)\",\"Bases: Module\",\"Fast self-attention used in Fastformer.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"espnet_initialization_fn()\",\"forward(xs_pad, mask)\",\"Forward method.\",\"Parameters:\",\"xs_pad – (batch, time, size = n_heads * attn_dim)\",\"mask – (batch, 1, time), nonpadding is 1, padding is 0\",\"Returns: (batch, time, size)\",\"Return type: torch.Tensor\",\"init_weights(module)\",\"transpose_for_scores(x)\",\"Reshape and transpose to compute scores.\",\"Parameters:x – (batch, time, size = n_heads * attn_dim)\",\"Returns: (batch, n_heads, time, attn_dim)\"]},\"750\":{\"h\":\"espnet2.asr.frontend.espnet_ssl.Featureizer\",\"t\":[\"source\",\"class espnet2.asr.frontend.espnet_ssl.Featureizer(num_layers: int)\",\"Bases: Module\",\"Simplified S3PRL-style featurizer.\",\"Outputs a learned weighted sum of input layers. Original code by Leo Yang (2022) in the S3PRL library. https://github.com/s3prl/s3prl/blob/main/s3prl/nn/upstream.py\",\"forward(all_hs: List[FloatTensor])\",\"Forward function.\",\"Parameters:all_hs (List *[*torch.FloatTensor]) – List[ (batch_size, seq_len, hidden_size) ]\",\"Returns:\",\"The weighted-sum result, (batch_size, seq_len, hidden_size)\"]},\"751\":{\"h\":\"espnet2.asr.state_spaces.residual.Feedforward\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.residual.Feedforward(*args)\",\"Bases: Residual\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"]},\"752\":{\"h\":\"espnet2.asr.frontend.fused.FusedFrontends\",\"t\":[\"source\",\"class espnet2.asr.frontend.fused.FusedFrontends(frontends: List[Dict[str, Any]] | None = None, align_method: str = 'linear_projection', proj_dim: int = 100, fs: int = 16000)\",\"Bases: AbsFrontend\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"753\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\"]},\"754\":{\"h\":\"espnet2.asr.encoder.beats_encoder.GLU_Linear\",\"t\":[\"source\",\"class espnet2.asr.encoder.beats_encoder.GLU_Linear(input_dim, output_dim, glu_type='sigmoid', bias_in_glu=True)\",\"Bases: Module\",\"GLU Linear layer\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward pass\"]},\"755\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt.GPURNNT\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt.GPURNNT(minibatch: int, maxT: int, maxU: int, alphabet_size: int, workspace, blank: int, fastemit_lambda: float, clamp: float, num_threads: int, stream)\",\"Bases: object\",\"Helper class to launch the CUDA Kernels to compute the Transducer Loss.\",\"Parameters:\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"workspace – An allocated chunk of memory that will be sliced off and reshaped into required blocks used as working memory.\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"num_threads – Number of OMP threads to launch.\",\"stream – Numba Cuda Stream.\",\"compute_cost_and_score(acts: Tensor, grads: Tensor | None, costs: Tensor, labels: Tensor, label_lengths: Tensor, input_lengths: Tensor) → RNNTStatus\",\"Compute both the loss and the gradients.\",\"Parameters:\",\"acts – A flattened tensor of shape [B, T, U, V+1] representing the activation matrix.\",\"grad – A flattented zero tensor of same shape as acts.\",\"costs – A zero vector of length B which will be updated inplace with the log probability costs.\",\"flat_labels – A flattened matrix of labels of shape [B, U]\",\"label_lengths – A vector of length B that contains the original lengths of the acoustic sequence.\",\"input_lengths – A vector of length B that contains the original lengths of the target sequence.\",\"Updates: : This will launch kernels that will update inline the following variables:\",\"grads: Gradients of the activation matrix wrt the costs vector.\",\"costs: Negative log likelihood of the forward variable.\",\"Returns: An enum that either represents a successful RNNT operation or failure.\",\"cost_and_grad(acts: Tensor, grads: Tensor, costs: Tensor, pad_labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\",\"log_softmax(acts: Tensor, denom: Tensor)\",\"Computes the log softmax denominator of the input activation tensor\",\"and stores the result in denom.\",\"Parameters:\",\"acts – Activation tensor of shape [B, T, U, V+1]. The input must be represented as a flat tensor of shape [B * T * U * (V+1)] to allow pointer indexing.\",\"denom – A zero tensor of same shape as acts.\",\"Updates: : This kernel inplace updates the denom tensor\",\"score_forward(acts: Tensor, costs: Tensor, pad_labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\"]},\"756\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.GradMultiply\",\"t\":[\"source\",\"class espnet2.asr.encoder.avhubert_encoder.GradMultiply(*args, **kwargs)\",\"Bases: Function\",\"static backward(ctx, grad)\",\"Define a formula for differentiating the operation with backward mode automatic differentiation.\",\"This function is to be overridden by all subclasses. (Defining this function is equivalent to defining the vjp function.)\",\"It must accept a context ctx as the first argument, followed by as many outputs as the forward() returned (None will be passed in for non tensor outputs of the forward function), and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not requiring grads, you can just pass None as a gradient for that input.\",\"The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computed w.r.t. the output.\",\"static forward(ctx, x, scale)\",\"Define the forward of the custom autograd Function.\",\"This function is to be overridden by all subclasses. There are two ways to define forward:\",\"Usage 1 (Combined forward and ctx):\",\"@staticmethod def forward(ctx: Any, *args: Any, **kwargs: Any) -> Any: pass\",\"It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types).\",\"See combining-forward-context for more details\",\"Usage 2 (Separate forward and ctx):\",\"@staticmethod def forward(*args: Any, **kwargs: Any) -> Any: pass @staticmethod def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -> None: pass\",\"The forward no longer accepts a ctx argument.\",\"Instead, you must also override the torch.autograd.Function.setup_context() staticmethod to handle setting up the ctx object. output is the output of the forward, inputs are a Tuple of inputs to the forward.\",\"See extending-autograd for more details\",\"The context can be used to store arbitrary data that can be then retrieved during the backward pass. Tensors should not be stored directly on ctx (though this is not currently enforced for backward compatibility). Instead, tensors should be saved either with ctx.save_for_backward() if they are intended to be used in backward (equivalently, vjp) or ctx.save_for_forward() if they are intended to be used for in jvp.\"]},\"757\":{\"h\":\"espnet2.asr.state_spaces.residual.Highway\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.residual.Highway(*args, scaling_correction=False, elemwise=False)\",\"Bases: Residual\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y, transposed=False)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"758\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"759\":{\"h\":\"espnet2.asr.frontend.huggingface.HuggingFaceFrontend\",\"t\":[\"source\",\"class espnet2.asr.frontend.huggingface.HuggingFaceFrontend(model, fs: int | str = 16000, download_dir: str | None = None, load_pretrained: bool = True)\",\"Bases: AbsFrontend\",\"Use pretrained models from Hugging Face Transformers for ASR\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inputs: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Wrapper for the transformers forward pass.\",\"Inputs are converted to numpy and re-encoded with the transformers processor.\",\"Parameters:\",\"input – Input (B, L) single channel waveform.\",\"input_lengths – Input lengths within batch.\",\"Returns: Output with dimensions (B, T, D), T is the processed length, : D is the feature dimension.\",\"Tensor: Output lengths within batch.\",\"Return type: Tensor\",\"output_size() → int\",\"reload_pretrained_parameters()\"]},\"760\":{\"h\":\"espnet2.asr.decoder.hugging_face_transformers_decoder.HuggingFaceTransformersDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.hugging_face_transformers_decoder.HuggingFaceTransformersDecoder(vocab_size: int, encoder_output_size: int, model_name_or_path: str, causal_lm: bool = False, prefix: str = '', postfix: str = '', overriding_architecture_config: str | dict | None = {}, load_pretrained_weights: bool = True, separate_lm_head: bool = False)\",\"Bases: AbsDecoder, BatchScorerInterface\",\"Hugging Face Transformers Decoder.\",\"Parameters:\",\"encoder_output_size – dimension of encoder attention\",\"model_name_or_path – Hugging Face Transformers model name\",\"Initializes the HuggingFaceTransformersDecoder.\",\"Parameters:\",\"vocab_size (int) – The size of the vocabulary.\",\"encoder_output_size (int) – The size of the encoder output.\",\"model_name_or_path (str) – The name or path of the pre-trained Transformers model.\",\"causal_lm (bool,optional) – Whether to use a causal language model. Defaults to False. This overrides the model_name_or_path if provided.\",\"prefix (str,optional) – Prefix to be added to the input tokens. Defaults to “”.\",\"postfix (str,optional) – Postfix to be added to the input tokens. Defaults to “”.\",\"overriding_architecture_config (strordict,optional) – Path to the configuration json file or the json dictionary itself. Defaults to None. If this is set, it can be used to override the default decoder configuration.\",\"load_pretrained_weights (bool) – Whether to load the pre-trained weights. Defaults to True.\",\"separate_lm_head (bool) – True ensures that the language model head is not shared with the input token embeddings. When False, the original structure is kept, ie, if the original Transformers implementation has tying of weights, it is retained. Defaults to False.\",\"Raises:\",\"ImportError – If the transformers library is not available.\",\"Exception – If the word embeddings attribute cannot be found in the model.\",\"add_prefix_postfix(enc_out, hlens, ys_in_pad, ys_in_lens)\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor, speech: Tensor | None = None) → Tuple[Tensor, List[Any]]\",\"Score new token batch (required).\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor) → Tuple[Tensor, Tensor]\",\"Forward decoder.\",\"Parameters:\",\"hs_pad – encoded memory, float32 (batch, maxlen_in, feat)\",\"hlens – (batch)\",\"ys_in_pad – input tensor (batch, maxlen_out, #mels)\",\"ys_in_lens – (batch)\",\"Returns: tuple containing:\",\"x: decoded token score before softmax (batch, maxlen_out, token) : if use_output_layer is True,\",\"olens: (batch, )\",\"Return type: (tuple)\",\"reload_pretrained_parameters()\",\"score(ys, state, x, speech=None)\",\"Score new token (required).\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – The encoder feature that generates ys.\",\"Returns: Tuple of : scores for next token that has a shape of (n_vocab) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\"]},\"761\":{\"h\":\"espnet2.asr.encoder.hugging_face_transformers_encoder.HuggingFaceTransformersEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.hugging_face_transformers_encoder.HuggingFaceTransformersEncoder(input_size: int, model_name_or_path: str, lang_token_id: int = -1)\",\"Bases: AbsEncoder\",\"Hugging Face Transformers PostEncoder.\",\"Initialize the module.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Forward.\",\"output_size() → int\",\"Get the output size.\",\"reload_pretrained_parameters()\"]},\"762\":{\"h\":\"espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder\",\"t\":[\"source\",\"class espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder(input_size: int, model_name_or_path: str, length_adaptor_n_layers: int = 0, lang_token_id: int = -1)\",\"Bases: AbsPostEncoder\",\"Hugging Face Transformers PostEncoder.\",\"Initialize the module.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Forward.\",\"output_size() → int\",\"Get the output size.\",\"reload_pretrained_parameters()\"]},\"763\":{\"h\":\"espnet2.asr.transducer.beam_search_transducer_streaming.Hypothesis\",\"t\":[\"source\",\"class espnet2.asr.transducer.beam_search_transducer_streaming.Hypothesis(score: float, yseq: List[int], dec_state: Tuple[Tensor, Tensor | None] | List[Tensor | None] | Tensor, lm_state: Dict[str, Any] | List[Any] | None = None)\",\"Bases: object\",\"Default hypothesis definition for Transducer search algorithms.\",\"dec_state : Tuple[Tensor, Tensor | None] | List[Tensor | None] | Tensor\",\"lm_state : Dict[str, Any] | List[Any]= None\",\"score : float\",\"yseq : List[int]\"]},\"764\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.I_Op\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.I_Op(value)\",\"Bases: Enum\",\"Represents an operation that is performed on the input tensor\",\"EXPONENTIAL = 0\",\"IDENTITY = 1\"]},\"765\":{\"h\":\"espnet2.asr.postencoder.length_adaptor_postencoder.LengthAdaptorPostEncoder\",\"t\":[\"source\",\"class espnet2.asr.postencoder.length_adaptor_postencoder.LengthAdaptorPostEncoder(input_size: int, length_adaptor_n_layers: int = 0, input_layer: str | None = None, output_size: int | None = None, dropout_rate: float = 0.1, return_int_enc: bool = False)\",\"Bases: AbsPostEncoder\",\"Length Adaptor PostEncoder.\",\"Initialize the module.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Forward.\",\"output_size() → int\",\"Get the output size.\"]},\"766\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.LightweightConvolution2DTransformerDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.transformer_decoder.LightweightConvolution2DTransformerDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, conv_wshare: int = 4, conv_kernel_length: ~typing.Sequence[int] = (11, 11, 11, 11, 11, 11), conv_usebias: int = False)\",\"Bases: BaseTransformerDecoder\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"]},\"767\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, conv_wshare: int = 4, conv_kernel_length: ~typing.Sequence[int] = (11, 11, 11, 11, 11, 11), conv_usebias: int = False)\",\"Bases: BaseTransformerDecoder\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"]},\"768\":{\"h\":\"espnet2.asr.preencoder.sinc.LightweightSincConvs\",\"t\":[\"source\",\"class espnet2.asr.preencoder.sinc.LightweightSincConvs(fs: int | str | float = 16000, in_channels: int = 1, out_channels: int = 256, activation_type: str = 'leakyrelu', dropout_type: str = 'dropout', windowing_type: str = 'hamming', scale_type: str = 'mel')\",\"Bases: AbsPreEncoder\",\"Lightweight Sinc Convolutions.\",\"Instead of using precomputed features, end-to-end speech recognition can also be done directly from raw audio using sinc convolutions, as described in “Lightweight End-to-End Speech Recognition from Raw Audio Data Using Sinc-Convolutions” by Kürzinger et al. https://arxiv.org/abs/2010.07597\",\"To use Sinc convolutions in your model instead of the default f-bank frontend, set this module as your pre-encoder with preencoder: sinc and use the input of the sliding window frontend with frontend: sliding_window in your yaml configuration file. So that the process flow is:\",\"Frontend (SlidingWindow) -> SpecAug -> Normalization -> Pre-encoder (LightweightSincConvs) -> Encoder -> Decoder\",\"Note that this method also performs data augmentation in time domain (vs. in spectral domain in the default frontend). Use plot_sinc_filters.py to visualize the learned Sinc filters.\",\"Initialize the module.\",\"Parameters:\",\"fs – Sample rate.\",\"in_channels – Number of input channels.\",\"out_channels – Number of output channels (for each input channel).\",\"activation_type – Choice of activation function.\",\"dropout_type – Choice of dropout function.\",\"windowing_type – Choice of windowing function.\",\"scale_type – Choice of filter-bank initialization scale.\",\"espnet_initialization_fn()\",\"Initialize sinc filters with filterbank values.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Apply Lightweight Sinc Convolutions.\",\"The input shall be formatted as (B, T, C_in, D_in) with B as batch size, T as time dimension, C_in as channels, and D_in as feature dimension.\",\"The output will then be (B, T, C_out*D_out) with C_out and D_out as output dimensions.\",\"The current module structure only handles D_in=400, so that D_out=1. Remark for the multichannel case: C_out is the number of out_channels given at initialization multiplied with C_in.\",\"gen_lsc_block(in_channels: int, out_channels: int, depthwise_kernel_size: int = 9, depthwise_stride: int = 1, depthwise_groups=None, pointwise_groups=0, dropout_probability: float = 0.15, avgpool=False)\",\"Generate a convolutional block for Lightweight Sinc convolutions.\",\"Each block consists of either a depthwise or a depthwise-separable convolutions together with dropout, (batch-)normalization layer, and an optional average-pooling layer.\",\"Parameters:\",\"in_channels – Number of input channels.\",\"out_channels – Number of output channels.\",\"depthwise_kernel_size – Kernel size of the depthwise convolution.\",\"depthwise_stride – Stride of the depthwise convolution.\",\"depthwise_groups – Number of groups of the depthwise convolution.\",\"pointwise_groups – Number of groups of the pointwise convolution.\",\"dropout_probability – Dropout probability in the block.\",\"avgpool – If True, an AvgPool layer is inserted.\",\"Returns: Neural network building block.\",\"Return type: torch.nn.Sequential\",\"output_size() → int\",\"Get the output size.\"]},\"769\":{\"h\":\"espnet2.asr.state_spaces.components.LinearActivation\",\"t\":[\"source\",\"espnet2.asr.state_spaces.components.LinearActivation(d_input, d_output, bias=True, zero_bias_init=False, transposed=False, initializer=None, activation=None, activate=False, weight_norm=False, **kwargs)\",\"Return a linear module, initialization, and activation.\"]},\"770\":{\"h\":\"espnet2.asr.decoder.linear_decoder.LinearDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.linear_decoder.LinearDecoder(vocab_size: int, encoder_output_size: int, pooling: str = 'mean', dropout: float = 0.0)\",\"Bases: AbsDecoder\",\"Initialize the module.\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor | None = None, ys_in_lens: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Forward method.\",\"Parameters:\",\"hs_pad – (B, Tmax, D)\",\"hlens – (B,)\",\"Returns: (B, n_classes)\",\"Return type: output\",\"output_size() → int\",\"Get the output size.\",\"score(ys, state, x)\",\"Classify x.\",\"Parameters:\",\"ys – Not used\",\"state – Not used\",\"x – (T, D). this should be a single sample without any padding ie batch size=1.\",\"Returns: log probabilities over (n_classes,) state: None\",\"Return type: logp\",\"Assumes that x is a single unpadded sequence.\"]},\"771\":{\"h\":\"espnet2.asr.encoder.linear_encoder.LinearEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.linear_encoder.LinearEncoder(input_size: int, output_size: int = 256, dropout_rate: float = 0.1, input_layer: str | None = 'conv2d', normalize_before: bool = True, padding_idx: int = -1)\",\"Bases: AbsEncoder\",\"Linear encoder module.\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"linear_units – the number of units of position-wise feed forward\",\"dropout_rate – dropout rate\",\"input_layer – input layer type\",\"normalize_before – whether to use layer_norm before the first block\",\"padding_idx – padding_idx for input_layer=embed\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size() → int\"]},\"772\":{\"h\":\"espnet2.asr.preencoder.linear.LinearProjection\",\"t\":[\"source\",\"class espnet2.asr.preencoder.linear.LinearProjection(input_size: int, output_size: int, dropout: float = 0.0)\",\"Bases: AbsPreEncoder\",\"Linear Projection Preencoder.\",\"Initialize the module.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Forward.\",\"output_size() → int\",\"Get the output size.\"]},\"773\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.LogSoftmaxGradModification\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.LogSoftmaxGradModification(*args, **kwargs)\",\"Bases: Function\",\"static backward(ctx, grad_output)\",\"Define a formula for differentiating the operation with backward mode automatic differentiation.\",\"This function is to be overridden by all subclasses. (Defining this function is equivalent to defining the vjp function.)\",\"It must accept a context ctx as the first argument, followed by as many outputs as the forward() returned (None will be passed in for non tensor outputs of the forward function), and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not requiring grads, you can just pass None as a gradient for that input.\",\"The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computed w.r.t. the output.\",\"static forward(ctx, acts, clamp)\",\"Define the forward of the custom autograd Function.\",\"This function is to be overridden by all subclasses. There are two ways to define forward:\",\"Usage 1 (Combined forward and ctx):\",\"@staticmethod def forward(ctx: Any, *args: Any, **kwargs: Any) -> Any: pass\",\"It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types).\",\"See combining-forward-context for more details\",\"Usage 2 (Separate forward and ctx):\",\"@staticmethod def forward(*args: Any, **kwargs: Any) -> Any: pass @staticmethod def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -> None: pass\",\"The forward no longer accepts a ctx argument.\",\"Instead, you must also override the torch.autograd.Function.setup_context() staticmethod to handle setting up the ctx object. output is the output of the forward, inputs are a Tuple of inputs to the forward.\",\"See extending-autograd for more details\",\"The context can be used to store arbitrary data that can be then retrieved during the backward pass. Tensors should not be stored directly on ctx (though this is not currently enforced for backward compatibility). Instead, tensors should be saved either with ctx.save_for_backward() if they are intended to be used in backward (equivalently, vjp) or ctx.save_for_forward() if they are intended to be used for in jvp.\"]},\"774\":{\"h\":\"espnet2.asr.encoder.longformer_encoder.LongformerEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.longformer_encoder.LongformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str = 'conv2d', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'abs_pos', selfattention_layer_type: str = 'lf_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, padding_idx: int = -1, interctc_layer_idx: List[int] = [], interctc_use_conditioning: bool = False, attention_windows: list = [100, 100, 100, 100, 100, 100], attention_dilation: list = [1, 1, 1, 1, 1, 1], attention_mode: str = 'sliding_chunks')\",\"Bases: ConformerEncoder\",\"Longformer SA Conformer encoder module.\",\"Parameters:\",\"input_size (int) – Input dimension.\",\"output_size (int) – Dimension of attention.\",\"attention_heads (int) – The number of heads of multi head attention.\",\"linear_units (int) – The number of units of position-wise feed forward.\",\"num_blocks (int) – The number of decoder blocks.\",\"dropout_rate (float) – Dropout rate.\",\"attention_dropout_rate (float) – Dropout rate in attention.\",\"positional_dropout_rate (float) – Dropout rate after adding positional encoding.\",\"input_layer (Union *[*str,torch.nn.Module]) – Input layer type.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. If True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) If False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type (str) – “linear”, “conv1d”, or “conv1d-linear”.\",\"positionwise_conv_kernel_size (int) – Kernel size of positionwise conv1d layer.\",\"rel_pos_type (str) – Whether to use the latest relative positional encoding or the legacy one. The legacy relative positional encoding will be deprecated in the future. More Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"encoder_pos_enc_layer_type (str) – Encoder positional encoding layer type.\",\"encoder_attn_layer_type (str) – Encoder attention layer type.\",\"activation_type (str) – Encoder activation function type.\",\"macaron_style (bool) – Whether to use macaron style for positionwise layer.\",\"use_cnn_module (bool) – Whether to use convolution module.\",\"zero_triu (bool) – Whether to zero the upper triangular part of attention matrix.\",\"cnn_module_kernel (int) – Kernerl size of convolution module.\",\"padding_idx (int) – Padding idx for input_layer=embed.\",\"attention_windows (list) – Layer-wise attention window sizes for longformer self-attn\",\"attention_dilation (list) – Layer-wise attention dilation sizes for longformer self-attn\",\"attention_mode (str) – Implementation for longformer self-attn. Default=”sliding_chunks” Choose ‘n2’, ‘tvm’ or ‘sliding_chunks’. More details in https://github.com/allenai/longformer\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, ctc: CTC | None = None, return_all_hs: bool = False) → Tuple[Tensor, Tensor, Tensor | None]\",\"Calculate forward propagation.\",\"Parameters:\",\"xs_pad (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"ilens (torch.Tensor) – Input length (#batch).\",\"prev_states (torch.Tensor) – Not to be used now.\",\"ctc (CTC) – ctc module for intermediate CTC loss\",\"return_all_hs (bool) – whether to return all hidden states\",\"Returns: Output tensor (#batch, L, output_size). torch.Tensor: Output length (#batch). torch.Tensor: Not to be used now.\",\"Return type: torch.Tensor\",\"output_size() → int\"]},\"775\":{\"h\":\"espnet2.asr.decoder.mlm_decoder.MLMDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.mlm_decoder.MLMDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False)\",\"Bases: AbsDecoder\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor) → Tuple[Tensor, Tensor]\",\"Forward decoder.\",\"Parameters:\",\"hs_pad – encoded memory, float32 (batch, maxlen_in, feat)\",\"hlens – (batch)\",\"ys_in_pad – input token ids, int64 (batch, maxlen_out) if input_layer == “embed” input tensor (batch, maxlen_out, #mels) in the other cases\",\"ys_in_lens – (batch)\",\"Returns: tuple containing: x: decoded token score before softmax (batch, maxlen_out, token)\",\"if use_output_layer is True,\",\"olens: (batch, )\",\"Return type: (tuple)\"]},\"776\":{\"h\":\"espnet2.asr.maskctc_model.MaskCTCInference\",\"t\":[\"source\",\"class espnet2.asr.maskctc_model.MaskCTCInference(asr_model: MaskCTCModel, n_iterations: int, threshold_probability: float)\",\"Bases: Module\",\"Mask-CTC-based non-autoregressive inference\",\"Initialize Mask-CTC inference\",\"forward(enc_out: Tensor) → List[Hypothesis]\",\"Perform Mask-CTC inference\",\"ids2text(ids: List[int])\"]},\"777\":{\"h\":\"espnet2.asr.maskctc_model.MaskCTCModel\",\"t\":[\"source\",\"class espnet2.asr.maskctc_model.MaskCTCModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, decoder: MLMDecoder, ctc: CTC, joint_network: Module | None = None, ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', sym_mask: str = '<mask>', extract_feats_in_collect_stats: bool = True)\",\"Bases: ESPnetASRModel\",\"Hybrid CTC/Masked LM Encoder-Decoder model (Mask-CTC)\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"batchify_nll(encoder_out: Tensor, encoder_out_lens: Tensor, ys_pad: Tensor, ys_pad_lens: Tensor, batch_size: int = 100)\",\"Compute negative log likelihood(nll) from transformer-decoder\",\"To avoid OOM, this fuction seperate the input into batches. Then call nll for each batch and combine and return results. :param encoder_out: (Batch, Length, Dim) :param encoder_out_lens: (Batch,) :param ys_pad: (Batch, Length) :param ys_pad_lens: (Batch,) :param batch_size: int, samples each batch contain when computing nll,\",\"you may change this to avoid OOM or increase GPU memory usage\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"nll(encoder_out: Tensor, encoder_out_lens: Tensor, ys_pad: Tensor, ys_pad_lens: Tensor) → Tensor\",\"Compute negative log likelihood(nll) from transformer-decoder\",\"Normally, this function is called in batchify_nll.\",\"Parameters:\",\"encoder_out – (Batch, Length, Dim)\",\"encoder_out_lens – (Batch,)\",\"ys_pad – (Batch, Length)\",\"ys_pad_lens – (Batch,)\"]},\"778\":{\"h\":\"espnet2.asr.frontend.melspec_torch.MelSpectrogramTorch\",\"t\":[\"source\",\"class espnet2.asr.frontend.melspec_torch.MelSpectrogramTorch(preemp: bool = True, n_fft: int = 512, log: bool = False, win_length: int = 400, hop_length: int = 160, f_min: int = 20, f_max: int = 7600, n_mels: int = 80, window_fn: str = 'hamming', mel_scale: str = 'htk', normalize: str | None = None)\",\"Bases: AbsFrontend\",\"Mel-Spectrogram using Torchaudio Implementation.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_length: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"779\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\",\"Return output length of feature dimension D.\"]},\"780\":{\"h\":\"espnet2.asr.encoder.multiconvformer_encoder.MultiConvConformerEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.multiconvformer_encoder.MultiConvConformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, cgmlp_linear_units: int = 2048, multicgmlp_type: str = 'concat_fusion', multicgmlp_kernel_sizes: int | str = '7,15,23,31', multicgmlp_merge_conv_kernel: int = 31, multicgmlp_use_non_linear: int = True, use_linear_after_conv: bool = False, gate_activation: str = 'identity', input_layer: str = 'conv2d', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'rel_pos', selfattention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, padding_idx: int = -1, interctc_layer_idx: List[int] = [], interctc_use_conditioning: bool = False, stochastic_depth_rate: float | List[float] = 0.0, layer_drop_rate: float = 0.0, max_pos_emb_len: int = 5000)\",\"Bases: AbsEncoder\",\"Multiconvformer encoder module.\",\"Link to the paper: https://arxiv.org/abs/2407.03718\",\"Parameters:\",\"input_size (int) – Input dimension.\",\"output_size (int) – Dimension of attention.\",\"attention_heads (int) – The number of heads of multi head attention.\",\"linear_units (int) – The number of units of position-wise feed forward.\",\"num_blocks (int) – The number of decoder blocks.\",\"dropout_rate (float) – Dropout rate.\",\"positional_dropout_rate (float) – Dropout rate after adding positional encoding.\",\"attention_dropout_rate (float) – Dropout rate in attention.\",\"cgmlp_linear_units (int) – The number of units used in CGMLP block.\",\"multicgmlp_type (str) – “sum”, “weighted_sum”, “concat” or “concat_fusion”.\",\"multicgmlp_kernel_sizes (str) – Comma seperated list of kernel sizes.\",\"multicgmlp_merge_conv_kernel (int) – The number of kernels used in depthwise convolution fusion in MultiCGMLP.\",\"use_linear_after_conv (bool) – Whether to use a linear layer after MultiCGMLP.\",\"gate_activation (str) – The activation function used in CGMLP gating.\",\"input_layer (Union *[*str,torch.nn.Module]) – Input layer type.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. If True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) If False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type (str) – “linear”, “conv1d”, or “conv1d-linear”.\",\"positionwise_conv_kernel_size (int) – Kernel size of positionwise conv1d layer.\",\"rel_pos_type (str) – Whether to use the latest relative positional encoding or the legacy one. The legacy relative positional encoding will be deprecated in the future. More Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"encoder_pos_enc_layer_type (str) – Encoder positional encoding layer type.\",\"encoder_attn_layer_type (str) – Encoder attention layer type.\",\"activation_type (str) – Encoder activation function type.\",\"macaron_style (bool) – Whether to use macaron style for positionwise layer.\",\"use_cnn_module (bool) – Whether to use convolution module.\",\"zero_triu (bool) – Whether to zero the upper triangular part of attention matrix.\",\"cnn_module_kernel (int) – Kernerl size of convolution module.\",\"padding_idx (int) – Padding idx for input_layer=embed.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None, ctc: CTC | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Calculate forward propagation.\",\"Parameters:\",\"xs_pad (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"ilens (torch.Tensor) – Input length (#batch).\",\"prev_states (torch.Tensor) – Not to be used now.\",\"Returns: Output tensor (#batch, L, output_size). torch.Tensor: Output length (#batch). torch.Tensor: Not to be used now.\",\"Return type: torch.Tensor\",\"output_size() → int\"]},\"781\":{\"h\":\"espnet2.asr.layers.multiconv_cgmlp.MultiConvolutionalGatingMLP\",\"t\":[\"source\",\"class espnet2.asr.layers.multiconv_cgmlp.MultiConvolutionalGatingMLP(size: int, linear_units: int, arch_type: str, kernel_sizes: str, merge_conv_kernel: int, use_non_linear: bool, dropout_rate: float, use_linear_after_conv: bool, activation, gate_activation: str)\",\"Bases: Module\",\"Convolutional Gating MLP (cgMLP).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, mask=None)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"782\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"783\":{\"h\":\"espnet2.asr.layers.multiconv_cgmlp.MultiConvolutionalSpatialGatingUnit\",\"t\":[\"source\",\"class espnet2.asr.layers.multiconv_cgmlp.MultiConvolutionalSpatialGatingUnit(size: int, arch_type: str, kernel_sizes: str, merge_conv_kernel: int, use_non_linear: bool, dropout_rate: float, use_linear_after_conv: bool, activation, gate_activation: str)\",\"Bases: Module\",\"Multi Convolutional Spatial Gating Unit (M-CSGU).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"espnet_initialization_fn()\",\"forward(x, gate_add=None)\",\"Forward method\",\"Parameters:\",\"x (torch.Tensor) – (N, T, D)\",\"gate_add (torch.Tensor) – (N, T, D/2)\",\"Returns: (N, T, D/2)\",\"Return type: out (torch.Tensor)\"]},\"784\":{\"h\":\"espnet2.asr.state_spaces.attention.MultiHeadedAttention\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.attention.MultiHeadedAttention(n_feat, n_head, dropout=0.0, transposed=False, **kwargs)\",\"Bases: SequenceModule\",\"Multi-Head Attention layer inheriting SequenceModule.\",\"Comparing default MHA module in ESPnet, this module returns additional dummy state and has step function for autoregressive inference.\",\"Parameters:\",\"n_head (int) – The number of heads.\",\"n_feat (int) – The number of features.\",\"dropout_rate (float) – Dropout rate.\",\"Construct an MultiHeadedAttention object.\",\"forward(query, memory=None, mask=None, *args, **kwargs)\",\"Compute scaled dot product attention.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"mask (torch.Tensor) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).\",\"Returns: Output tensor (#batch, time1, d_model).\",\"Return type: torch.Tensor\",\"forward_attention(value, scores, mask)\",\"Compute attention context vector.\",\"Parameters:\",\"value (torch.Tensor) – Transformed value (#batch, n_head, time2, d_k).\",\"scores (torch.Tensor) – Attention score (#batch, n_head, time1, time2).\",\"mask (torch.Tensor) – Mask (#batch, 1, time2) or (#batch, time1, time2).\",\"Returns: Transformed value (#batch, time1, d_model) : weighted by the attention score (#batch, time1, time2).\",\"Return type: torch.Tensor\",\"forward_qkv(query, key, value)\",\"Transform query, key and value.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"Returns: Transformed query tensor (#batch, n_head, time1, d_k). torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k). torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).\",\"Return type: torch.Tensor\",\"step(query, state, memory=None, mask=None, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\"]},\"785\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt.MultiblankGPURNNT\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt.MultiblankGPURNNT(sigma: float, num_big_blanks: int, minibatch: int, maxT: int, maxU: int, alphabet_size: int, workspace, big_blank_workspace, blank: int, fastemit_lambda: float, clamp: float, num_threads: int, stream)\",\"Bases: GPURNNT\",\"Helper class to launch the CUDA Kernels to compute Multi-blank\",\"Transducer Loss(https://arxiv.org/pdf/2211.03541).\",\"Parameters:\",\"sigma – Hyper-parameter related to the logit-normalization method in training multi-blank transducers.\",\"num_big_blanks – Number of big blank symbols the model has. This should not include the standard blank symbol.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V + 1 + num-big-blanks\",\"workspace – An allocated chunk of memory that will be sliced off and reshaped into required blocks used as working memory.\",\"big_blank_workspace – An allocated chunk of memory that will be sliced off and reshaped into required blocks used as working memory specifically for the multi-blank related computations.\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"num_threads – Number of OMP threads to launch.\",\"stream – Numba Cuda Stream.\",\"compute_cost_and_score(acts: Tensor, grads: Tensor | None, costs: Tensor, labels: Tensor, label_lengths: Tensor, input_lengths: Tensor) → RNNTStatus\",\"Compute both the loss and the gradients.\",\"Parameters:\",\"acts – A flattened tensor of shape [B, T, U, V+1] representing the activation matrix.\",\"grad – A flattented zero tensor of same shape as acts.\",\"costs – A zero vector of length B which will be updated inplace with the log probability costs.\",\"flat_labels – A flattened matrix of labels of shape [B, U]\",\"label_lengths – A vector of length B that contains the original lengths of the acoustic sequence.\",\"input_lengths – A vector of length B that contains the original lengths of the target sequence.\",\"Updates: : This will launch kernels that will update inline the following variables:\",\"grads: Gradients of the activation matrix wrt the costs vector.\",\"costs: Negative log likelihood of the forward variable.\",\"Returns: An enum that either represents a successful RNNT operation or failure.\",\"cost_and_grad(acts: Tensor, grads: Tensor, costs: Tensor, pad_labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\",\"score_forward(acts: Tensor, costs: Tensor, pad_labels: Tensor, label_lengths: Tensor, input_lengths: Tensor)\"]},\"786\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.MultiblankRNNTLossNumba\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.MultiblankRNNTLossNumba(blank, big_blank_durations, reduction='mean', fastemit_lambda: float = 0.0, clamp: float = -1, sigma: float = 0.0)\",\"Bases: Module\",\"Multiblank RNNT Loss Numba\",\"Parameters:\",\"blank (int) – standard blank label.\",\"big_blank_durations – list of durations for multi-blank transducer, e.g. [2, 4, 8].\",\"sigma – hyper-parameter for logit under-normalization method for training multi-blank transducers. Recommended value 0.05.\",\"https (Refer to) – //arxiv.org/pdf/2211.03541 for detailed explanations for the above parameters;\",\"reduction (string,optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: ‘mean’\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(acts, labels, act_lens, label_lens)\",\"MultiblankRNNTLossNumba Forward.\",\"log_probs: Tensor of (batch x seqLength x labelLength x outputDim) : containing output from network\",\"labels: 2 dimensional Tensor containing all the targets of : the batch with zero padded\",\"act_lens: Tensor of size (batch) containing size of each output : sequence from the network\",\"label_lens: Tensor of (batch) containing label length of each example\"]},\"787\":{\"h\":\"espnet2.asr.encoder.beats_encoder.MultiheadAttention\",\"t\":[\"source\",\"class espnet2.asr.encoder.beats_encoder.MultiheadAttention(embed_dim, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8, has_relative_attention_bias=False, num_buckets=32, max_distance=128, gru_rel_pos=False, rescale_init=False)\",\"Bases: Module\",\"Multi-headed attention.\",\"See “Attention Is All You Need” for more details.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"apply_sparse_mask(attn_weights, tgt_len: int, src_len: int, bsz: int)\",\"No op\",\"compute_bias(query_length, key_length)\",\"Compute relative position bias.\",\"forward(query, key: Tensor | None, value: Tensor | None, key_padding_mask: Tensor | None = None, incremental_state: Dict[str, Dict[str, Tensor | None]] | None = None, need_weights: bool = True, static_kv: bool = False, attn_mask: Tensor | None = None, before_softmax: bool = False, need_head_weights: bool = False, position_bias: Tensor | None = None) → Tuple[Tensor, Tensor | None, Tensor | None]\",\"Input shape: Time x Batch x Channel\",\"Parameters:\",\"key_padding_mask (ByteTensor,optional) – mask to exclude keys that are pads, of shape (batch, src_len), where padding elements are indicated by 1s.\",\"need_weights (bool,optional) – return the attention weights, averaged over heads (default: False).\",\"attn_mask (ByteTensor,optional) – typically used to implement causal attention, where the mask prevents the attention from looking forward in time (default: None).\",\"before_softmax (bool,optional) – return the raw attention weights and values before the attention softmax.\",\"need_head_weights (bool,optional) – return the attention weights for each head. Implies need_weights. Default: return the average attention weights over all heads.\",\"reset_parameters()\",\"Initiate parameters in the transformer model.\"]},\"788\":{\"h\":\"espnet2.asr.state_spaces.components.Normalization\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.components.Normalization(d, transposed=False, _name_='layer', **kwargs)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"789\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"step(x, **kwargs)\"]},\"790\":{\"h\":\"espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.whisper_decoder.OpenAIWhisperDecoder(vocab_size: int, encoder_output_size: int, dropout_rate: float = 0.0, whisper_model: str = 'small', download_dir: str | None = None, load_origin_token_embedding=False)\",\"Bases: AbsDecoder, BatchScorerInterface\",\"Transformer-based Speech-to-Text Decoder from OpenAI’s Whisper Model:\",\"URL: https://github.com/openai/whisper\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor) → Tuple[Tensor, List[Any]]\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor) → Tuple[Tensor, Tensor]\",\"Forward decoder.\",\"Parameters:\",\"hs_pad – encoded memory, float32 (batch, maxlen_in, feat)\",\"hlens – (batch)\",\"ys_in_pad – input token ids, int64 (batch, maxlen_out) if input_layer == “embed” input tensor (batch, maxlen_out, #mels) in the other cases\",\"ys_in_lens – (batch)\",\"Returns: tuple containing:\",\"x: decoded token score before softmax (batch, maxlen_out, token) : if use_output_layer is True,\",\"olens: (batch, )\",\"Return type: (tuple)\",\"forward_one_step(tgt: Tensor, tgt_mask: Tensor, memory: Tensor, *, cache: List[Tensor] | None = None) → Tuple[Tensor, List[Tensor]]\",\"Forward one step.\",\"Parameters:\",\"tgt – input token ids, int64 (batch, maxlen_out)\",\"tgt_mask – input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2)\",\"memory – encoded memory, float32 (batch, maxlen_in, feat)\",\"cache – cached output list of (batch, max_time_out-1, size)\",\"Returns: NN output value and cache per self.decoders. y.shape` is (batch, maxlen_out, token)\",\"Return type: y, cache\",\"NOTE (Shih-Lun): : cache implementation is ignored for now for simplicity & correctness\",\"score(ys, state, x)\",\"Score.\"]},\"791\":{\"h\":\"espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.whisper_encoder.OpenAIWhisperEncoder(input_size: int = 1, dropout_rate: float = 0.0, whisper_model: str = 'small', download_dir: str | None = None, use_specaug: bool = False, specaug_conf: dict | None = None, do_pad_trim: bool = False)\",\"Bases: AbsEncoder\",\"Transformer-based Speech Encoder from OpenAI’s Whisper Model:\",\"URL: https://github.com/openai/whisper\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"792\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"log_mel_spectrogram(audio: Tensor, ilens: Tensor | None = None) → Tensor\",\"Use log-mel spectrogram computation native to Whisper training\",\"output_size() → int\",\"pad_or_trim(array: Tensor, length: int, axis: int = -1) → Tensor\",\"Pad or trim the audio array to N_SAMPLES.\",\"Used in zero-shot inference cases.\",\"whisper_encode(input: Tensor, ilens: Tensor | None = None) → Tensor\"]},\"793\":{\"h\":\"espnet2.asr.state_spaces.s4.OptimModule\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.s4.OptimModule(*args, **kwargs)\",\"Bases: Module\",\"Interface for Module that allows registering buffers/parameters with configurable optimizer hyperparameters. # noqa\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"register(name, tensor, lr=None)\",\"Register a tensor with a configurable learning rate and 0 weight decay.\"]},\"794\":{\"h\":\"espnet2.asr.pit_espnet_model.PITLossWrapper\",\"t\":[\"source\",\"class espnet2.asr.pit_espnet_model.PITLossWrapper(criterion_fn: Callable, num_ref: int)\",\"Bases: AbsLossWrapper\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inf: Tensor, inf_lens: Tensor, ref: Tensor, ref_lens: Tensor, others: Dict | None = None)\",\"PITLoss Wrapper function. Similar to espnet2/enh/loss/wrapper/pit_solver.py\",\"Parameters:\",\"inf – Iterable[torch.Tensor], (batch, num_inf, …)\",\"inf_lens – Iterable[torch.Tensor], (batch, num_inf, …)\",\"ref – Iterable[torch.Tensor], (batch, num_ref, …)\",\"ref_lens – Iterable[torch.Tensor], (batch, num_ref, …)\",\"permute_inf – If true, permute the inference and inference_lens according to the optimal permutation.\",\"classmethod permutate(perm, *args)\"]},\"795\":{\"h\":\"espnet2.asr.partially_AR_model.PartiallyARInference\",\"t\":[\"source\",\"class espnet2.asr.partially_AR_model.PartiallyARInference(ctc: CTC, decoder: AbsDecoder, threshold_probability: float, sos: int | None = None, eos: int | None = None, mask_token: int | None = None, token_list: List[int] | None = None, scorers: Dict[str, ScorerInterface] | None = None, weights: Dict[str, float] | None = None, beam_size: int = 10, max_seq_len: int = 5, max_mask_parallel: int = -1)\",\"Bases: Module\",\"Mask-CTC-based partially autoregressive inference\",\"Initialize Mask-CTC inference\",\"forward(enc_out: Tensor, *args, **kwargs) → List[Hypothesis]\",\"Perform Semi-AR inference\",\"set_hyp_primer(primer: List[int])\"]},\"796\":{\"h\":\"espnet2.asr.decoder.rnn_decoder.RNNDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.rnn_decoder.RNNDecoder(vocab_size: int, encoder_output_size: int, rnn_type: str = 'lstm', num_layers: int = 1, hidden_size: int = 320, sampling_probability: float = 0.0, dropout: float = 0.0, context_residual: bool = False, replace_sos: bool = False, num_encs: int = 1, att_conf: dict = {'aconv_chans': 10, 'aconv_filts': 100, 'adim': 320, 'aheads': 4, 'atype': 'location', 'awin': 5, 'han_conv_chans': -1, 'han_conv_filts': 100, 'han_dim': 320, 'han_heads': 4, 'han_mode': False, 'han_type': None, 'han_win': 5, 'num_att': 1, 'num_encs': 1})\",\"Bases: AbsDecoder\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(hs_pad, hlens, ys_in_pad, ys_in_lens, strm_idx=0)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"797\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"init_state(x)\",\"Get an initial state for decoding (optional).\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"rnn_forward(ey, z_list, c_list, z_prev, c_prev)\",\"score(yseq, state, x)\",\"Score new token (required).\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – The encoder feature that generates ys.\",\"Returns: Tuple of : scores for next token that has a shape of (n_vocab) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"zero_state(hs_pad)\"]},\"798\":{\"h\":\"espnet2.asr.encoder.rnn_encoder.RNNEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.rnn_encoder.RNNEncoder(input_size: int, rnn_type: str = 'lstm', bidirectional: bool = True, use_projection: bool = True, num_layers: int = 4, hidden_size: int = 320, output_size: int = 320, dropout: float = 0.0, subsample: Sequence[int] | None = (2, 2, 1, 1))\",\"Bases: AbsEncoder\",\"RNNEncoder class.\",\"Parameters:\",\"input_size – The number of expected features in the input\",\"output_size – The number of output features\",\"hidden_size – The number of hidden features\",\"bidirectional – If True becomes a bidirectional LSTM\",\"use_projection – Use projection layer or not\",\"num_layers – Number of recurrent layers\",\"dropout – dropout probability\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"799\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\"]},\"800\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.RNNTLossNumba\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.RNNTLossNumba(blank=0, reduction='mean', fastemit_lambda: float = 0.0, clamp: float = -1)\",\"Bases: Module\",\"RNNT Loss Numba\",\"Parameters:\",\"blank (int,optional) – blank label. Default: 0.\",\"reduction (string,optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: ‘mean’\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(acts, labels, act_lens, label_lens)\",\"Forward RNNTLossNumba.\",\"log_probs: Tensor of (batch x seqLength x labelLength x outputDim) : containing output from network\",\"labels: 2 dimensional Tensor containing all the targets of the : batch with zero padded\",\"act_lens: Tensor of size (batch) containing size of each output : sequence from the network\",\"label_lens: Tensor of (batch) containing label length of each example\"]},\"801\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.RNNTStatus\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.RNNTStatus(value)\",\"Bases: Enum\",\"An enumeration.\",\"RNNT_STATUS_INVALID_VALUE = 1\",\"RNNT_STATUS_SUCCESS = 0\"]},\"802\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.R_Op\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.R_Op(value)\",\"Bases: Enum\",\"Represents a reduction operation performed on the input tensor\",\"ADD = 0\",\"MAXIMUM = 1\"]},\"803\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.ReduceHelper\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.ReduceHelper(I_opid: int, R_opid: int, acts: Tensor, output: Tensor, num_rows: int, num_cols: int, minus: bool, stream)\",\"CUDA Warp reduction kernel helper which reduces via the R_Op.Add and writes\",\"the result to output according to I_op id.\",\"The result is stored in the blockIdx.\"]},\"804\":{\"h\":\"NOTE\",\"t\":[\"Efficient warp occurs at input shapes of 2 ^ K.\",\"References\",\"Warp Primitives [https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/]\",\"Parameters:\",\"I_opid – Operator ID for input. See I_Op for more information.\",\"R_opid – Operator ID for reduction. See R_Op for more information.\",\"acts – Flatened activation matrix of shape [B * T * U * (V+1)].\",\"output – Flatened output matrix of shape [B * T * U * (V+1)]. Data will be overwritten.\",\"num_rows – Vocabulary size (including blank token) - V+1. Represents the number of threads per block.\",\"num_cols – Flattened shape of activation matrix, without vocabulary dimension (B * T * U). Represents number of blocks per grid.\",\"minus – Bool flag whether to add or subtract as reduction. If minus is set; calls _reduce_minus, else calls _reduce_rows kernel.\",\"stream – CUDA Stream.\"]},\"805\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.ResEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.avhubert_encoder.ResEncoder(relu_type, weights)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"806\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"threeD_to_2D_tensor(x)\"]},\"807\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.ResNet\",\"t\":[\"source\",\"class espnet2.asr.encoder.avhubert_encoder.ResNet(block, layers, num_classes=1000, relu_type='relu', gamma_zero=False, avg_pool_downsample=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"808\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"809\":{\"h\":\"espnet2.asr.state_spaces.residual.Residual\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.residual.Residual(i_layer, d_input, d_model, alpha=1.0, beta=1.0)\",\"Bases: Module\",\"Residual connection with constant affine weights.\",\"Can simulate standard residual, no residual, and “constant gates”.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"forward(x, y, transposed)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"810\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"811\":{\"h\":\"espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dInput\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dInput(d, transposed=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"812\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"813\":{\"h\":\"espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dOutput\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.components.ReversibleInstanceNorm1dOutput(norm_input)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"814\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"815\":{\"h\":\"espnet2.asr.frontend.s3prl.S3prlFrontend\",\"t\":[\"source\",\"class espnet2.asr.frontend.s3prl.S3prlFrontend(fs: int | str = 16000, frontend_conf: dict | None = {'badim': 320, 'bdropout_rate': 0.0, 'blayers': 3, 'bnmask': 2, 'bprojs': 320, 'btype': 'blstmp', 'bunits': 300, 'delay': 3, 'ref_channel': -1, 'taps': 5, 'use_beamformer': False, 'use_dnn_mask_for_wpe': True, 'use_wpe': False, 'wdropout_rate': 0.0, 'wlayers': 3, 'wprojs': 320, 'wtype': 'blstmp', 'wunits': 300}, download_dir: str | None = None, multilayer_feature: bool = False, layer: int = -1)\",\"Bases: AbsFrontend\",\"Speech Pretrained Representation frontend structure for ASR.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"816\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\",\"reload_pretrained_parameters()\"]},\"817\":{\"h\":\"espnet2.asr.state_spaces.s4.S4\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.s4.S4(d_model, d_state=64, l_max=None, channels=1, bidirectional=False, activation='gelu', postact='glu', hyper_act=None, dropout=0.0, tie_dropout=False, bottleneck=None, gate=None, transposed=True, verbose=False, **kernel_args)\",\"Bases: Module\",\"Initialize S4 module.\",\"d_state: the dimension of the state, also denoted by N l_max: the maximum kernel length, also denoted by L.\",\"Set l_max=None to always use a global kernel\",\"channels: can be interpreted as a number of “heads”; : the SSM is a map from a 1-dim to C-dim sequence. It’s not recommended to change this unless desperate for things to tune; instead, increase d_model for larger models\",\"bidirectional: if True, convolution kernel will be two-sided\"]},\"818\":{\"h\":\"Position-wise feedforward components:\",\"t\":[\"activation: activation in between SS and FF postact: activation after FF hyper_act: use a “hypernetwork” multiplication (experimental) dropout: standard dropout argument. tie_dropout=True ties the dropout\",\"mask across the sequence length, emulating nn.Dropout1d\"]},\"819\":{\"h\":\"Other arguments:\",\"t\":[\"transposed: choose backbone axis ordering of : (B, L, H) (if False) or (B, H, L) (if True) [B=batch size, L=sequence length, H=hidden dimension]\",\"gate: add gated activation (GSS) bottleneck: reduce SSM dimension (GSS)\",\"See the class SSKernel for the kernel constructor which accepts kernel_args. Relevant options that are worth considering and tuning include “mode” + “measure”, “dt_min”, “dt_max”, “lr”\",\"Other options are all experimental and should not need to be configured\",\"property d_output\",\"default_state(*batch_shape, device=None)\",\"forward(u, state=None, rate=1.0, lengths=None, **kwargs)\",\"Forward pass.\",\"u: (B H L) if self.transposed else (B L H) state: (H N) never needed unless you know what you’re doing\",\"Returns: same shape as u\",\"setup_step(**kwargs)\",\"step(u, state, **kwargs)\",\"Step one time step as a recurrent model.\",\"Intended to be used during validation.\",\"u: (B H) state: (B H N) Returns: output (B H), state (B H N)\"]},\"820\":{\"h\":\"espnet2.asr.decoder.s4_decoder.S4Decoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.s4_decoder.S4Decoder(vocab_size: int, encoder_output_size: int, input_layer: str = 'embed', dropinp: float = 0.0, dropout: float = 0.25, prenorm: bool = True, n_layers: int = 16, transposed: bool = False, tie_dropout: bool = False, n_repeat=1, layer=None, residual=None, norm=None, pool=None, track_norms=True, drop_path: float = 0.0)\",\"Bases: AbsDecoder, BatchScorerInterface\",\"S4 decoder module.\",\"Parameters:\",\"vocab_size – output dim\",\"encoder_output_size – dimension of hidden vector\",\"input_layer – input layer type\",\"dropinp – input dropout\",\"dropout – dropout parameter applied on every residual and every layer\",\"prenorm – pre-norm vs. post-norm\",\"n_layers – number of layers\",\"transposed – transpose inputs so each layer receives (batch, dim, length)\",\"tie_dropout – tie dropout mask across sequence like nn.Dropout1d/nn.Dropout2d\",\"n_repeat – each layer is repeated n times per stage before applying pooling\",\"layer – layer config, must be specified\",\"residual – residual config\",\"norm – normalization config (e.g. layer vs batch)\",\"pool – config for pooling layer per stage\",\"track_norms – log norms of each layer output\",\"drop_path – drop rate for stochastic depth\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor) → Tuple[Tensor, List[Any]]\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor, state=None) → Tuple[Tensor, Tensor]\",\"Forward decoder.\",\"Parameters:\",\"hs_pad – encoded memory, float32 (batch, maxlen_in, feat)\",\"hlens – (batch)\",\"ys_in_pad – input token ids, int64 (batch, maxlen_out) if input_layer == “embed” input tensor (batch, maxlen_out, #mels) in the other cases\",\"ys_in_lens – (batch)\",\"Returns: tuple containing:\",\"x: decoded token score before softmax (batch, maxlen_out, token) : if use_output_layer is True,\",\"olens: (batch, )\",\"Return type: (tuple)\",\"init_state(x: Tensor)\",\"Initialize state.\",\"score(ys, state, x)\",\"Score new token (required).\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – The encoder feature that generates ys.\",\"Returns: Tuple of : scores for next token that has a shape of (n_vocab) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\"]},\"821\":{\"h\":\"espnet2.asr.state_spaces.s4.SSKernel\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.s4.SSKernel(H, N=64, L=None, measure='legs', rank=1, channels=1, dt_min=0.001, dt_max=0.1, deterministic=False, lr=None, mode='nplr', n_ssm=None, verbose=False, measure_args={}, **kernel_args)\",\"Bases: Module\",\"Wrapper around SSKernel parameterizations.\",\"The SSKernel is expected to support the interface forward() default_state() _setup_step() step()\",\"State Space Kernel which computes the convolution kernel $\\\\bar{K}$.\",\"H: Number of independent SSM copies; : controls the size of the model. Also called d_model in the config.\",\"N: State size (dimensionality of parameters A, B, C). : Also called d_state in the config. Generally shouldn’t need to be adjusted and doens’t affect speed much.\",\"L: Maximum length of convolution kernel, if known. : Should work in the majority of cases even if not known.\",\"measure: Options for initialization of (A, B). : For NPLR mode, recommendations are “legs”, “fout”, “hippo” (combination of both). For Diag mode, recommendations are “diag-inv”, “diag-lin”, “diag-legs”, and “diag” (combination of diag-inv and diag-lin)\",\"rank: Rank of low-rank correction for NPLR mode. : Needs to be increased for measure “legt”\",\"channels: C channels turns the SSM from a 1-dim to C-dim map; : can think of it having C separate “heads” per SSM. This was partly a feature to make it easier to implement bidirectionality; it is recommended to set channels=1 and adjust H to control parameters instead\",\"dt_min, dt_max: min and max values for the step size dt (Delta) mode: Which kernel algorithm to use. ‘nplr’ is the full S4 model;\",\"‘diag’ is the simpler S4D; ‘slow’ is a dense version for testing\",\"n_ssm: Number of independent trainable (A, B) SSMs, : e.g. n_ssm=1 means all A/B parameters are tied across the H different instantiations of C. n_ssm=None means all H SSMs are completely independent. Generally, changing this option can save parameters but doesn’t affect performance or speed much. This parameter must divide H\",\"lr: Passing in a number (e.g. 0.001) sets : attributes of SSM parameers (A, B, dt). A custom optimizer hook is needed to configure the optimizer to set the learning rates appropriately for these parameters.\",\"default_state(*args, **kwargs)\",\"forward(state=None, L=None, rate=None)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"822\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_state(u, state)\",\"Forward the state through a sequence.\",\"i.e. computes the state after passing chunk through SSM\",\"state: (B, H, N) u: (B, H, L)\",\"Returns: (B, H, N)\",\"step(u, state, **kwargs)\"]},\"823\":{\"h\":\"espnet2.asr.state_spaces.s4.SSKernelDiag\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.s4.SSKernelDiag(A, B, C, log_dt, L=None, disc='bilinear', real_type='exp', lr=None, bandlimit=None)\",\"Bases: OptimModule\",\"Version using (complex) diagonal state matrix (S4D).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"default_state(*batch_shape)\",\"forward(L, state=None, rate=1.0, u=None)\",\"Forward pass.\",\"state: (B, H, N) initial state rate: sampling rate factor L: target length\",\"returns: (C, H, L) convolution kernel (generally C=1) (B, H, L) output from initial state\",\"forward_state(u, state)\",\"step(u, state)\"]},\"824\":{\"h\":\"espnet2.asr.state_spaces.s4.SSKernelNPLR\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.s4.SSKernelNPLR(w, P, B, C, log_dt, L=None, lr=None, verbose=False, keops=False, real_type='exp', real_tolerance=0.001, bandlimit=None)\",\"Bases: OptimModule\",\"Stores a representation of and computes the SSKernel function.\",\"K_L(A^dt, B^dt, C) corresponding to a discretized state space, where A is Normal + Low Rank (NPLR)\",\"Initialize kernel.\",\"L: Maximum length; this module computes an SSM kernel of length L A is represented by diag(w) - PP^* w: (S, N) diagonal part P: (R, S, N) low-rank part\",\"B: (S, N) C: (C, H, N) dt: (H) timescale per feature lr: [dict | float | None] hook to set lr of special parameters (A, B, dt)\",\"Dimensions: N (or d_state): state size H (or d_model): total SSM copies S (or n_ssm): number of trainable copies of (A, B, dt); must divide H R (or rank): rank of low-rank part C (or channels): system is 1-dim to C-dim\",\"The forward pass of this Module returns a tensor of shape (C, H, L)\",\"Note: tensor shape N here denotes half the true state size, : because of conjugate symmetry\",\"default_state(*batch_shape)\",\"forward(state=None, rate=1.0, L=None)\",\"Forward pass.\",\"state: (B, H, N) initial state rate: sampling rate factor L: target length\",\"returns: (C, H, L) convolution kernel (generally C=1) (B, H, L) output from initial state\",\"step(u, state)\",\"Step one time step as a recurrent model.\",\"Must have called self._setup_step() and created state with self.default_state() before calling this\"]},\"825\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.SamePad\",\"t\":[\"source\",\"class espnet2.asr.encoder.avhubert_encoder.SamePad(kernel_size, causal=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"826\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"827\":{\"h\":\"espnet2.asr.state_spaces.base.SequenceIdentity\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.base.SequenceIdentity(*args, transposed=False, **kwargs)\",\"Bases: SequenceIdentity\",\"Simple SequenceModule for testing purposes.\",\"Initialize SequenceModule.\",\"d_model: input dimension (sometimes denoted H for hidden dimension) transposed: if True, inputs have axis ordering (B, H, L) instead of (B, H, L)\",\"forward(x, state=None, **kwargs)\",\"Forward pass.\"]},\"828\":{\"h\":\"espnet2.asr.state_spaces.model.SequenceModel\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.model.SequenceModel(d_model, n_layers=1, transposed=False, dropout=0.0, tie_dropout=False, prenorm=True, n_repeat=1, layer=None, residual=None, norm=None, pool=None, track_norms=True, dropinp=0.0, drop_path=0.0)\",\"Bases: SequenceModule\",\"Isotropic deep sequence model backbone, in the style of ResNets / Transformers.\",\"The SequenceModel class implements a generic (batch, length, d_input) -> (batch, length, d_output) transformation\",\"Parameters:\",\"d_model – Resize input (useful for deep models with residuals)\",\"n_layers – Number of layers\",\"transposed – Transpose inputs so each layer receives (batch, dim, length)\",\"dropout – Dropout parameter applied on every residual and every layer\",\"tie_dropout – Tie dropout mask across sequence like nn.Dropout1d/nn.Dropout2d\",\"prenorm – Pre-norm vs. post-norm\",\"n_repeat – Each layer is repeated n times per stage before applying pooling\",\"layer – Layer config, must be specified\",\"residual – Residual config\",\"norm – Normalization config (e.g. layer vs batch)\",\"pool – Config for pooling layer per stage\",\"track_norms – Log norms of each layer output\",\"dropinp – Input dropout\",\"drop_path – Stochastic depth for each residual path\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_state\",\"Return dimension of output of self.state_to_tensor.\",\"default_state(*batch_shape, device=None)\",\"Create initial state for a batch of inputs.\",\"forward(inputs, *args, state=None, **kwargs)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"property state_to_tensor\",\"Return a function mapping a state to a single tensor.\",\"This method should be implemented if one wants to use the hidden state insteadof the output sequence for final prediction. Currently only used with the StateDecoder.\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\"]},\"829\":{\"h\":\"espnet2.asr.state_spaces.base.SequenceModule\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.base.SequenceModule(*args, **kwargs)\",\"Bases: Module\",\"Abstract sequence model class.\",\"All models must adhere to this interface\",\"A SequenceModule is generally a model that transforms an input of shape (n_batch, l_sequence, d_model) to (n_batch, l_sequence, d_output)\",\"REQUIRED methods and attributes forward, d_model, d_output: controls standard forward pass, a sequence-to-sequence transformation __init__ should also satisfy the following interface; see SequenceIdentity for an example\",\"def __init__(self, d_model, transposed=False,\",\"**\",\"kwargs)\",\"OPTIONAL methods default_state, step: allows stepping the model recurrently with a hidden state state_to_tensor, d_state: allows decoding from hidden state\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_model\",\"Model dimension (generally same as input dimension).\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, encoder) to track the internal shapes of the full model.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"property d_state\",\"Return dimension of output of self.state_to_tensor.\",\"default_state(*batch_shape, device=None)\",\"Create initial state for a batch of inputs.\",\"forward(x, state=None, **kwargs)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"property state_to_tensor\",\"Return a function mapping a state to a single tensor.\",\"This method should be implemented if one wants to use the hidden state insteadof the output sequence for final prediction. Currently only used with the StateDecoder.\",\"step(x, state=None, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\"]},\"830\":{\"h\":\"espnet2.asr.state_spaces.block.SequenceResidualBlock\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.block.SequenceResidualBlock(d_input, i_layer=None, prenorm=True, dropout=0.0, tie_dropout=False, transposed=False, layer=None, residual=None, norm=None, pool=None, drop_path=0.0)\",\"Bases: SequenceModule\",\"Residual block wrapper for black box layer.\",\"The SequenceResidualBlock class implements a generic (batch, length, d_input) -> (batch, length, d_input) transformation\",\"Parameters:\",\"d_input – Input feature dimension\",\"i_layer – Layer index, only needs to be passed into certain residuals like Decay\",\"dropout – Dropout for black box module\",\"tie_dropout – Tie dropout mask across sequence like nn.Dropout1d/nn.Dropout2d\",\"transposed – Transpose inputs so each layer receives (batch, dim, length)\",\"layer – Config for black box module\",\"residual – Config for residual function\",\"norm – Config for normalization layer\",\"pool – Config for pooling layer per stage\",\"drop_path – Drop ratio for stochastic depth\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"property d_state\",\"Return dimension of output of self.state_to_tensor.\",\"default_state(*args, **kwargs)\",\"Create initial state for a batch of inputs.\",\"forward(x, state=None, **kwargs)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"property state_to_tensor\",\"Return a function mapping a state to a single tensor.\",\"This method should be implemented if one wants to use the hidden state insteadof the output sequence for final prediction. Currently only used with the StateDecoder.\",\"step(x, state, **kwargs)\",\"Step the model recurrently for one step of the input sequence.\",\"For example, this should correspond to unrolling an RNN for one step. If the forward pass has signature (B, L, H1) -> (B, L, H2), this method should generally have signature (B, H1) -> (B, H2) with an optional recurrent state.\"]},\"831\":{\"h\":\"espnet2.asr.frontend.windowing.SlidingWindow\",\"t\":[\"source\",\"class espnet2.asr.frontend.windowing.SlidingWindow(win_length: int = 400, hop_length: int = 160, channels: int = 1, padding: int | None = None, fs=None)\",\"Bases: AbsFrontend\",\"Sliding Window.\",\"Provides a sliding window over a batched continuous raw audio tensor. Optionally, provides padding (Currently not implemented). Combine this module with a pre-encoder compatible with raw audio data, for example Sinc convolutions.\",\"Known issues: Output length is calculated incorrectly if audio shorter than win_length. WARNING: trailing values are discarded - padding not implemented yet. There is currently no additional window function applied to input values.\",\"Initialize.\",\"Parameters:\",\"win_length – Length of frame.\",\"hop_length – Relative starting point of next frame.\",\"channels – Number of input channels.\",\"padding – Padding (placeholder, currently not implemented).\",\"fs – Sampling rate (placeholder for compatibility, not used).\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Apply a sliding window on the input.\",\"Parameters:\",\"input – Input (B, T, C*D) or (B, T*C*D), with D=C=1.\",\"input_lengths – Input lengths within batch.\",\"Returns: Output with dimensions (B, T, C, D), with D=win_length. Tensor: Output lengths within batch.\",\"Return type: Tensor\",\"output_size() → int\",\"Return output length of feature dimension D, i.e. the window length.\"]},\"832\":{\"h\":\"espnet2.asr.preencoder.sinc.SpatialDropout\",\"t\":[\"source\",\"class espnet2.asr.preencoder.sinc.SpatialDropout(dropout_probability: float = 0.15, shape: tuple | list | None = None)\",\"Bases: Module\",\"Spatial dropout module.\",\"Apply dropout to full channels on tensors of input (B, C, D)\",\"Initialize.\",\"Parameters:\",\"dropout_probability – Dropout probability.\",\"shape (tuple,list) – Shape of input tensors.\",\"forward(x: Tensor) → Tensor\",\"Forward of spatial dropout module.\"]},\"833\":{\"h\":\"espnet2.asr.specaug.specaug.SpecAug\",\"t\":[\"source\",\"class espnet2.asr.specaug.specaug.SpecAug(apply_time_warp: bool = True, time_warp_window: int = 5, time_warp_mode: str = 'bicubic', apply_freq_mask: bool = True, freq_mask_width_range: int | Sequence[int] = (0, 20), num_freq_mask: int = 2, apply_time_mask: bool = True, time_mask_width_range: int | Sequence[int] | None = None, time_mask_width_ratio_range: float | Sequence[float] | None = None, num_time_mask: int = 2, replace_with_zero: bool = True)\",\"Bases: AbsSpecAug\",\"Implementation of SpecAug.\",\"Reference: : Daniel S. Park et al. “SpecAugment: A Simple Data <br/>\",\"Augmentation Method for Automatic Speech Recognition”\",\"WARNING\",\"When using cuda mode, time_warp doesn’t have reproducibility due to torch.nn.functional.interpolate.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, x_lengths=None)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"834\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"835\":{\"h\":\"espnet2.asr.state_spaces.components.SquaredReLU\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.components.SquaredReLU(*args, **kwargs)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"836\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"837\":{\"h\":\"espnet2.asr.state_spaces.components.StochasticDepth\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.components.StochasticDepth(p: float, mode: str)\",\"Bases: Module\",\"Stochastic depth module.\",\"See stochastic_depth().\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"838\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"839\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.SubModel\",\"t\":[\"source\",\"class espnet2.asr.encoder.avhubert_encoder.SubModel(resnet=None, input_dim=None, cfg=None)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"840\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"841\":{\"h\":\"espnet2.asr.encoder.beats_encoder.Swish\",\"t\":[\"source\",\"class espnet2.asr.encoder.beats_encoder.Swish\",\"Bases: Module\",\"Swish activation function\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward pass\"]},\"842\":{\"h\":\"espnet2.asr.state_spaces.components.TSInverseNormalization\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.components.TSInverseNormalization(method, normalizer)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"843\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"844\":{\"h\":\"espnet2.asr.state_spaces.components.TSNormalization\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.components.TSNormalization(method, horizon)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"845\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"846\":{\"h\":\"espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.hubert_encoder.TorchAudioHuBERTPretrainEncoder(input_size: int | None = None, extractor_mode: str = 'group_norm', extractor_conv_layer_config: List[List[int]] | None = [[512, 10, 5], [512, 3, 2], [512, 3, 2], [512, 3, 2], [512, 3, 2], [512, 2, 2], [512, 2, 2]], extractor_conv_bias: bool = False, encoder_embed_dim: int = 768, encoder_projection_dropout: float = 0.1, encoder_pos_conv_kernel: int = 128, encoder_pos_conv_groups: int = 16, encoder_num_layers: int = 12, encoder_num_heads: int = 12, encoder_attention_dropout: float = 0.1, encoder_ff_interm_features: int = 3072, encoder_ff_interm_dropout: float = 0.0, encoder_dropout: float = 0.1, encoder_layer_norm_first: bool = False, encoder_layer_drop: float = 0.05, mask_prob: float = 0.8, mask_selection: str = 'static', mask_other: float = 0.0, mask_length: int = 10, no_mask_overlap: bool = False, mask_min_space: int = 1, mask_channel_prob: float = 0.0, mask_channel_selection: str = 'static', mask_channel_other: float = 0.0, mask_channel_length: int = 10, no_mask_channel_overlap: bool = False, mask_channel_min_space: int = 1, skip_masked: bool = False, skip_nomask: bool = False, num_classes: int = 100, final_dim: int = 256, feature_grad_mult: float | None = 0.1, finetuning: bool = False, freeze_encoder_updates: int = 0)\",\"Bases: AbsEncoder\",\"Torch Audio Hubert encoder module.\",\"Parameters:\",\"extractor_mode – Operation mode of feature extractor. Valid values are “group_norm” or “layer_norm”.\",\"extractor_conv_layer_config – Configuration of convolution layers in feature extractor. List of convolution configuration, i.e. [[output_channel, kernel_size, stride], …]\",\"extractor_conv_bias – Whether to include bias term to each convolution operation.\",\"encoder_embed_dim – The dimension of embedding in encoder.\",\"encoder_projection_dropout – The dropout probability applied after the input feature is projected to “encoder_embed_dim”.\",\"encoder_pos_conv_kernel – Kernel size of convolutional positional embeddings.\",\"encoder_pos_conv_groups – Number of groups of convolutional positional embeddings.\",\"encoder_num_layers – Number of self attention layers in transformer block.\",\"encoder_num_heads – Number of heads in self attention layers.\",\"encoder_attention_dropout – Dropout probability applied after softmax in self-attention layer.\",\"encoder_ff_interm_features – Dimension of hidden features in feed forward layer.\",\"encoder_ff_interm_dropout – Dropout probability applied in feedforward layer.\",\"encoder_dropout – Dropout probability applied at the end of feed forward layer.\",\"encoder_layer_norm_first – Control the order of layer norm in transformer layer and each encoder layer. If True, in transformer layer, layer norm is applied before features are fed to encoder layers.\",\"encoder_layer_drop – Probability to drop each encoder layer during training.\",\"mask_prob – Probability for each token to be chosen as start of the span to be masked.\",\"mask_selection – How to choose the mask length. Options: [static, uniform, normal, poisson].\",\"mask_other – Secondary mask argument (used for more complex distributions).\",\"mask_length – The lengths of the mask.\",\"no_mask_overlap – Whether to allow masks to overlap.\",\"mask_min_space – Minimum space between spans (if no overlap is enabled).\",\"mask_channel_prob – (float): The probability of replacing a feature with 0.\",\"mask_channel_selection – How to choose the mask length for channel masking. Options: [static, uniform, normal, poisson].\",\"mask_channel_other – Secondary mask argument for channel masking(used for more complex distributions).\",\"mask_channel_length – Minimum space between spans (if no overlap is enabled) for channel masking.\",\"no_mask_channel_overlap – Whether to allow channel masks to overlap.\",\"mask_channel_min_space – Minimum space between spans for channel masking(if no overlap is enabled).\",\"skip_masked – If True, skip computing losses over masked frames.\",\"skip_nomask – If True, skip computing losses over unmasked frames.\",\"num_classes – The number of classes in the labels.\",\"final_dim – Project final representations and targets to final_dim.\",\"feature_grad_mult – The factor to scale the convolutional feature extraction layer gradients by. The scale factor will not affect the forward pass.\",\"finetuning – Whether to finetuning the model with ASR or other tasks.\",\"freeze_encoder_updates – The number of steps to freeze the encoder parameters in ASR finetuning.\",\"Hubert specific Args: : Please refer to: https://pytorch.org/audio/stable/generated/torchaudio.models.hubert_pretrain_model.html#torchaudio.models.hubert_pretrain_model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, ys_pad: Tensor | None = None, ys_pad_length: Tensor | None = None, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Forward Hubert Pretrain Encoder.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size() → int\",\"reload_pretrained_parameters()\"]},\"847\":{\"h\":\"espnet2.asr.decoder.transducer_decoder.TransducerDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.transducer_decoder.TransducerDecoder(vocab_size: int, rnn_type: str = 'lstm', num_layers: int = 1, hidden_size: int = 320, dropout: float = 0.0, dropout_embed: float = 0.0, embed_pad: int = 0)\",\"Bases: AbsDecoder\",\"(RNN-)Transducer decoder module.\",\"Parameters:\",\"vocab_size – Output dimension.\",\"layers_type – (RNN-)Decoder layers type.\",\"num_layers – Number of decoder layers.\",\"hidden_size – Number of decoder units per layer.\",\"dropout – Dropout rate for decoder layers.\",\"dropout_embed – Dropout rate for embedding layer.\",\"embed_pad – Embed/Blank symbol ID.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(hyps: List[Hypothesis] | List[ExtendedHypothesis], dec_states: Tuple[Tensor, Tensor | None], cache: Dict[str, Any], use_lm: bool) → Tuple[Tensor, Tuple[Tensor, Tensor], Tensor]\",\"One-step forward hypotheses.\",\"Parameters:\",\"hyps – Hypotheses.\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"cache – Pairs of (dec_out, dec_states) for each label sequences. (keys)\",\"use_lm – Whether to compute label ID sequences for LM.\",\"Returns: Decoder output sequences. (B, D_dec) dec_states: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec)) lm_labels: Label ID sequences for LM. (B,)\",\"Return type: dec_out\",\"create_batch_states(states: Tuple[Tensor, Tensor | None], new_states: List[Tuple[Tensor, Tensor | None]], check_list: List | None = None) → List[Tuple[Tensor, Tensor | None]]\",\"Create decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"new_states – Decoder hidden states. [N x ((1, D_dec), (1, D_dec))]\",\"Returns: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"Return type: states\",\"forward(labels: Tensor) → Tensor\",\"Encode source label sequences.\",\"Parameters:labels – Label ID sequences. (B, L)\",\"Returns: Decoder output sequences. (B, T, U, D_dec)\",\"Return type: dec_out\",\"init_state(batch_size: int) → Tuple[Tensor, tensor | None]\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Initial decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"rnn_forward(sequence: Tensor, state: Tuple[Tensor, Tensor | None]) → Tuple[Tensor, Tuple[Tensor, Tensor | None]]\",\"Encode source label sequences.\",\"Parameters:\",\"sequence – RNN input sequences. (B, D_emb)\",\"state – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"Returns: RNN output sequences. (B, D_dec) (h_next, c_next): Decoder hidden states. (N, B, D_dec), (N, B, D_dec))\",\"Return type: sequence\",\"score(hyp: Hypothesis, cache: Dict[str, Any]) → Tuple[Tensor, Tuple[Tensor, Tensor | None], Tensor]\",\"One-step forward hypothesis.\",\"Parameters:\",\"hyp – Hypothesis.\",\"cache – Pairs of (dec_out, state) for each label sequence. (key)\",\"Returns: Decoder output sequence. (1, D_dec) new_state: Decoder hidden states. ((N, 1, D_dec), (N, 1, D_dec)) label: Label ID for LM. (1,)\",\"Return type: dec_out\",\"select_state(states: Tuple[Tensor, Tensor | None], idx: int) → Tuple[Tensor, Tensor | None]\",\"Get specified ID state from decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID. : ((N, 1, D_dec), (N, 1, D_dec))\",\"set_device(device: device)\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\"]},\"848\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.TransformerDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.transformer_decoder.TransformerDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, layer_drop_rate: float = 0.0, qk_norm: bool = False, use_flash_attn: bool = True, gradient_checkpoint_layers: ~typing.List[int] = [])\",\"Bases: BaseTransformerDecoder\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"]},\"849\":{\"h\":\"espnet2.asr.encoder.transformer_encoder_multispkr.TransformerEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.transformer_encoder_multispkr.TransformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, num_blocks_sd: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d', pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, padding_idx: int = -1, num_inf: int = 1)\",\"Bases: AbsEncoder\",\"Transformer encoder module.\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of recognition encoder blocks\",\"num_blocks_sd – the number of speaker dependent encoder blocks\",\"dropout_rate – dropout rate\",\"attention_dropout_rate – dropout rate in attention\",\"positional_dropout_rate – dropout rate after adding positional encoding\",\"input_layer – input layer type\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type – linear of conv1d\",\"positionwise_conv_kernel_size – kernel size of positionwise conv1d layer\",\"padding_idx – padding_idx for input_layer=embed\",\"num_inf – number of inference output\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size() → int\"]},\"850\":{\"h\":\"espnet2.asr.decoder.transformer_decoder.TransformerMDDecoder\",\"t\":[\"source\",\"class espnet2.asr.decoder.transformer_decoder.TransformerMDDecoder(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, use_speech_attn: bool = True)\",\"Bases: BaseTransformerDecoder\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor, speech: Tensor | None = None) → Tuple[Tensor, List[Any]]\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(hs_pad: Tensor, hlens: Tensor, ys_in_pad: Tensor, ys_in_lens: Tensor, speech: Tensor | None = None, speech_lens: Tensor | None = None, return_hs: bool = False) → Tuple[Tensor, Tensor]\",\"Forward decoder.\",\"Parameters:\",\"hs_pad – encoded memory, float32 (batch, maxlen_in, feat)\",\"hlens – (batch)\",\"ys_in_pad – input token ids, int64 (batch, maxlen_out) if input_layer == “embed” input tensor (batch, maxlen_out, #mels) in the other cases\",\"ys_in_lens – (batch)\",\"return_hs – dec hidden state corresponding to ys, used for searchable hidden ints\",\"Returns: tuple containing:\",\"x: decoded token score before softmax (batch, maxlen_out, token) : if use_output_layer is True,\",\"olens: (batch, )\",\"Return type: (tuple)\",\"forward_one_step(tgt: Tensor, tgt_mask: Tensor, memory: Tensor, memory_mask: Tensor | None = None, *, speech: Tensor | None = None, speech_mask: Tensor | None = None, cache: List[Tensor] | None = None, return_hs: bool = False) → Tuple[Tensor, List[Tensor]]\",\"Forward one step.\",\"Parameters:\",\"tgt – input token ids, int64 (batch, maxlen_out)\",\"tgt_mask – input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2)\",\"memory – encoded memory, float32 (batch, maxlen_in, feat)\",\"memory_mask – encoded memory mask (batch, 1, maxlen_in)\",\"speech – encoded speech, float32 (batch, maxlen_in, feat)\",\"speech_mask – encoded memory mask (batch, 1, maxlen_in)\",\"cache – cached output list of (batch, max_time_out-1, size)\",\"return_hs – dec hidden state corresponding to ys, used for searchable hidden ints\",\"Returns: NN output value and cache per self.decoders. y.shape` is (batch, maxlen_out, token)\",\"Return type: y, cache\",\"score(ys, state, x, speech=None)\",\"Score.\"]},\"851\":{\"h\":\"espnet2.asr.encoder.beats_encoder.TransformerSentenceEncoderLayer\",\"t\":[\"source\",\"class espnet2.asr.encoder.beats_encoder.TransformerSentenceEncoderLayer(embedding_dim: float = 768, ffn_embedding_dim: float = 3072, num_attention_heads: float = 8, dropout: float = 0.1, attention_dropout: float = 0.1, activation_dropout: float = 0.1, activation_fn: str = 'relu', layer_norm_first: bool = False, deep_norm: bool = False, has_relative_attention_bias: bool = False, num_buckets: int = 0, max_distance: int = 0, rescale_init: bool = False, gru_rel_pos: bool = False, encoder_layers: int = 0)\",\"Bases: Module\",\"Transformer encoder layer.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, self_attn_mask: Tensor | None = None, self_attn_padding_mask: Tensor | None = None, need_weights: bool = False, pos_bias=None)\",\"Forward pass.\"]},\"852\":{\"h\":\"espnet2.asr.state_spaces.components.TransposedLN\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.components.TransposedLN(d, scalar=True)\",\"Bases: Module\",\"Transposed LayerNorm module.\",\"LayerNorm module over second dimension Assumes shape (B, D, L), where L can be 1 or more axis\",\"This is slow and a dedicated CUDA/Triton implementation shuld provide substantial end-to-end speedup\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"853\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"854\":{\"h\":\"espnet2.asr.frontend.cnn.TransposedLayerNorm\",\"t\":[\"source\",\"class espnet2.asr.frontend.cnn.TransposedLayerNorm(normalized_shape: int | List[int] | Size, eps: float = 1e-05, elementwise_affine: bool = True, bias: bool = True, device=None, dtype=None)\",\"Bases: LayerNorm\",\"Layer norm with transpose\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor) → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"855\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"856\":{\"h\":\"espnet2.asr.state_spaces.components.TransposedLinear\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.components.TransposedLinear(d_input, d_output, bias=True)\",\"Bases: Module\",\"Transposed linear module.\",\"Linear module on the second-to-last dimension Assumes shape (B, D, L), where L can be 1 or more axis\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"857\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"858\":{\"h\":\"espnet2.asr.state_spaces.base.TransposedModule\",\"t\":[\"source\",\"espnet2.asr.state_spaces.base.TransposedModule(module)\",\"Transpose module.\",\"Wrap a SequenceModule class to accept transposed parameter, handle state, absorb kwargs\"]},\"859\":{\"h\":\"espnet2.asr.state_spaces.pool.UpPool\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.pool.UpPool(d_input, d_output, stride, transposed=True, weight_norm=True, initializer=None, activation=None)\",\"Bases: SequenceModule\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"Output dimension of model.\",\"This attribute is required for all SequenceModule instantiations. It is used by the rest of the pipeline (e.g. model backbone, decoder) to track the internal shapes of the full model.\",\"default_state(*batch_shape, device=None)\",\"Create initial state for a batch of inputs.\",\"forward(x, skip=None)\",\"Forward pass.\",\"A sequence-to-sequence transformation with an optional state.\",\"Generally, this should map a tensor of shape (batch, length, self.d_model) to (batch, length, self.d_output)\",\"Additionally, it returns a “state” which can be any additional information For example, RNN and SSM layers may return their hidden state, while some types of transformer layers (e.g. Transformer-XL) may want to pass a state as well\",\"step(x, state, **kwargs)\",\"Step one time step as a recurrent model.\",\"x: (…, H)\"]},\"860\":{\"h\":\"espnet2.asr.state_spaces.pool.UpSample\",\"t\":[\"source\",\"class espnet2.asr.state_spaces.pool.UpSample(d_input, stride=1, expand=1, transposed=True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property d_output\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"861\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"step(x, state, **kwargs)\"]},\"862\":{\"h\":\"espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder\",\"t\":[\"source\",\"class espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder(input_size: int, rnn_type: str = 'lstm', bidirectional: bool = True, use_projection: bool = True, num_layers: int = 4, hidden_size: int = 320, output_size: int = 320, dropout: float = 0.0, in_channel: int = 1)\",\"Bases: AbsEncoder\",\"VGGRNNEncoder class.\",\"Parameters:\",\"input_size – The number of expected features in the input\",\"bidirectional – If True becomes a bidirectional LSTM\",\"use_projection – Use projection layer or not\",\"num_layers – Number of recurrent layers\",\"hidden_size – The number of hidden features\",\"output_size – The number of output features\",\"dropout – dropout probability\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"863\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\"]},\"864\":{\"h\":\"espnet2.asr.frontend.whisper.WhisperFrontend\",\"t\":[\"source\",\"class espnet2.asr.frontend.whisper.WhisperFrontend(whisper_model: str = 'small', fs: int | str = 16000, freeze_weights: bool = True, download_dir: str | None = None)\",\"Bases: AbsFrontend\",\"Speech Representation Using Encoder Outputs from OpenAI’s Whisper Model:\",\"URL: https://github.com/openai/whisper\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"865\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"log_mel_spectrogram(audio: Tensor, ilens: Tensor | None = None) → Tensor\",\"output_size() → int\",\"whisper_encode(input: Tensor, ilens: Tensor | None = None) → Tensor\"]},\"866\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank._MultiblankRNNTNumba\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank._MultiblankRNNTNumba(*args, **kwargs)\",\"Bases: Function\",\"Numba class for multi-blank transducer loss\",\"(https://arxiv.org/pdf/2211.03541.pdf)\",\"static backward(ctx, grad_output)\",\"Define a formula for differentiating the operation with backward mode automatic differentiation.\",\"This function is to be overridden by all subclasses. (Defining this function is equivalent to defining the vjp function.)\",\"It must accept a context ctx as the first argument, followed by as many outputs as the forward() returned (None will be passed in for non tensor outputs of the forward function), and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not requiring grads, you can just pass None as a gradient for that input.\",\"The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computed w.r.t. the output.\",\"static forward(ctx, acts, labels, act_lens, label_lens, blank, big_blank_durations, reduction, fastemit_lambda, clamp, sigma)\",\"MultiblankRNNTNumba Forward.\",\"big_blank_durations: list of durations for multi-blank transducer, e.g. : [2, 4, 8].\",\"sigma: hyper-parameter for logit under-normalization method for training : multi-blank transducers. Recommended value 0.05.\",\"Refer to https://arxiv.org/pdf/2211.03541 for detailed explanations for : the above parameters;\",\"For other parameters for this class, refer to comment for class _RNNTNumba\"]},\"867\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank._RNNTNumba\",\"t\":[\"source\",\"class espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank._RNNTNumba(*args, **kwargs)\",\"Bases: Function\",\"static backward(ctx, grad_output)\",\"Define a formula for differentiating the operation with backward mode automatic differentiation.\",\"This function is to be overridden by all subclasses. (Defining this function is equivalent to defining the vjp function.)\",\"It must accept a context ctx as the first argument, followed by as many outputs as the forward() returned (None will be passed in for non tensor outputs of the forward function), and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not requiring grads, you can just pass None as a gradient for that input.\",\"The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computed w.r.t. the output.\",\"static forward(ctx, acts, labels, act_lens, label_lens, blank, reduction, fastemit_lambda, clamp)\",\"RNNTNumba Forward.\",\"log_probs: Tensor of (batch x seqLength x labelLength x outputDim) : containing output from network\",\"labels: 2 dimensional Tensor containing all the targets of : the batch with zero padded\",\"act_lens: Tensor of size (batch) containing size of each : output sequence from the network\",\"label_lens: Tensor of (batch) containing label length of each example fastemit_lambda: Float scaling factor for FastEmit regularization. Refer to\",\"FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\"]},\"868\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.add\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.add(x, y)\"]},\"869\":{\"h\":\"espnet2.asr.decoder.rnn_decoder.build_attention_list\",\"t\":[\"source\",\"espnet2.asr.decoder.rnn_decoder.build_attention_list(eprojs: int, dunits: int, atype: str = 'location', num_att: int = 1, num_encs: int = 1, aheads: int = 4, adim: int = 320, awin: int = 5, aconv_chans: int = 10, aconv_filts: int = 100, han_mode: bool = False, han_type=None, han_heads: int = 4, han_dim: int = 320, han_conv_chans: int = -1, han_conv_filts: int = 100, han_win: int = 5)\"]},\"870\":{\"h\":\"espnet2.asr.state_spaces.cauchy.cauchy_mult\",\"t\":[\"source\"]},\"871\":{\"h\":\"espnet2.asr.state_spaces.cauchy.cauchy_mult_keops\",\"t\":[\"source\"]},\"872\":{\"h\":\"espnet2.asr.state_spaces.cauchy.cauchy_mult_torch\",\"t\":[\"source\"]},\"873\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.certify_inputs\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.certify_inputs(log_probs, labels, lengths, label_lengths)\"]},\"874\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.check_contiguous\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.check_contiguous(var, name)\"]},\"875\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.check_dim\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.check_dim(var, dim, name)\"]},\"876\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.check_type\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.check_type(var, t, name)\"]},\"877\":{\"h\":\"espnet2.asr.state_spaces.s4.combination\",\"t\":[\"source\",\"espnet2.asr.state_spaces.s4.combination(measures, N, R, S, **ssm_args)\"]},\"878\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_alphas_kernel\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_alphas_kernel(acts: Tensor, denom: Tensor, alphas: Tensor, llForward: Tensor, xlen: Tensor, ylen: Tensor, mlabels: Tensor, minibatch: int, maxT: int, maxU: int, alphabet_size: int, blank_: int)\",\"Compute alpha (forward variable) probabilities over the transduction step.\",\"Parameters:\",\"acts – Tensor of shape [B, T, U, V+1] flattened. Represents the logprobs activation tensor.\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"alphas – Zero tensor of shape [B, T, U]. Will be updated inside the kernel with the forward variable probabilities.\",\"llForward – Zero tensor of shape [B]. Represents the log-likelihood of the forward pass. Returned as the forward pass loss that is reduced by the optimizer.\",\"xlen – Vector of length B which contains the actual acoustic sequence lengths in the padded activation tensor.\",\"ylen – Vector of length B which contains the actual target sequence lengths in the padded activation tensor.\",\"mlabels – Matrix of shape [B, U+1] (+1 here is due to <SOS> token \",\"usually the RNNT blank). The matrix contains the padded target transcription that must be predicted.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"Updates: : Kernel inplace updates the following inputs:\",\"alphas: forward variable scores.\",\"llForward: log-likelihood of forward variable.\"]},\"879\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_betas_kernel\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_betas_kernel(acts: Tensor, denom: Tensor, betas: Tensor, llBackward: Tensor, xlen: Tensor, ylen: Tensor, mlabels: Tensor, minibatch: int, maxT: int, maxU: int, alphabet_size: int, blank_: int)\",\"Compute beta (backward variable) probabilities over the transduction step.\",\"Parameters:\",\"acts – Tensor of shape [B, T, U, V+1] flattened. Represents the logprobs activation tensor.\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"betas – Zero tensor of shape [B, T, U]. Will be updated inside the kernel with the backward variable probabilities.\",\"llBackward – Zero tensor of shape [B]. Represents the log-likelihood of the backward pass. Returned as the backward pass loss that is reduced by the optimizer.\",\"xlen – Vector of length B which contains the actual acoustic sequence lengths in the padded activation tensor.\",\"ylen – Vector of length B which contains the actual target sequence lengths in the padded activation tensor.\",\"mlabels – Matrix of shape [B, U+1] (+1 here is due to <SOS> token \",\"usually the RNNT blank). The matrix contains the padded target transcription that must be predicted.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"Updates: : Kernel inplace updates the following inputs:\",\"betas: backward variable scores.\",\"llBackward: log-likelihood of backward variable.\"]},\"880\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.compute_costs_data\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.compute_costs_data(source: Tensor, dest: Tensor, fastemit_lambda: float)\"]},\"881\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_grad_kernel\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_grad_kernel(grads: Tensor, acts: Tensor, denom: Tensor, alphas: Tensor, betas: Tensor, logll: Tensor, xlen: Tensor, ylen: Tensor, mlabels: Tensor, minibatch: int, maxT: int, maxU: int, alphabet_size: int, blank_: int, fastemit_lambda: float, clamp: float)\",\"Compute gradients over the transduction step.\",\"Parameters:\",\"grads – Zero Tensor of shape [B, T, U, V+1]. Is updated by this kernel to contain the gradients of this batch of samples.\",\"acts – Tensor of shape [B, T, U, V+1] flattened. Represents the logprobs activation tensor.\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"alphas – Alpha variable, contains forward probabilities. A tensor of shape [B, T, U].\",\"betas – Beta varoable, contains backward probabilities. A tensor of shape [B, T, U].\",\"logll – Log-likelihood of the forward variable, represented as a vector of shape [B]. Represents the log-likelihood of the forward pass.\",\"xlen – Vector of length B which contains the actual acoustic sequence lengths in the padded activation tensor.\",\"ylen – Vector of length B which contains the actual target sequence lengths in the padded activation tensor.\",\"mlabels – Matrix of shape [B, U+1] (+1 here is due to <SOS> token \",\"usually the RNNT blank). The matrix contains the padded target transcription that must be predicted.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"Updates: : Kernel inplace updates the following inputs:\",\"grads: Gradients with respect to the log likelihood (logll).\"]},\"882\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_multiblank_alphas_kernel\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_multiblank_alphas_kernel(acts: Tensor, denom: Tensor, sigma: float, alphas: Tensor, llForward: Tensor, xlen: Tensor, ylen: Tensor, mlabels: Tensor, minibatch: int, maxT: int, maxU: int, alphabet_size: int, blank_: int, big_blank_duration: Tensor, num_big_blanks: int)\",\"Compute alpha (forward variable) probabilities for multi-blank transducuer loss\",\"(https://arxiv.org/pdf/2211.03541).\",\"Parameters:\",\"acts – Tensor of shape [B, T, U, V + 1 + num_big_blanks] flattened. Represents the logprobs activation tensor.\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"sigma – Hyper-parameter for logit-undernormalization technique for training multi-blank transducers.\",\"alphas – Zero tensor of shape [B, T, U]. Will be updated inside the kernel with the forward variable probabilities.\",\"llForward – Zero tensor of shape [B]. Represents the log-likelihood of the forward pass. Returned as the forward pass loss that is reduced by the optimizer.\",\"xlen – Vector of length B which contains the actual acoustic sequence lengths in the padded activation tensor.\",\"ylen – Vector of length B which contains the actual target sequence lengths in the padded activation tensor.\",\"mlabels – Matrix of shape [B, U+1] (+1 here is due to <SOS> token \",\"usually the RNNT blank). The matrix contains the padded target transcription that must be predicted.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"blank – Index of the RNNT standard blank token in the vocabulary.\",\"big_blank_durations – Vector of supported big blank durations of the model.\",\"num_big_blanks – Number of big blanks of the model.\",\"Updates: : Kernel inplace updates the following inputs:\",\"alphas: forward variable scores.\",\"llForward: log-likelihood of forward variable.\"]},\"883\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_multiblank_betas_kernel\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_multiblank_betas_kernel(acts: Tensor, denom: Tensor, sigma: float, betas: Tensor, llBackward: Tensor, xlen: Tensor, ylen: Tensor, mlabels: Tensor, minibatch: int, maxT: int, maxU: int, alphabet_size: int, blank_: int, big_blank_duration: Tensor, num_big_blanks: int)\",\"Compute beta (backward variable) probabilities for multi-blank transducer loss\",\"(https://arxiv.org/pdf/2211.03541).\",\"Parameters:\",\"acts – Tensor of shape [B, T, U, V + 1 + num-big-blanks] flattened. Represents the logprobs activation tensor.\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"sigma – Hyper-parameter for logit-undernormalization technique for training multi-blank transducers.\",\"betas – Zero tensor of shape [B, T, U]. Will be updated inside the kernel with the backward variable probabilities.\",\"llBackward – Zero tensor of shape [B]. Represents the log-likelihood of the backward pass. Returned as the backward pass loss that is reduced by the optimizer.\",\"xlen – Vector of length B which contains the actual acoustic sequence lengths in the padded activation tensor.\",\"ylen – Vector of length B which contains the actual target sequence lengths in the padded activation tensor.\",\"mlabels – Matrix of shape [B, U+1] (+1 here is due to <SOS> token \",\"usually the RNNT blank). The matrix contains the padded target transcription that must be predicted.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"blank – Index of the RNNT standard blank token in the vocabulary.\",\"big_blank_durations – Vector of supported big blank durations of the model.\",\"num_big_blanks – Number of big blanks of the model.\",\"Updates: : Kernel inplace updates the following inputs:\",\"betas: backward variable scores.\",\"llBackward: log-likelihood of backward variable.\"]},\"884\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_multiblank_grad_kernel\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.compute_multiblank_grad_kernel(grads: Tensor, acts: Tensor, denom: Tensor, sigma: float, alphas: Tensor, betas: Tensor, logll: Tensor, xlen: Tensor, ylen: Tensor, mlabels: Tensor, minibatch: int, maxT: int, maxU: int, alphabet_size: int, blank_: int, big_blank_duration: Tensor, num_big_blanks: int, fastemit_lambda: float, clamp: float)\",\"Compute gradients for multi-blank transducer loss\",\"(https://arxiv.org/pdf/2211.03541).\",\"Parameters:\",\"grads – Zero Tensor of shape [B, T, U, V + 1 + num_big_blanks]. Is updated by this kernel to contain the gradients of this batch of samples.\",\"acts – Tensor of shape [B, T, U, V + 1 + num_big_blanks] flattened. Represents the logprobs activation tensor.\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"sigma – Hyper-parameter for logit-undernormalization technique for training multi-blank transducers.\",\"alphas – Alpha variable, contains forward probabilities. A tensor of shape [B, T, U].\",\"betas – Beta varoable, contains backward probabilities. A tensor of shape [B, T, U].\",\"logll – Log-likelihood of the forward variable, represented as a vector of shape [B]. Represents the log-likelihood of the forward pass.\",\"xlen – Vector of length B which contains the actual acoustic sequence lengths in the padded activation tensor.\",\"ylen – Vector of length B which contains the actual target sequence lengths in the padded activation tensor.\",\"mlabels – Matrix of shape [B, U+1] (+1 here is due to <SOS> token \",\"usually the RNNT blank). The matrix contains the padded target transcription that must be predicted.\",\"minibatch – Int representing the batch size.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"blank – Index of the RNNT blank token in the vocabulary. Generally the first or last token in the vocab.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"big_blank_durations – Vector of supported big blank durations of the model.\",\"num_big_blanks – Number of big blanks of the model.\",\"Updates: : Kernel inplace updates the following inputs:\",\"grads: Gradients with respect to the log likelihood (logll).\"]},\"885\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.conv3x3\",\"t\":[\"source\",\"espnet2.asr.encoder.avhubert_encoder.conv3x3(in_planes, out_planes, stride=1)\"]},\"886\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.copy_data_1d\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.copy_data_1d(source: Tensor, dest: Tensor, idx: int)\"]},\"887\":{\"h\":\"espnet2.asr.frontend.cnn.dim_1_layer_norm\",\"t\":[\"source\",\"espnet2.asr.frontend.cnn.dim_1_layer_norm(x, eps=1e-05, gamma=None, beta=None)\",\"Functional version of Dim1LayerNorm.\"]},\"888\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.div_up\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.div_up(x: int, y: int)\"]},\"889\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.download_avhubert\",\"t\":[\"source\",\"espnet2.asr.encoder.avhubert_encoder.download_avhubert(model_url, dir_path)\"]},\"890\":{\"h\":\"espnet2.asr.encoder.hubert_encoder.download_hubert\",\"t\":[\"source\",\"espnet2.asr.encoder.hubert_encoder.download_hubert(model_url, dir_path)\"]},\"891\":{\"h\":\"espnet2.asr.encoder.wav2vec2_encoder.download_w2v\",\"t\":[\"source\",\"espnet2.asr.encoder.wav2vec2_encoder.download_w2v(model_url, dir_path)\"]},\"892\":{\"h\":\"espnet2.asr.state_spaces.pool.downsample\",\"t\":[\"source\",\"espnet2.asr.state_spaces.pool.downsample(x, stride=1, expand=1, transposed=False)\"]},\"893\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.downsample_basic_block\",\"t\":[\"source\",\"espnet2.asr.encoder.avhubert_encoder.downsample_basic_block(inplanes, outplanes, stride)\"]},\"894\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.downsample_basic_block_v2\",\"t\":[\"source\",\"espnet2.asr.encoder.avhubert_encoder.downsample_basic_block_v2(inplanes, outplanes, stride)\"]},\"895\":{\"h\":\"espnet2.asr.state_spaces.s4.dplr\",\"t\":[\"source\",\"espnet2.asr.state_spaces.s4.dplr(scaling, N, rank=1, H=1, dtype=torch.float32, real_scale=1.0, imag_scale=1.0, random_real=False, random_imag=False, normalize=False, diagonal=True, random_B=False)\"]},\"896\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.dtype\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.dtype()\"]},\"897\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.exponential\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.exponential(x)\"]},\"898\":{\"h\":\"espnet2.asr.state_spaces.utils.extract_attrs_from_obj\",\"t\":[\"source\",\"espnet2.asr.state_spaces.utils.extract_attrs_from_obj(obj, *attrs)\"]},\"899\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.flatten_tensor\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.flatten_tensor(x: Tensor)\"]},\"900\":{\"h\":\"espnet2.asr.encoder.beats_encoder.gelu\",\"t\":[\"source\",\"espnet2.asr.encoder.beats_encoder.gelu(x: Tensor) → Tensor\"]},\"901\":{\"h\":\"espnet2.asr.encoder.beats_encoder.gelu_accurate\",\"t\":[\"source\",\"espnet2.asr.encoder.beats_encoder.gelu_accurate(x)\"]},\"902\":{\"h\":\"espnet2.asr.encoder.beats_encoder.get_activation_fn\",\"t\":[\"source\",\"espnet2.asr.encoder.beats_encoder.get_activation_fn(activation: str)\",\"Returns the activation function corresponding to activation\"]},\"903\":{\"h\":\"espnet2.asr.state_spaces.utils.get_class\",\"t\":[\"source\",\"espnet2.asr.state_spaces.utils.get_class(registry, _name_)\"]},\"904\":{\"h\":\"espnet2.asr.decoder.hugging_face_transformers_decoder.get_hugging_face_model_lm_head\",\"t\":[\"source\",\"espnet2.asr.decoder.hugging_face_transformers_decoder.get_hugging_face_model_lm_head(model)\"]},\"905\":{\"h\":\"espnet2.asr.decoder.hugging_face_transformers_decoder.get_hugging_face_model_network\",\"t\":[\"source\",\"espnet2.asr.decoder.hugging_face_transformers_decoder.get_hugging_face_model_network(model)\"]},\"906\":{\"h\":\"espnet2.asr.state_spaces.components.get_initializer\",\"t\":[\"source\",\"espnet2.asr.state_spaces.components.get_initializer(name, activation=None)\"]},\"907\":{\"h\":\"espnet2.asr.state_spaces.s4.get_logger\",\"t\":[\"source\",\"espnet2.asr.state_spaces.s4.get_logger(name='espnet2.asr.state_spaces.s4', level=20) → Logger\",\"Initialize multi-GPU-friendly python logger.\"]},\"908\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.get_workspace_size\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.get_workspace_size(maxT: int, maxU: int, minibatch: int, gpu: bool) → Tuple[int | None, RNNTStatus]\"]},\"909\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.identity\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.identity(x)\"]},\"910\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.index_put\",\"t\":[\"source\",\"espnet2.asr.encoder.avhubert_encoder.index_put(tensor, indices, value)\"]},\"911\":{\"h\":\"espnet2.asr.encoder.beats_encoder.init_bert_params\",\"t\":[\"source\",\"espnet2.asr.encoder.beats_encoder.init_bert_params(module)\",\"Initialize the weights specific to the BERT Model.\",\"This overrides the default initializations depending on the specified arguments. : 1. If normal_init_linear_weights is set then weights of linear layer will be initialized using the normal distribution and bais will be set to the specified value. 2. If normal_init_embed_weights is set then weights of embedding layer will be initialized using the normal distribution. 3. If normal_init_proj_weights is set then weights of in_project_weight for MultiHeadAttention initialized using the normal distribution (to be validated).\"]},\"912\":{\"h\":\"espnet2.asr.state_spaces.utils.instantiate\",\"t\":[\"source\",\"espnet2.asr.state_spaces.utils.instantiate(registry, config, *args, partial=False, wrap=None, **kwargs)\",\"Instantiate registered module.\",\"registry: Dictionary mapping names to functions or target paths : (e.g. {‘model’: ‘models.SequenceModel’})\",\"config: Dictionary with a ‘name’ key indicating which element of the registry : to grab, and kwargs to be passed into the target constructor\",\"wrap: wrap the target class (e.g. ema optimizer or tasks.wrap)\",\"*\",\"args,\",\"**\",\"kwargs: additional arguments\",\"to override the config to pass into the target constructor\"]},\"913\":{\"h\":\"espnet2.asr.state_spaces.utils.is_dict\",\"t\":[\"source\",\"espnet2.asr.state_spaces.utils.is_dict(x)\"]},\"914\":{\"h\":\"espnet2.asr.state_spaces.utils.is_list\",\"t\":[\"source\",\"espnet2.asr.state_spaces.utils.is_list(x)\"]},\"915\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.is_xla_tensor\",\"t\":[\"source\",\"espnet2.asr.encoder.avhubert_encoder.is_xla_tensor(tensor)\"]},\"916\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.log_plus\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.log_plus(p1: float, p2: float)\"]},\"917\":{\"h\":\"espnet2.asr.bayes_risk_ctc.log_substraction_exp\",\"t\":[\"source\"]},\"918\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.log_sum_exp\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.cpu_utils.cpu_rnnt.log_sum_exp(a: Tensor, b: Tensor)\",\"Logsumexp with safety checks for infs.\"]},\"919\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.logp\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.gpu_rnnt_kernel.logp(denom: Tensor, acts: Tensor, maxT: int, maxU: int, alphabet_size: int, mb: int, t: int, u: int, v: int)\",\"Compute the sum of log probability from the activation tensor\",\"and its denominator.\",\"Parameters:\",\"denom – Tensor of shape [B, T, U] flattened. Represents the denominator of the logprobs activation tensor across entire vocabulary.\",\"acts – Tensor of shape [B, T, U, V+1] flattened. Represents the logprobs activation tensor.\",\"maxT – The maximum possible acoustic sequence length. Represents T in the logprobs tensor.\",\"maxU – The maximum possible target sequence length. Represents U in the logprobs tensor.\",\"alphabet_size – The vocabulary dimension V+1 (inclusive of RNNT blank).\",\"mb – Batch indexer.\",\"t – Acoustic sequence timestep indexer.\",\"u – Target sequence timestep indexer.\",\"v – Vocabulary token indexer.\",\"Returns: The sum of logprobs[mb, t, u, v] + denom[mb, t, u]\"]},\"920\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.maximum\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.maximum(x, y)\"]},\"921\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.multiblank_rnnt_loss\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.multiblank_rnnt_loss(acts, labels, act_lens, label_lens, blank, big_blank_durations=[], reduction='mean', fastemit_lambda: float = 0.0, clamp: float = 0.0)\",\"Multi-blank RNN Transducer (https://arxiv.org/pdf/2211.03541.pdf)\",\"Loss (functional form) :param acts: Tensor of (batch x seqLength x labelLength x outputDim) containing :param output from network: :param labels: 2 dimensional Tensor containing all the targets of the batch with\",\"zero padded\",\"Parameters:\",\"act_lens – Tensor of size (batch) containing size of each output sequence from the network\",\"label_lens – Tensor of (batch) containing label length of each example\",\"blank (int) – standard blank label.\",\"big_blank_durations – list of durations for multi-blank transducer, e.g. [2, 4, 8].\",\"sigma – hyper-parameter for logit under-normalization method for training multi-blank transducers. Recommended value 0.05.\",\"https (Refer to) – //arxiv.org/pdf/2211.03541 for detailed explanations for the last two params.\",\"reduction (string,optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: ‘mean’\"]},\"922\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt.multiblank_rnnt_loss_gpu\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.rnnt.multiblank_rnnt_loss_gpu(acts: Tensor, labels: Tensor, input_lengths: Tensor, label_lengths: Tensor, costs: Tensor, grads: Tensor, blank_label: int, big_blank_durations: list, fastemit_lambda: float, clamp: float, num_threads: int, sigma: float)\",\"Wrapper method for accessing GPU Multi-blank RNNT loss\",\"(https://arxiv.org/pdf/2211.03541.pdf).\",\"CUDA implementation ported from [HawkAaron/warp-transducer] : (https://github.com/HawkAaron/warp-transducer).\",\"Parameters:\",\"acts – Activation tensor of shape [B, T, U, V + num_big_blanks + 1].\",\"labels – Ground truth labels of shape [B, U].\",\"input_lengths – Lengths of the acoustic sequence as a vector of ints [B].\",\"label_lengths – Lengths of the target sequence as a vector of ints [B].\",\"costs – Zero vector of length [B] in which costs will be set.\",\"grads – Zero tensor of shape [B, T, U, V + num_big_blanks + 1] where the gradient will be set.\",\"blank_label – Index of the standard blank token in the vocabulary.\",\"big_blank_durations – A list of supported durations for big blank symbols in the model, e.g. [2, 4, 8]. Note we only include durations for\",\"``\",\"big blanks’’ here and it should not include 1 for the standard blank. Those big blanks have vocabulary indices after the standard blank index.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"num_threads – Number of threads for OpenMP.\",\"sigma – logit-undernormalization weight used in the multi-blank model. Refer to the multi-blank paper https://arxiv.org/pdf/2211.03541 for detailed explanations.\"]},\"923\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.negate\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.rnnt_helper.negate(x)\"]},\"924\":{\"h\":\"espnet2.asr.state_spaces.s4.nplr\",\"t\":[\"source\",\"espnet2.asr.state_spaces.s4.nplr(measure, N, rank=1, dtype=torch.float32, diagonalize_precision=True)\",\"Decompose as Normal Plus Low-Rank (NPLR).\",\"Return w, p, q, V, B such that (w - p q^*, B) is unitarily equivalent to the original HiPPO A, B by the matrix V i.e. A = V[w - p q*]V*, B = V B\"]},\"925\":{\"h\":\"espnet2.asr.state_spaces.utils.omegaconf_filter_keys\",\"t\":[\"source\",\"espnet2.asr.state_spaces.utils.omegaconf_filter_keys(d, fn=None)\",\"Only keep keys where fn(key) is True. Support nested DictConfig.\"]},\"926\":{\"h\":\"espnet2.asr.state_spaces.s4.power\",\"t\":[\"source\",\"espnet2.asr.state_spaces.s4.power(L, A, v=None)\",\"Compute A^L and the scan sum_i A^i v_i.\",\"A: (…, N, N) v: (…, N, L)\"]},\"927\":{\"h\":\"espnet2.asr.encoder.beats_encoder.quant_noise\",\"t\":[\"source\",\"espnet2.asr.encoder.beats_encoder.quant_noise(module, p, block_size)\",\"Wraps modules and applies quantization noise to the weights for\",\"subsequent quantization with Iterative Product Quantization as described in “Training with Quantization Noise for Extreme Model Compression”\",\"Parameters:\",\"module ( -) – nn.Module\",\"p ( -) – amount of Quantization Noise\",\"block_size ( -) – size of the blocks for subsequent quantization with iPQ\",\"Remarks: : - Module weights must have the right sizes wrt the block size\",\"Only Linear, Embedding and Conv2d modules are supported for the moment\",\"For more detail on how to quantize by blocks with convolutional weights, see “And the Bit Goes Down: Revisiting the Quantization of Neural Networks”\",\"We implement the simplest form of noise here as stated in the paper which consists in randomly dropping blocks\"]},\"928\":{\"h\":\"espnet2.asr.state_spaces.s4.rank_correction\",\"t\":[\"source\",\"espnet2.asr.state_spaces.s4.rank_correction(measure, N, rank=1, dtype=torch.float32)\",\"Return low-rank matrix L such that A + L is normal.\"]},\"929\":{\"h\":\"espnet2.asr.state_spaces.s4.rank_zero_only\",\"t\":[\"source\",\"espnet2.asr.state_spaces.s4.rank_zero_only(fn: Callable) → Callable\",\"Decorator function from PyTorch Lightning.\",\"Function that can be used as a decorator to enable a function/method being called only on global rank 0.\"]},\"930\":{\"h\":\"espnet2.asr.decoder.hugging_face_transformers_decoder.read_json_config\",\"t\":[\"source\",\"espnet2.asr.decoder.hugging_face_transformers_decoder.read_json_config(conf_path)\",\"Read a json model config information.\",\"Parameters:conf_path (str) – Config path.\",\"Returns: Config information loaded from json file.\",\"Return type: dict[str, Any]\"]},\"931\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.reduce_exp\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.reduce_exp(acts: Tensor, denom, rows: int, cols: int, minus: bool, stream)\",\"Helper method to call the Warp Reduction Kernel to perform exp reduction.\"]},\"932\":{\"h\":\"NOTE\",\"t\":[\"Efficient warp occurs at input shapes of 2 ^ K.\",\"References\",\"Warp Primitives [https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/]\",\"Parameters:\",\"acts – Flatened activation matrix of shape [B * T * U * (V+1)].\",\"output – Flatened output matrix of shape [B * T * U * (V+1)]. Data will be overwritten.\",\"rows – Vocabulary size (including blank token) - V+1. Represents the number of threads per block.\",\"cols – Flattened shape of activation matrix, without vocabulary dimension (B * T * U). Represents number of blocks per grid.\",\"minus – Bool flag whether to add or subtract as reduction. If minus is set; calls _reduce_minus, else calls _reduce_rows kernel.\",\"stream – CUDA Stream.\"]},\"933\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.reduce_max\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.cuda_utils.reduce.reduce_max(acts: Tensor, denom, rows: int, cols: int, minus: bool, stream)\",\"Helper method to call the Warp Reduction Kernel to perform max reduction.\"]},\"934\":{\"h\":\"NOTE\",\"t\":[\"Efficient warp occurs at input shapes of 2 ^ K.\",\"References\",\"Warp Primitives [https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/]\",\"Parameters:\",\"acts – Flatened activation matrix of shape [B * T * U * (V+1)].\",\"output – Flatened output matrix of shape [B * T * U * (V+1)]. Data will be overwritten.\",\"rows – Vocabulary size (including blank token) - V+1. Represents the number of threads per block.\",\"cols – Flattened shape of activation matrix, without vocabulary dimension (B * T * U). Represents number of blocks per grid.\",\"minus – Bool flag whether to add or subtract as reduction. If minus is set; calls _reduce_minus, else calls _reduce_rows kernel.\",\"stream – CUDA Stream.\"]},\"935\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.rnnt_loss\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.rnnt_multi_blank.rnnt_loss(acts, labels, act_lens, label_lens, blank=0, reduction='mean', fastemit_lambda: float = 0.0, clamp: float = 0.0)\",\"RNN Transducer Loss (functional form)\",\"Parameters:\",\"acts – Tensor of (batch x seqLength x labelLength x outputDim) containing output from network\",\"labels – 2 dimensional Tensor containing all the targets of the batch with zero padded\",\"act_lens – Tensor of size (batch) containing size of each output sequence from the network\",\"label_lens – Tensor of (batch) containing label length of each example\",\"blank (int,optional) – blank label. Default: 0.\",\"reduction (string,optional) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied, ‘mean’: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: ‘mean’\"]},\"936\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt.rnnt_loss_cpu\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.rnnt.rnnt_loss_cpu(acts: Tensor, labels: Tensor, input_lengths: Tensor, label_lengths: Tensor, costs: Tensor, grads: Tensor, blank_label: int, fastemit_lambda: float, clamp: float, num_threads: int)\",\"Wrapper method for accessing CPU RNNT loss.\",\"CPU implementation ported from [HawkAaron/warp-transducer] : (https://github.com/HawkAaron/warp-transducer).\",\"Parameters:\",\"acts – Activation tensor of shape [B, T, U, V+1].\",\"labels – Ground truth labels of shape [B, U].\",\"input_lengths – Lengths of the acoustic sequence as a vector of ints [B].\",\"label_lengths – Lengths of the target sequence as a vector of ints [B].\",\"costs – Zero vector of length [B] in which costs will be set.\",\"grads – Zero tensor of shape [B, T, U, V+1] where the gradient will be set.\",\"blank_label – Index of the blank token in the vocabulary.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"num_threads – Number of threads for OpenMP.\"]},\"937\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.rnnt.rnnt_loss_gpu\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.rnnt.rnnt_loss_gpu(acts: Tensor, labels: Tensor, input_lengths: Tensor, label_lengths: Tensor, costs: Tensor, grads: Tensor, blank_label: int, fastemit_lambda: float, clamp: float, num_threads: int)\",\"Wrapper method for accessing GPU RNNT loss.\",\"CUDA implementation ported from [HawkAaron/warp-transducer] : (https://github.com/HawkAaron/warp-transducer).\",\"Parameters:\",\"acts – Activation tensor of shape [B, T, U, V+1].\",\"labels – Ground truth labels of shape [B, U].\",\"input_lengths – Lengths of the acoustic sequence as a vector of ints [B].\",\"label_lengths – Lengths of the target sequence as a vector of ints [B].\",\"costs – Zero vector of length [B] in which costs will be set.\",\"grads – Zero tensor of shape [B, T, U, V+1] where the gradient will be set.\",\"blank_label – Index of the blank token in the vocabulary.\",\"fastemit_lambda – Float scaling factor for FastEmit regularization. Refer to FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization.\",\"clamp – Float value. When set to value >= 0.0, will clamp the gradient to [-clamp, clamp].\",\"num_threads – Number of threads for OpenMP.\"]},\"938\":{\"h\":\"espnet2.asr.state_spaces.s4.ssm\",\"t\":[\"source\",\"espnet2.asr.state_spaces.s4.ssm(measure, N, R, H, **ssm_args)\",\"Dispatcher to create single SSM initialization.\",\"N: state size R: rank (for DPLR parameterization) H: number of independent SSM copies\"]},\"939\":{\"h\":\"espnet2.asr.state_spaces.components.stochastic_depth\",\"t\":[\"source\",\"espnet2.asr.state_spaces.components.stochastic_depth(input: tensor, p: float, mode: str, training: bool = True)\",\"Apply stochastic depth.\",\"Implements the Stochastic Depth from “Deep Networks with Stochastic Depth” used for randomly dropping residual branches of residual architectures.\",\"Parameters:\",\"input (Tensor *[*N,...]) – The input tensor or arbitrary dimensions with the first one being its batch i.e. a batch with N rows.\",\"p (float) – probability of the input to be zeroed.\",\"mode (str) – \\\"batch\\\" or \\\"row\\\". \\\"batch\\\" randomly zeroes the entire input, \\\"row\\\" zeroes randomly selected rows from the batch.\",\"training – apply stochastic depth if is True. Default: True\",\"Returns: The randomly zeroed tensor.\",\"Return type: Tensor[N, …]\"]},\"940\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.threads_per_block\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.threads_per_block()\"]},\"941\":{\"h\":\"espnet2.asr.encoder.avhubert_encoder.time_masking\",\"t\":[\"source\",\"espnet2.asr.encoder.avhubert_encoder.time_masking(xs_pad, min_T=5, max_T=20)\",\"Masking Contiguous Frames with random length of [min_T, max_T]\"]},\"942\":{\"h\":\"espnet2.asr.state_spaces.utils.to_dict\",\"t\":[\"source\",\"espnet2.asr.state_spaces.utils.to_dict(x, recursive=True)\",\"Convert Sequence or Mapping object to dict.\",\"lists get converted to\"]},\"943\":{\"h\":\"espnet2.asr.state_spaces.utils.to_list\",\"t\":[\"source\",\"espnet2.asr.state_spaces.utils.to_list(x, recursive=False)\",\"Convert an object to list.\",\"If Sequence (e.g. list, tuple, Listconfig): just return it\",\"Special case: If non-recursive and not a list, wrap in list\"]},\"944\":{\"h\":\"espnet2.asr.state_spaces.s4.transition\",\"t\":[\"source\",\"espnet2.asr.state_spaces.s4.transition(measure, N)\",\"A, B transition matrices for different measures.\"]},\"945\":{\"h\":\"espnet2.asr.state_spaces.pool.upsample\",\"t\":[\"source\",\"espnet2.asr.state_spaces.pool.upsample(x, stride=1, expand=1, transposed=False)\"]},\"946\":{\"h\":\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.warp_size\",\"t\":[\"source\",\"espnet2.asr.transducer.rnnt_multi_blank.utils.global_constants.warp_size()\"]},\"947\":{\"h\":\"espnet2.asvspoof.loss.am_softmax_loss.ASVSpoofAMSoftmaxLoss\",\"t\":[\"source\",\"class espnet2.asvspoof.loss.am_softmax_loss.ASVSpoofAMSoftmaxLoss(weight: float = 1.0, enc_dim: int = 128, s: float = 20, m: float = 0.5)\",\"Bases: AbsASVSpoofLoss\",\"Binary loss for ASV Spoofing.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(label: Tensor, emb: Tensor, **kwargs)\",\"Forward.\",\"Parameters:\",\"label (torch.Tensor) – ground truth label [Batch, 1]\",\"emb (torch.Tensor) – encoder embedding output [Batch, T, enc_dim]\",\"score(emb: Tensor)\",\"Prediction.\",\"Parameters:emb (torch.Tensor) – encoder embedding output [Batch, T, enc_dim]\"]},\"948\":{\"h\":\"espnet2.asvspoof.loss.binary_loss.ASVSpoofBinaryLoss\",\"t\":[\"source\",\"class espnet2.asvspoof.loss.binary_loss.ASVSpoofBinaryLoss(weight: float = 1.0)\",\"Bases: AbsASVSpoofLoss\",\"Binary loss for ASV Spoofing.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(pred: Tensor, label: Tensor, **kwargs)\",\"Forward.\",\"Parameters:\",\"pred (torch.Tensor) – prediction probability [Batch, 2]\",\"label (torch.Tensor) – ground truth label [Batch, 2]\",\"score(pred: Tensor)\"]},\"949\":{\"h\":\"espnet2.asvspoof.loss.oc_softmax_loss.ASVSpoofOCSoftmaxLoss\",\"t\":[\"source\",\"class espnet2.asvspoof.loss.oc_softmax_loss.ASVSpoofOCSoftmaxLoss(weight: float = 1.0, enc_dim: int = 128, m_real: float = 0.5, m_fake: float = 0.2, alpha: float = 20.0)\",\"Bases: AbsASVSpoofLoss\",\"Binary loss for ASV Spoofing.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(label: Tensor, emb: Tensor, **kwargs)\",\"Forward.\",\"Parameters:\",\"label (torch.Tensor) – ground truth label [Batch, 1]\",\"emb (torch.Tensor) – encoder embedding output [Batch, T, enc_dim]\",\"score(emb: Tensor)\",\"Prediction.\",\"Parameters:emb (torch.Tensor) – encoder embedding output [Batch, T, enc_dim]\"]},\"950\":{\"h\":\"espnet2.asvspoof.loss.abs_loss.AbsASVSpoofLoss\",\"t\":[\"source\",\"class espnet2.asvspoof.loss.abs_loss.AbsASVSpoofLoss(*args, **kwargs)\",\"Bases: Module, ABC\",\"Base class for all ASV Spoofing loss modules.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(ref, inf) → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"951\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"property name : str\",\"abstract score(pred) → Tensor\"]},\"952\":{\"h\":\"espnet2.asvspoof.decoder.abs_decoder.AbsDecoder\",\"t\":[\"source\",\"class espnet2.asvspoof.decoder.abs_decoder.AbsDecoder(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"953\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"954\":{\"h\":\"espnet2.asvspoof.espnet_model.ESPnetASVSpoofModel\",\"t\":[\"source\",\"class espnet2.asvspoof.espnet_model.ESPnetASVSpoofModel(frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, encoder: AbsEncoder, preencoder: AbsPreEncoder | None, decoder: AbsDecoder, losses: Dict[str, AbsASVSpoofLoss])\",\"Bases: AbsESPnetModel\",\"ASV Spoofing model\",\"A simple ASV Spoofing model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"encode(speech: Tensor, speech_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Frontend + Encoder\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch,)\",\"bottleneck_feats – (Batch, Length, …): used for enh + diar\",\"forward(speech: Tensor, speech_lengths: Tensor | None = None, label: Tensor | None = None, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, samples)\",\"spk_labels – (Batch, )\",\"kwargs – “utt_id” is among the input.\"]},\"955\":{\"h\":\"espnet2.asvspoof.decoder.linear_decoder.LinearDecoder\",\"t\":[\"source\",\"class espnet2.asvspoof.decoder.linear_decoder.LinearDecoder(encoder_output_size: int)\",\"Bases: AbsDecoder\",\"Linear decoder for speaker diarization\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor | None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – hidden_space [Batch, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\"]},\"956\":{\"h\":\"espnet2.cls.decoder.abs_decoder.AbsDecoder\",\"t\":[\"source\",\"class espnet2.cls.decoder.abs_decoder.AbsDecoder(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(hs_pad: Tensor, hlens: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"957\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"958\":{\"h\":\"espnet2.cls.espnet_model.ESPnetClassificationModel\",\"t\":[\"source\",\"class espnet2.cls.espnet_model.ESPnetClassificationModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, decoder: AbsDecoder, classification_type='multi-class', lsm_weight: float = 0.0, mixup_probability: float = 0.0, log_epoch_metrics: bool = False)\",\"Bases: AbsESPnetModel\",\"Classification model\",\"A simple Classification model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"encode(speech: Tensor, speech_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Encode the input speech.\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch,)\",\"Returns: (Batch, Length, n_classes)\",\"Return type: scores\",\"forward(speech: Tensor, speech_lengths: Tensor, label: Tensor, label_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Pass the input through the model and calculate the loss.\",\"Parameters:\",\"speech – (Batch, samples)\",\"speech_lengths – (Batch, )\",\"label – (Batch, Length)\",\"label_lengths – (Batch, )\",\"Returns: (1,) stats: dict weight\",\"Return type: loss\",\"get_vocab_size()\",\"score(speech: Tensor, speech_lengths: Tensor | None = None) → Tensor\",\"Forward pass at scoring (inference)\",\"Parameters:\",\"speech – (Batch, samples)\",\"speech_lengths – (Batch, )\",\"Returns: (Batch, n_classes)\",\"Return type: scores\",\"Assumes Batch=1\",\"setup_metrics_()\",\"update_mAP(mAP_computer)\"]},\"959\":{\"h\":\"espnet2.cls.decoder.linear_decoder.LinearDecoder\",\"t\":[\"source\",\"class espnet2.cls.decoder.linear_decoder.LinearDecoder(vocab_size: int, encoder_output_size: int, pooling: str = 'mean', dropout: float = 0.0)\",\"Bases: AbsDecoder\",\"Initialize the module.\",\"forward(hs_pad: Tensor, hlens: Tensor) → Tuple[Tensor, Tensor]\",\"LinearDecoder Forward.\",\"Parameters:\",\"hs_pad – (B, Tmax, D)\",\"hlens – (B,)\",\"Returns: (B, n_classes)\",\"Return type: output\",\"output_size() → int\",\"Get the output size.\",\"score(ys, state, x)\",\"Classify x.\",\"Parameters:\",\"ys – Not used\",\"state – Not used\",\"x – (T, D). this should be a single sample without any padding ie batch size=1.\",\"Returns: logits over (n_classes,) state: None\",\"Return type: ret1\",\"Assumes that x is a single unpadded sequence.\"]},\"960\":{\"h\":\"espnet2.cls.lightning_callbacks.MultilabelAUPRCCallback\",\"t\":[\"source\",\"class espnet2.cls.lightning_callbacks.MultilabelAUPRCCallback\",\"Bases: Callback\",\"Computes and logs Multilabel AUPRC (mAP) at the end of each validation epoch.\",\"To use this callback, you must implement a update_mAP method in the espnet model that accepts a MultilabelAUPRC object and calls its update method with predictions and targets. For example:\",\"``\",\"`\",\"python class MyESPnetModel(AbsESPnetModel):\",\"def update_mAP(self, mAP_function: MultilabelAUPRC): : … mAP_function.update(predictions, targets) …\",\"``\",\"` The model should also have a get_vocab_size() function that specifies the number of labels/classes.\",\"compute_mAP(trainer)\",\"Computes the mAP.\",\"on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)\",\"Called when the train batch ends.\"]},\"961\":{\"h\":\"NOTE\",\"t\":[\"The value outputs[\\\"loss\\\"] here will be the normalized value w.r.t accumulate_grad_batches of the loss returned from training_step.\",\"on_train_start(trainer, pl_module)\",\"Called when the train begins.\",\"on_validation_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0)\",\"Called when the validation batch ends.\",\"on_validation_epoch_end(trainer, pl_module)\",\"Called when the val epoch ends.\",\"on_validation_epoch_start(trainer, pl_module)\",\"Called when the val epoch begins.\",\"on_validation_start(trainer, pl_module)\",\"Called when the validation loop begins.\",\"setup_mAP(model)\"]},\"962\":{\"h\":\"espnet2.cls.espnet_model.label_to_onehot\",\"t\":[\"source\",\"espnet2.cls.espnet_model.label_to_onehot(label: Tensor, label_lengths: Tensor, vocab_size: int, classification_type: str, lsm_weight: float = 0.0) → Tensor\",\"Convert label to onehot.\",\"Args : label: (Batch, Length) pad value should be -1 label_lengths: (Batch,) only used in asserts vocab_size: int classification_type: str “multi-class” or “multi-label” lsm_weight: float, label smoothing weight\",\"Returns : onehot: (Batch, Length, vocab_size)\"]},\"963\":{\"h\":\"espnet2.diar.attractor.abs_attractor.AbsAttractor\",\"t\":[\"source\",\"class espnet2.diar.attractor.abs_attractor.AbsAttractor(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(enc_input: Tensor, ilens: Tensor, dec_input: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"964\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"965\":{\"h\":\"espnet2.diar.decoder.abs_decoder.AbsDecoder\",\"t\":[\"source\",\"class espnet2.diar.decoder.abs_decoder.AbsDecoder(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"966\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract property num_spk\"]},\"967\":{\"h\":\"espnet2.diar.abs_diar.AbsDiarization\",\"t\":[\"source\",\"class espnet2.diar.abs_diar.AbsDiarization(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor) → Tuple[Tensor, Tensor, OrderedDict]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"968\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract forward_rawwav(input: Tensor, ilens: Tensor) → Tuple[Tensor, Tensor, OrderedDict]\"]},\"969\":{\"h\":\"espnet2.diar.layers.abs_mask.AbsMask\",\"t\":[\"source\",\"class espnet2.diar.layers.abs_mask.AbsMask(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input, ilens, bottleneck_feat, num_spk) → Tuple[Tuple[Tensor], Tensor, OrderedDict]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"970\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract property max_num_spk : int\"]},\"971\":{\"h\":\"espnet2.diar.layers.tcn_nomask.ChannelwiseLayerNorm\",\"t\":[\"source\",\"class espnet2.diar.layers.tcn_nomask.ChannelwiseLayerNorm(channel_size)\",\"Bases: Module\",\"Channel-wise Layer Normalization (cLN).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y)\",\"Forward.\",\"Parameters:y – [M, N, K], M is batch size, N is channel size, K is length\",\"Returns: [M, N, K]\",\"Return type: cLN_y\",\"reset_parameters()\"]},\"972\":{\"h\":\"espnet2.diar.layers.tcn_nomask.Chomp1d\",\"t\":[\"source\",\"class espnet2.diar.layers.tcn_nomask.Chomp1d(chomp_size)\",\"Bases: Module\",\"To ensure the output length is the same as the input.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward.\",\"Parameters:x – [M, H, Kpad]\",\"Returns: [M, H, K]\"]},\"973\":{\"h\":\"espnet2.diar.layers.tcn_nomask.DepthwiseSeparableConv\",\"t\":[\"source\",\"class espnet2.diar.layers.tcn_nomask.DepthwiseSeparableConv(in_channels, out_channels, kernel_size, stride, padding, dilation, norm_type='gLN', causal=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward.\",\"Parameters:x – [M, H, K]\",\"Returns: [M, B, K]\",\"Return type: result\"]},\"974\":{\"h\":\"espnet2.diar.espnet_model.ESPnetDiarizationModel\",\"t\":[\"source\",\"class espnet2.diar.espnet_model.ESPnetDiarizationModel(frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, label_aggregator: Module, encoder: AbsEncoder, decoder: AbsDecoder, attractor: AbsAttractor | None, diar_weight: float = 1.0, attractor_weight: float = 1.0)\",\"Bases: AbsESPnetModel\",\"Speaker Diarization model\",\"If “attractor” is “None”, SA-EEND will be used. Else if “attractor” is not “None”, EEND-EDA will be used. For the details about SA-EEND and EEND-EDA, refer to the following papers: SA-EEND: https://arxiv.org/pdf/1909.06247.pdf EEND-EDA: https://arxiv.org/pdf/2005.09921.pdf, https://arxiv.org/pdf/2106.10654.pdf\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"attractor_loss(att_prob, label)\",\"static calc_diarization_error(pred, label, length)\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, spk_labels: Tensor | None = None, spk_labels_lengths: Tensor | None = None, **kwargs) → Dict[str, Tensor]\",\"create_length_mask(length, max_len, num_output)\",\"encode(speech: Tensor, speech_lengths: Tensor, bottleneck_feats: Tensor, bottleneck_feats_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Frontend + Encoder\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch,)\",\"bottleneck_feats – (Batch, Length, …): used for enh + diar\",\"forward(speech: Tensor, speech_lengths: Tensor | None = None, spk_labels: Tensor | None = None, spk_labels_lengths: Tensor | None = None, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, samples)\",\"speech_lengths – (Batch,) default None for chunk interator, because the chunk-iterator does not have the speech_lengths returned. see in espnet2/iterators/chunk_iter_factory.py\",\"spk_labels – (Batch, )\",\"kwargs – “utt_id” is among the input.\",\"pit_loss(pred, label, lengths)\",\"pit_loss_single_permute(pred, label, length)\"]},\"975\":{\"h\":\"espnet2.diar.layers.tcn_nomask.GlobalLayerNorm\",\"t\":[\"source\",\"class espnet2.diar.layers.tcn_nomask.GlobalLayerNorm(channel_size)\",\"Bases: Module\",\"Global Layer Normalization (gLN).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y)\",\"Forward.\",\"Parameters:y – [M, N, K], M is batch size, N is channel size, K is length\",\"Returns: [M, N, K]\",\"Return type: gLN_y\",\"reset_parameters()\"]},\"976\":{\"h\":\"espnet2.diar.label_processor.LabelProcessor\",\"t\":[\"source\",\"class espnet2.diar.label_processor.LabelProcessor(win_length: int = 512, hop_length: int = 128, center: bool = True)\",\"Bases: Module\",\"Label aggregator for speaker diarization\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor)\",\"Forward.\",\"Parameters:\",\"input – (Batch, Nsamples, Label_dim)\",\"ilens – (Batch)\",\"Returns: (Batch, Frames, Label_dim) olens: (Batch)\",\"Return type: output\"]},\"977\":{\"h\":\"espnet2.diar.decoder.linear_decoder.LinearDecoder\",\"t\":[\"source\",\"class espnet2.diar.decoder.linear_decoder.LinearDecoder(encoder_output_size: int, num_spk: int = 2)\",\"Bases: AbsDecoder\",\"Linear decoder for speaker diarization\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – hidden_space [Batch, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"property num_spk\"]},\"978\":{\"h\":\"espnet2.diar.layers.multi_mask.MultiMask\",\"t\":[\"source\",\"class espnet2.diar.layers.multi_mask.MultiMask(input_dim: int, bottleneck_dim: int = 128, max_num_spk: int = 3, mask_nonlinear='relu')\",\"Bases: AbsMask\",\"Multiple 1x1 convolution layer Module.\",\"This module corresponds to the final 1x1 conv block and non-linear function in TCNSeparator. This module has multiple 1x1 conv blocks. One of them is selected according to the given num_spk to handle flexible num_spk.\",\"Parameters:\",\"input_dim – Number of filters in autoencoder\",\"bottleneck_dim – Number of channels in bottleneck 1 * 1-conv block\",\"max_num_spk – Number of mask_conv1x1 modules (>= Max number of speakers in the dataset)\",\"mask_nonlinear – use which non-linear function to generate mask\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, bottleneck_feat: Tensor, num_spk: int) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Keep this API same with TasNet.\",\"Parameters:\",\"input – [M, K, N], M is batch size\",\"ilens (torch.Tensor) – (M,)\",\"bottleneck_feat – [M, K, B]\",\"num_spk – number of speakers\",\"**(**Training – oracle,\",\"Inference – estimated by other module (e.g, EEND-EDA))\",\"Returns: [(M, K, N), …] ilens (torch.Tensor): (M,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property max_num_spk : int\"]},\"979\":{\"h\":\"espnet2.diar.attractor.rnn_attractor.RnnAttractor\",\"t\":[\"source\",\"class espnet2.diar.attractor.rnn_attractor.RnnAttractor(encoder_output_size: int, layer: int = 1, unit: int = 512, dropout: float = 0.1, attractor_grad: bool = True)\",\"Bases: AbsAttractor\",\"encoder decoder attractor for speaker diarization\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(enc_input: Tensor, ilens: Tensor, dec_input: Tensor)\",\"Forward.\",\"Parameters:\",\"enc_input (torch.Tensor) – hidden_space [Batch, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"dec_input (torch.Tensor) – decoder input (zeros) [Batch, num_spk + 1, F]\",\"Returns: [Batch, num_spk + 1, F] att_prob: [Batch, num_spk + 1, 1]\",\"Return type: attractor\"]},\"980\":{\"h\":\"espnet2.diar.separator.tcn_separator_nomask.TCNSeparatorNomask\",\"t\":[\"source\",\"class espnet2.diar.separator.tcn_separator_nomask.TCNSeparatorNomask(input_dim: int, layer: int = 8, stack: int = 3, bottleneck_dim: int = 128, hidden_dim: int = 512, kernel: int = 3, causal: bool = False, norm_type: str = 'gLN')\",\"Bases: AbsSeparator\",\"Temporal Convolution Separator\",\"Note that this separator is equivalent to TCNSeparator except for not having the mask estimation part. This separator outputs the intermediate bottleneck feats (which is used as the input to diarization branch in enh_diar task). This separator is followed by MultiMask module, which estimates the masks.\",\"Parameters:\",\"input_dim – input feature dimension\",\"layer – int, number of layers in each stack.\",\"stack – int, number of stacks\",\"bottleneck_dim – bottleneck dimension\",\"hidden_dim – number of convolution channel\",\"kernel – int, kernel size.\",\"causal – bool, defalut False.\",\"norm_type – str, choose from ‘BN’, ‘gLN’, ‘cLN’\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor) → Tuple[Tensor, Tensor]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"Returns: [B, T, bottleneck_dim] ilens (torch.Tensor): (B,)\",\"Return type: feats (torch.Tensor)\",\"property num_spk\",\"property output_dim : int\"]},\"981\":{\"h\":\"espnet2.diar.layers.tcn_nomask.TemporalBlock\",\"t\":[\"source\",\"class espnet2.diar.layers.tcn_nomask.TemporalBlock(in_channels, out_channels, kernel_size, stride, padding, dilation, norm_type='gLN', causal=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward.\",\"Parameters:x – [M, B, K]\",\"Returns: [M, B, K]\"]},\"982\":{\"h\":\"espnet2.diar.layers.tcn_nomask.TemporalConvNet\",\"t\":[\"source\",\"class espnet2.diar.layers.tcn_nomask.TemporalConvNet(N, B, H, P, X, R, norm_type='gLN', causal=False)\",\"Bases: Module\",\"Basic Module of tasnet.\",\"Parameters:\",\"N – Number of filters in autoencoder\",\"B – Number of channels in bottleneck 1 * 1-conv block\",\"H – Number of channels in convolutional blocks\",\"P – Kernel size in convolutional blocks\",\"X – Number of convolutional blocks in each repeat\",\"R – Number of repeats\",\"norm_type – BN, gLN, cLN\",\"causal – causal or non-causal\",\"forward(mixture_w)\",\"Keep this API same with TasNet.\",\"Parameters:mixture_w – [M, N, K], M is batch size\",\"Returns: [M, B, K]\",\"Return type: bottleneck_feature\"]},\"983\":{\"h\":\"espnet2.diar.layers.tcn_nomask.check_nonlinear\",\"t\":[\"source\",\"espnet2.diar.layers.tcn_nomask.check_nonlinear(nolinear_type)\"]},\"984\":{\"h\":\"espnet2.diar.layers.tcn_nomask.chose_norm\",\"t\":[\"source\",\"espnet2.diar.layers.tcn_nomask.chose_norm(norm_type, channel_size)\",\"The input of normalization will be (M, C, K), where M is batch size.\",\"C is channel size and K is sequence length.\"]},\"985\":{\"h\":\"espnet2.fileio.datadir_writer.DatadirWriter\",\"t\":[\"source\",\"class espnet2.fileio.datadir_writer.DatadirWriter(p: Path | str)\",\"Bases: object\",\"Writer class to create kaldi like data directory.\"]},\"986\":{\"h\":\"Examples\",\"t\":[\">>> with DatadirWriter(\\\"output\\\") as writer: ... # output/sub.txt is created here ... subwriter = writer[\\\"sub.txt\\\"] ... # Write \\\"uttidA some/where/a.wav\\\" ... subwriter[\\\"uttidA\\\"] = \\\"some/where/a.wav\\\" ... subwriter[\\\"uttidB\\\"] = \\\"some/where/b.wav\\\"\",\"close()\"]},\"987\":{\"h\":\"espnet2.fileio.rand_gen_dataset.FloatRandomGenerateDataset\",\"t\":[\"source\",\"class espnet2.fileio.rand_gen_dataset.FloatRandomGenerateDataset(shape_file: Path | str, dtype: str | dtype = 'float32', loader_type: str = 'csv_int')\",\"Bases: Mapping\",\"Generate float array from shape.txt.\"]},\"988\":{\"h\":\"Examples\",\"t\":[\"shape.txt uttA 123,83 uttB 34,83\",\"dataset = FloatRandomGenerateDataset(“shape.txt”) array = dataset[“uttA”] assert array.shape == (123, 83) array = dataset[“uttB”] assert array.shape == (34, 83)\"]},\"989\":{\"h\":\"espnet2.fileio.rand_gen_dataset.IntRandomGenerateDataset\",\"t\":[\"source\",\"class espnet2.fileio.rand_gen_dataset.IntRandomGenerateDataset(shape_file: Path | str, low: int, high: int | None = None, dtype: str | dtype = 'int64', loader_type: str = 'csv_int')\",\"Bases: Mapping\",\"Generate float array from shape.txt\"]},\"990\":{\"h\":\"Examples\",\"t\":[\"shape.txt uttA 123,83 uttB 34,83\",\"dataset = IntRandomGenerateDataset(“shape.txt”, low=0, high=10) array = dataset[“uttA”] assert array.shape == (123, 83) array = dataset[“uttB”] assert array.shape == (34, 83)\"]},\"991\":{\"h\":\"espnet2.fileio.score_scp.MIDReader\",\"t\":[\"source\",\"class espnet2.fileio.score_scp.MIDReader(fname: ~pathlib.Path | str, add_rest: bool = True, dtype: type = <class 'numpy.int16'>)\",\"Bases: Mapping\",\"Reader class for ‘mid.scp’.\"]},\"992\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/a.mid key2 /some/path/b.mid key3 /some/path/c.mid key4 /some/path/d.mid …\",\">>> reader = XMLScpReader('mid.scp') >>> tempo, note_list = reader['key1']\",\"get_path(key)\",\"keys() → a set-like object providing a view on D's keys\"]},\"993\":{\"h\":\"espnet2.fileio.multi_sound_scp.MultiSoundScpReader\",\"t\":[\"source\",\"class espnet2.fileio.multi_sound_scp.MultiSoundScpReader(fname, dtype=None, always_2d: bool = False, stack_axis=0, pad=nan)\",\"Bases: Mapping\",\"Reader class for ‘wav.scp’ containing multiple sounds.\",\"This is useful when loading variable numbers of audios for different samples.\"]},\"994\":{\"h\":\"Examples\",\"t\":[\"wav.scp is a text file that looks like the following:\",\"key1 /some/path/a1.wav /another/path/a2.wav /yet/another/path/a3.wav key2 /some/path/b1.wav /another/path/b2.wav key3 /some/path/c1.wav /another/path/c2.wav /yet/another/path/c3.wav key4 /some/path/d1.wav …\",\">>> reader = SoundScpReader('wav.scp', stack_axis=0) >>> rate, stacked_arrays = reader['key1'] >>> assert stacked_arrays.shape[0] == 3\",\"Note: : All audios in each sample must have the same sampling rates. Audios of different lengths in each sample will be right-padded with np.nan <br/>\",\"to the same length.\",\"get_path(key)\",\"keys() → a set-like object providing a view on D's keys\",\"pad_to_same_length(arrays, pad=nan, axis=0)\",\"Right-pad arrays to the same length.\",\"Parameters:\",\"arrays (List *[*np.ndarray]) – List of arrays to pad\",\"pad (float) – Value to pad\",\"axis (int) – Axis to pad\",\"Returns: Padded array\",\"Return type: np.ndarray\"]},\"995\":{\"h\":\"espnet2.fileio.score_scp.NOTE\",\"t\":[\"source\",\"class espnet2.fileio.score_scp.NOTE(lyric, midi, st, et)\",\"Bases: object\"]},\"996\":{\"h\":\"espnet2.fileio.npy_scp.NpyScpReader\",\"t\":[\"source\",\"class espnet2.fileio.npy_scp.NpyScpReader(fname: Path | str)\",\"Bases: Mapping\",\"Reader class for a scp file of numpy file.\"]},\"997\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/a.npy key2 /some/path/b.npy key3 /some/path/c.npy key4 /some/path/d.npy …\",\">>> reader = NpyScpReader('npy.scp') >>> array = reader['key1']\",\"get_path(key)\",\"keys() → a set-like object providing a view on D's keys\"]},\"998\":{\"h\":\"espnet2.fileio.npy_scp.NpyScpWriter\",\"t\":[\"source\",\"class espnet2.fileio.npy_scp.NpyScpWriter(outdir: Path | str, scpfile: Path | str)\",\"Bases: object\",\"Writer class for a scp file of numpy file.\"]},\"999\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/a.npy key2 /some/path/b.npy key3 /some/path/c.npy key4 /some/path/d.npy …\",\">>> writer = NpyScpWriter('./data/', './data/feat.scp') >>> writer['aa'] = numpy_array >>> writer['bb'] = numpy_array\",\"close()\",\"get_path(key)\"]},\"1000\":{\"h\":\"espnet2.fileio.read_text.RandomTextReader\",\"t\":[\"source\",\"class espnet2.fileio.read_text.RandomTextReader(text_and_scp: str)\",\"Bases: Mapping\",\"Reader class for random access to text.\",\"Simple text reader for non-pair text data (for unsupervised ASR) : Instead of loading the whole text into memory (often large for UASR), the reader consumes text which stores in byte-offset of each text file and randomly selected unpaired text from it for training using mmap.\",\"Examples: : text : text1line text2line text3line <br/> scp : 11 00000000000000000010 00000000110000000020 00000000210000000030 <br/> scp explanation : (number of digits per int value) (text start at bytes 0 and end at bytes 10 (including “\",\"“)) : (text start at bytes 11 and end at bytes 20 (including “\",\"“)) : (text start at bytes 21 and end at bytes 30 (including “\",\"“))\",\"keys() → a set-like object providing a view on D's keys\"]},\"1001\":{\"h\":\"espnet2.fileio.rttm.RttmReader\",\"t\":[\"source\",\"class espnet2.fileio.rttm.RttmReader(fname: str)\",\"Bases: Mapping\",\"Reader class for ‘rttm.scp’.\"]},\"1002\":{\"h\":\"Examples\",\"t\":[\"SPEAKER file1 1 0 1023 <NA> <NA> spk1 <NA> SPEAKER file1 2 4000 3023 <NA> <NA> spk2 <NA> SPEAKER file1 3 500 4023 <NA> <NA> spk1 <NA> END file1 <NA> 4023 <NA> <NA> <NA> <NA>\",\"This is an extend version of standard RTTM format for espnet. The difference including:\",\"Use sample number instead of absolute time\",\"has a END label to represent the duration of a recording\",\"replace duration (5th field) with end time (For standard RTTM,\",\"see https://catalog.ldc.upenn.edu/docs/LDC2004T12/RTTM-format-v13.pdf)\",\"…\",\">>> reader = RttmReader('rttm') >>> spk_label = reader[\\\"file1\\\"]\",\"keys() → a set-like object providing a view on D's keys\"]},\"1003\":{\"h\":\"espnet2.fileio.score_scp.SingingScoreReader\",\"t\":[\"source\",\"class espnet2.fileio.score_scp.SingingScoreReader(fname: ~pathlib.Path | str, dtype: type = <class 'numpy.int16'>)\",\"Bases: Mapping\",\"Reader class for ‘score.scp’.\"]},\"1004\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/score.json key2 /some/path/score.json key3 /some/path/score.json key4 /some/path/score.json …\",\">>> reader = SoundScpReader('score.scp') >>> score = reader['key1']\",\"get_path(key)\",\"keys() → a set-like object providing a view on D's keys\"]},\"1005\":{\"h\":\"espnet2.fileio.score_scp.SingingScoreWriter\",\"t\":[\"source\",\"class espnet2.fileio.score_scp.SingingScoreWriter(outdir: Path | str, scpfile: Path | str)\",\"Bases: object\",\"Writer class for ‘score.scp’\"]},\"1006\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/score.json key2 /some/path/score.json key3 /some/path/score.json key4 /some/path/score.json …\",\">>> writer = SingingScoreWriter('./data/', './data/score.scp') >>> writer['aa'] = score_obj >>> writer['bb'] = score_obj\",\"close()\",\"get_path(key)\"]},\"1007\":{\"h\":\"espnet2.fileio.sound_scp.SoundScpReader\",\"t\":[\"source\",\"class espnet2.fileio.sound_scp.SoundScpReader(fname, dtype=None, always_2d: bool = False, multi_columns: bool = False, concat_axis=1)\",\"Bases: Mapping\",\"Reader class for ‘wav.scp’.\"]},\"1008\":{\"h\":\"Examples\",\"t\":[\"wav.scp is a text file that looks like the following:\",\"key1 /some/path/a.wav key2 /some/path/b.wav key3 /some/path/c.wav key4 /some/path/d.wav …\",\">>> reader = SoundScpReader('wav.scp') >>> rate, array = reader['key1']\",\"If multi_columns=True is given and multiple files are given in one line with space delimiter, and the output array are concatenated along channel direction\",\"key1 /some/path/a.wav /some/path/a2.wav key2 /some/path/b.wav /some/path/b2.wav …\",\">>> reader = SoundScpReader('wav.scp', multi_columns=True) >>> rate, array = reader['key1']\",\"In the above case, a.wav and a2.wav are concatenated.\",\"Note that even if multi_columns=True is given, SoundScpReader still supports a normal wav.scp, i.e., a wav file is given per line, but this option is disable by default because dict[str, list[str]] object is needed to be kept, but it increases the required amount of memory.\",\"get_path(key)\",\"keys() → a set-like object providing a view on D's keys\"]},\"1009\":{\"h\":\"espnet2.fileio.sound_scp.SoundScpWriter\",\"t\":[\"source\",\"class espnet2.fileio.sound_scp.SoundScpWriter(outdir: Path | str, scpfile: Path | str, format='wav', multi_columns: bool = False, output_name_format: str = '{key}.{audio_format}', output_name_format_multi_columns: str = '{key}-CH{channel}.{audio_format}', subtype: str | None = None)\",\"Bases: object\",\"Writer class for ‘wav.scp’\",\"Parameters:\",\"outdir\",\"scpfile\",\"format – The output audio format\",\"multi_columns – Save multi channel data as multiple monaural audio files\",\"output_name_format – The naming formam of generated audio files\",\"output_name_format_multi_columns – The naming formam of generated audio files when multi_columns is given\",\"dtype\",\"subtype\"]},\"1010\":{\"h\":\"Examples\",\"t\":[\">>> writer = SoundScpWriter('./data/', './data/wav.scp') >>> writer['aa'] = 16000, numpy_array >>> writer['bb'] = 16000, numpy_array\",\"aa ./data/aa.wav bb ./data/bb.wav\",\">>> writer = SoundScpWriter( './data/', './data/feat.scp', multi_columns=True, ) >>> numpy_array.shape (100, 2) >>> writer['aa'] = 16000, numpy_array\",\"aa ./data/aa-CH0.wav ./data/aa-CH1.wav\",\"close()\",\"get_path(key)\"]},\"1011\":{\"h\":\"espnet2.fileio.vad_scp.VADScpReader\",\"t\":[\"source\",\"class espnet2.fileio.vad_scp.VADScpReader(fname, dtype=<class 'numpy.float32'>)\",\"Bases: Mapping\",\"Reader class for ‘vad.scp’.\",\"Different from segments, the vad.scp would focus on utterance-level, while the segments are expected to focus on a whole session. The major usage in ESPnet is to guide the silence trim for UASR.\"]},\"1012\":{\"h\":\"Examples\",\"t\":[\"key1 0:1.2000 key2 3.0000:4.5000 7.0000:9:0000 …\",\">>> reader = VADScpReader('wav.scp') >>> array = reader['key1']\",\"keys() → a set-like object providing a view on D's keys\"]},\"1013\":{\"h\":\"espnet2.fileio.vad_scp.VADScpWriter\",\"t\":[\"source\",\"class espnet2.fileio.vad_scp.VADScpWriter(scpfile: Path | str, dtype=None)\",\"Bases: object\",\"Writer class for ‘vad.scp’\"]},\"1014\":{\"h\":\"Examples\",\"t\":[\"key1 0:1.2000 key2 3.0000:4.5000 7.0000:9:0000 …\",\">>> writer = VADScpWriter('./data/vad.scp') >>> writer['aa'] = list of tuples >>> writer['bb'] = list of tuples\",\"close()\"]},\"1015\":{\"h\":\"espnet2.fileio.score_scp.XMLReader\",\"t\":[\"source\",\"class espnet2.fileio.score_scp.XMLReader(fname: ~pathlib.Path | str, dtype: type = <class 'numpy.int16'>)\",\"Bases: Mapping\",\"Reader class for ‘xml.scp’.\"]},\"1016\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/a.xml key2 /some/path/b.xml key3 /some/path/c.xml key4 /some/path/d.xml …\",\">>> reader = XMLScpReader('xml.scp') >>> tempo, note_list = reader['key1']\",\"get_path(key)\",\"keys() → a set-like object providing a view on D's keys\"]},\"1017\":{\"h\":\"espnet2.fileio.score_scp.XMLWriter\",\"t\":[\"source\",\"class espnet2.fileio.score_scp.XMLWriter(outdir: Path | str, scpfile: Path | str)\",\"Bases: object\",\"Writer class for ‘midi.scp’\"]},\"1018\":{\"h\":\"Examples\",\"t\":[\"key1 /some/path/a.musicxml key2 /some/path/b.musicxml key3 /some/path/c.musicxml key4 /some/path/d.musicxml …\",\">>> writer = XMLScpWriter('./data/', './data/xml.scp') >>> writer['aa'] = xml_obj >>> writer['bb'] = xml_obj\",\"close()\",\"get_path(key)\"]},\"1019\":{\"h\":\"espnet2.fileio.read_text.load_num_sequence_text\",\"t\":[\"source\",\"espnet2.fileio.read_text.load_num_sequence_text(path: Path | str, loader_type: str = 'csv_int') → Dict[str, List[float | int]]\",\"Read a text file indicating sequences of number\"]},\"1020\":{\"h\":\"Examples\",\"t\":[\"key1 1 2 3 key2 34 5 6\",\">>> d = load_num_sequence_text('text') >>> np.testing.assert_array_equal(d[\\\"key1\\\"], np.array([1, 2, 3]))\"]},\"1021\":{\"h\":\"espnet2.fileio.rttm.load_rttm_text\",\"t\":[\"source\",\"espnet2.fileio.rttm.load_rttm_text(path: Path | str) → Dict[str, List[Tuple[str, float, float]]]\",\"Read a RTTM file\",\"Note: only support speaker information now\"]},\"1022\":{\"h\":\"espnet2.fileio.read_text.read_2columns_text\",\"t\":[\"source\",\"espnet2.fileio.read_text.read_2columns_text(path: Path | str, keys_to_load: Set[str | int] | None = None) → Dict[str, str]\",\"Read a text file having 2 columns as dict object.\",\"Only load the keys in keys_to_load if it is not None.\"]},\"1023\":{\"h\":\"Examples\",\"t\":[\"wav.scp: : key1 /some/path/a.wav key2 /some/path/b.wav\",\">>> read_2columns_text('wav.scp') {'key1': '/some/path/a.wav', 'key2': '/some/path/b.wav'}\"]},\"1024\":{\"h\":\"espnet2.fileio.read_text.read_label\",\"t\":[\"source\",\"espnet2.fileio.read_text.read_label(path: Path | str) → Dict[str, List[List[str | float | int]]]\",\"Read a text file indicating sequences of number\"]},\"1025\":{\"h\":\"Examples\",\"t\":[\"key1 start_time_1 end_time_1 phone_1 start_time_2 end_time_2 phone_2 ….\",\"key2 start_time_1 end_time_1 phone_1\",\">>> d = load_num_sequence_text('label') >>> np.testing.assert_array_equal(d[\\\"key1\\\"], [0.1, 0.2, \\\"啊\\\"]))\"]},\"1026\":{\"h\":\"espnet2.fileio.read_text.read_multi_columns_text\",\"t\":[\"source\",\"espnet2.fileio.read_text.read_multi_columns_text(path: Path | str, return_unsplit: bool = False) → Tuple[Dict[str, List[str]], Dict[str, str] | None]\",\"Read a text file having 2 or more columns as dict object.\"]},\"1027\":{\"h\":\"Examples\",\"t\":[\"wav.scp: : key1 /some/path/a1.wav /some/path/a2.wav key2 /some/path/b1.wav /some/path/b2.wav /some/path/b3.wav key3 /some/path/c1.wav …\",\">>> read_multi_columns_text('wav.scp') {'key1': ['/some/path/a1.wav', '/some/path/a2.wav'], 'key2': ['/some/path/b1.wav', '/some/path/b2.wav', '/some/path/b3.wav'], 'key3': ['/some/path/c1.wav']}\"]},\"1028\":{\"h\":\"espnet2.fileio.sound_scp.soundfile_read\",\"t\":[\"source\",\"espnet2.fileio.sound_scp.soundfile_read(wavs: str | List[str], dtype=None, always_2d: bool = False, concat_axis: int = 1, start: int = 0, end: int | None = None, return_subtype: bool = False) → Tuple[array, int]\"]},\"1029\":{\"h\":\"espnet2.enh.layers.uses2_comp.ATFBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.uses2_comp.ATFBlock(input_size, input_resolution=(130, 64), window_size=(10, 8), mlp_ratio=4, qkv_bias=True, qk_scale=None, dropout=0.0, att_dropout=0.0, drop_path=0.0, use_checkpoint=False, rnn_type='lstm', hidden_size=128, att_heads=4, activation='relu', bidirectional=True, norm_type='cLN', ch_mode='att', ch_att_dim=256, eps=1e-05, with_channel_modeling=True)\",\"Bases: Module\",\"Container module for a single Attentive Time-Frequency Block.\",\"Parameters:\",\"input_size (int) – dimension of the input feature.\",\"input_resolution (tuple) – frequency and time dimension of the input feature. Only used for efficient training. Should be close to the actual spectrum size (F, T) of training samples.\",\"window_size (tuple) – size of the Time-Frequency window in Swin-Transformer.\",\"mlp_ratio (int) – ratio of the MLP hidden size to embedding size in BasicLayer.\",\"qkv_bias (bool) – If True, add a learnable bias to query, key, value in BasicLayer.\",\"qk_scale (float) – Override default qk scale of head_dim ** -0.5 in BasicLayer if set.\",\"dropout (float) – dropout ratio. Default is 0.\",\"att_dropout (float) – attention dropout ratio in BasicLayer. Default is 0.\",\"drop_path (float) – drop-path ratio in BasicLayer. Default is 0.\",\"use_checkpoint (bool) – whether to use checkpointing to save memory.\",\"rnn_type (str) – type of the RNN cell in the improved Transformer layer.\",\"hidden_size (int) – hidden dimension of the RNN cell.\",\"att_heads (int) – number of attention heads in Transformer.\",\"dropout – dropout ratio. Default is 0.\",\"activation (str) – non-linear activation function applied in each block.\",\"bidirectional (bool) – whether the RNN layers are bidirectional.\",\"norm_type (str) – normalization type in the improved Transformer layer.\",\"ch_mode (str) – mode of channel modeling. Select from “att”, “tac”, and “att_tac”.\",\"ch_att_dim (int) – dimension of the channel attention.\",\"eps (float) – epsilon for layer normalization.\",\"with_channel_modeling (bool) – whether to use channel attention.\",\"forward(input, ref_channel=None, mem_size=20)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – feature sequence (batch, C, N, freq, time)\",\"ref_channel (Noneorint) – index of the reference channel.\",\"mem_size (int) – length of the memory tokens\",\"Returns: output sequence (batch, C, N, freq, time)\",\"Return type: output (torch.Tensor)\",\"freq_path_process(x)\",\"pad_to_window_multiples(input, window_size)\",\"Pad the input feature to multiples of the window size.\",\"Parameters:\",\"input (torch.Tensor) – input feature (…, freq, time)\",\"window_size (tuple) – size of the window (H, W).\",\"Returns: padded input feature (…, n * H, m * W)\",\"Return type: output (torch.Tensor)\",\"time_freq_process(x)\",\"time_path_process(x)\"]},\"1030\":{\"h\":\"espnet2.enh.decoder.abs_decoder.AbsDecoder\",\"t\":[\"source\",\"class espnet2.enh.decoder.abs_decoder.AbsDecoder(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor, fs: int | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1031\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_streaming(input_frame: Tensor)\",\"streaming_merge(chunks: Tensor, ilens: tensor | None = None)\",\"Stream merge.\",\"It merges the frame-level processed audio chunks in the streaming simulation. It is noted that, in real applications, the processed audio should be sent to the output channel frame by frame. You may refer to this function to manage your streaming output buffer.\",\"Parameters:\",\"chunks – List [(B, frame_size),]\",\"ilens – [B]\",\"Returns: [B, T]\",\"Return type: merge_audio\"]},\"1032\":{\"h\":\"espnet2.enh.diffusion.abs_diffusion.AbsDiffusion\",\"t\":[\"source\",\"class espnet2.enh.diffusion.abs_diffusion.AbsDiffusion(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract enhance(input: Tensor)\",\"abstract forward(input: Tensor, ilens: Tensor)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1033\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1034\":{\"h\":\"espnet2.enh.encoder.abs_encoder.AbsEncoder\",\"t\":[\"source\",\"class espnet2.enh.encoder.abs_encoder.AbsEncoder(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor, fs: int | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1035\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_streaming(input: Tensor)\",\"abstract property output_dim : int\",\"streaming_frame(audio: Tensor)\",\"Stream frame.\",\"It splits the continuous audio into frame-level audio chunks in the streaming simulation. It is noted that this function takes the entire long audio as input for a streaming simulation. You may refer to this function to manage your streaming input buffer in a real streaming application.\",\"Parameters:audio – (B, T)\",\"Returns: List [(B, frame_size),]\",\"Return type: chunked\"]},\"1036\":{\"h\":\"espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss(*args, **kwargs)\",\"Bases: Module, ABC\",\"Base class for all Enhancement loss modules.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(ref, inf) → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1037\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"property name : str\",\"property only_for_test : bool\"]},\"1038\":{\"h\":\"espnet2.enh.abs_enh.AbsEnhancement\",\"t\":[\"source\",\"class espnet2.enh.abs_enh.AbsEnhancement(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor) → Tuple[Tensor, Tensor, OrderedDict]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1039\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract forward_rawwav(input: Tensor, ilens: Tensor) → Tuple[Tensor, Tensor, OrderedDict]\"]},\"1040\":{\"h\":\"espnet2.enh.extractor.abs_extractor.AbsExtractor\",\"t\":[\"source\",\"class espnet2.enh.extractor.abs_extractor.AbsExtractor(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor, input_aux: Tensor, ilens_aux: Tensor, suffix_tag: str = '', additional: Dict | None = None) → Tuple[Tuple[Tensor], Tensor, OrderedDict]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1041\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1042\":{\"h\":\"espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper\",\"t\":[\"source\",\"class espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper(*args, **kwargs)\",\"Bases: Module, ABC\",\"Base class for all Enhancement loss wrapper modules.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(ref: List, inf: List, others: Dict) → Tuple[Tensor, Dict, Dict]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1043\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"weight = 1.0\"]},\"1044\":{\"h\":\"espnet2.enh.separator.abs_separator.AbsSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.abs_separator.AbsSeparator(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, ilens: Tensor, additional: Dict | None = None) → Tuple[Tuple[Tensor], Tensor, OrderedDict]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1045\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_streaming(input_frame: Tensor, buffer=None)\",\"abstract property num_spk\"]},\"1046\":{\"h\":\"espnet2.enh.separator.tfgridnetv3_separator.AllHeadPReLULayerNormalization4DC\",\"t\":[\"source\",\"class espnet2.enh.separator.tfgridnetv3_separator.AllHeadPReLULayerNormalization4DC(input_dimension, eps=1e-05)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1047\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1048\":{\"h\":\"espnet2.enh.separator.tfgridnetv2_separator.AllHeadPReLULayerNormalization4DCF\",\"t\":[\"source\",\"class espnet2.enh.separator.tfgridnetv2_separator.AllHeadPReLULayerNormalization4DCF(input_dimension, eps=1e-05)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1049\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1050\":{\"h\":\"espnet2.enh.diffusion.sampling.correctors.AnnealedLangevinDynamics\",\"t\":[\"source\",\"class espnet2.enh.diffusion.sampling.correctors.AnnealedLangevinDynamics(sde, score_fn, snr, n_steps)\",\"Bases: Corrector\",\"The original annealed Langevin dynamics predictor in NCSN/NCSNv2.\",\"update_fn(x, t, *args)\",\"One update of the corrector.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A PyTorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1051\":{\"h\":\"espnet2.enh.layers.dcunet.ArgsComplexMultiplicationWrapper\",\"t\":[\"source\",\"class espnet2.enh.layers.dcunet.ArgsComplexMultiplicationWrapper(module_cls, *args, **kwargs)\",\"Bases: Module\",\"Adapted from asteroid’s complex_nn.py, allowing\",\"args/kwargs to be passed through forward().\",\"Make a complex-valued module F from a real-valued module f by applying complex multiplication rules:\",\"F(a + i b) = f1(a) - f1(b) + i (f2(b) + f2(a))\",\"where f1, f2 are instances of f that do not share weights.\",\"Parameters:module_cls (callable) – A class or function that returns a Torch module/functional. Constructor of f in the formula above. Called 2x with *args, **kwargs, to construct the real and imaginary component modules.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, *args, **kwargs)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1052\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1053\":{\"h\":\"espnet2.enh.separator.asteroid_models.AsteroidModel_Converter\",\"t\":[\"source\",\"class espnet2.enh.separator.asteroid_models.AsteroidModel_Converter(encoder_output_dim: int, model_name: str, num_spk: int, pretrained_path: str = '', loss_type: str = 'si_snr', **model_related_kwargs)\",\"Bases: AbsSeparator\",\"The class to convert the models from asteroid to AbsSeprator.\",\"Parameters:\",\"encoder_output_dim – input feature dimension, default=1 after the NullEncoder\",\"num_spk – number of speakers\",\"loss_type – loss type of enhancement\",\"model_name – Asteroid model names, e.g. ConvTasNet, DPTNet. Refers to https://github.com/asteroid-team/asteroid/ blob/master/asteroid/models/_init_.py\",\"pretrained_path – the name of pretrained model from Asteroid in HF hub. Refers to: https://github.com/asteroid-team/asteroid/ blob/master/docs/source/readmes/pretrained_models.md and https://huggingface.co/models?filter=asteroid\",\"model_related_kwargs – more args towards each specific asteroid model.\",\"forward(input: Tensor, ilens: Tensor | None = None, additional: Dict | None = None)\",\"Whole forward of asteroid models.\",\"Parameters:\",\"input (torch.Tensor) – Raw Waveforms [B, T]\",\"ilens (torch.Tensor) – input lengths [B]\",\"additional (DictorNone) – other data included in model\",\"Returns: [(B, T), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, T), ‘mask_spk2’: torch.Tensor(Batch, T), … ‘mask_spkn’: torch.Tensor(Batch, T),\",\"]\",\"Return type: estimated Waveforms(List[Union(torch.Tensor])\",\"forward_rawwav(input: Tensor, ilens: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Output with waveforms.\",\"property num_spk\"]},\"1054\":{\"h\":\"espnet2.enh.layers.dnn_beamformer.AttentionReference\",\"t\":[\"source\",\"class espnet2.enh.layers.dnn_beamformer.AttentionReference(bidim, att_dim, eps=1e-06)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(psd_in: Tensor | ComplexTensor, ilens: LongTensor, scaling: float = 2.0) → Tuple[Tensor, LongTensor]\",\"Attention-based reference forward function.\",\"Parameters:\",\"psd_in (torch.complex64/ComplexTensor) – (B, F, C, C)\",\"ilens (torch.Tensor) – (B,)\",\"scaling (float)\",\"Returns: (B, C) ilens (torch.Tensor): (B,)\",\"Return type: u (torch.Tensor)\"]},\"1055\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.AttnBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.AttnBlock(channels)\",\"Bases: Module\",\"Channel-wise self-attention block.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1056\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1057\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layerspp.AttnBlockpp\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layerspp.AttnBlockpp(channels, skip_rescale=False, init_scale=0.0)\",\"Bases: Module\",\"Channel-wise self-attention block. Modified from DDPM.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1058\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1059\":{\"h\":\"espnet2.enh.layers.fasnet.BF_module\",\"t\":[\"source\",\"class espnet2.enh.layers.fasnet.BF_module(input_dim, feature_dim, hidden_dim, output_dim, num_spk=2, layer=4, segment_size=100, bidirectional=True, dropout=0.0, fasnet_type='ifasnet')\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, num_mic)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1060\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1061\":{\"h\":\"espnet2.enh.layers.bsrnn.BSRNN\",\"t\":[\"source\",\"class espnet2.enh.layers.bsrnn.BSRNN(input_dim=481, num_channel=16, num_layer=6, target_fs=48000, subbands=None, causal=False, num_spk=1, norm_type='GN')\",\"Bases: Module\",\"Band-Split RNN (BSRNN).\",\"References\",\"[1] J. Yu, H. Chen, Y. Luo, R. Gu, and C. Weng, “High fidelity speech enhancement with band-split RNN,” in Proc. ISCA Interspeech, 2023. https://isca-speech.org/archive/interspeech_2023/yu23b_interspeech.html [2] J. Yu, and Y. Luo, “Efficient monaural speech enhancement with universal sample rate band-split RNN,” in Proc. ICASSP, 2023. https://ieeexplore.ieee.org/document/10096020\",\"Parameters:\",\"input_dim (int) – maximum number of frequency bins corresponding to target_fs\",\"num_channel (int) – embedding dimension of each time-frequency bin\",\"num_layer (int) – number of time and frequency RNN layers\",\"target_fs (int) – maximum sampling frequency supported by the model\",\"subbands (listortuple,optional) – list of subband sizes to split the frequency band into. If specified, this will override the subband definition in the BandSplit class.\",\"causal (bool) – Whether or not to adopt causal processing if True, LSTM will be used instead of BLSTM for time modeling\",\"num_spk (int) – number of outputs to be generated\",\"norm_type (str) – type of normalization layer (cfLN / cLN / BN / GN)\",\"forward(x, fs=None)\",\"BSRNN forward.\",\"Parameters:\",\"x (torch.Tensor) – input tensor of shape (B, T, F, 2)\",\"fs (int,optional) – sampling rate of the input signal. if not None, the input signal will be truncated to only process the effective frequency subbands. if None, the input signal is assumed to be already truncated to only contain effective frequency subbands.\",\"Returns: output tensor of shape (B, num_spk, T, F, 2)\",\"Return type: out (torch.Tensor)\"]},\"1062\":{\"h\":\"espnet2.enh.separator.bsrnn_separator.BSRNNSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.bsrnn_separator.BSRNNSeparator(input_dim: int, num_spk: int = 1, num_channels: int = 16, num_layers: int = 6, target_fs: int = 48000, subbands: List[Tuple[int, int]] | None = None, causal: bool = False, norm_type: str = 'GN', predict_noise: bool = False, ref_channel: int | None = None)\",\"Bases: AbsSeparator\",\"Band-split RNN (BSRNN) separator.\",\"Reference: : [1] J. Yu, H. Chen, Y. Luo, R. Gu, and C. Weng, “High fidelity speech enhancement with band-split RNN,” in Proc. ISCA Interspeech, 2023. https://isca-speech.org/archive/interspeech_2023/yu23b_interspeech.html [2] J. Yu, and Y. Luo, “Efficient monaural speech enhancement with universal sample rate band-split RNN,” in Proc. ICASSP, 2023. https://ieeexplore.ieee.org/document/10096020\",\"Parameters:\",\"input_dim – (int) maximum number of frequency bins corresponding to target_fs\",\"num_spk – (int) number of speakers.\",\"num_channels – (int) feature dimension in the BandSplit block.\",\"num_layers – (int) number of processing layers.\",\"target_fs – (int) max sampling frequency that the model can handle.\",\"subbands (listortuple,optional) – list of subband sizes to split the frequency band into. If specified, this will override the subband definition in the BandSplit class.\",\"causal (bool) – whether or not to apply causal modeling. if True, LSTM will be used instead of BLSTM for time modeling\",\"norm_type (str) – type of the normalization layer (cfLN / cLN / BN / GN).\",\"predict_noise (bool) – whether or not to additionally predict the nosie signal in the input speech\",\"ref_channel – (int) reference channel. not used for now.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"BSRNN Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – STFT spectrum [B, T, (C,) F (,2)]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model. unused in this model.\",\"Returns: [(B, T, F), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1063\":{\"h\":\"espnet2.enh.layers.bsrnn.BandSplit\",\"t\":[\"source\",\"class espnet2.enh.layers.bsrnn.BandSplit(input_dim, target_fs=48000, subbands=None, channels=128, norm_type='GN')\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"static convert_size_to_indices(subbands, input_dim)\",\"Convert subband sizes to the corresponding start and end indices.\",\"forward(x, fs=None)\",\"BandSplit forward.\",\"Parameters:\",\"x (torch.Tensor) – input tensor of shape (B, T, F, 2)\",\"fs (int,optional) – sampling rate of the input signal. if not None, the input signal will be truncated to only process the effective frequency subbands. if None, the input signal is assumed to be already truncated to only contain effective frequency subbands.\",\"Returns: output tensor of shape (B, N, T, K’) : K’ might be smaller than len(self.subbands) if fs < self.target_fs.\",\"Return type: z (torch.Tensor)\",\"static validate_subbands(subbands, input_dim, target_fs=48000)\",\"Validate the subbands.\"]},\"1064\":{\"h\":\"espnet2.enh.layers.swin_transformer.BasicLayer\",\"t\":[\"source\",\"class espnet2.enh.layers.swin_transformer.BasicLayer(dim, input_resolution, depth, num_heads, window_size, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop=0.0, attn_drop=0.0, drop_path=0.0, norm_layer=<class 'torch.nn.modules.normalization.LayerNorm'>, use_checkpoint=False)\",\"Bases: Module\",\"A basic Swin Transformer layer for one stage.\",\"Parameters:\",\"dim (int) – Number of input channels.\",\"input_resolution (tuple *[*int]) – Input resolution.\",\"depth (int) – Number of blocks.\",\"num_heads (int) – Number of attention heads.\",\"window_size (int) – Local window size.\",\"mlp_ratio (float) – Ratio of MLP hidden dim to embedding dim.\",\"qkv_bias (bool,optional) – If True, add a learnable bias to query, key, value.\",\"qk_scale (float|None,optional) – If not None, override default qk scale.\",\"drop (float,optional) – Dropout rate.\",\"attn_drop (float,optional) – Attention dropout rate.\",\"drop_path (float|tuple *[*float],optional) – Stochastic depth rate.\",\"norm_layer (nn.Module,optional) – Normalization layer.\",\"use_checkpoint (bool) – Whether to use checkpointing to save memory.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr() → str\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(x, x_size)\",\"BasicLayer Forward.\",\"Parameters:\",\"x (Tensor) – Input feature with shape (B, H x W, C).\",\"x_size (tuple *[*int]) – Heigth and width of the input feature (H, W).\"]},\"1065\":{\"h\":\"espnet2.enh.layers.dcunet.BatchNorm\",\"t\":[\"source\",\"class espnet2.enh.layers.dcunet.BatchNorm(num_features: int, eps: float = 1e-05, momentum: float | None = 0.1, affine: bool = True, track_running_stats: bool = True, device=None, dtype=None)\",\"Bases: _BatchNorm\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"]},\"1066\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.CISDRLoss\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.time_domain.CISDRLoss(filter_length=512, name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"CI-SDR loss\",\"Reference: : Convolutive Transfer Function Invariant SDR Training Criteria for Multi-Channel Reverberant Speech Separation; C. Boeddeker et al., 2021; https://arxiv.org/abs/2011.15003\",\"Parameters:\",\"ref – (Batch, samples)\",\"inf – (Batch, samples)\",\"filter_length (int) – a time-invariant filter that allows slight distortion via filtering\",\"Returns: (Batch,)\",\"Return type: loss\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref: Tensor, inf: Tensor) → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1067\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1068\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.CRPBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.CRPBlock(features, n_stages, act=ReLU(), maxpool=True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1069\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1070\":{\"h\":\"espnet2.enh.layers.uses.ChannelAttention\",\"t\":[\"source\",\"class espnet2.enh.layers.uses.ChannelAttention(input_dim, att_heads=4, att_dim=256, activation='relu', eps=1e-05)\",\"Bases: Module\",\"Channel Attention module.\",\"Parameters:\",\"input_dim (int) – dimension of the input feature.\",\"att_heads (int) – number of attention heads in self-attention.\",\"att_dim (int) – projection dimension for query and key before self-attention.\",\"activation (str) – non-linear activation function.\",\"eps (float) – epsilon for layer normalization.\",\"forward(x, ref_channel=None)\",\"ChannelAttention Forward.\",\"Parameters:\",\"x (torch.Tensor) – input feature (batch, C, N, freq, time)\",\"ref_channel (Noneorint) – index of the reference channel.\",\"Returns: output feature (batch, C, N, freq, time)\",\"Return type: output (torch.Tensor)\"]},\"1071\":{\"h\":\"espnet2.enh.layers.uses2_swin.ChannelAttentionTAC\",\"t\":[\"source\",\"class espnet2.enh.layers.uses2_swin.ChannelAttentionTAC(input_dim, att_heads=4, att_dim=256, activation='relu', eps=1e-05)\",\"Bases: Module\",\"Channel Transform-Attention-Concatenate (TAttC) module.\",\"Parameters:\",\"input_dim (int) – dimension of the input feature.\",\"att_heads (int) – number of attention heads in self-attention.\",\"att_dim (int) – projection dimension for query and key before self-attention.\",\"activation (str) – non-linear activation function.\",\"eps (float) – epsilon for layer normalization.\",\"forward(x, ref_channel=None)\",\"TAttC Forward.\",\"Parameters:\",\"x (torch.Tensor) – input feature (batch, C, N, freq, time)\",\"ref_channel (Noneorint) – index of the reference channel.\",\"Returns: output feature (batch, C, N, freq, time)\",\"Return type: output (torch.Tensor)\"]},\"1072\":{\"h\":\"espnet2.enh.layers.bsrnn.ChannelFreqwiseLayerNorm\",\"t\":[\"source\",\"class espnet2.enh.layers.bsrnn.ChannelFreqwiseLayerNorm(channel_size, shape='BDTF')\",\"Bases: Module\",\"Channel-and-Frequency-wise Layer Normalization (cfLN).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y)\",\"Forward.\",\"Parameters:y – [M, N, T, K], M is batch size, N is channel size, T and K are lengths\",\"Returns: [M, N, T, K]\",\"Return type: gLN_y\",\"reset_parameters()\"]},\"1073\":{\"h\":\"espnet2.enh.layers.uses.ChannelTAC\",\"t\":[\"source\",\"class espnet2.enh.layers.uses.ChannelTAC(input_dim, eps=1e-05)\",\"Bases: Module\",\"Channel Transform-Average-Concatenate (TAC) module.\",\"Parameters:\",\"input_dim (int) – dimension of the input feature.\",\"eps (float) – epsilon for layer normalization.\",\"forward(x, ref_channel=None)\",\"ChannelTAC Forward.\",\"Parameters:\",\"x (torch.Tensor) – input feature (batch, C, N, freq, time)\",\"ref_channel (Noneorint) – index of the reference channel.\",\"Returns: output feature (batch, C, N, freq, time)\",\"Return type: output (torch.Tensor)\"]},\"1074\":{\"h\":\"espnet2.enh.layers.bsrnn.ChannelwiseLayerNorm\",\"t\":[\"source\",\"class espnet2.enh.layers.bsrnn.ChannelwiseLayerNorm(channel_size, shape='BDTF')\",\"Bases: Module\",\"Channel-wise Layer Normalization (cLN).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y)\",\"Forward.\",\"Parameters:y – [M, N, T, K], M is batch size, N is channel size, T and K are lengths\",\"Returns: [M, N, T, K]\",\"Return type: cLN_y\",\"reset_parameters()\"]},\"1075\":{\"h\":\"espnet2.enh.layers.tcn.Chomp1d\",\"t\":[\"source\",\"class espnet2.enh.layers.tcn.Chomp1d(chomp_size)\",\"Bases: Module\",\"To ensure the output length is the same as the input.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward.\",\"Parameters:x – [M, H, Kpad]\",\"Returns: [M, H, K]\"]},\"1076\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layerspp.Combine\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layerspp.Combine(dim1, dim2, method='cat')\",\"Bases: Module\",\"Combine information from skip connections.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1077\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1078\":{\"h\":\"espnet2.enh.layers.complexnn.ComplexBatchNorm\",\"t\":[\"source\",\"class espnet2.enh.layers.complexnn.ComplexBatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, complex_axis=1)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(inputs)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1079\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"reset_parameters()\",\"reset_running_stats()\"]},\"1080\":{\"h\":\"espnet2.enh.layers.complexnn.ComplexConv2d\",\"t\":[\"source\",\"class espnet2.enh.layers.complexnn.ComplexConv2d(in_channels, out_channels, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=1, groups=1, causal=True, complex_axis=1)\",\"Bases: Module\",\"ComplexConv2d.\",\"in_channels: real+imag out_channels: real+imag kernel_size : input [B,C,D,T] kernel size in [D,T] padding : input [B,C,D,T] padding in [D,T] causal: if causal, will padding time dimension’s left side,\",\"otherwise both\",\"forward(inputs)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1081\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1082\":{\"h\":\"espnet2.enh.layers.complexnn.ComplexConvTranspose2d\",\"t\":[\"source\",\"class espnet2.enh.layers.complexnn.ComplexConvTranspose2d(in_channels, out_channels, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), output_padding=(0, 0), causal=False, complex_axis=1, groups=1)\",\"Bases: Module\",\"ComplexConvTranspose2d.\",\"in_channels: real+imag out_channels: real+imag\",\"forward(inputs)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1083\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1084\":{\"h\":\"espnet2.enh.layers.dcunet.ComplexLinear\",\"t\":[\"source\",\"class espnet2.enh.layers.dcunet.ComplexLinear(input_dim, output_dim, complex_valued)\",\"Bases: Module\",\"A potentially complex-valued linear layer. Reduces to a regular linear\",\"layer if complex_valued=False.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1085\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1086\":{\"h\":\"espnet2.enh.layers.adapt_layers.ConcatAdaptLayer\",\"t\":[\"source\",\"class espnet2.enh.layers.adapt_layers.ConcatAdaptLayer(indim, enrolldim, ninputs=1)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(main, enroll)\",\"ConcatAdaptLayer forward.\",\"Parameters:\",\"main –\",\"tensor or tuple or list activations in the main neural network, which are adapted tuple/list may be useful when we want to apply the adaptation\",\"to both normal and skip connection at once\",\"enroll –\",\"tensor or tuple or list embedding extracted from enrollment tuple/list may be useful when we want to apply the adaptation\",\"to both normal and skip connection at once\"]},\"1087\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.CondCRPBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.CondCRPBlock(features, n_stages, num_classes, normalizer, act=ReLU())\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1088\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1089\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.CondMSFBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.CondMSFBlock(in_planes, features, num_classes, normalizer)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs, y, shape)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1090\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1091\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.CondRCUBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.CondRCUBlock(features, n_blocks, n_stages, num_classes, normalizer, act=ReLU())\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1092\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1093\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.CondRefineBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.CondRefineBlock(in_planes, features, num_classes, normalizer, act=ReLU(), start=False, end=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs, y, output_shape)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1094\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1095\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalBatchNorm2d\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalBatchNorm2d(num_features, num_classes, bias=True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1096\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1097\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalInstanceNorm2d\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalInstanceNorm2d(num_features, num_classes, bias=True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1098\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1099\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalInstanceNorm2dPlus\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalInstanceNorm2dPlus(num_features, num_classes, bias=True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1100\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1101\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalNoneNorm2d\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalNoneNorm2d(num_features, num_classes, bias=True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1102\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1103\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ConditionalResidualBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.ConditionalResidualBlock(input_dim, output_dim, num_classes, resample=1, act=ELU(alpha=1.0), normalization=<class 'espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalInstanceNorm2dPlus'>, adjust_padding=False, dilation=None)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1104\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1105\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalVarianceNorm2d\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.normalization.ConditionalVarianceNorm2d(num_features, num_classes, bias=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, y)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1106\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1107\":{\"h\":\"espnet2.enh.separator.conformer_separator.ConformerSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.conformer_separator.ConformerSeparator(input_dim: int, num_spk: int = 2, predict_noise: bool = False, adim: int = 384, aheads: int = 4, layers: int = 6, linear_units: int = 1536, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, normalize_before: bool = False, concat_after: bool = False, dropout_rate: float = 0.1, input_layer: str = 'linear', positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1, nonlinear: str = 'relu', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, conformer_enc_kernel_size: int = 7, padding_idx: int = -1)\",\"Bases: AbsSeparator\",\"Conformer separator.\",\"Parameters:\",\"input_dim – input feature dimension\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"adim (int) – Dimension of attention.\",\"aheads (int) – The number of heads of multi head attention.\",\"linear_units (int) – The number of units of position-wise feed forward.\",\"layers (int) – The number of transformer blocks.\",\"dropout_rate (float) – Dropout rate.\",\"input_layer (Union *[*str,torch.nn.Module]) – Input layer type.\",\"attention_dropout_rate (float) – Dropout rate in attention.\",\"positional_dropout_rate (float) – Dropout rate after adding positional encoding.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"conformer_pos_enc_layer_type (str) – Encoder positional encoding layer type.\",\"conformer_self_attn_layer_type (str) – Encoder attention layer type.\",\"conformer_activation_type (str) – Encoder activation function type.\",\"positionwise_layer_type (str) – “linear”, “conv1d”, or “conv1d-linear”.\",\"positionwise_conv_kernel_size (int) – Kernel size of positionwise conv1d layer.\",\"use_macaron_style_in_conformer (bool) – Whether to use macaron style for positionwise layer.\",\"use_cnn_in_conformer (bool) – Whether to use convolution module.\",\"conformer_enc_kernel_size (int) – Kernerl size of convolution module.\",\"padding_idx (int) – Padding idx for input_layer=embed.\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1108\":{\"h\":\"espnet2.enh.layers.tcndenseunet.Conv2DActNorm\",\"t\":[\"source\",\"class espnet2.enh.layers.tcndenseunet.Conv2DActNorm(in_channels, out_channels, ksz=(3, 3), stride=(1, 2), padding=(1, 0), upsample=False, activation=<class 'torch.nn.modules.activation.ELU'>)\",\"Bases: Module\",\"Basic Conv2D + activation + instance norm building block.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inp)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1109\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1110\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.Conv2d\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.Conv2d(in_ch, out_ch, kernel, up=False, down=False, resample_kernel=(1, 3, 3, 1), use_bias=True, kernel_init=None)\",\"Bases: Module\",\"Conv2d layer with optimal upsampling and downsampling (StyleGAN2).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1111\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1112\":{\"h\":\"espnet2.enh.decoder.conv_decoder.ConvDecoder\",\"t\":[\"source\",\"class espnet2.enh.decoder.conv_decoder.ConvDecoder(channel: int, kernel_size: int, stride: int)\",\"Bases: AbsDecoder\",\"Transposed Convolutional decoder for speech enhancement and separation\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, fs: int | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – spectrum [Batch, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"fs (int) – sampling rate in Hz (Not used)\",\"forward_streaming(input_frame: Tensor)\",\"streaming_merge(chunks: Tensor, ilens: tensor | None = None)\",\"Stream Merge.\",\"It merges the frame-level processed audio chunks in the streaming simulation. It is noted that, in real applications, the processed audio should be sent to the output channel frame by frame. You may refer to this function to manage your streaming output buffer.\",\"Parameters:\",\"chunks – List [(B, frame_size),]\",\"ilens – [B]\",\"Returns: [B, T]\",\"Return type: merge_audio\"]},\"1113\":{\"h\":\"espnet2.enh.encoder.conv_encoder.ConvEncoder\",\"t\":[\"source\",\"class espnet2.enh.encoder.conv_encoder.ConvEncoder(channel: int, kernel_size: int, stride: int)\",\"Bases: AbsEncoder\",\"Convolutional encoder for speech enhancement and separation\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, fs: int | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – mixed speech [Batch, sample]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"fs (int) – sampling rate in Hz (Not used)\",\"Returns: mixed feature after encoder [Batch, flens, channel]\",\"Return type: feature (torch.Tensor)\",\"forward_streaming(input: Tensor)\",\"property output_dim : int\",\"streaming_frame(audio: Tensor)\",\"Stream frame.\",\"It splits the continuous audio into frame-level audio chunks in the streaming simulation. It is noted that this function takes the entire long audio as input for a streaming simulation. You may refer to this function to manage your streaming input buffer in a real streaming application.\",\"Parameters:audio – (B, T)\",\"Returns: List [(B, frame_size),]\",\"Return type: chunked\"]},\"1114\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ConvMeanPool\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.ConvMeanPool(input_dim, output_dim, kernel_size=3, biases=True, adjust_padding=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inputs)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1115\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1116\":{\"h\":\"espnet2.enh.diffusion.sampling.correctors.Corrector\",\"t\":[\"source\",\"class espnet2.enh.diffusion.sampling.correctors.Corrector(sde, score_fn, snr, n_steps)\",\"Bases: ABC\",\"The abstract class for a corrector algorithm.\",\"abstract update_fn(x, t, *args)\",\"One update of the corrector.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A PyTorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1117\":{\"h\":\"espnet2.enh.separator.dan_separator.DANSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.dan_separator.DANSeparator(input_dim: int, rnn_type: str = 'blstm', num_spk: int = 2, nonlinear: str = 'tanh', layer: int = 2, unit: int = 512, emb_D: int = 40, dropout: float = 0.0)\",\"Bases: AbsSeparator\",\"Deep Attractor Network Separator\",\"Reference: : DEEP ATTRACTOR NETWORK FOR SINGLE-MICROPHONE SPEAKER SEPARATION; Zhuo Chen. et al., 2017; https://pubmed.ncbi.nlm.nih.gov/29430212/\",\"Parameters:\",\"input_dim – input feature dimension\",\"rnn_type – string, select from ‘blstm’, ‘lstm’ etc.\",\"bidirectional – bool, whether the inter-chunk RNN layers are bidirectional.\",\"num_spk – number of speakers\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of stacked RNN layers. Default is 3.\",\"unit – int, dimension of the hidden state.\",\"emb_D – int, dimension of the attribute vector for one tf-bin.\",\"dropout – float, dropout ratio. Default is 0.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model e.g. “feature_ref”: list of reference spectra List[(B, T, F)]\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1118\":{\"h\":\"espnet2.enh.separator.dccrn_separator.DCCRNSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.dccrn_separator.DCCRNSeparator(input_dim: int, num_spk: int = 1, rnn_layer: int = 2, rnn_units: int = 256, masking_mode: str = 'E', use_clstm: bool = True, bidirectional: bool = False, use_cbn: bool = False, kernel_size: int = 5, kernel_num: List[int] = [32, 64, 128, 256, 256, 256], use_builtin_complex: bool = True, use_noise_mask: bool = False)\",\"Bases: AbsSeparator\",\"DCCRN separator.\",\"Parameters:\",\"input_dim (int) – input dimension。\",\"num_spk (int,optional) – number of speakers. Defaults to 1.\",\"rnn_layer (int,optional) – number of lstm layers in the crn. Defaults to 2.\",\"rnn_units (int,optional) – rnn units. Defaults to 128.\",\"masking_mode (str,optional) – usage of the estimated mask. Defaults to “E”.\",\"use_clstm (bool,optional) – whether use complex LSTM. Defaults to False.\",\"bidirectional (bool,optional) – whether use BLSTM. Defaults to False.\",\"use_cbn (bool,optional) – whether use complex BN. Defaults to False.\",\"kernel_size (int,optional) – convolution kernel size. Defaults to 5.\",\"kernel_num (list,optional) – output dimension of each layer of the encoder.\",\"use_builtin_complex (bool,optional) – torch.complex if True, else ComplexTensor.\",\"use_noise_mask (bool,optional) – whether to estimate the mask of noise.\",\"apply_masks(masks: List[Tensor | ComplexTensor], real: Tensor, imag: Tensor)\",\"apply masks\",\"Parameters:\",\"masks – est_masks, [(B, T, F), …]\",\"real (torch.Tensor) – real part of the noisy spectrum, (B, F, T)\",\"imag (torch.Tensor) – imag part of the noisy spectrum, (B, F, T)\",\"Returns: [(B, T, F), …]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"create_masks(mask_tensor: Tensor)\",\"create estimated mask for each speaker\",\"Parameters:mask_tensor (torch.Tensor) – output of decoder, shape(B, 2*num_spk, F-1, T)\",\"flatten_parameters()\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, F), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1119\":{\"h\":\"espnet2.enh.layers.dcunet.DCUNet\",\"t\":[\"source\",\"class espnet2.enh.layers.dcunet.DCUNet(dcunet_architecture: str = 'DilDCUNet-v2', dcunet_time_embedding: str = 'gfp', dcunet_temb_layers_global: int = 2, dcunet_temb_layers_local: int = 1, dcunet_temb_activation: str = 'silu', dcunet_time_embedding_complex: bool = False, dcunet_fix_length: str = 'pad', dcunet_mask_bound: str = 'none', dcunet_norm_type: str = 'bN', dcunet_activation: str = 'relu', embed_dim: int = 128, **kwargs)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"fix_input_dims(x)\",\"fix_output_dims(out, x)\",\"forward(spec, t) → Tensor\",\"Input shape is expected to be $(batch, nfreqs, time)$, with $nfreqs - 1$\",\"divisible by $f_0 * f_1 * … * f_N$ where $f_k$ are the frequency strides of the encoders, and $time - 1$ is divisible by $t_0 * t_1 * … * t_N$ where $t_N$ are the time strides of the encoders. :param spec: complex spectrogram tensor. 1D, 2D or 3D tensor, time last. :type spec: Tensor\",\"Returns: Tensor, of shape (batch, time) or (time).\"]},\"1120\":{\"h\":\"espnet2.enh.layers.dcunet.DCUNetComplexDecoderBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.dcunet.DCUNetComplexDecoderBlock(in_chan, out_chan, kernel_size, stride, padding, dilation, output_padding=(0, 0), norm_type='bN', activation='leaky_relu', embed_dim=None, temb_layers=1, temb_activation='swish', complex_time_embedding=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, t_embed, output_size=None)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1121\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1122\":{\"h\":\"espnet2.enh.layers.dcunet.DCUNetComplexEncoderBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.dcunet.DCUNetComplexEncoderBlock(in_chan, out_chan, kernel_size, stride, padding, dilation, norm_type='bN', activation='leaky_relu', embed_dim=None, complex_time_embedding=False, temb_layers=1, temb_activation='silu')\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, t_embed)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1123\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1124\":{\"h\":\"espnet2.enh.layers.dc_crn.DC_CRN\",\"t\":[\"source\",\"class espnet2.enh.layers.dc_crn.DC_CRN(input_dim, input_channels: List = [2, 16, 32, 64, 128, 256], enc_hid_channels=8, enc_kernel_size=(1, 3), enc_padding=(0, 1), enc_last_kernel_size=(1, 4), enc_last_stride=(1, 2), enc_last_padding=(0, 1), enc_layers=5, skip_last_kernel_size=(1, 3), skip_last_stride=(1, 1), skip_last_padding=(0, 1), glstm_groups=2, glstm_layers=2, glstm_bidirectional=False, glstm_rearrange=False, output_channels=2)\",\"Bases: Module\",\"Densely-Connected Convolutional Recurrent Network (DC-CRN).\",\"Reference: Fig. 3 and Section III-B in [1]\",\"Parameters:\",\"input_dim (int) – input feature dimension\",\"input_channels (list) – number of input channels for the stacked DenselyConnectedBlock layers Its length should be (number of DenselyConnectedBlock layers). It is recommended to use even number of channels to avoid AssertError when glstm_bidirectional=True.\",\"enc_hid_channels (int) – common number of intermediate channels for all DenselyConnectedBlock of the encoder\",\"enc_kernel_size (tuple) – common kernel size for all DenselyConnectedBlock of the encoder\",\"enc_padding (tuple) – common padding for all DenselyConnectedBlock of the encoder\",\"enc_last_kernel_size (tuple) – common kernel size for the last Conv layer in all DenselyConnectedBlock of the encoder\",\"enc_last_stride (tuple) – common stride for the last Conv layer in all DenselyConnectedBlock of the encoder\",\"enc_last_padding (tuple) – common padding for the last Conv layer in all DenselyConnectedBlock of the encoder\",\"enc_layers (int) – common total number of Conv layers for all DenselyConnectedBlock layers of the encoder\",\"skip_last_kernel_size (tuple) – common kernel size for the last Conv layer in all DenselyConnectedBlock of the skip pathways\",\"skip_last_stride (tuple) – common stride for the last Conv layer in all DenselyConnectedBlock of the skip pathways\",\"skip_last_padding (tuple) – common padding for the last Conv layer in all DenselyConnectedBlock of the skip pathways\",\"glstm_groups (int) – number of groups in each Grouped LSTM layer\",\"glstm_layers (int) – number of Grouped LSTM layers\",\"glstm_bidirectional (bool) – whether to use BLSTM or unidirectional LSTM in Grouped LSTM layers\",\"glstm_rearrange (bool) – whether to apply the rearrange operation after each grouped LSTM layer\",\"output_channels (int) – number of output channels (must be an even number to recover both real and imaginary parts)\",\"forward(x)\",\"DC-CRN forward.\",\"Parameters:x (torch.Tensor) – Concatenated real and imaginary spectrum features (B, input_channels[0], T, F)\",\"Returns: (B, 2, output_channels, T, F)\",\"Return type: out (torch.Tensor)\"]},\"1125\":{\"h\":\"espnet2.enh.separator.dc_crn_separator.DC_CRNSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.dc_crn_separator.DC_CRNSeparator(input_dim: int, num_spk: int = 2, predict_noise: bool = False, input_channels: List = [2, 16, 32, 64, 128, 256], enc_hid_channels: int = 8, enc_kernel_size: Tuple = (1, 3), enc_padding: Tuple = (0, 1), enc_last_kernel_size: Tuple = (1, 4), enc_last_stride: Tuple = (1, 2), enc_last_padding: Tuple = (0, 1), enc_layers: int = 5, skip_last_kernel_size: Tuple = (1, 3), skip_last_stride: Tuple = (1, 1), skip_last_padding: Tuple = (0, 1), glstm_groups: int = 2, glstm_layers: int = 2, glstm_bidirectional: bool = False, glstm_rearrange: bool = False, mode: str = 'masking', ref_channel: int = 0)\",\"Bases: AbsSeparator\",\"Densely-Connected Convolutional Recurrent Network (DC-CRN) Separator\",\"Reference: : Deep Learning Based Real-Time Speech Enhancement for Dual-Microphone Mobile Phones; Tan et al., 2020 https://web.cse.ohio-state.edu/~wang.77/papers/TZW.taslp21.pdf\",\"Parameters:\",\"input_dim – input feature dimension\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"input_channels (list) – number of input channels for the stacked DenselyConnectedBlock layers Its length should be (number of DenselyConnectedBlock layers).\",\"enc_hid_channels (int) – common number of intermediate channels for all DenselyConnectedBlock of the encoder\",\"enc_kernel_size (tuple) – common kernel size for all DenselyConnectedBlock of the encoder\",\"enc_padding (tuple) – common padding for all DenselyConnectedBlock of the encoder\",\"enc_last_kernel_size (tuple) – common kernel size for the last Conv layer in all DenselyConnectedBlock of the encoder\",\"enc_last_stride (tuple) – common stride for the last Conv layer in all DenselyConnectedBlock of the encoder\",\"enc_last_padding (tuple) – common padding for the last Conv layer in all DenselyConnectedBlock of the encoder\",\"enc_layers (int) – common total number of Conv layers for all DenselyConnectedBlock layers of the encoder\",\"skip_last_kernel_size (tuple) – common kernel size for the last Conv layer in all DenselyConnectedBlock of the skip pathways\",\"skip_last_stride (tuple) – common stride for the last Conv layer in all DenselyConnectedBlock of the skip pathways\",\"skip_last_padding (tuple) – common padding for the last Conv layer in all DenselyConnectedBlock of the skip pathways\",\"glstm_groups (int) – number of groups in each Grouped LSTM layer\",\"glstm_layers (int) – number of Grouped LSTM layers\",\"glstm_bidirectional (bool) – whether to use BLSTM or unidirectional LSTM in Grouped LSTM layers\",\"glstm_rearrange (bool) – whether to apply the rearrange operation after each grouped LSTM layer\",\"output_channels (int) – number of output channels (even number)\",\"mode (str) – one of (“mapping”, “masking”) “mapping”: complex spectral mapping “masking”: complex masking\",\"ref_channel (int) – index of the reference microphone\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"DC-CRN Separator Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [Batch, T, F] or [Batch, T, C, F]\",\"ilens (torch.Tensor) – input lengths [Batch,]\",\"Returns: [(Batch, T, F), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1126\":{\"h\":\"espnet2.enh.layers.dnn_beamformer.DNN_Beamformer\",\"t\":[\"source\",\"class espnet2.enh.layers.dnn_beamformer.DNN_Beamformer(bidim, btype: str = 'blstmp', blayers: int = 3, bunits: int = 300, bprojs: int = 320, num_spk: int = 1, use_noise_mask: bool = True, nonlinear: str = 'sigmoid', dropout_rate: float = 0.0, badim: int = 320, ref_channel: int = -1, beamformer_type: str = 'mvdr_souden', rtf_iterations: int = 2, mwf_mu: float = 1.0, eps: float = 1e-06, diagonal_loading: bool = True, diag_eps: float = 1e-07, mask_flooring: bool = False, flooring_thres: float = 1e-06, use_torch_solver: bool = True, use_torchaudio_api: bool = False, btaps: int = 5, bdelay: int = 3)\",\"Bases: Module\",\"DNN mask based Beamformer.\",\"Citation: : Multichannel End-to-end Speech Recognition; T. Ochiai et al., 2017; http://proceedings.mlr.press/v70/ochiai17a/ochiai17a.pdf\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"apply_beamforming(data, ilens, psd_n, psd_speech, psd_distortion=None, rtf_mat=None, spk=0)\",\"Beamforming with the provided statistics.\",\"Parameters:\",\"data (torch.complex64/ComplexTensor) – (B, F, C, T)\",\"ilens (torch.Tensor) – (B,)\",\"psd_n (torch.complex64/ComplexTensor) – Noise covariance matrix for MVDR (B, F, C, C) Observation covariance matrix for MPDR/wMPDR (B, F, C, C) Stacked observation covariance for WPD (B,F,(btaps+1)*C,(btaps+1)*C)\",\"psd_speech (torch.complex64/ComplexTensor) – Speech covariance matrix (B, F, C, C)\",\"psd_distortion (torch.complex64/ComplexTensor) – Noise covariance matrix (B, F, C, C)\",\"rtf_mat (torch.complex64/ComplexTensor) – RTF matrix (B, F, C, num_spk)\",\"spk (int) – speaker index\",\"Returns: (B, F, T) ws (torch.complex64/ComplexTensor): (B, F) or (B, F, (btaps+1)*C)\",\"Return type: enhanced (torch.complex64/ComplexTensor)\",\"forward(data: Tensor | ComplexTensor, ilens: LongTensor, powers: List[Tensor] | None = None, oracle_masks: List[Tensor] | None = None) → Tuple[Tensor | ComplexTensor, LongTensor, Tensor]\",\"DNN_Beamformer forward function.\",\"Notation: : B: Batch C: Channel T: Time or Sequence length F: Freq\",\"Parameters:\",\"data (torch.complex64/ComplexTensor) – (B, T, C, F)\",\"ilens (torch.Tensor) – (B,)\",\"powers (List *[*torch.Tensor] orNone) – used for wMPDR or WPD (B, F, T)\",\"oracle_masks (List *[*torch.Tensor] orNone) – oracle masks (B, F, C, T) if not None, oracle_masks will be used instead of self.mask\",\"Returns: (B, T, F) ilens (torch.Tensor): (B,) masks (torch.Tensor): (B, T, C, F)\",\"Return type: enhanced (torch.complex64/ComplexTensor)\",\"predict_mask(data: Tensor | ComplexTensor, ilens: LongTensor) → Tuple[Tuple[Tensor, ...], LongTensor]\",\"Predict masks for beamforming.\",\"Parameters:\",\"data (torch.complex64/ComplexTensor) – (B, T, C, F), double precision\",\"ilens (torch.Tensor) – (B,)\",\"Returns: (B, T, C, F) ilens (torch.Tensor): (B,)\",\"Return type: masks (torch.Tensor)\"]},\"1127\":{\"h\":\"espnet2.enh.layers.dnn_wpe.DNN_WPE\",\"t\":[\"source\",\"class espnet2.enh.layers.dnn_wpe.DNN_WPE(wtype: str = 'blstmp', widim: int = 257, wlayers: int = 3, wunits: int = 300, wprojs: int = 320, dropout_rate: float = 0.0, taps: int = 5, delay: int = 3, use_dnn_mask: bool = True, nmask: int = 1, nonlinear: str = 'sigmoid', iterations: int = 1, normalization: bool = False, eps: float = 1e-06, diagonal_loading: bool = True, diag_eps: float = 1e-07, mask_flooring: bool = False, flooring_thres: float = 1e-06, use_torch_solver: bool = True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(data: Tensor | ComplexTensor, ilens: LongTensor) → Tuple[Tensor | ComplexTensor, LongTensor, Tensor | ComplexTensor]\",\"DNN_WPE forward function.\",\"Notation: : B: Batch C: Channel T: Time or Sequence length F: Freq or Some dimension of the feature vector\",\"Parameters:\",\"data – (B, T, C, F)\",\"ilens – (B,)\",\"Returns: (B, T, C, F) ilens: (B,) masks (torch.Tensor or List[torch.Tensor]): (B, T, C, F) power (List[torch.Tensor]): (B, F, T)\",\"Return type: enhanced (torch.Tensor or List[torch.Tensor])\",\"predict_mask(data: Tensor | ComplexTensor, ilens: LongTensor) → Tuple[Tensor, LongTensor]\",\"Predict mask for WPE dereverberation.\",\"Parameters:\",\"data (torch.complex64/ComplexTensor) – (B, T, C, F), double precision\",\"ilens (torch.Tensor) – (B,)\",\"Returns: (B, T, C, F) ilens (torch.Tensor): (B,)\",\"Return type: masks (torch.Tensor or List[torch.Tensor])\"]},\"1128\":{\"h\":\"espnet2.enh.layers.dnsmos.DNSMOS_local\",\"t\":[\"source\",\"class espnet2.enh.layers.dnsmos.DNSMOS_local(primary_model_path, p808_model_path, use_gpu=False, convert_to_torch=False)\",\"Bases: object\",\"audio_melspec(audio, n_mels=120, frame_size=320, hop_length=160, sr=16000, to_db=True)\",\"get_polyfit_val(sig, bak, ovr, is_personalized_MOS)\"]},\"1129\":{\"h\":\"espnet2.enh.layers.dnsmos.DNSMOS_web\",\"t\":[\"source\",\"class espnet2.enh.layers.dnsmos.DNSMOS_web(auth_key)\",\"Bases: object\"]},\"1130\":{\"h\":\"espnet2.enh.separator.dpcl_e2e_separator.DPCLE2ESeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.dpcl_e2e_separator.DPCLE2ESeparator(input_dim: int, rnn_type: str = 'blstm', num_spk: int = 2, predict_noise: bool = False, nonlinear: str = 'tanh', layer: int = 2, unit: int = 512, emb_D: int = 40, dropout: float = 0.0, alpha: float = 5.0, max_iteration: int = 500, threshold: float = 1e-05)\",\"Bases: AbsSeparator\",\"Deep Clustering End-to-End Separator\",\"References\",\"Single-Channel Multi-Speaker Separation using Deep Clustering; Yusuf Isik. et al., 2016; https://www.isca-speech.org/archive/interspeech_2016/isik16_interspeech.html\",\"Parameters:\",\"input_dim – input feature dimension\",\"rnn_type – string, select from ‘blstm’, ‘lstm’ etc.\",\"bidirectional – bool, whether the inter-chunk RNN layers are bidirectional.\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of stacked RNN layers. Default is 3.\",\"unit – int, dimension of the hidden state.\",\"emb_D – int, dimension of the feature vector for a tf-bin.\",\"dropout – float, dropout ratio. Default is 0.\",\"alpha – float, the clustering hardness parameter.\",\"max_iteration – int, the max iterations of soft kmeans.\",\"threshold – float, the threshold to end the soft k-means process.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. V: OrderedDict[\",\"others predicted data, e.g. masks: OrderedDict[ ‘mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1131\":{\"h\":\"espnet2.enh.separator.dpcl_separator.DPCLSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.dpcl_separator.DPCLSeparator(input_dim: int, rnn_type: str = 'blstm', num_spk: int = 2, nonlinear: str = 'tanh', layer: int = 2, unit: int = 512, emb_D: int = 40, dropout: float = 0.0)\",\"Bases: AbsSeparator\",\"Deep Clustering Separator.\",\"References\",\"[1] Deep clustering: Discriminative embeddings for segmentation and : separation; John R. Hershey. et al., 2016; https://ieeexplore.ieee.org/document/7471631\",\"[2] Manifold-Aware Deep Clustering: Maximizing Angles Between Embedding : Vectors Based on Regular Simplex; Tanaka, K. et al., 2021; https://www.isca-speech.org/archive/interspeech_2021/tanaka21_interspeech.html\",\"Parameters:\",\"input_dim – input feature dimension\",\"rnn_type – string, select from ‘blstm’, ‘lstm’ etc.\",\"bidirectional – bool, whether the inter-chunk RNN layers are bidirectional.\",\"num_spk – number of speakers\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of stacked RNN layers. Default is 3.\",\"unit – int, dimension of the hidden state.\",\"emb_D – int, dimension of the feature vector for a tf-bin.\",\"dropout – float, dropout ratio. Default is 0.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. tf_embedding: OrderedDict[\",\"’tf_embedding’: learned embedding of all T-F bins (B, T * F, D),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1132\":{\"h\":\"espnet2.enh.loss.wrappers.dpcl_solver.DPCLSolver\",\"t\":[\"source\",\"class espnet2.enh.loss.wrappers.dpcl_solver.DPCLSolver(criterion: AbsEnhLoss, weight=1.0)\",\"Bases: AbsLossWrapper\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref, inf, others={})\",\"A naive DPCL solver\",\"Parameters:\",\"ref (List *[*torch.Tensor]) – [(batch, …), …] x n_spk\",\"inf (List *[*torch.Tensor]) – [(batch, …), …]\",\"others (List) – other data included in this solver e.g. “tf_embedding” learned embedding of all T-F bins (B, T * F, D)\",\"Returns: (torch.Tensor): minimum loss with the best permutation stats: (dict), for collecting training status others: reserved\",\"Return type: loss\"]},\"1133\":{\"h\":\"espnet2.enh.layers.dpmulcat.DPMulCat\",\"t\":[\"source\",\"class espnet2.enh.layers.dpmulcat.DPMulCat(input_size: int, hidden_size: int, output_size: int, num_spk: int, dropout: float = 0.0, num_layers: int = 4, bidirectional: bool = True, input_normalize: bool = False)\",\"Bases: Module\",\"Dual-path RNN module with MulCat blocks.\",\"Parameters:\",\"input_size – int, dimension of the input feature. The input should have shape (batch, seq_len, input_size).\",\"hidden_size – int, dimension of the hidden state.\",\"output_size – int, dimension of the output size.\",\"num_spk – int, the number of speakers in the output.\",\"dropout – float, the dropout rate in the LSTM layer. (Default: 0.0)\",\"bidirectional – bool, whether the RNN layers are bidirectional. (Default: True)\",\"num_layers – int, number of stacked MulCat blocks. (Default: 4)\",\"input_normalize – bool, whether to apply GroupNorm on the input Tensor. (Default: False)\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Compute output after DPMulCat module.\",\"Parameters:input (torch.Tensor) – The input feature. Tensor of shape (batch, N, dim1, dim2) Apply RNN on dim1 first and then dim2\",\"Returns: (list(torch.Tensor) or list(list(torch.Tensor)) : In training mode, the module returns output of each DPMulCat block. In eval mode, the module only returns output in the last block.\"]},\"1134\":{\"h\":\"espnet2.enh.layers.dprnn.DPRNN\",\"t\":[\"source\",\"class espnet2.enh.layers.dprnn.DPRNN(rnn_type, input_size, hidden_size, output_size, dropout=0, num_layers=1, bidirectional=True)\",\"Bases: Module\",\"Deep dual-path RNN.\",\"Parameters:\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"input_size – int, dimension of the input feature. The input should have shape (batch, seq_len, input_size).\",\"hidden_size – int, dimension of the hidden state.\",\"output_size – int, dimension of the output size.\",\"dropout – float, dropout ratio. Default is 0.\",\"num_layers – int, number of stacked RNN layers. Default is 1.\",\"bidirectional – bool, whether the RNN layers are bidirectional. Default is True.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1135\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1136\":{\"h\":\"espnet2.enh.separator.dprnn_separator.DPRNNSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.dprnn_separator.DPRNNSeparator(input_dim: int, rnn_type: str = 'lstm', bidirectional: bool = True, num_spk: int = 2, predict_noise: bool = False, nonlinear: str = 'relu', layer: int = 3, unit: int = 512, segment_size: int = 20, dropout: float = 0.0)\",\"Bases: AbsSeparator\",\"Dual-Path RNN (DPRNN) Separator\",\"Parameters:\",\"input_dim – input feature dimension\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"bidirectional – bool, whether the inter-chunk RNN layers are bidirectional.\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of stacked RNN layers. Default is 3.\",\"unit – int, dimension of the hidden state.\",\"segment_size – dual-path segment size\",\"dropout – float, dropout ratio. Default is 0.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1137\":{\"h\":\"espnet2.enh.layers.dprnn.DPRNN_TAC\",\"t\":[\"source\",\"class espnet2.enh.layers.dprnn.DPRNN_TAC(rnn_type, input_size, hidden_size, output_size, dropout=0, num_layers=1, bidirectional=True)\",\"Bases: Module\",\"Deep duaL-path RNN with TAC applied to each layer/block.\",\"Parameters:\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"input_size – int, dimension of the input feature. The input should have shape (batch, seq_len, input_size).\",\"hidden_size – int, dimension of the hidden state.\",\"output_size – int, dimension of the output size.\",\"dropout – float, dropout ratio. Default is 0.\",\"num_layers – int, number of stacked RNN layers. Default is 1.\",\"bidirectional – bool, whether the RNN layers are bidirectional. Default is False.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, num_mic)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1138\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1139\":{\"h\":\"espnet2.enh.layers.dptnet.DPTNet\",\"t\":[\"source\",\"class espnet2.enh.layers.dptnet.DPTNet(rnn_type, input_size, hidden_size, output_size, att_heads=4, dropout=0, activation='relu', num_layers=1, bidirectional=True, norm_type='gLN')\",\"Bases: Module\",\"Dual-path transformer network.\",\"Parameters:\",\"rnn_type (str) – select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"input_size (int) – dimension of the input feature. Input size must be a multiple of att_heads.\",\"hidden_size (int) – dimension of the hidden state.\",\"output_size (int) – dimension of the output size.\",\"att_heads (int) – number of attention heads.\",\"dropout (float) – dropout ratio. Default is 0.\",\"activation (str) – activation function applied at the output of RNN.\",\"num_layers (int) – number of stacked RNN layers. Default is 1.\",\"bidirectional (bool) – whether the RNN layers are bidirectional. Default is True.\",\"norm_type (str) – type of normalization to use after each inter- or intra-chunk Transformer block.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1140\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"inter_chunk_process(x, layer_index)\",\"intra_chunk_process(x, layer_index)\"]},\"1141\":{\"h\":\"espnet2.enh.separator.dptnet_separator.DPTNetSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.dptnet_separator.DPTNetSeparator(input_dim: int, post_enc_relu: bool = True, rnn_type: str = 'lstm', bidirectional: bool = True, num_spk: int = 2, predict_noise: bool = False, unit: int = 256, att_heads: int = 4, dropout: float = 0.0, activation: str = 'relu', norm_type: str = 'gLN', layer: int = 6, segment_size: int = 20, nonlinear: str = 'relu')\",\"Bases: AbsSeparator\",\"Dual-Path Transformer Network (DPTNet) Separator\",\"Parameters:\",\"input_dim – input feature dimension\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"bidirectional – bool, whether the inter-chunk RNN layers are bidirectional.\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"unit – int, dimension of the hidden state.\",\"att_heads – number of attention heads.\",\"dropout – float, dropout ratio. Default is 0.\",\"activation – activation function applied at the output of RNN.\",\"norm_type – type of normalization to use after each inter- or intra-chunk Transformer block.\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of stacked RNN layers. Default is 3.\",\"segment_size – dual-path segment size\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"merge_feature(x, length=None)\",\"property num_spk\",\"split_feature(x)\"]},\"1142\":{\"h\":\"espnet2.enh.separator.svoice_separator.Decoder\",\"t\":[\"source\",\"class espnet2.enh.separator.svoice_separator.Decoder(kernel_size)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(est_source)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1143\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1144\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.Dense\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.Dense\",\"Bases: Module\",\"Linear layer with default_init.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"]},\"1145\":{\"h\":\"espnet2.enh.layers.tcndenseunet.DenseBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.tcndenseunet.DenseBlock(in_channels, out_channels, num_freqs, pre_blocks=2, freq_proc_blocks=1, post_blocks=2, ksz=(3, 3), activation=<class 'torch.nn.modules.activation.ELU'>, hid_chans=32)\",\"Bases: Module\",\"single DenseNet block as used in iNeuBe model.\",\"Parameters:\",\"in_channels – number of input channels (image axis).\",\"out_channels – number of output channels (image axis).\",\"num_freqs – number of complex frequencies in the input STFT complex image-like tensor. The input is batch, image_channels, frames, freqs.\",\"pre_blocks – dense block before point-wise convolution block over frequency axis.\",\"freq_proc_blocks – number of frequency axis processing blocks.\",\"post_blocks – dense block after point-wise convolution block over frequency axis.\",\"ksz – kernel size used in densenet Conv2D layers.\",\"activation – activation function to use in the whole iNeuBe model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"hid_chans – number of hidden channels in densenet Conv2D.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1146\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1147\":{\"h\":\"espnet2.enh.layers.dc_crn.DenselyConnectedBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.dc_crn.DenselyConnectedBlock(in_channels, out_channels, hid_channels=8, kernel_size=(1, 3), padding=(0, 1), last_kernel_size=(1, 4), last_stride=(1, 2), last_padding=(0, 1), last_output_padding=(0, 0), layers=5, transposed=False)\",\"Bases: Module\",\"Densely-Connected Convolutional Block.\",\"Parameters:\",\"in_channels (int) – number of input channels\",\"out_channels (int) – number of output channels\",\"hid_channels (int) – number of output channels in intermediate Conv layers\",\"kernel_size (tuple) – kernel size for all but the last Conv layers\",\"padding (tuple) – padding for all but the last Conv layers\",\"last_kernel_size (tuple) – kernel size for the last GluConv layer\",\"last_stride (tuple) – stride for the last GluConv layer\",\"last_padding (tuple) – padding for the last GluConv layer\",\"last_output_padding (tuple) – output padding for the last GluConvTranspose2d (only used when transposed=True)\",\"layers (int) – total number of Conv layers\",\"transposed (bool) – True to use GluConvTranspose2d in the last layer False to use GluConv2d in the last layer\",\"forward(input)\",\"DenselyConnectedBlock forward.\",\"Parameters:input (torch.Tensor) – (B, C, T_in, F_in)\",\"Returns: (B, C, T_out, F_out)\",\"Return type: out (torch.Tensor)\"]},\"1148\":{\"h\":\"espnet2.enh.layers.tcn.DepthwiseSeparableConv\",\"t\":[\"source\",\"class espnet2.enh.layers.tcn.DepthwiseSeparableConv(in_channels, out_channels, skip_channels, kernel_size, stride, padding, dilation, norm_type='gLN', causal=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward.\",\"Parameters:x – [M, H, K]\",\"Returns: [M, B, K] skip_out: [M, Sc, K]\",\"Return type: res_out\"]},\"1149\":{\"h\":\"espnet2.enh.layers.dcunet.DiffusionStepEmbedding\",\"t\":[\"source\",\"class espnet2.enh.layers.dcunet.DiffusionStepEmbedding(embed_dim, complex_valued=False)\",\"Bases: Module\",\"Diffusion-Step embedding as in DiffWave / Vaswani et al. 2017.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(t)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1150\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1151\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.Downsample\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.Downsample(channels, with_conv=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1152\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1153\":{\"h\":\"espnet2.enh.layers.swin_transformer.DropPath\",\"t\":[\"source\",\"class espnet2.enh.layers.swin_transformer.DropPath(drop_prob: float = 0.0, scale_by_keep: bool = True)\",\"Bases: Module\",\"Drop paths (Stochastic Depth) per sample.\",\"(when applied in main path of residual blocks) ported from https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/layers/_init_.py\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1154\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1155\":{\"h\":\"espnet2.enh.diffusion_enh.ESPnetDiffusionModel\",\"t\":[\"source\",\"class espnet2.enh.diffusion_enh.ESPnetDiffusionModel(encoder: AbsEncoder, diffusion: AbsDiffusion, decoder: AbsDecoder, num_spk: int = 1, normalize: bool = False, **kwargs)\",\"Bases: ESPnetEnhancementModel\",\"Target Speaker Extraction Frontend model\",\"Main entry of speech enhancement/separation model training.\",\"Parameters:\",\"encoder – waveform encoder that converts waveforms to feature representations\",\"separator – separator that enhance or separate the feature representations\",\"decoder – waveform decoder that converts the feature back to waveforms\",\"mask_module – mask module that converts the feature to masks NOTE: Only used for compatibility with joint speaker diarization. See test/espnet2/enh/test_espnet_enh_s2t_model.py for details.\",\"loss_wrappers – list of loss wrappers Each loss wrapper contains a criterion for loss calculation and the corresonding loss weight. The losses will be calculated in the order of the list and summed up.\",\"------------------------------------------------------------------\",\"stft_consistency – (deprecated, kept for compatibility) whether to compute the TF-domain loss while enforcing STFT consistency NOTE: STFT consistency is now always used for frequency-domain spectrum losses.\",\"loss_type – (deprecated, kept for compatibility) loss type\",\"mask_type – (deprecated, kept for compatibility) mask type in TF-domain model\",\"------------------------------------------------------------------\",\"flexible_numspk – whether to allow the model to predict a variable number of speakers in its output. NOTE: This should be used when training a speech separation model for unknown number of speakers.\",\"------------------------------------------------------------------\",\"extract_feats_in_collect_stats – used in espnet2/tasks/abs_task.py for determining whether or not to skip model building in collect_stats stage (stage 5 in egs2/\",\"*\",\"/enh1/enh.sh).\",\"normalize_variance – whether to normalize the signal variance before model forward, and revert it back after.\",\"normalize_variance_per_ch – whether to normalize the signal variance for each channel instead of the whole signal. NOTE: normalize_variance and normalize_variance_per_ch cannot be True at the same time.\",\"------------------------------------------------------------------\",\"categories – list of all possible categories of minibatches (order matters!) (e.g. [“1ch_8k_reverb”, “1ch_8k_both”] for multi-condition training) NOTE: this will be used to convert category index to the corresponding name for logging in forward_loss. Different categories will have different loss name suffixes.\",\"category_weights – list of weights for each category. Used to set loss weights for batches of different categories.\",\"------------------------------------------------------------------\",\"always_forward_in_48k – whether to always upsample the input speech to 48kHz for forward, and then downsample to the original sample rate for loss calculation. NOTE: this can be useful to train a model capable of handling various sampling rates while unifying bandwidth extension + speech enhancement.\",\"collect_feats(speech_mix: Tensor, speech_mix_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"enhance(feature_mix)\",\"forward(speech_mix: Tensor, speech_mix_lengths: Tensor | None = None, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech_mix – (Batch, samples) or (Batch, samples, channels)\",\"speech_ref1 – (Batch, samples) or (Batch, samples, channels)\",\"speech_ref2 – (Batch, samples) or (Batch, samples, channels)\",\"...\",\"speech_mix_lengths – (Batch,), default None for chunk interator, because the chunk-iterator does not have the speech_lengths returned. see in espnet2/iterators/chunk_iter_factory.py\",\"enroll_ref1 – (Batch, samples_aux) enrollment (raw audio or embedding) for speaker 1\",\"enroll_ref2 – (Batch, samples_aux) enrollment (raw audio or embedding) for speaker 2\",\"...\",\"kwargs – “utt_id” is among the input.\",\"forward_loss(speech_ref, speech_mix, speech_lengths) → Tuple[Tensor, Dict[str, Tensor], Tensor]\"]},\"1156\":{\"h\":\"espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel\",\"t\":[\"source\",\"class espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel(enh_model: ESPnetEnhancementModel, s2t_model: ESPnetASRModel | ESPnetSTModel | ESPnetDiarizationModel, calc_enh_loss: bool = True, bypass_enh_prob: float = 0)\",\"Bases: AbsESPnetModel\",\"Joint model Enhancement and Speech to Text.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"asr_pit_loss(speech, speech_lengths, text, text_lengths)\",\"batchify_nll(encoder_out: Tensor, encoder_out_lens: Tensor, ys_pad: Tensor, ys_pad_lens: Tensor, batch_size: int = 100)\",\"Compute negative log likelihood(nll) from transformer-decoder\",\"To avoid OOM, this fuction seperate the input into batches. Then call nll for each batch and combine and return results. :param encoder_out: (Batch, Length, Dim) :param encoder_out_lens: (Batch,) :param ys_pad: (Batch, Length) :param ys_pad_lens: (Batch,) :param batch_size: int, samples each batch contain when computing nll,\",\"you may change this to avoid OOM or increase GPU memory usage\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"encode(speech: Tensor, speech_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Frontend + Encoder. Note that this method is used by asr_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"encode_diar(speech: Tensor, speech_lengths: Tensor, num_spk: int) → Tuple[Tensor, Tensor]\",\"Frontend + Encoder. Note that this method is used by diar_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"num_spk – int\",\"forward(speech: Tensor, speech_lengths: Tensor | None = None, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, ) default None for chunk interator, because the chunk-iterator does not have the speech_lengths returned. see in espnet2/iterators/chunk_iter_factory.py\",\"task (For Enh+ASR) – text_spk1: (Batch, Length) text_spk2: (Batch, Length) … text_spk1_lengths: (Batch,) text_spk2_lengths: (Batch,) …\",\"tasks (For other) –\",\"text: (Batch, Length) default None just to keep the argument order text_lengths: (Batch,)\",\"default None for the same reason as speech_lengths\",\"inherite_attributes(inherite_enh_attrs: List[str] = [], inherite_s2t_attrs: List[str] = [])\",\"nll(encoder_out: Tensor, encoder_out_lens: Tensor, ys_pad: Tensor, ys_pad_lens: Tensor) → Tensor\",\"Compute negative log likelihood(nll) from transformer-decoder\",\"Normally, this function is called in batchify_nll.\",\"Parameters:\",\"encoder_out – (Batch, Length, Dim)\",\"encoder_out_lens – (Batch,)\",\"ys_pad – (Batch, Length)\",\"ys_pad_lens – (Batch,)\",\"permutation_invariant_training(losses: Tensor)\",\"Compute PIT loss.\",\"Parameters:losses (torch.Tensor) – (batch, nref, nhyp)\",\"Returns: list: (batch, n_spk) loss: torch.Tensor: (batch)\",\"Return type: perm\"]},\"1157\":{\"h\":\"espnet2.enh.espnet_model.ESPnetEnhancementModel\",\"t\":[\"source\",\"class espnet2.enh.espnet_model.ESPnetEnhancementModel(encoder: AbsEncoder, separator: AbsSeparator | None, decoder: AbsDecoder, mask_module: AbsMask | None, loss_wrappers: List[AbsLossWrapper] | None, stft_consistency: bool = False, loss_type: str = 'mask_mse', mask_type: str | None = None, flexible_numspk: bool = False, extract_feats_in_collect_stats: bool = False, normalize_variance: bool = False, normalize_variance_per_ch: bool = False, categories: list = [], category_weights: list = [], always_forward_in_48k: bool = False)\",\"Bases: AbsESPnetModel\",\"Speech enhancement or separation Frontend model\",\"Main entry of speech enhancement/separation model training.\",\"Parameters:\",\"encoder – waveform encoder that converts waveforms to feature representations\",\"separator – separator that enhance or separate the feature representations\",\"decoder – waveform decoder that converts the feature back to waveforms\",\"mask_module – mask module that converts the feature to masks NOTE: Only used for compatibility with joint speaker diarization. See test/espnet2/enh/test_espnet_enh_s2t_model.py for details.\",\"loss_wrappers – list of loss wrappers Each loss wrapper contains a criterion for loss calculation and the corresonding loss weight. The losses will be calculated in the order of the list and summed up.\",\"------------------------------------------------------------------\",\"stft_consistency – (deprecated, kept for compatibility) whether to compute the TF-domain loss while enforcing STFT consistency NOTE: STFT consistency is now always used for frequency-domain spectrum losses.\",\"loss_type – (deprecated, kept for compatibility) loss type\",\"mask_type – (deprecated, kept for compatibility) mask type in TF-domain model\",\"------------------------------------------------------------------\",\"flexible_numspk – whether to allow the model to predict a variable number of speakers in its output. NOTE: This should be used when training a speech separation model for unknown number of speakers.\",\"------------------------------------------------------------------\",\"extract_feats_in_collect_stats – used in espnet2/tasks/abs_task.py for determining whether or not to skip model building in collect_stats stage (stage 5 in egs2/\",\"*\",\"/enh1/enh.sh).\",\"normalize_variance – whether to normalize the signal variance before model forward, and revert it back after.\",\"normalize_variance_per_ch – whether to normalize the signal variance for each channel instead of the whole signal. NOTE: normalize_variance and normalize_variance_per_ch cannot be True at the same time.\",\"------------------------------------------------------------------\",\"categories – list of all possible categories of minibatches (order matters!) (e.g. [“1ch_8k_reverb”, “1ch_8k_both”] for multi-condition training) NOTE: this will be used to convert category index to the corresponding name for logging in forward_loss. Different categories will have different loss name suffixes.\",\"category_weights – list of weights for each category. Used to set loss weights for batches of different categories.\",\"------------------------------------------------------------------\",\"always_forward_in_48k – whether to always upsample the input speech to 48kHz for forward, and then downsample to the original sample rate for loss calculation. NOTE: this can be useful to train a model capable of handling various sampling rates while unifying bandwidth extension + speech enhancement.\",\"collect_feats(speech_mix: Tensor, speech_mix_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"forward(speech_mix: Tensor, speech_mix_lengths: Tensor | None = None, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech_mix – (Batch, samples) or (Batch, samples, channels)\",\"speech_ref – (Batch, num_speaker, samples) or (Batch, num_speaker, samples, channels)\",\"speech_mix_lengths – (Batch,), default None for chunk interator, because the chunk-iterator does not have the speech_lengths returned. see in espnet2/iterators/chunk_iter_factory.py\",\"kwargs – “utt_id” is among the input.\",\"forward_enhance(speech_mix: Tensor, speech_lengths: Tensor, additional: Dict | None = None, fs: int | None = None) → Tuple[Tensor, Tensor, Tensor]\",\"forward_loss(speech_pre: Tensor, speech_lengths: Tensor, feature_mix: Tensor, feature_pre: List[Tensor], others: OrderedDict, speech_ref: List[Tensor], noise_ref: List[Tensor] | None = None, dereverb_speech_ref: List[Tensor] | None = None, category: Tensor | None = None, num_spk: int | None = None, fs: int | None = None) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"static sort_by_perm(nn_output, perm)\",\"Sort the input list of tensors by the specified permutation.\",\"Parameters:\",\"nn_output – List[torch.Tensor(Batch, …)], len(nn_output) == num_spk\",\"perm – (Batch, num_spk) or List[torch.Tensor(num_spk)]\",\"Returns: List[torch.Tensor(Batch, …)]\",\"Return type: nn_output_new\"]},\"1158\":{\"h\":\"espnet2.enh.espnet_model_tse.ESPnetExtractionModel\",\"t\":[\"source\",\"class espnet2.enh.espnet_model_tse.ESPnetExtractionModel(encoder: AbsEncoder, extractor: AbsExtractor, decoder: AbsDecoder, loss_wrappers: List[AbsLossWrapper], num_spk: int = 1, flexible_numspk: bool = False, share_encoder: bool = True, extract_feats_in_collect_stats: bool = False)\",\"Bases: AbsESPnetModel\",\"Target Speaker Extraction Frontend model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech_mix: Tensor, speech_mix_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"forward(speech_mix: Tensor, speech_mix_lengths: Tensor | None = None, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech_mix – (Batch, samples) or (Batch, samples, channels)\",\"speech_ref1 – (Batch, samples) or (Batch, samples, channels)\",\"speech_ref2 – (Batch, samples) or (Batch, samples, channels)\",\"...\",\"speech_mix_lengths – (Batch,), default None for chunk interator, because the chunk-iterator does not have the speech_lengths returned. see in espnet2/iterators/chunk_iter_factory.py\",\"enroll_ref1 – (Batch, samples_aux) enrollment (raw audio or embedding) for speaker 1\",\"enroll_ref2 – (Batch, samples_aux) enrollment (raw audio or embedding) for speaker 2\",\"...\",\"kwargs – “utt_id” is among the input.\",\"forward_enhance(speech_mix: Tensor, speech_lengths: Tensor, enroll_ref: Tensor, enroll_ref_lengths: Tensor, additional: Dict | None = None) → Tuple[Tensor, Tensor, Tensor]\",\"forward_loss(speech_pre: Tensor, speech_lengths: Tensor, feature_mix: Tensor, feature_pre: Tensor, others: OrderedDict, speech_ref: Tensor) → Tuple[Tensor, Dict[str, Tensor], Tensor]\"]},\"1159\":{\"h\":\"espnet2.enh.separator.svoice_separator.Encoder\",\"t\":[\"source\",\"class espnet2.enh.separator.svoice_separator.Encoder(enc_kernel_size: int, enc_feat_dim: int)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(mixture)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1160\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1161\":{\"h\":\"espnet2.enh.diffusion.sampling.predictors.EulerMaruyamaPredictor\",\"t\":[\"source\",\"class espnet2.enh.diffusion.sampling.predictors.EulerMaruyamaPredictor(sde, score_fn, probability_flow=False)\",\"Bases: Predictor\",\"update_fn(x, t, *args)\",\"One update of the predictor.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A Pytorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1162\":{\"h\":\"espnet2.enh.separator.fasnet_separator.FaSNetSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.fasnet_separator.FaSNetSeparator(input_dim: int, enc_dim: int, feature_dim: int, hidden_dim: int, layer: int, segment_size: int, num_spk: int, win_len: int, context_len: int, fasnet_type: str, dropout: float = 0.0, sr: int = 16000, predict_noise: bool = False)\",\"Bases: AbsSeparator\",\"Filter-and-sum Network (FaSNet) Separator\",\"Parameters:\",\"input_dim – required by AbsSeparator. Not used in this model.\",\"enc_dim – encoder dimension\",\"feature_dim – feature dimension\",\"hidden_dim – hidden dimension in DPRNN\",\"layer – number of DPRNN blocks in iFaSNet\",\"segment_size – dual-path segment size\",\"num_spk – number of speakers\",\"win_len – window length in millisecond\",\"context_len – context length in millisecond\",\"fasnet_type – ‘fasnet’ or ‘ifasnet’. Select from origin fasnet or Implicit fasnet\",\"dropout – dropout rate. Default is 0.\",\"sr – samplerate of input audio\",\"predict_noise – whether to output the estimated noise signal\",\"forward(input: Tensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – (Batch, samples, channels)\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: separated (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1163\":{\"h\":\"espnet2.enh.layers.fasnet.FaSNet_TAC\",\"t\":[\"source\",\"class espnet2.enh.layers.fasnet.FaSNet_TAC(*args, **kwargs)\",\"Bases: FaSNet_base\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, num_mic)\",\"abstract forward function\",\"input: shape (batch, max_num_ch, T) num_mic: shape (batch, ), the number of channels for each input.\",\"Zero for fixed geometry configuration.\"]},\"1164\":{\"h\":\"espnet2.enh.layers.fasnet.FaSNet_base\",\"t\":[\"source\",\"class espnet2.enh.layers.fasnet.FaSNet_base(enc_dim, feature_dim, hidden_dim, layer, segment_size=24, nspk=2, win_len=16, context_len=16, dropout=0.0, sr=16000)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, num_mic)\",\"abstract forward function\",\"input: shape (batch, max_num_ch, T) num_mic: shape (batch, ), the number of channels for each input.\",\"Zero for fixed geometry configuration.\",\"pad_input(input, window)\",\"Zero-padding input according to window/stride size.\",\"seg_signal_context(x, window, context)\",\"Segmenting the signal into chunks with specific context.\",\"input: : x: size (B, ch, T) window: int context: int\",\"seq_cos_sim(ref, target)\",\"Cosine similarity between some reference mics and some target mics\",\"ref: shape (nmic1, L, seg1) target: shape (nmic2, L, seg2)\",\"signal_context(x, context)\",\"signal context function\",\"Segmenting the signal into chunks with specific context. input:\",\"x: size (B, dim, nframe) context: int\"]},\"1165\":{\"h\":\"espnet2.enh.layers.dcunet.FeatureMapDense\",\"t\":[\"source\",\"class espnet2.enh.layers.dcunet.FeatureMapDense(input_dim, output_dim, complex_valued=False)\",\"Bases: Module\",\"A fully connected layer that reshapes outputs to feature maps.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1166\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1167\":{\"h\":\"espnet2.enh.loss.wrappers.fixed_order.FixedOrderSolver\",\"t\":[\"source\",\"class espnet2.enh.loss.wrappers.fixed_order.FixedOrderSolver(criterion: AbsEnhLoss, weight=1.0)\",\"Bases: AbsLossWrapper\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref, inf, others={})\",\"An naive fixed-order solver\",\"Parameters:\",\"ref (List *[*torch.Tensor]) – [(batch, …), …] x n_spk\",\"inf (List *[*torch.Tensor]) – [(batch, …), …]\",\"Returns: (torch.Tensor): minimum loss with the best permutation stats: dict, for collecting training status others: reserved\",\"Return type: loss\"]},\"1168\":{\"h\":\"espnet2.enh.layers.tcndenseunet.FreqWiseBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.tcndenseunet.FreqWiseBlock(in_channels, num_freqs, out_channels, activation=<class 'torch.nn.modules.activation.ELU'>)\",\"Bases: Module\",\"FreqWiseBlock, see iNeuBe paper.\",\"Block that applies pointwise 2D convolution over STFT-like image tensor on frequency axis. The input is assumed to be [batch, image_channels, frames, freq].\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inp)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1169\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1170\":{\"h\":\"espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence(compute_on_mask=False, mask_type=None, name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: FrequencyDomainLoss\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property compute_on_mask : bool\",\"forward(ref, inf) → Tensor\",\"time-frequency absolute coherence loss.\",\"Reference: : Independent Vector Analysis with Deep Neural Network Source Priors; Li et al 2020; https://arxiv.org/abs/2008.11273\",\"Parameters:\",\"ref – (Batch, T, F) or (Batch, T, C, F)\",\"inf – (Batch, T, F) or (Batch, T, C, F)\",\"Returns: (Batch,)\",\"Return type: loss\",\"property mask_type : str\"]},\"1171\":{\"h\":\"espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy(compute_on_mask=False, mask_type=None, ignore_id=-100, name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: FrequencyDomainLoss\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property compute_on_mask : bool\",\"forward(ref, inf) → Tensor\",\"time-frequency cross-entropy loss.\",\"Parameters:\",\"ref – (Batch, T) or (Batch, T, C)\",\"inf – (Batch, T, nclass) or (Batch, T, C, nclass)\",\"Returns: (Batch,)\",\"Return type: loss\",\"property mask_type : str\"]},\"1172\":{\"h\":\"espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL(compute_on_mask=False, mask_type='IBM', loss_type='dpcl', name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: FrequencyDomainLoss\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property compute_on_mask : bool\",\"forward(ref, inf) → Tensor\",\"time-frequency Deep Clustering loss.\",\"References\",\"[1] Deep clustering: Discriminative embeddings for segmentation and : separation; John R. Hershey. et al., 2016; https://ieeexplore.ieee.org/document/7471631\",\"[2] Manifold-Aware Deep Clustering: Maximizing Angles Between Embedding : Vectors Based on Regular Simplex; Tanaka, K. et al., 2021; https://www.isca-speech.org/archive/interspeech_2021/tanaka21_interspeech.html\",\"Parameters:\",\"ref – List[(Batch, T, F) * spks]\",\"inf – (Batch, T*F, D)\",\"Returns: (Batch,)\",\"Return type: loss\",\"property mask_type : str\"]},\"1173\":{\"h\":\"espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1(compute_on_mask=False, mask_type='IBM', name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: FrequencyDomainLoss\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property compute_on_mask : bool\",\"forward(ref, inf) → Tensor\",\"time-frequency L1 loss.\",\"Parameters:\",\"ref – (Batch, T, F) or (Batch, T, C, F)\",\"inf – (Batch, T, F) or (Batch, T, C, F)\",\"Returns: (Batch,)\",\"Return type: loss\",\"property mask_type : str\"]},\"1174\":{\"h\":\"espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss(name, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: AbsEnhLoss, ABC\",\"Base class for all frequence-domain Enhancement loss modules.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract property compute_on_mask : bool\",\"create_mask_label(mix_spec, ref_spec, noise_spec=None)\",\"property is_dereverb_loss : bool\",\"property is_noise_loss : bool\",\"abstract property mask_type : str\",\"property name : str\",\"property only_for_test : bool\"]},\"1175\":{\"h\":\"espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE(compute_on_mask=False, mask_type='IBM', name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: FrequencyDomainLoss\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property compute_on_mask : bool\",\"forward(ref, inf) → Tensor\",\"time-frequency MSE loss.\",\"Parameters:\",\"ref – (Batch, T, F) or (Batch, T, C, F)\",\"inf – (Batch, T, F) or (Batch, T, C, F)\",\"Returns: (Batch,)\",\"Return type: loss\",\"property mask_type : str\"]},\"1176\":{\"h\":\"espnet2.enh.layers.dc_crn.GLSTM\",\"t\":[\"source\",\"class espnet2.enh.layers.dc_crn.GLSTM(hidden_size=1024, groups=2, layers=2, bidirectional=False, rearrange=False)\",\"Bases: Module\",\"Grouped LSTM.\",\"Reference: : Efficient Sequence Learning with Group Recurrent Networks; Gao et al., 2018\",\"Parameters:\",\"hidden_size (int) – total hidden size of all LSTMs in each grouped LSTM layer i.e., hidden size of each LSTM is hidden_size // groups\",\"groups (int) – number of LSTMs in each grouped LSTM layer\",\"layers (int) – number of grouped LSTM layers\",\"bidirectional (bool) – whether to use BLSTM or unidirectional LSTM\",\"rearrange (bool) – whether to apply the rearrange operation after each grouped LSTM layer\",\"forward(x)\",\"Grouped LSTM forward.\",\"Parameters:x (torch.Tensor) – (B, C, T, D)\",\"Returns: (B, C, T, D)\",\"Return type: out (torch.Tensor)\"]},\"1177\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layerspp.GaussianFourierProjection\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layerspp.GaussianFourierProjection(embedding_size=256, scale=1.0)\",\"Bases: Module\",\"Gaussian Fourier embeddings for noise levels.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1178\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1179\":{\"h\":\"espnet2.enh.layers.tcn.GlobalLayerNorm\",\"t\":[\"source\",\"class espnet2.enh.layers.tcn.GlobalLayerNorm(channel_size, shape='BDT')\",\"Bases: Module\",\"Global Layer Normalization (gLN).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y)\",\"Forward.\",\"Parameters:y – [M, N, K], M is batch size, N is channel size, K is length\",\"Returns: [M, N, K]\",\"Return type: gLN_y\",\"reset_parameters()\"]},\"1180\":{\"h\":\"espnet2.enh.layers.dc_crn.GluConv2d\",\"t\":[\"source\",\"class espnet2.enh.layers.dc_crn.GluConv2d(in_channels, out_channels, kernel_size, stride, padding=0)\",\"Bases: Module\",\"Conv2d with Gated Linear Units (GLU).\",\"Input and output shapes are the same as regular Conv2d layers.\",\"Reference: Section III-B in [1]\",\"Parameters:\",\"in_channels (int) – number of input channels\",\"out_channels (int) – number of output channels\",\"kernel_size (int/tuple) – kernel size in Conv2d\",\"stride (int/tuple) – stride size in Conv2d\",\"padding (int/tuple) – padding size in Conv2d\",\"forward(x)\",\"ConvGLU forward.\",\"Parameters:x (torch.Tensor) – (B, C_in, H_in, W_in)\",\"Returns: (B, C_out, H_out, W_out)\",\"Return type: out (torch.Tensor)\"]},\"1181\":{\"h\":\"espnet2.enh.layers.dc_crn.GluConvTranspose2d\",\"t\":[\"source\",\"class espnet2.enh.layers.dc_crn.GluConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding=0, output_padding=(0, 0))\",\"Bases: Module\",\"ConvTranspose2d with Gated Linear Units (GLU).\",\"Input and output shapes are the same as regular ConvTranspose2d layers.\",\"Reference: Section III-B in [1]\",\"Parameters:\",\"in_channels (int) – number of input channels\",\"out_channels (int) – number of output channels\",\"kernel_size (int/tuple) – kernel size in ConvTranspose2d\",\"stride (int/tuple) – stride size in ConvTranspose2d\",\"padding (int/tuple) – padding size in ConvTranspose2d\",\"output_padding (int/tuple) – Additional size added to one side of each dimension in the output shape\",\"forward(x)\",\"DeconvGLU forward.\",\"Parameters:x (torch.Tensor) – (B, C_in, H_in, W_in)\",\"Returns: (B, C_out, H_out, W_out)\",\"Return type: out (torch.Tensor)\"]},\"1182\":{\"h\":\"espnet2.enh.separator.tfgridnet_separator.GridNetBlock\",\"t\":[\"source\",\"class espnet2.enh.separator.tfgridnet_separator.GridNetBlock(emb_dim, emb_ks, emb_hs, n_freqs, hidden_channels, n_head=4, approx_qk_dim=512, activation='prelu', eps=1e-05)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"GridNetBlock Forward.\",\"Parameters:\",\"x – [B, C, T, Q]\",\"out – [B, C, T, Q]\"]},\"1183\":{\"h\":\"espnet2.enh.separator.tfgridnetv2_separator.GridNetV2Block\",\"t\":[\"source\",\"class espnet2.enh.separator.tfgridnetv2_separator.GridNetV2Block(emb_dim, emb_ks, emb_hs, n_freqs, hidden_channels, n_head=4, approx_qk_dim=512, activation='prelu', eps=1e-05)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"GridNetV2Block Forward.\",\"Parameters:\",\"x – [B, C, T, Q]\",\"out – [B, C, T, Q]\"]},\"1184\":{\"h\":\"espnet2.enh.separator.tfgridnetv3_separator.GridNetV3Block\",\"t\":[\"source\",\"class espnet2.enh.separator.tfgridnetv3_separator.GridNetV3Block(emb_dim, emb_ks, emb_hs, hidden_channels, n_head=4, qk_output_channel=4, activation='prelu', eps=1e-05)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"GridNetV2Block Forward.\",\"Parameters:\",\"x – [B, C, T, Q]\",\"out – [B, C, T, Q]\"]},\"1185\":{\"h\":\"espnet2.enh.layers.dptnet.ImprovedTransformerLayer\",\"t\":[\"source\",\"class espnet2.enh.layers.dptnet.ImprovedTransformerLayer(rnn_type, input_size, att_heads, hidden_size, dropout=0.0, activation='relu', bidirectional=True, norm='gLN')\",\"Bases: Module\",\"Container module of the (improved) Transformer proposed in [1].\",\"Reference: : Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation; Chen et al, Interspeech 2020.\",\"Parameters:\",\"rnn_type (str) – select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"input_size (int) – Dimension of the input feature.\",\"att_heads (int) – Number of attention heads.\",\"hidden_size (int) – Dimension of the hidden state.\",\"dropout (float) – Dropout ratio. Default is 0.\",\"activation (str) – activation function applied at the output of RNN.\",\"bidirectional (bool,optional) – True for bidirectional Inter-Chunk RNN (Intra-Chunk is always bidirectional).\",\"norm (str,optional) – Type of normalization to use.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, attn_mask=None)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1186\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1187\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.InstanceNorm2dPlus\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.normalization.InstanceNorm2dPlus(num_features, bias=True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1188\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1189\":{\"h\":\"espnet2.enh.diffusion.sampling.correctors.LangevinCorrector\",\"t\":[\"source\",\"class espnet2.enh.diffusion.sampling.correctors.LangevinCorrector(sde, score_fn, snr, n_steps)\",\"Bases: Corrector\",\"update_fn(x, t, *args)\",\"One update of the corrector.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A PyTorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1190\":{\"h\":\"espnet2.enh.separator.tfgridnetv3_separator.LayerNormalization\",\"t\":[\"source\",\"class espnet2.enh.separator.tfgridnetv3_separator.LayerNormalization(input_dim, dim=1, total_dim=4, eps=1e-05)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1191\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1192\":{\"h\":\"espnet2.enh.separator.tfgridnet_separator.LayerNormalization4D\",\"t\":[\"source\",\"class espnet2.enh.separator.tfgridnet_separator.LayerNormalization4D(input_dimension, eps=1e-05)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1193\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1194\":{\"h\":\"espnet2.enh.separator.tfgridnet_separator.LayerNormalization4DCF\",\"t\":[\"source\",\"class espnet2.enh.separator.tfgridnet_separator.LayerNormalization4DCF(input_dimension, eps=1e-05)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1195\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1196\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.MSFBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.MSFBlock(in_planes, features)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs, shape)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1197\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1198\":{\"h\":\"espnet2.enh.layers.bsrnn.MaskDecoder\",\"t\":[\"source\",\"class espnet2.enh.layers.bsrnn.MaskDecoder(freq_dim, subbands, channels=128, num_spk=1, norm_type='GN')\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"MaskDecoder forward.\",\"Parameters:x (torch.Tensor) – input tensor of shape (B, N, T, K)\",\"Returns: output mask of shape (B, num_spk, T, F, 2) r (torch.Tensor): output residual of shape (B, num_spk, T, F, 2)\",\"Return type: m (torch.Tensor)\"]},\"1199\":{\"h\":\"espnet2.enh.layers.mask_estimator.MaskEstimator\",\"t\":[\"source\",\"class espnet2.enh.layers.mask_estimator.MaskEstimator(type, idim, layers, units, projs, dropout, nmask=1, nonlinear='sigmoid')\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs: Tensor | ComplexTensor, ilens: LongTensor) → Tuple[Tuple[Tensor, ...], LongTensor]\",\"Mask estimator forward function.\",\"Parameters:\",\"xs – (B, F, C, T)\",\"ilens – (B,)\",\"Returns: The hidden vector (B, F, C, T) masks: A tuple of the masks. (B, F, C, T) ilens: (B,)\",\"Return type: hs (torch.Tensor)\"]},\"1200\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.MeanPoolConv\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.MeanPoolConv(input_dim, output_dim, kernel_size=3, biases=True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inputs)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1201\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1202\":{\"h\":\"espnet2.enh.layers.skim.MemLSTM\",\"t\":[\"source\",\"class espnet2.enh.layers.skim.MemLSTM(hidden_size, dropout=0.0, bidirectional=False, mem_type='hc', norm_type='cLN')\",\"Bases: Module\",\"the Mem-LSTM of SkiM\",\"Parameters:\",\"hidden_size – int, dimension of the hidden state.\",\"dropout – float, dropout ratio. Default is 0.\",\"bidirectional – bool, whether the LSTM layers are bidirectional. Default is False.\",\"mem_type – ‘hc’, ‘h’, ‘c’ or ‘id’. It controls whether the hidden (or cell) state of SegLSTM will be processed by MemLSTM. In ‘id’ mode, both the hidden and cell states will be identically returned.\",\"norm_type – gLN, cLN. cLN is for causal implementation.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr() → str\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(hc, S)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1203\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_one_step(hc, state)\"]},\"1204\":{\"h\":\"espnet2.enh.loss.wrappers.mixit_solver.MixITSolver\",\"t\":[\"source\",\"class espnet2.enh.loss.wrappers.mixit_solver.MixITSolver(criterion: AbsEnhLoss, weight: float = 1.0)\",\"Bases: AbsLossWrapper\",\"Mixture Invariant Training Solver.\",\"Parameters:\",\"criterion (AbsEnhLoss) – an instance of AbsEnhLoss\",\"weight (float) – weight (between 0 and 1) of current loss for multi-task learning.\",\"forward(ref: List[Tensor] | List[ComplexTensor], inf: List[Tensor] | List[ComplexTensor], others: Dict = {})\",\"MixIT solver.\",\"Parameters:\",\"ref (List *[*torch.Tensor]) – [(batch, …), …] x n_spk\",\"inf (List *[*torch.Tensor]) – [(batch, …), …] x n_est\",\"Returns: (torch.Tensor): minimum loss with the best permutation stats: dict, for collecting training status others: dict, in this PIT solver, permutation order will be returned\",\"Return type: loss\",\"property name\"]},\"1205\":{\"h\":\"espnet2.enh.layers.swin_transformer.Mlp\",\"t\":[\"source\",\"class espnet2.enh.layers.swin_transformer.Mlp(in_features, hidden_features=None, out_features=None, act_layer=<class 'torch.nn.modules.activation.GELU'>, drop=0.0)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1206\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1207\":{\"h\":\"espnet2.enh.layers.adapt_layers.MulAddAdaptLayer\",\"t\":[\"source\",\"class espnet2.enh.layers.adapt_layers.MulAddAdaptLayer(indim, enrolldim, ninputs=1, do_addition=True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(main, enroll)\",\"MulAddAdaptLayer Forward.\",\"Parameters:\",\"main –\",\"tensor or tuple or list activations in the main neural network, which are adapted tuple/list may be useful when we want to apply the adaptation\",\"to both normal and skip connection at once\",\"enroll –\",\"tensor or tuple or list embedding extracted from enrollment tuple/list may be useful when we want to apply the adaptation\",\"to both normal and skip connection at once\"]},\"1208\":{\"h\":\"espnet2.enh.layers.dpmulcat.MulCatBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.dpmulcat.MulCatBlock(input_size: int, hidden_size: int, dropout: float = 0.0, bidirectional: bool = True)\",\"Bases: Module\",\"The MulCat block.\",\"Parameters:\",\"input_size – int, dimension of the input feature. The input should have shape (batch, seq_len, input_size).\",\"hidden_size – int, dimension of the hidden state.\",\"dropout – float, the dropout rate in the LSTM layer. (Default: 0.0)\",\"bidirectional – bool, whether the RNN layers are bidirectional. (Default: True)\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Compute output after MulCatBlock.\",\"Parameters:input (torch.Tensor) – The input feature. Tensor of shape (batch, time, feature_dim)\",\"Returns: The output feature after MulCatBlock. : Tensor of shape (batch, time, feature_dim)\",\"Return type: (torch.Tensor)\"]},\"1209\":{\"h\":\"espnet2.enh.loss.wrappers.multilayer_pit_solver.MultiLayerPITSolver\",\"t\":[\"source\",\"class espnet2.enh.loss.wrappers.multilayer_pit_solver.MultiLayerPITSolver(criterion: AbsEnhLoss, weight=1.0, independent_perm=True, layer_weights=None)\",\"Bases: AbsLossWrapper\",\"Multi-Layer Permutation Invariant Training Solver.\",\"Compute the PIT loss given inferences of multiple layers and a single reference. It also support single inference and single reference in evaluation stage.\",\"Parameters:\",\"criterion (AbsEnhLoss) – an instance of AbsEnhLoss\",\"weight (float) – weight (between 0 and 1) of current loss for multi-task learning.\",\"independent_perm (bool) – If True, PIT will be performed in forward to find the best permutation; If False, the permutation from the last LossWrapper output will be inherited. Note: You should be careful about the ordering of loss wrappers defined in the yaml config, if this argument is False.\",\"layer_weights (Optional *[*List *[*float]]) – weights for each layer If not None, the loss of each layer will be weighted-summed using the specified weights.\",\"forward(ref, infs, others={})\",\"Permutation invariant training solver.\",\"Parameters:\",\"ref (List *[*torch.Tensor]) – [(batch, …), …] x n_spk\",\"infs (Union *[*List *[*torch.Tensor],List *[*List *[*torch.Tensor]]]) – [(batch, …), …]\",\"Returns: (torch.Tensor): minimum loss with the best permutation stats: dict, for collecting training status others: dict, in this PIT solver, permutation order will be returned\",\"Return type: loss\"]},\"1210\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.MultiResL1SpecLoss\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.time_domain.MultiResL1SpecLoss(window_sz=[512], hop_sz=None, eps=1e-08, time_domain_weight=0.5, normalize_variance=False, reduction='sum', name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"Multi-Resolution L1 time-domain + STFT mag loss\",\"Reference: Lu, Y. J., Cornell, S., Chang, X., Zhang, W., Li, C., Ni, Z., … & Watanabe, S. Towards Low-Distortion Multi-Channel Speech Enhancement: The ESPNET-Se Submission to the L3DAS22 Challenge. ICASSP 2022 p. 9201-9205.\",\"window_sz\",\"(list) list of STFT window sizes.\",\"hop_sz\",\"(list, optional) list of hop_sizes, default is each window_sz // 2.\",\"eps\",\"(float) stability epsilon\",\"time_domain_weight\",\"(float) weight for time domain loss.\",\"normalize_variance\",\"whether or not to normalize the variance when calculating the loss.\",\"Type: bool\",\"reduction\",\"select from “sum” and “mean”\",\"Type: str\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(target: Tensor, estimate: Tensor)\",\"forward.\",\"Parameters:\",\"target – (Batch, T)\",\"estimate – (Batch, T)\",\"Returns: (Batch,)\",\"Return type: loss\",\"get_magnitude(stft, eps=1e-06)\",\"property name : str\"]},\"1211\":{\"h\":\"espnet2.enh.layers.ncsnpp.NCSNpp\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp.NCSNpp(scale_by_sigma=True, nonlinearity='swish', nf=128, ch_mult=(1, 1, 2, 2, 2, 2, 2), num_res_blocks=2, attn_resolutions=(16,), resamp_with_conv=True, conditional=True, fir=True, fir_kernel=[1, 3, 3, 1], skip_rescale=True, resblock_type='biggan', progressive='output_skip', progressive_input='input_skip', progressive_combine='sum', init_scale=0.0, fourier_scale=16, image_size=256, embedding_type='fourier', dropout=0.0, centered=True, **unused_kwargs)\",\"Bases: Module\",\"NCSN++ model, adapted from https://github.com/yang-song/score_sde and\",\"https://github.com/sp-uhh/sgmse repository\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, time_cond)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1212\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"pad_spec(Y)\"]},\"1213\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.NIN\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.NIN(in_dim, num_units, init_scale=0.1)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1214\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1215\":{\"h\":\"espnet2.enh.layers.complexnn.NavieComplexLSTM\",\"t\":[\"source\",\"class espnet2.enh.layers.complexnn.NavieComplexLSTM(input_size, hidden_size, projection_dim=None, bidirectional=False, batch_first=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"flatten_parameters()\",\"forward(inputs)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1216\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1217\":{\"h\":\"espnet2.enh.separator.neural_beamformer.NeuralBeamformer\",\"t\":[\"source\",\"class espnet2.enh.separator.neural_beamformer.NeuralBeamformer(input_dim: int, num_spk: int = 1, loss_type: str = 'mask_mse', use_wpe: bool = False, wnet_type: str = 'blstmp', wlayers: int = 3, wunits: int = 300, wprojs: int = 320, wdropout_rate: float = 0.0, taps: int = 5, delay: int = 3, use_dnn_mask_for_wpe: bool = True, wnonlinear: str = 'crelu', multi_source_wpe: bool = True, wnormalization: bool = False, use_beamformer: bool = True, bnet_type: str = 'blstmp', blayers: int = 3, bunits: int = 300, bprojs: int = 320, badim: int = 320, ref_channel: int = -1, use_noise_mask: bool = True, bnonlinear: str = 'sigmoid', beamformer_type: str = 'mvdr_souden', rtf_iterations: int = 2, bdropout_rate: float = 0.0, shared_power: bool = True, use_torchaudio_api: bool = False, diagonal_loading: bool = True, diag_eps_wpe: float = 1e-07, diag_eps_bf: float = 1e-07, mask_flooring: bool = False, flooring_thres_wpe: float = 1e-06, flooring_thres_bf: float = 1e-06, use_torch_solver: bool = True)\",\"Bases: AbsSeparator\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.complex64/ComplexTensor) – mixed speech [Batch, Frames, Channel, Freq]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: List[torch.complex64/ComplexTensor] output lengths other predcited data: OrderedDict[\",\"’dereverb1’: ComplexTensor(Batch, Frames, Channel, Freq), ‘mask_dereverb1’: torch.Tensor(Batch, Frames, Channel, Freq), ‘mask_noise1’: torch.Tensor(Batch, Frames, Channel, Freq), ‘mask_spk1’: torch.Tensor(Batch, Frames, Channel, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Channel, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Channel, Freq),\",\"]\",\"Return type: enhanced speech (single-channel)\",\"property num_spk\"]},\"1218\":{\"h\":\"espnet2.enh.diffusion.sampling.correctors.NoneCorrector\",\"t\":[\"source\",\"class espnet2.enh.diffusion.sampling.correctors.NoneCorrector(*args, **kwargs)\",\"Bases: Corrector\",\"An empty corrector that does nothing.\",\"update_fn(x, t, *args)\",\"One update of the corrector.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A PyTorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1219\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.NoneNorm2d\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.normalization.NoneNorm2d(num_features, bias=True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1220\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1221\":{\"h\":\"espnet2.enh.diffusion.sampling.predictors.NonePredictor\",\"t\":[\"source\",\"class espnet2.enh.diffusion.sampling.predictors.NonePredictor(*args, **kwargs)\",\"Bases: Predictor\",\"An empty predictor that does nothing.\",\"update_fn(x, t, *args)\",\"One update of the predictor.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A Pytorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1222\":{\"h\":\"espnet2.enh.decoder.null_decoder.NullDecoder\",\"t\":[\"source\",\"class espnet2.enh.decoder.null_decoder.NullDecoder\",\"Bases: AbsDecoder\",\"Null decoder, return the same args.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, fs: int | None = None)\",\"Forward. The input should be the waveform already.\",\"Parameters:\",\"input (torch.Tensor) – wav [Batch, sample]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"fs (int) – sampling rate in Hz (Not used)\"]},\"1223\":{\"h\":\"espnet2.enh.encoder.null_encoder.NullEncoder\",\"t\":[\"source\",\"class espnet2.enh.encoder.null_encoder.NullEncoder\",\"Bases: AbsEncoder\",\"Null encoder.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, fs: int | None = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – mixed speech [Batch, sample]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"fs (int) – sampling rate in Hz (Not used)\",\"property output_dim : int\"]},\"1224\":{\"h\":\"espnet2.enh.diffusion.sdes.OUVESDE\",\"t\":[\"source\",\"class espnet2.enh.diffusion.sdes.OUVESDE(theta=1.5, sigma_min=0.05, sigma_max=0.5, N=1000, **ignored_kwargs)\",\"Bases: SDE\",\"Construct an Ornstein-Uhlenbeck Variance Exploding SDE.\",\"Note that the “steady-state mean” y is not provided at construction, but must rather be given as an argument to the methods which require it (e.g., sde or marginal_prob).\",\"dx = -theta (y-x) dt + sigma(t) dw\",\"with\",\"sigma(t) = sigma_min (sigma_max/sigma_min)^t * sqrt(2 log(sigma_max/sigma_min))\",\"Parameters:\",\"theta – stiffness parameter.\",\"sigma_min – smallest sigma.\",\"sigma_max – largest sigma.\",\"N – number of discretization steps\",\"property T\",\"End time of the SDE.\",\"copy()\",\"marginal_prob(x0, t, y)\",\"Parameters to determine the marginal distribution of\",\"the SDE, $p_t(x|args)$.\",\"prior_logp(z)\",\"Compute log-density of the prior distribution.\",\"Useful for computing the log-likelihood via probability flow ODE.\",\"Parameters:z – latent code\",\"Returns: log probability density\",\"prior_sampling(shape, y)\",\"Generate one sample from the prior distribution,\",\"$p_T(x|args)$ with shape shape.\",\"sde(x, t, y)\"]},\"1225\":{\"h\":\"espnet2.enh.diffusion.sdes.OUVPSDE\",\"t\":[\"source\",\"class espnet2.enh.diffusion.sdes.OUVPSDE(beta_min, beta_max, stiffness=1, N=1000, **ignored_kwargs)\",\"Bases: SDE\",\"OUVPSDE class.\",\"!!! SGMSE authors observed instabilities around t=0.2. !!!\",\"Construct an Ornstein-Uhlenbeck Variance Preserving SDE:\",\"dx = -1/2 * beta(t) * stiffness * (y-x) dt + sqrt(beta(t)) * dw\",\"with\",\"beta(t) = beta_min + t(beta_max - beta_min)\",\"Note that the “steady-state mean” y is not provided at construction, but must rather be given as an argument to the methods which require it (e.g., sde or marginal_prob).\",\"Parameters:\",\"beta_min – smallest sigma.\",\"beta_max – largest sigma.\",\"stiffness – stiffness factor of the drift. 1 by default.\",\"N – number of discretization steps\",\"property T\",\"End time of the SDE.\",\"copy()\",\"marginal_prob(x0, t, y)\",\"Parameters to determine the marginal distribution of\",\"the SDE, $p_t(x|args)$.\",\"prior_logp(z)\",\"Compute log-density of the prior distribution.\",\"Useful for computing the log-likelihood via probability flow ODE.\",\"Parameters:z – latent code\",\"Returns: log probability density\",\"prior_sampling(shape, y)\",\"Generate one sample from the prior distribution,\",\"$p_T(x|args)$ with shape shape.\",\"sde(x, t, y)\"]},\"1226\":{\"h\":\"espnet2.enh.layers.dcunet.OnReIm\",\"t\":[\"source\",\"class espnet2.enh.layers.dcunet.OnReIm(module_cls, *args, **kwargs)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1227\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1228\":{\"h\":\"espnet2.enh.loss.wrappers.pit_solver.PITSolver\",\"t\":[\"source\",\"class espnet2.enh.loss.wrappers.pit_solver.PITSolver(criterion: AbsEnhLoss, weight=1.0, independent_perm=True, flexible_numspk=False)\",\"Bases: AbsLossWrapper\",\"Permutation Invariant Training Solver.\",\"Parameters:\",\"criterion (AbsEnhLoss) – an instance of AbsEnhLoss\",\"weight (float) – weight (between 0 and 1) of current loss for multi-task learning.\",\"independent_perm (bool) –\",\"If True, PIT will be performed in forward to find the best permutation; If False, the permutation from the last LossWrapper output will be inherited. NOTE (wangyou): You should be careful about the ordering of loss\",\"wrappers defined in the yaml config, if this argument is False.\",\"flexible_numspk (bool) – If True, num_spk will be taken from inf to handle flexible numbers of speakers. This is because ref may include dummy data in this case.\",\"forward(ref, inf, others={})\",\"PITSolver forward.\",\"Parameters:\",\"ref (List *[*torch.Tensor]) – [(batch, …), …] x n_spk\",\"inf (List *[*torch.Tensor]) – [(batch, …), …]\",\"Returns: (torch.Tensor): minimum loss with the best permutation stats: dict, for collecting training status others: dict, in this PIT solver, permutation order will be returned\",\"Return type: loss\"]},\"1229\":{\"h\":\"espnet2.enh.diffusion.sampling.predictors.Predictor\",\"t\":[\"source\",\"class espnet2.enh.diffusion.sampling.predictors.Predictor(sde, score_fn, probability_flow=False)\",\"Bases: ABC\",\"The abstract class for a predictor algorithm.\",\"debug_update_fn(x, t, *args)\",\"abstract update_fn(x, t, *args)\",\"One update of the predictor.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A Pytorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1230\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.RCUBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.RCUBlock(features, n_blocks, n_stages, act=ReLU())\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1231\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1232\":{\"h\":\"espnet2.enh.separator.rnn_separator.RNNSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.rnn_separator.RNNSeparator(input_dim: int, rnn_type: str = 'blstm', num_spk: int = 2, predict_noise: bool = False, nonlinear: str = 'sigmoid', layer: int = 3, unit: int = 512, dropout: float = 0.0)\",\"Bases: AbsSeparator\",\"RNN Separator\",\"Parameters:\",\"input_dim – input feature dimension\",\"rnn_type – string, select from ‘blstm’, ‘lstm’ etc.\",\"bidirectional – bool, whether the inter-chunk RNN layers are bidirectional.\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of stacked RNN layers. Default is 3.\",\"unit – int, dimension of the hidden state.\",\"dropout – float, dropout ratio. Default is 0.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"forward_streaming(input_frame: Tensor, states=None)\",\"property num_spk\"]},\"1233\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.RefineBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.RefineBlock(in_planes, features, act=ReLU(), start=False, end=False, maxpool=True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs, output_shape)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1234\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1235\":{\"h\":\"espnet2.enh.layers.uses2_swin.ResSwinBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.uses2_swin.ResSwinBlock(input_size, input_resolution=(130, 256), swin_block_depth=(4, 4, 4, 4), window_size=(10, 8), mlp_ratio=2, qkv_bias=True, qk_scale=None, dropout=0.0, att_dropout=0.0, drop_path=0.0, activation='relu', att_heads=4, use_checkpoint=False, ch_mode='att_tac', ch_att_dim=256, eps=1e-05, with_channel_modeling=True)\",\"Bases: Module\",\"Container module for a single Residual Shifted-Window Transformer Block.\",\"Parameters:\",\"input_size (int) – dimension of the input feature.\",\"input_resolution (tuple) – frequency and time dimension of the input feature. Only used for efficient training. Should be close to the actual spectrum size (F, T) of training samples.\",\"swin_block_depth (Tuple *[*int]) – depth of each ResSwinBlock.\",\"window_size (tuple) – size of the Time-Frequency window in Swin-Transformer.\",\"mlp_ratio (int) – ratio of the MLP hidden size to embedding size in BasicLayer.\",\"qkv_bias (bool) – If True, add a learnable bias to query, key, value in BasicLayer.\",\"qk_scale (float) – Override default qk scale of head_dim ** -0.5 in BasicLayer if set.\",\"dropout (float) – dropout ratio in BasicLayer. Default is 0.\",\"att_dropout (float) – attention dropout ratio in BasicLayer. Default is 0.\",\"drop_path (float) – drop-path ratio in BasicLayer. Default is 0.\",\"activation (str) – non-linear activation function applied in each block.\",\"att_heads (int) – number of attention heads.\",\"use_checkpoint (bool) – whether to use checkpointing to save memory.\",\"ch_mode (str) – mode of channel modeling. Select from “att”, “tac” and “att_tac”.\",\"ch_att_dim (int) – dimension of the channel attention.\",\"eps (float) – epsilon for layer normalization.\",\"with_channel_modeling (bool) – whether to use channel attention.\",\"forward(input, ref_channel=None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – feature sequence (batch, C, N, freq, time)\",\"ref_channel (Noneorint) – index of the reference channel.\",\"Returns: output sequence (batch, C, N, freq, time)\",\"Return type: output (torch.Tensor)\",\"pad_to_window_multiples(input, window_size)\",\"Pad the input feature to multiples of the window size.\",\"Parameters:\",\"input (torch.Tensor) – input feature (batch, C, N, freq, time)\",\"window_size (tuple) – size of the window (H, W).\",\"Returns: padded input feature (batch, C, N, n * H, m * W)\",\"Return type: output (torch.Tensor)\"]},\"1236\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ResidualBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.ResidualBlock(input_dim, output_dim, resample=None, act=ELU(alpha=1.0), normalization=<class 'torch.nn.modules.instancenorm.InstanceNorm2d'>, adjust_padding=False, dilation=1)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1237\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1238\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layerspp.ResnetBlockBigGANpp\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layerspp.ResnetBlockBigGANpp(act, in_ch, out_ch=None, temb_dim=None, up=False, down=False, dropout=0.1, fir=False, fir_kernel=(1, 3, 3, 1), skip_rescale=True, init_scale=0.0)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, temb=None)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1239\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1240\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ResnetBlockDDPM\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.ResnetBlockDDPM(act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False, dropout=0.1)\",\"Bases: Module\",\"The ResNet Blocks used in DDPM.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, temb=None)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1241\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1242\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layerspp.ResnetBlockDDPMpp\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layerspp.ResnetBlockDDPMpp(act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False, dropout=0.1, skip_rescale=False, init_scale=0.0)\",\"Bases: Module\",\"ResBlock adapted from DDPM.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, temb=None)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1243\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1244\":{\"h\":\"espnet2.enh.diffusion.sampling.predictors.ReverseDiffusionPredictor\",\"t\":[\"source\",\"class espnet2.enh.diffusion.sampling.predictors.ReverseDiffusionPredictor(sde, score_fn, probability_flow=False)\",\"Bases: Predictor\",\"update_fn(x, t, *args)\",\"One update of the predictor.\",\"Parameters:\",\"x – A PyTorch tensor representing the current state\",\"t – A Pytorch tensor representing the current time step.\",\"*args – Possibly additional arguments, in particular y for OU processes\",\"Returns: A PyTorch tensor of the next state. x_mean: A PyTorch tensor. The next state without random noise. \",\"Useful for denoising.\",\"Return type: x\"]},\"1245\":{\"h\":\"espnet2.enh.diffusion.sdes.SDE\",\"t\":[\"source\",\"class espnet2.enh.diffusion.sdes.SDE(N)\",\"Bases: ABC\",\"SDE abstract class. Functions are designed for a mini-batch of inputs.\",\"Construct an SDE.\",\"Parameters:N – number of discretization time steps.\",\"abstract property T\",\"End time of the SDE.\",\"abstract copy()\",\"discretize(x, t, *args)\",\"Discretize the SDE in the form: x_{i+1} = x_i + f_i(x_i) + G_i z_i.\",\"Useful for reverse diffusion sampling and probabiliy flow sampling. Defaults to Euler-Maruyama discretization.\",\"Parameters:\",\"x – a torch tensor\",\"t – a torch float representing the time step (from 0 to self.T)\",\"Returns: f, G\",\"abstract marginal_prob(x, t, *args)\",\"Parameters to determine the marginal distribution of\",\"the SDE, $p_t(x|args)$.\",\"abstract prior_logp(z)\",\"Compute log-density of the prior distribution.\",\"Useful for computing the log-likelihood via probability flow ODE.\",\"Parameters:z – latent code\",\"Returns: log probability density\",\"abstract prior_sampling(shape, *args)\",\"Generate one sample from the prior distribution,\",\"$p_T(x|args)$ with shape shape.\",\"reverse(score_model, probability_flow=False)\",\"Create the reverse-time SDE/ODE.\",\"Parameters:\",\"score_model – A function that takes x, t and y and returns the score.\",\"probability_flow – If True, create the reverse-time ODE used for probability flow sampling.\",\"abstract sde(x, t, *args)\"]},\"1246\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.SDRLoss\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.time_domain.SDRLoss(filter_length=512, use_cg_iter=None, clamp_db=None, zero_mean=True, load_diag=None, name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"SDR loss.\",\"filter_length: int : The length of the distortion filter allowed (default: 512)\",\"use_cg_iter: : If provided, an iterative method is used to solve for the distortion filter coefficients instead of direct Gaussian elimination. This can speed up the computation of the metrics in case the filters are long. Using a value of 10 here has been shown to provide good accuracy in most cases and is sufficient when using this loss to train neural separation networks.\",\"clamp_db: float : clamp the output value in [-clamp_db, clamp_db]\",\"zero_mean: bool : When set to True, the mean of all signals is subtracted prior.\",\"load_diag: : If provided, this small value is added to the diagonal coefficients of the system metrices when solving for the filter coefficients. This can help stabilize the metric in the case where some of the reference signals may sometimes be zero\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref: Tensor, est: Tensor) → Tensor\",\"SDR forward.\",\"Parameters:\",\"ref – Tensor, (…, n_samples) reference signal\",\"est – Tensor (…, n_samples) estimated signal\",\"Returns: (…,) : the SDR loss (negative sdr)\",\"Return type: loss\"]},\"1247\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.SISNRLoss\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.time_domain.SISNRLoss(clamp_db=None, zero_mean=True, eps=None, name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"SI-SNR (or named SI-SDR) loss\",\"A more stable SI-SNR loss with clamp from fast_bss_eval.\",\"clamp_db\",\"float clamp the output value in [-clamp_db, clamp_db]\",\"zero_mean\",\"bool When set to True, the mean of all signals is subtracted prior.\",\"eps\",\"float Deprecated. Kept for compatibility.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref: Tensor, est: Tensor) → Tensor\",\"SI-SNR forward.\",\"Parameters:\",\"ref – Tensor, (…, n_samples) reference signal\",\"est – Tensor (…, n_samples) estimated signal\",\"Returns: (…,) : the SI-SDR loss (negative si-sdr)\",\"Return type: loss\"]},\"1248\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.SNRLoss\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.time_domain.SNRLoss(eps=1.1920928955078125e-07, name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref: Tensor, inf: Tensor) → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1249\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1250\":{\"h\":\"espnet2.enh.decoder.stft_decoder.STFTDecoder\",\"t\":[\"source\",\"class espnet2.enh.decoder.stft_decoder.STFTDecoder(n_fft: int = 512, win_length: int | None = None, hop_length: int = 128, window='hann', center: bool = True, normalized: bool = False, onesided: bool = True, default_fs: int = 16000, spec_transform_type: str | None = None, spec_factor: float = 0.15, spec_abs_exponent: float = 0.5)\",\"Bases: AbsDecoder\",\"STFT decoder for speech enhancement and separation\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: ComplexTensor, ilens: Tensor, fs: int = None)\",\"Forward.\",\"Parameters:\",\"input (ComplexTensor) – spectrum [Batch, T, (C,) F]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"fs (int) – sampling rate in Hz If not None, reconfigure iSTFT window and hop lengths for a new sampling rate while keeping their duration fixed.\",\"forward_streaming(input_frame: Tensor)\",\"Forward.\",\"Parameters:\",\"input (ComplexTensor) – spectrum [Batch, 1, F]\",\"output – wavs [Batch, 1, self.win_length]\",\"spec_back(spec)\",\"streaming_merge(chunks, ilens=None)\",\"streaming_merge. It merges the frame-level processed audio chunks in the streaming simulation. It is noted that, in real applications, the processed audio should be sent to the output channel frame by frame. You may refer to this function to manage your streaming output buffer.\",\"Parameters:\",\"chunks – List [(B, frame_size),]\",\"ilens – [B]\",\"Returns: [B, T]\",\"Return type: merge_audio\"]},\"1251\":{\"h\":\"espnet2.enh.encoder.stft_encoder.STFTEncoder\",\"t\":[\"source\",\"class espnet2.enh.encoder.stft_encoder.STFTEncoder(n_fft: int = 512, win_length: int | None = None, hop_length: int = 128, window='hann', center: bool = True, normalized: bool = False, onesided: bool = True, use_builtin_complex: bool = True, default_fs: int = 16000, spec_transform_type: str | None = None, spec_factor: float = 0.15, spec_abs_exponent: float = 0.5)\",\"Bases: AbsEncoder\",\"STFT encoder for speech enhancement and separation\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, fs: int = None)\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – mixed speech [Batch, sample]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"fs (int) – sampling rate in Hz If not None, reconfigure STFT window and hop lengths for a new sampling rate while keeping their duration fixed.\",\"Returns: [Batch, T, (C,) F] flens (torch.Tensor): [Batch]\",\"Return type: spectrum (ComplexTensor)\",\"forward_streaming(input: Tensor)\",\"Forward.\",\"Parameters:input (torch.Tensor) – mixed speech [Batch, frame_length]\",\"Returns: B, 1, F\",\"property output_dim : int\",\"spec_transform_func(spec)\",\"streaming_frame(audio)\",\"streaming_frame. It splits the continuous audio into frame-level audio chunks in the streaming simulation. It is noted that this function takes the entire long audio as input for a streaming simulation. You may refer to this function to manage your streaming input buffer in a real streaming application.\",\"Parameters:audio – (B, T)\",\"Returns: List [(B, frame_size),]\",\"Return type: chunked\"]},\"1252\":{\"h\":\"espnet2.enh.separator.svoice_separator.SVoiceSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.svoice_separator.SVoiceSeparator(input_dim: int, enc_dim: int, kernel_size: int, hidden_size: int, num_spk: int = 2, num_layers: int = 4, segment_size: int = 20, bidirectional: bool = True, input_normalize: bool = False)\",\"Bases: AbsSeparator\",\"SVoice model for speech separation.\",\"Reference: : Voice Separation with an Unknown Number of Multiple Speakers; E. Nachmani et al., 2020; https://arxiv.org/abs/2003.01531\",\"Parameters:\",\"enc_dim – int, dimension of the encoder module’s output. (Default: 128)\",\"kernel_size – int, the kernel size of Conv1D layer in both encoder and decoder modules. (Default: 8)\",\"hidden_size – int, dimension of the hidden state in RNN layers. (Default: 128)\",\"num_spk – int, the number of speakers in the output. (Default: 2)\",\"num_layers – int, number of stacked MulCat blocks. (Default: 4)\",\"segment_size – dual-path segment size. (Default: 20)\",\"bidirectional – bool, whether the RNN layers are bidirectional. (Default: True)\",\"input_normalize – bool, whether to apply GroupNorm on the input Tensor. (Default: False)\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1253\":{\"h\":\"espnet2.enh.diffusion.score_based_diffusion.ScoreModel\",\"t\":[\"source\",\"class espnet2.enh.diffusion.score_based_diffusion.ScoreModel(**kwargs)\",\"Bases: AbsDiffusion\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"enhance(noisy_specturm, sampler_type='pc', predictor='reverse_diffusion', corrector='ald', N=30, corrector_steps=1, snr=0.5, **kwargs)\",\"Enhance function.\",\"Parameters:\",\"noisy_specturm (torch.Tensor) – noisy feature in [Batch, T, F]\",\"sampler_type (str) – sampler, ‘pc’ for Predictor-Corrector and ‘ode’ for ODE sampler.\",\"predictor (str) – the name of Predictor. ‘reverse_diffusion’, ‘euler_maruyama’, or ‘none’\",\"corrector (str) – the name of Corrector. ‘langevin’, ‘ald’ or ‘none’\",\"N (int) – The number of reverse sampling steps.\",\"corrector_steps (int) – number of steps in the Corrector.\",\"snr (float) – The SNR to use for the corrector.\",\"Returns: enhanced feature in [Batch, T, F]\",\"Return type: X_Hat (torch.Tensor)\",\"forward(feature_ref, feature_mix)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1254\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_ode_sampler(y, N=None, minibatch=None, **kwargs)\",\"get_pc_sampler(predictor_name, corrector_name, y, N=None, minibatch=None, **kwargs)\",\"score_fn(x, t, y)\"]},\"1255\":{\"h\":\"espnet2.enh.layers.skim.SegLSTM\",\"t\":[\"source\",\"class espnet2.enh.layers.skim.SegLSTM(input_size, hidden_size, dropout=0.0, bidirectional=False, norm_type='cLN')\",\"Bases: Module\",\"the Seg-LSTM of SkiM\",\"Parameters:\",\"input_size – int, dimension of the input feature. The input should have shape (batch, seq_len, input_size).\",\"hidden_size – int, dimension of the hidden state.\",\"dropout – float, dropout ratio. Default is 0.\",\"bidirectional – bool, whether the LSTM layers are bidirectional. Default is False.\",\"norm_type – gLN, cLN. cLN is for causal implementation.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, hc)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1256\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1257\":{\"h\":\"espnet2.enh.layers.dprnn.SingleRNN\",\"t\":[\"source\",\"class espnet2.enh.layers.dprnn.SingleRNN(rnn_type, input_size, hidden_size, dropout=0, bidirectional=False)\",\"Bases: Module\",\"Container module for a single RNN layer.\",\"Parameters:\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"input_size – int, dimension of the input feature. The input should have shape (batch, seq_len, input_size).\",\"hidden_size – int, dimension of the hidden state.\",\"dropout – float, dropout ratio. Default is 0.\",\"bidirectional – bool, whether the RNN layers are bidirectional. Default is False.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, state=None)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1258\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1259\":{\"h\":\"espnet2.enh.layers.skim.SkiM\",\"t\":[\"source\",\"class espnet2.enh.layers.skim.SkiM(input_size, hidden_size, output_size, dropout=0.0, num_blocks=2, segment_size=20, bidirectional=True, mem_type='hc', norm_type='gLN', seg_overlap=False)\",\"Bases: Module\",\"Skipping Memory Net\",\"Parameters:\",\"input_size – int, dimension of the input feature. Input shape shoud be (batch, length, input_size)\",\"hidden_size – int, dimension of the hidden state.\",\"output_size – int, dimension of the output size.\",\"dropout – float, dropout ratio. Default is 0.\",\"num_blocks – number of basic SkiM blocks\",\"segment_size – segmentation size for splitting long features\",\"bidirectional – bool, whether the RNN layers are bidirectional.\",\"mem_type – ‘hc’, ‘h’, ‘c’, ‘id’ or None. It controls whether the hidden (or cell) state of SegLSTM will be processed by MemLSTM. In ‘id’ mode, both the hidden and cell states will be identically returned. When mem_type is None, the MemLSTM will be removed.\",\"norm_type – gLN, cLN. cLN is for causal implementation.\",\"seg_overlap – Bool, whether the segmentation will reserve 50% overlap for adjacent segments.Default is False.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1260\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"forward_stream(input_frame, states)\"]},\"1261\":{\"h\":\"espnet2.enh.separator.skim_separator.SkiMSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.skim_separator.SkiMSeparator(input_dim: int, causal: bool = True, num_spk: int = 2, predict_noise: bool = False, nonlinear: str = 'relu', layer: int = 3, unit: int = 512, segment_size: int = 20, dropout: float = 0.0, mem_type: str = 'hc', seg_overlap: bool = False)\",\"Bases: AbsSeparator\",\"Skipping Memory (SkiM) Separator\",\"Parameters:\",\"input_dim – input feature dimension\",\"causal – bool, whether the system is causal.\",\"num_spk – number of target speakers.\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"layer – int, number of SkiM blocks. Default is 3.\",\"unit – int, dimension of the hidden state.\",\"segment_size – segmentation size for splitting long features\",\"dropout – float, dropout ratio. Default is 0.\",\"mem_type – ‘hc’, ‘h’, ‘c’, ‘id’ or None. It controls whether the hidden (or cell) state of SegLSTM will be processed by MemLSTM. In ‘id’ mode, both the hidden and cell states will be identically returned. When mem_type is None, the MemLSTM will be removed.\",\"seg_overlap – Bool, whether the segmentation will reserve 50% overlap for adjacent segments. Default is False.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"forward_streaming(input_frame: Tensor, states=None)\",\"property num_spk\"]},\"1262\":{\"h\":\"espnet2.enh.layers.swin_transformer.SwinTransformerBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.swin_transformer.SwinTransformerBlock(dim, input_resolution, num_heads, window_size=[7, 7], shift_size=[0, 0], mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop=0.0, attn_drop=0.0, drop_path=0.0, act_layer=<class 'torch.nn.modules.activation.GELU'>, norm_layer=<class 'torch.nn.modules.normalization.LayerNorm'>)\",\"Bases: Module\",\"Swin Transformer Block.\",\"Parameters:\",\"dim (int) – Number of input channels.\",\"input_resolution (tuple *[*int]) – Input resulotion.\",\"num_heads (int) – Number of attention heads.\",\"window_size (int) – Window size.\",\"shift_size (int) – Shift size for SW-MSA.\",\"mlp_ratio (float) – Ratio of MLP hidden dim to embedding dim.\",\"qkv_bias (bool,optional) – If True, add a learnable bias to query, key, value.\",\"qk_scale (float|None,optional) – If not None, override default qk scale.\",\"drop (float,optional) – Dropout rate.\",\"attn_drop (float,optional) – Attention dropout rate.\",\"drop_path (float,optional) – Stochastic depth rate.\",\"act_layer (nn.Module,optional) – Activation layer.\",\"norm_layer (nn.Module,optional) – Normalization layer.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"calculate_mask(x_size)\",\"extra_repr() → str\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(x, x_size)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1263\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1264\":{\"h\":\"espnet2.enh.layers.tcndenseunet.TCNDenseUNet\",\"t\":[\"source\",\"class espnet2.enh.layers.tcndenseunet.TCNDenseUNet(n_spk=1, in_freqs=257, mic_channels=1, hid_chans=32, hid_chans_dense=32, ksz_dense=(3, 3), ksz_tcn=3, tcn_repeats=4, tcn_blocks=7, tcn_channels=384, activation=<class 'torch.nn.modules.activation.ELU'>)\",\"Bases: Module\",\"TCNDenseNet block from iNeuBe\",\"Reference: Lu, Y. J., Cornell, S., Chang, X., Zhang, W., Li, C., Ni, Z., … & Watanabe, S. Towards Low-Distortion Multi-Channel Speech Enhancement: The ESPNET-Se Submission to the L3DAS22 Challenge. ICASSP 2022 p. 9201-9205.\",\"Parameters:\",\"n_spk – number of output sources/speakers.\",\"in_freqs – number of complex STFT frequencies.\",\"mic_channels – number of microphones channels (only fixed-array geometry supported).\",\"hid_chans – number of channels in the subsampling/upsampling conv layers.\",\"hid_chans_dense – number of channels in the densenet layers (reduce this to reduce VRAM requirements).\",\"ksz_dense – kernel size in the densenet layers thorough iNeuBe.\",\"ksz_tcn – kernel size in the TCN submodule.\",\"tcn_repeats – number of repetitions of blocks in the TCN submodule.\",\"tcn_blocks – number of blocks in the TCN submodule.\",\"tcn_channels – number of channels in the TCN submodule.\",\"activation – activation function to use in the whole iNeuBe model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(tf_rep)\",\"forward.\",\"Parameters:tf_rep (torch.Tensor) – 4D tensor (multi-channel complex STFT of mixture) of shape [B, T, C, F] batch, frames, microphones, frequencies.\",\"Returns: complex 3D tensor monaural STFT of the targets : shape is [B, T, F] batch, frames, frequencies.\",\"Return type: out (torch.Tensor)\"]},\"1265\":{\"h\":\"espnet2.enh.layers.tcndenseunet.TCNResBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.tcndenseunet.TCNResBlock(in_chan, out_chan, ksz=3, stride=1, dilation=1, activation=<class 'torch.nn.modules.activation.ELU'>)\",\"Bases: Module\",\"single depth-wise separable TCN block as used in iNeuBe TCN.\",\"Parameters:\",\"in_chan – number of input feature channels.\",\"out_chan – number of output feature channels.\",\"ksz – kernel size.\",\"stride – stride in depth-wise convolution.\",\"dilation – dilation in depth-wise convolution.\",\"activation – activation function to use in the whole iNeuBe model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inp)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1266\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1267\":{\"h\":\"espnet2.enh.separator.tcn_separator.TCNSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.tcn_separator.TCNSeparator(input_dim: int, num_spk: int = 2, predict_noise: bool = False, layer: int = 8, stack: int = 3, bottleneck_dim: int = 128, hidden_dim: int = 512, kernel: int = 3, causal: bool = False, norm_type: str = 'gLN', nonlinear: str = 'relu', pre_mask_nonlinear: str = 'prelu', masking: bool = True)\",\"Bases: AbsSeparator\",\"Temporal Convolution Separator\",\"Parameters:\",\"input_dim – input feature dimension\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"layer – int, number of layers in each stack.\",\"stack – int, number of stacks\",\"bottleneck_dim – bottleneck dimension\",\"hidden_dim – number of convolution channel\",\"kernel – int, kernel size.\",\"causal – bool, defalut False.\",\"norm_type – str, choose from ‘BN’, ‘gLN’, ‘cLN’\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’, ‘linear’\",\"pre_mask_nonlinear – the non-linear function before masknet\",\"masking – whether to use the masking or mapping based method\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"forward_streaming(input_frame: Tensor, buffer=None)\",\"property num_spk\"]},\"1268\":{\"h\":\"espnet2.enh.extractor.td_speakerbeam_extractor.TDSpeakerBeamExtractor\",\"t\":[\"source\",\"class espnet2.enh.extractor.td_speakerbeam_extractor.TDSpeakerBeamExtractor(input_dim: int, layer: int = 8, stack: int = 3, bottleneck_dim: int = 128, hidden_dim: int = 512, skip_dim: int = 128, kernel: int = 3, causal: bool = False, norm_type: str = 'gLN', pre_nonlinear: str = 'prelu', nonlinear: str = 'relu', i_adapt_layer: int = 7, adapt_layer_type: str = 'mul', adapt_enroll_dim: int = 128, use_spk_emb: bool = False, spk_emb_dim: int = 256)\",\"Bases: AbsExtractor\",\"Time-Domain SpeakerBeam Extractor.\",\"Parameters:\",\"input_dim – input feature dimension\",\"layer – int, number of layers in each stack\",\"stack – int, number of stacks\",\"bottleneck_dim – bottleneck dimension\",\"hidden_dim – number of convolution channel\",\"skip_dim – int, number of skip connection channels\",\"kernel – int, kernel size.\",\"causal – bool, defalut False.\",\"norm_type – str, choose from ‘BN’, ‘gLN’, ‘cLN’\",\"pre_nonlinear – the nonlinear function right before mask estimation select from ‘prelu’, ‘relu’, ‘tanh’, ‘sigmoid’, ‘linear’\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’, ‘linear’\",\"i_adapt_layer – int, index of adaptation layer\",\"adapt_layer_type – str, type of adaptation layer see espnet2.enh.layers.adapt_layers for options\",\"adapt_enroll_dim – int, dimensionality of the speaker embedding\",\"use_spk_emb – bool, whether to use speaker embeddings as enrollment\",\"spk_emb_dim – int, dimension of input speaker embeddings only used when use_spk_emb is True\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, input_aux: Tensor, ilens_aux: Tensor, suffix_tag: str = '', additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"TD-SpeakerBeam Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"input_aux (torch.TensororComplexTensor) – Encoded auxiliary feature for the target speaker [B, T, N] or [B, N]\",\"ilens_aux (torch.Tensor) – input lengths of auxiliary input for the target speaker [Batch]\",\"suffix_tag (str) – suffix to append to the keys in others\",\"additional (Noneordict) – additional parameters not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"f’mask{suffix_tag}’: torch.Tensor(Batch, Frames, Freq), f’enroll_emb{suffix_tag}’: torch.Tensor(Batch, adapt_enroll_dim/adapt_enroll_dim*2),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\"]},\"1269\":{\"h\":\"espnet2.enh.separator.tfgridnet_separator.TFGridNet\",\"t\":[\"source\",\"class espnet2.enh.separator.tfgridnet_separator.TFGridNet(input_dim, n_srcs=2, n_fft=128, stride=64, window='hann', n_imics=1, n_layers=6, lstm_hidden_units=192, attn_n_head=4, attn_approx_qk_dim=512, emb_dim=48, emb_ks=4, emb_hs=1, activation='prelu', eps=1e-05, use_builtin_complex=False, ref_channel=-1)\",\"Bases: AbsSeparator\",\"Offline TFGridNet\",\"Reference: [1] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation”, in arXiv preprint arXiv:2211.12433, 2022. [2] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Making Time-Frequency Domain Models Great Again for Monaural Speaker Separation”, in arXiv preprint arXiv:2209.03952, 2022.\",\"NOTES: As outlined in the Reference, this model works best when trained with variance normalized mixture input and target, e.g., with mixture of shape [batch, samples, microphones], you normalize it by dividing with torch.std(mixture, (1, 2)). You must do the same for the target signals. It is encouraged to do so when not using scale-invariant loss functions such as SI-SDR.\",\"Parameters:\",\"input_dim – placeholder, not used\",\"n_srcs – number of output sources/speakers.\",\"n_fft – stft window size.\",\"stride – stft stride.\",\"window – stft window type choose between ‘hamming’, ‘hanning’ or None.\",\"n_imics – number of microphones channels (only fixed-array geometry supported).\",\"n_layers – number of TFGridNet blocks.\",\"lstm_hidden_units – number of hidden units in LSTM.\",\"attn_n_head – number of heads in self-attention\",\"attn_approx_qk_dim – approximate dimention of frame-level key and value tensors\",\"emb_dim – embedding dimension\",\"emb_ks – kernel size for unfolding and deconv1D\",\"emb_hs – hop size for unfolding and deconv1D\",\"activation – activation function to use in the whole TFGridNet model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"eps – small epsilon for normalization layers.\",\"use_builtin_complex – whether to use builtin complex type or not.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – batched multi-channel audio tensor with M audio channels and N samples [B, N, M]\",\"ilens (torch.Tensor) – input lengths [B]\",\"additional (DictorNone) – other data, currently unused in this model.\",\"Returns: [(B, T), …] list of len n_srcs : of mono audio tensors with T samples.\",\"ilens (torch.Tensor): (B,) additional (Dict or None): other data, currently unused in this model,\",\"we return it also in output.\",\"Return type: enhanced (List[Union(torch.Tensor)])\",\"property num_spk\",\"static pad2(input_tensor, target_len)\"]},\"1270\":{\"h\":\"espnet2.enh.separator.tfgridnetv2_separator.TFGridNetV2\",\"t\":[\"source\",\"class espnet2.enh.separator.tfgridnetv2_separator.TFGridNetV2(input_dim, n_srcs=2, n_fft=128, stride=64, window='hann', n_imics=1, n_layers=6, lstm_hidden_units=192, attn_n_head=4, attn_approx_qk_dim=512, emb_dim=48, emb_ks=4, emb_hs=1, activation='prelu', eps=1e-05, use_builtin_complex=False)\",\"Bases: AbsSeparator\",\"Offline TFGridNetV2. Compared with TFGridNet, TFGridNetV2 speeds up the code\",\"by vectorizing multiple heads in self-attention, and better dealing with Deconv1D in each intra- and inter-block when emb_ks == emb_hs.\",\"Reference: [1] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation”, in TASLP, 2023. [2] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Making Time-Frequency Domain Models Great Again for Monaural Speaker Separation”, in ICASSP, 2023.\",\"NOTES: As outlined in the Reference, this model works best when trained with variance normalized mixture input and target, e.g., with mixture of shape [batch, samples, microphones], you normalize it by dividing with torch.std(mixture, (1, 2)). You must do the same for the target signals. It is encouraged to do so when not using scale-invariant loss functions such as SI-SDR. Specifically, use:\",\"std_\",\"= std(mix) mix = mix /\",\"std_\",\"tgt = tgt /\",\"std_\",\"Parameters:\",\"input_dim – placeholder, not used\",\"n_srcs – number of output sources/speakers.\",\"n_fft – stft window size.\",\"stride – stft stride.\",\"window – stft window type choose between ‘hamming’, ‘hanning’ or None.\",\"n_imics – number of microphones channels (only fixed-array geometry supported).\",\"n_layers – number of TFGridNetV2 blocks.\",\"lstm_hidden_units – number of hidden units in LSTM.\",\"attn_n_head – number of heads in self-attention\",\"attn_approx_qk_dim – approximate dimention of frame-level key and value tensors\",\"emb_dim – embedding dimension\",\"emb_ks – kernel size for unfolding and deconv1D\",\"emb_hs – hop size for unfolding and deconv1D\",\"activation – activation function to use in the whole TFGridNetV2 model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"eps – small epsilon for normalization layers.\",\"use_builtin_complex – whether to use builtin complex type or not.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – batched multi-channel audio tensor with M audio channels and N samples [B, N, M]\",\"ilens (torch.Tensor) – input lengths [B]\",\"additional (DictorNone) – other data, currently unused in this model.\",\"Returns: [(B, T), …] list of len n_srcs : of mono audio tensors with T samples.\",\"ilens (torch.Tensor): (B,) additional (Dict or None): other data, currently unused in this model,\",\"we return it also in output.\",\"Return type: enhanced (List[Union(torch.Tensor)])\",\"property num_spk\",\"static pad2(input_tensor, target_len)\"]},\"1271\":{\"h\":\"espnet2.enh.separator.tfgridnetv3_separator.TFGridNetV3\",\"t\":[\"source\",\"class espnet2.enh.separator.tfgridnetv3_separator.TFGridNetV3(input_dim, n_srcs=2, n_imics=1, n_layers=6, lstm_hidden_units=192, attn_n_head=4, attn_qk_output_channel=4, emb_dim=48, emb_ks=4, emb_hs=1, activation='prelu', eps=1e-05)\",\"Bases: AbsSeparator\",\"Offline TFGridNetV3.\",\"On top of TFGridNetV2, TFGridNetV3 slightly modifies the internal architecture to make the model sampling-frequency-independent (SFI). This is achieved by making all network layers independent of the input time and frequency dimensions.\",\"Reference: [1] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation”, in TASLP, 2023. [2] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Making Time-Frequency Domain Models Great Again for Monaural Speaker Separation”, in ICASSP, 2023.\",\"NOTES: As outlined in the Reference, this model works best when trained with variance normalized mixture input and target, e.g., with mixture of shape [batch, samples, microphones], you normalize it by dividing with torch.std(mixture, (1, 2)). You must do the same for the target signals. It is encouraged to do so when not using scale-invariant loss functions such as SI-SDR. Specifically, use:\",\"std_\",\"= std(mix) mix = mix /\",\"std_\",\"tgt = tgt /\",\"std_\",\"Parameters:\",\"input_dim – placeholder, not used\",\"n_srcs – number of output sources/speakers.\",\"n_fft – stft window size.\",\"stride – stft stride.\",\"window – stft window type choose between ‘hamming’, ‘hanning’ or None.\",\"n_imics – number of microphones channels (only fixed-array geometry supported).\",\"n_layers – number of TFGridNetV3 blocks.\",\"lstm_hidden_units – number of hidden units in LSTM.\",\"attn_n_head – number of heads in self-attention\",\"attn_attn_qk_output_channel – output channels of point-wise conv2d for getting key and query\",\"emb_dim – embedding dimension\",\"emb_ks – kernel size for unfolding and deconv1D\",\"emb_hs – hop size for unfolding and deconv1D\",\"activation – activation function to use in the whole TFGridNetV3 model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"eps – small epsilon for normalization layers.\",\"use_builtin_complex – whether to use builtin complex type or not.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor) – batched multi-channel audio tensor with M audio channels and N samples [B, T, F]\",\"ilens (torch.Tensor) – input lengths [B]\",\"additional (DictorNone) – other data, currently unused in this model.\",\"Returns: [(B, T), …] list of len n_srcs : of mono audio tensors with T samples.\",\"ilens (torch.Tensor): (B,) additional (Dict or None): other data, currently unused in this model,\",\"we return it also in output.\",\"Return type: enhanced (List[Union(torch.Tensor)])\",\"property num_spk\"]},\"1272\":{\"h\":\"espnet2.enh.layers.tcn.TemporalBlock\",\"t\":[\"source\",\"class espnet2.enh.layers.tcn.TemporalBlock(in_channels, out_channels, skip_channels, kernel_size, stride, padding, dilation, norm_type='gLN', causal=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward.\",\"Parameters:x – [M, B, K]\",\"Returns: [M, B, K]\"]},\"1273\":{\"h\":\"espnet2.enh.layers.tcn.TemporalConvNet\",\"t\":[\"source\",\"class espnet2.enh.layers.tcn.TemporalConvNet(N, B, H, P, X, R, C, Sc=None, out_channel=None, norm_type='gLN', causal=False, pre_mask_nonlinear='linear', mask_nonlinear='relu')\",\"Bases: Module\",\"Basic Module of tasnet.\",\"Parameters:\",\"N – Number of filters in autoencoder\",\"B – Number of channels in bottleneck 1 * 1-conv block\",\"H – Number of channels in convolutional blocks\",\"P – Kernel size in convolutional blocks\",\"X – Number of convolutional blocks in each repeat\",\"R – Number of repeats\",\"C – Number of speakers\",\"Sc – Number of channels in skip-connection paths’ 1x1-conv blocks\",\"out_channel – Number of output channels if it is None, N will be used instead.\",\"norm_type – BN, gLN, cLN\",\"causal – causal or non-causal\",\"pre_mask_nonlinear – the non-linear function before masknet\",\"mask_nonlinear – use which non-linear function to generate mask\",\"forward(mixture_w)\",\"Keep this API same with TasNet.\",\"Parameters:mixture_w – [M, N, K], M is batch size\",\"Returns: [M, C, N, K]\",\"Return type: est_mask\"]},\"1274\":{\"h\":\"espnet2.enh.layers.tcn.TemporalConvNetInformed\",\"t\":[\"source\",\"class espnet2.enh.layers.tcn.TemporalConvNetInformed(N, B, H, P, X, R, Sc=None, out_channel=None, norm_type='gLN', causal=False, pre_mask_nonlinear='prelu', mask_nonlinear='relu', i_adapt_layer: int = 7, adapt_layer_type: str = 'mul', adapt_enroll_dim: int = 128, **adapt_layer_kwargs)\",\"Bases: TemporalConvNet\",\"Basic Module of TasNet with adaptation layers.\",\"Parameters:\",\"N – Number of filters in autoencoder\",\"B – Number of channels in bottleneck 1 * 1-conv block\",\"H – Number of channels in convolutional blocks\",\"P – Kernel size in convolutional blocks\",\"X – Number of convolutional blocks in each repeat\",\"R – Number of repeats\",\"Sc – Number of channels in skip-connection paths’ 1x1-conv blocks\",\"out_channel – Number of output channels if it is None, N will be used instead.\",\"norm_type – BN, gLN, cLN\",\"causal – causal or non-causal\",\"pre_mask_nonlinear – the non-linear function before masknet\",\"mask_nonlinear – use which non-linear function to generate mask\",\"i_adapt_layer – int, index of the adaptation layer\",\"adapt_layer_type – str, type of adaptation layer see espnet2.enh.layers.adapt_layers for options\",\"adapt_enroll_dim – int, dimensionality of the speaker embedding\",\"forward(mixture_w, enroll_emb)\",\"TasNet forward with adaptation layers.\",\"Parameters:\",\"mixture_w – [M, N, K], M is batch size\",\"enroll_emb – [M, 2*adapt_enroll_dim] if self.skip_connection [M, adapt_enroll_dim] if not self.skip_connection\",\"Returns: [M, N, K]\",\"Return type: est_mask\"]},\"1275\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.TimeDomainL1\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.time_domain.TimeDomainL1(name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref, inf) → Tensor\",\"Time-domain L1 loss forward.\",\"Parameters:\",\"ref – (Batch, T) or (Batch, T, C)\",\"inf – (Batch, T) or (Batch, T, C)\",\"Returns: (Batch,)\",\"Return type: loss\"]},\"1276\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.TimeDomainLoss\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.time_domain.TimeDomainLoss(name, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: AbsEnhLoss, ABC\",\"Base class for all time-domain Enhancement loss modules.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property is_dereverb_loss : bool\",\"property is_noise_loss : bool\",\"property name : str\",\"property only_for_test : bool\"]},\"1277\":{\"h\":\"espnet2.enh.loss.criterions.time_domain.TimeDomainMSE\",\"t\":[\"source\",\"class espnet2.enh.loss.criterions.time_domain.TimeDomainMSE(name=None, only_for_test=False, is_noise_loss=False, is_dereverb_loss=False)\",\"Bases: TimeDomainLoss\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ref, inf) → Tensor\",\"Time-domain MSE loss forward.\",\"Parameters:\",\"ref – (Batch, T) or (Batch, T, C)\",\"inf – (Batch, T) or (Batch, T, C)\",\"Returns: (Batch,)\",\"Return type: loss\"]},\"1278\":{\"h\":\"espnet2.enh.separator.transformer_separator.TransformerSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.transformer_separator.TransformerSeparator(input_dim: int, num_spk: int = 2, predict_noise: bool = False, adim: int = 384, aheads: int = 4, layers: int = 6, linear_units: int = 1536, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, normalize_before: bool = False, concat_after: bool = False, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1, use_scaled_pos_enc: bool = True, nonlinear: str = 'relu')\",\"Bases: AbsSeparator\",\"Transformer separator.\",\"Parameters:\",\"input_dim – input feature dimension\",\"num_spk – number of speakers\",\"predict_noise – whether to output the estimated noise signal\",\"adim (int) – Dimension of attention.\",\"aheads (int) – The number of heads of multi head attention.\",\"linear_units (int) – The number of units of position-wise feed forward.\",\"layers (int) – The number of transformer blocks.\",\"dropout_rate (float) – Dropout rate.\",\"attention_dropout_rate (float) – Dropout rate in attention.\",\"positional_dropout_rate (float) – Dropout rate after adding positional encoding.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type (str) – “linear”, “conv1d”, or “conv1d-linear”.\",\"positionwise_conv_kernel_size (int) – Kernel size of positionwise conv1d layer.\",\"use_scaled_pos_enc (bool) – use scaled positional encoding or not\",\"nonlinear – the nonlinear function for mask estimation, select from ‘relu’, ‘tanh’, ‘sigmoid’\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – Encoded feature [B, T, N]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data included in model NOTE: not used in this model\",\"Returns: [(B, T, N), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1279\":{\"h\":\"espnet2.enh.layers.uses.USES\",\"t\":[\"source\",\"class espnet2.enh.layers.uses.USES(input_size, output_size, bottleneck_size=64, num_blocks=6, num_spatial_blocks=3, segment_size=64, memory_size=20, memory_types=1, rnn_type='lstm', hidden_size=128, att_heads=4, dropout=0.0, activation='relu', bidirectional=True, norm_type='cLN', ch_mode='att', ch_att_dim=256, eps=1e-05)\",\"Bases: Module\",\"Unconstrained Speech Enhancement and Separation (USES) Network.\",\"Reference: : [1] W. Zhang, K. Saijo, Z.-Q., Wang, S. Watanabe, and Y. Qian, “Toward Universal Speech Enhancement for Diverse Input Conditions,” in Proc. ASRU, 2023.\",\"Parameters:\",\"input_size (int) – dimension of the input feature.\",\"output_size (int) – dimension of the output.\",\"bottleneck_size (int) – dimension of the bottleneck feature. Must be a multiple of att_heads.\",\"num_blocks (int) – number of processing blocks.\",\"num_spatial_blocks (int) – number of processing blocks with channel modeling.\",\"segment_size (int) – number of frames in each non-overlapping segment. This is used to segment long utterances into smaller segments for efficient processing.\",\"memory_size (int) – group size of global memory tokens. The basic use of memory tokens is to store the history information from previous segments. The memory tokens are updated by the output of the last block after processing each segment.\",\"memory_types (int) –\",\"numbre of memory token groups. Each group corresponds to a different type of processing, i.e.,\",\"the first group is used for denoising without dereverberation, the second group is used for denoising with dereverberation.\",\"rnn_type (str) – type of the RNN cell in the improved Transformer layer.\",\"hidden_size (int) – hidden dimension of the RNN cell.\",\"att_heads (int) – number of attention heads in Transformer.\",\"dropout (float) – dropout ratio. Default is 0.\",\"activation (str) – non-linear activation function applied in each block.\",\"bidirectional (bool) – whether the RNN layers are bidirectional.\",\"norm_type (str) – normalization type in the improved Transformer layer.\",\"ch_mode (str) – mode of channel modeling. Select from “att” and “tac”.\",\"ch_att_dim (int) – dimension of the channel attention.\",\"eps (float) – epsilon for layer normalization.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, ref_channel=None, mem_idx=None)\",\"USES forward.\",\"Parameters:\",\"input (torch.Tensor) – input feature (batch, mics, input_size, freq, time)\",\"ref_channel (Noneorint) – index of the reference channel. if None, simply average all channels. if int, take the specified channel instead of averaging.\",\"mem_idx (Noneorint) – index of the memory token group. if None, use the only group of memory tokens in the model. if int, use the specified group from multiple existing groups.\",\"Returns: output feature (batch, output_size, freq, time)\",\"Return type: output (torch.Tensor)\"]},\"1280\":{\"h\":\"espnet2.enh.separator.uses2_separator.USES2Separator\",\"t\":[\"source\",\"class espnet2.enh.separator.uses2_separator.USES2Separator(input_dim: int, num_spk: int = 2, enc_channels: int = 256, bottleneck_size: int = 64, num_blocks: int = 4, num_spatial_blocks: int = 2, ref_channel: int | None = None, tf_mode: str = 'comp', swin_block_depth: int | Tuple[int] = (4, 4, 4, 4), segment_size: int = 64, memory_size: int = 20, memory_types: int = 1, input_resolution: Tuple[int, int] = (130, 64), window_size: Tuple[int, int] = (10, 8), mlp_ratio: int = 4, qkv_bias: bool = True, qk_scale: float | None = None, rnn_type: str = 'lstm', bidirectional: bool = True, hidden_size: int = 128, att_heads: int = 4, dropout: float = 0.0, att_dropout: float = 0.0, drop_path: float = 0.0, norm_type: str = 'cLN', activation: str = 'relu', use_checkpoint: bool = False, ch_mode: str | List[str] = 'att_tac', ch_att_dim: int = 256, eps: float = 1e-05, additional: dict = {})\",\"Bases: AbsSeparator\",\"Unconstrained Speech Enhancement and Separation v2 (USES2) Network.\",\"Reference: : [1] W. Zhang, J.-w. Jung, and Y. Qian, “Improving Design of Input Condition Invariant Speech Enhancement,” in Proc. ICASSP, 2024. [2] W. Zhang, K. Saijo, Z.-Q., Wang, S. Watanabe, and Y. Qian, “Toward Universal Speech Enhancement for Diverse Input Conditions,” in Proc. ASRU, 2023.\",\"Parameters:\",\"input_dim (int) – input feature dimension. Not used as the model is independent of the input size.\",\"num_spk (int) – number of speakers.\",\"enc_channels (int) – feature dimension after the Conv1D encoder.\",\"bottleneck_size (int) – dimension of the bottleneck feature. Must be a multiple of att_heads.\",\"num_blocks (int) – number of processing blocks.\",\"num_spatial_blocks (int) – number of processing blocks with channel modeling.\",\"ref_channel (int) – reference channel (used in channel modeling modules).\",\"tf_mode (str) – mode of Time-Frequency modeling. Select from “swin” and “comp”.\",\"swin_block_depth (Tuple *[*int]) – depth of each Swin-Transformer block.\",\"segment_size (int) – number of frames in each non-overlapping segment. This is only used when tf_mode is “comp”, and is used to segment long utterances into smaller chunks for efficient processing.\",\"memory_size (int) – group size of global memory tokens. This is only used when tf_mode is “comp”. The basic use of memory tokens is to store the history information from previous segments. The memory tokens are updated by the output of the last block after processing each segment.\",\"memory_types (int) –\",\"numbre of memory token groups. This is only used when tf_mode is “comp”. Each group corresponds to a different type of processing, i.e.,\",\"the first group is used for denoising without dereverberation, the second group is used for denoising with dereverberation.\",\"input_resolution (tuple) – frequency and time dimension of the input feature. Only used for efficient training. Should be close to the actual spectrum size (F, T) of training samples.\",\"window_size (tuple) – size of the Time-Frequency window in Swin-Transformer.\",\"mlp_ratio (int) – ratio of the MLP hidden size to embedding size in BasicLayer.\",\"qkv_bias (bool) – If True, add a learnable bias to query, key, value in BasicLayer.\",\"qk_scale (float) – Override default qk scale of head_dim ** -0.5 in BasicLayer if set.\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"bidirectional (bool) – whether the inter-chunk RNN layers are bidirectional.\",\"hidden_size (int) – dimension of the hidden state.\",\"att_heads (int) – number of attention heads.\",\"dropout (float) – dropout ratio. Default is 0.\",\"att_dropout (float) – attention dropout ratio in BasicLayer.\",\"drop_path (float) – drop-path ratio in BasicLayer.\",\"norm_type – type of normalization to use after each inter- or intra-chunk NN block.\",\"activation – the nonlinear activation function.\",\"use_checkpoint (bool) – whether to use checkpointing to save memory.\",\"ch_mode (strorlist) – mode of channel modeling. Select from “att”, “tac”, and “att_tac”.\",\"ch_att_dim (int) – dimension of the channel attention.\",\"ref_channel – Optional[int], index of the reference channel.\",\"eps (float) – epsilon for layer normalization.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – STFT spectrum [B, T, (C,) F (,2)] B is the batch size T is the number of time frames C is the number of microphone channels (optional) F is the number of frequency bins 2 is real and imaginary parts (optional if input is a complex tensor)\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) –\",\"other data included in model “mode”: one of (“no_dereverb”, “dereverb”, “both”), only used when\",\"self.tf_mode == “comp”\",\"”no_dereverb”: only use the first memory group for denoising : without dereverberation\",\"”dereverb”: only use the second memory group for denoising : with dereverberation\",\"”both”: use both memory groups for denoising with and without : dereverberation\",\"Returns: [(B, T, F), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1281\":{\"h\":\"espnet2.enh.layers.uses2_comp.USES2_Comp\",\"t\":[\"source\",\"class espnet2.enh.layers.uses2_comp.USES2_Comp(input_size, output_size, bottleneck_size=64, num_blocks=4, num_spatial_blocks=2, segment_size=64, memory_size=20, memory_types=1, input_resolution=(130, 64), window_size=(10, 8), mlp_ratio=4, qkv_bias=True, qk_scale=None, att_heads=4, dropout=0.0, att_dropout=0.0, drop_path=0.0, use_checkpoint=False, rnn_type='lstm', hidden_size=128, activation='relu', bidirectional=True, norm_type='cLN', ch_mode='att_tac', ch_att_dim=256, eps=1e-05)\",\"Bases: Module\",\"Unconstrained Speech Enhancement and Separation v2 (USES2-Comp) Network.\",\"Reference: : [1] W. Zhang, J.-w. Jung, and Y. Qian, “Improving Design of Input : Condition Invariant Speech Enhancement,” in Proc. ICASSP, 2024. <br/> [2] W. Zhang, K. Saijo, Z.-Q., Wang, S. Watanabe, and Y. Qian, : “Toward Universal Speech Enhancement for Diverse Input Conditions,” in Proc. ASRU, 2023.\",\"Parameters:\",\"input_size (int) – dimension of the input feature.\",\"output_size (int) – dimension of the output.\",\"bottleneck_size (int) – dimension of the bottleneck feature. Must be a multiple of att_heads.\",\"num_blocks (int) – number of processing blocks.\",\"num_spatial_blocks (int) – number of processing blocks with channel modeling.\",\"segment_size (int) – number of frames in each non-overlapping segment. This is used to segment long utterances into smaller segments for efficient processing.\",\"memory_size (int) – group size of global memory tokens. The basic use of memory tokens is to store the history information from previous segments. The memory tokens are updated by the output of the last block after processing each segment.\",\"memory_types (int) –\",\"numbre of memory token groups. Each group corresponds to a different type of processing, i.e.,\",\"the first group is used for denoising without dereverberation, the second group is used for denoising with dereverberation.\",\"input_resolution (tuple) – frequency and time dimension of the input feature. Only used for efficient training. Should be close to the actual spectrum size (F, T) of training samples.\",\"window_size (tuple) – size of the Time-Frequency window in Swin-Transformer.\",\"mlp_ratio (int) – ratio of the MLP hidden size to embedding size in BasicLayer.\",\"qkv_bias (bool) – If True, add a learnable bias to query, key, value in BasicLayer.\",\"qk_scale (float) – Override default qk scale of head_dim ** -0.5 in BasicLayer if set.\",\"att_heads (int) – number of attention heads in Transformer.\",\"dropout (float) – dropout ratio. Default is 0.\",\"att_dropout (float) – dropout ratio in attention in BasicLayer.\",\"drop_path (float) – drop-path ratio in BasicLayer.\",\"use_checkpoint (bool) – whether to use checkpointing to save memory.\",\"rnn_type (str) – type of the RNN cell in the improved Transformer layer.\",\"hidden_size (int) – hidden dimension of the RNN cell.\",\"activation (str) – non-linear activation function applied in each block.\",\"bidirectional (bool) – whether the RNN layers are bidirectional.\",\"norm_type (str) – normalization type in the improved Transformer layer.\",\"ch_mode (str) – mode of channel modeling. Select from “att”, “tac”, and “att_tac”.\",\"ch_att_dim (int) – dimension of the channel attention.\",\"eps (float) – epsilon for layer normalization.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, ref_channel=None, mem_idx=None)\",\"USES2-Comp forward.\",\"Parameters:\",\"input (torch.Tensor) – input feature (batch, mics, input_size, freq, time)\",\"ref_channel (Noneorint) – index of the reference channel.\",\"Returns: output feature (batch, output_size, freq, time)\",\"Return type: output (torch.Tensor)\"]},\"1282\":{\"h\":\"espnet2.enh.layers.uses2_swin.USES2_Swin\",\"t\":[\"source\",\"class espnet2.enh.layers.uses2_swin.USES2_Swin(input_size, output_size, bottleneck_size=64, num_blocks=3, num_spatial_blocks=2, swin_block_depth=(4, 4, 4, 4), input_resolution=(130, 256), window_size=(10, 8), mlp_ratio=4, qkv_bias=True, qk_scale=None, att_heads=4, dropout=0.0, att_dropout=0.0, drop_path=0.0, activation='relu', use_checkpoint=False, ch_mode='att_tac', ch_att_dim=256, eps=1e-05)\",\"Bases: Module\",\"Unconstrained Speech Enhancement and Separation v2 (USES2-Swin) Network.\",\"Reference: : [1] W. Zhang, J.-w. Jung, and Y. Qian, “Improving Design of Input : Condition Invariant Speech Enhancement,” in Proc. ICASSP, 2024. <br/> [2] W. Zhang, K. Saijo, Z.-Q., Wang, S. Watanabe, and Y. Qian, : “Toward Universal Speech Enhancement for Diverse Input Conditions,” in Proc. ASRU, 2023.\",\"Parameters:\",\"input_size (int) – dimension of the input feature.\",\"output_size (int) – dimension of the output.\",\"bottleneck_size (int) – dimension of the bottleneck feature. Must be a multiple of att_heads.\",\"num_blocks (int) – number of ResSwinBlock blocks.\",\"num_spatial_blocks (int) – number of ResSwinBlock blocks with channel modeling.\",\"swin_block_depth (Tuple *[*int]) – depth of each ResSwinBlock.\",\"input_resolution (tuple) – frequency and time dimension of the input feature. Only used for efficient training. Should be close to the actual spectrum size (F, T) of training samples.\",\"window_size (tuple) – size of the Time-Frequency window in Swin-Transformer.\",\"mlp_ratio (int) – ratio of the MLP hidden size to embedding size in BasicLayer.\",\"qkv_bias (bool) – If True, add a learnable bias to query, key, value in BasicLayer.\",\"qk_scale (float) – Override default qk scale of head_dim ** -0.5 in BasicLayer if set.\",\"att_heads (int) – number of attention heads in Transformer.\",\"dropout (float) – dropout ratio in BasicLayer. Default is 0.\",\"att_dropout (float) – attention dropout ratio in BasicLayer.\",\"drop_path (float) – drop-path ratio in BasicLayer.\",\"activation (str) – non-linear activation function applied in each block.\",\"use_checkpoint (bool) – whether to use checkpointing to save memory.\",\"ch_mode (str) – mode of channel modeling. Select from “att”, “tac”, and “att_tac”\",\"ch_att_dim (int) – dimension of the channel attention.\",\"eps (float) – epsilon for layer normalization.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, ref_channel=None)\",\"USES2-Swin forward.\",\"Parameters:\",\"input (torch.Tensor) – input feature (batch, mics, input_size, freq, time)\",\"ref_channel (Noneorint) – index of the reference channel.\",\"Returns: output feature (batch, output_size, freq, time)\",\"Return type: output (torch.Tensor)\"]},\"1283\":{\"h\":\"espnet2.enh.separator.uses_separator.USESSeparator\",\"t\":[\"source\",\"class espnet2.enh.separator.uses_separator.USESSeparator(input_dim: int, num_spk: int = 2, enc_channels: int = 256, bottleneck_size: int = 64, num_blocks: int = 6, num_spatial_blocks: int = 3, ref_channel: int | None = None, segment_size: int = 64, memory_size: int = 20, memory_types: int = 1, rnn_type: str = 'lstm', bidirectional: bool = True, hidden_size: int = 128, att_heads: int = 4, dropout: float = 0.0, norm_type: str = 'cLN', activation: str = 'relu', ch_mode: str | List[str] = 'att', ch_att_dim: int = 256, eps: float = 1e-05, additional: dict = {})\",\"Bases: AbsSeparator\",\"Unconstrained Speech Enhancement and Separation (USES) Network.\",\"Reference: : [1] W. Zhang, K. Saijo, Z.-Q., Wang, S. Watanabe, and Y. Qian, “Toward Universal Speech Enhancement for Diverse Input Conditions,” in Proc. ASRU, 2023.\",\"Parameters:\",\"input_dim (int) – input feature dimension. Not used as the model is independent of the input size.\",\"num_spk (int) – number of speakers.\",\"enc_channels (int) – feature dimension after the Conv1D encoder.\",\"bottleneck_size (int) – dimension of the bottleneck feature. Must be a multiple of att_heads.\",\"num_blocks (int) – number of processing blocks.\",\"num_spatial_blocks (int) – number of processing blocks with channel modeling.\",\"ref_channel (int) – reference channel (used in channel modeling modules).\",\"segment_size (int) – number of frames in each non-overlapping segment. This is used to segment long utterances into smaller chunks for efficient processing.\",\"memory_size (int) – group size of global memory tokens. The basic use of memory tokens is to store the history information from previous segments. The memory tokens are updated by the output of the last block after processing each segment.\",\"memory_types (int) –\",\"numbre of memory token groups. Each group corresponds to a different type of processing, i.e.,\",\"the first group is used for denoising without dereverberation, the second group is used for denoising with dereverberation.\",\"rnn_type – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.\",\"bidirectional (bool) – whether the inter-chunk RNN layers are bidirectional.\",\"hidden_size (int) – dimension of the hidden state.\",\"att_heads (int) – number of attention heads.\",\"dropout (float) – dropout ratio. Default is 0.\",\"norm_type – type of normalization to use after each inter- or intra-chunk NN block.\",\"activation – the nonlinear activation function.\",\"ch_mode – str or list, mode of channel modeling. Select from “att” and “tac”.\",\"ch_att_dim (int) – dimension of the channel attention.\",\"ref_channel – Optional[int], index of the reference channel.\",\"eps (float) – epsilon for layer normalization.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.TensororComplexTensor) – STFT spectrum [B, T, (C,) F (,2)] B is the batch size T is the number of time frames C is the number of microphone channels (optional) F is the number of frequency bins 2 is real and imaginary parts (optional if input is a complex tensor)\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) –\",\"other data included in model “mode”: one of (“no_dereverb”, “dereverb”, “both”)\",\"“no_dereverb”: only use the first memory group for denoising\",\"without dereverberation\",\"”dereverb”: only use the second memory group for denoising : with dereverberation\",\"”both”: use both memory groups for denoising with and without : dereverberation\",\"Returns: [(B, T, F), …] ilens (torch.Tensor): (B,) others predicted data, e.g. masks: OrderedDict[\",\"’mask_spk1’: torch.Tensor(Batch, Frames, Freq), ‘mask_spk2’: torch.Tensor(Batch, Frames, Freq), … ‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),\",\"]\",\"Return type: masked (List[Union(torch.Tensor, ComplexTensor)])\",\"property num_spk\"]},\"1284\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.Upsample\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.Upsample(channels, with_conv=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1285\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1286\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.UpsampleConv\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.layers.UpsampleConv(input_dim, output_dim, kernel_size=3, biases=True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inputs)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1287\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1288\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.VarianceNorm2d\",\"t\":[\"source\",\"class espnet2.enh.layers.ncsnpp_utils.normalization.VarianceNorm2d(num_features, bias=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1289\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1290\":{\"h\":\"espnet2.enh.layers.swin_transformer.WindowAttention\",\"t\":[\"source\",\"class espnet2.enh.layers.swin_transformer.WindowAttention(dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0.0, proj_drop=0.0)\",\"Bases: Module\",\"Window-based multi-head self-attention (W-MSA) with relative position bias.\",\"It supports both of shifted and non-shifted windows.\",\"Parameters:\",\"dim (int) – Number of input channels.\",\"window_size (tuple *[*int]) – The height and width of the window.\",\"num_heads (int) – Number of attention heads.\",\"qkv_bias (bool,optional) – If True, add a learnable bias to query, key, value.\",\"qk_scale (float|None,optional) – If not None, override the default qk scale\",\"attn_drop (float,optional) – Dropout ratio of attention weight.\",\"proj_drop (float,optional) – Dropout ratio of output.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr() → str\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(x, mask=None)\",\"WindowAttention Forward.\",\"Parameters:\",\"x – input features with shape of (num_windows*B, N, C)\",\"mask – (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\",\"get_relative_position_index(H, W)\"]},\"1291\":{\"h\":\"espnet2.enh.layers.beamformer.apply_beamforming_vector\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.apply_beamforming_vector(beamform_vector: Tensor | ComplexTensor, mix: Tensor | ComplexTensor) → Tensor | ComplexTensor\"]},\"1292\":{\"h\":\"espnet2.enh.diffusion.sdes.batch_broadcast\",\"t\":[\"source\",\"espnet2.enh.diffusion.sdes.batch_broadcast(a, x)\",\"Broadcasts a over all dimensions of x, except the batch dimension,\",\"which must match.\"]},\"1293\":{\"h\":\"espnet2.enh.layers.beamformer.blind_analytic_normalization\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.blind_analytic_normalization(ws, psd_noise, eps=1e-08)\",\"Blind analytic normalization (BAN) for post-filtering\",\"Parameters:\",\"ws (torch.complex64/ComplexTensor) – beamformer vector (…, F, C)\",\"psd_noise (torch.complex64/ComplexTensor) – noise PSD matrix (…, F, C, C)\",\"eps (float)\",\"Returns: normalized beamformer vector (…, F)\",\"Return type: ws_ban (torch.complex64/ComplexTensor)\"]},\"1294\":{\"h\":\"espnet2.enh.layers.complex_utils.cat\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.cat(seq: Sequence[ComplexTensor | Tensor], *args, **kwargs)\"]},\"1295\":{\"h\":\"espnet2.enh.layers.tcn.check_nonlinear\",\"t\":[\"source\",\"espnet2.enh.layers.tcn.check_nonlinear(nolinear_type)\"]},\"1296\":{\"h\":\"espnet2.enh.layers.bsrnn.choose_norm\",\"t\":[\"source\",\"espnet2.enh.layers.bsrnn.choose_norm(norm_type, channel_size, shape='BDTF')\",\"The input of normalization will be (M, C, K), where M is batch size.\",\"C is channel size and K is sequence length.\"]},\"1297\":{\"h\":\"espnet2.enh.layers.complexnn.complex_cat\",\"t\":[\"source\",\"espnet2.enh.layers.complexnn.complex_cat(inputs, axis)\"]},\"1298\":{\"h\":\"espnet2.enh.layers.complex_utils.complex_norm\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.complex_norm(c: Tensor | ComplexTensor, dim=-1, keepdim=False) → Tensor\"]},\"1299\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.contract_inner\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.layers.contract_inner(x, y)\",\"tensordot(x, y, 1).\"]},\"1300\":{\"h\":\"espnet2.enh.layers.conv_utils.conv2d_output_shape\",\"t\":[\"source\",\"espnet2.enh.layers.conv_utils.conv2d_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1)\"]},\"1301\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.conv_downsample_2d\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.conv_downsample_2d(x, w, k=None, factor=2, gain=1)\",\"Fused tf.nn.conv2d() followed by downsample_2d().\",\"Padding is performed only once at the beginning, not between the operations. The fused op is considerably more efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of arbitrary order. :param x: Input tensor of the shape [N, C, H, W] or\",\"`\",\"[N, H, W,\",\"C]`.\",\"Parameters:\",\"w – Weight tensor of the shape [filterH, filterW, inChannels, outChannels]. Grouped convolution can be performed by inChannels = x.shape[0] // numGroups.\",\"k – FIR filter of the shape [firH, firW] or [firN] (separable). The default is [1] * factor, which corresponds to average pooling.\",\"factor – Integer downsampling factor (default: 2).\",\"gain – Scaling factor for signal magnitude (default: 1.0).\",\"Returns: Tensor of the shape [N, C, H // factor, W // factor] or [N, H // factor, W // factor, C], and same datatype as x.\"]},\"1302\":{\"h\":\"espnet2.enh.layers.conv_utils.convtransp2d_output_shape\",\"t\":[\"source\",\"espnet2.enh.layers.conv_utils.convtransp2d_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1, out_pad=0)\"]},\"1303\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ddpm_conv1x1\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.layers.ddpm_conv1x1(in_planes, out_planes, stride=1, bias=True, init_scale=1.0, padding=0)\",\"1x1 convolution with DDPM initialization.\"]},\"1304\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ddpm_conv3x3\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.layers.ddpm_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1.0, padding=1)\",\"3x3 convolution with DDPM initialization.\"]},\"1305\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.default_init\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.layers.default_init(scale=1.0)\",\"The same initialization used in DDPM.\"]},\"1306\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.downsample_2d\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.downsample_2d(x, k=None, factor=2, gain=1)\",\"Downsample a batch of 2D images with the given filter.\",\"Accepts a batch of 2D images of the shape [N, C, H, W] or [N, H, W, C] and downsamples each image with the given filter. The filter is normalized so that if the input pixels are constant, they will be scaled by the specified gain. Pixels outside the image are assumed to be zero, and the filter is padded with zeros so that its shape is a multiple of the downsampling factor. :param x: Input tensor of the shape [N, C, H, W] or\",\"`\",\"[N, H, W,\",\"C]`.\",\"Parameters:\",\"k – FIR filter of the shape [firH, firW] or [firN] (separable). The default is [1] * factor, which corresponds to average pooling.\",\"factor – Integer downsampling factor (default: 2).\",\"gain – Scaling factor for signal magnitude (default: 1.0).\",\"Returns: Tensor of the shape [N, C, H // factor, W // factor]\"]},\"1307\":{\"h\":\"espnet2.enh.layers.complex_utils.einsum\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.einsum(equation, *operands)\"]},\"1308\":{\"h\":\"espnet2.enh.layers.beamformer.generalized_eigenvalue_decomposition\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.generalized_eigenvalue_decomposition(a: Tensor, b: Tensor, eps=1e-06)\",\"Solves the generalized eigenvalue decomposition through Cholesky decomposition.\",\"ported from https://github.com/asteroid-team/asteroid/blob/master/asteroid/dsp/beamforming.py#L464\",\"a @ e_vec = e_val * b @ e_vec | | Cholesky decomposition on b: | b = L @ L^H, where L is a lower triangular matrix | | Let C = L^-1 @ a @ L^-H, it is Hermitian. | => C @ y = lambda * y => e_vec = L^-H @ y\",\"Reference: https://www.netlib.org/lapack/lug/node54.html\",\"Parameters:\",\"a – A complex Hermitian or real symmetric matrix whose eigenvalues and eigenvectors will be computed. (…, C, C)\",\"b – A complex Hermitian or real symmetric definite positive matrix. (…, C, C)\",\"Returns: generalized eigenvalues (ascending order) e_vec: generalized eigenvectors\",\"Return type: e_val\"]},\"1309\":{\"h\":\"espnet2.enh.layers.beamformer.get_WPD_filter\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_WPD_filter(Phi: Tensor | ComplexTensor, Rf: Tensor | ComplexTensor, reference_vector: Tensor, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08) → Tensor | ComplexTensor\",\"Return the WPD vector.\",\"WPD is the Weighted Power minimization Distortionless response convolutional beamformer. As follows:\",\"h = (Rf^-1 @ Phi_{xx}) / tr[(Rf^-1) @ Phi_{xx}] @ u\",\"Reference: : T. Nakatani and K. Kinoshita, “A Unified Convolutional Beamformer for Simultaneous Denoising and Dereverberation,” in IEEE Signal Processing Letters, vol. 26, no. 6, pp. 903-907, June 2019, doi: 10.1109/LSP.2019.2911179. https://ieeexplore.ieee.org/document/8691481\",\"Parameters:\",\"Phi (torch.complex64/ComplexTensor) – (B, F, (btaps+1) * C, (btaps+1) * C) is the PSD of zero-padded speech [x^T(t,f) 0 … 0]^T.\",\"Rf (torch.complex64/ComplexTensor) – (B, F, (btaps+1) * C, (btaps+1) * C) is the power normalized spatio-temporal covariance matrix.\",\"reference_vector (torch.Tensor) – (B, (btaps+1) * C) is the reference_vector.\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float)\",\"eps (float)\",\"Returns: (B, F, (btaps + 1) * C)\",\"Return type: filter_matrix (torch.complex64/ComplexTensor)\"]},\"1310\":{\"h\":\"espnet2.enh.layers.beamformer.get_WPD_filter_v2\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_WPD_filter_v2(Phi: Tensor | ComplexTensor, Rf: Tensor | ComplexTensor, reference_vector: Tensor, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08) → Tensor | ComplexTensor\",\"Return the WPD vector (v2).\",\"This implementation is more efficient than get_WPD_filter as : it skips unnecessary computation with zeros.\",\"Parameters:\",\"Phi (torch.complex64/ComplexTensor) – (B, F, C, C) is speech PSD.\",\"Rf (torch.complex64/ComplexTensor) – (B, F, (btaps+1) * C, (btaps+1) * C) is the power normalized spatio-temporal covariance matrix.\",\"reference_vector (torch.Tensor) – (B, C) is the reference_vector.\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float)\",\"eps (float)\",\"Returns: (B, F, (btaps+1) * C)\",\"Return type: filter_matrix (torch.complex64/ComplexTensor)\"]},\"1311\":{\"h\":\"espnet2.enh.layers.beamformer.get_WPD_filter_with_rtf\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_WPD_filter_with_rtf(psd_observed_bar: Tensor | ComplexTensor, psd_speech: Tensor | ComplexTensor, psd_noise: Tensor | ComplexTensor, iterations: int = 3, reference_vector: int | Tensor | None = None, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-15) → Tensor | ComplexTensor\",\"Return the WPD vector calculated with RTF.\",\"WPD is the Weighted Power minimization Distortionless response convolutional beamformer. As follows:\",\"h = (Rf^-1 @ vbar) / (vbar^H @ R^-1 @ vbar)\",\"Reference: : T. Nakatani and K. Kinoshita, “A Unified Convolutional Beamformer for Simultaneous Denoising and Dereverberation,” in IEEE Signal Processing Letters, vol. 26, no. 6, pp. 903-907, June 2019, doi: 10.1109/LSP.2019.2911179. https://ieeexplore.ieee.org/document/8691481\",\"Parameters:\",\"psd_observed_bar (torch.complex64/ComplexTensor) – stacked observation covariance matrix\",\"psd_speech (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_noise (torch.complex64/ComplexTensor) – noise covariance matrix (…, F, C, C)\",\"iterations (int) – number of iterations in power method\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float)\",\"eps (float)\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)r\"]},\"1312\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.get_act\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.layers.get_act(config)\",\"Get activation functions from the config file.\"]},\"1313\":{\"h\":\"espnet2.enh.layers.dcunet.get_activation\",\"t\":[\"source\",\"espnet2.enh.layers.dcunet.get_activation(name)\"]},\"1314\":{\"h\":\"espnet2.enh.layers.wpe.get_correlations\",\"t\":[\"source\",\"espnet2.enh.layers.wpe.get_correlations(Y: Tensor | ComplexTensor, inverse_power: Tensor, taps, delay) → Tuple[Tensor | ComplexTensor, Tensor | ComplexTensor]\",\"Calculates weighted correlations of a window of length taps\",\"Parameters:\",\"Y – Complex-valued STFT signal with shape (F, C, T)\",\"inverse_power – Weighting factor with shape (F, T)\",\"taps (int) – Lenghts of correlation window\",\"delay (int) – Delay for the weighting factor\",\"Returns: Correlation matrix of shape (F, taps*C, taps*C) Correlation vector of shape (F, taps, C, C)\"]},\"1315\":{\"h\":\"espnet2.enh.layers.beamformer.get_covariances\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_covariances(Y: Tensor | ComplexTensor, inverse_power: Tensor, bdelay: int, btaps: int, get_vector: bool = False) → Tensor | ComplexTensor\",\"Calculates the power normalized spatio-temporal covariance : matrix of the framed signal.\",\"Parameters:\",\"Y – Complex STFT signal with shape (B, F, C, T)\",\"inverse_power – Weighting factor with shape (B, F, T)\",\"Returns: (B, F, (btaps+1) * C, (btaps+1) * C) Correlation vector: (B, F, btaps + 1, C, C)\",\"Return type: Correlation matrix\"]},\"1316\":{\"h\":\"espnet2.enh.layers.bsrnn.get_erb_subbands\",\"t\":[\"source\",\"espnet2.enh.layers.bsrnn.get_erb_subbands(input_dim, min_freq_idx=0, n_erbs=64, target_fs=48000)\",\"Get Equivalent Rectangular Bandwidth (ERB) division of subbands.\",\"Reference: : https://github.com/Xiaobin-Rong/gtcrn/blob/main/stream/gtcrn.py#L11-L49\",\"Parameters:\",\"input_dim (int) – number of frequency bins corresponding to target_fs Assumed to be n_fft // 2 + 1, where n_fft is the FFT size.\",\"min_freq_idx (int) – bin index of the minimum frequency to start the ERB bands. Frequency bins below this value will be kept as is. min_freq_idx / input_dim * target_fs / 2 is the minimum frequency in Hz.\",\"n_erbs (int) – number of ERB frequency bands to be used.\",\"target_fs (int) – target sampling frequency in Hz.\",\"Returns: a tuple of overlapping subbands (start and end indices) : len(subbands) = min_freq_idx + n_erbs\",\"Return type: subbands (tuple)\"]},\"1317\":{\"h\":\"espnet2.enh.layers.wpe.get_filter_matrix_conj\",\"t\":[\"source\",\"espnet2.enh.layers.wpe.get_filter_matrix_conj(correlation_matrix: Tensor | ComplexTensor, correlation_vector: Tensor | ComplexTensor, eps: float = 1e-10) → Tensor | ComplexTensor\",\"Calculate (conjugate) filter matrix based on correlations for one freq.\",\"Parameters:\",\"correlation_matrix – Correlation matrix (F, taps * C, taps * C)\",\"correlation_vector – Correlation vector (F, taps, C, C)\",\"eps\",\"Returns: (F, taps, C, C)\",\"Return type: filter_matrix_conj (torch.complex/ComplexTensor)\"]},\"1318\":{\"h\":\"espnet2.enh.layers.beamformer.get_gev_vector\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_gev_vector(psd_noise: Tensor | ComplexTensor, psd_speech: Tensor | ComplexTensor, mode='power', reference_vector: int | Tensor = 0, iterations: int = 3, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08) → Tensor | ComplexTensor\",\"Return the generalized eigenvalue (GEV) beamformer vector:\",\"psd_speech @ h = lambda * psd_noise @ h\",\"Reference: : Blind acoustic beamforming based on generalized eigenvalue decomposition; E. Warsitz and R. Haeb-Umbach, 2007.\",\"Parameters:\",\"psd_noise (torch.complex64/ComplexTensor) – noise covariance matrix (…, F, C, C)\",\"psd_speech (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"mode (str) – one of (“power”, “evd”) “power”: power method “evd”: eigenvalue decomposition (only for torch builtin complex tensors)\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"iterations (int) – number of iterations in power method\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float)\",\"eps (float)\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1319\":{\"h\":\"espnet2.enh.layers.beamformer.get_lcmv_vector_with_rtf\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_lcmv_vector_with_rtf(psd_n: Tensor | ComplexTensor, rtf_mat: Tensor | ComplexTensor, reference_vector: int | Tensor | None = None, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08) → Tensor | ComplexTensor\",\"Return the LCMV (Linearly Constrained Minimum Variance) vector : calculated with RTF: <br/> h = (Npsd^-1 @ rtf_mat) @ (rtf_mat^H @ Npsd^-1 @ rtf_mat)^-1 @ p\",\"Reference: : H. L. Van Trees, “Optimum array processing: Part IV of detection, estimation, and modulation theory,” John Wiley & Sons, 2004. (Chapter 6.7)\",\"Parameters:\",\"psd_n (torch.complex64/ComplexTensor) – observation/noise covariance matrix (…, F, C, C)\",\"rtf_mat (torch.complex64/ComplexTensor) – RTF matrix (…, F, C, num_spk)\",\"reference_vector (torch.Tensororint) – (…, num_spk) or scalar\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float)\",\"eps (float)\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1320\":{\"h\":\"espnet2.enh.layers.bsrnn.get_mel_subbands\",\"t\":[\"source\",\"espnet2.enh.layers.bsrnn.get_mel_subbands(input_dim, n_mels=40, target_fs=48000)\",\"Get mel division of subbands.\",\"Reference: : https://github.com/lucidrains/BS-RoFormer/blob/main/bs_roformer/mel_band_roformer.py#L432-L464\",\"Parameters:\",\"input_dim (int) – number of frequency bins corresponding to target_fs Assumed to be n_fft // 2 + 1, where n_fft is the FFT size.\",\"n_mels (int) – number of mel frequency bands to be used.\",\"target_fs (int) – target sampling frequency in Hz.\",\"Returns: a tuple of overlapping subbands (start and end indices)\",\"Return type: subbands (tuple)\"]},\"1321\":{\"h\":\"espnet2.enh.layers.beamformer.get_mvdr_vector\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_mvdr_vector(psd_s, psd_n, reference_vector: Tensor, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the MVDR (Minimum Variance Distortionless Response) vector:\",\"h = (Npsd^-1 @ Spsd) / (Tr(Npsd^-1 @ Spsd)) @ u\",\"Reference: : On optimal frequency-domain multichannel linear filtering for noise reduction; M. Souden et al., 2010; https://ieeexplore.ieee.org/document/5089420\",\"Parameters:\",\"psd_s (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_n (torch.complex64/ComplexTensor) – observation/noise covariance matrix (…, F, C, C)\",\"reference_vector (torch.Tensor) – (…, C)\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float)\",\"eps (float)\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1322\":{\"h\":\"espnet2.enh.layers.beamformer.get_mvdr_vector_with_rtf\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_mvdr_vector_with_rtf(psd_n: Tensor | ComplexTensor, psd_speech: Tensor | ComplexTensor, psd_noise: Tensor | ComplexTensor, iterations: int = 3, reference_vector: int | Tensor | None = None, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08) → Tensor | ComplexTensor\",\"Return the MVDR (Minimum Variance Distortionless Response) vector : calculated with RTF: <br/> h = (Npsd^-1 @ rtf) / (rtf^H @ Npsd^-1 @ rtf)\",\"Reference: : On optimal frequency-domain multichannel linear filtering for noise reduction; M. Souden et al., 2010; https://ieeexplore.ieee.org/document/5089420\",\"Parameters:\",\"psd_n (torch.complex64/ComplexTensor) – observation/noise covariance matrix (…, F, C, C)\",\"psd_speech (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_noise (torch.complex64/ComplexTensor) – noise covariance matrix (…, F, C, C)\",\"iterations (int) – number of iterations in power method\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float)\",\"eps (float)\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1323\":{\"h\":\"espnet2.enh.layers.beamformer.get_mwf_vector\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_mwf_vector(psd_s, psd_n, reference_vector: Tensor | int, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the MWF (Minimum Multi-channel Wiener Filter) vector:\",\"h = (Npsd^-1 @ Spsd) @ u\",\"Parameters:\",\"psd_s (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_n (torch.complex64/ComplexTensor) – power-normalized observation covariance matrix (…, F, C, C)\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float)\",\"eps (float)\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1324\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.normalization.get_normalization\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.normalization.get_normalization(config, conditional=False)\",\"Obtain normalization modules from the config file.\"]},\"1325\":{\"h\":\"espnet2.enh.layers.wpe.get_power\",\"t\":[\"source\",\"espnet2.enh.layers.wpe.get_power(signal, dim=-2) → Tensor\",\"Calculates power for signal\",\"Parameters:\",\"signal – Single frequency signal with shape (F, C, T).\",\"axis – reduce_mean axis\",\"Returns: Power with shape (F, T)\"]},\"1326\":{\"h\":\"espnet2.enh.layers.beamformer.get_power_spectral_density_matrix\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_power_spectral_density_matrix(xs, mask, normalization=True, reduction='mean', eps: float = 1e-15)\",\"Return cross-channel power spectral density (PSD) matrix\",\"Parameters:\",\"xs (torch.complex64/ComplexTensor) – (…, F, C, T)\",\"reduction (str) – “mean” or “median”\",\"mask (torch.Tensor) – (…, F, C, T)\",\"normalization (bool)\",\"eps (float)\",\"Returns : psd (torch.complex64/ComplexTensor): (…, F, C, C)\"]},\"1327\":{\"h\":\"espnet2.enh.layers.beamformer.get_rank1_mwf_vector\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_rank1_mwf_vector(psd_speech, psd_noise, reference_vector: Tensor | int, denoising_weight: float = 1.0, approx_low_rank_psd_speech: bool = False, iterations: int = 3, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the R1-MWF (Rank-1 Multi-channel Wiener Filter) vector\",\"h = (Npsd^-1 @ Spsd) / (mu + Tr(Npsd^-1 @ Spsd)) @ u\",\"Reference: : [1] Rank-1 constrained multichannel Wiener filter for speech recognition in noisy environments; Z. Wang et al, 2018 https://hal.inria.fr/hal-01634449/document [2] Low-rank approximation based multichannel Wiener filter algorithms for noise reduction with application in cochlear implants; R. Serizel, 2014 https://ieeexplore.ieee.org/document/6730918\",\"Parameters:\",\"psd_speech (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_noise (torch.complex64/ComplexTensor) – noise covariance matrix (…, F, C, C)\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"denoising_weight (float) – a trade-off parameter between noise reduction and speech distortion. A larger value leads to more noise reduction at the expense of more speech distortion. When denoising_weight = 0, it corresponds to MVDR beamformer.\",\"approx_low_rank_psd_speech (bool) – whether to replace original input psd_speech with its low-rank approximation as in [1]\",\"iterations (int) – number of iterations in power method, only used when approx_low_rank_psd_speech = True\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float)\",\"eps (float)\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1328\":{\"h\":\"espnet2.enh.layers.beamformer.get_rtf\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_rtf(psd_speech, psd_noise, mode='power', reference_vector: int | Tensor = 0, iterations: int = 3)\",\"Calculate the relative transfer function (RTF)\",\"Algorithm of power method: : 1. rtf = reference_vector 2. for i in range(iterations): : rtf = (psd_noise^-1 @ psd_speech) @ rtf rtf = rtf / ||rtf||_2 # this normalization can be skipped 3. rtf = psd_noise @ rtf 4. rtf = rtf / rtf[…, ref_channel, :]\",\"Note: 4) Normalization at the reference channel is not performed here.\",\"Parameters:\",\"psd_speech (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_noise (torch.complex64/ComplexTensor) – noise covariance matrix (…, F, C, C)\",\"mode (str) – one of (“power”, “evd”) “power”: power method “evd”: eigenvalue decomposition\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"iterations (int) – number of iterations in power method\",\"Returns: (…, F, C, 1)\",\"Return type: rtf (torch.complex64/ComplexTensor)\"]},\"1329\":{\"h\":\"espnet2.enh.layers.beamformer.get_rtf_matrix\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_rtf_matrix(psd_speeches, psd_noises, diagonal_loading: bool = True, ref_channel: int = 0, rtf_iterations: int = 3, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Calculate the RTF matrix with each column the relative transfer function of the corresponding source.\"]},\"1330\":{\"h\":\"espnet2.enh.layers.beamformer.get_sdw_mwf_vector\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.get_sdw_mwf_vector(psd_speech, psd_noise, reference_vector: Tensor | int, denoising_weight: float = 1.0, approx_low_rank_psd_speech: bool = False, iterations: int = 3, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08)\",\"Return the SDW-MWF (Speech Distortion Weighted Multi-channel Wiener Filter) vector\",\"h = (Spsd + mu * Npsd)^-1 @ Spsd @ u\",\"Reference: : [1] Spatially pre-processed speech distortion weighted multi-channel Wiener filtering for noise reduction; A. Spriet et al, 2004 https://dl.acm.org/doi/abs/10.1016/j.sigpro.2004.07.028 [2] Rank-1 constrained multichannel Wiener filter for speech recognition in noisy environments; Z. Wang et al, 2018 https://hal.inria.fr/hal-01634449/document [3] Low-rank approximation based multichannel Wiener filter algorithms for noise reduction with application in cochlear implants; R. Serizel, 2014 https://ieeexplore.ieee.org/document/6730918\",\"Parameters:\",\"psd_speech (torch.complex64/ComplexTensor) – speech covariance matrix (…, F, C, C)\",\"psd_noise (torch.complex64/ComplexTensor) – noise covariance matrix (…, F, C, C)\",\"reference_vector (torch.Tensororint) – (…, C) or scalar\",\"denoising_weight (float) – a trade-off parameter between noise reduction and speech distortion. A larger value leads to more noise reduction at the expense of more speech distortion. The plain MWF is obtained with denoising_weight = 1 (by default).\",\"approx_low_rank_psd_speech (bool) – whether to replace original input psd_speech with its low-rank approximation as in [2]\",\"iterations (int) – number of iterations in power method, only used when approx_low_rank_psd_speech = True\",\"diagonal_loading (bool) – Whether to add a tiny term to the diagonal of psd_n\",\"diag_eps (float)\",\"eps (float)\",\"Returns: (…, F, C)\",\"Return type: beamform_vector (torch.complex64/ComplexTensor)\"]},\"1331\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.get_weight\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.get_weight(module, shape, weight_var='weight', kernel_init=None)\",\"Get/create weight tensor for a convolution or fully-connected layer.\"]},\"1332\":{\"h\":\"espnet2.enh.layers.beamformer.gev_phase_correction\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.gev_phase_correction(vector)\",\"Phase correction to reduce distortions due to phase inconsistencies.\",\"ported from https://github.com/fgnt/nn-gev/blob/master/fgnt/beamforming.py#L169\",\"Parameters:vector – Beamforming vector with shape (…, F, C)\",\"Returns: Phase corrected beamforming vectors\",\"Return type: w\"]},\"1333\":{\"h\":\"espnet2.enh.layers.ifasnet.iFaSNet\",\"t\":[\"source\",\"class espnet2.enh.layers.ifasnet.iFaSNet(*args, **kwargs)\",\"Bases: FaSNet_base\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input, num_mic)\",\"abstract forward function\",\"input: shape (batch, max_num_ch, T) num_mic: shape (batch, ), the number of channels for each input.\",\"Zero for fixed geometry configuration.\"]},\"1334\":{\"h\":\"espnet2.enh.separator.ineube_separator.iNeuBe\",\"t\":[\"source\",\"class espnet2.enh.separator.ineube_separator.iNeuBe(n_spk=1, n_fft=512, stride=128, window='hann', mic_channels=1, hid_chans=32, hid_chans_dense=32, ksz_dense=(3, 3), ksz_tcn=3, tcn_repeats=4, tcn_blocks=7, tcn_channels=384, activation='elu', output_from='dnn1', n_chunks=3, freeze_dnn1=False, tik_eps=1e-08)\",\"Bases: AbsSeparator\",\"iNeuBe, iterative neural/beamforming enhancement\",\"Reference: Lu, Y. J., Cornell, S., Chang, X., Zhang, W., Li, C., Ni, Z., … & Watanabe, S. Towards Low-Distortion Multi-Channel Speech Enhancement: The ESPNET-Se Submission to the L3DAS22 Challenge. ICASSP 2022 p. 9201-9205.\",\"NOTES: As outlined in the Reference, this model works best when coupled with the MultiResL1SpecLoss defined in criterions/time_domain.py. The model is trained with variance normalized mixture input and target. e.g. with mixture of shape [batch, microphones, samples] you normalize it by dividing with torch.std(mixture, (1, 2)). You must do the same for the target signal. In the Reference, the variance normalization was performed offline (we normalized by the std computed on the entire training set and not for each input separately). However we found out that also normalizing each input and target separately works well.\",\"Parameters:\",\"n_spk – number of output sources/speakers.\",\"n_fft – stft window size.\",\"stride – stft stride.\",\"window – stft window type choose between ‘hamming’, ‘hanning’ or None.\",\"mic_channels – number of microphones channels (only fixed-array geometry supported).\",\"hid_chans – number of channels in the subsampling/upsampling conv layers.\",\"hid_chans_dense – number of channels in the densenet layers (reduce this to reduce VRAM requirements).\",\"ksz_dense – kernel size in the densenet layers thorough iNeuBe.\",\"ksz_tcn – kernel size in the TCN submodule.\",\"tcn_repeats – number of repetitions of blocks in the TCN submodule.\",\"tcn_blocks – number of blocks in the TCN submodule.\",\"tcn_channels – number of channels in the TCN submodule.\",\"activation – activation function to use in the whole iNeuBe model, you can use any torch supported activation e.g. ‘relu’ or ‘elu’.\",\"output_from – output the estimate from ‘dnn1’, ‘mfmcwf’ or ‘dnn2’.\",\"n_chunks – number of future and past frames to consider for mfMCWF computation.\",\"freeze_dnn1 – whether or not freezing dnn1 parameters during training of dnn2.\",\"tik_eps – diagonal loading in the mfMCWF computation.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor | ComplexTensor, ilens: Tensor, additional: Dict | None = None) → Tuple[List[Tensor | ComplexTensor], Tensor, OrderedDict]\",\"Forward.\",\"Parameters:\",\"input (torch.Tensor/ComplexTensor) – batched multi-channel audio tensor with C audio channels and T samples [B, T, C]\",\"ilens (torch.Tensor) – input lengths [Batch]\",\"additional (DictorNone) – other data, currently unused in this model.\",\"Returns: [(B, T), …] list of len n_spk : of mono audio tensors with T samples.\",\"ilens (torch.Tensor): (B,) additional (Dict or None): other data, currently unused in this model,\",\"we return it also in output.\",\"Return type: enhanced (List[Union[torch.Tensor, ComplexTensor]])\",\"static mfmcwf(mixture, estimate, n_chunks, tik_eps)\",\"multi-frame multi-channel wiener filter.\",\"Parameters:\",\"mixture (torch.Tensor) – multi-channel STFT complex mixture tensor, of shape [B, T, C, F] batch, frames, microphones, frequencies.\",\"estimate (torch.Tensor) – monaural STFT complex estimate of target source [B, T, F] batch, frames, frequencies.\",\"n_chunks (int) – number of past and future mfMCWF frames. If 0 then standard MCWF.\",\"tik_eps (float) – diagonal loading for matrix inversion in MCWF computation.\",\"Returns: monaural STFT complex estimate : of target source after MFMCWF [B, T, F] batch, frames, frequencies.\",\"Return type: beamformed (torch.Tensor)\",\"property num_spk\",\"static pad2(input_tensor, target_len)\",\"static unfold(tf_rep, chunk_size)\",\"unfolding STFT representation to add context in the mics channel.\",\"Parameters:\",\"mixture (torch.Tensor) – 3D tensor (monaural complex STFT) of shape [B, T, F] batch, frames, microphones, frequencies.\",\"n_chunks (int) – number of past and future to consider.\",\"Returns: complex 3D tensor STFT with context channel. : shape now is [B, T, C, F] batch, frames, context, frequencies. Basically same shape as a multi-channel STFT with C microphones.\",\"Return type: est_unfolded (torch.Tensor)\"]},\"1335\":{\"h\":\"espnet2.enh.layers.adapt_layers.into_orig_type\",\"t\":[\"source\",\"espnet2.enh.layers.adapt_layers.into_orig_type(x, orig_type)\",\"Inverts into_tuple function.\"]},\"1336\":{\"h\":\"espnet2.enh.layers.adapt_layers.into_tuple\",\"t\":[\"source\",\"espnet2.enh.layers.adapt_layers.into_tuple(x)\",\"Transforms tensor/list/tuple into tuple.\"]},\"1337\":{\"h\":\"espnet2.enh.layers.complex_utils.inverse\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.inverse(c: Tensor | ComplexTensor) → Tensor | ComplexTensor\"]},\"1338\":{\"h\":\"espnet2.enh.layers.complex_utils.is_complex\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.is_complex(c)\"]},\"1339\":{\"h\":\"espnet2.enh.layers.complex_utils.is_torch_complex_tensor\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.is_torch_complex_tensor(c)\"]},\"1340\":{\"h\":\"espnet2.enh.layers.adapt_layers.make_adapt_layer\",\"t\":[\"source\",\"espnet2.enh.layers.adapt_layers.make_adapt_layer(type, indim, enrolldim, ninputs=1)\"]},\"1341\":{\"h\":\"espnet2.enh.layers.dcunet.make_unet_encoder_decoder_args\",\"t\":[\"source\",\"espnet2.enh.layers.dcunet.make_unet_encoder_decoder_args(encoder_args, decoder_args)\"]},\"1342\":{\"h\":\"espnet2.enh.layers.complex_utils.matmul\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.matmul(a: Tensor | ComplexTensor, b: Tensor | ComplexTensor) → Tensor | ComplexTensor\"]},\"1343\":{\"h\":\"espnet2.enh.layers.dprnn.merge_feature\",\"t\":[\"source\",\"espnet2.enh.layers.dprnn.merge_feature(input, rest)\"]},\"1344\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.naive_downsample_2d\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.naive_downsample_2d(x, factor=2)\"]},\"1345\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.naive_upsample_2d\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.naive_upsample_2d(x, factor=2)\"]},\"1346\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ncsn_conv1x1\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.layers.ncsn_conv1x1(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1.0, padding=0)\",\"1x1 convolution. Same as NCSNv1/v2.\"]},\"1347\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.ncsn_conv3x3\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.layers.ncsn_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1.0, padding=1)\",\"3x3 convolution with PyTorch initialization. Same as NCSNv1/NCSNv2.\"]},\"1348\":{\"h\":\"espnet2.enh.layers.complex_utils.new_complex_like\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.new_complex_like(ref: Tensor | ComplexTensor, real_imag: Tuple[Tensor, Tensor])\"]},\"1349\":{\"h\":\"espnet2.enh.layers.conv_utils.num2tuple\",\"t\":[\"source\",\"espnet2.enh.layers.conv_utils.num2tuple(num)\"]},\"1350\":{\"h\":\"espnet2.enh.separator.svoice_separator.overlap_and_add\",\"t\":[\"source\",\"espnet2.enh.separator.svoice_separator.overlap_and_add(signal, frame_step)\",\"Reconstructs a signal from a framed representation.\",\"Adds potentially overlapping frames of a signal with shape […, frames, frame_length], offsetting subsequent frames by frame_step. The resulting tensor has shape […, output_size] where\",\"output_size = (frames - 1) * frame_step + frame_length\",\"Args: : signal: A […, frames, frame_length] Tensor. All dimensions may be unknown, : and rank must be at least 2. <br/> frame_step: An integer denoting overlap offsets. : Must be less than or equal to frame_length.\",\"Returns: : A Tensor with shape […, output_size] containing the : overlap-added frames of signal’s inner-most two dimensions. <br/> output_size = (frames - 1) * frame_step + frame_length\",\"Based on\",\"https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/signal/python/ops/reconstruction_ops.py\"]},\"1351\":{\"h\":\"espnet2.enh.layers.beamformer.perform_WPD_filtering\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.perform_WPD_filtering(filter_matrix: Tensor | ComplexTensor, Y: Tensor | ComplexTensor, bdelay: int, btaps: int) → Tensor | ComplexTensor\",\"Perform WPD filtering.\",\"Parameters:\",\"filter_matrix – Filter matrix (B, F, (btaps + 1) * C)\",\"Y – Complex STFT signal with shape (B, F, C, T)\",\"Returns: (B, F, T)\",\"Return type: enhanced (torch.complex64/ComplexTensor)\"]},\"1352\":{\"h\":\"espnet2.enh.layers.wpe.perform_filter_operation\",\"t\":[\"source\",\"espnet2.enh.layers.wpe.perform_filter_operation(Y: Tensor | ComplexTensor, filter_matrix_conj: Tensor | ComplexTensor, taps, delay) → Tensor | ComplexTensor\",\"Parameters:\",\"Y – Complex-valued STFT signal of shape (F, C, T)\",\"Matrix (filter)\"]},\"1353\":{\"h\":\"espnet2.enh.layers.dnsmos.poly1d\",\"t\":[\"source\",\"espnet2.enh.layers.dnsmos.poly1d(coefficients, use_numpy=False)\"]},\"1354\":{\"h\":\"espnet2.enh.layers.beamformer.prepare_beamformer_stats\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.prepare_beamformer_stats(signal, masks_speech, mask_noise, powers=None, beamformer_type='mvdr', bdelay=3, btaps=5, eps=1e-06)\",\"Prepare necessary statistics for constructing the specified beamformer.\",\"Parameters:\",\"signal (torch.complex64/ComplexTensor) – (…, F, C, T)\",\"masks_speech (List *[*torch.Tensor]) – (…, F, C, T) masks for all speech sources\",\"mask_noise (torch.Tensor) – (…, F, C, T) noise mask\",\"powers (List *[*torch.Tensor]) – powers for all speech sources (…, F, T) used for wMPDR or WPD beamformers\",\"beamformer_type (str) – one of the pre-defined beamformer types\",\"bdelay (int) – delay factor, used for WPD beamformser\",\"btaps (int) – number of filter taps, used for WPD beamformser\",\"eps (torch.Tensor) – tiny constant\",\"Returns: a dictionary containing all necessary statistics : e.g. “psd_n”, “psd_speech”, “psd_distortion” Note: * When masks_speech is a tensor or a single-element list, all returned \",\"statistics are tensors;\",\"When masks_speech is a multi-element list, some returned statistics can be a list, e.g., “psd_n” for MVDR, “psd_speech” and “psd_distortion”.\",\"Return type: beamformer_stats (dict)\"]},\"1355\":{\"h\":\"espnet2.enh.layers.complex_utils.reverse\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.reverse(a: Tensor | ComplexTensor, dim=0)\"]},\"1356\":{\"h\":\"espnet2.enh.layers.beamformer.signal_framing\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.signal_framing(signal: Tensor | ComplexTensor, frame_length: int, frame_step: int, bdelay: int, do_padding: bool = False, pad_value: int = 0, indices: List | None = None) → Tensor | ComplexTensor\",\"Expand signal into several frames, with each frame of length frame_length.\",\"Parameters:\",\"signal – (…, T)\",\"frame_length – length of each segment\",\"frame_step – step for selecting frames\",\"bdelay – delay for WPD\",\"do_padding – whether or not to pad the input signal at the beginning of the time dimension\",\"pad_value – value to fill in the padding\",\"Returns: if do_padding: (…, T, frame_length) else: (…, T - bdelay - frame_length + 2, frame_length)\",\"Return type: torch.Tensor\"]},\"1357\":{\"h\":\"espnet2.enh.layers.complex_utils.solve\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.solve(b: Tensor | ComplexTensor, a: Tensor | ComplexTensor)\",\"Solve the linear equation ax = b.\"]},\"1358\":{\"h\":\"espnet2.enh.layers.dprnn.split_feature\",\"t\":[\"source\",\"espnet2.enh.layers.dprnn.split_feature(input, segment_size)\"]},\"1359\":{\"h\":\"espnet2.enh.layers.complex_utils.stack\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.stack(seq: Sequence[ComplexTensor | Tensor], *args, **kwargs)\"]},\"1360\":{\"h\":\"espnet2.enh.layers.ifasnet.test_model\",\"t\":[\"source\",\"espnet2.enh.layers.ifasnet.test_model(model)\"]},\"1361\":{\"h\":\"espnet2.enh.layers.beamformer.tik_reg\",\"t\":[\"source\",\"espnet2.enh.layers.beamformer.tik_reg(mat, reg: float = 1e-08, eps: float = 1e-08)\",\"Perform Tikhonov regularization (only modifying real part).\",\"Parameters:\",\"mat (torch.complex64/ComplexTensor) – input matrix (…, C, C)\",\"reg (float) – regularization factor\",\"eps (float)\",\"Returns: regularized matrix (…, C, C)\",\"Return type: ret (torch.complex64/ComplexTensor)\"]},\"1362\":{\"h\":\"espnet2.enh.layers.swin_transformer.to_2tuple\",\"t\":[\"source\",\"espnet2.enh.layers.swin_transformer.to_2tuple(x)\"]},\"1363\":{\"h\":\"espnet2.enh.layers.complex_utils.to_complex\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.to_complex(c)\"]},\"1364\":{\"h\":\"espnet2.enh.layers.complex_utils.to_double\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.to_double(c)\"]},\"1365\":{\"h\":\"espnet2.enh.layers.complex_utils.to_float\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.to_float(c)\"]},\"1366\":{\"h\":\"espnet2.enh.layers.dcunet.torch_complex_from_reim\",\"t\":[\"source\",\"espnet2.enh.layers.dcunet.torch_complex_from_reim(re, im)\"]},\"1367\":{\"h\":\"espnet2.enh.layers.complex_utils.trace\",\"t\":[\"source\",\"espnet2.enh.layers.complex_utils.trace(a: Tensor | ComplexTensor)\"]},\"1368\":{\"h\":\"espnet2.enh.layers.dcunet.unet_decoder_args\",\"t\":[\"source\",\"espnet2.enh.layers.dcunet.unet_decoder_args(encoders, *, skip_connections)\",\"Get list of decoder arguments for upsampling (right) side of a symmetric u-net,\",\"given the arguments used to construct the encoder. :param encoders (tuple of length N of tuples of: (in_chan, out_chan, kernel_size, stride, padding)):\",\"List of arguments used to construct the encoders\",\"Parameters:skip_connections (bool) – Whether to include skip connections in the calculation of decoder input channels.\",\"Returns: tuple of length N of tuples of : (in_chan, out_chan, kernel_size, stride, padding): Arguments to be used to construct decoders\"]},\"1369\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.upfirdn2d.upfirdn2d\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.upfirdn2d.upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0))\"]},\"1370\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.upfirdn2d.upfirdn2d_native\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.upfirdn2d.upfirdn2d_native(input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1)\"]},\"1371\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.upsample_2d\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.upsample_2d(x, k=None, factor=2, gain=1)\",\"Upsample a batch of 2D images with the given filter.\",\"Accepts a batch of 2D images of the shape [N, C, H, W] or [N, H, W, C] and upsamples each image with the given filter. The filter is normalized so that if the input pixels are constant, they will be scaled by the specified gain. Pixels outside the image are assumed to be zero, and the filter is padded with zeros so that its shape is a multiple of the upsampling factor. :param x: Input tensor of the shape [N, C, H, W] or\",\"`\",\"[N, H, W,\",\"C]`.\",\"Parameters:\",\"k – FIR filter of the shape [firH, firW] or [firN] (separable). The default is [1] * factor, which corresponds to nearest-neighbor upsampling.\",\"factor – Integer upsampling factor (default: 2).\",\"gain – Scaling factor for signal magnitude (default: 1.0).\",\"Returns: Tensor of the shape [N, C, H * factor, W * factor]\"]},\"1372\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.upsample_conv_2d\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.up_or_down_sampling.upsample_conv_2d(x, w, k=None, factor=2, gain=1)\",\"Fused upsample_2d() followed by tf.nn.conv2d().\",\"Padding is performed only once at the beginning, not between the operations. The fused op is considerably more efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of arbitrary order. :param x: Input tensor of the shape [N, C, H, W] or\",\"`\",\"[N, H, W,\",\"C]`.\",\"Parameters:\",\"w – Weight tensor of the shape [filterH, filterW, inChannels, outChannels]. Grouped convolution can be performed by inChannels = x.shape[0] // numGroups.\",\"k – FIR filter of the shape [firH, firW] or [firN] (separable). The default is [1] * factor, which corresponds to nearest-neighbor upsampling.\",\"factor – Integer upsampling factor (default: 2).\",\"gain – Scaling factor for signal magnitude (default: 1.0).\",\"Returns: Tensor of the shape [N, C, H * factor, W * factor] or [N, H * factor, W * factor, C], and same datatype as x.\"]},\"1373\":{\"h\":\"espnet2.enh.layers.ncsnpp_utils.layers.variance_scaling\",\"t\":[\"source\",\"espnet2.enh.layers.ncsnpp_utils.layers.variance_scaling(scale, mode, distribution, in_axis=1, out_axis=0, dtype=torch.float32, device='cpu')\",\"Ported from JAX.\"]},\"1374\":{\"h\":\"espnet2.enh.layers.swin_transformer.window_partition\",\"t\":[\"source\",\"espnet2.enh.layers.swin_transformer.window_partition(x, window_size)\",\"Partition the input 2D feature into non-overlapping windows.\",\"Parameters:\",\"x – input 2D feature of shape H x W (B, H, W, C)\",\"window_size (Tuple *[*int,int]) – window size\",\"Returns: (num_windows*B, window_size, window_size, C)\",\"Return type: windows\"]},\"1375\":{\"h\":\"espnet2.enh.layers.swin_transformer.window_reverse\",\"t\":[\"source\",\"espnet2.enh.layers.swin_transformer.window_reverse(windows, window_size, H, W)\",\"Restore the windowed feature into its original shape.\",\"Parameters:\",\"windows – (num_windows*B, window_size, window_size, C)\",\"window_size (Tuple *[*int,int]) – Window size\",\"H (int) – Height of the 2D feature\",\"W (int) – Width of the 2D feature\",\"Returns: (B, H, W, C)\",\"Return type: x\"]},\"1376\":{\"h\":\"espnet2.enh.layers.wpe.wpe\",\"t\":[\"source\",\"espnet2.enh.layers.wpe.wpe(Y: Tensor | ComplexTensor, taps=10, delay=3, iterations=3) → Tensor | ComplexTensor\",\"WPE\",\"Parameters:\",\"Y – Complex valued STFT signal with shape (F, C, T)\",\"taps – Number of filter taps\",\"delay – Delay as a guard interval, such that X does not become zero.\",\"iterations\",\"Returns: (F, C, T)\",\"Return type: enhanced\"]},\"1377\":{\"h\":\"espnet2.enh.layers.wpe.wpe_one_iteration\",\"t\":[\"source\",\"espnet2.enh.layers.wpe.wpe_one_iteration(Y: Tensor | ComplexTensor, power: Tensor, taps: int = 10, delay: int = 3, eps: float = 1e-10, inverse_power: bool = True) → Tensor | ComplexTensor\",\"WPE for one iteration\",\"Parameters:\",\"Y – Complex valued STFT signal with shape (…, C, T)\",\"power – : (…, T)\",\"taps – Number of filter taps\",\"delay – Delay as a guard interval, such that X does not become zero.\",\"eps\",\"inverse_power (bool)\",\"Returns: (…, C, T)\",\"Return type: enhanced\"]},\"1378\":{\"h\":\"espnet2.fst.lm_rescore.compute_am_scores_and_lm_scores\",\"t\":[\"source\"]},\"1379\":{\"h\":\"espnet2.fst.lm_rescore.nbest_am_lm_scores\",\"t\":[\"source\"]},\"1380\":{\"h\":\"espnet2.fst.lm_rescore.remove_repeated_and_leq\",\"t\":[\"source\"]},\"1381\":{\"h\":\"espnet2.gan_codec.abs_gan_codec.AbsGANCodec\",\"t\":[\"source\",\"class espnet2.gan_codec.abs_gan_codec.AbsGANCodec(*args, **kwargs)\",\"Bases: ABC, Module\",\"GAN-based Neural Codec model abstract class.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract decode(*args, **kwargs) → Tensor\",\"Return decoded waveform from codecs.\",\"abstract encode(*args, **kwargs) → Tensor\",\"Return encoded codecs from waveform.\",\"abstract forward(forward_generator, *args, **kwargs) → Dict[str, Tensor | Dict[str, Tensor] | int]\",\"Return generator or discriminator loss.\",\"abstract meta_info() → Dict[str, Any]\",\"Return meta information of the codec.\"]},\"1382\":{\"h\":\"espnet2.gan_codec.shared.loss.loss_balancer.Balancer\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.loss.loss_balancer.Balancer(total_norm: float = 1.0, ema_decay: float = 0.999, per_batch_item: bool = True, epsilon: float = 1e-12)\",\"Bases: object\",\"property metrics\"]},\"1383\":{\"h\":\"espnet2.gan_codec.shared.discriminator.stft_discriminator.ComplexConv2d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.discriminator.stft_discriminator.ComplexConv2d(dim, dim_out, kernel_size, stride=1, padding=0)\",\"Bases: Module\",\"ComplexConv2d module.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1384\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1385\":{\"h\":\"espnet2.gan_codec.shared.discriminator.stft_discriminator.ComplexSTFTDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.discriminator.stft_discriminator.ComplexSTFTDiscriminator(*, in_channels: int = 1, channels: int = 32, strides: Any = [[1, 2], [2, 2], [1, 2], [2, 2], [1, 2], [2, 2]], chan_mults: List[int] = [1, 2, 4, 4, 8, 8], n_fft: int = 1024, hop_length: int = 256, win_length: int = 1024, stft_normalized: bool = False, logits_abs: bool = True)\",\"Bases: Module\",\"ComplexSTFT Discriminator used in SoundStream.\",\"Initialize Complex STFT Discriminator used in SoundStream.\",\"Adapted from https://github.com/alibaba-damo-academy/FunCodec.git\",\"Parameters:\",\"in_channels (int) – Input channel.\",\"channels (int) – Output channel.\",\"strides (List *[*List *(*int,int)]) – detailed strides in conv2d modules.\",\"chan_mults (List *[*int]) – Channel multiplers.\",\"n_fft (int) – n_fft in the STFT.\",\"hop_length (int) – hop_length in the STFT.\",\"stft_normalized (bool) – whether to normalize the stft output.\",\"logits_abs (bool) – whether to use the absolute number of output logits.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input signal (B, 1, T).\",\"Returns: List of list of the discriminator output.\",\"Return type: List[List[Tensor]]\",\"Reference: : Paper: https://arxiv.org/pdf/2107.03312.pdf Implementation: https://github.com/alibaba-damo-academy/FunCodec.git\"]},\"1386\":{\"h\":\"espnet2.gan_codec.shared.discriminator.stft_discriminator.ComplexSTFTResidualUnit\",\"t\":[\"source\",\"espnet2.gan_codec.shared.discriminator.stft_discriminator.ComplexSTFTResidualUnit(in_channel, out_channel, strides)\",\"Complex STFT Residual block.\",\"Parameters:\",\"in_channel (int) – Input channel.\",\"out_channel (int) – Output channel.\",\"strides (int) – Strides of the whole module.\",\"Returns: Output nn module with complex conv2ds.\",\"Return type: nn.Module\"]},\"1387\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet.ConvLayerNorm\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.encoder.seanet.ConvLayerNorm(normalized_shape: int | List[int] | Size, **kwargs)\",\"Bases: LayerNorm\",\"Convolution-friendly LayerNorm that moves channels to last dimensions\",\"before running the normalization and moves them back to original position right after.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1388\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1389\":{\"h\":\"espnet2.gan_codec.dac.dac.DAC\",\"t\":[\"source\",\"class espnet2.gan_codec.dac.dac.DAC(sampling_rate: int = 24000, generator_params: Dict[str, Any] = {'decoder_final_activation': None, 'decoder_final_activation_params': None, 'decoder_trim_right_ratio': 1.0, 'encdec_activation': 'Snake', 'encdec_activation_params': {}, 'encdec_causal': False, 'encdec_channels': 1, 'encdec_compress': 2, 'encdec_dilation_base': 2, 'encdec_kernel_size': 7, 'encdec_last_kernel_size': 7, 'encdec_lstm': 2, 'encdec_n_filters': 32, 'encdec_n_residual_layers': 1, 'encdec_norm': 'weight_norm', 'encdec_norm_params': {}, 'encdec_pad_mode': 'reflect', 'encdec_ratios': [8, 5, 4, 2], 'encdec_residual_kernel_size': 7, 'encdec_true_skip': False, 'hidden_dim': 128, 'quantizer_bins': 1024, 'quantizer_decay': 0.99, 'quantizer_dropout': True, 'quantizer_kmeans_init': True, 'quantizer_kmeans_iters': 50, 'quantizer_n_q': 8, 'quantizer_target_bandwidth': [7.5, 15], 'quantizer_threshold_ema_dead_code': 2}, discriminator_params: Dict[str, Any] = {'msmpmb_discriminator_params': {'band_discriminator_params': {'bands': [(0.0, 0.1), (0.1, 0.25), (0.25, 0.5), (0.5, 0.75), (0.75, 1.0)], 'channel': 32, 'hop_factor': 0.25, 'sample_rate': 24000}, 'fft_sizes': [2048, 1024, 512], 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'rates': [], 'sample_rate': 24000}, 'scale_follow_official_norm': False}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, use_feat_match_loss: bool = True, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, use_mel_loss: bool = True, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 24000, 'log_base': None, 'n_mels': 80, 'range_end': 11, 'range_start': 6, 'window': 'hann'}, use_dual_decoder: bool = True, lambda_quantization: float = 1.0, lambda_reconstruct: float = 1.0, lambda_commit: float = 1.0, lambda_adv: float = 1.0, lambda_feat_match: float = 2.0, lambda_mel: float = 45.0, cache_generator_outputs: bool = False)\",\"Bases: AbsGANCodec\",\"DAC model.\",\"Intialize DAC model.\",\"Parameters:TODO (jiatong)\",\"decode(x: Tensor, **kwargs) → Tensor\",\"Run encoding.\",\"Parameters:x (Tensor) – Input codes (T_code, N_stream).\",\"Returns: Generated waveform (T_wav,).\",\"Return type: Tensor\",\"encode(x: Tensor, **kwargs) → Tensor\",\"Run encoding.\",\"Parameters:x (Tensor) – Input audio (T_wav,).\",\"Returns: Generated codes (T_code, N_stream).\",\"Return type: Tensor\",\"forward(audio: Tensor, forward_generator: bool = True, **kwargs) → Dict[str, Any]\",\"Perform generator forward.\",\"Parameters:\",\"audio (Tensor) – Audio waveform tensor (B, T_wav).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(x: Tensor, **kwargs) → Dict[str, Tensor]\",\"Run inference.\",\"Parameters:x (Tensor) – Input audio (T_wav,).\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"codec (Tensor): Generated neural codec (T_code, N_stream).\",\"Return type: Dict[str, Tensor]\",\"meta_info() → Dict[str, Any]\",\"Return meta information of the codec.\"]},\"1390\":{\"h\":\"espnet2.gan_codec.dac.dac.DACDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_codec.dac.dac.DACDiscriminator(msmpmb_discriminator_params: Dict[str, Any] = {'band_discriminator_params': {'bands': [(0.0, 0.1), (0.1, 0.25), (0.25, 0.5), (0.5, 0.75), (0.75, 1.0)], 'channel': 32, 'hop_factor': 0.25, 'sample_rate': 24000}, 'fft_sizes': [2048, 1024, 512], 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'rates': [], 'sample_rate': 24000}, scale_follow_official_norm: bool = False)\",\"Bases: Module\",\"DAC discriminator module.\",\"Initialize DAC Discriminator module.\",\"Args:\",\"forward(x: Tensor) → List[List[Tensor]]\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, : which consists of each layer output tensors. Multi scale and multi period ones are concatenated.\",\"Return type: List[List[Tensor]]\"]},\"1391\":{\"h\":\"espnet2.gan_codec.dac.dac.DACGenerator\",\"t\":[\"source\",\"class espnet2.gan_codec.dac.dac.DACGenerator(sample_rate: int = 24000, hidden_dim: int = 128, codebook_dim: int = 8, encdec_channels: int = 1, encdec_n_filters: int = 32, encdec_n_residual_layers: int = 1, encdec_ratios: List[int] = [8, 5, 4, 2], encdec_activation: str = 'Snake', encdec_activation_params: Dict[str, Any] = {}, encdec_norm: str = 'weight_norm', encdec_norm_params: Dict[str, Any] = {}, encdec_kernel_size: int = 7, encdec_residual_kernel_size: int = 7, encdec_last_kernel_size: int = 7, encdec_dilation_base: int = 2, encdec_causal: bool = False, encdec_pad_mode: str = 'reflect', encdec_true_skip: bool = False, encdec_compress: int = 2, encdec_lstm: int = 2, decoder_trim_right_ratio: float = 1.0, decoder_final_activation: str | None = None, decoder_final_activation_params: dict | None = None, quantizer_n_q: int = 8, quantizer_bins: int = 1024, quantizer_decay: float = 0.99, quantizer_kmeans_init: bool = True, quantizer_kmeans_iters: int = 50, quantizer_threshold_ema_dead_code: int = 2, quantizer_target_bandwidth: List[float] = [7.5, 15], quantizer_dropout: bool = True)\",\"Bases: Module\",\"DAC generator module.\",\"Initialize DAC Generator.\",\"Parameters:TODO (jiatong)\",\"decode(codes: Tensor)\",\"DAC codec decoding.\",\"Parameters:codecs (torch.Tensor) – neural codecs in shape ().\",\"Returns: resynthesized audio.\",\"Return type: torch.Tensor\",\"encode(x: Tensor, target_bw: float | None = None)\",\"DAC codec encoding.\",\"Parameters:x (torch.Tensor) – Input tensor of shape (B, 1, T).\",\"Returns: neural codecs in shape ().\",\"Return type: torch.Tensor\",\"forward(x: Tensor, use_dual_decoder: bool = False)\",\"DAC forward propagation.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor of shape (B, 1, T).\",\"use_dual_decoder (bool) – Whether to use dual decoder for encoder out\",\"Returns: resynthesized audio. torch.Tensor: commitment loss. torch.Tensor: quantization loss torch.Tensor: resynthesized audio from encoder.\",\"Return type: torch.Tensor\"]},\"1392\":{\"h\":\"espnet2.gan_codec.shared.discriminator.msstft_discriminator.DiscriminatorSTFT\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.discriminator.msstft_discriminator.DiscriminatorSTFT(filters: int, in_channels: int = 1, out_channels: int = 1, n_fft: int = 1024, hop_length: int = 256, win_length: int = 1024, max_filters: int = 1024, filters_scale: int = 1, kernel_size: Tuple[int, int] = (3, 9), dilations: List = [1, 2, 4], stride: Tuple[int, int] = (1, 2), normalized: bool = True, norm: str = 'weight_norm', activation: str = 'LeakyReLU', activation_params: dict = {'negative_slope': 0.2})\",\"Bases: Module\",\"STFT sub-discriminator.\",\"Parameters:\",\"filters (int) – Number of filters in convolutions.\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"n_fft (int) – Size of FFT for each scale.\",\"hop_length (int) – Length of hop between STFT windows for each scale.\",\"kernel_size (tupleofint) – Inner Conv2d kernel sizes.\",\"stride (tupleofint) – Inner Conv2d strides.\",\"dilations (listofint) – Inner Conv2d dilation on the time dimension.\",\"win_length (int) – Window size for each scale.\",\"normalized (bool) – Whether to normalize by magnitude after stft.\",\"norm (str) – Normalization method.\",\"activation (str) – Activation function.\",\"activation_params (dict) – Parameters to provide to the activation function.\",\"growth (int) – Growth factor for the filters.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1393\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1394\":{\"h\":\"espnet2.gan_codec.shared.loss.loss_balancer.EMA\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.loss.loss_balancer.EMA(ema_decay)\",\"Bases: object\"]},\"1395\":{\"h\":\"espnet2.gan_codec.espnet_model.ESPnetGANCodecModel\",\"t\":[\"source\",\"class espnet2.gan_codec.espnet_model.ESPnetGANCodecModel(codec: AbsGANCodec)\",\"Bases: AbsGANESPnetModel\",\"ESPnet model for GAN-based neural codec task.\",\"Initialize ESPnetGANCodecModel module.\",\"collect_feats(audio: Tensor, **kwargs) → Dict[str, Tensor]\",\"Calculate features and return them as a dict.\",\"Parameters:audio (Tensor) – Audio waveform tensor (B, T_wav).\",\"Returns: Dict of features.\",\"Return type: Dict[str, Tensor]\",\"decode(codes: Tensor)\",\"Codec Decoding Process.\",\"Parameters:codes (Tensor) – codec tokens [N_stream, B, T]\",\"Returns: Generated waveform (B, 1, n_sample)\",\"Return type: Tensor\",\"decode_continuous(z: Tensor)\",\"Codec Decoding Process without dequntization.\",\"Parameters:z (Tensor) – continuous codec representation (B, D, T)\",\"Returns: Generated waveform (B, 1, n_sample)\",\"Return type: Tensor\",\"encode(audio: Tensor, **kwargs)\",\"Codec Encoding Process.\",\"Parameters:audio (Tensor) – Audio waveform tensor (B, 1, T_wav) or (B, T_wav) or (T_wav)\",\"Returns: Generated codecs (N_stream, B, T)\",\"Return type: Tensor\",\"encode_continuous(audio)\",\"Codec Encoding Process without quantization.\",\"Parameters:audio (Tensor) – Audio waveform tensor: (B, 1, T_wav) or (B, T_wav) or (T_wav)\",\"Returns: Generated codes (B, D, T)\",\"Return type: Tensor\",\"forward(audio: Tensor, forward_generator: bool = True, **kwargs) → Dict[str, Any]\",\"Return generator or discriminator loss with dict format.\",\"Parameters:\",\"audio (Tensor) – Audio waveform tensor (B, T_wav).\",\"forward_generator (bool) – Whether to forward generator.\",\"kwargs – “utt_id” is among the input.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"meta_info() → Dict[str, Any]\",\"Return meta information of the codec.\"]},\"1396\":{\"h\":\"espnet2.gan_codec.encodec.encodec.Encodec\",\"t\":[\"source\",\"class espnet2.gan_codec.encodec.encodec.Encodec(sampling_rate: int = 24000, generator_params: Dict[str, Any] = {'decoder_final_activation': None, 'decoder_final_activation_params': None, 'decoder_trim_right_ratio': 1.0, 'encdec_activation': 'ELU', 'encdec_activation_params': {'alpha': 1.0}, 'encdec_causal': False, 'encdec_channels': 1, 'encdec_compress': 2, 'encdec_dilation_base': 2, 'encdec_kernel_size': 7, 'encdec_last_kernel_size': 7, 'encdec_lstm': 2, 'encdec_n_filters': 32, 'encdec_n_residual_layers': 1, 'encdec_norm': 'weight_norm', 'encdec_norm_params': {}, 'encdec_pad_mode': 'reflect', 'encdec_ratios': [8, 5, 4, 2], 'encdec_residual_kernel_size': 7, 'encdec_true_skip': False, 'hidden_dim': 128, 'quantizer_bins': 1024, 'quantizer_decay': 0.99, 'quantizer_kmeans_init': True, 'quantizer_kmeans_iters': 50, 'quantizer_n_q': 8, 'quantizer_target_bandwidth': [7.5, 15], 'quantizer_threshold_ema_dead_code': 2}, discriminator_params: Dict[str, Any] = {'activation': 'LeakyReLU', 'activation_params': {'negative_slope': 0.3}, 'filters': 32, 'hop_lengths': [256, 512, 128, 64, 32], 'in_channels': 1, 'n_ffts': [1024, 2048, 512, 256, 128], 'norm': 'weight_norm', 'out_channels': 1, 'sep_channels': False, 'win_lengths': [1024, 2048, 512, 256, 128]}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, use_feat_match_loss: bool = True, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, use_mel_loss: bool = True, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 24000, 'log_base': None, 'n_mels': 80, 'range_end': 11, 'range_start': 6, 'window': 'hann'}, use_dual_decoder: bool = True, lambda_quantization: float = 1.0, lambda_reconstruct: float = 1.0, lambda_commit: float = 1.0, lambda_adv: float = 1.0, lambda_feat_match: float = 2.0, lambda_mel: float = 45.0, cache_generator_outputs: bool = False, use_loss_balancer: bool = False, balance_ema_decay: float = 0.99)\",\"Bases: SoundStream\",\"Encodec Model: https://arxiv.org/abs/2210.13438\",\"The key differences between this and SoundStream are the discriminator and loss balancer. Check SoundStream model for many details\",\"Intialize SoundStream model.\",\"Parameters:TODO (jiatong)\"]},\"1397\":{\"h\":\"espnet2.gan_codec.encodec.encodec.EncodecDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_codec.encodec.encodec.EncodecDiscriminator(msstft_discriminator_params: Dict[str, Any] = {'activation': 'LeakyReLU', 'activation_params': {'negative_slope: 0.3'}, 'filters': 32, 'hop_lengths': [256, 512, 128, 64, 32], 'in_channels': 1, 'n_fft': [1024, 2048, 512, 256, 128], 'norm': 'weight_norm', 'out_channels': 1, 'win_lengths': [1024, 2048, 512, 256, 128]})\",\"Bases: Module\",\"Encodec Discriminator with only Multi-Scale STFT discriminator module\",\"Initialize Encodec Discriminator module.\",\"Args: msstft_discriminator_params (Dict[str, Any]) with following arguments: : in_channels (int): Number of input channels. out_channels (int): Number of output channels. filters (int): Number of filters in convolutions. norm (str): normalization choice of Convolutional layers n_ffts (Sequence[int]): Size of FFT for each scale. hop_lengths (Sequence[int]): Length of hop between STFT windows for <br/>\",\"each scale. <br/> win_lengths (Sequence[int]): Window size for each scale. activation (str): activation function choice of convolutional layer activation_params (Dict[str, Any]): parameters for activation function)\",\"forward(x: Tensor) → List[List[Tensor]]\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, : which consists of each layer output tensors. Only one discriminator here, but still make it as List of List for consistency.\",\"Return type: List[List[Tensor]]\"]},\"1398\":{\"h\":\"espnet2.gan_codec.hificodec.module.Encoder\",\"t\":[\"source\",\"class espnet2.gan_codec.hificodec.module.Encoder(resblock_num, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_kernel_sizes)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1399\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"remove_weight_norm()\"]},\"1400\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.EuclideanCodebook\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.quantizer.modules.core_vq.EuclideanCodebook(dim: int, codebook_size: int, kmeans_init: int = False, kmeans_iters: int = 10, decay: float = 0.99, epsilon: float = 1e-05, threshold_ema_dead_code: int = 2)\",\"Bases: Module\",\"Codebook with Euclidean distance.\",\"Parameters:\",\"dim (int) – Dimension.\",\"codebook_size (int) – Codebook size.\",\"kmeans_init (bool) – Whether to use k-means to initialize the codebooks. If set to true, run the k-means algorithm on the first training batch and use the learned centroids as initialization.\",\"kmeans_iters (int) – Number of iterations used for k-means algorithm at initialization.\",\"decay (float) – Decay for exponential moving average over the codebooks.\",\"epsilon (float) – Epsilon value for numerical stability.\",\"threshold_ema_dead_code (int) – Threshold for dead code expiration. Replace any codes that have an exponential moving average cluster size less than the specified threshold with randomly selected vector from the current batch.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"decode(embed_ind)\",\"dequantize(embed_ind)\",\"encode(x)\",\"expire_codes_(batch_samples)\",\"forward(x)\",\"Codebook Forward with EMA.\",\"Parameters:x (Tensor) – Vector for quantization (B, T, D)\",\"Returns: Quantized output (B, T, D) Tensor: Codebook Index (B, T)\",\"Return type: Tensor\",\"init_embed_(data)\",\"postprocess_emb(embed_ind, shape)\",\"preprocess(x)\",\"quantize(x)\",\"replace_(samples, mask)\",\"update_ema()\"]},\"1401\":{\"h\":\"espnet2.gan_codec.funcodec.funcodec.FunCodec\",\"t\":[\"source\",\"class espnet2.gan_codec.funcodec.funcodec.FunCodec(sampling_rate: int = 24000, generator_params: Dict[str, Any] = {'codec_domain': ['time', 'time'], 'decoder_final_activation': None, 'decoder_final_activation_params': None, 'decoder_trim_right_ratio': 1.0, 'domain_conf': {}, 'encdec_activation': 'ELU', 'encdec_activation_params': {'alpha': 1.0}, 'encdec_causal': False, 'encdec_channels': 1, 'encdec_compress': 2, 'encdec_dilation_base': 2, 'encdec_kernel_size': 7, 'encdec_last_kernel_size': 7, 'encdec_lstm': 2, 'encdec_n_filters': 32, 'encdec_n_residual_layers': 1, 'encdec_norm': 'weight_norm', 'encdec_norm_params': {}, 'encdec_pad_mode': 'reflect', 'encdec_ratios': [(8, 1), (5, 1), (4, 1), (2, 1)], 'encdec_residual_kernel_size': 7, 'encdec_true_skip': False, 'hidden_dim': 128, 'quantizer_bins': 1024, 'quantizer_decay': 0.99, 'quantizer_dropout': True, 'quantizer_kmeans_init': True, 'quantizer_kmeans_iters': 50, 'quantizer_n_q': 8, 'quantizer_target_bandwidth': [7.5, 15], 'quantizer_threshold_ema_dead_code': 2}, discriminator_params: Dict[str, Any] = {'complexstft_discriminator_params': {'chan_mults': (1, 2, 4, 4, 8, 8), 'channels': 32, 'hop_length': 256, 'in_channels': 1, 'logits_abs': True, 'n_fft': 1024, 'stft_normalized': False, 'strides': ((1, 2), (2, 2), (1, 2), (2, 2), (1, 2), (2, 2)), 'win_length': 1024}, 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'scale_discriminator_params': {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scale_follow_official_norm': False, 'scales': 3}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, use_feat_match_loss: bool = True, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, use_mel_loss: bool = True, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 24000, 'log_base': None, 'n_mels': 80, 'range_end': 11, 'range_start': 6, 'window': 'hann'}, use_dual_decoder: bool = False, lambda_quantization: float = 1.0, lambda_reconstruct: float = 1.0, lambda_commit: float = 1.0, lambda_adv: float = 1.0, lambda_feat_match: float = 2.0, lambda_mel: float = 45.0, cache_generator_outputs: bool = False)\",\"Bases: AbsGANCodec\",\"FunCodec model.\",\"Intialize FunCodec model.\",\"Parameters:TODO (jiatong)\",\"decode(x: Tensor, **kwargs) → Tensor\",\"Run encoding.\",\"Parameters:x (Tensor) – Input codes (T_code, N_stream).\",\"Returns: Generated waveform (T_wav,).\",\"Return type: Tensor\",\"encode(x: Tensor, **kwargs) → Tensor\",\"Run encoding.\",\"Parameters:x (Tensor) – Input audio (T_wav,).\",\"Returns: Generated codes (T_code, N_stream).\",\"Return type: Tensor\",\"forward(audio: Tensor, forward_generator: bool = True, **kwargs) → Dict[str, Any]\",\"Perform generator forward.\",\"Parameters:\",\"audio (Tensor) – Audio waveform tensor (B, T_wav).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(x: Tensor, **kwargs) → Dict[str, Tensor]\",\"Run inference.\",\"Parameters:x (Tensor) – Input audio (T_wav,).\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"codec (Tensor): Generated neural codec (T_code, N_stream).\",\"Return type: Dict[str, Tensor]\",\"meta_info() → Dict[str, Any]\",\"Return meta information of the codec.\"]},\"1402\":{\"h\":\"espnet2.gan_codec.funcodec.funcodec.FunCodecDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_codec.funcodec.funcodec.FunCodecDiscriminator(scales: int = 3, scale_downsample_pooling: str = 'AvgPool1d', scale_downsample_pooling_params: Dict[str, Any] = {'kernel_size': 4, 'padding': 2, 'stride': 2}, scale_discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1}, scale_follow_official_norm: bool = False, periods: List[int] = [2, 3, 5, 7, 11], period_discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, complexstft_discriminator_params: Dict[str, Any] = {'chan_mults': [1, 2, 4, 4, 8, 8], 'channels': 32, 'hop_length': 256, 'in_channels': 1, 'n_fft': 1024, 'stft_normalized': False, 'strides': [[1, 2], [2, 2], [1, 2], [2, 2], [1, 2], [2, 2]], 'win_length': 1024})\",\"Bases: Module\",\"FunCodec discriminator module.\",\"Initialize FunCodec Discriminator module.\",\"Parameters:\",\"scales (int) – Number of multi-scales.\",\"sclae_downsample_pooling (str) – Pooling module name for downsampling of the inputs.\",\"scale_downsample_pooling_params (Dict *[*str,Any]) – Parameters for the above pooling module.\",\"scale_discriminator_params (Dict *[*str,Any]) – Parameters for hifi-gan scale discriminator module.\",\"scale_follow_official_norm (bool) – Whether to follow the norm setting of the official implementaion. The first discriminator uses spectral norm and the other discriminators use weight norm.\",\"complexstft_discriminator_params (Dict *[*str,Any]) – Parameters for the complex stft discriminator module.\",\"forward(x: Tensor) → List[List[Tensor]]\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, : which consists of each layer output tensors. Multi scale and multi period ones are concatenated.\",\"Return type: List[List[Tensor]]\"]},\"1403\":{\"h\":\"espnet2.gan_codec.funcodec.funcodec.FunCodecGenerator\",\"t\":[\"source\",\"class espnet2.gan_codec.funcodec.funcodec.FunCodecGenerator(sample_rate: int = 24000, hidden_dim: int = 128, codebook_dim: int = 8, encdec_channels: int = 1, encdec_n_filters: int = 32, encdec_n_residual_layers: int = 1, encdec_ratios: List[Tuple[int, int]] = [(4, 1), (4, 1), (4, 2), (4, 1)], encdec_activation: str = 'ELU', encdec_activation_params: Dict[str, Any] = {'alpha': 1.0}, encdec_norm: str = 'weight_norm', encdec_norm_params: Dict[str, Any] = {}, encdec_kernel_size: int = 7, encdec_residual_kernel_size: int = 7, encdec_last_kernel_size: int = 7, encdec_dilation_base: int = 2, encdec_causal: bool = False, encdec_pad_mode: str = 'reflect', encdec_true_skip: bool = False, encdec_compress: int = 2, encdec_lstm: int = 2, decoder_trim_right_ratio: float = 1.0, decoder_final_activation: str | None = None, decoder_final_activation_params: dict | None = None, quantizer_n_q: int = 8, quantizer_bins: int = 1024, quantizer_decay: float = 0.99, quantizer_kmeans_init: bool = True, quantizer_kmeans_iters: int = 50, quantizer_threshold_ema_dead_code: int = 2, quantizer_target_bandwidth: List[float] = [7.5, 15], quantizer_dropout: bool = True, codec_domain: List = ('time', 'time'), domain_conf: Dict | None = {}, audio_normalize: bool = False)\",\"Bases: Module\",\"FunCodec generator module.\",\"Initialize FunCodec Generator.\",\"Parameters:TODO (jiatong)\",\"decode(codes: Tensor)\",\"FunCodec codec decoding.\",\"Parameters:codecs (torch.Tensor) – neural codecs in shape ().\",\"Returns: resynthesized audio.\",\"Return type: torch.Tensor\",\"encode(x: Tensor, target_bw: float | None = None)\",\"FunCodec codec encoding.\",\"Parameters:x (torch.Tensor) – Input tensor of shape (B, 1, T).\",\"Returns: neural codecs in shape ().\",\"Return type: torch.Tensor\",\"forward(x: Tensor, use_dual_decoder: bool = False)\",\"FunCodec forward propagation.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor of shape (B, 1, T).\",\"use_dual_decoder (bool) – Whether to use dual decoder for encoder out\",\"Returns: resynthesized audio. torch.Tensor: commitment loss. torch.Tensor: quantization loss torch.Tensor: resynthesized audio from encoder.\",\"Return type: torch.Tensor\",\"freq_to_time_transfer(x: Tensor, scale: Tensor | None = None)\",\"time_to_freq_transfer(x: Tensor)\"]},\"1404\":{\"h\":\"espnet2.gan_codec.hificodec.module.Generator\",\"t\":[\"source\",\"class espnet2.gan_codec.hificodec.module.Generator(upsample_rates, upsample_kernel_sizes, upsample_initial_channel, resblock_num, resblock_kernel_sizes, resblock_dilation_sizes, out_dim)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1405\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"remove_weight_norm()\"]},\"1406\":{\"h\":\"espnet2.gan_codec.hificodec.module.GroupResidualVectorQuantization\",\"t\":[\"source\",\"class espnet2.gan_codec.hificodec.module.GroupResidualVectorQuantization(quantizer_target_bandwidth, hidden_dim, quantizer_n_q, quantizer_bins, quantizer_decay, quantizer_kmeans_init, quantizer_kmeans_iters, quantizer_threshold_ema_dead_code, **kwargs)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"decode(code: Tensor)\",\"HiFICodec codec decoding.\",\"Parameters:codes (torch.Tensor) – neural codecs in shape ().\",\"Returns: resynthesized audio.\",\"Return type: torch.Tensor\",\"encode(xin: Tensor, frame_rate: int, target_bw: float | None = None)\",\"HiFICodec codec encoding.\",\"Parameters:x (torch.Tensor) – Input tensor of shape (B, 1, T).\",\"Returns: neural codecs in shape ().\",\"Return type: torch.Tensor\",\"forward(xin: Tensor, sample_rate: int, bandwidth: float | None = None) → QuantizedResult\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1407\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1408\":{\"h\":\"espnet2.gan_codec.hificodec.hificodec.HiFiCodec\",\"t\":[\"source\",\"class espnet2.gan_codec.hificodec.hificodec.HiFiCodec(sampling_rate: int = 16000, generator_params: Dict[str, Any] = {'hidden_dim': 256, 'quantizer_bins': 1024, 'quantizer_decay': 0.99, 'quantizer_kmeans_init': True, 'quantizer_kmeans_iters': 50, 'quantizer_n_q': 8, 'quantizer_target_bandwidth': [7.5, 15], 'quantizer_threshold_ema_dead_code': 2, 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'resblock_kernel_sizes': [3, 7, 11], 'resblock_num': '1', 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 11, 8, 4], 'upsample_rates': [8, 5, 4, 2]}, discriminator_params: Dict[str, Any] = {'msstft_discriminator_params': {'activation': 'LeakyReLU', 'activation_params': {'negative_slope': 0.2}, 'filters': 32, 'hop_lengths': [256, 512, 128, 64, 32], 'in_channels': 1, 'n_ffts': [1024, 2048, 512, 256, 128], 'norm': 'weight_norm', 'out_channels': 1, 'win_lengths': [1024, 2048, 512, 256, 128]}, 'periods': [2, 3, 5, 7, 11], 'periods_discriminator_params': {'bias': False, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'scale_discriminator_params': {'bias': False, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scale_follow_official_norm': False, 'scales': 3}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, use_feat_match_loss: bool = True, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, use_mel_loss: bool = True, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 16000, 'log_base': None, 'n_mels': 80, 'range_end': 11, 'range_start': 6, 'window': 'hann'}, use_dual_decoder: bool = True, lambda_quantization: float = 1.0, lambda_reconstruct: float = 1.0, lambda_commit: float = 1.0, lambda_adv: float = 1.0, lambda_feat_match: float = 2.0, lambda_mel: float = 45.0, cache_generator_outputs: bool = False, use_loss_balancer: bool = False, balance_ema_decay: float = 0.99)\",\"Bases: AbsGANCodec\",\"HiFiCodec model.\",\"Intialize HiFiCodec model.\",\"decode(x: Tensor, **kwargs) → Tensor\",\"Run encoding.\",\"Parameters:x (Tensor) – Input codes (T_code, N_stream).\",\"Returns: Generated waveform (T_wav,).\",\"Return type: Tensor\",\"encode(x: Tensor, **kwargs) → Tensor\",\"Run encoding.\",\"Parameters:x (Tensor) – Input audio (T_wav,).\",\"Returns: Generated codes (T_code, N_stream).\",\"Return type: Tensor\",\"forward(audio: Tensor, forward_generator: bool = True, **kwargs) → Dict[str, Any]\",\"Perform generator forward.\",\"Parameters:\",\"audio (Tensor) – Audio waveform tensor (B, T_wav).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(x: Tensor, **kwargs) → Dict[str, Tensor]\",\"Run inference.\",\"Parameters:x (Tensor) – Input audio (T_wav,).\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"codec (Tensor): Generated neural codec (T_code, N_stream).\",\"Return type: Dict[str, Tensor]\",\"meta_info() → Dict[str, Any]\",\"Return meta information of the codec.\"]},\"1409\":{\"h\":\"espnet2.gan_codec.hificodec.hificodec.HiFiCodecDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_codec.hificodec.hificodec.HiFiCodecDiscriminator(msstft_discriminator_params: Dict[str, Any] = {'activation': 'LeakyReLU', 'activation_params': {'negative_slope': 0.2}, 'filters': 32, 'hop_lengths': [256, 512, 128, 64, 32], 'in_channels': 1, 'n_ffts': [1024, 2048, 512, 256, 128], 'norm': 'weight_norm', 'out_channels': 1, 'win_lengths': [1024, 2048, 512, 256, 128]}, scales: int = 3, scale_downsample_pooling: str = 'AvgPool1d', scale_downsample_pooling_params: Dict[str, Any] = {'kernel_size': 4, 'padding': 2, 'stride': 2}, scale_discriminator_params: Dict[str, Any] = {'bias': False, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, scale_follow_official_norm: bool = False, periods: List[int] = [2, 3, 5, 7, 11], periods_discriminator_params: Dict[str, Any] = {'bias': False, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True})\",\"Bases: Module\",\"HiFiCodec discriminator module.\",\"Initialize HiFiCodec Discriminator module.\",\"Parameters:\",\"msstft_discriminator_params (Dict *[*str,Any]) – Parameters for multi-scales STFT discriminator module.\",\"scales (int) – Number of multi-scales.\",\"sclae_downsample_pooling (str) – Pooling module name for downsampling of the inputs.\",\"scale_downsample_pooling_params (Dict *[*str,Any]) – Parameters for the above pooling module.\",\"scale_discriminator_params (Dict *[*str,Any]) – Parameters for hifi-gan scale discriminator module.\",\"periods (List *[*int]) – List of periods.\",\"discriminator_params (Dict *[*str,Any]) – Parameters for hifi-gan period discriminator module. The period parameter will be overwritten.\",\"forward(x: Tensor) → List[List[Tensor]]\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, : which consists of each layer output tensors. Multi scale and multi period ones are concatenated.\",\"Return type: List[List[Tensor]]\"]},\"1410\":{\"h\":\"espnet2.gan_codec.hificodec.hificodec.HiFiCodecGenerator\",\"t\":[\"source\",\"class espnet2.gan_codec.hificodec.hificodec.HiFiCodecGenerator(sample_rate: int = 16000, hidden_dim: int = 128, resblock_num: str = '1', resblock_kernel_sizes: List[int] = [3, 7, 11], resblock_dilation_sizes: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], upsample_rates: List[int] = [8, 5, 4, 2], upsample_kernel_sizes: List[int] = [16, 11, 8, 4], upsample_initial_channel: int = 512, quantizer_n_q: int = 8, quantizer_bins: int = 1024, quantizer_decay: float = 0.99, quantizer_kmeans_init: bool = True, quantizer_kmeans_iters: int = 50, quantizer_threshold_ema_dead_code: int = 2, quantizer_target_bandwidth: List[float] = [7.5, 15])\",\"Bases: Module\",\"HiFiCodec generator module.\",\"Initialize HiFiCodec Generator.\",\"Parameters:TODO\",\"decode(codes: Tensor)\",\"HiFiCodec codec decoding.\",\"Parameters:codecs (torch.Tensor) – neural codecs in shape ().\",\"Returns: resynthesized audio.\",\"Return type: torch.Tensor\",\"encode(x: Tensor, target_bw: float | None = None)\",\"HiFiCodec codec encoding.\",\"Parameters:x (torch.Tensor) – Input tensor of shape (B, 1, T).\",\"Returns: neural codecs in shape ().\",\"Return type: torch.Tensor\",\"forward(x: Tensor, use_dual_decoder: bool = False)\",\"HiFiCodec forward propagation.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor of shape (B, 1, T).\",\"use_dual_decoder (bool) – Whether to use dual decoder for encoder out\",\"Returns: resynthesized audio. torch.Tensor: commitment loss. torch.Tensor: quantization loss torch.Tensor: resynthesized audio from encoder.\",\"Return type: torch.Tensor\"]},\"1411\":{\"h\":\"espnet2.gan_codec.shared.discriminator.stft_discriminator.ModReLU\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.discriminator.stft_discriminator.ModReLU\",\"Bases: Module\",\"ComplexReLU module.\",\"Reference: : https://arxiv.org/abs/1705.09792https://github.com/pytorch/pytorch/issues/47052#issuecomment-718948801\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1412\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1413\":{\"h\":\"espnet2.gan_codec.shared.discriminator.msmpmb_discriminator.MultiBandDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.discriminator.msmpmb_discriminator.MultiBandDiscriminator(window_length: int, hop_factor: float = 0.25, sample_rate: int = 44100, bands: list = [(0.0, 0.1), (0.1, 0.25), (0.25, 0.5), (0.5, 0.75), (0.75, 1.0)], channel: int = 32)\",\"Bases: Module\",\"Complex multi-band spectrogram discriminator.\",\"Parameters:\",\"window_length (int) – Window length of STFT.\",\"hop_factor (float,optional) – Hop factor of the STFT, defaults to 0.25 * window_length.\",\"sample_rate (int,optional) – Sampling rate of audio in Hz, by default 44100\",\"bands (list,optional) – Bands to run discriminator over.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1414\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"spectrogram(x)\"]},\"1415\":{\"h\":\"espnet2.gan_codec.shared.discriminator.msstft_discriminator.MultiDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.discriminator.msstft_discriminator.MultiDiscriminator\",\"Bases: ABC, Module\",\"Base implementation for discriminators composed of sub-discriminators\",\"acting at different scales.\",\"abstract forward(x: Tensor)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1416\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract property num_discriminators : int\",\"Number of discriminators.\"]},\"1417\":{\"h\":\"espnet2.gan_codec.shared.discriminator.msmpmb_discriminator.MultiScaleDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.discriminator.msmpmb_discriminator.MultiScaleDiscriminator(rate: int = 1, sample_rate: int = 44100)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1418\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1419\":{\"h\":\"espnet2.gan_codec.shared.loss.freq_loss.MultiScaleMelSpectrogramLoss\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.loss.freq_loss.MultiScaleMelSpectrogramLoss(fs: int = 22050, range_start: int = 6, range_end: int = 11, window: str = 'hann', n_mels: int = 80, fmin: int | None = 0, fmax: int | None = None, center: bool = True, normalized: bool = False, onesided: bool = True, log_base: float | None = 10.0, alphas: bool = True)\",\"Bases: Module\",\"Multi-Scale spectrogram loss.\",\"Parameters:\",\"fs (int) – Sampling rate.\",\"range_start (int) – Power of 2 to use for the first scale.\",\"range_stop (int) – Power of 2 to use for the last scale.\",\"window (str) – Window type.\",\"n_mels (int) – Number of mel bins.\",\"fmin (Optional *[*int]) – Minimum frequency for Mel.\",\"fmax (Optional *[*int]) – Maximum frequency for Mel.\",\"center (bool) – Whether to use center window.\",\"normalized (bool) – Whether to use normalized one.\",\"onesided (bool) – Whether to use oneseded one.\",\"log_base (Optional *[*float]) – Log base value.\",\"alphas (bool) – Whether to use alphas as coefficients or not..\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y_hat: Tensor, y: Tensor) → Tensor\",\"Calculate Mel-spectrogram loss.\",\"Parameters:\",\"y_hat (Tensor) – Generated waveform tensor (B, 1, T).\",\"y (Tensor) – Groundtruth waveform tensor (B, 1, T).\",\"Returns: Mel-spectrogram loss value.\",\"Return type: Tensor\"]},\"1420\":{\"h\":\"espnet2.gan_codec.shared.discriminator.msmpmb_discriminator.MultiScaleMultiPeriodMultiBandDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.discriminator.msmpmb_discriminator.MultiScaleMultiPeriodMultiBandDiscriminator(rates: list = [], fft_sizes: list = [2048, 1024, 512], sample_rate: int = 44100, periods: List[int] = [2, 3, 5, 7, 11], period_discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, band_discriminator_params: Dict[str, Any] = {'bands': [(0.0, 0.1), (0.1, 0.25), (0.25, 0.5), (0.5, 0.75), (0.75, 1.0)], 'channel': 32, 'hop_factor': 0.25, 'sample_rate': 24000})\",\"Bases: Module\",\"Discriminator that combines multiple discriminators.\",\"Parameters:\",\"rates (list,optional) – sampling rates (in Hz) to run MSD at, by default [] If empty, MSD is not used.\",\"periods (list,optional) – periods (of samples) to run MPD at, by default [2, 3, 5, 7, 11]\",\"fft_sizes (list,optional) – Window sizes of the FFT to run MRD at, by default [2048, 1024, 512]\",\"sample_rate (int,optional) – Sampling rate of audio in Hz, by default 44100\",\"bands (list,optional) – Bands to run MRD at, by default BANDS\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1421\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"preprocess(y)\"]},\"1422\":{\"h\":\"espnet2.gan_codec.shared.discriminator.msstft_discriminator.MultiScaleSTFTDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.discriminator.msstft_discriminator.MultiScaleSTFTDiscriminator(filters: int, in_channels: int = 1, out_channels: int = 1, sep_channels: bool = False, n_ffts: List[int] = [1024, 2048, 512], hop_lengths: List[int] = [256, 512, 128], win_lengths: List[int] = [1024, 2048, 512], **kwargs)\",\"Bases: MultiDiscriminator\",\"Multi-Scale STFT (MS-STFT) discriminator.\",\"Parameters:\",\"filters (int) – Number of filters in convolutions.\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"sep_channels (bool) – Separate channels to distinct samples for stereo support.\",\"n_ffts (Sequence *[*int]) – Size of FFT for each scale.\",\"hop_lengths (Sequence *[*int]) – Length of hop between STFT windows for each scale.\",\"win_lengths (Sequence *[*int]) – Window size for each scale.\",\"**kwargs – Additional args for STFTDiscriminator.\",\"forward(x: Tensor)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1423\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"property num_discriminators\",\"Number of discriminators.\"]},\"1424\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet.NormConv1d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.encoder.seanet.NormConv1d(*args, causal: bool = False, norm: str = 'none', norm_kwargs: Dict[str, Any] = {}, **kwargs)\",\"Bases: Module\",\"Wrapper around Conv1d and normalization applied to this conv\",\"to provide a uniform interface across normalization approaches.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1425\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1426\":{\"h\":\"espnet2.gan_codec.shared.discriminator.msstft_conv.NormConv2d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.discriminator.msstft_conv.NormConv2d(*args, norm: str = 'none', norm_kwargs: Dict[str, Any] = {}, **kwargs)\",\"Bases: Module\",\"Wrapper around Conv2d and normalization applied to this conv\",\"to provide a uniform interface across normalization approaches.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1427\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1428\":{\"h\":\"espnet2.gan_codec.shared.decoder.seanet.NormConvTranspose1d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.decoder.seanet.NormConvTranspose1d(*args, causal: bool = False, norm: str = 'none', norm_kwargs: Dict[str, Any] = {}, **kwargs)\",\"Bases: Module\",\"Wrapper around ConvTranspose1d and normalization applied to this conv\",\"to provide a uniform interface across normalization approaches.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1429\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1430\":{\"h\":\"espnet2.gan_codec.shared.decoder.seanet_2d.NormConvTranspose2d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.decoder.seanet_2d.NormConvTranspose2d(*args, causal: bool = False, norm: str = 'none', norm_kwargs: Dict[str, Any] = {}, **kwargs)\",\"Bases: Module\",\"Wrapper around ConvTranspose2d and normalization applied to this conv\",\"to provide a uniform interface across normalization approaches.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1431\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1432\":{\"h\":\"espnet2.gan_codec.shared.quantizer.residual_vq.QuantizedResult\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.quantizer.residual_vq.QuantizedResult(quantized: torch.Tensor, codes: torch.Tensor, bandwidth: torch.Tensor, penalty: torch.Tensor | None = None)\",\"Bases: object\",\"bandwidth : Tensor\",\"codes : Tensor\",\"penalty : Tensor | None= None\",\"quantized : Tensor\"]},\"1433\":{\"h\":\"espnet2.gan_codec.hificodec.module.ResBlock1\",\"t\":[\"source\",\"class espnet2.gan_codec.hificodec.module.ResBlock1(channels, kernel_size=3, dilation=(1, 3, 5))\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1434\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"remove_weight_norm()\"]},\"1435\":{\"h\":\"espnet2.gan_codec.hificodec.module.ResBlock2\",\"t\":[\"source\",\"class espnet2.gan_codec.hificodec.module.ResBlock2(channels, kernel_size=3, dilation=(1, 3))\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1436\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"remove_weight_norm()\"]},\"1437\":{\"h\":\"espnet2.gan_codec.shared.decoder.seanet_2d.ReshapeModule\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.decoder.seanet_2d.ReshapeModule(dim=2)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1438\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1439\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.ResidualVectorQuantization\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.quantizer.modules.core_vq.ResidualVectorQuantization(*, num_quantizers, **kwargs)\",\"Bases: Module\",\"Residual vector quantization implementation.\",\"Follows Algorithm 1. in https://arxiv.org/pdf/2107.03312.pdf\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"decode(q_indices: Tensor) → Tensor\",\"encode(x: Tensor, n_q: int | None = None, st: int | None = None) → Tensor\",\"forward(x, n_q: int | None = None)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1440\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1441\":{\"h\":\"espnet2.gan_codec.shared.quantizer.residual_vq.ResidualVectorQuantizer\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.quantizer.residual_vq.ResidualVectorQuantizer(dimension: int = 256, codebook_dim: int = 512, n_q: int = 8, bins: int = 1024, decay: float = 0.99, kmeans_init: bool = True, kmeans_iters: int = 50, threshold_ema_dead_code: int = 2, quantizer_dropout: bool = False)\",\"Bases: Module\",\"Residual Vector Quantizer.\",\"Parameters:\",\"dimension (int) – Dimension of the codebooks.\",\"n_q (int) – Number of residual vector quantizers used.\",\"bins (int) – Codebook size.\",\"decay (float) – Decay for exponential moving average over the codebooks.\",\"kmeans_init (bool) – Whether to use kmeans to initialize the codebooks.\",\"kmeans_iters (int) – Number of iterations used for kmeans initialization.\",\"threshold_ema_dead_code (int) – Threshold for dead code expiration. Replace any codes that have an exponential moving average cluster size less than the specified threshold with randomly selected vector from the current batch.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"decode(codes: Tensor) → Tensor\",\"Decode the given codes to the quantized representation.\",\"encode(x: Tensor, sample_rate: int, bandwidth: float | None = None, st: int | None = None) → Tensor\",\"Encode a given input tensor with the specified sample rate at\",\"the given bandwidth. The RVQ encode method sets the appropriate number of quantizer to use and returns indices for each quantizer.\",\"forward(x: Tensor, sample_rate: int, bandwidth: float | None = None) → QuantizedResult\",\"Residual vector quantization on the given input tensor.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor.\",\"sample_rate (int) – Sample rate of the input tensor.\",\"bandwidth (float) – Target bandwidth.\",\"Returns: The quantized (or approximately quantized) representation with the associated bandwidth and any penalty term for the loss.\",\"Return type:QuantizedResult\",\"get_bandwidth_per_quantizer(sample_rate: int)\",\"Return bandwidth per quantizer for a given input sample rate.\",\"get_num_quantizers_for_bandwidth(sample_rate: int, bandwidth: float | None = None) → int\",\"Return n_q based on specified target bandwidth.\"]},\"1442\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet.SConv1d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.encoder.seanet.SConv1d(in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, dilation: int = 1, groups: int = 1, bias: bool = True, causal: bool = False, norm: str = 'none', norm_kwargs: Dict[str, Any] = {}, pad_mode: str = 'reflect')\",\"Bases: Module\",\"Conv1d with some builtin handling of asymmetric or causal padding\",\"and normalization.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1443\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1444\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet_2d.SConv2d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.encoder.seanet_2d.SConv2d(in_channels: int, out_channels: int, kernel_size: int | Tuple[int, int], stride: int | Tuple[int, int] = 1, dilation: int | Tuple[int, int] = 1, groups: int = 1, bias: bool = True, causal: bool = False, norm: str = 'none', norm_kwargs: Dict[str, Any] = {}, pad_mode: str = 'reflect')\",\"Bases: Module\",\"Conv1d with some builtin handling of asymmetric or causal padding\",\"and normalization. Note: causal padding only make sense on time (the last) axis. Frequency (the second last) axis are always non-causally padded.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1445\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1446\":{\"h\":\"espnet2.gan_codec.shared.decoder.seanet.SConvTranspose1d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.decoder.seanet.SConvTranspose1d(in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, causal: bool = False, norm: str = 'none', trim_right_ratio: float = 1.0, norm_kwargs: Dict[str, Any] = {})\",\"Bases: Module\",\"ConvTranspose1d with some builtin handling of asymmetric or causal padding\",\"and normalization.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1447\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1448\":{\"h\":\"espnet2.gan_codec.shared.decoder.seanet_2d.SConvTranspose2d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.decoder.seanet_2d.SConvTranspose2d(in_channels: int, out_channels: int, kernel_size: int | Tuple[int, int], stride: int | Tuple[int, int] = 1, causal: bool = False, norm: str = 'none', trim_right_ratio: float = 1.0, norm_kwargs: Dict[str, Any] = {}, out_padding: int | List[Tuple[int, int]] = 0, groups: int = 1)\",\"Bases: Module\",\"ConvTranspose2d with some builtin handling of asymmetric or causal padding\",\"and normalization. Note: causal padding only make sense on time (the last) axis. Frequency (the second last) axis are always non-causally padded.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1449\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1450\":{\"h\":\"espnet2.gan_codec.shared.decoder.seanet.SEANetDecoder\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.decoder.seanet.SEANetDecoder(channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 1, ratios: List[int] = [8, 5, 4, 2], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0}, final_activation: str | None = None, final_activation_params: dict | None = None, norm: str = 'weight_norm', norm_params: Dict[str, Any] = {}, kernel_size: int = 7, last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False, pad_mode: str = 'reflect', true_skip: bool = False, compress: int = 2, lstm: int = 2, trim_right_ratio: float = 1.0)\",\"Bases: Module\",\"SEANet decoder.\",\"Parameters:\",\"channels (int) – Audio channels.\",\"dimension (int) – Intermediate representation dimension.\",\"n_filters (int) – Base width for the model.\",\"n_residual_layers (int) – nb of residual layers.\",\"ratios (Sequence *[*int]) – kernel size and stride ratios\",\"activation (str) – Activation function.\",\"activation_params (dict) – Parameters to provide to the activation function\",\"final_activation (str) – Final activation function after all convolutions.\",\"final_activation_params (dict) – Parameters to provide to the activation function\",\"norm (str) – Normalization method.\",\"norm_params (dict) – Parameters to provide to the underlying normalization used along with the convolution.\",\"kernel_size (int) – Kernel size for the initial convolution.\",\"last_kernel_size (int) – Kernel size for the initial convolution.\",\"residual_kernel_size (int) – Kernel size for the residual layers.\",\"dilation_base (int) – How much to increase the dilation with each layer.\",\"causal (bool) – Whether to use fully causal convolution.\",\"pad_mode (str) – Padding mode for the convolutions.\",\"true_skip (bool) – Whether to use true skip connection or a simple (streamable) convolution as the skip connection in the residual network blocks.\",\"compress (int) – Reduced dimensionality in residual branches (from Demucs v3).\",\"lstm (int) – Number of LSTM layers at the end of the encoder.\",\"trim_right_ratio (float) – Ratio for trimming at the right of the transposed convolution under the causal setup. If equal to 1.0, it means that all the trimming is done at the right.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(z)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1451\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1452\":{\"h\":\"espnet2.gan_codec.shared.decoder.seanet_2d.SEANetDecoder2d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.decoder.seanet_2d.SEANetDecoder2d(channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 1, ratios: List[Tuple[int, int]] = [(4, 1), (4, 1), (4, 2), (4, 1)], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0}, final_activation: str | None = None, final_activation_params: dict | None = None, norm: str = 'weight_norm', norm_params: Dict[str, Any] = {}, kernel_size: int = 7, last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False, pad_mode: str = 'reflect', true_skip: bool = False, compress: int = 2, lstm: int = 2, trim_right_ratio: float = 1.0, res_seq=True, last_out_padding: List[int] = [(0, 1), (0, 0)], tr_conv_group_ratio: int = -1, conv_group_ratio: int = -1)\",\"Bases: Module\",\"SEANet decoder.\",\"Parameters:\",\"channels (int) – Audio channels.\",\"dimension (int) – Intermediate representation dimension.\",\"n_filters (int) – Base width for the model.\",\"n_residual_layers (int) – nb of residual layers.\",\"ratios (Sequence *[*int]) – kernel size and stride ratios\",\"activation (str) – Activation function.\",\"activation_params (dict) – Parameters to provide to the activation function\",\"final_activation (str) – Final activation function after all convolutions.\",\"final_activation_params (dict) – Parameters to provide to the activation function\",\"norm (str) – Normalization method.\",\"norm_params (dict) – Parameters to provide to the underlying normalization used along with the convolution.\",\"kernel_size (int) – Kernel size for the initial convolution.\",\"last_kernel_size (int) – Kernel size for the initial convolution.\",\"residual_kernel_size (int) – Kernel size for the residual layers.\",\"dilation_base (int) – How much to increase the dilation with each layer.\",\"causal (bool) – Whether to use fully causal convolution.\",\"pad_mode (str) – Padding mode for the convolutions.\",\"true_skip (bool) – Whether to use true skip connection or a simple (streamable) convolution as the skip connection in the residual network blocks.\",\"compress (int) – Reduced dimensionality in residual branches (from Demucs v3).\",\"lstm (int) – Number of LSTM layers at the end of the encoder.\",\"trim_right_ratio (float) – Ratio for trimming at the right of the transposed convolution under the causal setup. If equal to 1.0, it means that all the trimming is done at the right.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(z)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1453\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\"]},\"1454\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet.SEANetEncoder\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.encoder.seanet.SEANetEncoder(channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 1, ratios: List[int] = [8, 5, 4, 2], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0}, norm: str = 'weight_norm', norm_params: Dict[str, Any] = {}, kernel_size: int = 7, last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False, pad_mode: str = 'reflect', true_skip: bool = False, compress: int = 2, lstm: int = 2)\",\"Bases: Module\",\"SEANet encoder.\",\"Parameters:\",\"channels (int) – Audio channels.\",\"dimension (int) – Intermediate representation dimension.\",\"n_filters (int) – Base width for the model.\",\"n_residual_layers (int) – nb of residual layers.\",\"ratios (Sequence *[*int]) – kernel size and stride ratios. The encoder uses downsampling ratios instead of upsampling ratios, hence it will use the ratios in the reverse order to the ones specified here that must match the decoder order\",\"activation (str) – Activation function.\",\"activation_params (dict) – Parameters to provide to the activation function\",\"norm (str) – Normalization method.\",\"norm_params (dict) – Parameters to provide to the underlying normalization used along with the convolution.\",\"kernel_size (int) – Kernel size for the initial convolution.\",\"last_kernel_size (int) – Kernel size for the initial convolution.\",\"residual_kernel_size (int) – Kernel size for the residual layers.\",\"dilation_base (int) – How much to increase the dilation with each layer.\",\"causal (bool) – Whether to use fully causal convolution.\",\"pad_mode (str) – Padding mode for the convolutions.\",\"true_skip (bool) – Whether to use true skip connection or a simple (streamable) convolution as the skip connection in the residual network blocks.\",\"compress (int) – Reduced dimensionality in residual branches (from Demucs v3).\",\"lstm (int) – Number of LSTM layers at the end of the encoder.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1455\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1456\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet_2d.SEANetEncoder2d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.encoder.seanet_2d.SEANetEncoder2d(channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 1, ratios: List[Tuple[int, int]] = [(4, 1), (4, 1), (4, 2), (4, 1)], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0}, norm: str = 'weight_norm', norm_params: Dict[str, Any] = {}, kernel_size: int = 7, last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False, pad_mode: str = 'reflect', true_skip: bool = False, compress: int = 2, lstm: int = 2, res_seq=True, conv_group_ratio: int = -1)\",\"Bases: Module\",\"SEANet encoder.\",\"Parameters:\",\"input_size (int) – Audio channels.\",\"dimension (int) – Intermediate representation dimension.\",\"n_filters (int) – Base width for the model.\",\"n_residual_layers (int) – nb of residual layers.\",\"ratios (Sequence *[*int]) – kernel size and stride ratios. The encoder uses downsampling ratios instead of upsampling ratios, hence it will use the ratios in the reverse order to the ones specified here that must match the decoder order\",\"activation (str) – Activation function. ELU = Exponential Linear Unit\",\"activation_params (dict) – Parameters to provide to the activation function\",\"norm (str) – Normalization method.\",\"norm_params (dict) – Parameters to provide to the underlying normalization used along with the convolution.\",\"kernel_size (int) – Kernel size for the initial convolution.\",\"last_kernel_size (int) – Kernel size for the initial convolution.\",\"residual_kernel_size (int) – Kernel size for the residual layers.\",\"dilation_base (int) – How much to increase the dilation with each layer.\",\"causal (bool) – Whether to use fully causal convolution.\",\"pad_mode (str) – Padding mode for the convolutions.\",\"true_skip (bool) – Whether to use true skip connection or a simple (streamable) convolution as the skip connection in the residual network blocks.\",\"compress (int) – Reduced dimensionality in residual branches (from Demucs v3).\",\"lstm (int) – Number of LSTM layers at the end of the encoder.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1457\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1458\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet.SEANetResnetBlock\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.encoder.seanet.SEANetResnetBlock(dim: int, kernel_sizes: List[int] = [3, 1], dilations: List[int] = [1, 1], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0}, norm: str = 'weight_norm', norm_params: Dict[str, Any] = {}, causal: bool = False, pad_mode: str = 'reflect', compress: int = 2, true_skip: bool = True)\",\"Bases: Module\",\"Residual block from SEANet model.\",\"Parameters:\",\"dim (int) – Dimension of the input/output\",\"kernel_sizes (list) – List of kernel sizes for the convolutions.\",\"dilations (list) – List of dilations for the convolutions.\",\"activation (str) – Activation function.\",\"activation_params (dict) – Parameters to provide to the activation function\",\"norm (str) – Normalization method.\",\"norm_params (dict) – Parameters to provide to the underlying normalization used along with the convolution.\",\"causal (bool) – Whether to use fully causal convolution.\",\"pad_mode (str) – Padding mode for the convolutions.\",\"compress (int) – Reduced dimensionality in residual branches (from Demucs v3)\",\"true_skip (bool) – Whether to use true skip connection or a simple convolution as the skip connection.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1459\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1460\":{\"h\":\"espnet2.gan_codec.shared.decoder.seanet_2d.SEANetResnetBlock2d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.decoder.seanet_2d.SEANetResnetBlock2d(dim: int, kernel_sizes: List[Tuple[int, int]] = [(3, 3), (1, 1)], dilations: List[Tuple[int, int]] = [(1, 1), (1, 1)], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0}, norm: str = 'weight_norm', norm_params: Dict[str, Any] = {}, causal: bool = False, pad_mode: str = 'reflect', compress: int = 2, true_skip: bool = True, conv_group_ratio: int = -1)\",\"Bases: Module\",\"Residual block from SEANet model.\",\"Parameters:\",\"dim (int) – Dimension of the input/output\",\"kernel_sizes (list) – List of kernel sizes for the convolutions.\",\"dilations (list) – List of dilations for the convolutions.\",\"activation (str) – Activation function.\",\"activation_params (dict) – Parameters to provide to the activation function\",\"norm (str) – Normalization method.\",\"norm_params (dict) – Parameters to provide to the underlying normalization used along with the convolution.\",\"causal (bool) – Whether to use fully causal convolution.\",\"pad_mode (str) – Padding mode for the convolutions.\",\"compress (int) – Reduced dimensionality in residual branches (from Demucs v3)\",\"true_skip (bool) – Whether to use true skip connection or a simple convolution as the skip connection.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1461\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1462\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet.SLSTM\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.encoder.seanet.SLSTM(dimension: int, num_layers: int = 2, skip: bool = True)\",\"Bases: Module\",\"LSTM without worrying about the hidden state, nor the layout of the data.\",\"Expects input as convolutional layout.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1463\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1464\":{\"h\":\"espnet2.gan_codec.shared.encoder.snake_activation.Snake1d\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.encoder.snake_activation.Snake1d\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1465\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1466\":{\"h\":\"espnet2.gan_codec.soundstream.soundstream.SoundStream\",\"t\":[\"source\",\"class espnet2.gan_codec.soundstream.soundstream.SoundStream(sampling_rate: int = 24000, generator_params: Dict[str, Any] = {'decoder_final_activation': None, 'decoder_final_activation_params': None, 'decoder_trim_right_ratio': 1.0, 'encdec_activation': 'ELU', 'encdec_activation_params': {'alpha': 1.0}, 'encdec_causal': False, 'encdec_channels': 1, 'encdec_compress': 2, 'encdec_dilation_base': 2, 'encdec_kernel_size': 7, 'encdec_last_kernel_size': 7, 'encdec_lstm': 2, 'encdec_n_filters': 32, 'encdec_n_residual_layers': 1, 'encdec_norm': 'weight_norm', 'encdec_norm_params': {}, 'encdec_pad_mode': 'reflect', 'encdec_ratios': [8, 5, 4, 2], 'encdec_residual_kernel_size': 7, 'encdec_true_skip': False, 'hidden_dim': 128, 'quantizer_bins': 1024, 'quantizer_decay': 0.99, 'quantizer_kmeans_init': True, 'quantizer_kmeans_iters': 50, 'quantizer_n_q': 8, 'quantizer_target_bandwidth': [7.5, 15], 'quantizer_threshold_ema_dead_code': 2}, discriminator_params: Dict[str, Any] = {'complexstft_discriminator_params': {'chan_mults': (1, 2, 4, 4, 8, 8), 'channels': 32, 'hop_length': 256, 'in_channels': 1, 'logits_abs': True, 'n_fft': 1024, 'stft_normalized': False, 'strides': ((1, 2), (2, 2), (1, 2), (2, 2), (1, 2), (2, 2)), 'win_length': 1024}, 'scale_discriminator_params': {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scale_follow_official_norm': False, 'scales': 3}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, use_feat_match_loss: bool = True, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, use_mel_loss: bool = True, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 24000, 'log_base': None, 'n_mels': 80, 'range_end': 11, 'range_start': 6, 'window': 'hann'}, use_dual_decoder: bool = True, lambda_quantization: float = 1.0, lambda_reconstruct: float = 1.0, lambda_commit: float = 1.0, lambda_adv: float = 1.0, lambda_feat_match: float = 2.0, lambda_mel: float = 45.0, cache_generator_outputs: bool = False, use_loss_balancer: bool = False, balance_ema_decay: float = 0.99)\",\"Bases: AbsGANCodec\",\"SoundStream model.\",\"Intialize SoundStream model.\",\"Parameters:TODO (jiatong)\",\"decode(x: Tensor, **kwargs) → Tensor\",\"Run encoding.\",\"Parameters:x (Tensor) – Input codes (T_code, N_stream).\",\"Returns: Generated waveform (T_wav,).\",\"Return type: Tensor\",\"encode(x: Tensor, **kwargs) → Tensor\",\"Run encoding.\",\"Parameters:x (Tensor) – Input audio (T_wav,).\",\"Returns: Generated codes (T_code, N_stream).\",\"Return type: Tensor\",\"forward(audio: Tensor, forward_generator: bool = True, **kwargs) → Dict[str, Any]\",\"Perform generator forward.\",\"Parameters:\",\"audio (Tensor) – Audio waveform tensor (B, T_wav).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(x: Tensor, **kwargs) → Dict[str, Tensor]\",\"Run inference.\",\"Parameters:x (Tensor) – Input audio (T_wav,).\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"codec (Tensor): Generated neural codec (T_code, N_stream).\",\"Return type: Dict[str, Tensor]\",\"meta_info() → Dict[str, Any]\",\"Return meta information of the codec.\"]},\"1467\":{\"h\":\"espnet2.gan_codec.soundstream.soundstream.SoundStreamDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_codec.soundstream.soundstream.SoundStreamDiscriminator(scales: int = 3, scale_downsample_pooling: str = 'AvgPool1d', scale_downsample_pooling_params: Dict[str, Any] = {'kernel_size': 4, 'padding': 2, 'stride': 2}, scale_discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1}, scale_follow_official_norm: bool = False, complexstft_discriminator_params: Dict[str, Any] = {'chan_mults': [1, 2, 4, 4, 8, 8], 'channels': 32, 'hop_length': 256, 'in_channels': 1, 'n_fft': 1024, 'stft_normalized': False, 'strides': [[1, 2], [2, 2], [1, 2], [2, 2], [1, 2], [2, 2]], 'win_length': 1024})\",\"Bases: Module\",\"SoundStream discriminator module.\",\"Initialize SoundStream Discriminator module.\",\"Parameters:\",\"scales (int) – Number of multi-scales.\",\"sclae_downsample_pooling (str) – Pooling module name for downsampling of the inputs.\",\"scale_downsample_pooling_params (Dict *[*str,Any]) – Parameters for the above pooling module.\",\"scale_discriminator_params (Dict *[*str,Any]) – Parameters for hifi-gan scale discriminator module.\",\"scale_follow_official_norm (bool) – Whether to follow the norm setting of the official implementaion. The first discriminator uses spectral norm and the other discriminators use weight norm.\",\"complexstft_discriminator_params (Dict *[*str,Any]) – Parameters for the complex stft discriminator module.\",\"forward(x: Tensor) → List[List[Tensor]]\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, : which consists of each layer output tensors. Multi scale and multi period ones are concatenated.\",\"Return type: List[List[Tensor]]\"]},\"1468\":{\"h\":\"espnet2.gan_codec.soundstream.soundstream.SoundStreamGenerator\",\"t\":[\"source\",\"class espnet2.gan_codec.soundstream.soundstream.SoundStreamGenerator(sample_rate: int = 24000, hidden_dim: int = 128, encdec_channels: int = 1, encdec_n_filters: int = 32, encdec_n_residual_layers: int = 1, encdec_ratios: List[int] = [8, 5, 4, 2], encdec_activation: str = 'ELU', encdec_activation_params: Dict[str, Any] = {'alpha': 1.0}, encdec_norm: str = 'weight_norm', encdec_norm_params: Dict[str, Any] = {}, encdec_kernel_size: int = 7, encdec_residual_kernel_size: int = 7, encdec_last_kernel_size: int = 7, encdec_dilation_base: int = 2, encdec_causal: bool = False, encdec_pad_mode: str = 'reflect', encdec_true_skip: bool = False, encdec_compress: int = 2, encdec_lstm: int = 2, decoder_trim_right_ratio: float = 1.0, decoder_final_activation: str | None = None, decoder_final_activation_params: dict | None = None, quantizer_n_q: int = 8, quantizer_bins: int = 1024, quantizer_decay: float = 0.99, quantizer_kmeans_init: bool = True, quantizer_kmeans_iters: int = 50, quantizer_threshold_ema_dead_code: int = 2, quantizer_target_bandwidth: List[float] = [7.5, 15])\",\"Bases: Module\",\"SoundStream generator module.\",\"Initialize SoundStream Generator.\",\"Parameters:TODO (jiatong)\",\"decode(codes: Tensor)\",\"Soundstream codec decoding.\",\"Parameters:codecs (torch.Tensor) – neural codecs in shape ().\",\"Returns: resynthesized audio.\",\"Return type: torch.Tensor\",\"encode(x: Tensor, target_bw: float | None = None)\",\"Soundstream codec encoding.\",\"Parameters:x (torch.Tensor) – Input tensor of shape (B, 1, T).\",\"Returns: neural codecs in shape ().\",\"Return type: torch.Tensor\",\"forward(x: Tensor, use_dual_decoder: bool = False)\",\"Soundstream forward propagation.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor of shape (B, 1, T).\",\"use_dual_decoder (bool) – Whether to use dual decoder for encoder out\",\"Returns: resynthesized audio. torch.Tensor: commitment loss. torch.Tensor: quantization loss torch.Tensor: resynthesized audio from encoder.\",\"Return type: torch.Tensor\"]},\"1469\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.VectorQuantization\",\"t\":[\"source\",\"class espnet2.gan_codec.shared.quantizer.modules.core_vq.VectorQuantization(dim: int, codebook_size: int, codebook_dim: int | None = None, decay: float = 0.99, epsilon: float = 1e-05, kmeans_init: bool = True, kmeans_iters: int = 50, threshold_ema_dead_code: int = 2, commitment_weight: float = 1.0, quantizer_dropout: bool = False)\",\"Bases: Module\",\"Vector quantization implementation.\",\"Currently supports only euclidean distance. :param dim: Dimension :type dim: int :param codebook_size: Codebook size :type codebook_size: int :param codebook_dim: Codebook dimension. If not defined, uses the specified\",\"dimension in dim.\",\"Parameters:\",\"decay (float) – Decay for exponential moving average over the codebooks.\",\"epsilon (float) – Epsilon value for numerical stability.\",\"kmeans_init (bool) – Whether to use kmeans to initialize the codebooks.\",\"kmeans_iters (int) – Number of iterations used for kmeans initialization.\",\"threshold_ema_dead_code (int) – Threshold for dead code expiration. Replace any codes that have an exponential moving average cluster size less than the specified threshold with randomly selected vector from the current batch.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"property codebook\",\"decode(embed_ind)\",\"encode(x)\",\"forward(x, mask=None)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1470\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1471\":{\"h\":\"espnet2.gan_codec.shared.discriminator.msmpmb_discriminator.WNConv1d\",\"t\":[\"source\",\"espnet2.gan_codec.shared.discriminator.msmpmb_discriminator.WNConv1d(*args, **kwargs)\"]},\"1472\":{\"h\":\"espnet2.gan_codec.shared.discriminator.msmpmb_discriminator.WNConv2d\",\"t\":[\"source\",\"espnet2.gan_codec.shared.discriminator.msmpmb_discriminator.WNConv2d(*args, **kwargs)\"]},\"1473\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.all_gather_sizes\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.all_gather_sizes(x, dim)\"]},\"1474\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.all_gather_variably_sized\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.all_gather_variably_sized(x, sizes, dim=0)\"]},\"1475\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.distrib.all_reduce\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.distrib.all_reduce(tensor: ~torch.Tensor, op=<RedOpType.SUM: 0>)\"]},\"1476\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet.apply_parametrization_norm\",\"t\":[\"source\",\"espnet2.gan_codec.shared.encoder.seanet.apply_parametrization_norm(module: Module, norm: str = 'none') → Module\"]},\"1477\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.distrib.average_metrics\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.distrib.average_metrics(metrics: Dict[str, float], count=1.0)\",\"Average a dictionary of metrics across all workers, using the optional\",\"count as unormalized weight.\"]},\"1478\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.distrib.broadcast_tensors\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.distrib.broadcast_tensors(tensors: Iterable[Tensor], src: int = 0)\",\"Broadcast the tensors from the given parameters to all workers.\",\"This can be used to ensure that all workers have the same model to start with.\"]},\"1479\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.cdist\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.cdist(x, y, eps=1e-08)\"]},\"1480\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.default\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.default(val: Any, d: Any) → Any\"]},\"1481\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.ema_inplace\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.ema_inplace(moving_avg, new, decay: float)\"]},\"1482\":{\"h\":\"espnet2.gan_codec.shared.discriminator.msstft_discriminator.get_2d_padding\",\"t\":[\"source\",\"espnet2.gan_codec.shared.discriminator.msstft_discriminator.get_2d_padding(kernel_size: Tuple[int, int], dilation: Tuple[int, int] = (1, 1))\"]},\"1483\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet_2d.get_activation\",\"t\":[\"source\",\"espnet2.gan_codec.shared.encoder.seanet_2d.get_activation(activation: str | None = None, channels=None, **kwargs)\"]},\"1484\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet.get_extra_padding_for_conv1d\",\"t\":[\"source\",\"espnet2.gan_codec.shared.encoder.seanet.get_extra_padding_for_conv1d(x: Tensor, kernel_size: int, stride: int, padding_total: int = 0) → int\",\"Pad for a convolution to make sure that the last window is full.\",\"Extra padding is added at the end. This is required to ensure that we can rebuild an output of the same length, as otherwise, even with padding, some time steps might get removed. For instance, with total padding = 4, kernel size = 4, stride = 2:\",\"0 0 1 2 3 4 5 0 0 # (0s are padding) 1 2 3 # (out-frames of a convolution, last 0 is never used) 0 0 1 2 3 4 5 0 # (out-tr.conv., but pos.5 will get removed as padding)\",\"1 2 3 4 # once you removed padding, we are missing one time step !\"]},\"1485\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet.get_norm_module\",\"t\":[\"source\",\"espnet2.gan_codec.shared.encoder.seanet.get_norm_module(module: Module, causal: bool = False, norm: str = 'none', **norm_kwargs) → Module\",\"Return the proper normalization module.\",\"If causal is True, this will ensure the returned module is causal, or return an error if the normalization doesn’t support causal evaluation.\"]},\"1486\":{\"h\":\"espnet2.gan_codec.hificodec.module.get_padding\",\"t\":[\"source\",\"espnet2.gan_codec.hificodec.module.get_padding(kernel_size, dilation=1)\"]},\"1487\":{\"h\":\"espnet2.gan_codec.hificodec.module.init_weights\",\"t\":[\"source\",\"espnet2.gan_codec.hificodec.module.init_weights(m, mean=0.0, std=0.01)\"]},\"1488\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.distrib.is_distributed\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.distrib.is_distributed()\"]},\"1489\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.kmeans\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.kmeans(samples, num_clusters, num_iters=10, use_cosine_sim=False, sample_fn=<function sample_vectors>, all_reduce_fn=<function noop>)\"]},\"1490\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.l2norm\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.l2norm(t, dim=-1, eps=1e-06)\"]},\"1491\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.laplace_smoothing\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.laplace_smoothing(x, n_categories: int, epsilon: float = 1e-05)\"]},\"1492\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.noop\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.noop(*args, **kwargs)\"]},\"1493\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet.pad1d\",\"t\":[\"source\",\"espnet2.gan_codec.shared.encoder.seanet.pad1d(x: Tensor, paddings: Tuple[int, int], mode: str = 'zero', value: float = 0.0)\",\"Tiny wrapper around F.pad, just to allow for reflect padding on small input.\",\"If this is the case, we insert extra 0 padding to the right before the reflection happen.\"]},\"1494\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet_2d.pad2d\",\"t\":[\"source\",\"espnet2.gan_codec.shared.encoder.seanet_2d.pad2d(x: Tensor, paddings: Tuple[Tuple[int, int], Tuple[int, int]], mode: str = 'zero', value: float = 0.0)\",\"Tiny wrapper around F.pad, just to allow for reflect padding on small input.\",\"If this is the case, we insert extra 0 padding to the right before the reflection happen.\"]},\"1495\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.pad_shape\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.pad_shape(shape, size, dim=0)\"]},\"1496\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.distrib.rank\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.distrib.rank()\"]},\"1497\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.sample_multinomial\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.sample_multinomial(total_count, probs)\"]},\"1498\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.sample_vectors\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.sample_vectors(samples, num: int)\"]},\"1499\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.sample_vectors_distributed\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.sample_vectors_distributed(local_samples, num)\"]},\"1500\":{\"h\":\"espnet2.gan_codec.shared.encoder.snake_activation.snake\",\"t\":[\"source\",\"espnet2.gan_codec.shared.encoder.snake_activation.snake()\"]},\"1501\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.distrib.sync_buffer\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.distrib.sync_buffer(buffers, average=True)\",\"Sync grad for buffers.\",\"If average is False, broadcast instead of averaging.\"]},\"1502\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.distrib.sync_grad\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.distrib.sync_grad(params)\",\"Simpler alternative to DistributedDataParallel, that doesn’t rely\",\"on any black magic. For simple models it can also be as fast. Just call this on your model parameters after the call to backward!\"]},\"1503\":{\"h\":\"espnet2.gan_codec.shared.encoder.seanet_2d.tuple_it\",\"t\":[\"source\",\"espnet2.gan_codec.shared.encoder.seanet_2d.tuple_it(x, num=2)\"]},\"1504\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.core_vq.uniform_init\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.core_vq.uniform_init(*shape: int)\"]},\"1505\":{\"h\":\"espnet2.gan_codec.shared.decoder.seanet.unpad1d\",\"t\":[\"source\",\"espnet2.gan_codec.shared.decoder.seanet.unpad1d(x: Tensor, paddings: Tuple[int, int])\",\"Remove padding from x, handling properly zero padding. Only for 1d!\"]},\"1506\":{\"h\":\"espnet2.gan_codec.shared.decoder.seanet_2d.unpad2d\",\"t\":[\"source\",\"espnet2.gan_codec.shared.decoder.seanet_2d.unpad2d(x: Tensor, paddings: Tuple[Tuple[int, int], Tuple[int, int]])\",\"Remove padding from x, handling properly zero padding. Only for 1d!\"]},\"1507\":{\"h\":\"espnet2.gan_codec.shared.quantizer.modules.distrib.world_size\",\"t\":[\"source\",\"espnet2.gan_codec.shared.quantizer.modules.distrib.world_size()\"]},\"1508\":{\"h\":\"espnet2.gan_svs.abs_gan_svs.AbsGANSVS\",\"t\":[\"source\",\"class espnet2.gan_svs.abs_gan_svs.AbsGANSVS(*args, **kwargs)\",\"Bases: AbsSVS, ABC\",\"GAN-based SVS model abstract class.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(forward_generator, *args, **kwargs) → Dict[str, Tensor | Dict[str, Tensor] | int]\",\"Return generator or discriminator loss.\"]},\"1509\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.AvocodoDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_svs.avocodo.avocodo.AvocodoDiscriminator(combd: Dict[str, Any] = {'combd_d_d': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'combd_d_g': [[1, 4, 16, 64, 256, 1], [1, 4, 16, 64, 256, 1], [1, 4, 16, 64, 256, 1]], 'combd_d_k': [[7, 11, 11, 11, 11, 5], [11, 21, 21, 21, 21, 5], [15, 41, 41, 41, 41, 5]], 'combd_d_p': [[3, 5, 5, 5, 5, 2], [5, 10, 10, 10, 10, 2], [7, 20, 20, 20, 20, 2]], 'combd_d_s': [[1, 1, 4, 4, 4, 1], [1, 1, 4, 4, 4, 1], [1, 1, 4, 4, 4, 1]], 'combd_h_u': [[16, 64, 256, 1024, 1024, 1024], [16, 64, 256, 1024, 1024, 1024], [16, 64, 256, 1024, 1024, 1024]], 'combd_op_f': [1, 1, 1], 'combd_op_g': [1, 1, 1], 'combd_op_k': [3, 3, 3]}, sbd: Dict[str, Any] = {'pqmf_config': {'fsbd': [64, 256, 0.1, 9.0], 'sbd': [16, 256, 0.03, 10.0]}, 'sbd_band_ranges': [[0, 6], [0, 11], [0, 16], [0, 64]], 'sbd_dilations': [[[5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11]], [[3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 3, 5], [2, 3, 5]]], 'sbd_filters': [[64, 128, 256, 256, 256], [64, 128, 256, 256, 256], [64, 128, 256, 256, 256], [32, 64, 128, 128, 128]], 'sbd_kernel_sizes': [[[7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7]], [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]], [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]]], 'sbd_strides': [[1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1]], 'sbd_transpose': [False, False, False, True], 'segment_size': 8192, 'use_sbd': True}, pqmf_config: Dict[str, Any] = {'lv1': [2, 256, 0.25, 10.0], 'lv2': [4, 192, 0.13, 10.0]}, projection_filters: List[int] = [0, 1, 1, 1])\",\"Bases: Module\",\"Avocodo Discriminator module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y: Tensor, y_hats: Tensor) → List[List[Tensor]]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1510\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1511\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.AvocodoDiscriminatorPlus\",\"t\":[\"source\",\"class espnet2.gan_svs.avocodo.avocodo.AvocodoDiscriminatorPlus(combd: Dict[str, Any] = {'combd_d_d': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'combd_d_g': [[1, 4, 16, 64, 256, 1], [1, 4, 16, 64, 256, 1], [1, 4, 16, 64, 256, 1]], 'combd_d_k': [[7, 11, 11, 11, 11, 5], [11, 21, 21, 21, 21, 5], [15, 41, 41, 41, 41, 5]], 'combd_d_p': [[3, 5, 5, 5, 5, 2], [5, 10, 10, 10, 10, 2], [7, 20, 20, 20, 20, 2]], 'combd_d_s': [[1, 1, 4, 4, 4, 1], [1, 1, 4, 4, 4, 1], [1, 1, 4, 4, 4, 1]], 'combd_h_u': [[16, 64, 256, 1024, 1024, 1024], [16, 64, 256, 1024, 1024, 1024], [16, 64, 256, 1024, 1024, 1024]], 'combd_op_f': [1, 1, 1], 'combd_op_g': [1, 1, 1], 'combd_op_k': [3, 3, 3]}, sbd: Dict[str, Any] = {'pqmf_config': {'fsbd': [64, 256, 0.1, 9.0], 'sbd': [16, 256, 0.03, 10.0]}, 'sbd_band_ranges': [[0, 6], [0, 11], [0, 16], [0, 64]], 'sbd_dilations': [[[5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11]], [[3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 3, 5], [2, 3, 5]]], 'sbd_filters': [[64, 128, 256, 256, 256], [64, 128, 256, 256, 256], [64, 128, 256, 256, 256], [32, 64, 128, 128, 128]], 'sbd_kernel_sizes': [[[7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7]], [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]], [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]]], 'sbd_strides': [[1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1]], 'sbd_transpose': [False, False, False, True], 'segment_size': 8192, 'use_sbd': True}, pqmf_config: Dict[str, Any] = {'lv1': [2, 256, 0.25, 10.0], 'lv2': [4, 192, 0.13, 10.0]}, projection_filters: List[int] = [0, 1, 1, 1], sample_rate: int = 22050, multi_freq_disc_params: Dict[str, Any] = {'divisors': [32, 16, 8, 4, 2, 1, 1], 'domain': 'double', 'hidden_channels': [256, 512, 512], 'hop_length_factors': [4, 8, 16], 'mel_scale': True, 'strides': [1, 2, 1, 2, 1, 2, 1]})\",\"Bases: Module\",\"Avocodo discriminator with additional MFD.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y: Tensor, y_hats: Tensor) → List[List[Tensor]]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1512\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1513\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.AvocodoGenerator\",\"t\":[\"source\",\"class espnet2.gan_svs.avocodo.avocodo.AvocodoGenerator(in_channels: int = 80, out_channels: int = 1, channels: int = 512, global_channels: int = -1, kernel_size: int = 7, upsample_scales: List[int] = [8, 8, 2, 2], upsample_kernel_sizes: List[int] = [16, 16, 4, 4], resblock_kernel_sizes: List[int] = [3, 7, 11], resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], projection_filters: List[int] = [0, 1, 1, 1], projection_kernels: List[int] = [0, 5, 7, 11], use_additional_convs: bool = True, bias: bool = True, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, use_weight_norm: bool = True)\",\"Bases: Module\",\"Avocodo generator module.\",\"Initialize AvocodoGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"channels (int) – Number of hidden representation channels.\",\"global_channels (int) – Number of global conditioning channels.\",\"kernel_size (int) – Kernel size of initial and final conv layer.\",\"upsample_scales (List *[*int]) – List of upsampling scales.\",\"upsample_kernel_sizes (List *[*int]) – List of kernel sizes for upsample layers.\",\"resblock_kernel_sizes (List *[*int]) – List of kernel sizes for residual blocks.\",\"resblock_dilations (List *[*List *[*int]]) – List of list of dilations for residual blocks.\",\"use_additional_convs (bool) – Whether to use additional conv layers in residual blocks.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c: Tensor, g: Tensor | None = None) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"c (Tensor) – Input tensor (B, in_channels, T).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"Returns: List of output tensors (B, out_channels, T).\",\"Return type: List[Tensor]\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"This initialization follows the official implementation manner. https://github.com/jik876/hifi-gan/blob/master/models.py\"]},\"1514\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.BaseFrequenceDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_svs.visinger2.visinger2_vocoder.BaseFrequenceDiscriminator(in_channels, hidden_channels=512, divisors=[32, 16, 8, 4, 2, 1, 1], strides=[1, 2, 1, 2, 1, 2, 1])\",\"Bases: Module\",\"Base Frequence Discriminator\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"hidden_channels (int,optional) – Number of channels in hidden layers. Defaults to 512.\",\"divisors (List *[*int],optional) – List of divisors for the number of channels in each layer. The length of the list determines the number of layers. Defaults to [32, 16, 8, 4, 2, 1, 1].\",\"strides (List *[*int],optional) – List of stride values for each layer. The length of the list determines the number of layers.Defaults to [1, 2, 1, 2, 1, 2, 1].\",\"forward(x)\",\"Perform forward pass through the base frequency discriminator.\",\"Parameters:x (torch.Tensor) – Input tensor of shape (B, in_channels, freq_bins, time_steps).\",\"Returns: List of output tensors from each layer of the : discriminator, where the first tensor corresponds to the output of the first layer, and so on.\",\"Return type: List[torch.Tensor]\"]},\"1515\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.CoMBD\",\"t\":[\"source\",\"class espnet2.gan_svs.avocodo.avocodo.CoMBD(h, pqmf_list=None, use_spectral_norm=False)\",\"Bases: Module\",\"CoMBD (Collaborative Multi-band Discriminator) module\",\"from from https://arxiv.org/abs/2206.13404\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(ys, ys_hat)\",\"Forward CoMBD.\",\"Parameters:\",\"ys (List *[*Tensor]) – List of ground truth signals of shape (B, 1, T).\",\"ys_hat (List *[*Tensor]) – List of predicted signals of shape (B, 1, T).\",\"Returns: Tuple containing the list of output tensors of shape (B, C_out, T_out) for real and fake, respectively, and the list of feature maps of shape (B, C, T) at each Conv1d layer for real and fake, respectively.\",\"Return type: Tuple[List[Tensor], List[Tensor], List[List[Tensor]], List[List[Tensor]]]\"]},\"1516\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.CoMBDBlock\",\"t\":[\"source\",\"class espnet2.gan_svs.avocodo.avocodo.CoMBDBlock(h_u: List[int], d_k: List[int], d_s: List[int], d_d: List[int], d_g: List[int], d_p: List[int], op_f: int, op_k: int, op_g: int, use_spectral_norm=False)\",\"Bases: Module\",\"CoMBD (Collaborative Multi-band Discriminator) block module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward pass through the CoMBD block.\",\"Parameters:x (Tensor) – Input tensor of shape (B, C_in, T_in).\",\"Returns: Tuple containing the output tensor of : shape (B, C_out, T_out)\",\"and a list of feature maps of shape (B, C, T) at each Conv1d layer.\",\"Return type: Tuple[Tensor, List[Tensor]]\"]},\"1517\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.ConvReluNorm\",\"t\":[\"source\",\"class espnet2.gan_svs.visinger2.visinger2_vocoder.ConvReluNorm(in_channels, hidden_channels, out_channels, kernel_size, n_layers, dropout_rate)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1518\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1519\":{\"h\":\"espnet2.gan_svs.vits.pitch_predictor.Decoder\",\"t\":[\"source\",\"class espnet2.gan_svs.vits.pitch_predictor.Decoder(out_channels: int = 192, attention_dim: int = 192, attention_heads: int = 2, linear_units: int = 768, blocks: int = 6, pw_layer_type: str = 'conv1d', pw_conv_kernel_size: int = 3, pos_enc_layer_type: str = 'rel_pos', self_attention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', normalize_before: bool = True, use_macaron_style: bool = False, use_conformer_conv: bool = False, conformer_kernel_size: int = 7, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0, global_channels: int = -1)\",\"Bases: Module\",\"Pitch or Mel decoder module in VISinger 2.\",\"Initialize Decoder in VISinger 2.\",\"Parameters:\",\"out_channels (int) – The output dimension of the module.\",\"attention_dim (int) – The dimension of the attention mechanism.\",\"attention_heads (int) – The number of attention heads.\",\"linear_units (int) – The number of units in the linear layer.\",\"blocks (int) – The number of encoder blocks.\",\"pw_layer_type (str) – The type of position-wise layer to use.\",\"pw_conv_kernel_size (int) – The kernel size of the position-wise convolutional layer.\",\"pos_enc_layer_type (str) – The type of positional encoding layer to use.\",\"self_attention_layer_type (str) – The type of self-attention layer to use.\",\"activation_type (str) – The type of activation function to use.\",\"normalize_before (bool) – Whether to normalize the data before the position-wise layer or after.\",\"use_macaron_style (bool) – Whether to use the macaron style or not.\",\"use_conformer_conv (bool) – Whether to use Conformer style conv or not.\",\"conformer_kernel_size (int) – The kernel size of the conformer convolutional layer.\",\"dropout_rate (float) – The dropout rate to use.\",\"positional_dropout_rate (float) – The positional dropout rate to use.\",\"attention_dropout_rate (float) – The attention dropout rate to use.\",\"global_channels (int) – The number of channels to use for global conditioning.\",\"forward(x, x_lengths, g=None)\",\"Forward pass of the Decoder.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, 2 + attention_dim, T).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"g (Tensor,optional) – Global conditioning tensor (B, global_channels, 1).\",\"Returns: Output tensor (B, 1, T). Tensor: Output mask (B, 1, T).\",\"Return type: Tensor\"]},\"1520\":{\"h\":\"espnet2.gan_svs.vits.duration_predictor.DurationPredictor\",\"t\":[\"source\",\"class espnet2.gan_svs.vits.duration_predictor.DurationPredictor(channels, filter_channels, kernel_size, dropout_rate, global_channels=0)\",\"Bases: Module\",\"Initialize duration predictor module.\",\"Parameters:\",\"channels (int) – Number of input channels.\",\"filter_channels (int) – Number of filter channels.\",\"kernel_size (int) – Size of the convolutional kernel.\",\"dropout_rate (float) – Dropout rate.\",\"global_channels (int,optional) – Number of global conditioning channels.\",\"forward(x, x_mask, g=None)\",\"Forward pass through the duration predictor module.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T).\",\"x_mask (Tensor) – Mask tensor (B, 1, T).\",\"g (Tensor,optional) – Global condition tensor (B, global_channels, 1).\",\"Returns: Predicted duration tensor (B, 2, T).\",\"Return type: Tensor\"]},\"1521\":{\"h\":\"espnet2.gan_svs.espnet_model.ESPnetGANSVSModel\",\"t\":[\"source\",\"class espnet2.gan_svs.espnet_model.ESPnetGANSVSModel(postfrontend: AbsFrontend | None, text_extract: AbsFeatsExtract | None, feats_extract: AbsFeatsExtract | None, score_feats_extract: AbsFeatsExtract | None, label_extract: AbsFeatsExtract | None, pitch_extract: AbsFeatsExtract | None, ying_extract: AbsFeatsExtract | None, duration_extract: AbsFeatsExtract | None, energy_extract: AbsFeatsExtract | None, normalize: InversibleInterface | None, pitch_normalize: InversibleInterface | None, energy_normalize: InversibleInterface | None, svs: AbsGANSVS)\",\"Bases: AbsGANESPnetModel\",\"ESPnet model for GAN-based singing voice synthesis task.\",\"Initialize ESPnetGANSVSModel module.\",\"collect_feats(text: Tensor, text_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, label: Tensor | None = None, label_lengths: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration_phn: Tensor | None = None, duration_phn_lengths: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_ruled_phn_lengths: Tensor | None = None, duration_syb: Tensor | None = None, duration_syb_lengths: Tensor | None = None, slur: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, ying: Tensor | None = None, ying_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs) → Dict[str, Tensor]\",\"Calculate features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"label (Option *[*Tensor]) – Label tensor (B, T_label).\",\"label_lengths (Optional *[*Tensor]) – Label lrngth tensor (B,).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (B, T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (B, T_label).\",\"midi_lengths (Optional *[*Tensor]) – Midi lrngth tensor (B,).\",\"duration_phn (Optional *[*Tensor]) – duration tensor (T_label).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (T_phone).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (T_phone).\",\"slur (Optional *[*Tensor]) – slur tensor (B, T_slur).\",\"pitch (Optional *[*Tensor]) – Pitch tensor (B, T_wav). - f0 sequence\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"Returns: Dict of features.\",\"Return type: Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, feats: Tensor | None = None, feats_lengths: Tensor | None = None, label: Tensor | None = None, label_lengths: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration_phn: Tensor | None = None, duration_phn_lengths: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_ruled_phn_lengths: Tensor | None = None, duration_syb: Tensor | None = None, duration_syb_lengths: Tensor | None = None, slur: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, ying: Tensor | None = None, ying_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, forward_generator: bool = True, **kwargs) → Dict[str, Any]\",\"Return generator or discriminator loss with dict format.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"label (Option *[*Tensor]) – Label tensor (B, T_label).\",\"label_lengths (Optional *[*Tensor]) – Label lrngth tensor (B,).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (B, T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (B, T_label).\",\"midi_lengths (Optional *[*Tensor]) – Midi lrngth tensor (B,).\",\"duration_phn (Optional *[*Tensor]) – duration tensor (B, T_label).\",\"duration_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (B, T_phone).\",\"duration_ruled_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (B, T_syllable).\",\"duration_syb_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"slur (Optional *[*Tensor]) – slur tensor (B, T_slur).\",\"pitch (Optional *[*Tensor]) – Pitch tensor (B, T_wav). - f0 sequence\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor]) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor]) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"forward_generator (bool) – Whether to forward generator.\",\"kwargs – “utt_id” is among the input.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(text: Tensor, singing: Tensor | None = None, label: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, duration_phn: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_syb: Tensor | None = None, slur: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **decode_config) → Dict[str, Tensor]\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (T_text).\",\"singing (Tensor) – Singing waveform tensor (T_wav).\",\"label (Option *[*Tensor]) – Label tensor (T_label).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (T_l abel).\",\"duration_phn (Optional *[*Tensor]) – duration tensor (T_label).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (T_phone).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (T_phone).\",\"slur (Optional *[*Tensor]) – slur tensor (T_phone).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (D,).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (1,).\",\"lids (Optional *[*Tensor]) – Language ID tensor (1,).\",\"pitch (Optional *[*Tensor) – Pitch tensor (T_wav).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"Returns: Dict of outputs.\",\"Return type: Dict[str, Tensor]\"]},\"1522\":{\"h\":\"espnet2.gan_svs.post_frontend.fused.FusedPostFrontends\",\"t\":[\"source\",\"class espnet2.gan_svs.post_frontend.fused.FusedPostFrontends(postfrontends=None, align_method='linear_projection', proj_dim=100, fs=16000, input_fs=24000)\",\"Bases: AbsFrontend\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1523\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\"]},\"1524\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.Generator_Harm\",\"t\":[\"source\",\"class espnet2.gan_svs.visinger2.visinger2_vocoder.Generator_Harm(hidden_channels: int = 192, n_harmonic: int = 64, kernel_size: int = 3, padding: int = 1, dropout_rate: float = 0.1, sample_rate: int = 22050, hop_size: int = 256)\",\"Bases: Module\",\"Initialize harmonic generator module.\",\"Parameters:\",\"hidden_channels (int) – Number of channels in the input and hidden layers.\",\"n_harmonic (int) – Number of harmonic channels.\",\"kernel_size (int) – Size of the convolutional kernel.\",\"padding (int) – Amount of padding added to the input.\",\"dropout_rate (float) – Dropout rate.\",\"sample_rate (int) – Sampling rate of the input audio.\",\"hop_size (int) – Hop size used in the analysis of the input audio.\",\"forward(f0, harm, mask)\",\"Generate harmonics from F0 and harmonic data.\",\"Parameters:\",\"f0 (Tensor) – Pitch (F0) tensor (B, 1, T).\",\"harm (Tensor) – Harmonic data tensor (B, hidden_channels, T).\",\"mask (Tensor) – Mask tensor for harmonic data (B, 1, T).\",\"Returns: Harmonic signal tensor (B, n_harmonic, T * hop_length).\",\"Return type: Tensor\"]},\"1525\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.Generator_Noise\",\"t\":[\"source\",\"class espnet2.gan_svs.visinger2.visinger2_vocoder.Generator_Noise(win_length: int = 1024, hop_length: int = 256, n_fft: int = 1024, hidden_channels: int = 192, kernel_size: int = 3, padding: int = 1, dropout_rate: float = 0.1)\",\"Bases: Module\",\"Initialize the Generator_Noise module.\",\"Parameters:\",\"win_length (int,optional) – Window length. If None, set to n_fft.\",\"hop_length (int) – Hop length.\",\"n_fft (int) – FFT size.\",\"hidden_channels (int) – Number of hidden representation channels.\",\"kernel_size (int) – Size of the convolutional kernel.\",\"padding (int) – Size of the padding applied to the input.\",\"dropout_rate (float) – Dropout rate.\",\"forward(x, mask)\",\"Forward Generator Noise.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, hidden_channels, T).\",\"mask (Tensor) – Mask tensor (B, 1, T).\",\"Returns: Output tensor (B, 1, T * hop_size).\",\"Return type: Tensor\"]},\"1526\":{\"h\":\"espnet2.gan_svs.joint.joint_score2wav.JointScore2Wav\",\"t\":[\"source\",\"class espnet2.gan_svs.joint.joint_score2wav.JointScore2Wav(idim: int, odim: int, segment_size: int = 32, sampling_rate: int = 22050, score2mel_type: str = 'xiaoice', score2mel_params: Dict[str, Any] = {'adim': 384, 'aheads': 4, 'conformer_activation_type': 'swish', 'conformer_dec_kernel_size': 31, 'conformer_enc_kernel_size': 7, 'conformer_pos_enc_layer_type': 'rel_pos', 'conformer_rel_pos_type': 'latest', 'conformer_self_attn_layer_type': 'rel_selfattn', 'decoder_concat_after': False, 'decoder_normalize_before': True, 'decoder_type': 'transformer', 'dlayers': 6, 'dunits': 1536, 'duration_predictor_chans': 384, 'duration_predictor_dropout_rate': 0.1, 'duration_predictor_kernel_size': 3, 'duration_predictor_layers': 2, 'elayers': 6, 'encoder_concat_after': False, 'encoder_normalize_before': True, 'encoder_type': 'transformer', 'eunits': 1536, 'init_dec_alpha': 1.0, 'init_enc_alpha': 1.0, 'init_type': 'xavier_uniform', 'lambda_dur': 0.1, 'lambda_mel': 1, 'lambda_pitch': 0.01, 'lambda_vuv': 0.01, 'langs': None, 'loss_function': 'XiaoiceSing2', 'loss_type': 'L1', 'midi_dim': 129, 'positionwise_conv_kernel_size': 1, 'positionwise_layer_type': 'conv1d', 'postnet_chans': 512, 'postnet_dropout_rate': 0.5, 'postnet_filts': 5, 'postnet_layers': 5, 'reduction_factor': 1, 'spk_embed_dim': None, 'spk_embed_integration_type': 'add', 'spks': None, 'tempo_dim': 500, 'transformer_dec_attn_dropout_rate': 0.1, 'transformer_dec_dropout_rate': 0.1, 'transformer_dec_positional_dropout_rate': 0.1, 'transformer_enc_attn_dropout_rate': 0.1, 'transformer_enc_dropout_rate': 0.1, 'transformer_enc_positional_dropout_rate': 0.1, 'use_batch_norm': True, 'use_cnn_in_conformer': True, 'use_macaron_style_in_conformer': True, 'use_masking': False, 'use_scaled_pos_enc': True, 'use_weighted_masking': False, 'zero_triu': False}, vocoder_type: str = 'hifigan_generator', vocoder_params: Dict[str, Any] = {'bias': True, 'channels': 512, 'global_channels': -1, 'kernel_size': 7, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'resblock_dilations': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'resblock_kernel_sizes': [3, 7, 11], 'upsample_kernel_sizes': [16, 16, 4, 4], 'upsample_scales': [8, 8, 2, 2], 'use_additional_convs': True, 'use_weight_norm': True}, use_pqmf: bool = False, pqmf_params: Dict[str, Any] = {'beta': 9.0, 'cutoff_ratio': 0.142, 'subbands': 4, 'taps': 62}, discriminator_type: str = 'hifigan_multi_scale_multi_period_discriminator', discriminator_params: Dict[str, Any] = {'follow_official_norm': False, 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'scale_discriminator_params': {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scales': 1}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, use_feat_match_loss: bool = True, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, use_mel_loss: bool = True, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 22050, 'hop_length': 256, 'log_base': None, 'n_fft': 1024, 'n_mels': 80, 'win_length': None, 'window': 'hann'}, lambda_score2mel: float = 1.0, lambda_adv: float = 1.0, lambda_feat_match: float = 2.0, lambda_mel: float = 45.0, cache_generator_outputs: bool = False)\",\"Bases: AbsGANSVS\",\"General class to jointly train score2mel and vocoder parts.\",\"Initialize JointScore2Wav module.\",\"Parameters:\",\"idim (int) – Input vocabrary size.\",\"odim (int) – Acoustic feature dimension. The actual output channels will be 1 since the model is the end-to-end text-to-wave model but for the compatibility odim is used to indicate the acoustic feature dimension.\",\"segment_size (int) – Segment size for random windowed inputs.\",\"sampling_rate (int) – Sampling rate, not used for the training but it will be referred in saving waveform during the inference.\",\"text2mel_type (str) – The text2mel model type.\",\"text2mel_params (Dict *[*str,Any]) – Parameter dict for text2mel model.\",\"use_pqmf (bool) – Whether to use PQMF for multi-band vocoder.\",\"pqmf_params (Dict *[*str,Any]) – Parameter dict for PQMF module.\",\"vocoder_type (str) – The vocoder model type.\",\"vocoder_params (Dict *[*str,Any]) – Parameter dict for vocoder model.\",\"discriminator_type (str) – Discriminator type.\",\"discriminator_params (Dict *[*str,Any]) – Parameter dict for discriminator.\",\"generator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for generator adversarial loss.\",\"discriminator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for discriminator adversarial loss.\",\"use_feat_match_loss (bool) – Whether to use feat match loss.\",\"feat_match_loss_params (Dict *[*str,Any]) – Parameter dict for feat match loss.\",\"use_mel_loss (bool) – Whether to use mel loss.\",\"mel_loss_params (Dict *[*str,Any]) – Parameter dict for mel loss.\",\"lambda_text2mel (float) – Loss scaling coefficient for text2mel model loss.\",\"lambda_adv (float) – Loss scaling coefficient for adversarial loss.\",\"lambda_feat_match (float) – Loss scaling coefficient for feat match loss.\",\"lambda_mel (float) – Loss scaling coefficient for mel loss.\",\"cache_generator_outputs (bool) – Whether to cache generator outputs.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: LongTensor | None = None, duration: Dict[str, Tensor] | None = None, slur: LongTensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, forward_generator: bool = True) → Dict[str, Any]\",\"Perform generator forward.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, Tmax).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, Tmax).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"slur (FloatTensor) – Batch of padded slur (B, Tmax).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(text: Tensor, feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, noise_scale: float = 0.667, noise_scale_dur: float = 0.8, alpha: float = 1.0, max_len: int | None = None, use_teacher_forcing: bool = False) → Dict[str, Tensor]\",\"Run inference.\",\"Parameters:\",\"text (Tensor) – Input text index tensor (T_text,).\",\"feats (Tensor) – Feature tensor (T_feats, aux_channels).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, Tmax).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"sids (Tensor) – Speaker index tensor (1,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (spk_embed_dim,).\",\"lids (Tensor) – Language index tensor (1,).\",\"noise_scale (float) – Noise scale value for flow.\",\"noise_scale_dur (float) – Noise scale value for duration predictor.\",\"alpha (float) – Alpha parameter to control the speed of generated singing.\",\"max_len (Optional *[*int]) – Maximum length.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"feat_gan (Tensor): Generated feature tensor (T_text, C).\",\"Return type: Dict[str, Tensor]\",\"property require_raw_singing\",\"Return whether or not singing is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\"]},\"1527\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.LayerNorm\",\"t\":[\"source\",\"class espnet2.gan_svs.visinger2.visinger2_vocoder.LayerNorm(channels, eps=1e-05)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1528\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1529\":{\"h\":\"espnet2.gan_svs.vits.length_regulator.LengthRegulator\",\"t\":[\"source\",\"class espnet2.gan_svs.vits.length_regulator.LengthRegulator(pad_value=0.0)\",\"Bases: Module\",\"Length Regulator\",\"Initilize length regulator module.\",\"Parameters:pad_value (float,optional) – Value used for padding.\",\"LR(x, duration, use_state_info=False)\",\"Length regulates input mel-spectrograms to match duration.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, dim, T).\",\"duration (Tensor) – Duration tensor (B, T).\",\"use_state_info (bool,optional) – Whether to use position information or not.\",\"Returns: Output tensor (B, dim, D_frame). Tensor: Output length (B,).\",\"Return type: Tensor\",\"expand(batch, predicted, use_state_info=False)\",\"Expand input mel-spectrogram based on the predicted duration.\",\"Parameters:\",\"batch (Tensor) – Input tensor (T, dim).\",\"predicted (Tensor) – Predicted duration tensor (T,).\",\"use_state_info (bool,optional) – Whether to use position information or not.\",\"Returns: Output tensor (D_frame, dim).\",\"Return type: Tensor\",\"forward(x, duration, use_state_info=False)\",\"Forward pass through the length regulator module.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, dim, T).\",\"duration (Tensor) – Duration tensor (B, T).\",\"use_state_info (bool,optional) – Whether to use position information or not.\",\"Returns: Output tensor (B, dim, D_frame). Tensor: Output length (B,).\",\"Return type: Tensor\"]},\"1530\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.MDC\",\"t\":[\"source\",\"class espnet2.gan_svs.avocodo.avocodo.MDC(in_channels, out_channels, strides, kernel_size, dilations, use_spectral_norm=False)\",\"Bases: Module\",\"Multiscale Dilated Convolution from https://arxiv.org/pdf/1609.07093.pdf\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1531\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1532\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.MDCDConfig\",\"t\":[\"source\",\"class espnet2.gan_svs.avocodo.avocodo.MDCDConfig(h)\",\"Bases: object\"]},\"1533\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.MelScale\",\"t\":[\"source\",\"class espnet2.gan_svs.visinger2.visinger2_vocoder.MelScale(n_mels: int = 128, sample_rate: int = 24000, f_min: float = 0.0, f_max: float | None = None, n_stft: int | None = None)\",\"Bases: Module\",\"Turn a normal STFT into a mel frequency STFT, using a conversion\",\"matrix. This uses triangular filter banks. User can control which device the filter bank (fb) is (e.g. fb.to(spec_f.device)). :param n_mels: Number of mel filterbanks. (Default: 128) :type n_mels: int, optional :param sample_rate: Sample rate of audio signal. (Default: 16000) :type sample_rate: int, optional :param f_min: Minimum frequency. (Default: 0.) :type f_min: float, optional :param f_max: Maximum frequency.\",\"(Default: sample_rate // 2)\",\"Parameters:n_stft (int,optional) – Number of bins in STFT. Calculated from first input if None is given. See n_fft in :class:Spectrogram. (Default: None)\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(specgram: Tensor) → Tensor\",\"Forward MelScale\",\"Parameters:specgram (Tensor) – A spectrogram STFT of dimension (…, freq, time).\",\"Returns: Mel frequency spectrogram of size (…, n_mels, time).\",\"Return type: Tensor\"]},\"1534\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.MultiFrequencyDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_svs.visinger2.visinger2_vocoder.MultiFrequencyDiscriminator(sample_rate: int = 22050, hop_lengths=[128, 256, 512], hidden_channels=[256, 512, 512], domain='double', mel_scale=True, divisors=[32, 16, 8, 4, 2, 1, 1], strides=[1, 2, 1, 2, 1, 2, 1])\",\"Bases: Module\",\"Multi-Frequency Discriminator module in UnivNet.\",\"Initialize Multi-Frequency Discriminator module.\",\"Parameters:\",\"hop_lengths (list) – List of hop lengths.\",\"hidden_channels (list) – List of number of channels in hidden layers.\",\"domain (str) – Domain of input signal. Default is “double”.\",\"mel_scale (bool) – Whether to use mel-scale frequency. Default is True.\",\"divisors (list) – List of divisors for each layer in the discriminator. Default is [32, 16, 8, 4, 2, 1, 1].\",\"strides (list) – List of strides for each layer in the discriminator. Default is [1, 2, 1, 2, 1, 2, 1].\",\"forward(x)\",\"Forward pass of Multi-Frequency Discriminator module.\",\"Parameters:x (Tensor) – Input tensor (B, 1, T * hop_size).\",\"Returns: List of feature maps.\",\"Return type: List[Tensor]\"]},\"1535\":{\"h\":\"espnet2.gan_svs.vits.phoneme_predictor.PhonemePredictor\",\"t\":[\"source\",\"class espnet2.gan_svs.vits.phoneme_predictor.PhonemePredictor(vocabs: int, hidden_channels: int = 192, attention_dim: int = 192, attention_heads: int = 2, linear_units: int = 768, blocks: int = 2, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 3, positional_encoding_layer_type: str = 'rel_pos', self_attention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', normalize_before: bool = True, use_macaron_style: bool = False, use_conformer_conv: bool = False, conformer_kernel_size: int = 7, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0)\",\"Bases: Module\",\"Phoneme Predictor module in VISinger.\",\"Initialize PhonemePredictor module.\",\"Parameters:\",\"vocabs (int) – The number of vocabulary.\",\"hidden_channels (int) – The number of hidden channels.\",\"attention_dim (int) – The number of attention dimension.\",\"attention_heads (int) – The number of attention heads.\",\"linear_units (int) – The number of linear units.\",\"blocks (int) – The number of encoder blocks.\",\"positionwise_layer_type (str) – The type of position-wise layer.\",\"positionwise_conv_kernel_size (int) – The size of position-wise convolution kernel.\",\"positional_encoding_layer_type (str) – The type of positional encoding layer.\",\"self_attention_layer_type (str) – The type of self-attention layer.\",\"activation_type (str) – The type of activation function.\",\"normalize_before (bool) – Whether to apply normalization before the position-wise layer or not.\",\"use_macaron_style (bool) – Whether to use macaron style or not.\",\"use_conformer_conv (bool) – Whether to use Conformer convolution or not.\",\"conformer_kernel_size (int) – The size of Conformer kernel.\",\"dropout_rate (float) – The dropout rate.\",\"positional_dropout_rate (float) – The dropout rate for positional encoding.\",\"attention_dropout_rate (float) – The dropout rate for attention.\",\"forward(x, x_mask)\",\"Perform forward propagation.\",\"Parameters:\",\"x (Tensor) – The input tensor of shape (B, dim, length).\",\"x_mask (Tensor) – The mask tensor for the input tensor of shape (B, length).\",\"Returns: The predicted phoneme tensor of shape (length, B, vocab_size).\",\"Return type: Tensor\"]},\"1536\":{\"h\":\"espnet2.gan_svs.vits.prior_decoder.PriorDecoder\",\"t\":[\"source\",\"class espnet2.gan_svs.vits.prior_decoder.PriorDecoder(out_channels: int = 384, attention_dim: int = 192, attention_heads: int = 2, linear_units: int = 768, blocks: int = 6, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 3, positional_encoding_layer_type: str = 'rel_pos', self_attention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', normalize_before: bool = True, use_macaron_style: bool = False, use_conformer_conv: bool = False, conformer_kernel_size: int = 7, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0, global_channels: int = 0)\",\"Bases: Module\",\"Initialize prior decoder module.\",\"Parameters:\",\"out_channels (int) – Output channels of the prior decoder. Defaults to 384.\",\"attention_dim (int) – Dimension of the attention mechanism. Defaults to 192.\",\"attention_heads (int) – Number of attention heads. Defaults to 2.\",\"linear_units (int) – Number of units in the linear layer. Defaults to 768.\",\"blocks (int) – Number of blocks in the encoder. Defaults to 6.\",\"positionwise_layer_type (str) – Type of the positionwise layer. Defaults to “conv1d”.\",\"positionwise_conv_kernel_size (int) – Kernel size of the positionwise convolutional layer. Defaults to 3.\",\"positional_encoding_layer_type (str) – Type of positional encoding layer. Defaults to “rel_pos”.\",\"self_attention_layer_type (str) – Type of self-attention layer. Defaults to “rel_selfattn”.\",\"activation_type (str) – Type of activation. Defaults to “swish”.\",\"normalize_before (bool) – Flag for normalization. Defaults to True.\",\"use_macaron_style (bool) – Flag for macaron style. Defaults to False.\",\"use_conformer_conv (bool) – Flag for using conformer convolution. Defaults to False.\",\"conformer_kernel_size (int) – Kernel size for conformer convolution. Defaults to 7.\",\"dropout_rate (float) – Dropout rate. Defaults to 0.1.\",\"positional_dropout_rate (float) – Dropout rate for positional encoding. Defaults to 0.0.\",\"attention_dropout_rate (float) – Dropout rate for attention. Defaults to 0.0.\",\"global_channels (int) – Number of global channels. Defaults to 0.\",\"forward(x, x_lengths, g=None)\",\"Forward pass of the PriorDecoder module.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, attention_dim + 2, T).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"g (Tensor) – Tensor for multi-singer. (B, global_channels, 1)\",\"Returns: Output tensor (B, out_channels, T). Tensor: Output mask tensor (B, 1, T).\",\"Return type: Tensor\"]},\"1537\":{\"h\":\"espnet2.gan_svs.vits.modules.Projection\",\"t\":[\"source\",\"class espnet2.gan_svs.vits.modules.Projection(hidden_channels, out_channels)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, x_mask)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1538\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1539\":{\"h\":\"espnet2.gan_svs.post_frontend.s3prl.S3prlPostFrontend\",\"t\":[\"source\",\"class espnet2.gan_svs.post_frontend.s3prl.S3prlPostFrontend(fs: int | str = 16000, input_fs: int | str = 24000, postfrontend_conf: dict | None = {'badim': 320, 'bdropout_rate': 0.0, 'blayers': 3, 'bnmask': 2, 'bprojs': 320, 'btype': 'blstmp', 'bunits': 300, 'delay': 3, 'ref_channel': -1, 'taps': 5, 'use_beamformer': False, 'use_dnn_mask_for_wpe': True, 'use_wpe': False, 'wdropout_rate': 0.0, 'wlayers': 3, 'wprojs': 320, 'wtype': 'blstmp', 'wunits': 300}, download_dir: str | None = None, multilayer_feature: bool = False, layer: int = -1)\",\"Bases: AbsFrontend\",\"Pretrained SSL model for VISinger2 Plus.\",\"Based on S3prlFrontend, S3prlPostFrontend added a resampler to resample the input audio to the sample rate of the pretrained model.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1540\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\",\"reload_pretrained_parameters()\"]},\"1541\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.SBD\",\"t\":[\"source\",\"class espnet2.gan_svs.avocodo.avocodo.SBD(h, use_spectral_norm=False)\",\"Bases: Module\",\"SBD (Sub-band Discriminator) from https://arxiv.org/pdf/2206.13404.pdf\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(y, y_hat)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1542\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1543\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.SBDBlock\",\"t\":[\"source\",\"class espnet2.gan_svs.avocodo.avocodo.SBDBlock(segment_dim, strides, filters, kernel_size, dilations, use_spectral_norm=False)\",\"Bases: Module\",\"SBD (Sub-band Discriminator) Block\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1544\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1545\":{\"h\":\"espnet2.gan_svs.uhifigan.sine_generator.SineGen\",\"t\":[\"source\",\"class espnet2.gan_svs.uhifigan.sine_generator.SineGen(sample_rate, harmonic_num=0, sine_amp=0.1, noise_std=0.003, voiced_threshold=0, flag_for_pulse=False)\",\"Bases: Module\",\"Definition of sine generator\",\"SineGen(samp_rate, harmonic_num = 0, : sine_amp = 0.1, noise_std = 0.003, voiced_threshold = 0, flag_for_pulse=False)\",\"sample_rate: sampling rate in Hz harmonic_num: number of harmonic overtones (default 0) sine_amp: amplitude of sine-wavefrom (default 0.1) noise_std: std of Gaussian noise (default 0.003) voiced_thoreshold: F0 threshold for U/V classification (default 0) flag_for_pulse: this SinGen is used inside PulseGen (default False)\",\"Note: when flag_for_pulse is True, the first time step of a voiced : segment is always sin(np.pi) or cos(0)\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(f0)\",\"Forward SineGen.\",\"sine_tensor, uv = forward(f0) input F0: tensor(batchsize=1, length, dim=1)\",\"f0 for unvoiced steps should be 0\",\"output sine_tensor: tensor(batchsize=1, length, dim) output uv: tensor(batchsize=1, length, 1)\"]},\"1546\":{\"h\":\"espnet2.gan_svs.vits.text_encoder.TextEncoder\",\"t\":[\"source\",\"class espnet2.gan_svs.vits.text_encoder.TextEncoder(vocabs: int, attention_dim: int = 192, attention_heads: int = 2, linear_units: int = 768, blocks: int = 6, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 3, positional_encoding_layer_type: str = 'rel_pos', self_attention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', normalize_before: bool = True, use_macaron_style: bool = False, use_conformer_conv: bool = False, conformer_kernel_size: int = 7, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0, use_slur=True)\",\"Bases: Module\",\"Text encoder module in VISinger.\",\"This is a module of text encoder described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Instead of the relative positional Transformer, we use conformer architecture as the encoder module, which contains additional convolution layers.\",\"Initialize TextEncoder module.\",\"Parameters:\",\"vocabs (int) – Vocabulary size.\",\"attention_dim (int) – Attention dimension.\",\"attention_heads (int) – Number of attention heads.\",\"linear_units (int) – Number of linear units of positionwise layers.\",\"blocks (int) – Number of encoder blocks.\",\"positionwise_layer_type (str) – Positionwise layer type.\",\"positionwise_conv_kernel_size (int) – Positionwise layer’s kernel size.\",\"positional_encoding_layer_type (str) – Positional encoding layer type.\",\"self_attention_layer_type (str) – Self-attention layer type.\",\"activation_type (str) – Activation function type.\",\"normalize_before (bool) – Whether to apply LayerNorm before attention.\",\"use_macaron_style (bool) – Whether to use macaron style components.\",\"use_conformer_conv (bool) – Whether to use conformer conv layers.\",\"conformer_kernel_size (int) – Conformer’s conv kernel size.\",\"dropout_rate (float) – Dropout rate.\",\"positional_dropout_rate (float) – Dropout rate for positional encoding.\",\"attention_dropout_rate (float) – Dropout rate for attention.\",\"use_slur (bool) – Whether to use slur embedding.\",\"forward(phone: Tensor, phone_lengths: Tensor, midi_id: Tensor, dur: Tensor, slur: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"phone (Tensor) – Input index tensor (B, T_text).\",\"phone_lengths (Tensor) – Length tensor (B,).\",\"midi_id (Tensor) – Input midi tensor (B, T_text).\",\"dur (Tensor) – Input duration tensor (B, T_text).\",\"Returns: Encoded hidden representation (B, attention_dim, T_text). Tensor: Mask tensor for padded part (B, 1, T_text). Tensor: Encoded hidden representation for duration\",\"(B, attention_dim, T_text).\",\"Tensor: Encoded hidden representation for pitch : (B, attention_dim, T_text).\",\"Return type: Tensor\"]},\"1547\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.TorchSTFT\",\"t\":[\"source\",\"class espnet2.gan_svs.visinger2.visinger2_vocoder.TorchSTFT(sample_rate, fft_size, hop_size, win_size, normalized=False, domain='linear', mel_scale=False, ref_level_db=20, min_level_db=-100)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"complex(x)\",\"transform(x)\"]},\"1548\":{\"h\":\"espnet2.gan_svs.uhifigan.uhifigan.UHiFiGANGenerator\",\"t\":[\"source\",\"class espnet2.gan_svs.uhifigan.uhifigan.UHiFiGANGenerator(in_channels=80, out_channels=1, channels=512, global_channels: int = -1, kernel_size=7, downsample_scales=(2, 2, 8, 8), downsample_kernel_sizes=(4, 4, 16, 16), upsample_scales=(8, 8, 2, 2), upsample_kernel_sizes=(16, 16, 4, 4), resblock_kernel_sizes=(3, 7, 11), resblock_dilations=[(1, 3, 5), (1, 3, 5), (1, 3, 5)], projection_filters: List[int] = [0, 1, 1, 1], projection_kernels: List[int] = [0, 5, 7, 11], dropout=0.3, use_additional_convs=True, bias=True, nonlinear_activation='LeakyReLU', nonlinear_activation_params={'negative_slope': 0.1}, use_causal_conv=False, use_weight_norm=True, use_avocodo=False)\",\"Bases: Module\",\"UHiFiGAN generator module.\",\"Initialize Unet-based HiFiGANGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"channels (int) – Number of hidden representation channels.\",\"global_channels (int) – Number of global conditioning channels.\",\"kernel_size (int) – Kernel size of initial and final conv layer.\",\"upsample_scales (list) – List of upsampling scales.\",\"upsample_kernel_sizes (list) – List of kernel sizes for upsampling layers.\",\"resblock_kernel_sizes (list) – List of kernel sizes for residual blocks.\",\"resblock_dilations (list) – List of dilation list for residual blocks.\",\"use_additional_convs (bool) – Whether to use additional conv layers in residual blocks.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (dict) – Hyperparameters for activation function.\",\"use_causal_conv (bool) – Whether to use causal structure.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c=None, f0=None, excitation=None, g: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"c (Tensor) – Input tensor (B, in_channels, T).\",\"f0 (Tensor) – Input tensor (B, 1, T).\",\"excitation (Tensor) – Input tensor (B, frame_len, T).\",\"Returns: Output tensor (B, out_channels, T).\",\"Return type: Tensor\",\"inference(excitation=None, f0=None, c=None, normalize_before=False)\",\"Perform inference.\",\"Parameters:\",\"c (Union *[*Tensor,ndarray]) – Input tensor (T, in_channels).\",\"normalize_before (bool) – Whether to perform normalization.\",\"Returns: Output tensor (T ** prod(upsample_scales), out_channels).\",\"Return type: Tensor\",\"register_stats(stats)\",\"Register stats for de-normalization as buffer.\",\"Parameters:stats (str) – Path of statistics file (“.npy” or “.h5”).\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"This initialization follows the official implementation manner. https://github.com/jik876/hifi-gan/blob/master/models.py\"]},\"1549\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.VISinger2Discriminator\",\"t\":[\"source\",\"class espnet2.gan_svs.visinger2.visinger2_vocoder.VISinger2Discriminator(scales: int = 1, scale_downsample_pooling: str = 'AvgPool1d', scale_downsample_pooling_params: Dict[str, Any] = {'kernel_size': 4, 'padding': 2, 'stride': 2}, scale_discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1}, follow_official_norm: bool = True, periods: List[int] = [2, 3, 5, 7, 11], period_discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, multi_freq_disc_params: Dict[str, Any] = {'divisors': [32, 16, 8, 4, 2, 1, 1], 'domain': 'double', 'hidden_channels': [256, 512, 512], 'hop_length_factors': [4, 8, 16], 'mel_scale': True, 'sample_rate': 22050, 'strides': [1, 2, 1, 2, 1, 2, 1]})\",\"Bases: Module\",\"Discriminator module for VISinger2, including MSD, MPD, and MFD.\",\"Parameters:\",\"scales (int) – Number of scales to be used in the multi-scale discriminator.\",\"scale_downsample_pooling (str) – Type of pooling used for downsampling.\",\"scale_downsample_pooling_params (Dict *[*str,Any]) – Parameters for the downsampling pooling layer.\",\"scale_discriminator_params (Dict *[*str,Any]) – Parameters for the scale discriminator.\",\"follow_official_norm (bool) – Whether to follow the official normalization.\",\"periods (List *[*int]) – List of periods to be used in the multi-period discriminator.\",\"period_discriminator_params (Dict *[*str,Any]) – Parameters for the period discriminator.\",\"multi_freq_disc_params (Dict *[*str,Any]) – Parameters for the multi-frequency discriminator.\",\"use_spectral_norm (bool) – Whether to use spectral normalization or not.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1550\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1551\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.VISinger2VocoderGenerator\",\"t\":[\"source\",\"class espnet2.gan_svs.visinger2.visinger2_vocoder.VISinger2VocoderGenerator(in_channels: int = 80, out_channels: int = 1, channels: int = 512, global_channels: int = -1, kernel_size: int = 7, upsample_scales: List[int] = [8, 8, 2, 2], upsample_kernel_sizes: List[int] = [16, 16, 4, 4], resblock_kernel_sizes: List[int] = [3, 7, 11], resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], n_harmonic: int = 64, use_additional_convs: bool = True, bias: bool = True, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1}, use_weight_norm: bool = True)\",\"Bases: Module\",\"Initialize HiFiGANGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"channels (int) – Number of hidden representation channels.\",\"global_channels (int) – Number of global conditioning channels.\",\"kernel_size (int) – Kernel size of initial and final conv layer.\",\"upsample_scales (List *[*int]) – List of upsampling scales.\",\"upsample_kernel_sizes (List *[*int]) – List of kernel sizes for upsample layers.\",\"resblock_kernel_sizes (List *[*int]) – List of kernel sizes for residual blocks.\",\"resblock_dilations (List *[*List *[*int]]) – List of list of dilations for residual blocks.\",\"n_harmonic (int) – Number of harmonics used to synthesize a sound signal.\",\"use_additional_convs (bool) – Whether to use additional conv layers in residual blocks.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c, ddsp, g: Tensor | None = None) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"c (Tensor) – Input tensor (B, in_channels, T).\",\"ddsp (Tensor) – Input tensor (B, n_harmonic + 2, T * hop_length).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"Returns: Output tensor (B, out_channels, T).\",\"Return type: Tensor\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"This initialization follows the official implementation manner. https://github.com/jik876/hifi-gan/blob/master/models.py\"]},\"1552\":{\"h\":\"espnet2.gan_svs.vits.generator.VISingerGenerator\",\"t\":[\"source\",\"class espnet2.gan_svs.vits.generator.VISingerGenerator(vocabs: int, aux_channels: int = 513, hidden_channels: int = 192, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, global_channels: int = -1, segment_size: int = 32, text_encoder_attention_heads: int = 2, text_encoder_ffn_expand: int = 4, text_encoder_blocks: int = 6, text_encoder_positionwise_layer_type: str = 'conv1d', text_encoder_positionwise_conv_kernel_size: int = 1, text_encoder_positional_encoding_layer_type: str = 'rel_pos', text_encoder_self_attention_layer_type: str = 'rel_selfattn', text_encoder_activation_type: str = 'swish', text_encoder_normalize_before: bool = True, text_encoder_dropout_rate: float = 0.1, text_encoder_positional_dropout_rate: float = 0.0, text_encoder_attention_dropout_rate: float = 0.0, text_encoder_conformer_kernel_size: int = 7, use_macaron_style_in_text_encoder: bool = True, use_conformer_conv_in_text_encoder: bool = True, decoder_kernel_size: int = 7, decoder_channels: int = 512, decoder_downsample_scales: List[int] = [2, 2, 8, 8], decoder_downsample_kernel_sizes: List[int] = [4, 4, 16, 16], decoder_upsample_scales: List[int] = [8, 8, 2, 2], decoder_upsample_kernel_sizes: List[int] = [16, 16, 4, 4], decoder_resblock_kernel_sizes: List[int] = [3, 7, 11], decoder_resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], use_avocodo=False, projection_filters: List[int] = [0, 1, 1, 1], projection_kernels: List[int] = [0, 5, 7, 11], n_harmonic: int = 64, use_weight_norm_in_decoder: bool = True, posterior_encoder_kernel_size: int = 5, posterior_encoder_layers: int = 16, posterior_encoder_stacks: int = 1, posterior_encoder_base_dilation: int = 1, posterior_encoder_dropout_rate: float = 0.0, use_weight_norm_in_posterior_encoder: bool = True, flow_flows: int = 4, flow_kernel_size: int = 5, flow_base_dilation: int = 1, flow_layers: int = 4, flow_dropout_rate: float = 0.0, use_weight_norm_in_flow: bool = True, use_only_mean_in_flow: bool = True, generator_type: str = 'visinger', vocoder_generator_type: str = 'hifigan', fs: int = 22050, hop_length: int = 256, win_length: int | None = 1024, n_fft: int = 1024, use_phoneme_predictor: bool = False, expand_f0_method: str = 'repeat', hubert_channels: int | None = 0)\",\"Bases: Module\",\"Generator module in VISinger.\",\"Initialize VITS generator module.\",\"Parameters:\",\"vocabs (int) – Input vocabulary size.\",\"aux_channels (int) – Number of acoustic feature channels.\",\"hidden_channels (int) – Number of hidden channels.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"global_channels (int) – Number of global conditioning channels.\",\"segment_size (int) – Segment size for decoder.\",\"text_encoder_attention_heads (int) – Number of heads in conformer block of text encoder.\",\"text_encoder_ffn_expand (int) – Expansion ratio of FFN in conformer block of text encoder.\",\"text_encoder_blocks (int) – Number of conformer blocks in text encoder.\",\"text_encoder_positionwise_layer_type (str) – Position-wise layer type in conformer block of text encoder.\",\"text_encoder_positionwise_conv_kernel_size (int) – Position-wise convolution kernel size in conformer block of text encoder. Only used when the above layer type is conv1d or conv1d-linear.\",\"text_encoder_positional_encoding_layer_type (str) – Positional encoding layer type in conformer block of text encoder.\",\"text_encoder_self_attention_layer_type (str) – Self-attention layer type in conformer block of text encoder.\",\"text_encoder_activation_type (str) – Activation function type in conformer block of text encoder.\",\"text_encoder_normalize_before (bool) – Whether to apply layer norm before self-attention in conformer block of text encoder.\",\"text_encoder_dropout_rate (float) – Dropout rate in conformer block of text encoder.\",\"text_encoder_positional_dropout_rate (float) – Dropout rate for positional encoding in conformer block of text encoder.\",\"text_encoder_attention_dropout_rate (float) – Dropout rate for attention in conformer block of text encoder.\",\"text_encoder_conformer_kernel_size (int) – Conformer conv kernel size. It will be used when only use_conformer_conv_in_text_encoder = True.\",\"use_macaron_style_in_text_encoder (bool) – Whether to use macaron style FFN in conformer block of text encoder.\",\"use_conformer_conv_in_text_encoder (bool) – Whether to use covolution in conformer block of text encoder.\",\"decoder_kernel_size (int) – Decoder kernel size.\",\"decoder_channels (int) – Number of decoder initial channels.\",\"decoder_downsample_scales (List *[*int]) – List of downsampling scales in decoder.\",\"decoder_downsample_kernel_sizes (List *[*int]) – List of kernel sizes for downsampling layers in decoder.\",\"decoder_upsample_scales (List *[*int]) – List of upsampling scales in decoder.\",\"decoder_upsample_kernel_sizes (List *[*int]) – List of kernel sizes for upsampling layers in decoder.\",\"decoder_resblock_kernel_sizes (List *[*int]) – List of kernel sizes for resblocks in decoder.\",\"decoder_resblock_dilations (List *[*List *[*int]]) – List of list of dilations for resblocks in decoder.\",\"use_avocodo (bool) – Whether to use Avocodo model in the generator.\",\"projection_filters (List *[*int]) – List of projection filter sizes.\",\"projection_kernels (List *[*int]) – List of projection kernel sizes.\",\"n_harmonic (int) – Number of harmonic components.\",\"use_weight_norm_in_decoder (bool) – Whether to apply weight normalization in decoder.\",\"posterior_encoder_kernel_size (int) – Posterior encoder kernel size.\",\"posterior_encoder_layers (int) – Number of layers of posterior encoder.\",\"posterior_encoder_stacks (int) – Number of stacks of posterior encoder.\",\"posterior_encoder_base_dilation (int) – Base dilation of posterior encoder.\",\"posterior_encoder_dropout_rate (float) – Dropout rate for posterior encoder.\",\"use_weight_norm_in_posterior_encoder (bool) – Whether to apply weight normalization in posterior encoder.\",\"flow_flows (int) – Number of flows in flow.\",\"flow_kernel_size (int) – Kernel size in flow.\",\"flow_base_dilation (int) – Base dilation in flow.\",\"flow_layers (int) – Number of layers in flow.\",\"flow_dropout_rate (float) – Dropout rate in flow\",\"use_weight_norm_in_flow (bool) – Whether to apply weight normalization in flow.\",\"use_only_mean_in_flow (bool) – Whether to use only mean in flow.\",\"generator_type (str) – Type of generator to use for the model.\",\"vocoder_generator_type (str) – Type of vocoder generator to use for the model.\",\"fs (int) – Sample rate of the audio.\",\"hop_length (int) – Number of samples between successive frames in STFT.\",\"win_length (int) – Window size of the STFT.\",\"n_fft (int) – Length of the FFT window to be used.\",\"use_phoneme_predictor (bool) – Whether to use phoneme predictor in the model.\",\"expand_f0_method (str) – The method used to expand F0. Use “repeat” or “interpolation”.\",\"hubert_channels (int) – Number of channels in the Hubert model. This is used in VISinger2 Plus.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, label: Tensor | None = None, label_lengths: Tensor | None = None, melody: Tensor | None = None, gt_dur: Tensor | None = None, score_dur: Tensor | None = None, slur: Tensor | None = None, pitch: Tensor | None = None, ying: Tensor | None = None, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, Tmax).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"label (LongTensor) – Batch of padded label ids (B, Tmax).\",\"label_lengths (LongTensor) – Batch of the lengths of padded label ids (B, ).\",\"melody (LongTensor) – Batch of padded midi (B, Tmax).\",\"gt_dur (LongTensor) – Batch of padded ground truth duration (B, Tmax).\",\"score_dur (LongTensor) – Batch of padded score duration (B, Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"ying (Optional *[*Tensor]) – Batch of padded ying (B, Tmax).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"Returns: Waveform tensor (B, 1, segment_size * upsample_factor). Tensor: Duration negative log-likelihood (NLL) tensor (B,). Tensor: Monotonic attention weight tensor (B, 1, T_feats, T_text). Tensor: Segments start index tensor (B,). Tensor: Text mask tensor (B, 1, T_text). Tensor: Feature mask tensor (B, 1, T_feats). tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]: \",\"Tensor: Posterior encoder hidden representation (B, H, T_feats).\",\"Tensor: Flow hidden representation (B, H, T_feats).\",\"Tensor: Expanded text encoder projected mean (B, H, T_feats).\",\"Tensor: Expanded text encoder projected scale (B, H, T_feats).\",\"Tensor: Posterior encoder projected mean (B, H, T_feats).\",\"Tensor: Posterior encoder projected scale (B, H, T_feats).\",\"Return type: Tensor\",\"inference(text: Tensor, text_lengths: Tensor, feats: Tensor | None = None, feats_lengths: Tensor | None = None, label: Tensor | None = None, label_lengths: Tensor | None = None, melody: Tensor | None = None, score_dur: Tensor | None = None, slur: Tensor | None = None, gt_dur: Tensor | None = None, pitch: Tensor | None = None, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, noise_scale: float = 0.667, noise_scale_dur: float = 0.8, alpha: float = 1.0, max_len: int | None = None, use_teacher_forcing: bool = False) → Tuple[Tensor, Tensor, Tensor]\",\"Run inference.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, Tmax).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"label (LongTensor) – Batch of padded label ids (B, Tmax).\",\"label_lengths (LongTensor) – Batch of the lengths of padded label ids (B, ).\",\"melody (LongTensor) – Batch of padded midi (B, Tmax).\",\"gt_dur (LongTensor) – Batch of padded ground truth duration (B, Tmax).\",\"score_dur (LongTensor) – Batch of padded score duration (B, Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"ying (Optional *[*Tensor]) – Batch of padded ying (B, Tmax).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"noise_scale (float) – Noise scale parameter for flow.\",\"noise_scale_dur (float) – Noise scale parameter for duration predictor.\",\"alpha (float) – Alpha parameter to control the speed of generated speech.\",\"max_len (Optional *[*int]) – Maximum length of acoustic feature sequence.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Generated waveform tensor (B, T_wav).\",\"Return type: Tensor\"]},\"1553\":{\"h\":\"espnet2.gan_svs.vits.vits.VITS\",\"t\":[\"source\",\"class espnet2.gan_svs.vits.vits.VITS(idim: int, odim: int, sampling_rate: int = 22050, generator_type: str = 'visinger', vocoder_generator_type: str = 'hifigan', generator_params: Dict[str, Any] = {'decoder_channels': 512, 'decoder_kernel_size': 7, 'decoder_resblock_dilations': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'decoder_resblock_kernel_sizes': [3, 7, 11], 'decoder_upsample_kernel_sizes': [16, 16, 4, 4], 'decoder_upsample_scales': [8, 8, 2, 2], 'expand_f0_method': 'repeat', 'flow_base_dilation': 1, 'flow_dropout_rate': 0.0, 'flow_flows': 4, 'flow_kernel_size': 5, 'flow_layers': 4, 'global_channels': -1, 'hidden_channels': 192, 'hubert_channels': 0, 'langs': None, 'posterior_encoder_base_dilation': 1, 'posterior_encoder_dropout_rate': 0.0, 'posterior_encoder_kernel_size': 5, 'posterior_encoder_layers': 16, 'posterior_encoder_stacks': 1, 'projection_filters': [0, 1, 1, 1], 'projection_kernels': [0, 5, 7, 11], 'segment_size': 32, 'spk_embed_dim': None, 'spks': None, 'text_encoder_activation_type': 'swish', 'text_encoder_attention_dropout_rate': 0.0, 'text_encoder_attention_heads': 2, 'text_encoder_blocks': 6, 'text_encoder_conformer_kernel_size': 7, 'text_encoder_dropout_rate': 0.1, 'text_encoder_ffn_expand': 4, 'text_encoder_normalize_before': True, 'text_encoder_positional_dropout_rate': 0.0, 'text_encoder_positional_encoding_layer_type': 'rel_pos', 'text_encoder_positionwise_conv_kernel_size': 1, 'text_encoder_positionwise_layer_type': 'conv1d', 'text_encoder_self_attention_layer_type': 'rel_selfattn', 'use_conformer_conv_in_text_encoder': True, 'use_macaron_style_in_text_encoder': True, 'use_only_mean_in_flow': True, 'use_phoneme_predictor': False, 'use_weight_norm_in_decoder': True, 'use_weight_norm_in_flow': True, 'use_weight_norm_in_posterior_encoder': True}, discriminator_type: str = 'hifigan_multi_scale_multi_period_discriminator', discriminator_params: Dict[str, Any] = {'avocodo': {'combd': {'combd_d_d': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'combd_d_g': [[1, 4, 16, 64, 256, 1], [1, 4, 16, 64, 256, 1], [1, 4, 16, 64, 256, 1]], 'combd_d_k': [[7, 11, 11, 11, 11, 5], [11, 21, 21, 21, 21, 5], [15, 41, 41, 41, 41, 5]], 'combd_d_p': [[3, 5, 5, 5, 5, 2], [5, 10, 10, 10, 10, 2], [7, 20, 20, 20, 20, 2]], 'combd_d_s': [[1, 1, 4, 4, 4, 1], [1, 1, 4, 4, 4, 1], [1, 1, 4, 4, 4, 1]], 'combd_h_u': [[16, 64, 256, 1024, 1024, 1024], [16, 64, 256, 1024, 1024, 1024], [16, 64, 256, 1024, 1024, 1024]], 'combd_op_f': [1, 1, 1], 'combd_op_g': [1, 1, 1], 'combd_op_k': [3, 3, 3]}, 'pqmf_config': {'lv1': [2, 256, 0.25, 10.0], 'lv2': [4, 192, 0.13, 10.0]}, 'sbd': {'pqmf_config': {'fsbd': [64, 256, 0.1, 9.0], 'sbd': [16, 256, 0.03, 10.0]}, 'sbd_band_ranges': [[0, 6], [0, 11], [0, 16], [0, 64]], 'sbd_dilations': [[[5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11]], [[3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 3, 5], [2, 3, 5]]], 'sbd_filters': [[64, 128, 256, 256, 256], [64, 128, 256, 256, 256], [64, 128, 256, 256, 256], [32, 64, 128, 128, 128]], 'sbd_kernel_sizes': [[[7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7], [7, 7, 7]], [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]], [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]]], 'sbd_strides': [[1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1]], 'sbd_transpose': [False, False, False, True], 'use_sbd': True}}, 'hifigan_multi_scale_multi_period_discriminator': {'follow_official_norm': False, 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'scale_discriminator_params': {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scales': 1}}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 22050, 'hop_length': 256, 'log_base': None, 'n_fft': 1024, 'n_mels': 80, 'win_length': None, 'window': 'hann'}, lambda_adv: float = 1.0, lambda_mel: float = 45.0, lambda_feat_match: float = 2.0, lambda_dur: float = 0.1, lambda_kl: float = 1.0, lambda_pitch: float = 10.0, lambda_phoneme: float = 1.0, lambda_c_yin: float = 45.0, cache_generator_outputs: bool = True)\",\"Bases: AbsGANSVS\",\"VITS module (generator + discriminator).\",\"This is a module of VITS described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Initialize VITS module.\",\"Parameters:\",\"idim (int) – Input vocabrary size.\",\"odim (int) – Acoustic feature dimension. The actual output channels will be 1 since VITS is the end-to-end text-to-wave model but for the compatibility odim is used to indicate the acoustic feature dimension.\",\"sampling_rate (int) – Sampling rate, not used for the training but it will be referred in saving waveform during the inference.\",\"generator_type (str) – Generator type.\",\"vocoder_generator_type (str) – Type of vocoder generator to use in the model.\",\"generator_params (Dict *[*str,Any]) – Parameter dict for generator.\",\"discriminator_type (str) – Discriminator type.\",\"discriminator_params (Dict *[*str,Any]) – Parameter dict for discriminator.\",\"generator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for generator adversarial loss.\",\"discriminator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for discriminator adversarial loss.\",\"feat_match_loss_params (Dict *[*str,Any]) – Parameter dict for feat match loss.\",\"mel_loss_params (Dict *[*str,Any]) – Parameter dict for mel loss.\",\"lambda_adv (float) – Loss scaling coefficient for adversarial loss.\",\"lambda_mel (float) – Loss scaling coefficient for mel spectrogram loss.\",\"lambda_feat_match (float) – Loss scaling coefficient for feat match loss.\",\"lambda_dur (float) – Loss scaling coefficient for duration loss.\",\"lambda_kl (float) – Loss scaling coefficient for KL divergence loss.\",\"lambda_pitch (float) – Loss scaling coefficient for pitch loss.\",\"lambda_phoneme (float) – Loss scaling coefficient for phoneme loss.\",\"lambda_c_yin (float) – Loss scaling coefficient for yin loss.\",\"cache_generator_outputs (bool) – Whether to cache generator outputs.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, ssl_feats: Tensor | None = None, ssl_feats_lengths: Tensor | None = None, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: LongTensor | None = None, ying: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: LongTensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, forward_generator: bool = True) → Dict[str, Any]\",\"Perform generator forward.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"ssl_feats (Tensor) – SSL feature tensor (B, T_feats, hubert_channels).\",\"ssl_feats_lengths (Tensor) – SSL feature length tensor (B,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, T_text).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, T_text).\",\"pitch (FloatTensor) – Batch of padded f0 (B, T_feats).\",\"ying (Optional *[*Tensor]) – Batch of padded ying (B, T_feats).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, T_text).\",\"slur (FloatTensor) – Batch of padded slur (B, T_text).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(text: Tensor, feats: Tensor | None = None, ssl_feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, noise_scale: float = 0.667, noise_scale_dur: float = 0.8, alpha: float = 1.0, max_len: int | None = None, use_teacher_forcing: bool = False) → Dict[str, Tensor]\",\"Run inference.\",\"Parameters:\",\"text (Tensor) – Input text index tensor (T_text,).\",\"feats (Tensor) – Feature tensor (T_feats, aux_channels).\",\"ssl_feats (Tensor) – SSL Feature tensor (T_feats, hubert_channels).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, T_text).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, T_text).\",\"pitch (FloatTensor) – Batch of padded f0 (B, T_feats).\",\"slur (LongTensor) – Batch of padded slur (B, T_text).\",\"sids (Tensor) – Speaker index tensor (1,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (spk_embed_dim,).\",\"lids (Tensor) – Language index tensor (1,).\",\"noise_scale (float) – Noise scale value for flow.\",\"noise_scale_dur (float) – Noise scale value for duration predictor.\",\"alpha (float) – Alpha parameter to control the speed of generated singing.\",\"max_len (Optional *[*int]) – Maximum length.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, T_text).\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"Return type: Dict[str, Tensor]\",\"property require_raw_singing\",\"Return whether or not singing is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\"]},\"1554\":{\"h\":\"espnet2.gan_svs.pits.modules.WN\",\"t\":[\"source\",\"class espnet2.gan_svs.pits.modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=0, p_dropout=0)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, x_mask, g=None, **kwargs)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1555\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels)\",\"remove_weight_norm()\"]},\"1556\":{\"h\":\"espnet2.gan_svs.pits.ying_decoder.YingDecoder\",\"t\":[\"source\",\"class espnet2.gan_svs.pits.ying_decoder.YingDecoder(hidden_channels, kernel_size, dilation_rate, n_layers, yin_start, yin_scope, yin_shift_range, gin_channels=0)\",\"Bases: Module\",\"Ying decoder module.\",\"Initialize the YingDecoder module.\",\"Parameters:\",\"hidden_channels (int) – Number of hidden channels.\",\"kernel_size (int) – Size of the convolutional kernel.\",\"dilation_rate (int) – Dilation rate of the convolutional layers.\",\"n_layers (int) – Number of convolutional layers.\",\"yin_start (int) – Start point of the yin target signal.\",\"yin_scope (int) – Scope of the yin target signal.\",\"yin_shift_range (int) – Maximum number of frames to shift the yin target signal.\",\"gin_channels (int,optional) – Number of global conditioning channels. Defaults to 0.\",\"crop_scope(x, yin_start, scope_shift)\",\"Crop the input tensor.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor of shape [B, C, T].\",\"yin_start (int) – Starting point of the yin target signal.\",\"scope_shift (torch.Tensor) – Shift tensor of shape [B].\",\"Returns: Cropped tensor of shape [B, C, yin_scope].\",\"Return type: torch.Tensor\",\"forward(z_yin, yin_gt, z_mask, g=None)\",\"Forward pass of the decoder.\",\"Parameters:\",\"z_yin (torch.Tensor) – The input yin note sequence of shape (B, C, T_yin).\",\"yin_gt (torch.Tensor) – The ground truth yin note sequence of shape (B, C, T_yin).\",\"z_mask (torch.Tensor) – The mask tensor of shape (B, 1, T_yin).\",\"g (torch.Tensor) – The global conditioning tensor.\",\"Returns: The predicted yin note sequence of shape (B, C, T_yin). torch.Tensor: The shifted ground truth yin note sequence of shape\",\"(B, C, T_yin).\",\"torch.Tensor: The cropped ground truth yin note sequence of shape : (B, C, T_yin).\",\"torch.Tensor: The cropped input yin note sequence of shape (B, C, T_yin). torch.Tensor: The scope shift tensor of shape (B,).\",\"Return type: torch.Tensor\",\"infer(z_yin, z_mask, g=None)\",\"Generate yin prediction.\",\"Parameters:\",\"z_yin (torch.Tensor) – Input yin target tensor of shape [B, yin_scope, C].\",\"z_mask (torch.Tensor) – Input mask tensor of shape [B, yin_scope, 1].\",\"g (torch.Tensor,optional) – Global conditioning tensor of shape [B, gin_channels, 1]. Defaults to None.\",\"Returns: Predicted yin tensor of shape [B, yin_scope, C].\",\"Return type: torch.Tensor\"]},\"1557\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.amp_to_impulse_response\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.amp_to_impulse_response(amp, target_size)\"]},\"1558\":{\"h\":\"espnet2.gan_svs.visinger2.visinger2_vocoder.create_fb_matrix\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.visinger2_vocoder.create_fb_matrix(n_freqs: int, f_min: float, f_max: float, n_mels: int, sample_rate: int, norm: str | None = None) → Tensor\",\"Create a frequency bin conversion matrix.\",\"Parameters:\",\"n_freqs (int) – Number of frequencies to highlight/apply\",\"f_min (float) – Minimum frequency (Hz)\",\"f_max (float) – Maximum frequency (Hz)\",\"n_mels (int) – Number of mel filterbanks\",\"sample_rate (int) – Sample rate of the audio waveform\",\"norm (Optional *[*str]) – If ‘slaney’,\",\"band (divide the triangular mel weights by the widthofthe mel)\",\"**(**Default ( *(*area normalization).) – None)\",\"Returns: Triangular filter banks (fb matrix) of size (n_freqs, n_mels) meaning number of frequencies to highlight/apply to x the number of filterbanks. Each column is a filterbank so that assuming there is a matrix A of size (…, n_freqs), the applied result would be A * create_fb_matrix(A.size(-1), …).\",\"Return type: Tensor\"]},\"1559\":{\"h\":\"espnet2.gan_svs.utils.expand_f0.expand_f0\",\"t\":[\"source\",\"espnet2.gan_svs.utils.expand_f0.expand_f0(f0_frame, hop_length, method='interpolation')\",\"Expand f0 to output wave length.\",\"Parameters:\",\"f0_frame (Tensor) – Input tensor (B, 1, frame_len).\",\"hop_length (Tensor) – Hop length.\",\"method (str) – Method to expand f0. Choose either ‘interpolation’ or ‘repeat’.\",\"Returns: Output tensor (B, 1, wav_len).\",\"Return type: Tensor\"]},\"1560\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.extract_loudness\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.extract_loudness(signal, sampling_rate, block_size, n_fft=2048)\"]},\"1561\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.extract_pitch\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.extract_pitch(signal, sampling_rate, block_size)\"]},\"1562\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.fft_convolve\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.fft_convolve(signal, kernel)\"]},\"1563\":{\"h\":\"espnet2.gan_svs.avocodo.avocodo.get_padding\",\"t\":[\"source\",\"espnet2.gan_svs.avocodo.avocodo.get_padding(kernel_size, dilation=1)\"]},\"1564\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.gru\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.gru(n_input, hidden_size)\"]},\"1565\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.harmonic_synth\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.harmonic_synth(pitch, amplitudes, sampling_rate)\"]},\"1566\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.init_kernels\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.init_kernels(win_len, win_inc, fft_len, win_type=None, invers=False)\"]},\"1567\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.mean_std_loudness\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.mean_std_loudness(dataset)\"]},\"1568\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.mlp\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.mlp(in_size, hidden_size, n_layers)\"]},\"1569\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.multiscale_fft\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.multiscale_fft(signal, scales, overlap)\"]},\"1570\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.remove_above_nyquist\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.remove_above_nyquist(amplitudes, pitch, sampling_rate)\"]},\"1571\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.resample\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.resample(x, factor: int)\"]},\"1572\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.safe_log\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.safe_log(x)\"]},\"1573\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.scale_function\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.scale_function(x)\"]},\"1574\":{\"h\":\"espnet2.gan_svs.vits.modules.sequence_mask\",\"t\":[\"source\",\"espnet2.gan_svs.vits.modules.sequence_mask(length, max_length=None)\"]},\"1575\":{\"h\":\"espnet2.gan_svs.visinger2.ddsp.upsample\",\"t\":[\"source\",\"espnet2.gan_svs.visinger2.ddsp.upsample(signal, factor)\"]},\"1576\":{\"h\":\"espnet2.gan_tts.abs_gan_tts.AbsGANTTS\",\"t\":[\"source\",\"class espnet2.gan_tts.abs_gan_tts.AbsGANTTS(*args, **kwargs)\",\"Bases: AbsTTS, ABC\",\"GAN-based TTS model abstract class.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(forward_generator, *args, **kwargs) → Dict[str, Tensor | Dict[str, Tensor] | int]\",\"Return generator or discriminator loss.\"]},\"1577\":{\"h\":\"espnet2.gan_tts.jets.alignments.AlignmentModule\",\"t\":[\"source\",\"class espnet2.gan_tts.jets.alignments.AlignmentModule(adim, odim, cache_prior=True)\",\"Bases: Module\",\"Alignment Learning Framework proposed for parallel TTS models in:\",\"https://arxiv.org/abs/2108.10447\",\"Initialize AlignmentModule.\",\"Parameters:\",\"adim (int) – Dimension of attention.\",\"odim (int) – Dimension of feats.\",\"cache_prior (bool) – Whether to cache beta-binomial prior.\",\"forward(text, feats, text_lengths, feats_lengths, x_masks=None)\",\"Calculate alignment loss.\",\"Parameters:\",\"text (Tensor) – Batched text embedding (B, T_text, adim).\",\"feats (Tensor) – Batched acoustic feature (B, T_feats, odim).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"x_masks (Tensor) – Mask tensor (B, T_text).\",\"Returns: Log probability of attention matrix (B, T_feats, T_text).\",\"Return type: Tensor\"]},\"1578\":{\"h\":\"espnet2.gan_tts.wavenet.residual_block.Conv1d\",\"t\":[\"source\",\"class espnet2.gan_tts.wavenet.residual_block.Conv1d(*args, **kwargs)\",\"Bases: Conv1d\",\"Conv1d module with customized initialization.\",\"Initialize Conv1d module.\",\"reset_parameters()\",\"Reset parameters.\"]},\"1579\":{\"h\":\"espnet2.gan_tts.wavenet.residual_block.Conv1d1x1\",\"t\":[\"source\",\"class espnet2.gan_tts.wavenet.residual_block.Conv1d1x1(in_channels: int, out_channels: int, bias: bool)\",\"Bases: Conv1d\",\"1x1 Conv1d with customized initialization.\",\"Initialize 1x1 Conv1d module.\"]},\"1580\":{\"h\":\"espnet2.gan_tts.parallel_wavegan.upsample.Conv2d\",\"t\":[\"source\",\"class espnet2.gan_tts.parallel_wavegan.upsample.Conv2d(*args, **kwargs)\",\"Bases: Conv2d\",\"Conv2d module with customized initialization.\",\"Initialize Conv2d module.\",\"reset_parameters()\",\"Reset parameters.\"]},\"1581\":{\"h\":\"espnet2.gan_tts.vits.flow.ConvFlow\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.flow.ConvFlow(in_channels: int, hidden_channels: int, kernel_size: int, layers: int, bins: int = 10, tail_bound: float = 5.0)\",\"Bases: Module\",\"Convolutional flow module.\",\"Initialize ConvFlow module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"hidden_channels (int) – Number of hidden channels.\",\"kernel_size (int) – Kernel size.\",\"layers (int) – Number of layers.\",\"bins (int) – Number of bins.\",\"tail_bound (float) – Tail bound value.\",\"forward(x: Tensor, x_mask: Tensor, g: Tensor | None = None, inverse: bool = False) → Tensor | Tuple[Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, channels, T).\",\"x_mask (Tensor) – Mask tensor (B,).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, channels, 1).\",\"inverse (bool) – Whether to inverse the flow.\",\"Returns: Output tensor (B, channels, T). Tensor: Log-determinant tensor for NLL (B,) if not inverse.\",\"Return type: Tensor\"]},\"1582\":{\"h\":\"espnet2.gan_tts.parallel_wavegan.upsample.ConvInUpsampleNetwork\",\"t\":[\"source\",\"class espnet2.gan_tts.parallel_wavegan.upsample.ConvInUpsampleNetwork(upsample_scales: List[int], nonlinear_activation: str | None = None, nonlinear_activation_params: Dict[str, Any] = {}, interpolate_mode: str = 'nearest', freq_axis_kernel_size: int = 1, aux_channels: int = 80, aux_context_window: int = 0)\",\"Bases: Module\",\"Convolution + upsampling network module.\",\"Initialize ConvInUpsampleNetwork module.\",\"Parameters:\",\"upsample_scales (list) – List of upsampling scales.\",\"nonlinear_activation (Optional *[*str]) – Activation function name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Arguments for the specified activation function.\",\"mode (str) – Interpolation mode.\",\"freq_axis_kernel_size (int) – Kernel size in the direction of frequency axis.\",\"aux_channels (int) – Number of channels of pre-conv layer.\",\"aux_context_window (int) – Context window size of the pre-conv layer.\",\"forward(c: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:c (Tensor) – Input tensor (B, C, T_feats).\",\"Returns: Upsampled tensor (B, C, T_wav), : where T_wav = T_feats * prod(upsample_scales).\",\"Return type: Tensor\"]},\"1583\":{\"h\":\"espnet2.gan_tts.vits.flow.DilatedDepthSeparableConv\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.flow.DilatedDepthSeparableConv(channels: int, kernel_size: int, layers: int, dropout_rate: float = 0.0, eps: float = 1e-05)\",\"Bases: Module\",\"Dilated depth-separable conv module.\",\"Initialize DilatedDepthSeparableConv module.\",\"Parameters:\",\"channels (int) – Number of channels.\",\"kernel_size (int) – Kernel size.\",\"layers (int) – Number of layers.\",\"dropout_rate (float) – Dropout rate.\",\"eps (float) – Epsilon for layer norm.\",\"forward(x: Tensor, x_mask: Tensor, g: Tensor | None = None) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T).\",\"x_mask (Tensor) – Mask tensor (B, 1, T).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"Returns: Output tensor (B, channels, T).\",\"Return type: Tensor\"]},\"1584\":{\"h\":\"espnet2.gan_tts.hifigan.loss.DiscriminatorAdversarialLoss\",\"t\":[\"source\",\"class espnet2.gan_tts.hifigan.loss.DiscriminatorAdversarialLoss(average_by_discriminators: bool = True, loss_type: str = 'mse')\",\"Bases: Module\",\"Discriminator adversarial loss module.\",\"Initialize DiscriminatorAversarialLoss module.\",\"Parameters:\",\"average_by_discriminators (bool) – Whether to average the loss by the number of discriminators.\",\"loss_type (str) – Loss type, “mse” or “hinge”.\",\"forward(outputs_hat: List[List[Tensor]] | List[Tensor] | Tensor, outputs: List[List[Tensor]] | List[Tensor] | Tensor) → Tuple[Tensor, Tensor]\",\"Calcualate discriminator adversarial loss.\",\"Parameters:\",\"outputs_hat (Union *[*List *[*List *[*Tensor]],List *[*Tensor],Tensor]) – Discriminator outputs, list of discriminator outputs, or list of list of discriminator outputs calculated from generator.\",\"outputs (Union *[*List *[*List *[*Tensor]],List *[*Tensor],Tensor]) – Discriminator outputs, list of discriminator outputs, or list of list of discriminator outputs calculated from groundtruth.\",\"Returns: Discriminator real loss value. Tensor: Discriminator fake loss value.\",\"Return type: Tensor\"]},\"1585\":{\"h\":\"espnet2.gan_tts.espnet_model.ESPnetGANTTSModel\",\"t\":[\"source\",\"class espnet2.gan_tts.espnet_model.ESPnetGANTTSModel(feats_extract: AbsFeatsExtract | None, normalize: InversibleInterface | None, pitch_extract: AbsFeatsExtract | None, pitch_normalize: InversibleInterface | None, energy_extract: AbsFeatsExtract | None, energy_normalize: InversibleInterface | None, tts: AbsGANTTS)\",\"Bases: AbsGANESPnetModel\",\"ESPnet model for GAN-based text-to-speech task.\",\"Initialize ESPnetGANTTSModel module.\",\"collect_feats(text: Tensor, text_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, durations: Tensor | None = None, durations_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs) → Dict[str, Tensor]\",\"Calculate features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B, 1).\",\"durations (Optional *[*Tensor) – Duration tensor.\",\"durations_lengths (Optional *[*Tensor) – Duration length tensor (B,).\",\"pitch (Optional *[*Tensor) – Pitch tensor.\",\"pitch_lengths (Optional *[*Tensor) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"Returns: Dict of features.\",\"Return type: Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, durations: Tensor | None = None, durations_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, forward_generator: bool = True, **kwargs) → Dict[str, Any]\",\"Return generator or discriminator loss with dict format.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"duration (Optional *[*Tensor]) – Duration tensor.\",\"duration_lengths (Optional *[*Tensor]) – Duration length tensor (B,).\",\"pitch (Optional *[*Tensor]) – Pitch tensor.\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor]) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor]) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"forward_generator (bool) – Whether to forward generator.\",\"kwargs – “utt_id” is among the input.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\"]},\"1586\":{\"h\":\"espnet2.gan_tts.vits.flow.ElementwiseAffineFlow\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.flow.ElementwiseAffineFlow(channels: int)\",\"Bases: Module\",\"Elementwise affine flow module.\",\"Initialize ElementwiseAffineFlow module.\",\"Parameters:channels (int) – Number of channels.\",\"forward(x: Tensor, x_mask: Tensor, inverse: bool = False, **kwargs) → Tensor | Tuple[Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, channels, T).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"inverse (bool) – Whether to inverse the flow.\",\"Returns: Output tensor (B, channels, T). Tensor: Log-determinant tensor for NLL (B,) if not inverse.\",\"Return type: Tensor\"]},\"1587\":{\"h\":\"espnet2.gan_tts.hifigan.loss.FeatureMatchLoss\",\"t\":[\"source\",\"class espnet2.gan_tts.hifigan.loss.FeatureMatchLoss(average_by_layers: bool = True, average_by_discriminators: bool = True, include_final_outputs: bool = False)\",\"Bases: Module\",\"Feature matching loss module.\",\"Initialize FeatureMatchLoss module.\",\"Parameters:\",\"average_by_layers (bool) – Whether to average the loss by the number of layers.\",\"average_by_discriminators (bool) – Whether to average the loss by the number of discriminators.\",\"include_final_outputs (bool) – Whether to include the final output of each discriminator for loss calculation.\",\"forward(feats_hat: List[List[Tensor]] | List[Tensor], feats: List[List[Tensor]] | List[Tensor]) → Tensor\",\"Calculate feature matching loss.\",\"Parameters:\",\"feats_hat (Union *[*List *[*List *[*Tensor]],List *[*Tensor]]) – List of list of discriminator outputs or list of discriminator outputs calcuated from generator’s outputs.\",\"feats (Union *[*List *[*List *[*Tensor]],List *[*Tensor]]) – List of list of discriminator outputs or list of discriminator outputs calcuated from groundtruth..\",\"Returns: Feature matching loss value.\",\"Return type: Tensor\"]},\"1588\":{\"h\":\"espnet2.gan_tts.vits.flow.FlipFlow\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.flow.FlipFlow(*args, **kwargs)\",\"Bases: Module\",\"Flip flow module.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, *args, inverse: bool = False, **kwargs) → Tensor | Tuple[Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, channels, T).\",\"inverse (bool) – Whether to inverse the flow.\",\"Returns: Flipped tensor (B, channels, T). Tensor: Log-determinant tensor for NLL (B,) if not inverse.\",\"Return type: Tensor\"]},\"1589\":{\"h\":\"espnet2.gan_tts.jets.loss.ForwardSumLoss\",\"t\":[\"source\",\"class espnet2.gan_tts.jets.loss.ForwardSumLoss\",\"Bases: Module\",\"Forwardsum loss described at https://openreview.net/forum?id=0NQwnnwAORi\",\"Initialize forwardsum loss module.\",\"forward(log_p_attn: Tensor, ilens: Tensor, olens: Tensor, blank_prob: float = 0.36787944117144233) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"log_p_attn (Tensor) – Batch of log probability of attention matrix (B, T_feats, T_text).\",\"ilens (Tensor) – Batch of the lengths of each input (B,).\",\"olens (Tensor) – Batch of the lengths of each target (B,).\",\"blank_prob (float) – Blank symbol probability.\",\"Returns: forwardsum loss value.\",\"Return type: Tensor\"]},\"1590\":{\"h\":\"espnet2.gan_tts.jets.length_regulator.GaussianUpsampling\",\"t\":[\"source\",\"class espnet2.gan_tts.jets.length_regulator.GaussianUpsampling(delta=0.1)\",\"Bases: Module\",\"Gaussian upsampling with fixed temperature as in:\",\"https://arxiv.org/abs/2010.04301\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(hs, ds, h_masks=None, d_masks=None)\",\"Upsample hidden states according to durations.\",\"Parameters:\",\"hs (Tensor) – Batched hidden state to be expanded (B, T_text, adim).\",\"ds (Tensor) – Batched token duration (B, T_text).\",\"h_masks (Tensor) – Mask tensor (B, T_feats).\",\"d_masks (Tensor) – Mask tensor (B, T_text).\",\"Returns: Expanded hidden state (B, T_feat, adim).\",\"Return type: Tensor\"]},\"1591\":{\"h\":\"espnet2.gan_tts.hifigan.loss.GeneratorAdversarialLoss\",\"t\":[\"source\",\"class espnet2.gan_tts.hifigan.loss.GeneratorAdversarialLoss(average_by_discriminators: bool = True, loss_type: str = 'mse')\",\"Bases: Module\",\"Generator adversarial loss module.\",\"Initialize GeneratorAversarialLoss module.\",\"Parameters:\",\"average_by_discriminators (bool) – Whether to average the loss by the number of discriminators.\",\"loss_type (str) – Loss type, “mse” or “hinge”.\",\"forward(outputs: List[List[Tensor]] | List[Tensor] | Tensor) → Tensor\",\"Calcualate generator adversarial loss.\",\"Parameters:outputs (Union *[*List *[*List *[*Tensor]],List *[*Tensor],Tensor]) – Discriminator outputs, list of discriminator outputs, or list of list of discriminator outputs..\",\"Returns: Generator adversarial loss value.\",\"Return type: Tensor\"]},\"1592\":{\"h\":\"espnet2.gan_tts.hifigan.hifigan.HiFiGANGenerator\",\"t\":[\"source\",\"class espnet2.gan_tts.hifigan.hifigan.HiFiGANGenerator(in_channels: int = 80, out_channels: int = 1, channels: int = 512, global_channels: int = -1, kernel_size: int = 7, upsample_scales: List[int] = [8, 8, 2, 2], upsample_kernel_sizes: List[int] = [16, 16, 4, 4], resblock_kernel_sizes: List[int] = [3, 7, 11], resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], use_additional_convs: bool = True, bias: bool = True, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1}, use_weight_norm: bool = True)\",\"Bases: Module\",\"HiFiGAN generator module.\",\"Initialize HiFiGANGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"channels (int) – Number of hidden representation channels.\",\"global_channels (int) – Number of global conditioning channels.\",\"kernel_size (int) – Kernel size of initial and final conv layer.\",\"upsample_scales (List *[*int]) – List of upsampling scales.\",\"upsample_kernel_sizes (List *[*int]) – List of kernel sizes for upsample layers.\",\"resblock_kernel_sizes (List *[*int]) – List of kernel sizes for residual blocks.\",\"resblock_dilations (List *[*List *[*int]]) – List of list of dilations for residual blocks.\",\"use_additional_convs (bool) – Whether to use additional conv layers in residual blocks.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c: Tensor, g: Tensor | None = None) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"c (Tensor) – Input tensor (B, in_channels, T).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"Returns: Output tensor (B, out_channels, T).\",\"Return type: Tensor\",\"inference(c: Tensor, g: Tensor | None = None) → Tensor\",\"Perform inference.\",\"Parameters:\",\"c (torch.Tensor) – Input tensor (T, in_channels).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (global_channels, 1).\",\"Returns: Output tensor (T ** upsample_factor, out_channels).\",\"Return type: Tensor\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"This initialization follows the official implementation manner. https://github.com/jik876/hifi-gan/blob/master/models.py\"]},\"1593\":{\"h\":\"espnet2.gan_tts.hifigan.hifigan.HiFiGANMultiPeriodDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_tts.hifigan.hifigan.HiFiGANMultiPeriodDiscriminator(periods: List[int] = [2, 3, 5, 7, 11], discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True})\",\"Bases: Module\",\"HiFiGAN multi-period discriminator module.\",\"Initialize HiFiGANMultiPeriodDiscriminator module.\",\"Parameters:\",\"periods (List *[*int]) – List of periods.\",\"discriminator_params (Dict *[*str,Any]) – Parameters for hifi-gan period discriminator module. The period parameter will be overwritten.\",\"forward(x: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, which consists of each : layer output tensors.\",\"Return type: List\"]},\"1594\":{\"h\":\"espnet2.gan_tts.hifigan.hifigan.HiFiGANMultiScaleDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_tts.hifigan.hifigan.HiFiGANMultiScaleDiscriminator(scales: int = 3, downsample_pooling: str = 'AvgPool1d', downsample_pooling_params: Dict[str, Any] = {'kernel_size': 4, 'padding': 2, 'stride': 2}, discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1}, follow_official_norm: bool = False)\",\"Bases: Module\",\"HiFi-GAN multi-scale discriminator module.\",\"Initilize HiFiGAN multi-scale discriminator module.\",\"Parameters:\",\"scales (int) – Number of multi-scales.\",\"downsample_pooling (str) – Pooling module name for downsampling of the inputs.\",\"downsample_pooling_params (Dict *[*str,Any]) – Parameters for the above pooling module.\",\"discriminator_params (Dict *[*str,Any]) – Parameters for hifi-gan scale discriminator module.\",\"follow_official_norm (bool) – Whether to follow the norm setting of the official implementaion. The first discriminator uses spectral norm and the other discriminators use weight norm.\",\"forward(x: Tensor) → List[List[Tensor]]\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, : which consists of eachlayer output tensors.\",\"Return type: List[List[torch.Tensor]]\"]},\"1595\":{\"h\":\"espnet2.gan_tts.hifigan.hifigan.HiFiGANMultiScaleMultiPeriodDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_tts.hifigan.hifigan.HiFiGANMultiScaleMultiPeriodDiscriminator(scales: int = 3, scale_downsample_pooling: str = 'AvgPool1d', scale_downsample_pooling_params: Dict[str, Any] = {'kernel_size': 4, 'padding': 2, 'stride': 2}, scale_discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1}, follow_official_norm: bool = True, periods: List[int] = [2, 3, 5, 7, 11], period_discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True})\",\"Bases: Module\",\"HiFi-GAN multi-scale + multi-period discriminator module.\",\"Initilize HiFiGAN multi-scale + multi-period discriminator module.\",\"Parameters:\",\"scales (int) – Number of multi-scales.\",\"scale_downsample_pooling (str) – Pooling module name for downsampling of the inputs.\",\"scale_downsample_pooling_params (dict) – Parameters for the above pooling module.\",\"scale_discriminator_params (dict) – Parameters for hifi-gan scale discriminator module.\",\"follow_official_norm (bool) – Whether to follow the norm setting of the official implementaion. The first discriminator uses spectral norm and the other discriminators use weight norm.\",\"periods (list) – List of periods.\",\"period_discriminator_params (dict) – Parameters for hifi-gan period discriminator module. The period parameter will be overwritten.\",\"forward(x: Tensor) → List[List[Tensor]]\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, : which consists of each layer output tensors. Multi scale and multi period ones are concatenated.\",\"Return type: List[List[Tensor]]\"]},\"1596\":{\"h\":\"espnet2.gan_tts.hifigan.hifigan.HiFiGANPeriodDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_tts.hifigan.hifigan.HiFiGANPeriodDiscriminator(in_channels: int = 1, out_channels: int = 1, period: int = 3, kernel_sizes: List[int] = [5, 3], channels: int = 32, downsample_scales: List[int] = [3, 3, 3, 3, 1], max_downsample_channels: int = 1024, bias: bool = True, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1}, use_weight_norm: bool = True, use_spectral_norm: bool = False)\",\"Bases: Module\",\"HiFiGAN period discriminator module.\",\"Initialize HiFiGANPeriodDiscriminator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"period (int) – Period.\",\"kernel_sizes (list) – Kernel sizes of initial conv layers and the final conv layer.\",\"channels (int) – Number of initial channels.\",\"downsample_scales (List *[*int]) – List of downsampling scales.\",\"max_downsample_channels (int) – Number of maximum downsampling channels.\",\"use_additional_convs (bool) – Whether to use additional conv layers in residual blocks.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"use_spectral_norm (bool) – Whether to use spectral norm. If set to true, it will be applied to all of the conv layers.\",\"apply_spectral_norm()\",\"Apply spectral normalization module from all of the layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(x: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:c (Tensor) – Input tensor (B, in_channels, T).\",\"Returns: List of each layer’s tensors.\",\"Return type: list\"]},\"1597\":{\"h\":\"espnet2.gan_tts.hifigan.hifigan.HiFiGANScaleDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_tts.hifigan.hifigan.HiFiGANScaleDiscriminator(in_channels: int = 1, out_channels: int = 1, kernel_sizes: List[int] = [15, 41, 5, 3], channels: int = 128, max_downsample_channels: int = 1024, max_groups: int = 16, bias: int = True, downsample_scales: List[int] = [2, 2, 4, 4, 1], nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1}, use_weight_norm: bool = True, use_spectral_norm: bool = False)\",\"Bases: Module\",\"HiFi-GAN scale discriminator module.\",\"Initilize HiFiGAN scale discriminator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"kernel_sizes (List *[*int]) – List of four kernel sizes. The first will be used for the first conv layer, and the second is for downsampling part, and the remaining two are for the last two output layers.\",\"channels (int) – Initial number of channels for conv layer.\",\"max_downsample_channels (int) – Maximum number of channels for downsampling layers.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"downsample_scales (List *[*int]) – List of downsampling scales.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"use_spectral_norm (bool) – Whether to use spectral norm. If set to true, it will be applied to all of the conv layers.\",\"apply_spectral_norm()\",\"Apply spectral normalization module from all of the layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(x: Tensor) → List[Tensor]\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of output tensors of each layer.\",\"Return type: List[Tensor]\",\"remove_spectral_norm()\",\"Remove spectral normalization module from all of the layers.\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\"]},\"1598\":{\"h\":\"espnet2.gan_tts.jets.jets.JETS\",\"t\":[\"source\",\"class espnet2.gan_tts.jets.jets.JETS(idim: int, odim: int, sampling_rate: int = 22050, generator_type: str = 'jets_generator', generator_params: Dict[str, Any] = {'adim': 256, 'aheads': 2, 'conformer_activation_type': 'swish', 'conformer_dec_kernel_size': 31, 'conformer_enc_kernel_size': 7, 'conformer_pos_enc_layer_type': 'rel_pos', 'conformer_rel_pos_type': 'latest', 'conformer_self_attn_layer_type': 'rel_selfattn', 'decoder_concat_after': False, 'decoder_normalize_before': True, 'decoder_type': 'transformer', 'dlayers': 4, 'dunits': 1024, 'duration_predictor_chans': 384, 'duration_predictor_dropout_rate': 0.1, 'duration_predictor_kernel_size': 3, 'duration_predictor_layers': 2, 'elayers': 4, 'encoder_concat_after': False, 'encoder_normalize_before': True, 'encoder_type': 'transformer', 'energy_embed_dropout': 0.5, 'energy_embed_kernel_size': 1, 'energy_predictor_chans': 384, 'energy_predictor_dropout': 0.5, 'energy_predictor_kernel_size': 3, 'energy_predictor_layers': 2, 'eunits': 1024, 'generator_bias': True, 'generator_channels': 512, 'generator_global_channels': -1, 'generator_kernel_size': 7, 'generator_nonlinear_activation': 'LeakyReLU', 'generator_nonlinear_activation_params': {'negative_slope': 0.1}, 'generator_out_channels': 1, 'generator_resblock_dilations': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'generator_resblock_kernel_sizes': [3, 7, 11], 'generator_upsample_kernel_sizes': [16, 16, 4, 4], 'generator_upsample_scales': [8, 8, 2, 2], 'generator_use_additional_convs': True, 'generator_use_weight_norm': True, 'gst_conv_chans_list': [32, 32, 64, 64, 128, 128], 'gst_conv_kernel_size': 3, 'gst_conv_layers': 6, 'gst_conv_stride': 2, 'gst_gru_layers': 1, 'gst_gru_units': 128, 'gst_heads': 4, 'gst_tokens': 10, 'init_dec_alpha': 1.0, 'init_enc_alpha': 1.0, 'init_type': 'xavier_uniform', 'langs': -1, 'pitch_embed_dropout': 0.5, 'pitch_embed_kernel_size': 1, 'pitch_predictor_chans': 384, 'pitch_predictor_dropout': 0.5, 'pitch_predictor_kernel_size': 5, 'pitch_predictor_layers': 5, 'positionwise_conv_kernel_size': 1, 'positionwise_layer_type': 'conv1d', 'reduction_factor': 1, 'segment_size': 64, 'spk_embed_dim': None, 'spk_embed_integration_type': 'add', 'spks': -1, 'stop_gradient_from_energy_predictor': False, 'stop_gradient_from_pitch_predictor': True, 'transformer_dec_attn_dropout_rate': 0.1, 'transformer_dec_dropout_rate': 0.1, 'transformer_dec_positional_dropout_rate': 0.1, 'transformer_enc_attn_dropout_rate': 0.1, 'transformer_enc_dropout_rate': 0.1, 'transformer_enc_positional_dropout_rate': 0.1, 'use_batch_norm': True, 'use_cnn_in_conformer': True, 'use_gst': False, 'use_macaron_style_in_conformer': True, 'use_masking': False, 'use_scaled_pos_enc': True, 'use_weighted_masking': False, 'zero_triu': False}, discriminator_type: str = 'hifigan_multi_scale_multi_period_discriminator', discriminator_params: Dict[str, Any] = {'follow_official_norm': False, 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'scale_discriminator_params': {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scales': 1}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 22050, 'hop_length': 256, 'log_base': None, 'n_fft': 1024, 'n_mels': 80, 'win_length': None, 'window': 'hann'}, lambda_adv: float = 1.0, lambda_mel: float = 45.0, lambda_feat_match: float = 2.0, lambda_var: float = 1.0, lambda_align: float = 2.0, cache_generator_outputs: bool = True, plot_pred_mos: bool = False, mos_pred_tool: str = 'utmos')\",\"Bases: AbsGANTTS\",\"JETS module (generator + discriminator).\",\"This is a module of JETS described in\",\"`\",\"JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech’_.\",\"Initialize JETS module.\",\"Parameters:\",\"idim (int) – Input vocabrary size.\",\"odim (int) – Acoustic feature dimension. The actual output channels will be 1 since JETS is the end-to-end text-to-wave model but for the compatibility odim is used to indicate the acoustic feature dimension.\",\"sampling_rate (int) – Sampling rate, not used for the training but it will be referred in saving waveform during the inference.\",\"generator_type (str) – Generator type.\",\"generator_params (Dict *[*str,Any]) – Parameter dict for generator.\",\"discriminator_type (str) – Discriminator type.\",\"discriminator_params (Dict *[*str,Any]) – Parameter dict for discriminator.\",\"generator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for generator adversarial loss.\",\"discriminator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for discriminator adversarial loss.\",\"feat_match_loss_params (Dict *[*str,Any]) – Parameter dict for feat match loss.\",\"mel_loss_params (Dict *[*str,Any]) – Parameter dict for mel loss.\",\"lambda_adv (float) – Loss scaling coefficient for adversarial loss.\",\"lambda_mel (float) – Loss scaling coefficient for mel spectrogram loss.\",\"lambda_feat_match (float) – Loss scaling coefficient for feat match loss.\",\"lambda_var (float) – Loss scaling coefficient for variance loss.\",\"lambda_align (float) – Loss scaling coefficient for alignment loss.\",\"cache_generator_outputs (bool) – Whether to cache generator outputs.\",\"plot_pred_mos (bool) – Whether to plot predicted MOS during the training.\",\"mos_pred_tool (str) – MOS prediction tool name.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, forward_generator: bool = True, **kwargs) → Dict[str, Any]\",\"Perform generator forward.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, T_feats, aux_channels).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B,) or (B, 1).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, spk_embed_dim).\",\"lids (Optional *[*Tensor]) – Language index tensor (B,) or (B, 1).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(text: Tensor, feats: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, use_teacher_forcing: bool = False, **kwargs) → Dict[str, Tensor]\",\"Run inference.\",\"Parameters:\",\"text (Tensor) – Input text index tensor (T_text,).\",\"feats (Tensor) – Feature tensor (T_feats, aux_channels).\",\"pitch (Tensor) – Pitch tensor (T_feats, 1).\",\"energy (Tensor) – Energy tensor (T_feats, 1).\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"duration (Tensor): Predicted duration tensor (T_text,).\",\"Return type: Dict[str, Tensor]\",\"property require_raw_speech\",\"Return whether or not speech is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\"]},\"1599\":{\"h\":\"espnet2.gan_tts.jets.generator.JETSGenerator\",\"t\":[\"source\",\"class espnet2.gan_tts.jets.generator.JETSGenerator(idim: int, odim: int, adim: int = 256, aheads: int = 2, elayers: int = 4, eunits: int = 1024, dlayers: int = 4, dunits: int = 1024, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'transformer', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, energy_predictor_layers: int = 2, energy_predictor_chans: int = 384, energy_predictor_kernel_size: int = 3, energy_predictor_dropout: float = 0.5, energy_embed_kernel_size: int = 9, energy_embed_dropout: float = 0.5, stop_gradient_from_energy_predictor: bool = False, pitch_predictor_layers: int = 2, pitch_predictor_chans: int = 384, pitch_predictor_kernel_size: int = 3, pitch_predictor_dropout: float = 0.5, pitch_embed_kernel_size: int = 9, pitch_embed_dropout: float = 0.5, stop_gradient_from_pitch_predictor: bool = False, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, segment_size: int = 64, generator_out_channels: int = 1, generator_channels: int = 512, generator_global_channels: int = -1, generator_kernel_size: int = 7, generator_upsample_scales: List[int] = [8, 8, 2, 2], generator_upsample_kernel_sizes: List[int] = [16, 16, 4, 4], generator_resblock_kernel_sizes: List[int] = [3, 7, 11], generator_resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], generator_use_additional_convs: bool = True, generator_bias: bool = True, generator_nonlinear_activation: str = 'LeakyReLU', generator_nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1}, generator_use_weight_norm: bool = True)\",\"Bases: Module\",\"Generator module in JETS.\",\"Initialize JETS generator module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"conformer_rel_pos_type (str) – Relative pos encoding type in conformer.\",\"conformer_pos_enc_layer_type (str) – Pos encoding layer type in conformer.\",\"conformer_self_attn_layer_type (str) – Self-attention layer type in conformer\",\"conformer_activation_type (str) – Activation function type in conformer.\",\"use_macaron_style_in_conformer – Whether to use macaron style FFN.\",\"use_cnn_in_conformer – Whether to use CNN in conformer.\",\"zero_triu – Whether to use zero triu in relative self-attention module.\",\"conformer_enc_kernel_size – Kernel size of encoder conformer.\",\"conformer_dec_kernel_size – Kernel size of decoder conformer.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"pitch_predictor_layers (int) – Number of pitch predictor layers.\",\"pitch_predictor_chans (int) – Number of pitch predictor channels.\",\"pitch_predictor_kernel_size (int) – Kernel size of pitch predictor.\",\"pitch_predictor_dropout_rate (float) – Dropout rate in pitch predictor.\",\"pitch_embed_kernel_size (float) – Kernel size of pitch embedding.\",\"pitch_embed_dropout_rate (float) – Dropout rate for pitch embedding.\",\"stop_gradient_from_pitch_predictor – Whether to stop gradient from pitch predictor to encoder.\",\"energy_predictor_layers (int) – Number of energy predictor layers.\",\"energy_predictor_chans (int) – Number of energy predictor channels.\",\"energy_predictor_kernel_size (int) – Kernel size of energy predictor.\",\"energy_predictor_dropout_rate (float) – Dropout rate in energy predictor.\",\"energy_embed_kernel_size (float) – Kernel size of energy embedding.\",\"energy_embed_dropout_rate (float) – Dropout rate for energy embedding.\",\"stop_gradient_from_energy_predictor – Whether to stop gradient from energy predictor to encoder.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – The number of GST embeddings.\",\"gst_heads (int) – The number of heads in GST multihead attention.\",\"gst_conv_layers (int) – The number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – The number of GRU layers in GST.\",\"gst_gru_units (int) – The number of GRU units in GST.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"segment_size (int) – Segment size for random windowed discriminator\",\"generator_out_channels (int) – Number of output channels.\",\"generator_channels (int) – Number of hidden representation channels.\",\"generator_global_channels (int) – Number of global conditioning channels.\",\"generator_kernel_size (int) – Kernel size of initial and final conv layer.\",\"generator_upsample_scales (List *[*int]) – List of upsampling scales.\",\"generator_upsample_kernel_sizes (List *[*int]) – List of kernel sizes for upsample layers.\",\"generator_resblock_kernel_sizes (List *[*int]) – List of kernel sizes for residual blocks.\",\"generator_resblock_dilations (List *[*List *[*int]]) – List of list of dilations for residual blocks.\",\"generator_use_additional_convs (bool) – Whether to use additional conv layers in residual blocks.\",\"generator_bias (bool) – Whether to add bias parameter in convolution layers.\",\"generator_nonlinear_activation (str) – Activation function module name.\",\"generator_nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"generator_use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, pitch: Tensor, pitch_lengths: Tensor, energy: Tensor, energy_lengths: Tensor, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, T_feats, aux_channels).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"pitch (Tensor) – Batch of padded token-averaged pitch (B, T_text, 1).\",\"pitch_lengths (LongTensor) – Batch of pitch lengths (B, T_text).\",\"energy (Tensor) – Batch of padded token-averaged energy (B, T_text, 1).\",\"energy_lengths (LongTensor) – Batch of energy lengths (B, T_text).\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B,) or (B, 1).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, spk_embed_dim).\",\"lids (Optional *[*Tensor]) – Language index tensor (B,) or (B, 1).\",\"Returns: Waveform tensor (B, 1, segment_size * upsample_factor). Tensor: Binarization loss (). Tensor: Log probability attention matrix (B, T_feats, T_text). Tensor: Segments start index tensor (B,). Tensor: predicted duration (B, T_text). Tensor: ground-truth duration obtained from an alignment module (B, T_text). Tensor: predicted pitch (B, T_text,1). Tensor: ground-truth averaged pitch (B, T_text, 1). Tensor: predicted energy (B, T_text, 1). Tensor: ground-truth averaged energy (B, T_text, 1).\",\"Return type: Tensor\",\"inference(text: Tensor, text_lengths: Tensor, feats: Tensor | None = None, feats_lengths: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, use_teacher_forcing: bool = False) → Tuple[Tensor, Tensor, Tensor]\",\"Run inference.\",\"Parameters:\",\"text (Tensor) – Input text index tensor (B, T_text,).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, T_feats, aux_channels).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"pitch (Tensor) – Pitch tensor (B, T_feats, 1)\",\"energy (Tensor) – Energy tensor (B, T_feats, 1)\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B,) or (B, 1).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, spk_embed_dim).\",\"lids (Optional *[*Tensor]) – Language index tensor (B,) or (B, 1).\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Generated waveform tensor (B, T_wav). Tensor: Duration tensor (B, T_text).\",\"Return type: Tensor\"]},\"1600\":{\"h\":\"espnet2.gan_tts.joint.joint_text2wav.JointText2Wav\",\"t\":[\"source\",\"class espnet2.gan_tts.joint.joint_text2wav.JointText2Wav(idim: int, odim: int, segment_size: int = 32, sampling_rate: int = 22050, text2mel_type: str = 'fastspeech2', text2mel_params: Dict[str, Any] = {'adim': 384, 'aheads': 2, 'conformer_activation_type': 'swish', 'conformer_dec_kernel_size': 31, 'conformer_enc_kernel_size': 7, 'conformer_pos_enc_layer_type': 'rel_pos', 'conformer_rel_pos_type': 'latest', 'conformer_self_attn_layer_type': 'rel_selfattn', 'decoder_concat_after': False, 'decoder_normalize_before': True, 'decoder_type': 'conformer', 'dlayers': 4, 'dunits': 1536, 'duration_predictor_chans': 384, 'duration_predictor_dropout_rate': 0.1, 'duration_predictor_kernel_size': 3, 'duration_predictor_layers': 2, 'elayers': 4, 'encoder_concat_after': False, 'encoder_normalize_before': True, 'encoder_type': 'conformer', 'energy_embed_dropout': 0.5, 'energy_embed_kernel_size': 1, 'energy_predictor_chans': 384, 'energy_predictor_dropout': 0.5, 'energy_predictor_kernel_size': 3, 'energy_predictor_layers': 2, 'eunits': 1536, 'gst_conv_chans_list': [32, 32, 64, 64, 128, 128], 'gst_conv_kernel_size': 3, 'gst_conv_layers': 6, 'gst_conv_stride': 2, 'gst_gru_layers': 1, 'gst_gru_units': 128, 'gst_heads': 4, 'gst_tokens': 10, 'init_dec_alpha': 1.0, 'init_enc_alpha': 1.0, 'init_type': 'xavier_uniform', 'langs': -1, 'pitch_embed_dropout': 0.5, 'pitch_embed_kernel_size': 1, 'pitch_predictor_chans': 384, 'pitch_predictor_dropout': 0.5, 'pitch_predictor_kernel_size': 5, 'pitch_predictor_layers': 5, 'positionwise_conv_kernel_size': 1, 'positionwise_layer_type': 'conv1d', 'postnet_chans': 512, 'postnet_dropout_rate': 0.5, 'postnet_filts': 5, 'postnet_layers': 5, 'reduction_factor': 1, 'spk_embed_dim': None, 'spk_embed_integration_type': 'add', 'spks': -1, 'stop_gradient_from_energy_predictor': False, 'stop_gradient_from_pitch_predictor': True, 'transformer_dec_attn_dropout_rate': 0.1, 'transformer_dec_dropout_rate': 0.1, 'transformer_dec_positional_dropout_rate': 0.1, 'transformer_enc_attn_dropout_rate': 0.1, 'transformer_enc_dropout_rate': 0.1, 'transformer_enc_positional_dropout_rate': 0.1, 'use_batch_norm': True, 'use_cnn_in_conformer': True, 'use_gst': False, 'use_macaron_style_in_conformer': True, 'use_masking': False, 'use_scaled_pos_enc': True, 'use_weighted_masking': False, 'zero_triu': False}, vocoder_type: str = 'hifigan_generator', vocoder_params: Dict[str, Any] = {'bias': True, 'channels': 512, 'global_channels': -1, 'kernel_size': 7, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'resblock_dilations': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'resblock_kernel_sizes': [3, 7, 11], 'upsample_kernel_sizes': [16, 16, 4, 4], 'upsample_scales': [8, 8, 2, 2], 'use_additional_convs': True, 'use_weight_norm': True}, use_pqmf: bool = False, pqmf_params: Dict[str, Any] = {'beta': 9.0, 'cutoff_ratio': 0.142, 'subbands': 4, 'taps': 62}, discriminator_type: str = 'hifigan_multi_scale_multi_period_discriminator', discriminator_params: Dict[str, Any] = {'follow_official_norm': False, 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'scale_discriminator_params': {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scales': 1}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, use_feat_match_loss: bool = True, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, use_mel_loss: bool = True, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 22050, 'hop_length': 256, 'log_base': None, 'n_fft': 1024, 'n_mels': 80, 'win_length': None, 'window': 'hann'}, lambda_text2mel: float = 1.0, lambda_adv: float = 1.0, lambda_feat_match: float = 2.0, lambda_mel: float = 45.0, cache_generator_outputs: bool = False)\",\"Bases: AbsGANTTS\",\"General class to jointly train text2mel and vocoder parts.\",\"Initialize JointText2Wav module.\",\"Parameters:\",\"idim (int) – Input vocabrary size.\",\"odim (int) – Acoustic feature dimension. The actual output channels will be 1 since the model is the end-to-end text-to-wave model but for the compatibility odim is used to indicate the acoustic feature dimension.\",\"segment_size (int) – Segment size for random windowed inputs.\",\"sampling_rate (int) – Sampling rate, not used for the training but it will be referred in saving waveform during the inference.\",\"text2mel_type (str) – The text2mel model type.\",\"text2mel_params (Dict *[*str,Any]) – Parameter dict for text2mel model.\",\"use_pqmf (bool) – Whether to use PQMF for multi-band vocoder.\",\"pqmf_params (Dict *[*str,Any]) – Parameter dict for PQMF module.\",\"vocoder_type (str) – The vocoder model type.\",\"vocoder_params (Dict *[*str,Any]) – Parameter dict for vocoder model.\",\"discriminator_type (str) – Discriminator type.\",\"discriminator_params (Dict *[*str,Any]) – Parameter dict for discriminator.\",\"generator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for generator adversarial loss.\",\"discriminator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for discriminator adversarial loss.\",\"use_feat_match_loss (bool) – Whether to use feat match loss.\",\"feat_match_loss_params (Dict *[*str,Any]) – Parameter dict for feat match loss.\",\"use_mel_loss (bool) – Whether to use mel loss.\",\"mel_loss_params (Dict *[*str,Any]) – Parameter dict for mel loss.\",\"lambda_text2mel (float) – Loss scaling coefficient for text2mel model loss.\",\"lambda_adv (float) – Loss scaling coefficient for adversarial loss.\",\"lambda_feat_match (float) – Loss scaling coefficient for feat match loss.\",\"lambda_mel (float) – Loss scaling coefficient for mel loss.\",\"cache_generator_outputs (bool) – Whether to cache generator outputs.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, forward_generator: bool = True, **kwargs) → Dict[str, Any]\",\"Perform generator forward.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, T_feats, aux_channels).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(text: Tensor, **kwargs) → Dict[str, Tensor]\",\"Run inference.\",\"Parameters:text (Tensor) – Input text index tensor (T_text,).\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"feat_gan (Tensor): Generated feature tensor (T_text, C).\",\"Return type: Dict[str, Tensor]\",\"property require_raw_speech\",\"Return whether or not speech is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\"]},\"1601\":{\"h\":\"espnet2.gan_tts.vits.loss.KLDivergenceLoss\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.loss.KLDivergenceLoss(*args, **kwargs)\",\"Bases: Module\",\"KL divergence loss.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(z_p: Tensor, logs_q: Tensor, m_p: Tensor, logs_p: Tensor, z_mask: Tensor) → Tensor\",\"Calculate KL divergence loss.\",\"Parameters:\",\"z_p (Tensor) – Flow hidden representation (B, H, T_feats).\",\"logs_q (Tensor) – Posterior encoder projected scale (B, H, T_feats).\",\"m_p (Tensor) – Expanded text encoder projected mean (B, H, T_feats).\",\"logs_p (Tensor) – Expanded text encoder projected scale (B, H, T_feats).\",\"z_mask (Tensor) – Mask tensor (B, 1, T_feats).\",\"Returns: KL divergence loss.\",\"Return type: Tensor\"]},\"1602\":{\"h\":\"espnet2.gan_tts.vits.loss.KLDivergenceLossWithoutFlow\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.loss.KLDivergenceLossWithoutFlow(*args, **kwargs)\",\"Bases: Module\",\"KL divergence loss without flow.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(m_q: Tensor, logs_q: Tensor, m_p: Tensor, logs_p: Tensor) → Tensor\",\"Calculate KL divergence loss without flow.\",\"Parameters:\",\"m_q (Tensor) – Posterior encoder projected mean (B, H, T_feats).\",\"logs_q (Tensor) – Posterior encoder projected scale (B, H, T_feats).\",\"m_p (Tensor) – Expanded text encoder projected mean (B, H, T_feats).\",\"logs_p (Tensor) – Expanded text encoder projected scale (B, H, T_feats).\"]},\"1603\":{\"h\":\"espnet2.gan_tts.vits.flow.LogFlow\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.flow.LogFlow(*args, **kwargs)\",\"Bases: Module\",\"Log flow module.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, x_mask: Tensor, inverse: bool = False, eps: float = 1e-05, **kwargs) → Tensor | Tuple[Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, channels, T).\",\"x_mask (Tensor) – Mask tensor (B, 1, T).\",\"inverse (bool) – Whether to inverse the flow.\",\"eps (float) – Epsilon for log.\",\"Returns: Output tensor (B, channels, T). Tensor: Log-determinant tensor for NLL (B,) if not inverse.\",\"Return type: Tensor\"]},\"1604\":{\"h\":\"espnet2.gan_tts.melgan.melgan.MelGANDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_tts.melgan.melgan.MelGANDiscriminator(in_channels: int = 1, out_channels: int = 1, kernel_sizes: List[int] = [5, 3], channels: int = 16, max_downsample_channels: int = 1024, bias: bool = True, downsample_scales: List[int] = [4, 4, 4, 4], nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, pad: str = 'ReflectionPad1d', pad_params: Dict[str, Any] = {})\",\"Bases: Module\",\"MelGAN discriminator module.\",\"Initilize MelGANDiscriminator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"kernel_sizes (List *[*int]) – List of two kernel sizes. The prod will be used for the first conv layer, and the first and the second kernel sizes will be used for the last two layers. For example if kernel_sizes = [5, 3], the first layer kernel size will be 5 * 3 = 15, the last two layers’ kernel size will be 5 and 3, respectively.\",\"channels (int) – Initial number of channels for conv layer.\",\"max_downsample_channels (int) – Maximum number of channels for downsampling layers.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"downsample_scales (List *[*int]) – List of downsampling scales.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"pad (str) – Padding function module name before dilated convolution layer.\",\"pad_params (Dict *[*str,Any]) – Hyperparameters for padding function.\",\"forward(x: Tensor) → List[Tensor]\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of output tensors of each layer.\",\"Return type: List[Tensor]\"]},\"1605\":{\"h\":\"espnet2.gan_tts.melgan.melgan.MelGANGenerator\",\"t\":[\"source\",\"class espnet2.gan_tts.melgan.melgan.MelGANGenerator(in_channels: int = 80, out_channels: int = 1, kernel_size: int = 7, channels: int = 512, bias: bool = True, upsample_scales: List[int] = [8, 8, 2, 2], stack_kernel_size: int = 3, stacks: int = 3, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, pad: str = 'ReflectionPad1d', pad_params: Dict[str, Any] = {}, use_final_nonlinear_activation: bool = True, use_weight_norm: bool = True)\",\"Bases: Module\",\"MelGAN generator module.\",\"Initialize MelGANGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"kernel_size (int) – Kernel size of initial and final conv layer.\",\"channels (int) – Initial number of channels for conv layer.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"upsample_scales (List *[*int]) – List of upsampling scales.\",\"stack_kernel_size (int) – Kernel size of dilated conv layers in residual stack.\",\"stacks (int) – Number of stacks in a single residual stack.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"pad (str) – Padding function module name before dilated convolution layer.\",\"pad_params (Dict *[*str,Any]) – Hyperparameters for padding function.\",\"use_final_nonlinear_activation (torch.nn.Module) – Activation function for the final layer.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:c (Tensor) – Input tensor (B, channels, T).\",\"Returns: Output tensor (B, 1, T ** prod(upsample_scales)).\",\"Return type: Tensor\",\"inference(c: Tensor) → Tensor\",\"Perform inference.\",\"Parameters:c (Tensor) – Input tensor (T, in_channels).\",\"Returns: Output tensor (T ** prod(upsample_scales), out_channels).\",\"Return type: Tensor\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"This initialization follows official implementation manner. https://github.com/descriptinc/melgan-neurips/blob/master/mel2wav/modules.py\"]},\"1606\":{\"h\":\"espnet2.gan_tts.melgan.melgan.MelGANMultiScaleDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_tts.melgan.melgan.MelGANMultiScaleDiscriminator(in_channels: int = 1, out_channels: int = 1, scales: int = 3, downsample_pooling: str = 'AvgPool1d', downsample_pooling_params: Dict[str, Any] = {'count_include_pad': False, 'kernel_size': 4, 'padding': 1, 'stride': 2}, kernel_sizes: List[int] = [5, 3], channels: int = 16, max_downsample_channels: int = 1024, bias: bool = True, downsample_scales: List[int] = [4, 4, 4, 4], nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, pad: str = 'ReflectionPad1d', pad_params: Dict[str, Any] = {}, use_weight_norm: bool = True)\",\"Bases: Module\",\"MelGAN multi-scale discriminator module.\",\"Initilize MelGANMultiScaleDiscriminator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"scales (int) – Number of multi-scales.\",\"downsample_pooling (str) – Pooling module name for downsampling of the inputs.\",\"downsample_pooling_params (Dict *[*str,Any]) – Parameters for the above pooling module.\",\"kernel_sizes (List *[*int]) – List of two kernel sizes. The sum will be used for the first conv layer, and the first and the second kernel sizes will be used for the last two layers.\",\"channels (int) – Initial number of channels for conv layer.\",\"max_downsample_channels (int) – Maximum number of channels for downsampling layers.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"downsample_scales (List *[*int]) – List of downsampling scales.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"pad (str) – Padding function module name before dilated convolution layer.\",\"pad_params (Dict *[*str,Any]) – Hyperparameters for padding function.\",\"use_weight_norm (bool) – Whether to use weight norm.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(x: Tensor) → List[List[Tensor]]\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: List of list of each discriminator outputs, which : consists of each layer output tensors.\",\"Return type: List[List[Tensor]]\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\",\"This initialization follows official implementation manner. https://github.com/descriptinc/melgan-neurips/blob/master/mel2wav/modules.py\"]},\"1607\":{\"h\":\"espnet2.gan_tts.hifigan.loss.MelSpectrogramLoss\",\"t\":[\"source\",\"class espnet2.gan_tts.hifigan.loss.MelSpectrogramLoss(fs: int = 22050, n_fft: int = 1024, hop_length: int = 256, win_length: int | None = None, window: str = 'hann', n_mels: int = 80, fmin: int | None = 0, fmax: int | None = None, center: bool = True, normalized: bool = False, onesided: bool = True, log_base: float | None = 10.0)\",\"Bases: Module\",\"Mel-spectrogram loss.\",\"Initialize Mel-spectrogram loss.\",\"Parameters:\",\"fs (int) – Sampling rate.\",\"n_fft (int) – FFT points.\",\"hop_length (int) – Hop length.\",\"win_length (Optional *[*int]) – Window length.\",\"window (str) – Window type.\",\"n_mels (int) – Number of Mel basis.\",\"fmin (Optional *[*int]) – Minimum frequency for Mel.\",\"fmax (Optional *[*int]) – Maximum frequency for Mel.\",\"center (bool) – Whether to use center window.\",\"normalized (bool) – Whether to use normalized one.\",\"onesided (bool) – Whether to use oneseded one.\",\"log_base (Optional *[*float]) – Log base value.\",\"forward(y_hat: Tensor, y: Tensor, spec: Tensor | None = None, use_mse: bool = False) → Tensor\",\"Calculate Mel-spectrogram loss.\",\"Parameters:\",\"y_hat (Tensor) – Generated waveform tensor (B, 1, T).\",\"y (Tensor) – Groundtruth waveform tensor (B, 1, T).\",\"spec (Optional *[*Tensor]) – Groundtruth linear amplitude spectrum tensor (B, T, n_fft // 2 + 1). if provided, use it instead of groundtruth waveform.\",\"use_l2 (bool) – Whether to use mse_loss instead of l1\",\"Returns: Mel-spectrogram loss value.\",\"Return type: Tensor\"]},\"1608\":{\"h\":\"espnet2.gan_tts.melgan.pqmf.PQMF\",\"t\":[\"source\",\"class espnet2.gan_tts.melgan.pqmf.PQMF(subbands: int = 4, taps: int = 62, cutoff_ratio: float = 0.142, beta: float = 9.0)\",\"Bases: Module\",\"PQMF module.\",\"This module is based on Near-perfect-reconstruction pseudo-QMF banks.\",\"Initilize PQMF module.\",\"The cutoff_ratio and beta parameters are optimized for #subbands = 4. See dicussion in https://github.com/kan-bayashi/ParallelWaveGAN/issues/195.\",\"Parameters:\",\"subbands (int) – The number of subbands.\",\"taps (int) – The number of filter taps.\",\"cutoff_ratio (float) – Cut-off frequency ratio.\",\"beta (float) – Beta coefficient for kaiser window.\",\"analysis(x: Tensor) → Tensor\",\"Analysis with PQMF.\",\"Parameters:x (Tensor) – Input tensor (B, 1, T).\",\"Returns: Output tensor (B, subbands, T // subbands).\",\"Return type: Tensor\",\"synthesis(x: Tensor) → Tensor\",\"Synthesis with PQMF.\",\"Parameters:x (Tensor) – Input tensor (B, subbands, T // subbands).\",\"Returns: Output tensor (B, 1, T).\",\"Return type: Tensor\"]},\"1609\":{\"h\":\"espnet2.gan_tts.parallel_wavegan.parallel_wavegan.ParallelWaveGANDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_tts.parallel_wavegan.parallel_wavegan.ParallelWaveGANDiscriminator(in_channels: int = 1, out_channels: int = 1, kernel_size: int = 3, layers: int = 10, conv_channels: int = 64, dilation_factor: int = 1, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, bias: bool = True, use_weight_norm: bool = True)\",\"Bases: Module\",\"Parallel WaveGAN Discriminator module.\",\"Initialize ParallelWaveGANDiscriminator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"kernel_size (int) – Number of output channels.\",\"layers (int) – Number of conv layers.\",\"conv_channels (int) – Number of chnn layers.\",\"dilation_factor (int) – Dilation factor. For example, if dilation_factor = 2, the dilation will be 2, 4, 8, …, and so on.\",\"nonlinear_activation (str) – Nonlinear function after each conv.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Nonlinear function parameters\",\"bias (bool) – Whether to use bias parameter in conv.\",\"use_weight_norm (bool) – If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(x: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input noise signal (B, 1, T).\",\"Returns: Output tensor (B, 1, T).\",\"Return type: Tensor\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\"]},\"1610\":{\"h\":\"espnet2.gan_tts.parallel_wavegan.parallel_wavegan.ParallelWaveGANGenerator\",\"t\":[\"source\",\"class espnet2.gan_tts.parallel_wavegan.parallel_wavegan.ParallelWaveGANGenerator(in_channels: int = 1, out_channels: int = 1, kernel_size: int = 3, layers: int = 30, stacks: int = 3, residual_channels: int = 64, gate_channels: int = 128, skip_channels: int = 64, aux_channels: int = 80, aux_context_window: int = 2, dropout_rate: float = 0.0, bias: bool = True, use_weight_norm: bool = True, upsample_conditional_features: bool = True, upsample_net: str = 'ConvInUpsampleNetwork', upsample_params: Dict[str, Any] = {'upsample_scales': [4, 4, 4, 4]})\",\"Bases: Module\",\"Parallel WaveGAN Generator module.\",\"Initialize ParallelWaveGANGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"kernel_size (int) – Kernel size of dilated convolution.\",\"layers (int) – Number of residual block layers.\",\"stacks (int) – Number of stacks i.e., dilation cycles.\",\"residual_channels (int) – Number of channels in residual conv.\",\"gate_channels (int) – Number of channels in gated conv.\",\"skip_channels (int) – Number of channels in skip conv.\",\"aux_channels (int) – Number of channels for auxiliary feature conv.\",\"aux_context_window (int) – Context window size for auxiliary feature.\",\"dropout_rate (float) – Dropout rate. 0.0 means no dropout applied.\",\"bias (bool) – Whether to use bias parameter in conv layer.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"upsample_conditional_features (bool) – Whether to use upsampling network.\",\"upsample_net (str) – Upsampling network architecture.\",\"upsample_params (Dict *[*str,Any]) – Upsampling network parameters.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c: Tensor, z: Tensor | None = None) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"c (Tensor) – Local conditioning auxiliary features (B, C ,T_feats).\",\"z (Tensor) – Input noise signal (B, 1, T_wav).\",\"Returns: Output tensor (B, out_channels, T_wav)\",\"Return type: Tensor\",\"inference(c: Tensor, z: Tensor | None = None) → Tensor\",\"Perform inference.\",\"Parameters:\",\"c (Tensor) – Local conditioning auxiliary features (T_feats ,C).\",\"z (Optional *[*Tensor]) – Input noise signal (T_wav, 1).\",\"Returns: Output tensor (T_wav, out_channels)\",\"Return type: Tensor\",\"property receptive_field_size\",\"Return receptive field size.\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\"]},\"1611\":{\"h\":\"espnet2.gan_tts.vits.posterior_encoder.PosteriorEncoder\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.posterior_encoder.PosteriorEncoder(in_channels: int = 513, out_channels: int = 192, hidden_channels: int = 192, kernel_size: int = 5, layers: int = 16, stacks: int = 1, base_dilation: int = 1, global_channels: int = -1, dropout_rate: float = 0.0, bias: bool = True, use_weight_norm: bool = True)\",\"Bases: Module\",\"Posterior encoder module in VITS.\",\"This is a module of posterior encoder described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Initilialize PosteriorEncoder module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"hidden_channels (int) – Number of hidden channels.\",\"kernel_size (int) – Kernel size in WaveNet.\",\"layers (int) – Number of layers of WaveNet.\",\"stacks (int) – Number of repeat stacking of WaveNet.\",\"base_dilation (int) – Base dilation factor.\",\"global_channels (int) – Number of global conditioning channels.\",\"dropout_rate (float) – Dropout rate.\",\"bias (bool) – Whether to use bias parameters in conv.\",\"use_weight_norm (bool) – Whether to apply weight norm.\",\"forward(x: Tensor, x_lengths: Tensor, g: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T_feats).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"Returns: Encoded hidden representation tensor (B, out_channels, T_feats). Tensor: Projected mean tensor (B, out_channels, T_feats). Tensor: Projected scale tensor (B, out_channels, T_feats). Tensor: Mask tensor for input tensor (B, 1, T_feats).\",\"Return type: Tensor\"]},\"1612\":{\"h\":\"espnet2.gan_tts.vits.residual_coupling.ResidualAffineCouplingBlock\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.residual_coupling.ResidualAffineCouplingBlock(in_channels: int = 192, hidden_channels: int = 192, flows: int = 4, kernel_size: int = 5, base_dilation: int = 1, layers: int = 4, global_channels: int = -1, dropout_rate: float = 0.0, use_weight_norm: bool = True, bias: bool = True, use_only_mean: bool = True)\",\"Bases: Module\",\"Residual affine coupling block module.\",\"This is a module of residual affine coupling block, which used as “Flow” in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Initilize ResidualAffineCouplingBlock module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"hidden_channels (int) – Number of hidden channels.\",\"flows (int) – Number of flows.\",\"kernel_size (int) – Kernel size for WaveNet.\",\"base_dilation (int) – Base dilation factor for WaveNet.\",\"layers (int) – Number of layers of WaveNet.\",\"stacks (int) – Number of stacks of WaveNet.\",\"global_channels (int) – Number of global channels.\",\"dropout_rate (float) – Dropout rate.\",\"use_weight_norm (bool) – Whether to use weight normalization in WaveNet.\",\"bias (bool) – Whether to use bias paramters in WaveNet.\",\"use_only_mean (bool) – Whether to estimate only mean.\",\"forward(x: Tensor, x_mask: Tensor, g: Tensor | None = None, inverse: bool = False) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"inverse (bool) – Whether to inverse the flow.\",\"Returns: Output tensor (B, in_channels, T).\",\"Return type: Tensor\"]},\"1613\":{\"h\":\"espnet2.gan_tts.vits.residual_coupling.ResidualAffineCouplingLayer\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.residual_coupling.ResidualAffineCouplingLayer(in_channels: int = 192, hidden_channels: int = 192, kernel_size: int = 5, base_dilation: int = 1, layers: int = 5, stacks: int = 1, global_channels: int = -1, dropout_rate: float = 0.0, use_weight_norm: bool = True, bias: bool = True, use_only_mean: bool = True)\",\"Bases: Module\",\"Residual affine coupling layer.\",\"Initialzie ResidualAffineCouplingLayer module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"hidden_channels (int) – Number of hidden channels.\",\"kernel_size (int) – Kernel size for WaveNet.\",\"base_dilation (int) – Base dilation factor for WaveNet.\",\"layers (int) – Number of layers of WaveNet.\",\"stacks (int) – Number of stacks of WaveNet.\",\"global_channels (int) – Number of global channels.\",\"dropout_rate (float) – Dropout rate.\",\"use_weight_norm (bool) – Whether to use weight normalization in WaveNet.\",\"bias (bool) – Whether to use bias paramters in WaveNet.\",\"use_only_mean (bool) – Whether to estimate only mean.\",\"forward(x: Tensor, x_mask: Tensor, g: Tensor | None = None, inverse: bool = False) → Tensor | Tuple[Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, global_channels, 1).\",\"inverse (bool) – Whether to inverse the flow.\",\"Returns: Output tensor (B, in_channels, T). Tensor: Log-determinant tensor for NLL (B,) if not inverse.\",\"Return type: Tensor\"]},\"1614\":{\"h\":\"espnet2.gan_tts.hifigan.residual_block.ResidualBlock\",\"t\":[\"source\",\"class espnet2.gan_tts.hifigan.residual_block.ResidualBlock(kernel_size: int = 3, channels: int = 512, dilations: List[int] = [1, 3, 5], bias: bool = True, use_additional_convs: bool = True, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.1})\",\"Bases: Module\",\"Residual block module in HiFiGAN.\",\"Initialize ResidualBlock module.\",\"Parameters:\",\"kernel_size (int) – Kernel size of dilation convolution layer.\",\"channels (int) – Number of channels for convolution layer.\",\"dilations (List *[*int]) – List of dilation factors.\",\"use_additional_convs (bool) – Whether to use additional convolution layers.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"forward(x: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input tensor (B, channels, T).\",\"Returns: Output tensor (B, channels, T).\",\"Return type: Tensor\"]},\"1615\":{\"h\":\"espnet2.gan_tts.melgan.residual_stack.ResidualStack\",\"t\":[\"source\",\"class espnet2.gan_tts.melgan.residual_stack.ResidualStack(kernel_size: int = 3, channels: int = 32, dilation: int = 1, bias: bool = True, nonlinear_activation: str = 'LeakyReLU', nonlinear_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, pad: str = 'ReflectionPad1d', pad_params: Dict[str, Any] = {})\",\"Bases: Module\",\"Residual stack module introduced in MelGAN.\",\"Initialize ResidualStack module.\",\"Parameters:\",\"kernel_size (int) – Kernel size of dilation convolution layer.\",\"channels (int) – Number of channels of convolution layers.\",\"dilation (int) – Dilation factor.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"nonlinear_activation (str) – Activation function module name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Hyperparameters for activation function.\",\"pad (str) – Padding function module name before dilated convolution layer.\",\"pad_params (Dict *[*str,Any]) – Hyperparameters for padding function.\",\"forward(c: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:c (Tensor) – Input tensor (B, channels, T).\",\"Returns: Output tensor (B, chennels, T).\",\"Return type: Tensor\"]},\"1616\":{\"h\":\"espnet2.gan_tts.vits.duration_predictor.StochasticDurationPredictor\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.duration_predictor.StochasticDurationPredictor(channels: int = 192, kernel_size: int = 3, dropout_rate: float = 0.5, flows: int = 4, dds_conv_layers: int = 3, global_channels: int = -1)\",\"Bases: Module\",\"Stochastic duration predictor module.\",\"This is a module of stochastic duration predictor described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Initialize StochasticDurationPredictor module.\",\"Parameters:\",\"channels (int) – Number of channels.\",\"kernel_size (int) – Kernel size.\",\"dropout_rate (float) – Dropout rate.\",\"flows (int) – Number of flows.\",\"dds_conv_layers (int) – Number of conv layers in DDS conv.\",\"global_channels (int) – Number of global conditioning channels.\",\"forward(x: Tensor, x_mask: Tensor, w: Tensor | None = None, g: Tensor | None = None, inverse: bool = False, noise_scale: float = 1.0) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, channels, T_text).\",\"x_mask (Tensor) – Mask tensor (B, 1, T_text).\",\"w (Optional *[*Tensor]) – Duration tensor (B, 1, T_text).\",\"g (Optional *[*Tensor]) – Global conditioning tensor (B, channels, 1)\",\"inverse (bool) – Whether to inverse the flow.\",\"noise_scale (float) – Noise scale value.\",\"Returns: If not inverse, negative log-likelihood (NLL) tensor (B,). : If inverse, log-duration tensor (B, 1, T_text).\",\"Return type: Tensor\"]},\"1617\":{\"h\":\"espnet2.gan_tts.parallel_wavegan.upsample.Stretch2d\",\"t\":[\"source\",\"class espnet2.gan_tts.parallel_wavegan.upsample.Stretch2d(x_scale: int, y_scale: int, mode: str = 'nearest')\",\"Bases: Module\",\"Stretch2d module.\",\"Initialize Stretch2d module.\",\"Parameters:\",\"x_scale (int) – X scaling factor (Time axis in spectrogram).\",\"y_scale (int) – Y scaling factor (Frequency axis in spectrogram).\",\"mode (str) – Interpolation mode.\",\"forward(x: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input tensor (B, C, F, T).\",\"Returns: Interpolated tensor (B, C, F * y_scale, T * x_scale),\",\"Return type: Tensor\"]},\"1618\":{\"h\":\"espnet2.gan_tts.style_melgan.style_melgan.StyleMelGANDiscriminator\",\"t\":[\"source\",\"class espnet2.gan_tts.style_melgan.style_melgan.StyleMelGANDiscriminator(repeats: int = 2, window_sizes: List[int] = [512, 1024, 2048, 4096], pqmf_params: List[List[int]] = [[1, None, None, None], [2, 62, 0.267, 9.0], [4, 62, 0.142, 9.0], [8, 62, 0.07949, 9.0]], discriminator_params: Dict[str, Any] = {'bias': True, 'channels': 16, 'downsample_scales': [4, 4, 4, 1], 'kernel_sizes': [5, 3], 'max_downsample_channels': 512, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.2}, 'out_channels': 1, 'pad': 'ReflectionPad1d', 'pad_params': {}}, use_weight_norm: bool = True)\",\"Bases: Module\",\"Style MelGAN disciminator module.\",\"Initilize StyleMelGANDiscriminator module.\",\"Parameters:\",\"repeats (int) – Number of repititons to apply RWD.\",\"window_sizes (List *[*int]) – List of random window sizes.\",\"pqmf_params (List *[*List *[*int]]) – List of list of Parameters for PQMF modules\",\"discriminator_params (Dict *[*str,Any]) – Parameters for base discriminator module.\",\"use_weight_nom (bool) – Whether to apply weight normalization.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(x: Tensor) → List[Tensor]\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input tensor (B, 1, T).\",\"Returns: List of discriminator outputs, #items in the list will be : equal to repeats * #discriminators.\",\"Return type: List\",\"reset_parameters()\",\"Reset parameters.\"]},\"1619\":{\"h\":\"espnet2.gan_tts.style_melgan.style_melgan.StyleMelGANGenerator\",\"t\":[\"source\",\"class espnet2.gan_tts.style_melgan.style_melgan.StyleMelGANGenerator(in_channels: int = 128, aux_channels: int = 80, channels: int = 64, out_channels: int = 1, kernel_size: int = 9, dilation: int = 2, bias: bool = True, noise_upsample_scales: List[int] = [11, 2, 2, 2], noise_upsample_activation: str = 'LeakyReLU', noise_upsample_activation_params: Dict[str, Any] = {'negative_slope': 0.2}, upsample_scales: List[int] = [2, 2, 2, 2, 2, 2, 2, 2, 1], upsample_mode: str = 'nearest', gated_function: str = 'softmax', use_weight_norm: bool = True)\",\"Bases: Module\",\"Style MelGAN generator module.\",\"Initilize StyleMelGANGenerator module.\",\"Parameters:\",\"in_channels (int) – Number of input noise channels.\",\"aux_channels (int) – Number of auxiliary input channels.\",\"channels (int) – Number of channels for conv layer.\",\"out_channels (int) – Number of output channels.\",\"kernel_size (int) – Kernel size of conv layers.\",\"dilation (int) – Dilation factor for conv layers.\",\"bias (bool) – Whether to add bias parameter in convolution layers.\",\"noise_upsample_scales (List *[*int]) – List of noise upsampling scales.\",\"noise_upsample_activation (str) – Activation function module name for noise upsampling.\",\"noise_upsample_activation_params (Dict *[*str,Any]) – Hyperparameters for the above activation function.\",\"upsample_scales (List *[*int]) – List of upsampling scales.\",\"upsample_mode (str) – Upsampling mode in TADE layer.\",\"gated_function (str) – Gated function used in TADEResBlock (“softmax” or “sigmoid”).\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(c: Tensor, z: Tensor | None = None) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"c (Tensor) – Auxiliary input tensor (B, channels, T).\",\"z (Tensor) – Input noise tensor (B, in_channels, 1).\",\"Returns: Output tensor (B, out_channels, T ** prod(upsample_scales)).\",\"Return type: Tensor\",\"inference(c: Tensor) → Tensor\",\"Perform inference.\",\"Parameters:c (Tensor) – Input tensor (T, in_channels).\",\"Returns: Output tensor (T ** prod(upsample_scales), out_channels).\",\"Return type: Tensor\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\",\"reset_parameters()\",\"Reset parameters.\"]},\"1620\":{\"h\":\"espnet2.gan_tts.style_melgan.tade_res_block.TADELayer\",\"t\":[\"source\",\"class espnet2.gan_tts.style_melgan.tade_res_block.TADELayer(in_channels: int = 64, aux_channels: int = 80, kernel_size: int = 9, bias: bool = True, upsample_factor: int = 2, upsample_mode: str = 'nearest')\",\"Bases: Module\",\"TADE Layer module.\",\"Initilize TADELayer module.\",\"Parameters:\",\"in_channels (int) – Number of input channles.\",\"aux_channels (int) – Number of auxirialy channles.\",\"kernel_size (int) – Kernel size.\",\"bias (bool) – Whether to use bias parameter in conv.\",\"upsample_factor (int) – Upsample factor.\",\"upsample_mode (str) – Upsample mode.\",\"forward(x: Tensor, c: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T).\",\"c (Tensor) – Auxiliary input tensor (B, aux_channels, T’).\",\"Returns: Output tensor (B, in_channels, T * in_upsample_factor). Tensor: Upsampled aux tensor (B, in_channels, T * aux_upsample_factor).\",\"Return type: Tensor\"]},\"1621\":{\"h\":\"espnet2.gan_tts.style_melgan.tade_res_block.TADEResBlock\",\"t\":[\"source\",\"class espnet2.gan_tts.style_melgan.tade_res_block.TADEResBlock(in_channels: int = 64, aux_channels: int = 80, kernel_size: int = 9, dilation: int = 2, bias: bool = True, upsample_factor: int = 2, upsample_mode: str = 'nearest', gated_function: str = 'softmax')\",\"Bases: Module\",\"TADEResBlock module.\",\"Initialize TADEResBlock module.\",\"Parameters:\",\"in_channels (int) – Number of input channles.\",\"aux_channels (int) – Number of auxirialy channles.\",\"kernel_size (int) – Kernel size.\",\"bias (bool) – Whether to use bias parameter in conv.\",\"upsample_factor (int) – Upsample factor.\",\"upsample_mode (str) – Upsample mode.\",\"gated_function (str) – Gated function type (softmax of sigmoid).\",\"forward(x: Tensor, c: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, in_channels, T).\",\"c (Tensor) – Auxiliary input tensor (B, aux_channels, T’).\",\"Returns: Output tensor (B, in_channels, T * in_upsample_factor). Tensor: Upsampled auxirialy tensor (B, in_channels, T * in_upsample_factor).\",\"Return type: Tensor\"]},\"1622\":{\"h\":\"espnet2.gan_tts.vits.text_encoder.TextEncoder\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.text_encoder.TextEncoder(vocabs: int, attention_dim: int = 192, attention_heads: int = 2, linear_units: int = 768, blocks: int = 6, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 3, positional_encoding_layer_type: str = 'rel_pos', self_attention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', normalize_before: bool = True, use_macaron_style: bool = False, use_conformer_conv: bool = False, conformer_kernel_size: int = 7, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.0, attention_dropout_rate: float = 0.0)\",\"Bases: Module\",\"Text encoder module in VITS.\",\"This is a module of text encoder described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Instead of the relative positional Transformer, we use conformer architecture as the encoder module, which contains additional convolution layers.\",\"Initialize TextEncoder module.\",\"Parameters:\",\"vocabs (int) – Vocabulary size.\",\"attention_dim (int) – Attention dimension.\",\"attention_heads (int) – Number of attention heads.\",\"linear_units (int) – Number of linear units of positionwise layers.\",\"blocks (int) – Number of encoder blocks.\",\"positionwise_layer_type (str) – Positionwise layer type.\",\"positionwise_conv_kernel_size (int) – Positionwise layer’s kernel size.\",\"positional_encoding_layer_type (str) – Positional encoding layer type.\",\"self_attention_layer_type (str) – Self-attention layer type.\",\"activation_type (str) – Activation function type.\",\"normalize_before (bool) – Whether to apply LayerNorm before attention.\",\"use_macaron_style (bool) – Whether to use macaron style components.\",\"use_conformer_conv (bool) – Whether to use conformer conv layers.\",\"conformer_kernel_size (int) – Conformer’s conv kernel size.\",\"dropout_rate (float) – Dropout rate.\",\"positional_dropout_rate (float) – Dropout rate for positional encoding.\",\"attention_dropout_rate (float) – Dropout rate for attention.\",\"forward(x: Tensor, x_lengths: Tensor) → Tuple[Tensor, Tensor, Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input index tensor (B, T_text).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"Returns: Encoded hidden representation (B, attention_dim, T_text). Tensor: Projected mean tensor (B, attention_dim, T_text). Tensor: Projected scale tensor (B, attention_dim, T_text). Tensor: Mask tensor for input tensor (B, 1, T_text).\",\"Return type: Tensor\"]},\"1623\":{\"h\":\"espnet2.gan_tts.vits.flow.Transpose\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.flow.Transpose(dim1: int, dim2: int)\",\"Bases: Module\",\"Transpose module for torch.nn.Sequential().\",\"Initialize Transpose module.\",\"forward(x: Tensor) → Tensor\",\"Transpose.\"]},\"1624\":{\"h\":\"espnet2.gan_tts.parallel_wavegan.upsample.UpsampleNetwork\",\"t\":[\"source\",\"class espnet2.gan_tts.parallel_wavegan.upsample.UpsampleNetwork(upsample_scales: List[int], nonlinear_activation: str | None = None, nonlinear_activation_params: Dict[str, Any] = {}, interpolate_mode: str = 'nearest', freq_axis_kernel_size: int = 1)\",\"Bases: Module\",\"Upsampling network module.\",\"Initialize UpsampleNetwork module.\",\"Parameters:\",\"upsample_scales (List *[*int]) – List of upsampling scales.\",\"nonlinear_activation (Optional *[*str]) – Activation function name.\",\"nonlinear_activation_params (Dict *[*str,Any]) – Arguments for the specified activation function.\",\"interpolate_mode (str) – Interpolation mode.\",\"freq_axis_kernel_size (int) – Kernel size in the direction of frequency axis.\",\"forward(c: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:c – Input tensor (B, C, T_feats).\",\"Returns: Upsampled tensor (B, C, T_wav).\",\"Return type: Tensor\"]},\"1625\":{\"h\":\"espnet2.gan_tts.vits.vits.VITS\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.vits.VITS(idim: int, odim: int, sampling_rate: int = 22050, generator_type: str = 'vits_generator', generator_params: Dict[str, Any] = {'decoder_channels': 512, 'decoder_kernel_size': 7, 'decoder_resblock_dilations': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'decoder_resblock_kernel_sizes': [3, 7, 11], 'decoder_upsample_kernel_sizes': [16, 16, 4, 4], 'decoder_upsample_scales': [8, 8, 2, 2], 'flow_base_dilation': 1, 'flow_dropout_rate': 0.0, 'flow_flows': 4, 'flow_kernel_size': 5, 'flow_layers': 4, 'global_channels': -1, 'hidden_channels': 192, 'langs': None, 'posterior_encoder_base_dilation': 1, 'posterior_encoder_dropout_rate': 0.0, 'posterior_encoder_kernel_size': 5, 'posterior_encoder_layers': 16, 'posterior_encoder_stacks': 1, 'segment_size': 32, 'spk_embed_dim': None, 'spks': None, 'stochastic_duration_predictor_dds_conv_layers': 3, 'stochastic_duration_predictor_dropout_rate': 0.5, 'stochastic_duration_predictor_flows': 4, 'stochastic_duration_predictor_kernel_size': 3, 'text_encoder_activation_type': 'swish', 'text_encoder_attention_dropout_rate': 0.0, 'text_encoder_attention_heads': 2, 'text_encoder_blocks': 6, 'text_encoder_conformer_kernel_size': 7, 'text_encoder_dropout_rate': 0.1, 'text_encoder_ffn_expand': 4, 'text_encoder_normalize_before': True, 'text_encoder_positional_dropout_rate': 0.0, 'text_encoder_positional_encoding_layer_type': 'rel_pos', 'text_encoder_positionwise_conv_kernel_size': 1, 'text_encoder_positionwise_layer_type': 'conv1d', 'text_encoder_self_attention_layer_type': 'rel_selfattn', 'use_conformer_conv_in_text_encoder': True, 'use_macaron_style_in_text_encoder': True, 'use_only_mean_in_flow': True, 'use_weight_norm_in_decoder': True, 'use_weight_norm_in_flow': True, 'use_weight_norm_in_posterior_encoder': True}, discriminator_type: str = 'hifigan_multi_scale_multi_period_discriminator', discriminator_params: Dict[str, Any] = {'follow_official_norm': False, 'period_discriminator_params': {'bias': True, 'channels': 32, 'downsample_scales': [3, 3, 3, 3, 1], 'in_channels': 1, 'kernel_sizes': [5, 3], 'max_downsample_channels': 1024, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'periods': [2, 3, 5, 7, 11], 'scale_discriminator_params': {'bias': True, 'channels': 128, 'downsample_scales': [2, 2, 4, 4, 1], 'in_channels': 1, 'kernel_sizes': [15, 41, 5, 3], 'max_downsample_channels': 1024, 'max_groups': 16, 'nonlinear_activation': 'LeakyReLU', 'nonlinear_activation_params': {'negative_slope': 0.1}, 'out_channels': 1, 'use_spectral_norm': False, 'use_weight_norm': True}, 'scale_downsample_pooling': 'AvgPool1d', 'scale_downsample_pooling_params': {'kernel_size': 4, 'padding': 2, 'stride': 2}, 'scales': 1}, generator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, discriminator_adv_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'loss_type': 'mse'}, feat_match_loss_params: Dict[str, Any] = {'average_by_discriminators': False, 'average_by_layers': False, 'include_final_outputs': True}, mel_loss_params: Dict[str, Any] = {'fmax': None, 'fmin': 0, 'fs': 22050, 'hop_length': 256, 'log_base': None, 'n_fft': 1024, 'n_mels': 80, 'win_length': None, 'window': 'hann'}, lambda_adv: float = 1.0, lambda_mel: float = 45.0, lambda_feat_match: float = 2.0, lambda_dur: float = 1.0, lambda_kl: float = 1.0, cache_generator_outputs: bool = True, plot_pred_mos: bool = False, mos_pred_tool: str = 'utmos')\",\"Bases: AbsGANTTS\",\"VITS module (generator + discriminator).\",\"This is a module of VITS described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"Initialize VITS module.\",\"Parameters:\",\"idim (int) – Input vocabrary size.\",\"odim (int) – Acoustic feature dimension. The actual output channels will be 1 since VITS is the end-to-end text-to-wave model but for the compatibility odim is used to indicate the acoustic feature dimension.\",\"sampling_rate (int) – Sampling rate, not used for the training but it will be referred in saving waveform during the inference.\",\"generator_type (str) – Generator type.\",\"generator_params (Dict *[*str,Any]) – Parameter dict for generator.\",\"discriminator_type (str) – Discriminator type.\",\"discriminator_params (Dict *[*str,Any]) – Parameter dict for discriminator.\",\"generator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for generator adversarial loss.\",\"discriminator_adv_loss_params (Dict *[*str,Any]) – Parameter dict for discriminator adversarial loss.\",\"feat_match_loss_params (Dict *[*str,Any]) – Parameter dict for feat match loss.\",\"mel_loss_params (Dict *[*str,Any]) – Parameter dict for mel loss.\",\"lambda_adv (float) – Loss scaling coefficient for adversarial loss.\",\"lambda_mel (float) – Loss scaling coefficient for mel spectrogram loss.\",\"lambda_feat_match (float) – Loss scaling coefficient for feat match loss.\",\"lambda_dur (float) – Loss scaling coefficient for duration loss.\",\"lambda_kl (float) – Loss scaling coefficient for KL divergence loss.\",\"cache_generator_outputs (bool) – Whether to cache generator outputs.\",\"plot_pred_mos (bool) – Whether to plot predicted MOS during the training.\",\"mos_pred_tool (str) – MOS prediction tool name.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, forward_generator: bool = True) → Dict[str, Any]\",\"Perform generator forward.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, T_feats, aux_channels).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B,) or (B, 1).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, spk_embed_dim).\",\"lids (Optional *[*Tensor]) – Language index tensor (B,) or (B, 1).\",\"forward_generator (bool) – Whether to forward generator.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\",\"inference(text: Tensor, feats: Tensor | None = None, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, durations: Tensor | None = None, noise_scale: float = 0.667, noise_scale_dur: float = 0.8, alpha: float = 1.0, max_len: int | None = None, use_teacher_forcing: bool = False) → Dict[str, Tensor]\",\"Run inference.\",\"Parameters:\",\"text (Tensor) – Input text index tensor (T_text,).\",\"feats (Tensor) – Feature tensor (T_feats, aux_channels).\",\"sids (Tensor) – Speaker index tensor (1,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (spk_embed_dim,).\",\"lids (Tensor) – Language index tensor (1,).\",\"durations (Tensor) – Ground-truth duration tensor (T_text,).\",\"noise_scale (float) – Noise scale value for flow.\",\"noise_scale_dur (float) – Noise scale value for duration predictor.\",\"alpha (float) – Alpha parameter to control the speed of generated speech.\",\"max_len (Optional *[*int]) – Maximum length.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns:\",\"wav (Tensor): Generated waveform tensor (T_wav,).\",\"att_w (Tensor): Monotonic attention weight tensor (T_feats, T_text).\",\"duration (Tensor): Predicted duration tensor (T_text,).\",\"Return type: Dict[str, Tensor]\",\"property require_raw_speech\",\"Return whether or not speech is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\"]},\"1626\":{\"h\":\"espnet2.gan_tts.vits.generator.VITSGenerator\",\"t\":[\"source\",\"class espnet2.gan_tts.vits.generator.VITSGenerator(vocabs: int, aux_channels: int = 513, hidden_channels: int = 192, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, global_channels: int = -1, segment_size: int = 32, text_encoder_attention_heads: int = 2, text_encoder_ffn_expand: int = 4, text_encoder_blocks: int = 6, text_encoder_positionwise_layer_type: str = 'conv1d', text_encoder_positionwise_conv_kernel_size: int = 1, text_encoder_positional_encoding_layer_type: str = 'rel_pos', text_encoder_self_attention_layer_type: str = 'rel_selfattn', text_encoder_activation_type: str = 'swish', text_encoder_normalize_before: bool = True, text_encoder_dropout_rate: float = 0.1, text_encoder_positional_dropout_rate: float = 0.0, text_encoder_attention_dropout_rate: float = 0.0, text_encoder_conformer_kernel_size: int = 7, use_macaron_style_in_text_encoder: bool = True, use_conformer_conv_in_text_encoder: bool = True, decoder_kernel_size: int = 7, decoder_channels: int = 512, decoder_upsample_scales: List[int] = [8, 8, 2, 2], decoder_upsample_kernel_sizes: List[int] = [16, 16, 4, 4], decoder_resblock_kernel_sizes: List[int] = [3, 7, 11], decoder_resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], use_weight_norm_in_decoder: bool = True, posterior_encoder_kernel_size: int = 5, posterior_encoder_layers: int = 16, posterior_encoder_stacks: int = 1, posterior_encoder_base_dilation: int = 1, posterior_encoder_dropout_rate: float = 0.0, use_weight_norm_in_posterior_encoder: bool = True, flow_flows: int = 4, flow_kernel_size: int = 5, flow_base_dilation: int = 1, flow_layers: int = 4, flow_dropout_rate: float = 0.0, use_weight_norm_in_flow: bool = True, use_only_mean_in_flow: bool = True, stochastic_duration_predictor_kernel_size: int = 3, stochastic_duration_predictor_dropout_rate: float = 0.5, stochastic_duration_predictor_flows: int = 4, stochastic_duration_predictor_dds_conv_layers: int = 3)\",\"Bases: Module\",\"Generator module in VITS.\",\"This is a module of VITS described in Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.\",\"As text encoder, we use conformer architecture instead of the relative positional Transformer, which contains additional convolution layers.\",\"Initialize VITS generator module.\",\"Parameters:\",\"vocabs (int) – Input vocabulary size.\",\"aux_channels (int) – Number of acoustic feature channels.\",\"hidden_channels (int) – Number of hidden channels.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"global_channels (int) – Number of global conditioning channels.\",\"segment_size (int) – Segment size for decoder.\",\"text_encoder_attention_heads (int) – Number of heads in conformer block of text encoder.\",\"text_encoder_ffn_expand (int) – Expansion ratio of FFN in conformer block of text encoder.\",\"text_encoder_blocks (int) – Number of conformer blocks in text encoder.\",\"text_encoder_positionwise_layer_type (str) – Position-wise layer type in conformer block of text encoder.\",\"text_encoder_positionwise_conv_kernel_size (int) – Position-wise convolution kernel size in conformer block of text encoder. Only used when the above layer type is conv1d or conv1d-linear.\",\"text_encoder_positional_encoding_layer_type (str) – Positional encoding layer type in conformer block of text encoder.\",\"text_encoder_self_attention_layer_type (str) – Self-attention layer type in conformer block of text encoder.\",\"text_encoder_activation_type (str) – Activation function type in conformer block of text encoder.\",\"text_encoder_normalize_before (bool) – Whether to apply layer norm before self-attention in conformer block of text encoder.\",\"text_encoder_dropout_rate (float) – Dropout rate in conformer block of text encoder.\",\"text_encoder_positional_dropout_rate (float) – Dropout rate for positional encoding in conformer block of text encoder.\",\"text_encoder_attention_dropout_rate (float) – Dropout rate for attention in conformer block of text encoder.\",\"text_encoder_conformer_kernel_size (int) – Conformer conv kernel size. It will be used when only use_conformer_conv_in_text_encoder = True.\",\"use_macaron_style_in_text_encoder (bool) – Whether to use macaron style FFN in conformer block of text encoder.\",\"use_conformer_conv_in_text_encoder (bool) – Whether to use covolution in conformer block of text encoder.\",\"decoder_kernel_size (int) – Decoder kernel size.\",\"decoder_channels (int) – Number of decoder initial channels.\",\"decoder_upsample_scales (List *[*int]) – List of upsampling scales in decoder.\",\"decoder_upsample_kernel_sizes (List *[*int]) – List of kernel size for upsampling layers in decoder.\",\"decoder_resblock_kernel_sizes (List *[*int]) – List of kernel size for resblocks in decoder.\",\"decoder_resblock_dilations (List *[*List *[*int]]) – List of list of dilations for resblocks in decoder.\",\"use_weight_norm_in_decoder (bool) – Whether to apply weight normalization in decoder.\",\"posterior_encoder_kernel_size (int) – Posterior encoder kernel size.\",\"posterior_encoder_layers (int) – Number of layers of posterior encoder.\",\"posterior_encoder_stacks (int) – Number of stacks of posterior encoder.\",\"posterior_encoder_base_dilation (int) – Base dilation of posterior encoder.\",\"posterior_encoder_dropout_rate (float) – Dropout rate for posterior encoder.\",\"use_weight_norm_in_posterior_encoder (bool) – Whether to apply weight normalization in posterior encoder.\",\"flow_flows (int) – Number of flows in flow.\",\"flow_kernel_size (int) – Kernel size in flow.\",\"flow_base_dilation (int) – Base dilation in flow.\",\"flow_layers (int) – Number of layers in flow.\",\"flow_dropout_rate (float) – Dropout rate in flow\",\"use_weight_norm_in_flow (bool) – Whether to apply weight normalization in flow.\",\"use_only_mean_in_flow (bool) – Whether to use only mean in flow.\",\"stochastic_duration_predictor_kernel_size (int) – Kernel size in stochastic duration predictor.\",\"stochastic_duration_predictor_dropout_rate (float) – Dropout rate in stochastic duration predictor.\",\"stochastic_duration_predictor_flows (int) – Number of flows in stochastic duration predictor.\",\"stochastic_duration_predictor_dds_conv_layers (int) – Number of DDS conv layers in stochastic duration predictor.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, aux_channels, T_feats).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B,) or (B, 1).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, spk_embed_dim).\",\"lids (Optional *[*Tensor]) – Language index tensor (B,) or (B, 1).\",\"Returns: Waveform tensor (B, 1, segment_size * upsample_factor). Tensor: Duration negative log-likelihood (NLL) tensor (B,). Tensor: Monotonic attention weight tensor (B, 1, T_feats, T_text). Tensor: Segments start index tensor (B,). Tensor: Text mask tensor (B, 1, T_text). Tensor: Feature mask tensor (B, 1, T_feats). tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]: \",\"Tensor: Posterior encoder hidden representation (B, H, T_feats).\",\"Tensor: Flow hidden representation (B, H, T_feats).\",\"Tensor: Expanded text encoder projected mean (B, H, T_feats).\",\"Tensor: Expanded text encoder projected scale (B, H, T_feats).\",\"Tensor: Posterior encoder projected mean (B, H, T_feats).\",\"Tensor: Posterior encoder projected scale (B, H, T_feats).\",\"Return type: Tensor\",\"inference(text: Tensor, text_lengths: Tensor, feats: Tensor | None = None, feats_lengths: Tensor | None = None, sids: Tensor | None = None, spembs: Tensor | None = None, lids: Tensor | None = None, dur: Tensor | None = None, noise_scale: float = 0.667, noise_scale_dur: float = 0.8, alpha: float = 1.0, max_len: int | None = None, use_teacher_forcing: bool = False) → Tuple[Tensor, Tensor, Tensor]\",\"Run inference.\",\"Parameters:\",\"text (Tensor) – Input text index tensor (B, T_text,).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats (Tensor) – Feature tensor (B, aux_channels, T_feats,).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"sids (Optional *[*Tensor]) – Speaker index tensor (B,) or (B, 1).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, spk_embed_dim).\",\"lids (Optional *[*Tensor]) – Language index tensor (B,) or (B, 1).\",\"dur (Optional *[*Tensor]) – Ground-truth duration (B, T_text,). If provided, skip the prediction of durations (i.e., teacher forcing).\",\"noise_scale (float) – Noise scale parameter for flow.\",\"noise_scale_dur (float) – Noise scale parameter for duration predictor.\",\"alpha (float) – Alpha parameter to control the speed of generated speech.\",\"max_len (Optional *[*int]) – Maximum length of acoustic feature sequence.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Generated waveform tensor (B, T_wav). Tensor: Monotonic attention weight tensor (B, T_feats, T_text). Tensor: Duration tensor (B, T_text).\",\"Return type: Tensor\"]},\"1627\":{\"h\":\"espnet2.gan_tts.jets.loss.VarianceLoss\",\"t\":[\"source\",\"class espnet2.gan_tts.jets.loss.VarianceLoss(use_masking: bool = True, use_weighted_masking: bool = False)\",\"Bases: Module\",\"Initialize JETS variance loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"forward(d_outs: Tensor, ds: Tensor, p_outs: Tensor, ps: Tensor, e_outs: Tensor, es: Tensor, ilens: Tensor) → Tuple[Tensor, Tensor, Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"d_outs (LongTensor) – Batch of outputs of duration predictor (B, T_text).\",\"ds (LongTensor) – Batch of durations (B, T_text).\",\"p_outs (Tensor) – Batch of outputs of pitch predictor (B, T_text, 1).\",\"ps (Tensor) – Batch of target token-averaged pitch (B, T_text, 1).\",\"e_outs (Tensor) – Batch of outputs of energy predictor (B, T_text, 1).\",\"es (Tensor) – Batch of target token-averaged energy (B, T_text, 1).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"Returns: Duration predictor loss value. Tensor: Pitch predictor loss value. Tensor: Energy predictor loss value.\",\"Return type: Tensor\"]},\"1628\":{\"h\":\"espnet2.gan_tts.wavenet.wavenet.WaveNet\",\"t\":[\"source\",\"class espnet2.gan_tts.wavenet.wavenet.WaveNet(in_channels: int = 1, out_channels: int = 1, kernel_size: int = 3, layers: int = 30, stacks: int = 3, base_dilation: int = 2, residual_channels: int = 64, aux_channels: int = -1, gate_channels: int = 128, skip_channels: int = 64, global_channels: int = -1, dropout_rate: float = 0.0, bias: bool = True, use_weight_norm: bool = True, use_first_conv: bool = False, use_last_conv: bool = False, scale_residual: bool = False, scale_skip_connect: bool = False)\",\"Bases: Module\",\"WaveNet with global conditioning.\",\"Initialize WaveNet module.\",\"Parameters:\",\"in_channels (int) – Number of input channels.\",\"out_channels (int) – Number of output channels.\",\"kernel_size (int) – Kernel size of dilated convolution.\",\"layers (int) – Number of residual block layers.\",\"stacks (int) – Number of stacks i.e., dilation cycles.\",\"base_dilation (int) – Base dilation factor.\",\"residual_channels (int) – Number of channels in residual conv.\",\"gate_channels (int) – Number of channels in gated conv.\",\"skip_channels (int) – Number of channels in skip conv.\",\"aux_channels (int) – Number of channels for local conditioning feature.\",\"global_channels (int) – Number of channels for global conditioning feature.\",\"dropout_rate (float) – Dropout rate. 0.0 means no dropout applied.\",\"bias (bool) – Whether to use bias parameter in conv layer.\",\"use_weight_norm (bool) – Whether to use weight norm. If set to true, it will be applied to all of the conv layers.\",\"use_first_conv (bool) – Whether to use the first conv layers.\",\"use_last_conv (bool) – Whether to use the last conv layers.\",\"scale_residual (bool) – Whether to scale the residual outputs.\",\"scale_skip_connect (bool) – Whether to scale the skip connection outputs.\",\"apply_weight_norm()\",\"Apply weight normalization module from all of the layers.\",\"forward(x: Tensor, x_mask: Tensor | None = None, c: Tensor | None = None, g: Tensor | None = None) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"x (Tensor) – Input noise signal (B, 1, T) if use_first_conv else (B, residual_channels, T).\",\"x_mask (Optional *[*Tensor]) – Mask tensor (B, 1, T).\",\"c (Optional *[*Tensor]) – Local conditioning features (B, aux_channels, T).\",\"g (Optional *[*Tensor]) – Global conditioning features (B, global_channels, 1).\",\"Returns: Output tensor (B, out_channels, T) if use_last_conv else : (B, residual_channels, T).\",\"Return type: Tensor\",\"property receptive_field_size : int\",\"Return receptive field size.\",\"remove_weight_norm()\",\"Remove weight normalization module from all of the layers.\"]},\"1629\":{\"h\":\"espnet2.gan_tts.jets.alignments.average_by_duration\",\"t\":[\"source\",\"espnet2.gan_tts.jets.alignments.average_by_duration(ds, xs, text_lengths, feats_lengths)\",\"Average frame-level features into token-level according to durations\",\"Parameters:\",\"ds (Tensor) – Batched token duration (B, T_text).\",\"xs (Tensor) – Batched feature sequences to be averaged (B, T_feats).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats_lengths (Tensor) – Feature length tensor (B,).\",\"Returns: Batched feature averaged according to the token duration (B, T_text).\",\"Return type: Tensor\"]},\"1630\":{\"h\":\"espnet2.gan_tts.vits.monotonic_align.setup.build_ext\",\"t\":[\"source\"]},\"1631\":{\"h\":\"espnet2.gan_tts.melgan.pqmf.design_prototype_filter\",\"t\":[\"source\",\"espnet2.gan_tts.melgan.pqmf.design_prototype_filter(taps: int = 62, cutoff_ratio: float = 0.142, beta: float = 9.0) → ndarray\",\"Design prototype filter for PQMF.\",\"This method is based on A Kaiser window approach for the design of prototype filters of cosine modulated filterbanks.\",\"Parameters:\",\"taps (int) – The number of filter taps.\",\"cutoff_ratio (float) – Cut-off frequency ratio.\",\"beta (float) – Beta coefficient for kaiser window.\",\"Returns: Impluse response of prototype filter (taps + 1,).\",\"Return type: ndarray\"]},\"1632\":{\"h\":\"espnet2.gan_tts.utils.get_random_segments.get_random_segments\",\"t\":[\"source\",\"espnet2.gan_tts.utils.get_random_segments.get_random_segments(x: Tensor, x_lengths: Tensor, segment_size: int) → Tuple[Tensor, Tensor]\",\"Get random segments.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, C, T).\",\"x_lengths (Tensor) – Length tensor (B,).\",\"segment_size (int) – Segment size.\",\"Returns: Segmented tensor (B, C, segment_size). Tensor: Start index tensor (B,).\",\"Return type: Tensor\"]},\"1633\":{\"h\":\"espnet2.gan_tts.utils.get_random_segments.get_segments\",\"t\":[\"source\",\"espnet2.gan_tts.utils.get_random_segments.get_segments(x: Tensor, start_idxs: Tensor, segment_size: int) → Tensor\",\"Get segments.\",\"Parameters:\",\"x (Tensor) – Input tensor (B, C, T).\",\"start_idxs (Tensor) – Start index tensor (B,).\",\"segment_size (int) – Segment size.\",\"Returns: Segmented tensor (B, C, segment_size).\",\"Return type: Tensor\"]},\"1634\":{\"h\":\"espnet2.gan_tts.vits.transform.piecewise_rational_quadratic_transform\",\"t\":[\"source\",\"espnet2.gan_tts.vits.transform.piecewise_rational_quadratic_transform(inputs, unnormalized_widths, unnormalized_heights, unnormalized_derivatives, inverse=False, tails=None, tail_bound=1.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001)\"]},\"1635\":{\"h\":\"espnet2.gan_tts.vits.transform.rational_quadratic_spline\",\"t\":[\"source\",\"espnet2.gan_tts.vits.transform.rational_quadratic_spline(inputs, unnormalized_widths, unnormalized_heights, unnormalized_derivatives, inverse=False, left=0.0, right=1.0, bottom=0.0, top=1.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001)\"]},\"1636\":{\"h\":\"espnet2.gan_tts.vits.transform.unconstrained_rational_quadratic_spline\",\"t\":[\"source\",\"espnet2.gan_tts.vits.transform.unconstrained_rational_quadratic_spline(inputs, unnormalized_widths, unnormalized_heights, unnormalized_derivatives, inverse=False, tails='linear', tail_bound=1.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001)\"]},\"1637\":{\"h\":\"espnet2.gan_tts.jets.alignments.viterbi_decode\",\"t\":[\"source\",\"espnet2.gan_tts.jets.alignments.viterbi_decode(log_p_attn, text_lengths, feats_lengths)\",\"Extract duration from an attention probability matrix\",\"Parameters:\",\"log_p_attn (Tensor) – Batched log probability of attention matrix (B, T_feats, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"feats_legnths (Tensor) – Feature length tensor (B,).\",\"Returns: Batched token duration extracted from log_p_attn (B, T_text). Tensor: Binarization loss tensor ().\",\"Return type: Tensor\"]},\"1638\":{\"h\":\"espnet2.hubert.hubert_loss.HubertPretrainLoss\",\"t\":[\"source\",\"class espnet2.hubert.hubert_loss.HubertPretrainLoss(pred_masked_weight: float = 1.0, pred_nomask_weight: float = 0.0, loss_weights: float = 10.0)\",\"Bases: Module\",\"Hubert criterion module.\",\"Parameters:\",\"pred_masked_weight – weight for predictive loss for masked frames\",\"pred_nomask_weight – weight for predictive loss for unmasked frames\",\"loss_weights – weights for additional loss terms (not first one)\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(model, enc_outputs, reduce=True)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1639\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1640\":{\"h\":\"espnet2.hubert.espnet_model.HubertPretrainModel\",\"t\":[\"source\",\"class espnet2.hubert.espnet_model.HubertPretrainModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = False, report_wer: bool = False, sym_space: str = '<space>', sym_blank: str = '<blank>', pred_masked_weight: float = 1.0, pred_nomask_weight: float = 0.0, loss_weights: float = 0.0, **kwargs)\",\"Bases: AbsESPnetModel\",\"Hubert Pretrain model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"compute_correct(logits)\",\"encode(speech: Tensor, speech_lengths: Tensor, y_pad: Tensor, y_pad_length: Tensor) → Tuple[Tensor, Tensor]\",\"Frontend + Encoder. Note that this method is used by asr_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"y_pad – (Batch, Length, …)\",\"y_pad_length – (Batch, )\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\"]},\"1641\":{\"h\":\"espnet2.hubert.espnet_model.TorchAudioHubertPretrainModel\",\"t\":[\"source\",\"class espnet2.hubert.espnet_model.TorchAudioHubertPretrainModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, ignore_id: int = -1, **kwargs)\",\"Bases: AbsESPnetModel\",\"TorchAudio Hubert Pretrain model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"encode(speech: Tensor, speech_lengths: Tensor, y_pad: Tensor, y_pad_length: Tensor) → Tuple[Tensor, Tensor]\",\"Frontend + Encoder. Note that this method is used by asr_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"y_pad – (Batch, Length, …)\",\"y_pad_length – (Batch, )\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\"]},\"1642\":{\"h\":\"espnet2.iterators.abs_iter_factory.AbsIterFactory\",\"t\":[\"source\",\"class espnet2.iterators.abs_iter_factory.AbsIterFactory\",\"Bases: ABC\",\"abstract build_iter(epoch: int, shuffle: bool | None = None) → Iterator\"]},\"1643\":{\"h\":\"espnet2.iterators.category_chunk_iter_factory.CategoryChunkIterFactory\",\"t\":[\"source\",\"class espnet2.iterators.category_chunk_iter_factory.CategoryChunkIterFactory(dataset, batch_size: int, batches: AbsSampler | Sequence[Sequence[Any]], chunk_length: int | str, chunk_shift_ratio: float = 0.5, num_cache_chunks: int = 1024, num_samples_per_epoch: int | None = None, seed: int = 0, shuffle: bool = False, num_workers: int = 0, collate_fn=None, pin_memory: bool = False, excluded_key_prefixes: List[str] | None = None, discard_short_samples: bool = True, default_fs: int | None = None, chunk_max_abs_length: int | None = None)\",\"Bases: AbsIterFactory\",\"Creates chunks from a sequence\"]},\"1644\":{\"h\":\"Examples\",\"t\":[\">>> batches = [[\\\"id1\\\"], [\\\"id2\\\"], ...] >>> batch_size = 128 >>> chunk_length = 1000 >>> iter_factory = ChunkIterFactory(dataset, batches, batch_size, chunk_length) >>> it = iter_factory.build_iter(epoch) >>> for ids, batch in it: ... ...\",\"This class is a modified class from ChunkIterFacotry.\",\"Get categorical balanced chunks for batch instead of per category\",\"TODO(jiatong): add additional setup to save/load shuffled chunk information\",\"build_iter(epoch: int, shuffle: bool | None = None) → Iterator[Tuple[List[str], Dict[str, Tensor]]]\",\"prepare_for_collate(id_list, batches)\"]},\"1645\":{\"h\":\"espnet2.iterators.category_iter_factory.CategoryIterFactory\",\"t\":[\"source\",\"class espnet2.iterators.category_iter_factory.CategoryIterFactory(dataset, batches: AbsSampler | Sequence[Sequence[Any]], num_iters_per_epoch: int | None = None, seed: int = 0, sampler_args: dict | None = None, batch_type: str = 'catbel', shuffle: bool = False, num_workers: int = 0, collate_fn=None, pin_memory: bool = False)\",\"Bases: AbsIterFactory\",\"Build iterator for each epoch.\",\"This class simply creates pytorch DataLoader except for the following points:\",\"The random seed is decided according to the number of epochs. This feature\",\"guarantees reproducibility when resuming from middle of training process.\",\"Enable to restrict the number of samples for one epoch. This features controls the interval number between training and evaluation.\",\"Parameters:\",\"dataset – The dataset to iterate over\",\"batches – The batches to iterate over\",\"num_iters_per_epoch – The number of iterations per epoch\",\"seed – The random seed\",\"sampler_args – The arguments to pass to the batch sampler\",\"batch_type –\",\"The type of batch sampler to use: catbel: Category-balanced batch sampler,\",\"ensures equal representation of all categories in each batch\",\"catpow: Category-power batch sampler, : applies power law sampling based on category frequency to address class imbalance\",\"catpow_dataset: Category-power batch sampler with dataset-level : upsampling, performs dataset-level upsampling before applying power law sampling on categories within each dataset\",\"shuffle – Whether to shuffle the batches\",\"num_workers – The number of workers to use\",\"collate_fn – The collate function to use\",\"pin_memory – Whether to pin the memory\",\"build_iter(epoch: int, shuffle: bool | None = None) → DataLoader\"]},\"1646\":{\"h\":\"espnet2.iterators.chunk_iter_factory.ChunkIterFactory\",\"t\":[\"source\",\"class espnet2.iterators.chunk_iter_factory.ChunkIterFactory(dataset, batch_size: int, batches: AbsSampler | Sequence[Sequence[Any]], chunk_length: int | str, chunk_shift_ratio: float = 0.5, num_cache_chunks: int = 1024, num_samples_per_epoch: int | None = None, seed: int = 0, shuffle: bool = False, num_workers: int = 0, collate_fn=None, pin_memory: bool = False, excluded_key_prefixes: List[str] | None = None, discard_short_samples: bool = True, default_fs: int | None = None, chunk_max_abs_length: int | None = None)\",\"Bases: AbsIterFactory\",\"Creates chunks from a sequence\"]},\"1647\":{\"h\":\"Examples\",\"t\":[\">>> batches = [[\\\"id1\\\"], [\\\"id2\\\"], ...] >>> batch_size = 128 >>> chunk_length = 1000 >>> iter_factory = ChunkIterFactory(dataset, batches, batch_size, chunk_length) >>> it = iter_factory.build_iter(epoch) >>> for ids, batch in it: ... ...\",\"The number of mini-batches are varied in each epochs and we can’t get the number in advance because IterFactory doesn’t be given to the length information.\",\"Since the first reason, “num_iters_per_epoch” can’t be implemented for this iterator. Instead of it, “num_samples_per_epoch” is implemented.\",\"build_iter(epoch: int, shuffle: bool | None = None) → Iterator[Tuple[List[str], Dict[str, Tensor]]]\",\"prepare_for_collate(id_list, batches)\"]},\"1648\":{\"h\":\"espnet2.iterators.multiple_iter_factory.MultipleIterFactory\",\"t\":[\"source\",\"class espnet2.iterators.multiple_iter_factory.MultipleIterFactory(build_funcs: Collection[Callable[[], AbsIterFactory]], seed: int = 0, shuffle: bool = False)\",\"Bases: AbsIterFactory\",\"build_iter(epoch: int, shuffle: bool | None = None) → Iterator\"]},\"1649\":{\"h\":\"espnet2.iterators.category_iter_factory.RawSampler\",\"t\":[\"source\",\"class espnet2.iterators.category_iter_factory.RawSampler(batches)\",\"Bases: AbsSampler\",\"generate(seed)\"]},\"1650\":{\"h\":\"espnet2.iterators.sequence_iter_factory.SequenceIterFactory\",\"t\":[\"source\",\"class espnet2.iterators.sequence_iter_factory.SequenceIterFactory(dataset, batches: AbsSampler | Sequence[Sequence[Any]], num_iters_per_epoch: int | None = None, seed: int = 0, shuffle: bool = False, shuffle_within_batch: bool = False, num_workers: int = 0, collate_fn=None, pin_memory: bool = False)\",\"Bases: AbsIterFactory\",\"Build iterator for each epoch.\",\"This class simply creates pytorch DataLoader except for the following points:\",\"The random seed is decided according to the number of epochs. This feature\",\"guarantees reproducibility when resuming from middle of training process.\",\"Enable to restrict the number of samples for one epoch. This features controls the interval number between training and evaluation.\",\"build_iter(epoch: int, shuffle: bool | None = None) → DataLoader\"]},\"1651\":{\"h\":\"espnet2.iterators.category_iter_factory.worker_init_fn\",\"t\":[\"source\",\"espnet2.iterators.category_iter_factory.worker_init_fn(worker_id, base_seed=0)\",\"Set random seed for each worker in DataLoader.\"]},\"1652\":{\"h\":\"espnet2.layers.abs_normalize.AbsNormalize\",\"t\":[\"source\",\"class espnet2.layers.abs_normalize.AbsNormalize(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1653\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1654\":{\"h\":\"espnet2.layers.sinc_conv.BarkScale\",\"t\":[\"source\",\"class espnet2.layers.sinc_conv.BarkScale\",\"Bases: object\",\"Bark frequency scale.\",\"Has wider bandwidths at lower frequencies, see: Critical bandwidth: BARK Zwicker and Terhardt, 1980\",\"classmethod bank(channels: int, fs: float) → Tensor\",\"Obtain initialization values for the Bark scale.\",\"Parameters:\",\"channels – Number of channels.\",\"fs – Sample rate.\",\"Returns: Filter start frequencíes. torch.Tensor: Filter stop frequencíes.\",\"Return type: torch.Tensor\",\"static convert(f)\",\"Convert Hz to Bark.\",\"static invert(x)\",\"Convert Bark to Hz.\"]},\"1655\":{\"h\":\"espnet2.layers.augmentation.DataAugmentation\",\"t\":[\"source\",\"class espnet2.layers.augmentation.DataAugmentation(effects: List[Tuple[float, List[Tuple[float, str, Dict]]] | Tuple[float, str, Dict]], apply_n: Tuple[int, int] = [1, 1])\",\"Bases: object\",\"A series of data augmentation effects that can be applied to a given waveform.\",\"Note: Currently we only support single-channel waveforms.\",\"Parameters:\",\"effects (list) –\",\"a list of effects to be applied to the waveform. .. rubric:: Example\",\"[ : [0.1, “lowpass”, {“cutoff_freq”: 1000, “Q”: 0.707}], [0.1, “highpass”, {“cutoff_freq”: 3000, “Q”: 0.707}], [0.1, “equalization”, {“center_freq”: 1000, “gain”: 0, “Q”: 0.707}], [\",\"0.1, [\",\"[0.3, “speed_perturb”, {“factor”: 0.9}], [0.3, “speed_perturb”, {“factor”: 1.1}], <br/> ] <br/> ],\",\"]\",\"Description: : - The above list defines a series of data augmentation effects that will be randomly sampled to apply to a given waveform.\",\"The data structure of each element can be either type1=Tuple[float, str, Dict] or type2=Tuple[float, type1].\",\"In type1, the three values are the weight of sampling this effect, the name (key) of the effect, and the keyword arguments for the effect.\",\"In type2, the first value is the weight of sampling this effect. The second value is a list of type1 elements which are similarly defined as above.\",\"Note that he effects defined in each type2 data are mutually exclusive (i.e., only one of them can be applied each time). This can be useful when you want to avoid applying some specific effects at the same time.\",\"apply_n (list) – range of the number of effects to be applied to the waveform.\"]},\"1656\":{\"h\":\"espnet2.layers.global_mvn.GlobalMVN\",\"t\":[\"source\",\"class espnet2.layers.global_mvn.GlobalMVN(stats_file: Path | str, norm_means: bool = True, norm_vars: bool = True, eps: float = 1e-20)\",\"Bases: AbsNormalize, InversibleInterface\",\"Apply global mean and variance normalization\",\"TODO(kamo): Make this class portable somehow\",\"Parameters:\",\"stats_file – npy file\",\"norm_means – Apply mean normalization\",\"norm_vars – Apply var normalization\",\"eps\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(x: Tensor, ilens: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Forward function\",\"Parameters:\",\"x – (B, L, …)\",\"ilens – (B,)\",\"inverse(x: Tensor, ilens: Tensor | None = None) → Tuple[Tensor, Tensor]\"]},\"1657\":{\"h\":\"espnet2.layers.houlsby_adapter_layer.Houlsby_Adapter\",\"t\":[\"source\",\"class espnet2.layers.houlsby_adapter_layer.Houlsby_Adapter(input_size: int, bottleneck: int)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1658\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1659\":{\"h\":\"espnet2.layers.inversible_interface.InversibleInterface\",\"t\":[\"source\",\"class espnet2.layers.inversible_interface.InversibleInterface\",\"Bases: ABC\",\"abstract inverse(input: Tensor, input_lengths: Tensor | None = None) → Tuple[Tensor, Tensor]\"]},\"1660\":{\"h\":\"espnet2.layers.label_aggregation.LabelAggregate\",\"t\":[\"source\",\"class espnet2.layers.label_aggregation.LabelAggregate(win_length: int = 512, hop_length: int = 128, center: bool = True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(input: Tensor, ilens: Tensor | None = None) → Tuple[Tensor, Tensor | None]\",\"LabelAggregate forward function.\",\"Parameters:\",\"input – (Batch, Nsamples, Label_dim)\",\"ilens – (Batch)\",\"Returns: (Batch, Frames, Label_dim)\",\"Return type: output\"]},\"1661\":{\"h\":\"espnet2.layers.sinc_conv.LogCompression\",\"t\":[\"source\",\"class espnet2.layers.sinc_conv.LogCompression\",\"Bases: Module\",\"Log Compression Activation.\",\"Activation function log(abs(x) + 1).\",\"Initialize.\",\"forward(x: Tensor) → Tensor\",\"Forward.\",\"Applies the Log Compression function elementwise on tensor x.\"]},\"1662\":{\"h\":\"espnet2.layers.log_mel.LogMel\",\"t\":[\"source\",\"class espnet2.layers.log_mel.LogMel(fs: int = 16000, n_fft: int = 512, n_mels: int = 80, fmin: float | None = None, fmax: float | None = None, htk: bool = False, log_base: float | None = None)\",\"Bases: Module\",\"Convert STFT to fbank feats\",\"The arguments is same as librosa.filters.mel\",\"Parameters:\",\"fs – number > 0 [scalar] sampling rate of the incoming signal\",\"n_fft – int > 0 [scalar] number of FFT components\",\"n_mels – int > 0 [scalar] number of Mel bands to generate\",\"fmin – float >= 0 [scalar] lowest frequency (in Hz)\",\"fmax – float >= 0 [scalar] highest frequency (in Hz). If None, use fmax = fs / 2.0\",\"htk – use HTK formula instead of Slaney\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(feat: Tensor, ilens: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1663\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1664\":{\"h\":\"espnet2.layers.mask_along_axis.MaskAlongAxis\",\"t\":[\"source\",\"class espnet2.layers.mask_along_axis.MaskAlongAxis(mask_width_range: int | Sequence[int] = (0, 30), num_mask: int = 2, dim: int | str = 'time', replace_with_zero: bool = True)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(spec: Tensor, spec_lengths: Tensor | None = None)\",\"Forward function.\",\"Parameters:spec – (Batch, Length, Freq)\"]},\"1665\":{\"h\":\"espnet2.layers.mask_along_axis.MaskAlongAxisVariableMaxWidth\",\"t\":[\"source\",\"class espnet2.layers.mask_along_axis.MaskAlongAxisVariableMaxWidth(mask_width_ratio_range: float | Sequence[float] = (0.0, 0.05), num_mask: int = 2, dim: int | str = 'time', replace_with_zero: bool = True)\",\"Bases: Module\",\"Mask input spec along a specified axis with variable maximum width.\",\"Formula: : max_width = max_width_ratio * seq_len\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(spec: Tensor, spec_lengths: Tensor | None = None)\",\"Forward function.\",\"Parameters:spec – (Batch, Length, Freq)\"]},\"1666\":{\"h\":\"espnet2.layers.sinc_conv.MelScale\",\"t\":[\"source\",\"class espnet2.layers.sinc_conv.MelScale\",\"Bases: object\",\"Mel frequency scale.\",\"classmethod bank(channels: int, fs: float) → Tensor\",\"Obtain initialization values for the mel scale.\",\"Parameters:\",\"channels – Number of channels.\",\"fs – Sample rate.\",\"Returns: Filter start frequencíes. torch.Tensor: Filter stop frequencies.\",\"Return type: torch.Tensor\",\"static convert(f)\",\"Convert Hz to mel.\",\"static invert(x)\",\"Convert mel to Hz.\"]},\"1667\":{\"h\":\"espnet2.layers.mixup_augmentation.MixupAugment\",\"t\":[\"source\",\"class espnet2.layers.mixup_augmentation.MixupAugment(mixup_probability: float)\",\"Bases: Module\",\"Mixup augmentation module for multi-label classification.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(speech: Tensor, onehot: Tensor, speech_lengths: Tensor)\",\"Applies mixup augmentation.\",\"Parameters:\",\"speech – (Batch, Length)\",\"onehot – (Batch, n_classes)\",\"speech_lengths – (Batch,)\",\"Returns: (Batch, Length) onehot: (Batch, n_classes) speech_lengths: (Batch,): Minimum of the two lengths mixed.\",\"Return type: speech\"]},\"1668\":{\"h\":\"espnet2.layers.sinc_conv.SincConv\",\"t\":[\"source\",\"class espnet2.layers.sinc_conv.SincConv(in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, window_func: str = 'hamming', scale_type: str = 'mel', fs: int | float = 16000)\",\"Bases: Module\",\"Sinc Convolution.\",\"This module performs a convolution using Sinc filters in time domain as kernel. Sinc filters function as band passes in spectral domain. The filtering is done as a convolution in time domain, and no transformation to spectral domain is necessary.\",\"This implementation of the Sinc convolution is heavily inspired by Ravanelli et al. https://github.com/mravanelli/SincNet, and adapted for the ESpnet toolkit. Combine Sinc convolutions with a log compression activation function, as in: https://arxiv.org/abs/2010.07597\",\"Notes: Currently, the same filters are applied to all input channels. The windowing function is applied on the kernel to obtained a smoother filter, and not on the input values, which is different to traditional ASR.\",\"Initialize Sinc convolutions.\",\"Parameters:\",\"in_channels – Number of input channels.\",\"out_channels – Number of output channels.\",\"kernel_size – Sinc filter kernel size (needs to be an odd number).\",\"stride – See torch.nn.functional.conv1d.\",\"padding – See torch.nn.functional.conv1d.\",\"dilation – See torch.nn.functional.conv1d.\",\"window_func – Window function on the filter, one of [“hamming”, “none”].\",\"fs (str,int,float) – Sample rate of the input data\",\"forward(xs: Tensor) → Tensor\",\"Sinc convolution forward function.\",\"Parameters:xs – Batch in form of torch.Tensor (B, C_in, D_in).\",\"Returns: Batch in form of torch.Tensor (B, C_out, D_out).\",\"Return type: xs\",\"get_odim(idim: int) → int\",\"Obtain the output dimension of the filter.\",\"static hamming_window(x: Tensor) → Tensor\",\"Hamming Windowing function.\",\"init_filters()\",\"Initialize filters with filterbank values.\",\"static none_window(x: Tensor) → Tensor\",\"Identity-like windowing function.\",\"static sinc(x: Tensor) → Tensor\",\"Sinc function.\"]},\"1669\":{\"h\":\"espnet2.layers.stft.Stft\",\"t\":[\"source\",\"class espnet2.layers.stft.Stft(n_fft: int = 512, win_length: int | None = None, hop_length: int = 128, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True)\",\"Bases: Module, InversibleInterface\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(input: Tensor, ilens: Tensor | None = None) → Tuple[Tensor, Tensor | None]\",\"STFT forward function.\",\"Parameters:\",\"input – (Batch, Nsamples) or (Batch, Nsample, Channels)\",\"ilens – (Batch)\",\"Returns: (Batch, Frames, Freq, 2) or (Batch, Frames, Channels, Freq, 2)\",\"Return type: output\",\"inverse(input: Tensor | ComplexTensor, ilens: Tensor | None = None) → Tuple[Tensor, Tensor | None]\",\"Inverse STFT.\",\"Parameters:\",\"input – Tensor(batch, T, F, 2) or ComplexTensor(batch, T, F)\",\"ilens – (batch,)\",\"Returns: (batch, samples) ilens: (batch,)\",\"Return type: wavs\"]},\"1670\":{\"h\":\"espnet2.layers.time_warp.TimeWarp\",\"t\":[\"source\",\"class espnet2.layers.time_warp.TimeWarp(window: int = 80, mode: str = 'bicubic')\",\"Bases: Module\",\"Time warping using torch.interpolate.\",\"Parameters:\",\"window – time warp parameter\",\"mode – Interpolate mode\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(x: Tensor, x_lengths: Tensor | None = None)\",\"Forward function.\",\"Parameters:\",\"x – (Batch, Time, Freq)\",\"x_lengths – (Batch,)\"]},\"1671\":{\"h\":\"espnet2.layers.utterance_mvn.UtteranceMVN\",\"t\":[\"source\",\"class espnet2.layers.utterance_mvn.UtteranceMVN(norm_means: bool = True, norm_vars: bool = False, eps: float = 1e-20)\",\"Bases: AbsNormalize\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(x: Tensor, ilens: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Forward function\",\"Parameters:\",\"x – (B, L, …)\",\"ilens – (B,)\"]},\"1672\":{\"h\":\"espnet2.layers.augmentation.bandpass_filtering\",\"t\":[\"source\",\"espnet2.layers.augmentation.bandpass_filtering(waveform, sample_rate: int, center_freq: int = 3000, Q: float = 0.707, const_skirt_gain: bool = False)\",\"Bandpass filter the input signal.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"center_freq_freq (int) – filter’s center_freq frequency\",\"Q (floatortorch.Tensor) – https://en.wikipedia.org/wiki/Q_factor\",\"const_skirt_gain (bool) – If True, uses a constant skirt gain (peak gain = Q). If False, uses a constant 0dB peak gain.\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1673\":{\"h\":\"espnet2.layers.augmentation.bandreject_filtering\",\"t\":[\"source\",\"espnet2.layers.augmentation.bandreject_filtering(waveform, sample_rate: int, center_freq: int = 3000, Q: float = 0.707)\",\"Two-pole band-reject filter the input signal.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"center_freq_freq (int) – filter’s center_freq frequency\",\"Q (floatortorch.Tensor) – https://en.wikipedia.org/wiki/Q_factor\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1674\":{\"h\":\"espnet2.layers.augmentation.bandwidth_limitation\",\"t\":[\"source\",\"espnet2.layers.augmentation.bandwidth_limitation(waveform, sample_rate: int, res_type='random')\",\"Apply the bandwidth limitation distortion to the input signal.\",\"Parameters:\",\"waveform (np.ndarray) – a single speech sample (…, Time)\",\"sample_rate (int) – input sampling rate in Hz\",\"fs_new (int) – effective sampling rate in Hz\",\"res_type (str) – resampling method\",\"Returns: bandwidth-limited speech sample (…, Time)\",\"Return type: ret (np.ndarray)\"]},\"1675\":{\"h\":\"espnet2.layers.create_adapter_utils.check_target_module_exists\",\"t\":[\"source\",\"espnet2.layers.create_adapter_utils.check_target_module_exists(key: str, target_modules: List[str])\",\"Check if the target_modules matchs the given key.\"]},\"1676\":{\"h\":\"espnet2.layers.augmentation.clipping\",\"t\":[\"source\",\"espnet2.layers.augmentation.clipping(waveform, sample_rate: int, min_quantile: float = 0.0, max_quantile: float = 0.9)\",\"Apply the clipping distortion to the input signal.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz (not used)\",\"min_quantile (float) – lower bound on the total percent of samples to be clipped\",\"max_quantile (float) – upper bound on the total percent of samples to be clipped\",\"Returns: clipped signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1677\":{\"h\":\"espnet2.layers.augmentation.codecs\",\"t\":[\"source\",\"espnet2.layers.augmentation.codecs(waveform, sample_rate: int, format: str, compression: float | None = None, encoding: str | None = None, bits_per_sample: int | None = None)\",\"Apply the specified codecs to the input signal.\",\"Warning: Wait until torchaudio 2.1 for this function to work.\"]},\"1678\":{\"h\":\"NOTE\",\"t\":[\"This function only supports CPU backend.\",\"The GSM codec can be used to emulate phone line channel effects.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"format (str) – file format. Valid values are “wav”, “mp3”, “ogg”, “vorbis”, “amr-nb”, “amb”, “flac”, “sph”, “gsm”, and “htk”.\",\"compression (floatorNone,optional) –\",\"used for formats other than WAV\",\"For more details see torchaudio.backend.sox_io_backend.save().\",\"encoding (strorNone,optional) – change the encoding for the supported formats Valid values are “PCM_S” (signed integer Linear PCM), “PCM_U” (unsigned integer Linear PCM), “PCM_F” (floating point PCM), “ULAW” (mu-law), and “ALAW” (a-law). For more details see torchaudio.backend.sox_io_backend.save().\",\"bits_per_sample (intorNone,optional) – change the bit depth for the supported formats For more details see torchaudio.backend.sox_io_backend.save().\",\"Returns: compressed signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1679\":{\"h\":\"espnet2.layers.augmentation.contrast\",\"t\":[\"source\",\"espnet2.layers.augmentation.contrast(waveform, sample_rate: int = 16000, enhancement_amount: float = 75.0)\",\"Apply contrast effect to the input signal to make it sound louder.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz (not used)\",\"enhancement_amount (float) – controls the amount of the enhancement Allowed range of values for enhancement_amount : 0-100 Note that enhancement_amount = 0 still gives a significant contrast enhancement.\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1680\":{\"h\":\"espnet2.layers.augmentation.corrupt_phase\",\"t\":[\"source\",\"espnet2.layers.augmentation.corrupt_phase(waveform, sample_rate, scale: float = 0.5, n_fft: float = 0.032, win_length: float | None = None, hop_length: float = 0.008, window: str | None = 'hann')\",\"Adding random noise to the phase of input waveform.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"scale (float) – scale factor for the phase noise\",\"n_fft (float) – length of FFT (in second)\",\"win_length (floatorNone) – The window length (in second) used for STFT If None, it is treated as equal to n_fft\",\"hop_length (float) – The hop size (in second) used for STFT\",\"window (strorNone) – The windowing function applied to the signal after padding with zeros\",\"Returns: phase-corrupted signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1681\":{\"h\":\"espnet2.layers.create_adapter.create_adapter\",\"t\":[\"source\",\"espnet2.layers.create_adapter.create_adapter(model: Module, adapter: str, adapter_conf: dict)\",\"Create adapter for the base model.\",\"Parameters:\",\"model (torch.nn.Module) – Base model to be adapted.\",\"adapter_type (str) – Name of adapter\",\"adapter_conf (dict) – Configuration for the adapter e.g. {“rank”: 8, “alpha”: 8, …} for lora\"]},\"1682\":{\"h\":\"espnet2.layers.create_adapter_fn.create_houlsby_adapter\",\"t\":[\"source\",\"espnet2.layers.create_adapter_fn.create_houlsby_adapter(model: Module, bottleneck: int = 32, target_layers: List[int] = [])\"]},\"1683\":{\"h\":\"espnet2.layers.create_adapter_fn.create_lora_adapter\",\"t\":[\"source\",\"espnet2.layers.create_adapter_fn.create_lora_adapter(model: Module, rank: int = 8, alpha: int = 8, dropout_rate: float = 0.0, target_modules: List[str] = ['query'], bias_type: str | None = 'none')\",\"Create LoRA adapter for the base model.\",\"See: https://arxiv.org/pdf/2106.09685.pdf\",\"Parameters:\",\"model (torch.nn.Module) – Base model to be adapted.\",\"rank (int) – Rank of LoRA matrices. Defaults to 8.\",\"alpha (int) – Constant number for LoRA scaling. Defaults to 8.\",\"dropout_rate (float) – Dropout probability for LoRA layers. Defaults to 0.0.\",\"target_modules (List *[*str]) – List of module(s) to apply LoRA adaptation. e.g. [“query”, “key”, “value”] for all layers, while [“encoder.encoders.blocks.0.attn.key”] for a specific layer.\",\"bias_type (str) – Bias training type for LoRA adaptaion, can be one of [“none”, “all”, “lora_only”]. “none” means not training any bias vectors; “all” means training all bias vectors, include LayerNorm biases; “lora_only” means only training bias vectors in LoRA adapted modules.\"]},\"1684\":{\"h\":\"espnet2.layers.create_adapter_fn.create_new_houlsby_module\",\"t\":[\"source\",\"espnet2.layers.create_adapter_fn.create_new_houlsby_module(target_module: Module, bottleneck: int)\",\"Create a new houlsby adapter module for the given target module.\",\"Currently, only support: Wav2Vec2EncoderLayerStableLayerNorm & TransformerSentenceEncoderLayer\"]},\"1685\":{\"h\":\"espnet2.layers.create_adapter_fn.create_new_lora_module\",\"t\":[\"source\",\"espnet2.layers.create_adapter_fn.create_new_lora_module(target_module: Module, rank: int, alpha: int, dropout_rate: float)\",\"Create a new lora module for the given target module.\"]},\"1686\":{\"h\":\"espnet2.layers.augmentation.deemphasis\",\"t\":[\"source\",\"espnet2.layers.augmentation.deemphasis(waveform, sample_rate: int, coeff: float = 0.97)\",\"De-emphasize a waveform along the time dimension.\",\"y[i] = x[i] + coeff * y[i - 1]\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz (not used)\",\"coeff (float) – de-emphasis coefficient. Typically between 0.0 and 1.0.\",\"Returns: de-emphasized signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1687\":{\"h\":\"espnet2.layers.augmentation.equalization_filtering\",\"t\":[\"source\",\"espnet2.layers.augmentation.equalization_filtering(waveform, sample_rate: int, center_freq: int = 1000, gain: float = 0.0, Q: float = 0.707)\",\"Equalization filter the input signal.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"center_freq (int) – filter’s center frequency\",\"gain (floatortorch.Tensor) – desired gain at the boost (or attenuation) in dB\",\"Q (floatortorch.Tensor) – https://en.wikipedia.org/wiki/Q_factor\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1688\":{\"h\":\"espnet2.layers.create_adapter_utils.get_submodules\",\"t\":[\"source\",\"espnet2.layers.create_adapter_utils.get_submodules(model: Module, key: str)\",\"Return the submodules of the given key.\"]},\"1689\":{\"h\":\"espnet2.layers.augmentation.highpass_filtering\",\"t\":[\"source\",\"espnet2.layers.augmentation.highpass_filtering(waveform, sample_rate: int, cutoff_freq: int = 3000, Q: float = 0.707)\",\"Highpass filter the input signal.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"cutoff_freq (int) – filter cutoff frequency\",\"Q (floatortorch.Tensor) – https://en.wikipedia.org/wiki/Q_factor\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1690\":{\"h\":\"espnet2.layers.augmentation.lowpass_filtering\",\"t\":[\"source\",\"espnet2.layers.augmentation.lowpass_filtering(waveform, sample_rate: int, cutoff_freq: int = 1000, Q: float = 0.707)\",\"Lowpass filter the input signal.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"cutoff_freq (int) – filter cutoff frequency\",\"Q (floatortorch.Tensor) – https://en.wikipedia.org/wiki/Q_factor\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1691\":{\"h\":\"espnet2.layers.mask_along_axis.mask_along_axis\",\"t\":[\"source\",\"espnet2.layers.mask_along_axis.mask_along_axis(spec: Tensor, spec_lengths: Tensor, mask_width_range: Sequence[int] = (0, 30), dim: int = 1, num_mask: int = 2, replace_with_zero: bool = True)\",\"Apply mask along the specified direction.\",\"Parameters:\",\"spec – (Batch, Length, Freq)\",\"spec_lengths – (Length): Not using lengths in this implementation\",\"mask_width_range – Select the width randomly between this range\"]},\"1692\":{\"h\":\"espnet2.layers.augmentation.pitch_shift\",\"t\":[\"source\",\"espnet2.layers.augmentation.pitch_shift(waveform, sample_rate: int, n_steps: int, bins_per_octave: int = 12, n_fft: float = 0.032, win_length: float | None = None, hop_length: float = 0.008, window: str | None = 'hann')\",\"Shift the pitch of a waveform by n_steps steps.\",\"Note: this function is slow.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"n_steps (int) – the (fractional) steps to shift the pitch -4 for shifting pitch down by 4/bins_per_octave octaves 4 for shifting pitch up by 4/bins_per_octave octaves\",\"bins_per_octave (int) – number of steps per octave\",\"n_fft (float) – length of FFT (in second)\",\"win_length (floatorNone) – The window length (in second) used for STFT If None, it is treated as equal to n_fft\",\"hop_length (float) – The hop size (in second) used for STFT\",\"window (strorNone) – The windowing function applied to the signal after padding with zeros\",\"Returns: filtered signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1693\":{\"h\":\"espnet2.layers.augmentation.polarity_inverse\",\"t\":[\"source\",\"espnet2.layers.augmentation.polarity_inverse(waveform, sample_rate)\"]},\"1694\":{\"h\":\"espnet2.layers.augmentation.preemphasis\",\"t\":[\"source\",\"espnet2.layers.augmentation.preemphasis(waveform, sample_rate: int, coeff: float = 0.97)\",\"Pre-emphasize a waveform along the time dimension.\",\"y[i] = x[i] - coeff * x[i - 1]\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz (not used)\",\"coeff (float) – pre-emphasis coefficient. Typically between 0.0 and 1.0.\",\"Returns: pre-emphasized signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1695\":{\"h\":\"espnet2.layers.create_adapter_utils.replace_module\",\"t\":[\"source\",\"espnet2.layers.create_adapter_utils.replace_module(parent_module: Module, child_name: str, old_module: Module, new_module: Module)\",\"Replace the target module with the new module.\"]},\"1696\":{\"h\":\"espnet2.layers.augmentation.reverse\",\"t\":[\"source\",\"espnet2.layers.augmentation.reverse(waveform, sample_rate)\"]},\"1697\":{\"h\":\"espnet2.layers.augmentation.speed_perturb\",\"t\":[\"source\",\"espnet2.layers.augmentation.speed_perturb(waveform, sample_rate: int, factor: float)\",\"Speed perturbation which also changes the pitch.\",\"Note: This function should be used with caution as it changes the signal duration.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"factor (float) – speed factor (e.g., 0.9 for 90% speed)\",\"lengths (torch.Tensor) – lengths of the input signals\",\"Returns: perturbed signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1698\":{\"h\":\"espnet2.layers.augmentation.time_stretch\",\"t\":[\"source\",\"espnet2.layers.augmentation.time_stretch(waveform, sample_rate: int, factor: float, n_fft: float = 0.032, win_length: float | None = None, hop_length: float = 0.008, window: str | None = 'hann')\",\"Time scaling (speed up in time without modifying pitch) via phase vocoder.\",\"Note: This function should be used with caution as it changes the signal duration.\",\"Parameters:\",\"waveform (torch.Tensor) – audio signal (…, time)\",\"sample_rate (int) – sampling rate in Hz\",\"factor (float) – speed-up factor (e.g., 0.9 for 90% speed and 1.3 for 130% speed)\",\"n_fft (float) – length of FFT (in second)\",\"win_length (floatorNone) – The window length (in second) used for STFT If None, it is treated as equal to n_fft\",\"hop_length (float) – The hop size (in second) used for STFT\",\"window (strorNone) – The windowing function applied to the signal after padding with zeros\",\"Returns: perturbed signal (…, time)\",\"Return type: ret (torch.Tensor)\"]},\"1699\":{\"h\":\"espnet2.layers.time_warp.time_warp\",\"t\":[\"source\",\"espnet2.layers.time_warp.time_warp(x: Tensor, window: int = 80, mode: str = 'bicubic')\",\"Time warping using torch.interpolate.\",\"Parameters:\",\"x – (Batch, Time, Freq)\",\"window – time warp parameter\",\"mode – Interpolate mode\"]},\"1700\":{\"h\":\"espnet2.layers.utterance_mvn.utterance_mvn\",\"t\":[\"source\",\"espnet2.layers.utterance_mvn.utterance_mvn(x: Tensor, ilens: Tensor | None = None, norm_means: bool = True, norm_vars: bool = False, eps: float = 1e-20) → Tuple[Tensor, Tensor]\",\"Apply utterance mean and variance normalization\",\"Parameters:\",\"x – (B, T, D), assumed zero padded\",\"ilens – (B,)\",\"norm_means\",\"norm_vars\",\"eps\"]},\"1701\":{\"h\":\"espnet2.layers.augmentation.weighted_sample_without_replacement\",\"t\":[\"source\",\"espnet2.layers.augmentation.weighted_sample_without_replacement(population, weights, k, rng=<module 'random' from '/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/random.py'>)\"]},\"1702\":{\"h\":\"espnet2.lid.espnet_model.ESPnetLIDModel\",\"t\":[\"source\",\"class espnet2.lid.espnet_model.ESPnetLIDModel(frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, encoder: AbsEncoder | None, pooling: AbsPooling | None, projector: AbsProjector | None, loss: AbsLoss | None, extract_feats_in_collect_stats: bool | None = None)\",\"Bases: AbsESPnetModel\",\"ESPnet LID model\",\"Support for language identification and language embedding extraction.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, lid_labels: Tensor | None = None, **kwargs) → Dict[str, Tensor]\",\"encode_frame(feats: Tensor) → Tensor\",\"extract_feats(speech: Tensor, speech_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"forward(speech: Tensor, speech_lengths: Tensor, lid_labels: Tensor | None = None, extract_embd: bool = False, **kwargs) → Tuple[Tensor, Tensor] | Tuple[Tensor, Dict[str, Tensor], Tensor] | Tensor\",\"Forward pass of the LID model.\",\"Processes raw speech through frontend, encoder, pooling, and loss modules.\",\"Parameters:\",\"speech – Input waveform tensor (batch_size, num_samples)\",\"speech_lengths – Lengths of each input in the batch (batch_size,)\",\"lid_labels – Ground truth language labels (batch_size,)\",\"extract_embd – If True, return language embeddings and predictions (inference mode)\",\"Returns: Tuple(lang_embd, pred_lids) \",\"If training:\",\"Tuple(loss, stats_dict, batch_weight)\",\"Return type:\",\"If extract_embd=True (inference mode)\",\"project_lang_embd(utt_level_feat: Tensor) → Tensor\"]},\"1703\":{\"h\":\"espnet2.legacy.transform.add_deltas.AddDeltas\",\"t\":[\"source\",\"class espnet2.legacy.transform.add_deltas.AddDeltas(window=2, order=2)\",\"Bases: object\",\"Add Deltas class.\",\"Initialize the class.\"]},\"1704\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttAdd\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttAdd(eprojs, dunits, att_dim, han_mode=False)\",\"Bases: Module\",\"Additive attention.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\",\"Initialize additive attention.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=2.0)\",\"Forward AttAdd.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – dummy (does not use)\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights (B x T_max)\",\"Return type: torch.Tensor\",\"reset()\",\"Reset states.\"]},\"1705\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttCov\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttCov(eprojs, dunits, att_dim, han_mode=False)\",\"Bases: Module\",\"Coverage mechanism attention.\",\"Reference: Get To The Point: Summarization with Pointer-Generator Network : (https://arxiv.org/abs/1704.04368)\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\",\"Initialize AttCov.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev_list, scaling=2.0)\",\"Calculate AttCov forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev_list (list) – list of previous attention weight\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: list of previous attention weights\",\"Return type: list\",\"reset()\",\"Reset states.\"]},\"1706\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttCovLoc\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttCovLoc(eprojs, dunits, att_dim, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"Coverage mechanism location aware attention.\",\"This attention is a combination of coverage and location-aware attentions.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\",\"Initialize AttCovLoc.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev_list, scaling=2.0)\",\"Calculate AttCovLoc forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev_list (list) – list of previous attention weight\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: list of previous attention weights\",\"Return type: list\",\"reset()\",\"Reset states.\"]},\"1707\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttDot\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttDot(eprojs, dunits, att_dim, han_mode=False)\",\"Bases: Module\",\"Dot product attention.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\",\"Initialize Attention Dot.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=2.0)\",\"Forward AttDot.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – dummy (does not use)\",\"att_prev (torch.Tensor) – dummy (does not use)\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weight (B x T_max)\",\"Return type: torch.Tensor\",\"reset()\",\"Reset states.\"]},\"1708\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttForward\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttForward(eprojs, dunits, att_dim, aconv_chans, aconv_filts)\",\"Bases: Module\",\"Forward attention module.\",\"Reference: Forward attention in sequence-to-sequence acoustic modeling for speech synthesis\",\"(https://arxiv.org/pdf/1807.06736.pdf)\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"Initialize AttForward.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=1.0, last_attended_idx=None, backward_window=1, forward_window=3)\",\"Calculate AttForward forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – attention weights of previous step\",\"scaling (float) – scaling parameter before applying softmax\",\"last_attended_idx (int) – index of the inputs of the last attended\",\"backward_window (int) – backward window size in attention constraint\",\"forward_window (int) – forward window size in attetion constraint\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights (B x T_max)\",\"Return type: torch.Tensor\",\"reset()\",\"Reset states.\"]},\"1709\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttForwardTA\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttForwardTA(eunits, dunits, att_dim, aconv_chans, aconv_filts, odim)\",\"Bases: Module\",\"Forward attention with transition agent module.\",\"Reference: Forward attention in sequence-to-sequence acoustic modeling for speech synthesis\",\"(https://arxiv.org/pdf/1807.06736.pdf)\",\"Parameters:\",\"eunits (int) – # units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"odim (int) – output dimension\",\"Initialize AttForwardTA.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, out_prev, scaling=1.0, last_attended_idx=None, backward_window=1, forward_window=3)\",\"Calculate AttForwardTA forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B, Tmax, eunits)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B, dunits)\",\"att_prev (torch.Tensor) – attention weights of previous step\",\"out_prev (torch.Tensor) – decoder outputs of previous step (B, odim)\",\"scaling (float) – scaling parameter before applying softmax\",\"last_attended_idx (int) – index of the inputs of the last attended\",\"backward_window (int) – backward window size in attention constraint\",\"forward_window (int) – forward window size in attetion constraint\",\"Returns: attention weighted encoder state (B, dunits)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights (B, Tmax)\",\"Return type: torch.Tensor\",\"reset()\",\"Reset states.\"]},\"1710\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttLoc\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttLoc(eprojs, dunits, att_dim, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"Location-aware attention module.\",\"Reference: Attention-Based Models for Speech Recognition : (https://arxiv.org/pdf/1506.07503.pdf)\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\",\"Initialize AttLoc.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=2.0, last_attended_idx=None, backward_window=1, forward_window=3)\",\"Calculate AttLoc forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – previous attention weight (B x T_max)\",\"scaling (float) – scaling parameter before applying softmax\",\"forward_window (int) – forward window size when constraining attention\",\"last_attended_idx (int) – index of the inputs of the last attended\",\"backward_window (int) – backward window size in attention constraint\",\"forward_window – forward window size in attetion constraint\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights (B x T_max)\",\"Return type: torch.Tensor\",\"reset()\",\"Reset states.\"]},\"1711\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttLoc2D\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttLoc2D(eprojs, dunits, att_dim, att_win, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"2D location-aware attention.\",\"This attention is an extended version of location aware attention. It take not only one frame before attention weights, but also earlier frames into account.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"att_win (int) – attention window size (default=5)\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\",\"Initialize AttLoc2D.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=2.0)\",\"Calculate AttLoc2D forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – previous attention weight (B x att_win x T_max)\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights (B x att_win x T_max)\",\"Return type: torch.Tensor\",\"reset()\",\"Reset states.\"]},\"1712\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttLocRec\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttLocRec(eprojs, dunits, att_dim, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"Location-aware recurrent attention.\",\"This attention is an extended version of location aware attention. With the use of RNN, it take the effect of the history of attention weights into account.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"att_dim (int) – attention dimension\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\",\"Initialize AttLocRec.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev_states, scaling=2.0)\",\"Calculate AttLocRec forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev_states (tuple) – previous attention weight and lstm states ((B, T_max), ((B, att_dim), (B, att_dim)))\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights and lstm states (w, (hx, cx)) ((B, T_max), ((B, att_dim), (B, att_dim)))\",\"Return type: tuple\",\"reset()\",\"Reset states.\"]},\"1713\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttMultiHeadAdd\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttMultiHeadAdd(eprojs, dunits, aheads, att_dim_k, att_dim_v, han_mode=False)\",\"Bases: Module\",\"Multi head additive attention.\",\"Reference: Attention is all you need : (https://arxiv.org/abs/1706.03762)\",\"This attention is multi head attention using additive attention for each head.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"aheads (int) – # heads of multi head attention\",\"att_dim_k (int) – dimension k in multi head attention\",\"att_dim_v (int) – dimension v in multi head attention\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v\",\"Initialize AttMultiHeadAdd.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, **kwargs)\",\"Calculate AttMultiHeadAdd forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – dummy (does not use)\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: list of previous attention weight (B x T_max) * aheads\",\"Return type: list\",\"reset()\",\"Reset states.\"]},\"1714\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttMultiHeadDot\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttMultiHeadDot(eprojs, dunits, aheads, att_dim_k, att_dim_v, han_mode=False)\",\"Bases: Module\",\"Multi head dot product attention.\",\"Reference: Attention is all you need : (https://arxiv.org/abs/1706.03762)\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"aheads (int) – # heads of multi head attention\",\"att_dim_k (int) – dimension k in multi head attention\",\"att_dim_v (int) – dimension v in multi head attention\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v\",\"Initialize AttMultiHeadDot.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, **kwargs)\",\"Calculate AttMultiHeadDot forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – dummy (does not use)\",\"Returns: attention weighted encoder state (B x D_enc)\",\"Return type: torch.Tensor\",\"Returns: list of previous attention weight (B x T_max) * aheads\",\"Return type: list\",\"reset()\",\"Reset states.\"]},\"1715\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttMultiHeadLoc\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttMultiHeadLoc(eprojs, dunits, aheads, att_dim_k, att_dim_v, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"Multi head location based attention.\",\"Reference: Attention is all you need : (https://arxiv.org/abs/1706.03762)\",\"This attention is multi head attention using location-aware attention for each head.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"aheads (int) – # heads of multi head attention\",\"att_dim_k (int) – dimension k in multi head attention\",\"att_dim_v (int) – dimension v in multi head attention\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v\",\"Initialize AttMultiHeadLoc.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=2.0, **kwargs)\",\"Calculate AttMultiHeadLoc forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – list of previous attention weight (B x T_max) * aheads\",\"scaling (float) – scaling parameter before applying softmax\",\"Returns: attention weighted encoder state (B x D_enc)\",\"Return type: torch.Tensor\",\"Returns: list of previous attention weight (B x T_max) * aheads\",\"Return type: list\",\"reset()\",\"Reset states.\"]},\"1716\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttMultiHeadMultiResLoc\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.AttMultiHeadMultiResLoc(eprojs, dunits, aheads, att_dim_k, att_dim_v, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"Multi head multi resolution location based attention.\",\"Reference: Attention is all you need : (https://arxiv.org/abs/1706.03762)\",\"This attention is multi head attention using location-aware attention for each head. Furthermore, it uses different filter size for each head.\",\"Parameters:\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"aheads (int) – # heads of multi head attention\",\"att_dim_k (int) – dimension k in multi head attention\",\"att_dim_v (int) – dimension v in multi head attention\",\"aconv_chans (int) – maximum # channels of attention convolution each head use #ch = aconv_chans * (head + 1) / aheads e.g. aheads=4, aconv_chans=100 => filter size = 25, 50, 75, 100\",\"aconv_filts (int) – filter size of attention convolution\",\"han_mode (bool) – flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v\",\"Initialize AttMultiHeadMultiResLoc.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev, **kwargs)\",\"Calculate AttMultiHeadMultiResLoc forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – list of previous attention weight (B x T_max) * aheads\",\"Returns: attention weighted encoder state (B x D_enc)\",\"Return type: torch.Tensor\",\"Returns: list of previous attention weight (B x T_max) * aheads\",\"Return type: list\",\"reset()\",\"Reset states.\"]},\"1717\":{\"h\":\"espnet2.legacy.transform.perturb.BandpassPerturbation\",\"t\":[\"source\",\"class espnet2.legacy.transform.perturb.BandpassPerturbation(lower=0.0, upper=0.75, seed=None, axes=(-1,))\",\"Bases: object\",\"BandpassPerturbation class.\",\"Randomly dropout along the frequency axis.\",\"The original idea comes from the following: : “randomly-selected frequency band was cut off under the constraint of : leaving at least 1,000 Hz band within the range of less than 4,000Hz.” <br/> (The Hitachi/JHU CHiME-5 system: Advances in speech recognition for : everyday home environments using multiple microphone arrays; http://spandh.dcs.shef.ac.uk/chime_workshop/papers/CHiME_2018_paper_kanda.pdf)\",\"Initialize class.\"]},\"1718\":{\"h\":\"espnet2.legacy.utils.cli_writers.BaseWriter\",\"t\":[\"source\",\"class espnet2.legacy.utils.cli_writers.BaseWriter\",\"Bases: object\",\"Declare Base Writer.\",\"close()\",\"Close self.\"]},\"1719\":{\"h\":\"espnet2.legacy.nets.batch_beam_search.BatchBeamSearch\",\"t\":[\"source\",\"class espnet2.legacy.nets.batch_beam_search.BatchBeamSearch(scorers: Dict[str, ScorerInterface], weights: Dict[str, float], beam_size: int, vocab_size: int, sos: int, eos: int, token_list: List[str] | None = None, pre_beam_ratio: float = 1.5, pre_beam_score_key: str | None = None, return_hs: bool = False, hyp_primer: List[int] | None = None, normalize_length: bool = False)\",\"Bases: BeamSearch\",\"Batch beam search implementation.\",\"Initialize beam search.\",\"Parameters:\",\"scorers (dict *[*str,ScorerInterface]) – Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None\",\"weights (dict *[*str,float]) – Dict of weights for each scorers The scorer will be ignored if its weight is 0\",\"beam_size (int) – The number of hypotheses kept during search\",\"vocab_size (int) – The number of vocabulary\",\"sos (int) – Start of sequence id\",\"eos (int) – End of sequence id\",\"token_list (list *[*str]) – List of tokens for debug log\",\"pre_beam_score_key (str) – key of scores to perform pre-beam search\",\"pre_beam_ratio (float) – beam size in the pre-beam search will be int(pre_beam_ratio * beam_size)\",\"return_hs (bool) – Whether to return hidden intermediates\",\"normalize_length (bool) – If true, select the best ended hypotheses based on length-normalized scores rather than the accumulated scores\",\"batch_beam(weighted_scores: Tensor, ids: Tensor) → Tuple[Tensor, Tensor, Tensor, Tensor]\",\"Batch-compute topk full token ids and partial token ids.\",\"Parameters:\",\"weighted_scores (torch.Tensor) – The weighted sum scores for each tokens. Its shape is (n_beam, self.vocab_size).\",\"ids (torch.Tensor) – The partial token ids to compute topk. Its shape is (n_beam, self.pre_beam_size).\",\"Returns: The topk full (prev_hyp, new_token) ids and partial (prev_hyp, new_token) ids. Their shapes are all (self.beam_size,)\",\"Return type: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\",\"batchfy(hyps: List[Hypothesis]) → BatchHypothesis\",\"Convert list to batch.\",\"init_hyp(x: Tensor) → BatchHypothesis\",\"Get an initial hypothesis data.\",\"Parameters:x (torch.Tensor) – The encoder output feature\",\"Returns: The initial hypothesis.\",\"Return type:Hypothesis\",\"merge_states(states: Any, part_states: Any, part_idx: int) → Any\",\"Merge states for new hypothesis.\",\"Parameters:\",\"states – states of self.full_scorers\",\"part_states – states of self.part_scorers\",\"part_idx (int) – The new token id for part_scores\",\"Returns: The new score dict. : Its keys are names of self.full_scorers and self.part_scorers. Its values are states of the scorers.\",\"Return type: Dict[str, torch.Tensor]\",\"post_process(i: int, maxlen: int, minlen: int, maxlenratio: float, running_hyps: BatchHypothesis, ended_hyps: List[Hypothesis]) → BatchHypothesis\",\"Perform post-processing of beam search iterations.\",\"Parameters:\",\"i (int) – The length of hypothesis tokens.\",\"maxlen (int) – The maximum length of tokens in beam search.\",\"maxlenratio (int) – The maximum length ratio in beam search.\",\"running_hyps (BatchHypothesis) – The running hypotheses in beam search.\",\"ended_hyps (List[Hypothesis]) – The ended hypotheses in beam search.\",\"Returns: The new running hypotheses.\",\"Return type:BatchHypothesis\",\"score_full(hyp: BatchHypothesis, x: Tensor, pre_x: Tensor | None = None) → Tuple[Dict[str, Tensor], Dict[str, Any]]\",\"Score new hypothesis by self.full_scorers.\",\"Parameters:\",\"hyp (Hypothesis) – Hypothesis with prefix tokens to score\",\"x (torch.Tensor) – Corresponding input feature\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attn (T, D) Sequential attn computes attn first on pre_x then on x, thereby attending to two sources in sequence.\",\"Returns: Tuple of : score dict of hyp that has string keys of self.full_scorers and tensor score values of shape: (self.n_vocab,), and state dict that has string keys and state values of self.full_scorers\",\"Return type: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]\",\"score_partial(hyp: BatchHypothesis, ids: Tensor, x: Tensor, pre_x: Tensor | None = None) → Tuple[Dict[str, Tensor], Dict[str, Any]]\",\"Score new hypothesis by self.full_scorers.\",\"Parameters:\",\"hyp (Hypothesis) – Hypothesis with prefix tokens to score\",\"ids (torch.Tensor) – 2D tensor of new partial tokens to score\",\"x (torch.Tensor) – Corresponding input feature\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attn (T, D) Sequential attn computes attn first on pre_x then on x, thereby attending to two sources in sequence.\",\"Returns: Tuple of : score dict of hyp that has string keys of self.full_scorers and tensor score values of shape: (self.n_vocab,), and state dict that has string keys and state values of self.full_scorers\",\"Return type: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]\",\"search(running_hyps: BatchHypothesis, x: Tensor, pre_x: Tensor | None = None) → BatchHypothesis\",\"Search new tokens for running hypotheses and encoded speech x.\",\"Parameters:\",\"running_hyps (BatchHypothesis) – Running hypotheses on beam\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attention (T, D)\",\"Returns: Best sorted hypotheses\",\"Return type:BatchHypothesis\",\"unbatchfy(batch_hyps: BatchHypothesis) → List[Hypothesis]\",\"Revert batch to list.\"]},\"1720\":{\"h\":\"espnet2.legacy.nets.batch_beam_search_online.BatchBeamSearchOnline\",\"t\":[\"source\",\"class espnet2.legacy.nets.batch_beam_search_online.BatchBeamSearchOnline(*args, block_size=40, hop_size=16, look_ahead=16, disable_repetition_detection=False, encoded_feat_length_limit=0, decoder_text_length_limit=0, incremental_decode=False, time_sync=False, ctc=None, hold_n=0, transducer_conf=None, joint_network=None, **kwargs)\",\"Bases: BatchBeamSearch\",\"Online beam search implementation.\",\"This simulates streaming decoding. It requires encoded features of entire utterance and extracts block by block from it as it shoud be done in streaming processing. This is based on Tsunoo et al, “STREAMING TRANSFORMER ASR WITH BLOCKWISE SYNCHRONOUS BEAM SEARCH” (https://arxiv.org/abs/2006.14941).\",\"Initialize beam search.\",\"assemble_hyps(ended_hyps)\",\"Assemble the hypotheses.\",\"extend(x: Tensor, hyps: Hypothesis) → List[Hypothesis]\",\"Extend probabilities and states with more encoded chunks.\",\"Parameters:\",\"x (torch.Tensor) – The extended encoder output feature\",\"hyps (Hypothesis) – Current list of hypothesis\",\"Returns: The extended hypothesis\",\"Return type:Hypothesis\",\"forward(x: Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0, is_final: bool = True) → List[Hypothesis]\",\"Perform beam search.\",\"Parameters:\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"maxlenratio (float) – Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\",\"minlenratio (float) – Input length ratio to obtain min output length.\",\"Returns: N-best decoding results\",\"Return type: list[Hypothesis]\",\"process_one_block(h, is_final, maxlen, minlen, maxlenratio)\",\"Recognize one block.\",\"process_one_block_time_sync(h, is_final, maxlen, maxlenratio)\",\"Recognize one block w/ time sync.\",\"reset()\",\"Reset parameters.\",\"score_full(hyp: BatchHypothesis, x: Tensor, pre_x: Tensor | None = None) → Tuple[Dict[str, Tensor], Dict[str, Any]]\",\"Score new hypothesis by self.full_scorers.\",\"Parameters:\",\"hyp (Hypothesis) – Hypothesis with prefix tokens to score\",\"x (torch.Tensor) – Corresponding input feature\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attention (T, D)\",\"Returns: Tuple of : score dict of hyp that has string keys of self.full_scorers and tensor score values of shape: (self.n_vocab,), and state dict that has string keys and state values of self.full_scorers\",\"Return type: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]\"]},\"1721\":{\"h\":\"espnet2.legacy.nets.batch_beam_search_online_sim.BatchBeamSearchOnlineSim\",\"t\":[\"source\",\"class espnet2.legacy.nets.batch_beam_search_online_sim.BatchBeamSearchOnlineSim(scorers: Dict[str, ScorerInterface], weights: Dict[str, float], beam_size: int, vocab_size: int, sos: int, eos: int, token_list: List[str] | None = None, pre_beam_ratio: float = 1.5, pre_beam_score_key: str | None = None, return_hs: bool = False, hyp_primer: List[int] | None = None, normalize_length: bool = False)\",\"Bases: BatchBeamSearch\",\"Online beam search implementation.\",\"This simulates streaming decoding. It requires encoded features of entire utterance and extracts block by block from it as it shoud be done in streaming processing. This is based on Tsunoo et al, “STREAMING TRANSFORMER ASR WITH BLOCKWISE SYNCHRONOUS BEAM SEARCH” (https://arxiv.org/abs/2006.14941).\",\"Initialize beam search.\",\"Parameters:\",\"scorers (dict *[*str,ScorerInterface]) – Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None\",\"weights (dict *[*str,float]) – Dict of weights for each scorers The scorer will be ignored if its weight is 0\",\"beam_size (int) – The number of hypotheses kept during search\",\"vocab_size (int) – The number of vocabulary\",\"sos (int) – Start of sequence id\",\"eos (int) – End of sequence id\",\"token_list (list *[*str]) – List of tokens for debug log\",\"pre_beam_score_key (str) – key of scores to perform pre-beam search\",\"pre_beam_ratio (float) – beam size in the pre-beam search will be int(pre_beam_ratio * beam_size)\",\"return_hs (bool) – Whether to return hidden intermediates\",\"normalize_length (bool) – If true, select the best ended hypotheses based on length-normalized scores rather than the accumulated scores\",\"extend(x: Tensor, hyps: Hypothesis) → List[Hypothesis]\",\"Extend probabilities and states with more encoded chunks.\",\"Parameters:\",\"x (torch.Tensor) – The extended encoder output feature\",\"hyps (Hypothesis) – Current list of hypothesis\",\"Returns: The extended hypothesis\",\"Return type:Hypothesis\",\"forward(x: Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0) → List[Hypothesis]\",\"Perform beam search.\",\"Parameters:\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"maxlenratio (float) – Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\",\"minlenratio (float) – Input length ratio to obtain min output length.\",\"Returns: N-best decoding results\",\"Return type: list[Hypothesis]\",\"set_block_size(block_size: int)\",\"Set block size for streaming decoding.\",\"Parameters:block_size (int) – The block size of encoder\",\"set_hop_size(hop_size: int)\",\"Set hop size for streaming decoding.\",\"Parameters:hop_size (int) – The hop size of encoder\",\"set_look_ahead(look_ahead: int)\",\"Set look ahead size for streaming decoding.\",\"Parameters:look_ahead (int) – The look ahead size of encoder\",\"set_streaming_config(asr_config: str)\",\"Set config file for streaming decoding.\",\"Parameters:asr_config (str) – The config file for asr training\"]},\"1722\":{\"h\":\"espnet2.legacy.nets.batch_beam_search.BatchHypothesis\",\"t\":[\"source\",\"class espnet2.legacy.nets.batch_beam_search.BatchHypothesis(yseq: Tensor = tensor([]), score: Tensor = tensor([]), length: Tensor = tensor([]), scores: Dict[str, Tensor] = {}, states: Dict[str, Dict] = {}, hs: List[Tensor] = [])\",\"Bases: NamedTuple\",\"Batchfied/Vectorized hypothesis data type.\",\"Create new instance of BatchHypothesis(yseq, score, length, scores, states, hs)\",\"hs : List[Tensor]\",\"Alias for field number 5\",\"length : Tensor\",\"Alias for field number 2\",\"score : Tensor\",\"Alias for field number 1\",\"scores : Dict[str, Tensor]\",\"Alias for field number 3\",\"states : Dict[str, Dict]\",\"Alias for field number 4\",\"yseq : Tensor\",\"Alias for field number 0\"]},\"1723\":{\"h\":\"espnet2.legacy.nets.scorer_interface.BatchPartialScorerInterface\",\"t\":[\"source\",\"class espnet2.legacy.nets.scorer_interface.BatchPartialScorerInterface\",\"Bases: BatchScorerInterface, PartialScorerInterface\",\"Batch partial scorer interface for beam search.\",\"batch_score_partial(ys: Tensor, next_tokens: Tensor, states: List[Any], xs: Tensor) → Tuple[Tensor, Any]\",\"Score new token (required).\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"next_tokens (torch.Tensor) – torch.int64 tokens to score (n_batch, n_token).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of a score tensor for ys that has a shape (n_batch, n_vocab) and next states for ys\",\"Return type: tuple[torch.Tensor, Any]\"]},\"1724\":{\"h\":\"espnet2.legacy.nets.scorer_interface.BatchScorerInterface\",\"t\":[\"source\",\"class espnet2.legacy.nets.scorer_interface.BatchScorerInterface\",\"Bases: ScorerInterface\",\"Batch scorer interface.\",\"batch_init_state(x: Tensor) → Any\",\"Get an initial state for decoding (optional).\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor) → Tuple[Tensor, List[Any]]\",\"Score new token batch (required).\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\"]},\"1725\":{\"h\":\"espnet2.legacy.nets.beam_search.BeamSearch\",\"t\":[\"source\",\"class espnet2.legacy.nets.beam_search.BeamSearch(scorers: Dict[str, ScorerInterface], weights: Dict[str, float], beam_size: int, vocab_size: int, sos: int, eos: int, token_list: List[str] | None = None, pre_beam_ratio: float = 1.5, pre_beam_score_key: str | None = None, return_hs: bool = False, hyp_primer: List[int] | None = None, normalize_length: bool = False)\",\"Bases: Module\",\"Beam search implementation.\",\"Initialize beam search.\",\"Parameters:\",\"scorers (dict *[*str,ScorerInterface]) – Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None\",\"weights (dict *[*str,float]) – Dict of weights for each scorers The scorer will be ignored if its weight is 0\",\"beam_size (int) – The number of hypotheses kept during search\",\"vocab_size (int) – The number of vocabulary\",\"sos (int) – Start of sequence id\",\"eos (int) – End of sequence id\",\"token_list (list *[*str]) – List of tokens for debug log\",\"pre_beam_score_key (str) – key of scores to perform pre-beam search\",\"pre_beam_ratio (float) – beam size in the pre-beam search will be int(pre_beam_ratio * beam_size)\",\"return_hs (bool) – Whether to return hidden intermediates\",\"normalize_length (bool) – If true, select the best ended hypotheses based on length-normalized scores rather than the accumulated scores\",\"static append_token(xs: Tensor, x: int) → Tensor\",\"Append new token to prefix tokens.\",\"Parameters:\",\"xs (torch.Tensor) – The prefix token\",\"x (int) – The new token to append\",\"Returns: New tensor contains: xs + [x] with xs.dtype and xs.device\",\"Return type: torch.Tensor\",\"beam(weighted_scores: Tensor, ids: Tensor) → Tuple[Tensor, Tensor]\",\"Compute topk full token ids and partial token ids.\",\"Parameters:\",\"weighted_scores (torch.Tensor) – The weighted sum scores for each tokens.\",\"` (Its shape is)\",\"ids (torch.Tensor) – The partial token ids to compute topk\",\"Returns: The topk full token ids and partial token ids. Their shapes are (self.beam_size,)\",\"Return type: Tuple[torch.Tensor, torch.Tensor]\",\"forward(x: Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0, pre_x: Tensor | None = None) → List[Hypothesis]\",\"Perform beam search.\",\"Parameters:\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"maxlenratio (float) – Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths If maxlenratio<0.0, its absolute value is interpreted as a constant max output length.\",\"minlenratio (float) – Input length ratio to obtain min output length. If minlenratio<0.0, its absolute value is interpreted as a constant min output length.\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attn (T, D) Sequential attn computes attn first on pre_x then on x, thereby attending to two sources in sequence.\",\"Returns: N-best decoding results\",\"Return type: list[Hypothesis]\",\"init_hyp(x: Tensor) → List[Hypothesis]\",\"Get an initial hypothesis data.\",\"Parameters:x (torch.Tensor) – The encoder output feature\",\"Returns: The initial hypothesis.\",\"Return type:Hypothesis\",\"static merge_scores(prev_scores: Dict[str, float], next_full_scores: Dict[str, Tensor], full_idx: int, next_part_scores: Dict[str, Tensor], part_idx: int) → Dict[str, Tensor]\",\"Merge scores for new hypothesis.\",\"Parameters:\",\"prev_scores (Dict *[*str,float]) – The previous hypothesis scores by self.scorers\",\"next_full_scores (Dict *[*str,torch.Tensor]) – scores by self.full_scorers\",\"full_idx (int) – The next token id for next_full_scores\",\"next_part_scores (Dict *[*str,torch.Tensor]) – scores of partial tokens by self.part_scorers\",\"part_idx (int) – The new token id for next_part_scores\",\"Returns: The new score dict. : Its keys are names of self.full_scorers and self.part_scorers. Its values are scalar tensors by the scorers.\",\"Return type: Dict[str, torch.Tensor]\",\"merge_states(states: Any, part_states: Any, part_idx: int) → Any\",\"Merge states for new hypothesis.\",\"Parameters:\",\"states – states of self.full_scorers\",\"part_states – states of self.part_scorers\",\"part_idx (int) – The new token id for part_scores\",\"Returns: The new score dict. : Its keys are names of self.full_scorers and self.part_scorers. Its values are states of the scorers.\",\"Return type: Dict[str, torch.Tensor]\",\"post_process(i: int, maxlen: int, minlen: int, maxlenratio: float, running_hyps: List[Hypothesis], ended_hyps: List[Hypothesis]) → List[Hypothesis]\",\"Perform post-processing of beam search iterations.\",\"Parameters:\",\"i (int) – The length of hypothesis tokens.\",\"maxlen (int) – The maximum length of tokens in beam search.\",\"maxlenratio (int) – The maximum length ratio in beam search.\",\"running_hyps (List[Hypothesis]) – The running hypotheses in beam search.\",\"ended_hyps (List[Hypothesis]) – The ended hypotheses in beam search.\",\"Returns: The new running hypotheses.\",\"Return type: List[Hypothesis]\",\"score_full(hyp: Hypothesis, x: Tensor, pre_x: Tensor | None = None) → Tuple[Dict[str, Tensor], Dict[str, Any]]\",\"Score new hypothesis by self.full_scorers.\",\"Parameters:\",\"hyp (Hypothesis) – Hypothesis with prefix tokens to score\",\"x (torch.Tensor) – Corresponding input feature\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attn (T, D) Sequential attn computes attn first on pre_x then on x, thereby attending to two sources in sequence.\",\"Returns: Tuple of : score dict of hyp that has string keys of self.full_scorers and tensor score values of shape: (self.n_vocab,), and state dict that has string keys and state values of self.full_scorers\",\"Return type: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]\",\"score_partial(hyp: Hypothesis, ids: Tensor, x: Tensor) → Tuple[Dict[str, Tensor], Dict[str, Any]]\",\"Score new hypothesis by self.part_scorers.\",\"Parameters:\",\"hyp (Hypothesis) – Hypothesis with prefix tokens to score\",\"ids (torch.Tensor) – 1D tensor of new partial tokens to score\",\"x (torch.Tensor) – Corresponding input feature\",\"Returns: Tuple of : score dict of hyp that has string keys of self.part_scorers and tensor score values of shape: (len(ids),), and state dict that has string keys and state values of self.part_scorers\",\"Return type: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]\",\"search(running_hyps: List[Hypothesis], x: Tensor, pre_x: Tensor | None = None) → List[Hypothesis]\",\"Search new tokens for running hypotheses and encoded speech x.\",\"Parameters:\",\"running_hyps (List[Hypothesis]) – Running hypotheses on beam\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"pre_x (torch.Tensor) – Encoded speech feature for sequential attn (T, D) Sequential attn computes attn first on pre_x then on x, thereby attending to two sources in sequence.\",\"Returns: Best sorted hypotheses\",\"Return type: List[Hypotheses]\",\"set_hyp_primer(hyp_primer: List[int] | None = None) → None\",\"Set the primer sequence for decoding.\",\"Used for OpenAI Whisper models.\"]},\"1726\":{\"h\":\"espnet2.legacy.nets.beam_search_timesync.BeamSearchTimeSync\",\"t\":[\"source\",\"class espnet2.legacy.nets.beam_search_timesync.BeamSearchTimeSync(sos: int, beam_size: int, scorers: ~typing.Dict[str, ~espnet2.legacy.nets.scorer_interface.ScorerInterface], weights: ~typing.Dict[str, float], token_list=<class 'dict'>, pre_beam_ratio: float = 1.5, blank: int = 0, force_lid: bool = False, temp: float = 1.0)\",\"Bases: Module\",\"Time synchronous beam search algorithm.\",\"Initialize beam search.\",\"Parameters:\",\"beam_size – num hyps\",\"sos – sos index\",\"ctc – CTC module\",\"pre_beam_ratio – pre_beam_ratio * beam_size = pre_beam pre_beam is used to select candidates from vocab to extend hypotheses\",\"decoder – decoder ScorerInterface\",\"ctc_weight – ctc_weight\",\"blank – blank index\",\"cached_score(h: Tuple[int], cache: dict, scorer: ScorerInterface) → Any\",\"Retrieve decoder/LM scores which may be cached.\",\"forward(x: Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0) → List[Hypothesis]\",\"Perform beam search.\",\"Parameters:enc_output (torch.Tensor)\",\"Returns: list[Hypothesis]\",\"joint_score(hyps: Any, ctc_score_dp: Any) → Any\",\"Calculate joint score for hyps.\",\"reset(enc_output: Tensor)\",\"Reset object for a new utterance.\",\"time_step(p_ctc: Any, ctc_score_dp: Any, hyps: Any) → Any\",\"Execute a single time step.\"]},\"1727\":{\"h\":\"espnet2.legacy.nets.beam_search_timesync_streaming.BeamSearchTimeSyncStreaming\",\"t\":[\"source\",\"class espnet2.legacy.nets.beam_search_timesync_streaming.BeamSearchTimeSyncStreaming(sos: int, beam_size: int, scorers: ~typing.Dict[str, ~espnet2.legacy.nets.scorer_interface.ScorerInterface], weights: ~typing.Dict[str, float], token_list=<class 'dict'>, pre_beam_ratio: float = 1.5, blank: int = 0, hold_n: int = 0)\",\"Bases: Module\",\"Time synchronous beam search algorithm.\",\"Initialize beam search.\",\"Parameters:\",\"beam_size – num hyps\",\"sos – sos index\",\"ctc – CTC module\",\"pre_beam_ratio – pre_beam_ratio * beam_size = pre_beam pre_beam is used to select candidates from vocab to extend hypotheses\",\"decoder – decoder ScorerInterface\",\"ctc_weight – ctc_weight\",\"blank – blank index\",\"cached_score(h: Tuple[int], cache: dict, scorer: ScorerInterface, recompute_cache: bool = False) → Any\",\"Retrieve decoder/LM scores which may be cached.\",\"forward(x: Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0, start_idx: int = 0, is_final: bool = False, incremental_decode: bool = False) → List[Hypothesis]\",\"Perform beam search.\",\"Parameters:enc_output (torch.Tensor)\",\"Returns: list[Hypothesis]\",\"joint_score(hyps: Any, ctc_score_dp: Any, recompute_cache: bool = False) → Any\",\"Calculate joint score for hyps.\",\"reset(enc_output: Tensor)\",\"Reset object for a new utterance.\",\"time_step(p_ctc: Any, ctc_score_dp: Any, hyps: Any, recompute_cache: bool = False) → Any\",\"Execute a single time step.\"]},\"1728\":{\"h\":\"espnet2.legacy.transform.cmvn.CMVN\",\"t\":[\"source\",\"class espnet2.legacy.transform.cmvn.CMVN(stats, norm_means=True, norm_vars=False, filetype='mat', utt2spk=None, spk2utt=None, reverse=False, std_floor=1e-20)\",\"Bases: object\",\"CMVN class.\",\"Initialize class.\"]},\"1729\":{\"h\":\"espnet2.legacy.nets.ctc_prefix_score.CTCPrefixScore\",\"t\":[\"source\",\"class espnet2.legacy.nets.ctc_prefix_score.CTCPrefixScore(x, blank, eos, xp)\",\"Bases: object\",\"Compute CTC label sequence scores.\",\"which is based on Algorithm 2 in WATANABE et al. “HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,” but extended to efficiently compute the probablities of multiple labels simultaneously\",\"Initialize CTCPrefixScore.\",\"initial_state()\",\"Obtain an initial CTC state.\",\"Returns: CTC state\"]},\"1730\":{\"h\":\"espnet2.legacy.nets.ctc_prefix_score.CTCPrefixScoreTH\",\"t\":[\"source\",\"class espnet2.legacy.nets.ctc_prefix_score.CTCPrefixScoreTH(x, xlens, blank, eos, margin=0)\",\"Bases: object\",\"Batch processing of CTCPrefixScore.\",\"which is based on Algorithm 2 in WATANABE et al. “HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,” but extended to efficiently compute the label probablities for multiple hypotheses simultaneously See also Seki et al. “Vectorized Beam Search for CTC-Attention-Based Speech Recognition,” In INTERSPEECH (pp. 3825-3829), 2019.\",\"Construct CTC prefix scorer.\",\"Parameters:\",\"x (torch.Tensor) – input label posterior sequences (B, T, O)\",\"xlens (torch.Tensor) – input lengths (B,)\",\"blank (int) – blank label id\",\"eos (int) – end-of-sequence id\",\"margin (int) – margin parameter for windowing (0 means no windowing)\",\"extend_prob(x)\",\"Extend CTC prob.\",\"Parameters:x (torch.Tensor) – input label posterior sequences (B, T, O)\",\"extend_state(state)\",\"Compute CTC prefix state.\",\":param state : CTC state :return ctc_state\",\"index_select_state(state, best_ids)\",\"Select CTC states according to best ids.\",\":param state : CTC state :param best_ids : index numbers selected by beam pruning (B, W) :return selected_state\"]},\"1731\":{\"h\":\"espnet2.legacy.nets.scorers.ctc.CTCPrefixScorer\",\"t\":[\"source\",\"class espnet2.legacy.nets.scorers.ctc.CTCPrefixScorer(ctc: Module, eos: int)\",\"Bases: BatchPartialScorerInterface\",\"Decoder interface wrapper for CTCPrefixScore.\",\"Initialize class.\",\"Parameters:\",\"ctc (torch.nn.Module) – The CTC implementation. For example, espnet2.legacy.nets.pytorch_backend.ctc.CTC\",\"eos (int) – The end-of-sequence id.\",\"batch_init_state(x: Tensor)\",\"Get an initial state for decoding.\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"batch_score_partial(y, ids, state, x)\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D prefix token\",\"ids (torch.Tensor) – torch.int64 next token to score\",\"state – decoder state for prefix tokens\",\"x (torch.Tensor) – 2D encoder feature that generates ys\",\"Returns: Tuple of a score tensor for y that has a shape (len(next_tokens),) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"extend_prob(x: Tensor)\",\"Extend probs for decoding.\",\"This extension is for streaming decoding as in Eq (14) in https://arxiv.org/abs/2006.14941\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"extend_state(state)\",\"Extend state for decoding.\",\"This extension is for streaming decoding as in Eq (14) in https://arxiv.org/abs/2006.14941\",\"Parameters:state – The states of hyps\",\"Returns: exteded state\",\"init_state(x: Tensor)\",\"Get an initial state for decoding.\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"score_partial(y, ids, state, x)\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D prefix token\",\"next_tokens (torch.Tensor) – torch.int64 next token to score\",\"state – decoder state for prefix tokens\",\"x (torch.Tensor) – 2D encoder feature that generates ys\",\"Returns: Tuple of a score tensor for y that has a shape (len(next_tokens),) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"select_state(state, i, new_id=None)\",\"Select state with relative ids in the main beam search.\",\"Parameters:\",\"state – Decoder state for prefix tokens\",\"i (int) – Index to select a state in the main beam search\",\"new_id (int) – New label id to select a state if necessary\",\"Returns: pruned state\",\"Return type: state\"]},\"1732\":{\"h\":\"espnet2.legacy.nets.beam_search_timesync_streaming.CacheItem\",\"t\":[\"source\",\"class espnet2.legacy.nets.beam_search_timesync_streaming.CacheItem(state: Any, scores: Any, log_sum: float)\",\"Bases: object\",\"For caching attentional decoder and LM states.\",\"log_sum : float\",\"scores : Any\",\"state : Any\"]},\"1733\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.wavenet.CausalConv1d\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.wavenet.CausalConv1d(in_channels, out_channels, kernel_size, dilation=1, bias=True)\",\"Bases: Module\",\"1D dilated causal convolution.\",\"Initialize CausalConv1d.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input tensor with the shape (B, in_channels, T).\",\"Returns: Tensor with the shape (B, out_channels, T)\",\"Return type: Tensor\"]},\"1734\":{\"h\":\"espnet2.legacy.transform.channel_selector.ChannelSelector\",\"t\":[\"source\",\"class espnet2.legacy.transform.channel_selector.ChannelSelector(train_channel='random', eval_channel=0, axis=1)\",\"Bases: object\",\"Select 1ch from multi-channel signal.\",\"Initialize the class.\"]},\"1735\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.contextual_block_encoder_layer.ContextualBlockEncoderLayer\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.contextual_block_encoder_layer.ContextualBlockEncoderLayer(size, self_attn, feed_forward, dropout_rate, total_layer_num, normalize_before=True, concat_after=False)\",\"Bases: Module\",\"Contexutal Block Encoder layer module.\",\"Parameters:\",\"size (int) – Input dimension.\",\"self_attn (torch.nn.Module) – Self-attention module instance. MultiHeadedAttention or RelPositionMultiHeadedAttention instance can be used as the argument.\",\"feed_forward (torch.nn.Module) – Feed-forward module instance. PositionwiseFeedForward, MultiLayeredConv1d, or Conv1dLinear instance can be used as the argument.\",\"dropout_rate (float) – Dropout rate.\",\"total_layer_num (int) – Total number of layers\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"Construct an EncoderLayer object.\",\"forward(x, mask, infer_mode=False, past_ctx=None, next_ctx=None, is_short_segment=False, layer_idx=0, cache=None)\",\"Calculate forward propagation.\",\"forward_infer(x, mask, past_ctx=None, next_ctx=None, is_short_segment=False, layer_idx=0, cache=None)\",\"Compute encoded features.\",\"Parameters:\",\"x_input (torch.Tensor) – Input tensor (#batch, time, size).\",\"mask (torch.Tensor) – Mask tensor for the input (#batch, 1, time).\",\"past_ctx (torch.Tensor) – Previous contexutal vector\",\"next_ctx (torch.Tensor) – Next contexutal vector\",\"cache (torch.Tensor) – Cache tensor of the input (#batch, time - 1, size).\",\"Returns: Output tensor (#batch, time, size). torch.Tensor: Mask tensor (#batch, 1, time). cur_ctx (torch.Tensor): Current contexutal vector next_ctx (torch.Tensor): Next contexutal vector layer_idx (int): layer index number\",\"Return type: torch.Tensor\",\"forward_train(x, mask, past_ctx=None, next_ctx=None, layer_idx=0, cache=None)\",\"Compute encoded features.\",\"Parameters:\",\"x_input (torch.Tensor) – Input tensor (#batch, time, size).\",\"mask (torch.Tensor) – Mask tensor for the input (#batch, 1, time).\",\"past_ctx (torch.Tensor) – Previous contexutal vector\",\"next_ctx (torch.Tensor) – Next contexutal vector\",\"cache (torch.Tensor) – Cache tensor of the input (#batch, time - 1, size).\",\"Returns: Output tensor (#batch, time, size). torch.Tensor: Mask tensor (#batch, 1, time). cur_ctx (torch.Tensor): Current contexutal vector next_ctx (torch.Tensor): Next contexutal vector layer_idx (int): layer index number\",\"Return type: torch.Tensor\"]},\"1736\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.conv1d_nets.Conv1d\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transducer.conv1d_nets.Conv1d(idim: int, odim: int, kernel_size: int | Tuple, stride: int | Tuple = 1, dilation: int | Tuple = 1, groups: int | Tuple = 1, bias: bool = True, batch_norm: bool = False, relu: bool = True, dropout_rate: float = 0.0)\",\"Bases: Module\",\"1D convolution module for custom encoder.\",\"Parameters:\",\"idim – Input dimension.\",\"odim – Output dimension.\",\"kernel_size – Size of the convolving kernel.\",\"stride – Stride of the convolution.\",\"dilation – Spacing between the kernel points.\",\"groups – Number of blocked connections from input channels to output channels.\",\"bias – Whether to add a learnable bias to the output.\",\"batch_norm – Whether to use batch normalization after convolution.\",\"relu – Whether to use a ReLU activation after convolution.\",\"dropout_rate – Dropout rate.\",\"Construct a Conv1d module object.\",\"create_new_mask(mask: Tensor) → Tensor\",\"Create new mask.\",\"Parameters:mask – Mask of input sequences. (B, 1, T)\",\"Returns: Mask of output sequences. (B, 1, sub(T))\",\"Return type: mask\",\"create_new_pos_embed(pos_embed: Tensor) → Tensor\",\"Create new positional embedding vector.\",\"Parameters:pos_embed – Input sequences positional embedding. (B, 2 * (T - 1), D_att)\",\"Returns: Output sequences positional embedding. : (B, 2 * (sub(T) - 1), D_att)\",\"Return type: pos_embed\",\"forward(sequence: Tensor | Tuple[Tensor, Tensor], mask: Tensor) → Tuple[Tensor | Tuple[Tensor, Tensor], Tensor]\",\"Forward ConvEncoderLayer module object.\",\"Parameters:\",\"sequence –\",\"Input sequences. (B, T, D_in)\",\"or (B, T, D_in), (B, 2 * (T - 1), D_att)\",\"mask – Mask of input sequences. (B, 1, T)\",\"Returns: Output sequences. : (B, sub(T), D_out) : or (B, sub(T), D_out), (B, 2 * (sub(T) - 1), D_att)\",\"mask: Mask of output sequences. (B, 1, sub(T))\",\"Return type: sequence\"]},\"1737\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.multi_layer_conv.Conv1dLinear\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.multi_layer_conv.Conv1dLinear(in_chans, hidden_chans, kernel_size, dropout_rate)\",\"Bases: Module\",\"Conv1D + Linear for Transformer block.\",\"A variant of MultiLayeredConv1d, which replaces second conv-layer to linear.\",\"Initialize Conv1dLinear module.\",\"Parameters:\",\"in_chans (int) – Number of input channels.\",\"hidden_chans (int) – Number of hidden channels.\",\"kernel_size (int) – Kernel size of conv1d.\",\"dropout_rate (float) – Dropout rate.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (torch.Tensor) – Batch of input tensors (B, T, in_chans).\",\"Returns: Batch of output tensors (B, T, hidden_chans).\",\"Return type: torch.Tensor\"]},\"1738\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv1dSubsampling1\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv1dSubsampling1(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 1D subsampling.\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv1dSubsampling1 object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 2.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 2.\",\"Return type: torch.Tensor\"]},\"1739\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv1dSubsampling2\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv1dSubsampling2(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 1D subsampling (to 1/2 length).\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv1dSubsampling2 object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 2.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 2.\",\"Return type: torch.Tensor\"]},\"1740\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv1dSubsampling3\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv1dSubsampling3(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 1D subsampling (to 1/3 length).\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv1dSubsampling3 object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 2.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 2.\",\"Return type: torch.Tensor\"]},\"1741\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 2D subsampling (to 1/4 length).\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv2dSubsampling object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 4.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 4.\",\"Return type: torch.Tensor\"]},\"1742\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling1\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling1(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Similar to Conv2dSubsampling module, but without any subsampling performed.\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv2dSubsampling1 object.\",\"forward(x, x_mask)\",\"Pass x through 2 Conv2d layers without subsampling.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim). : where time’ = time - 4.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’). : where time’ = time - 4.\",\"Return type: torch.Tensor\"]},\"1743\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling2\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling2(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 2D subsampling (to 1/2 length).\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv2dSubsampling2 object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 2.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 2.\",\"Return type: torch.Tensor\"]},\"1744\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling6\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling6(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 2D subsampling (to 1/6 length).\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv2dSubsampling6 object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 6.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 6.\",\"Return type: torch.Tensor\"]},\"1745\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling8\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling8(idim, odim, dropout_rate, pos_enc=None)\",\"Bases: Module\",\"Convolutional 2D subsampling (to 1/8 length).\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"pos_enc (torch.nn.Module) – Custom position encoding layer.\",\"Construct an Conv2dSubsampling8 object.\",\"forward(x, x_mask, prefix_embeds=None)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"prefix_embeds (torch.TensororNone) – Prefix token embeddings (#batch, prefix_len, odim).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 8.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 8.\",\"Return type: torch.Tensor\"]},\"1746\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.subsampling_without_posenc.Conv2dSubsamplingWOPosEnc\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.subsampling_without_posenc.Conv2dSubsamplingWOPosEnc(idim, odim, dropout_rate, kernels, strides)\",\"Bases: Module\",\"Convolutional 2D subsampling.\",\"Parameters:\",\"idim (int) – Input dimension.\",\"odim (int) – Output dimension.\",\"dropout_rate (float) – Dropout rate.\",\"kernels (list) – kernel sizes\",\"strides (list) – stride sizes\",\"Construct an Conv2dSubsamplingWOPosEnc object.\",\"forward(x, x_mask)\",\"Subsample x.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (#batch, time, idim).\",\"x_mask (torch.Tensor) – Input mask (#batch, 1, time).\",\"Returns: Subsampled tensor (#batch, time’, odim), : where time’ = time // 4.\",\"torch.Tensor: Subsampled mask (#batch, 1, time’), : where time’ = time // 4.\",\"Return type: torch.Tensor\"]},\"1747\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.conformer.convolution.ConvolutionModule\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.conformer.convolution.ConvolutionModule(channels, kernel_size, activation=ReLU(), bias=True)\",\"Bases: Module\",\"ConvolutionModule in Conformer model.\",\"Parameters:\",\"channels (int) – The number of channels of conv layers.\",\"kernel_size (int) – Kernerl size of conv layers.\",\"Construct an ConvolutionModule object.\",\"forward(x)\",\"Compute convolution module.\",\"Parameters:x (torch.Tensor) – Input tensor (#batch, time, channels).\",\"Returns: Output tensor (#batch, time, channels).\",\"Return type: torch.Tensor\"]},\"1748\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.embedding.ConvolutionalPositionalEmbedding\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.embedding.ConvolutionalPositionalEmbedding(embed_dim: int, dropout: float, max_len: int = 5000, num_layers: int = 1, kernel_size: int = 128, groups: int = 16, weight_norm: str = 'new', use_residual: bool = False)\",\"Bases: Module\",\"Convolutional positional embedding.\",\"Used in wav2vec2/HuBERT SSL models. https://arxiv.org/abs/1904.11660\",\"Parameters:\",\"embed_dim (int) – Feature dimension of the input Tensor.\",\"dropout (float) – unused\",\"max_len (int) – unused\",\"num_layers (int) – number of conv layers\",\"kernel_size (int) – The number of frames to be use.\",\"groups (int) – The number of groups in feature dimensions.\",\"weight_norm (str) – [new, legacy, none]. How to init conv weights. Recommended setting is none if num_layers > 1.\",\"Initialize Convoluational Positional Embedding.\",\"forward(x)\",\"Forward Method.\",\"Parameters:x (Tensor) – shape [batch, frame, feature].\",\"Returns: The resulting feature. Shape [batch, frame, feature].\",\"Return type: Tensor\"]},\"1749\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.custom_decoder.CustomDecoder\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transducer.custom_decoder.CustomDecoder(odim: int, dec_arch: List, input_layer: str = 'embed', repeat_block: int = 0, joint_activation_type: str = 'tanh', positional_encoding_type: str = 'abs_pos', positionwise_layer_type: str = 'linear', positionwise_activation_type: str = 'relu', input_layer_dropout_rate: float = 0.0, blank_id: int = 0)\",\"Bases: TransducerDecoderInterface, Module\",\"Custom decoder module for Transducer model.\",\"Parameters:\",\"odim – Output dimension.\",\"dec_arch – Decoder block architecture (type and parameters).\",\"input_layer – Input layer type.\",\"repeat_block – Number of times dec_arch is repeated.\",\"joint_activation_type – Type of activation for joint network.\",\"positional_encoding_type – Positional encoding type.\",\"positionwise_layer_type – Positionwise layer type.\",\"positionwise_activation_type – Positionwise activation type.\",\"input_layer_dropout_rate – Dropout rate for input layer.\",\"blank_id – Blank symbol ID.\",\"Construct a CustomDecoder object.\",\"batch_score(hyps: List[Hypothesis] | List[ExtendedHypothesis], dec_states: List[Tensor | None], cache: Dict[str, Any], use_lm: bool) → Tuple[Tensor, List[Tensor | None], Tensor]\",\"One-step forward hypotheses.\",\"Parameters:\",\"hyps – Hypotheses.\",\"dec_states – Decoder hidden states. [N x (B, U, D_dec)]\",\"cache – Pairs of (h_dec, dec_states) for each label sequences. (keys)\",\"use_lm – Whether to compute label ID sequences for LM.\",\"Returns: Decoder output sequences. (B, D_dec) dec_states: Decoder hidden states. [N x (B, U, D_dec)] lm_labels: Label ID sequences for LM. (B,)\",\"Return type: dec_out\",\"create_batch_states(states: List[Tensor | None], new_states: List[Tensor | None], check_list: List[List[int]]) → List[Tensor | None]\",\"Create decoder hidden states sequences.\",\"Parameters:\",\"states – Decoder hidden states. [N x (B, U, D_dec)]\",\"new_states – Decoder hidden states. [B x [N x (1, U, D_dec)]]\",\"check_list – Label ID sequences.\",\"Returns: New decoder hidden states. [N x (B, U, D_dec)]\",\"Return type: states\",\"forward(dec_input: Tensor, dec_mask: Tensor) → Tuple[Tensor, Tensor]\",\"Encode label ID sequences.\",\"Parameters:\",\"dec_input – Label ID sequences. (B, U)\",\"dec_mask – Label mask sequences. (B, U)\",\"Returns: Decoder output sequences. (B, U, D_dec) dec_output_mask: Mask of decoder output sequences. (B, U)\",\"Return type: dec_output\",\"init_state(batch_size: int | None = None) → List[Tensor | None]\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Initial decoder hidden states. [N x None]\",\"Return type: state\",\"score(hyp: Hypothesis, cache: Dict[str, Any]) → Tuple[Tensor, List[Tensor | None], Tensor]\",\"One-step forward hypothesis.\",\"Parameters:\",\"hyp – Hypothesis.\",\"cache – Pairs of (dec_out, dec_state) for each label sequence. (key)\",\"Returns: Decoder output sequence. (1, D_dec) dec_state: Decoder hidden states. [N x (1, U, D_dec)] lm_label: Label ID for LM. (1,)\",\"Return type: dec_out\",\"select_state(states: List[Tensor | None], idx: int) → List[Tensor | None]\",\"Get specified ID state from decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. [N x (B, U, D_dec)]\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID. [N x (1, U, D_dec)]\",\"Return type: state_idx\",\"set_device(device: device)\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\"]},\"1750\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.tacotron2.decoder.Decoder\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.tacotron2.decoder.Decoder(idim, odim, att, dlayers=2, dunits=1024, prenet_layers=2, prenet_units=256, postnet_layers=5, postnet_chans=512, postnet_filts=5, output_activation_fn=None, cumulate_att_w=True, use_batch_norm=True, use_concate=True, dropout_rate=0.5, zoneout_rate=0.1, reduction_factor=1)\",\"Bases: Module\",\"Decoder module of Spectrogram prediction network.\",\"This is a module of decoder of Spectrogram prediction network in Tacotron2, which described in Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. The decoder generates the sequence of features from the sequence of the hidden states.\",\"Initialize Tacotron2 decoder module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"att (torch.nn.Module) – Instance of attention class.\",\"dlayers (int,optional) – The number of decoder lstm layers.\",\"dunits (int,optional) – The number of decoder lstm units.\",\"prenet_layers (int,optional) – The number of prenet layers.\",\"prenet_units (int,optional) – The number of prenet units.\",\"postnet_layers (int,optional) – The number of postnet layers.\",\"postnet_filts (int,optional) – The number of postnet filter size.\",\"postnet_chans (int,optional) – The number of postnet filter channels.\",\"output_activation_fn (torch.nn.Module,optional) – Activation function for outputs.\",\"cumulate_att_w (bool,optional) – Whether to cumulate previous attention weight.\",\"use_batch_norm (bool,optional) – Whether to use batch normalization.\",\"use_concate (bool,optional) – Whether to concatenate encoder embedding with decoder lstm outputs.\",\"dropout_rate (float,optional) – Dropout rate.\",\"zoneout_rate (float,optional) – Zoneout rate.\",\"reduction_factor (int,optional) – Reduction factor.\",\"calculate_all_attentions(hs, hlens, ys)\",\"Calculate all of the attention weights.\",\"Parameters:\",\"hs (Tensor) – Batch of the sequences of padded hidden states (B, Tmax, idim).\",\"hlens (LongTensor) – Batch of lengths of each input batch (B,).\",\"ys (Tensor) – Batch of the sequences of padded target features (B, Lmax, odim).\",\"Returns: Batch of attention weights (B, Lmax, Tmax).\",\"Return type: numpy.ndarray\",\"####### NOTE This computation is performed in teacher-forcing manner.\",\"forward(hs, hlens, ys)\",\"Calculate forward propagation.\",\"Parameters:\",\"hs (Tensor) – Batch of the sequences of padded hidden states (B, Tmax, idim).\",\"hlens (LongTensor) – Batch of lengths of each input batch (B,).\",\"ys (Tensor) – Batch of the sequences of padded target features (B, Lmax, odim).\",\"Returns: Batch of output tensors after postnet (B, Lmax, odim). Tensor: Batch of output tensors before postnet (B, Lmax, odim). Tensor: Batch of logits of stop prediction (B, Lmax). Tensor: Batch of attention weights (B, Lmax, Tmax).\",\"Return type: Tensor\",\"####### NOTE This computation is performed in teacher-forcing manner.\",\"inference(h, threshold=0.5, minlenratio=0.0, maxlenratio=10.0, use_att_constraint=False, backward_window=None, forward_window=None)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"h (Tensor) – Input sequence of encoder hidden states (T, C).\",\"threshold (float,optional) – Threshold to stop generation.\",\"minlenratio (float,optional) – Minimum length ratio. If set to 1.0 and the length of input is 10, the minimum length of outputs will be 10 * 1 = 10.\",\"minlenratio – Minimum length ratio. If set to 10 and the length of input is 10, the maximum length of outputs will be 10 * 10 = 100.\",\"use_att_constraint (bool) – Whether to apply attention constraint introduced in Deep Voice 3.\",\"backward_window (int) – Backward window size in attention constraint.\",\"forward_window (int) – Forward window size in attention constraint.\",\"Returns: Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T).\",\"Return type: Tensor\",\"####### NOTE This computation is performed in auto-regressive manner.\"]},\"1751\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.decoder_layer.DecoderLayer\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.decoder_layer.DecoderLayer(size, self_attn, src_attn, feed_forward, dropout_rate, normalize_before=True, concat_after=False, sequential_attn=None)\",\"Bases: Module\",\"Single decoder layer module.\",\"Parameters:\",\"size (int) – Input dimension.\",\"self_attn (torch.nn.Module) – Self-attention module instance. MultiHeadedAttention instance can be used as the argument.\",\"src_attn (torch.nn.Module) – Self-attention module instance. MultiHeadedAttention instance can be used as the argument.\",\"feed_forward (torch.nn.Module) – Feed-forward module instance. PositionwiseFeedForward, MultiLayeredConv1d, or Conv1dLinear instance can be used as the argument.\",\"dropout_rate (float) – Dropout rate.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"sequential_attn (bool) – computes attn first on pre_x then on x, thereby attending to two sources in sequence.\",\"Construct an DecoderLayer object.\",\"forward(tgt, tgt_mask, memory, memory_mask, cache=None, pre_memory=None, pre_memory_mask=None)\",\"Compute decoded features.\",\"Parameters:\",\"tgt (torch.Tensor) – Input tensor (#batch, maxlen_out, size).\",\"tgt_mask (torch.Tensor) – Mask for input tensor (#batch, maxlen_out).\",\"memory (torch.Tensor) – Encoded memory, float32 (#batch, maxlen_in, size).\",\"memory_mask (torch.Tensor) – Encoded memory mask (#batch, 1, maxlen_in).\",\"cache (List *[*torch.Tensor]) – List of cached tensors. Each tensor shape should be (#batch, maxlen_out - 1, size).\",\"pre_memory (torch.Tensor) – Encoded memory (#batch, maxlen_in, size).\",\"pre_memory_mask (torch.Tensor) – Encoded memory mask (#batch, maxlen_in).\",\"Returns: Output tensor(#batch, maxlen_out, size). torch.Tensor: Mask for output tensor (#batch, maxlen_out). torch.Tensor: Encoded memory (#batch, maxlen_in, size). torch.Tensor: Encoded memory mask (#batch, maxlen_in).\",\"Return type: torch.Tensor\",\"forward_partially_AR(tgt, tgt_mask, tgt_lengths, memory, memory_mask, cache=None)\",\"Forward partially in autoregression fashion.\"]},\"1752\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.fastspeech.duration_predictor.DurationPredictor\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.fastspeech.duration_predictor.DurationPredictor(idim, n_layers=2, n_chans=384, kernel_size=3, dropout_rate=0.1, offset=1.0)\",\"Bases: Module\",\"Duration predictor module.\",\"This is a module of duration predictor described in FastSpeech: Fast, Robust and Controllable Text to Speech. The duration predictor predicts a duration of each frame in log domain from the hidden embeddings of encoder.\"]},\"1753\":{\"h\":\"NOTE\",\"t\":[\"The calculation domain of outputs is different between in forward and in inference. In forward, the outputs are calculated in log domain but in inference, those are calculated in linear domain.\",\"Initilize duration predictor module.\",\"Parameters:\",\"idim (int) – Input dimension.\",\"n_layers (int,optional) – Number of convolutional layers.\",\"n_chans (int,optional) – Number of channels of convolutional layers.\",\"kernel_size (int,optional) – Kernel size of convolutional layers.\",\"dropout_rate (float,optional) – Dropout rate.\",\"offset (float,optional) – Offset value to avoid nan in log domain.\",\"forward(xs, x_masks=None)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of input sequences (B, Tmax, idim).\",\"x_masks (ByteTensor,optional) – Batch of masks indicating padded part (B, Tmax).\",\"Returns: Batch of predicted durations in log domain (B, Tmax).\",\"Return type: Tensor\",\"inference(xs, x_masks=None)\",\"Inference duration.\",\"Parameters:\",\"xs (Tensor) – Batch of input sequences (B, Tmax, idim).\",\"x_masks (ByteTensor,optional) – Batch of masks indicating padded part (B, Tmax).\",\"Returns: Batch of predicted durations in linear domain (B, Tmax).\",\"Return type: LongTensor\"]},\"1754\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.fastspeech.duration_predictor.DurationPredictorLoss\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.fastspeech.duration_predictor.DurationPredictorLoss(offset=1.0, reduction='mean')\",\"Bases: Module\",\"Loss function module for duration predictor.\",\"The loss value is Calculated in log domain to make it Gaussian.\",\"Initilize duration predictor loss module.\",\"Parameters:\",\"offset (float,optional) – Offset value to avoid nan in log domain.\",\"reduction (str) – Reduction type in loss calculation.\",\"forward(outputs, targets)\",\"Calculate forward propagation.\",\"Parameters:\",\"outputs (Tensor) – Batch of prediction durations in log domain (B, T)\",\"targets (LongTensor) – Batch of groundtruth durations in linear domain (B, T)\",\"Returns: Mean squared error loss value.\",\"Return type: Tensor\"]},\"1755\":{\"h\":\"NOTE\",\"t\":[\"outputs is in log domain but targets is in linear domain.\"]},\"1756\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.dynamic_conv.DynamicConvolution\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.dynamic_conv.DynamicConvolution(wshare, n_feat, dropout_rate, kernel_size, use_kernel_mask=False, use_bias=False)\",\"Bases: Module\",\"Dynamic Convolution layer.\",\"This implementation is based on https://github.com/pytorch/fairseq/tree/master/fairseq\",\"Parameters:\",\"wshare (int) – the number of kernel of convolution\",\"n_feat (int) – the number of features\",\"dropout_rate (float) – dropout_rate\",\"kernel_size (int) – kernel size (length)\",\"use_kernel_mask (bool) – Use causal mask or not for convolution kernel\",\"use_bias (bool) – Use bias term or not.\",\"Construct Dynamic Convolution layer.\",\"forward(query, key, value, mask)\",\"Forward of ‘Dynamic Convolution’.\",\"This function takes query, key and value but uses only quert. This is just for compatibility with self-attention layer (attention.py)\",\"Parameters:\",\"query (torch.Tensor) – (batch, time1, d_model) input tensor\",\"key (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"value (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"mask (torch.Tensor) – (batch, time1, time2) mask\",\"Returns: (batch, time1, d_model) output\",\"Return type: x (torch.Tensor)\"]},\"1757\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.dynamic_conv2d.DynamicConvolution2D\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.dynamic_conv2d.DynamicConvolution2D(wshare, n_feat, dropout_rate, kernel_size, use_kernel_mask=False, use_bias=False)\",\"Bases: Module\",\"Dynamic 2-Dimensional Convolution layer.\",\"This implementation is based on https://github.com/pytorch/fairseq/tree/master/fairseq\",\"Parameters:\",\"wshare (int) – the number of kernel of convolution\",\"n_feat (int) – the number of features\",\"dropout_rate (float) – dropout_rate\",\"kernel_size (int) – kernel size (length)\",\"use_kernel_mask (bool) – Use causal mask or not for convolution kernel\",\"use_bias (bool) – Use bias term or not.\",\"Construct Dynamic 2-Dimensional Convolution layer.\",\"forward(query, key, value, mask)\",\"Forward of ‘Dynamic 2-Dimensional Convolution’.\",\"This function takes query, key and value but uses only query. This is just for compatibility with self-attention layer (attention.py)\",\"Parameters:\",\"query (torch.Tensor) – (batch, time1, d_model) input tensor\",\"key (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"value (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"mask (torch.Tensor) – (batch, time1, time2) mask\",\"Returns: (batch, time1, d_model) output\",\"Return type: x (torch.Tensor)\"]},\"1758\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.tacotron2.encoder.Encoder\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.tacotron2.encoder.Encoder(idim, input_layer='embed', embed_dim=512, elayers=1, eunits=512, econv_layers=3, econv_chans=512, econv_filts=5, use_batch_norm=True, use_residual=False, dropout_rate=0.5, padding_idx=0)\",\"Bases: Module\",\"Encoder module of Spectrogram prediction network.\",\"This is a module of encoder of Spectrogram prediction network in Tacotron2, which described in Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. This is the encoder which converts either a sequence of characters or acoustic features into the sequence of hidden states.\",\"Initialize Tacotron2 encoder module.\",\"Parameters:\",\"idim (int)\",\"input_layer (str) – Input layer type.\",\"embed_dim (int,optional)\",\"elayers (int,optional)\",\"eunits (int,optional)\",\"econv_layers (int,optional)\",\"econv_filts (int,optional)\",\"econv_chans (int,optional)\",\"use_batch_norm (bool,optional)\",\"use_residual (bool,optional)\",\"dropout_rate (float,optional)\",\"forward(xs, ilens=None)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of the padded sequence. Either character ids (B, Tmax) or acoustic feature (B, Tmax, idim * encoder_reduction_factor). Padded value should be 0.\",\"ilens (LongTensor) – Batch of lengths of each input batch (B,).\",\"Returns: Batch of the sequences of encoder states(B, Tmax, eunits). LongTensor: Batch of lengths of each sequence (B,)\",\"Return type: Tensor\",\"inference(x)\",\"Inference.\",\"Parameters:x (Tensor) – The sequeunce of character ids (T,) or acoustic feature (T, idim * encoder_reduction_factor).\",\"Returns: The sequences of encoder states(T, eunits).\",\"Return type: Tensor\"]},\"1759\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.encoder_layer.EncoderLayer\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.encoder_layer.EncoderLayer(size, self_attn, feed_forward, dropout_rate, normalize_before=True, concat_after=False, stochastic_depth_rate=0.0)\",\"Bases: Module\",\"Encoder layer module.\",\"Parameters:\",\"size (int) – Input dimension.\",\"self_attn (torch.nn.Module) – Self-attention module instance. MultiHeadedAttention or RelPositionMultiHeadedAttention instance can be used as the argument.\",\"feed_forward (torch.nn.Module) – Feed-forward module instance. PositionwiseFeedForward, MultiLayeredConv1d, or Conv1dLinear instance can be used as the argument.\",\"dropout_rate (float) – Dropout rate.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"concat_after (bool) – Whether to concat attention layer’s input and output. if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"stochastic_depth_rate (float) – Proability to skip this layer. During training, the layer may skip residual computation and return input as-is with given probability.\",\"Construct an EncoderLayer object.\",\"forward(x, mask, cache=None)\",\"Compute encoded features.\",\"Parameters:\",\"x_input (torch.Tensor) – Input tensor (#batch, time, size).\",\"mask (torch.Tensor) – Mask tensor for the input (#batch, 1, time).\",\"cache (torch.Tensor) – Cache tensor of the input (#batch, time - 1, size).\",\"Returns: Output tensor (#batch, time, size). torch.Tensor: Mask tensor (#batch, 1, time).\",\"Return type: torch.Tensor\"]},\"1760\":{\"h\":\"espnet2.legacy.nets.e2e_mt_common.ErrorCalculator\",\"t\":[\"source\",\"class espnet2.legacy.nets.e2e_mt_common.ErrorCalculator(char_list, sym_space, sym_pad, report_bleu=False)\",\"Bases: object\",\"Calculate BLEU for ST and MT models during training.\",\"Parameters:\",\"y_hats – numpy array with predicted text\",\"y_pads – numpy array with true (target) text\",\"char_list – vocabulary list\",\"sym_space – space symbol\",\"sym_pad – pad symbol\",\"report_bleu – report BLUE score if True\",\"Construct an ErrorCalculator object.\",\"calculate_bleu_ctc(ys_hat, ys_pad)\",\"Calculate sentence-level BLEU score for CTC.\",\"Parameters:\",\"ys_hat (torch.Tensor) – prediction (batch, seqlen)\",\"ys_pad (torch.Tensor) – reference (batch, seqlen)\",\"Returns: corpus-level BLEU score\",\":rtype float\",\"calculate_corpus_bleu(ys_hat, ys_pad)\",\"Calculate corpus-level BLEU score in a mini-batch.\",\"Parameters:\",\"seqs_hat (torch.Tensor) – prediction (batch, seqlen)\",\"seqs_true (torch.Tensor) – reference (batch, seqlen)\",\"Returns: corpus-level BLEU score\",\":rtype float\"]},\"1761\":{\"h\":\"espnet2.legacy.utils.dummy_chainer.Evaluator\",\"t\":[\"source\",\"class espnet2.legacy.utils.dummy_chainer.Evaluator\",\"Bases: object\",\"A dummy Evaluator wrapper.\"]},\"1762\":{\"h\":\"espnet2.legacy.nets.transducer_decoder_interface.ExtendedHypothesis\",\"t\":[\"source\",\"class espnet2.legacy.nets.transducer_decoder_interface.ExtendedHypothesis(score: float, yseq: List[int], dec_state: Tuple[Tensor, Tensor | None] | List[Tensor | None] | Tensor, lm_state: Dict[str, Any] | List[Any] | None = None, dec_out: List[Tensor] | None = None, lm_scores: Tensor | None = None)\",\"Bases: Hypothesis\",\"Extended hypothesis definition for NSC beam search and mAES.\",\"dec_out : List[Tensor]= None\",\"lm_scores : Tensor= None\"]},\"1763\":{\"h\":\"espnet2.legacy.utils.dummy_chainer.Extension\",\"t\":[\"source\",\"class espnet2.legacy.utils.dummy_chainer.Extension(*args, **kwargs)\",\"Bases: object\",\"A dummy Extension wrapper.\",\"Initliaze Dummy Extension.\"]},\"1764\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformerLoss\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformerLoss(use_masking=True, use_weighted_masking=False)\",\"Bases: Module\",\"Loss function module for feed-forward Transformer.\",\"Initialize feed-forward Transformer loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"forward(after_outs, before_outs, d_outs, ys, ds, ilens, olens)\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, Lmax, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, Lmax, odim).\",\"d_outs (Tensor) – Batch of outputs of duration predictor (B, Tmax).\",\"ys (Tensor) – Batch of target features (B, Lmax, odim).\",\"ds (Tensor) – Batch of durations (B, Tmax).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: L1 loss value. Tensor: Duration predictor loss value.\",\"Return type: Tensor\"]},\"1765\":{\"h\":\"espnet2.legacy.transform.spec_augment.FreqMask\",\"t\":[\"source\"]},\"1766\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.frontends.frontend.Frontend\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.frontends.frontend.Frontend(idim: int, use_wpe: bool = False, wtype: str = 'blstmp', wlayers: int = 3, wunits: int = 300, wprojs: int = 320, wdropout_rate: float = 0.0, taps: int = 5, delay: int = 3, use_dnn_mask_for_wpe: bool = True, use_beamformer: bool = False, btype: str = 'blstmp', blayers: int = 3, bunits: int = 300, bprojs: int = 320, bnmask: int = 2, badim: int = 320, ref_channel: int = -1, bdropout_rate=0.0)\",\"Bases: Module\",\"Frontend class.\",\"Initialize frontend.\",\"forward(x: ComplexTensor, ilens: LongTensor | ndarray | List[int]) → Tuple[ComplexTensor, LongTensor, ComplexTensor | None]\",\"Calculate frontend forward propagation.\"]},\"1767\":{\"h\":\"espnet2.legacy.transform.functional.FuncTrans\",\"t\":[\"source\"]},\"1768\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.GDCAttLoc\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.GDCAttLoc(eprojs, dunits, att_dim, aconv_chans, aconv_filts, han_mode=False)\",\"Bases: Module\",\"Global duration control attention module.\",\"Reference: Singing-Tacotron: Global Duration Control Attention and Dynamic Filter for End-to-end Singing Voice Synthesis (https://arxiv.org/abs/2202.07907) :param int eprojs: # projection-units of encoder :param int dunits: # units of decoder :param int att_dim: attention dimension :param int aconv_chans: # channels of attention convolution :param int aconv_filts: filter size of attention convolution :param bool han_mode: flag to swith on mode of hierarchical attention\",\"and not store pre_compute_enc_h\",\"Initialize GDCAttLoc.\",\"forward(enc_hs_pad, enc_hs_len, trans_token, dec_z, att_prev, scaling=1.0, last_attended_idx=None, backward_window=1, forward_window=3)\",\"Calcualte AttLoc forward propagation.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B x T_max x D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"trans_token (torch.Tensor) – Global transition token for duration (B x T_max x 1)\",\"dec_z (torch.Tensor) – decoder hidden state (B x D_dec)\",\"att_prev (torch.Tensor) – previous attention weight (B x T_max)\",\"scaling (float) – scaling parameter before applying softmax\",\"forward_window (int) – forward window size when constraining attention\",\"last_attended_idx (int) – index of the inputs of the last attended\",\"backward_window (int) – backward window size in attention constraint\",\"forward_window – forward window size in attetion constraint\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights (B x T_max)\",\"Return type: torch.Tensor\",\"reset()\",\"Reset states.\"]},\"1769\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.gtn_ctc.GTNCTCLossFunction\",\"t\":[\"source\"]},\"1770\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.e2e_tts_tacotron2.GuidedAttentionLoss\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.e2e_tts_tacotron2.GuidedAttentionLoss(sigma=0.4, alpha=1.0, reset_always=True)\",\"Bases: Module\",\"Guided attention loss function module.\",\"This module calculates the guided attention loss described in Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention, which forces the attention to be diagonal.\",\"Initialize guided attention loss module.\",\"Parameters:\",\"sigma (float,optional) – Standard deviation to control how close attention to a diagonal.\",\"alpha (float,optional) – Scaling coefficient (lambda).\",\"reset_always (bool,optional) – Whether to always reset masks.\",\"forward(att_ws, ilens, olens)\",\"Calculate forward propagation.\",\"Parameters:\",\"att_ws (Tensor) – Batch of attention weights (B, T_max_out, T_max_in).\",\"ilens (LongTensor) – Batch of input lengths (B,).\",\"olens (LongTensor) – Batch of output lengths (B,).\",\"Returns: Guided attention loss value.\",\"Return type: Tensor\"]},\"1771\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.e2e_tts_transformer.GuidedMultiHeadAttentionLoss\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.e2e_tts_transformer.GuidedMultiHeadAttentionLoss(sigma=0.4, alpha=1.0, reset_always=True)\",\"Bases: GuidedAttentionLoss\",\"Guided attention loss function module for multi head attention.\",\"Parameters:\",\"sigma (float,optional) – Standard deviation to control\",\"diagonal. (how close attention to a)\",\"alpha (float,optional) – Scaling coefficient (lambda).\",\"reset_always (bool,optional) – Whether to always reset masks.\",\"Initialize guided attention loss module.\",\"Parameters:\",\"sigma (float,optional) – Standard deviation to control how close attention to a diagonal.\",\"alpha (float,optional) – Scaling coefficient (lambda).\",\"reset_always (bool,optional) – Whether to always reset masks.\",\"forward(att_ws, ilens, olens)\",\"Calculate forward propagation.\",\"Parameters:\",\"att_ws (Tensor) – Batch of multi head attention weights (B, H, T_max_out, T_max_in).\",\"ilens (LongTensor) – Batch of input lengths (B,).\",\"olens (LongTensor) – Batch of output lengths (B,).\",\"Returns: Guided attention loss value.\",\"Return type: Tensor\"]},\"1772\":{\"h\":\"espnet2.legacy.utils.cli_readers.HDF5Reader\",\"t\":[\"source\",\"class espnet2.legacy.utils.cli_readers.HDF5Reader(rspecifier, return_shape=False)\",\"Bases: object\",\"HDF5 Reader Class.\",\"Initialize HDF5 Reader.\"]},\"1773\":{\"h\":\"espnet2.legacy.utils.cli_writers.HDF5Writer\",\"t\":[\"source\",\"class espnet2.legacy.utils.cli_writers.HDF5Writer(wspecifier, write_num_frames=None, compress=False)\",\"Bases: BaseWriter\",\"HDF5Writer Class.\"]},\"1774\":{\"h\":\"Examples\",\"t\":[\">>> with HDF5Writer('ark:out.h5', compress=True) as f: ... f['key'] = array\",\"Initialize HDF5 Writer.\"]},\"1775\":{\"h\":\"espnet2.legacy.nets.transducer_decoder_interface.Hypothesis\",\"t\":[\"source\",\"class espnet2.legacy.nets.transducer_decoder_interface.Hypothesis(score: float, yseq: List[int], dec_state: Tuple[Tensor, Tensor | None] | List[Tensor | None] | Tensor, lm_state: Dict[str, Any] | List[Any] | None = None)\",\"Bases: object\",\"Default hypothesis definition for Transducer search algorithms.\",\"dec_state : Tuple[Tensor, Tensor | None] | List[Tensor | None] | Tensor\",\"lm_state : Dict[str, Any] | List[Any]= None\",\"score : float\",\"yseq : List[int]\"]},\"1776\":{\"h\":\"espnet2.legacy.transform.spectrogram.IStft\",\"t\":[\"source\",\"class espnet2.legacy.transform.spectrogram.IStft(n_shift, win_length=None, window='hann', center=True)\",\"Bases: object\",\"iSTFT Class.\",\"Initialize Class.\"]},\"1777\":{\"h\":\"espnet2.legacy.transform.transform_interface.Identity\",\"t\":[\"source\",\"class espnet2.legacy.transform.transform_interface.Identity\",\"Bases: TransformInterface\",\"Identity Function.\"]},\"1778\":{\"h\":\"espnet2.legacy.utils.dummy_chainer.Iterator\",\"t\":[\"source\",\"class espnet2.legacy.utils.dummy_chainer.Iterator(*args, **kwargs)\",\"Bases: object\",\"A dummy Iterator wrapper.\",\"Initliaze Dummy Iterator.\",\"serialize(*args, **kwargs)\",\"Append values to serializer.\"]},\"1779\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.joint_network.JointNetwork\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transducer.joint_network.JointNetwork(joint_output_size: int, encoder_output_size: int, decoder_output_size: int, joint_space_size: int, joint_activation_type: int)\",\"Bases: Module\",\"Transducer joint network module.\",\"Parameters:\",\"joint_output_size – Joint network output dimension\",\"encoder_output_size – Encoder output dimension.\",\"decoder_output_size – Decoder output dimension.\",\"joint_space_size – Dimension of joint space.\",\"joint_activation_type – Type of activation for joint network.\",\"Joint network initializer.\",\"forward(enc_out: Tensor, dec_out: Tensor, is_aux: bool = False, quantization: bool = False) → Tensor\",\"Joint computation of encoder and decoder hidden state sequences.\",\"Parameters:\",\"enc_out – Expanded encoder output state sequences (B, T, 1, D_enc)\",\"dec_out – Expanded decoder output state sequences (B, 1, U, D_dec)\",\"is_aux – Whether auxiliary tasks in used.\",\"quantization – Whether dynamic quantization is used.\",\"Returns: Joint output state sequences. (B, T, U, D_out)\",\"Return type: joint_out\"]},\"1780\":{\"h\":\"espnet2.legacy.utils.cli_readers.KaldiReader\",\"t\":[\"source\",\"class espnet2.legacy.utils.cli_readers.KaldiReader(rspecifier, return_shape=False, segments=None)\",\"Bases: object\",\"Kaldi Reader Class.\",\"Initialize Kaldi Reader.\"]},\"1781\":{\"h\":\"espnet2.legacy.utils.cli_writers.KaldiWriter\",\"t\":[\"source\",\"class espnet2.legacy.utils.cli_writers.KaldiWriter(wspecifier, write_num_frames=None, compress=False, compression_method=2)\",\"Bases: BaseWriter\",\"Kaldi Writer Class.\",\"Initialize Kaldi writer.\"]},\"1782\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.label_smoothing_loss.LabelSmoothingLoss\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.label_smoothing_loss.LabelSmoothingLoss(size, padding_idx, smoothing, normalize_length=False, criterion=KLDivLoss())\",\"Bases: Module\",\"Label-smoothing loss.\",\"Parameters:\",\"size (int) – the number of class\",\"padding_idx (int) – ignored class id\",\"smoothing (float) – smoothing rate (0.0 means the conventional CE)\",\"normalize_length (bool) – normalize loss by sequence length if True\",\"criterion (torch.nn.Module) – loss function to be smoothed\",\"Construct an LabelSmoothingLoss object.\",\"forward(x, target)\",\"Compute loss between x and target.\",\"Parameters:\",\"x (torch.Tensor) – prediction (batch, seqlen, class)\",\"target (torch.Tensor) – target signal masked with self.padding_id (batch, seqlen)\",\"Returns: scalar float value\",\":rtype torch.Tensor\"]},\"1783\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.layer_norm.LayerNorm\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.layer_norm.LayerNorm(nout, dim=-1)\",\"Bases: LayerNorm\",\"Layer normalization module.\",\"Parameters:\",\"nout (int) – Output dim size.\",\"dim (int) – Dimension to be normalized.\",\"Construct an LayerNorm object.\",\"forward(x)\",\"Apply layer normalization.\",\"Parameters:x (torch.Tensor) – Input tensor.\",\"Returns: Normalized tensor.\",\"Return type: torch.Tensor\"]},\"1784\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.embedding.LearnableFourierPosEnc\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.embedding.LearnableFourierPosEnc(d_model, dropout_rate=0.0, max_len=5000, gamma=1.0, apply_scaling=False, hidden_dim=None)\",\"Bases: Module\",\"Learnable Fourier Features for Positional Encoding.\",\"See https://arxiv.org/pdf/2106.02795.pdf\",\"Parameters:\",\"d_model (int) – Embedding dimension.\",\"dropout_rate (float) – Dropout rate.\",\"max_len (int) – Maximum input length.\",\"gamma (float) – init parameter for the positional kernel variance see https://arxiv.org/pdf/2106.02795.pdf.\",\"apply_scaling (bool) – Whether to scale the input before adding the pos encoding.\",\"hidden_dim (int) – if not None, we modulate the pos encodings with an MLP whose hidden layer has hidden_dim neurons.\",\"Initialize class.\",\"extend_pe(x)\",\"Reset the positional encodings.\",\"forward(x: Tensor)\",\"Add positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, time, *).\",\"Returns: Encoded tensor (batch, time, *).\",\"Return type: torch.Tensor\"]},\"1785\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.attention.LegacyRelPositionMultiHeadedAttention\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.attention.LegacyRelPositionMultiHeadedAttention(n_head, n_feat, dropout_rate, zero_triu=False)\",\"Bases: MultiHeadedAttention\",\"Multi-Head Attention layer with relative position encoding (old version).\",\"Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"Paper: https://arxiv.org/abs/1901.02860\",\"Parameters:\",\"n_head (int) – The number of heads.\",\"n_feat (int) – The number of features.\",\"dropout_rate (float) – Dropout rate.\",\"zero_triu (bool) – Whether to zero the upper triangular part of attention matrix.\",\"Construct an RelPositionMultiHeadedAttention object.\",\"forward(query, key, value, pos_emb, mask)\",\"Compute ‘Scaled Dot Product Attention’ with rel. positional encoding.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"pos_emb (torch.Tensor) – Positional embedding tensor (#batch, time1, size).\",\"mask (torch.Tensor) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).\",\"Returns: Output tensor (#batch, time1, d_model).\",\"Return type: torch.Tensor\",\"rel_shift(x)\",\"Compute relative positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, head, time1, time2).\",\"Returns: Output tensor.\",\"Return type: torch.Tensor\"]},\"1786\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.embedding.LegacyRelPositionalEncoding\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.embedding.LegacyRelPositionalEncoding(d_model, dropout_rate, max_len=5000)\",\"Bases: PositionalEncoding\",\"Relative positional encoding module (old version).\",\"Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"See : Appendix B in https://arxiv.org/abs/1901.02860\",\"Parameters:\",\"d_model (int) – Embedding dimension.\",\"dropout_rate (float) – Dropout rate.\",\"max_len (int) – Maximum input length.\",\"Initialize class.\",\"forward(x)\",\"Compute positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, time, *).\",\"Returns: Encoded tensor (batch, time, *). torch.Tensor: Positional embedding tensor (1, time, *).\",\"Return type: torch.Tensor\"]},\"1787\":{\"h\":\"espnet2.legacy.nets.scorers.length_bonus.LengthBonus\",\"t\":[\"source\",\"class espnet2.legacy.nets.scorers.length_bonus.LengthBonus(n_vocab: int)\",\"Bases: BatchScorerInterface\",\"Length bonus in beam search.\",\"Initialize class.\",\"Parameters:n_vocab (int) – The number of tokens in vocabulary for beam search\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor) → Tuple[Tensor, List[Any]]\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"score(y, state, x)\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – 2D encoder feature that generates ys.\",\"Returns: Tuple of : torch.float32 scores for next token (n_vocab) and None\",\"Return type: tuple[torch.Tensor, Any]\"]},\"1788\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.fastspeech.length_regulator.LengthRegulator\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.fastspeech.length_regulator.LengthRegulator(pad_value=0.0)\",\"Bases: Module\",\"Length regulator module for feed-forward Transformer.\",\"This is a module of length regulator described in FastSpeech: Fast, Robust and Controllable Text to Speech. The length regulator expands char or phoneme-level embedding features to frame-level by repeating each feature based on the corresponding predicted durations.\",\"Initilize length regulator module.\",\"Parameters:pad_value (float,optional) – Value used for padding.\",\"forward(xs, ds, alpha=1.0)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of sequences of char or phoneme embeddings (B, Tmax, D).\",\"ds (LongTensor) – Batch of durations of each frame (B, T).\",\"alpha (float,optional) – Alpha value to control speed of speech.\",\"Returns: replicated input tensor based on durations (B, T*, D).\",\"Return type: Tensor\"]},\"1789\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.lightconv.LightweightConvolution\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.lightconv.LightweightConvolution(wshare, n_feat, dropout_rate, kernel_size, use_kernel_mask=False, use_bias=False)\",\"Bases: Module\",\"Lightweight Convolution layer.\",\"This implementation is based on https://github.com/pytorch/fairseq/tree/master/fairseq\",\"Parameters:\",\"wshare (int) – the number of kernel of convolution\",\"n_feat (int) – the number of features\",\"dropout_rate (float) – dropout_rate\",\"kernel_size (int) – kernel size (length)\",\"use_kernel_mask (bool) – Use causal mask or not for convolution kernel\",\"use_bias (bool) – Use bias term or not.\",\"Construct Lightweight Convolution layer.\",\"forward(query, key, value, mask)\",\"Forward of ‘Lightweight Convolution’.\",\"This function takes query, key and value but uses only query. This is just for compatibility with self-attention layer (attention.py)\",\"Parameters:\",\"query (torch.Tensor) – (batch, time1, d_model) input tensor\",\"key (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"value (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"mask (torch.Tensor) – (batch, time1, time2) mask\",\"Returns: (batch, time1, d_model) output\",\"Return type: x (torch.Tensor)\"]},\"1790\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.lightconv2d.LightweightConvolution2D\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.lightconv2d.LightweightConvolution2D(wshare, n_feat, dropout_rate, kernel_size, use_kernel_mask=False, use_bias=False)\",\"Bases: Module\",\"Lightweight 2-Dimensional Convolution layer.\",\"This implementation is based on https://github.com/pytorch/fairseq/tree/master/fairseq\",\"Parameters:\",\"wshare (int) – the number of kernel of convolution\",\"n_feat (int) – the number of features\",\"dropout_rate (float) – dropout_rate\",\"kernel_size (int) – kernel size (length)\",\"use_kernel_mask (bool) – Use causal mask or not for convolution kernel\",\"use_bias (bool) – Use bias term or not.\",\"Construct Lightweight 2-Dimensional Convolution layer.\",\"forward(query, key, value, mask)\",\"Forward of ‘Lightweight 2-Dimensional Convolution’.\",\"This function takes query, key and value but uses only query. This is just for compatibility with self-attention layer (attention.py)\",\"Parameters:\",\"query (torch.Tensor) – (batch, time1, d_model) input tensor\",\"key (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"value (torch.Tensor) – (batch, time2, d_model) NOT USED\",\"mask (torch.Tensor) – (batch, time1, time2) mask\",\"Returns: (batch, time1, d_model) output\",\"Return type: x (torch.Tensor)\"]},\"1791\":{\"h\":\"espnet2.legacy.transform.spectrogram.LogMelSpectrogram\",\"t\":[\"source\",\"class espnet2.legacy.transform.spectrogram.LogMelSpectrogram(fs, n_mels, n_fft, n_shift, win_length=None, window='hann', fmin=None, fmax=None, eps=1e-10)\",\"Bases: object\",\"LogMel Spectrogram Class.\",\"Initialize LogMel Spectrogram Class.\"]},\"1792\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.longformer_attention.LongformerAttention\",\"t\":[\"source\"]},\"1793\":{\"h\":\"espnet2.legacy.nets.scorer_interface.MaskParallelScorerInterface\",\"t\":[\"source\",\"class espnet2.legacy.nets.scorer_interface.MaskParallelScorerInterface\",\"Bases: ScorerInterface\",\"Mask Parallel Scorer Interface.\",\"Initialize method.\"]},\"1794\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.attention.MultiHeadedAttention\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.attention.MultiHeadedAttention(n_head, n_feat, dropout_rate, qk_norm=False, use_flash_attn=False, causal=False, cross_attn=False, use_sdpa=False)\",\"Bases: Module\",\"Multi-Head Attention layer.\",\"Parameters:\",\"n_head (int) – The number of heads.\",\"n_feat (int) – The number of features.\",\"dropout_rate (float) – Dropout rate.\",\"qk_norm (bool) – Normalize q and k before dot product.\",\"use_flash_attn (bool) – Use flash_attn implementation.\",\"causal (bool) – Apply causal attention.\",\"cross_attn (bool) – Cross attention instead of self attention.\",\"use_sdpa (bool) – Use PyTorch’s scaled dot product attention.\",\"Construct an MultiHeadedAttention object.\",\"forward(query, key, value, mask, expand_kv=False)\",\"Compute scaled dot product attention.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"mask (torch.Tensor) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).\",\"expand_kv (bool) – Used only for partially autoregressive (PAR) decoding. When set to True, Linear layers are computed only for the first batch. This is useful to reduce the memory usage during decoding when the batch size is #beam_size x #mask_count, which can be large. Typically, in single waveform inference of PAR, Linear layers should not be computed for all batches for source-attention.\",\"Returns: Output tensor (#batch, time1, d_model).\",\"Return type: torch.Tensor\",\"forward_attention(value, scores, mask)\",\"Compute attention context vector.\",\"Parameters:\",\"value (torch.Tensor) – Transformed value (#batch, n_head, time2, d_k).\",\"scores (torch.Tensor) – Attention score (#batch, n_head, time1, time2).\",\"mask (torch.Tensor) – Mask (#batch, 1, time2) or (#batch, time1, time2).\",\"Returns: Transformed value (#batch, time1, d_model) : weighted by the attention score (#batch, time1, time2).\",\"Return type: torch.Tensor\",\"forward_qkv(query, key, value, expand_kv=False)\",\"Transform query, key and value.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"expand_kv (bool) – Used only for partially autoregressive (PAR) decoding.\",\"Returns: Transformed query tensor (#batch, n_head, time1, d_k). torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k). torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).\",\"Return type: torch.Tensor\"]},\"1795\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.multi_layer_conv.MultiLayeredConv1d\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.multi_layer_conv.MultiLayeredConv1d(in_chans, hidden_chans, kernel_size, dropout_rate)\",\"Bases: Module\",\"Multi-layered conv1d for Transformer block.\",\"This is a module of multi-leyered conv1d designed to replace positionwise feed-forward network in Transforner block, which is introduced in FastSpeech: Fast, Robust and Controllable Text to Speech.\",\"Initialize MultiLayeredConv1d module.\",\"Parameters:\",\"in_chans (int) – Number of input channels.\",\"hidden_chans (int) – Number of hidden channels.\",\"kernel_size (int) – Kernel size of conv1d.\",\"dropout_rate (float) – Dropout rate.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (torch.Tensor) – Batch of input tensors (B, T, in_chans).\",\"Returns: Batch of output tensors (B, T, hidden_chans).\",\"Return type: torch.Tensor\"]},\"1796\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.repeat.MultiSequential\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.repeat.MultiSequential(*args, layer_drop_rate=0.0)\",\"Bases: Sequential\",\"Multi-input multi-output torch.nn.Sequential.\",\"Initialize MultiSequential with layer_drop.\",\"Parameters:layer_drop_rate (float) – Probability of dropping out each fn (layer).\",\"forward(*args)\",\"Repeat.\"]},\"1797\":{\"h\":\"espnet2.legacy.utils.dummy_chainer.MultiprocessIterator\",\"t\":[\"source\",\"class espnet2.legacy.utils.dummy_chainer.MultiprocessIterator\",\"Bases: object\",\"A dummy MultiprocessIterator wrapper.\",\"start_shuffle(*args, **kwargs)\",\"Report at every step.\"]},\"1798\":{\"h\":\"espnet2.legacy.nets.scorers.ngram.NgramFullScorer\",\"t\":[\"source\",\"class espnet2.legacy.nets.scorers.ngram.NgramFullScorer(ngram_model, token_list)\",\"Bases: Ngrambase, BatchScorerInterface\",\"Fullscorer for ngram.\",\"Initialize Ngrambase.\",\"Parameters:\",\"ngram_model – ngram model path\",\"token_list – token list from dict or model.json\",\"score(y, state, x)\",\"Score interface for both full and partial scorer.\",\"Parameters:\",\"y – previous char\",\"state – previous state\",\"x – encoded feature\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\"]},\"1799\":{\"h\":\"espnet2.legacy.nets.scorers.ngram.NgramPartScorer\",\"t\":[\"source\",\"class espnet2.legacy.nets.scorers.ngram.NgramPartScorer(ngram_model, token_list)\",\"Bases: Ngrambase, PartialScorerInterface\",\"Partialscorer for ngram.\",\"Initialize Ngrambase.\",\"Parameters:\",\"ngram_model – ngram model path\",\"token_list – token list from dict or model.json\",\"score_partial(y, next_token, state, x)\",\"Score interface for both full and partial scorer.\",\"Parameters:\",\"y – previous char\",\"next_token – next token need to be score\",\"state – previous state\",\"x – encoded feature\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"select_state(state, i)\",\"Empty select state for scorer interface.\"]},\"1800\":{\"h\":\"espnet2.legacy.nets.scorers.ngram.Ngrambase\",\"t\":[\"source\",\"class espnet2.legacy.nets.scorers.ngram.Ngrambase(ngram_model, token_list)\",\"Bases: ABC\",\"Ngram base implemented through ScorerInterface.\",\"Initialize Ngrambase.\",\"Parameters:\",\"ngram_model – ngram model path\",\"token_list – token list from dict or model.json\",\"init_state(x)\",\"Initialize tmp state.\",\"score_partial_(y, next_token, state, x)\",\"Score interface for both full and partial scorer.\",\"Parameters:\",\"y – previous char\",\"next_token – next token need to be score\",\"state – previous state\",\"x – encoded feature\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\"]},\"1801\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.NoAtt\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.attentions.NoAtt\",\"Bases: Module\",\"No attention.\",\"Initialize No Attention.\",\"forward(enc_hs_pad, enc_hs_len, dec_z, att_prev)\",\"Forward NoAtt.\",\"Parameters:\",\"enc_hs_pad (torch.Tensor) – padded encoder hidden state (B, T_max, D_enc)\",\"enc_hs_len (list) – padded encoder hidden state length (B)\",\"dec_z (torch.Tensor) – dummy (does not use)\",\"att_prev (torch.Tensor) – dummy (does not use)\",\"Returns: attention weighted encoder state (B, D_enc)\",\"Return type: torch.Tensor\",\"Returns: previous attention weights\",\"Return type: torch.Tensor\",\"reset()\",\"Reset states.\"]},\"1802\":{\"h\":\"espnet2.legacy.transform.perturb.NoiseInjection\",\"t\":[\"source\",\"class espnet2.legacy.transform.perturb.NoiseInjection(utt2noise=None, lower=-20, upper=-5, utt2ratio=None, filetype='list', dbunit=True, seed=None)\",\"Bases: object\",\"Add isotropic noise.\",\"Initialize class.\"]},\"1803\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.wavenet.OneHot\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.wavenet.OneHot(depth)\",\"Bases: Module\",\"Convert to one-hot vector.\",\"Parameters:depth (int) – Dimension of one-hot vector.\",\"Initialize OneHot class.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (LongTensor) – long tensor variable with the shape (B, T)\",\"Returns: float tensor variable with the shape (B, depth, T)\",\"Return type: Tensor\"]},\"1804\":{\"h\":\"espnet2.legacy.nets.scorer_interface.PartialScorerInterface\",\"t\":[\"source\",\"class espnet2.legacy.nets.scorer_interface.PartialScorerInterface\",\"Bases: ScorerInterface\",\"Partial scorer interface for beam search.\",\"The partial scorer performs scoring when non-partial scorer finished scoring, and receives pre-pruned next tokens to score because it is too heavy to score all the tokens.\"]},\"1805\":{\"h\":\"Examples\",\"t\":[\"Prefix search for connectionist-temporal-classification models : * espnet2.legacy.nets.scorers.ctc.CTCPrefixScorer\",\"score_partial(y: Tensor, next_tokens: Tensor, state: Any, x: Tensor) → Tuple[Tensor, Any]\",\"Score new token (required).\",\"Parameters:\",\"y (torch.Tensor) – 1D prefix token\",\"next_tokens (torch.Tensor) – torch.int64 next token to score\",\"state – decoder state for prefix tokens\",\"x (torch.Tensor) – The encoder feature that generates ys\",\"Returns: Tuple of a score tensor for y that has a shape (len(next_tokens),) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\"]},\"1806\":{\"h\":\"espnet2.legacy.nets.beam_search_partially_AR.PartiallyARBeamSearch\",\"t\":[\"source\",\"class espnet2.legacy.nets.beam_search_partially_AR.PartiallyARBeamSearch(*args, **kwargs)\",\"Bases: BatchBeamSearch\",\"Partially autoregressive beam search implementation.\",\"Partially autoregressive hypothesis is a set of BatchHypothesis.\",\"We need to use add_mask function to add a hypothesis for a mask. Before search and beam search method, each partially autoregressive hypothesis is extracted to BatchHypothesis, and applied the same process as the batched_beam_search.\",\"Initialize method.\",\"add_mask(primer: List[int], eos: int)\",\"Add a mask to a batch of hypotheses.\",\"Parameters:primer (torch.Tensor) – Primer yseq.\",\"batch_beam(weighted_scores: Tensor) → Tuple[Tensor, Tensor]\",\"Batch-compute topk full token ids and partial token ids.\",\"Parameters:weighted_scores (torch.Tensor) – The weighted sum scores for each tokens. Its shape is (n_beam, self.vocab_size).\",\"Returns: The topk full (prev_hyp, new_token) ids and partial (prev_hyp, new_token) ids. Their shapes are all (self.beam_size,)\",\"Return type: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\",\"forward(x: Tensor, max_seq_len: int | None = None) → List[Hypothesis]\",\"Perform beam search.\",\"Parameters:\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"maxlenratio (float) – Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths If maxlenratio<0.0, its absolute value is interpreted as a constant max output length.\",\"minlenratio (float) – Input length ratio to obtain min output length.\",\"Returns: N-best decoding results\",\"Return type: list[Hypothesis]\",\"init_hyp(x: Tensor) → PartiallyARHypothesis\",\"Get an initial hypothesis data for each mask.\",\"Parameters:x (torch.Tensor) – The encoder output feature\",\"Returns: The initial hypothesis.\",\"Return type:PartiallyARHypothesis\",\"init_masks()\",\"Initialize masks.\",\"post_process(i: int, maxlen: int, running_hyps: PartiallyARHypothesis, ended_hyps: List[List[Hypothesis]]) → BatchHypothesis\",\"Perform post-processing of beam search iterations.\",\"Extract BatchHypothesis for each mask, and perform post-process. Then merge BatchHypothesis.\",\"Parameters:\",\"i (int) – The length of hypothesis tokens.\",\"maxlen (int) – The maximum length of tokens in beam search.\",\"maxlenratio (int) – The maximum length ratio in beam search.\",\"running_hyps (BatchHypothesis) – The running hypotheses in beam search.\",\"ended_hyps (List[Hypothesis]) – The ended hypotheses in beam search.\",\"Returns: The new running hypotheses.\",\"Return type:BatchHypothesis\",\"score_full(hyp: PartiallyARHypothesis, x: Tensor, is_first: bool = False) → Tuple[Dict[str, Tensor], Dict[str, Any]]\",\"Score new hypothesis by self.full_scorers.\",\"Parameters:\",\"hyp (PartiallyARHypothesis) – Hypothesis with prefix tokens to score\",\"x (torch.Tensor) – Corresponding input feature\",\"Returns: Tuple of : score dict of hyp that has string keys of self.full_scorers and tensor score values of shape: (self.n_vocab,), and state dict that has string keys and state values of self.full_scorers\",\"Return type: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]\",\"search(running_hyps: PartiallyARHypothesis, x: Tensor) → PartiallyARHypothesis\",\"Search new tokens for running hypotheses and encoded speech x.\",\"Parameters:\",\"running_hyps (BatchHypothesis) – Running hypotheses on beam\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"Returns: Best sorted hypotheses\",\"Return type:BatchHypothesis\"]},\"1807\":{\"h\":\"espnet2.legacy.nets.beam_search_partially_AR.PartiallyARHypothesis\",\"t\":[\"source\",\"class espnet2.legacy.nets.beam_search_partially_AR.PartiallyARHypothesis(yseq: Tensor, score: float | Tensor | None = None, states: Dict[str, Any] = {}, yseq_length: Tensor = tensor([]), eos: Tensor = tensor([]))\",\"Bases: NamedTuple\",\"Hypothesis data type for partially autoregressive decoding.\",\"Create new instance of PartiallyARHypothesis(yseq, score, states, yseq_length, eos)\",\"asdict() → dict\",\"Convert data to JSON-friendly dict.\",\"eos : Tensor\",\"Alias for field number 4\",\"score : float | Tensor\",\"Alias for field number 1\",\"states : Dict[str, Any]\",\"Alias for field number 2\",\"yseq : Tensor\",\"Alias for field number 0\",\"yseq_length : Tensor\",\"Alias for field number 3\"]},\"1808\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding(d_model, dropout_rate, max_len=5000, reverse=False)\",\"Bases: Module\",\"Positional encoding.\",\"Parameters:\",\"d_model (int) – Embedding dimension.\",\"dropout_rate (float) – Dropout rate.\",\"max_len (int) – Maximum input length.\",\"reverse (bool) – Whether to reverse the input position. Only for\",\"current (the class LegacyRelPositionalEncoding. We remove it in the)\",\"RelPositionalEncoding. (class)\",\"Construct an PositionalEncoding object.\",\"extend_pe(x)\",\"Reset the positional encodings.\",\"forward(x: Tensor)\",\"Add positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, time, *).\",\"Returns: Encoded tensor (batch, time, *).\",\"Return type: torch.Tensor\"]},\"1809\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.positionwise_feed_forward.PositionwiseFeedForward\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.positionwise_feed_forward.PositionwiseFeedForward(idim, hidden_units, dropout_rate, activation=ReLU())\",\"Bases: Module\",\"Positionwise feed forward layer.\",\"Parameters:\",\"idim (int) – Input dimenstion.\",\"hidden_units (int) – The number of hidden units.\",\"dropout_rate (float) – Dropout rate.\",\"Construct an PositionwiseFeedForward object.\",\"forward(x)\",\"Forward function.\"]},\"1810\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.tacotron2.decoder.Postnet\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.tacotron2.decoder.Postnet(idim, odim, n_layers=5, n_chans=512, n_filts=5, dropout_rate=0.5, use_batch_norm=True)\",\"Bases: Module\",\"Postnet module for Spectrogram prediction network.\",\"This is a module of Postnet in Spectrogram prediction network, which described in Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. The Postnet predicts refines the predicted Mel-filterbank of the decoder, which helps to compensate the detail structure of spectrogram.\",\"Initialize postnet module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"n_layers (int,optional) – The number of layers.\",\"n_filts (int,optional) – The number of filter size.\",\"n_units (int,optional) – The number of filter channels.\",\"use_batch_norm (bool,optional) – Whether to use batch normalization..\",\"dropout_rate (float,optional) – Dropout rate..\",\"forward(xs)\",\"Calculate forward propagation.\",\"Parameters:xs (Tensor) – Batch of the sequences of padded input tensors (B, idim, Tmax).\",\"Returns: Batch of padded output tensor. (B, odim, Tmax).\",\"Return type: Tensor\"]},\"1811\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.tacotron2.decoder.Prenet\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.tacotron2.decoder.Prenet(idim, n_layers=2, n_units=256, dropout_rate=0.5)\",\"Bases: Module\",\"Prenet module for decoder of Spectrogram prediction network.\",\"This is a module of Prenet in the decoder of Spectrogram prediction network, which described in Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. The Prenet preforms nonlinear conversion of inputs before input to auto-regressive lstm, which helps to learn diagonal attentions.\"]},\"1812\":{\"h\":\"NOTE\",\"t\":[\"This module alway applies dropout even in evaluation. See the detail in Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.\",\"Initialize prenet module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"n_layers (int,optional) – The number of prenet layers.\",\"n_units (int,optional) – The number of prenet units.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Batch of input tensors (B, …, idim).\",\"Returns: Batch of output tensors (B, …, odim).\",\"Return type: Tensor\"]},\"1813\":{\"h\":\"espnet2.legacy.transform.perturb.RIRConvolve\",\"t\":[\"source\",\"class espnet2.legacy.transform.perturb.RIRConvolve(utt2rir, filetype='list')\",\"Bases: object\",\"RIR Convolve class.\",\"Initialize class.\"]},\"1814\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.encoders.RNN\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.encoders.RNN(idim, elayers, cdim, hdim, dropout, typ='blstm')\",\"Bases: Module\",\"RNN module.\",\"Parameters:\",\"idim (int) – dimension of inputs\",\"elayers (int) – number of encoder layers\",\"cdim (int) – number of rnn units (resulted in cdim * 2 if bidirectional)\",\"hdim (int) – number of final projection units\",\"dropout (float) – dropout rate\",\"typ (str) – The RNN type\",\"Initialize RNN.\",\"forward(xs_pad, ilens, prev_state=None)\",\"Forward RNN.\",\"Parameters:\",\"xs_pad (torch.Tensor) – batch of padded input sequences (B, Tmax, D)\",\"ilens (torch.Tensor) – batch of lengths of input sequences (B)\",\"prev_state (torch.Tensor) – batch of previous RNN states\",\"Returns: batch of hidden state sequences (B, Tmax, eprojs)\",\"Return type: torch.Tensor\"]},\"1815\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.rnn_decoder.RNNDecoder\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transducer.rnn_decoder.RNNDecoder(odim: int, dtype: str, dlayers: int, dunits: int, embed_dim: int, dropout_rate: float = 0.0, dropout_rate_embed: float = 0.0, blank_id: int = 0)\",\"Bases: TransducerDecoderInterface, Module\",\"RNN decoder module for Transducer model.\",\"Parameters:\",\"odim – Output dimension.\",\"dtype – Decoder units type.\",\"dlayers – Number of decoder layers.\",\"dunits – Number of decoder units per layer..\",\"embed_dim – Embedding layer dimension.\",\"dropout_rate – Dropout rate for decoder layers.\",\"dropout_rate_embed – Dropout rate for embedding layer.\",\"blank_id – Blank symbol ID.\",\"Transducer initializer.\",\"batch_score(hyps: List[Hypothesis] | List[ExtendedHypothesis], dec_states: Tuple[Tensor, Tensor | None], cache: Dict[str, Any], use_lm: bool) → Tuple[Tensor, Tuple[Tensor, Tensor], Tensor]\",\"One-step forward hypotheses.\",\"Parameters:\",\"hyps – Hypotheses.\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"cache – Pairs of (dec_out, dec_states) for each label sequences. (keys)\",\"use_lm – Whether to compute label ID sequences for LM.\",\"Returns: Decoder output sequences. (B, D_dec) dec_states: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec)) lm_labels: Label ID sequences for LM. (B,)\",\"Return type: dec_out\",\"create_batch_states(states: Tuple[Tensor, Tensor | None], new_states: List[Tuple[Tensor, Tensor | None]], check_list: List | None = None) → List[Tuple[Tensor, Tensor | None]]\",\"Create decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"new_states – Decoder hidden states. [N x ((1, D_dec), (1, D_dec))]\",\"Returns: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"Return type: states\",\"forward(labels: Tensor) → Tensor\",\"Encode source label sequences.\",\"Parameters:labels – Label ID sequences. (B, L)\",\"Returns: Decoder output sequences. (B, T, U, D_dec)\",\"Return type: dec_out\",\"init_state(batch_size: int) → Tuple[Tensor, tensor | None]\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Initial decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"rnn_forward(sequence: Tensor, state: Tuple[Tensor, Tensor | None]) → Tuple[Tensor, Tuple[Tensor, Tensor | None]]\",\"Encode source label sequences.\",\"Parameters:\",\"sequence – RNN input sequences. (B, D_emb)\",\"state – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"Returns: RNN output sequences. (B, D_dec) (h_next, c_next): Decoder hidden states. (N, B, D_dec), (N, B, D_dec))\",\"Return type: sequence\",\"score(hyp: Hypothesis, cache: Dict[str, Any]) → Tuple[Tensor, Tuple[Tensor, Tensor | None], Tensor]\",\"One-step forward hypothesis.\",\"Parameters:\",\"hyp – Hypothesis.\",\"cache – Pairs of (dec_out, state) for each label sequence. (key)\",\"Returns: Decoder output sequence. (1, D_dec) new_state: Decoder hidden states. ((N, 1, D_dec), (N, 1, D_dec)) label: Label ID for LM. (1,)\",\"Return type: dec_out\",\"select_state(states: Tuple[Tensor, Tensor | None], idx: int) → Tuple[Tensor, Tensor | None]\",\"Get specified ID state from decoder hidden states.\",\"Parameters:\",\"states – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID. : ((N, 1, D_dec), (N, 1, D_dec))\",\"set_device(device: device)\",\"Set GPU device to use.\",\"Parameters:device – Device ID.\"]},\"1816\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.encoders.RNNP\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.rnn.encoders.RNNP(idim, elayers, cdim, hdim, subsample, dropout, typ='blstm')\",\"Bases: Module\",\"RNN with projection layer module.\",\"Parameters:\",\"idim (int) – dimension of inputs\",\"elayers (int) – number of encoder layers\",\"cdim (int) – number of rnn units (resulted in cdim * 2 if bidirectional)\",\"hdim (int) – number of projection units\",\"subsample (np.ndarray) – list of subsampling numbers\",\"dropout (float) – dropout rate\",\"typ (str) – The RNN type\",\"Initialize RNNP.\",\"forward(xs_pad, ilens, prev_state=None)\",\"Calculate RNNP forward propagation.\",\"Parameters:\",\"xs_pad (torch.Tensor) – batch of padded input sequences (B, Tmax, idim)\",\"ilens (torch.Tensor) – batch of lengths of input sequences (B)\",\"prev_state (torch.Tensor) – batch of previous RNN states\",\"Returns: batch of hidden state sequences (B, Tmax, hdim)\",\"Return type: torch.Tensor\"]},\"1817\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.attention.RelPositionMultiHeadedAttention\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.attention.RelPositionMultiHeadedAttention(n_head, n_feat, dropout_rate, zero_triu=False)\",\"Bases: MultiHeadedAttention\",\"Multi-Head Attention layer with relative position encoding (new implementation).\",\"Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"Paper: https://arxiv.org/abs/1901.02860\",\"Parameters:\",\"n_head (int) – The number of heads.\",\"n_feat (int) – The number of features.\",\"dropout_rate (float) – Dropout rate.\",\"zero_triu (bool) – Whether to zero the upper triangular part of attention matrix.\",\"Construct an RelPositionMultiHeadedAttention object.\",\"forward(query, key, value, pos_emb, mask)\",\"Compute ‘Scaled Dot Product Attention’ with rel. positional encoding.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"pos_emb (torch.Tensor) – Positional embedding tensor (#batch, 2*time1-1, size).\",\"mask (torch.Tensor) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).\",\"Returns: Output tensor (#batch, time1, d_model).\",\"Return type: torch.Tensor\",\"rel_shift(x)\",\"Compute relative positional encoding.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor (batch, head, time1, 2*time1-1).\",\"vector. (time1 means the lengthofquery)\",\"Returns: Output tensor.\",\"Return type: torch.Tensor\"]},\"1818\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.embedding.RelPositionalEncoding\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.embedding.RelPositionalEncoding(d_model, dropout_rate, max_len=5000)\",\"Bases: Module\",\"Relative positional encoding module (new implementation).\",\"Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"See : Appendix B in https://arxiv.org/abs/1901.02860\",\"Parameters:\",\"d_model (int) – Embedding dimension.\",\"dropout_rate (float) – Dropout rate.\",\"max_len (int) – Maximum input length.\",\"Construct an PositionalEncoding object.\",\"extend_pe(x)\",\"Reset the positional encodings.\",\"forward(x: Tensor)\",\"Add positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, time, *).\",\"Returns: Encoded tensor (batch, time, *).\",\"Return type: torch.Tensor\"]},\"1819\":{\"h\":\"espnet2.legacy.utils.dummy_chainer.Reporter\",\"t\":[\"source\",\"class espnet2.legacy.utils.dummy_chainer.Reporter\",\"Bases: object\",\"A dummy chainer reporter wrapper.\",\"report(*args, **kwargs)\",\"Report at every step.\"]},\"1820\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.embedding.ScaledPositionalEncoding\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.embedding.ScaledPositionalEncoding(d_model, dropout_rate, max_len=5000)\",\"Bases: PositionalEncoding\",\"Scaled positional encoding module.\",\"See Sec. 3.2 https://arxiv.org/abs/1809.08895\",\"Parameters:\",\"d_model (int) – Embedding dimension.\",\"dropout_rate (float) – Dropout rate.\",\"max_len (int) – Maximum input length.\",\"Initialize class.\",\"forward(x)\",\"Add positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, time, *).\",\"Returns: Encoded tensor (batch, time, *).\",\"Return type: torch.Tensor\",\"reset_parameters()\",\"Reset parameters.\"]},\"1821\":{\"h\":\"espnet2.legacy.nets.scorer_interface.ScorerInterface\",\"t\":[\"source\",\"class espnet2.legacy.nets.scorer_interface.ScorerInterface\",\"Bases: object\",\"Scorer interface for beam search.\",\"The scorer performs scoring of the all tokens in vocabulary.\"]},\"1822\":{\"h\":\"Examples\",\"t\":[\"Search heuristics: * espnet2.legacy.nets.scorers.length_bonus.LengthBonus\",\"Decoder networks of the sequence-to-sequence models: : * espnet2.legacy.nets.pytorch_backend.nets.transformer.decoder.Decoder\",\"espnet2.legacy.nets.pytorch_backend.nets.rnn.decoders.Decoder\",\"Neural language models: : * espnet2.legacy.nets.pytorch_backend.lm.transformer.TransformerLM\",\"espnet2.legacy.nets.pytorch_backend.lm.default.DefaultRNNLM\",\"espnet2.legacy.nets.pytorch_backend.lm.seq_rnn.SequentialRNNLM\",\"final_score(state: Any) → float\",\"Score eos (optional).\",\"Parameters:state – Scorer state for prefix tokens\",\"Returns: final score\",\"Return type: float\",\"init_state(x: Tensor) → Any\",\"Get an initial state for decoding (optional).\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"score(y: Tensor, state: Any, x: Tensor) → Tuple[Tensor, Any]\",\"Score new token (required).\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – The encoder feature that generates ys.\",\"Returns: Tuple of : scores for next token that has a shape of (n_vocab) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\",\"select_state(state: Any, i: int, new_id: int | None = None) → Any\",\"Select state with relative ids in the main beam search.\",\"Parameters:\",\"state – Decoder state for prefix tokens\",\"i (int) – Index to select a state in the main beam search\",\"new_id (int) – New label index to select a state if necessary\",\"Returns: pruned state\",\"Return type: state\"]},\"1823\":{\"h\":\"espnet2.legacy.utils.dummy_chainer.SerialIterator\",\"t\":[\"source\",\"class espnet2.legacy.utils.dummy_chainer.SerialIterator\",\"Bases: object\",\"A dummy SerialIterator wrapper.\",\"start_shuffle(*args, **kwargs)\",\"Report at every step.\"]},\"1824\":{\"h\":\"espnet2.legacy.utils.io_utils.SoundHDF5File\",\"t\":[\"source\",\"class espnet2.legacy.utils.io_utils.SoundHDF5File(filepath, mode='r+', format=None, dtype='int16', **kwargs)\",\"Bases: object\",\"Collecting sound files to a HDF5 file.\",\">>> f = SoundHDF5File('a.flac.h5', mode='a') >>> array = np.random.randint(0, 100, 100, dtype=np.int16) >>> f['id'] = (array, 16000) >>> array, rate = f['id']\",\"Param: str filepath:\",\"Param: str mode:\",\"Param: str format: The type used when saving wav. flac, nist, htk, etc.\",\"Param: str dtype:\",\"Initialize Sound HDF5 File.\",\"close()\",\"Close file.\",\"create_dataset(name, shape=None, data=None, **kwds)\",\"Create Dataset.\",\"items()\",\"Return keys and values of file key.\",\"keys()\",\"Return keys of files.\",\"values()\",\"Return values of file key.\"]},\"1825\":{\"h\":\"espnet2.legacy.utils.cli_readers.SoundHDF5Reader\",\"t\":[\"source\",\"class espnet2.legacy.utils.cli_readers.SoundHDF5Reader(rspecifier, return_shape=False)\",\"Bases: object\",\"Sound HDF5 Reader class.\",\"Initialize Sound HDF5 Reader.\"]},\"1826\":{\"h\":\"espnet2.legacy.utils.cli_writers.SoundHDF5Writer\",\"t\":[\"source\",\"class espnet2.legacy.utils.cli_writers.SoundHDF5Writer(wspecifier, write_num_frames=None, pcm_format='wav')\",\"Bases: BaseWriter\",\"SoundHDF5Writer Class.\"]},\"1827\":{\"h\":\"Examples\",\"t\":[\">>> fs = 16000 >>> with SoundHDF5Writer('ark:out.h5') as f: ... f['key'] = fs, array\",\"Initialize Sound HDF5 Writer.\"]},\"1828\":{\"h\":\"espnet2.legacy.utils.cli_readers.SoundReader\",\"t\":[\"source\",\"class espnet2.legacy.utils.cli_readers.SoundReader(rspecifier, return_shape=False)\",\"Bases: object\",\"Sound Reader Class.\",\"Initialize Sound Reader.\"]},\"1829\":{\"h\":\"espnet2.legacy.utils.cli_writers.SoundWriter\",\"t\":[\"source\",\"class espnet2.legacy.utils.cli_writers.SoundWriter(wspecifier, write_num_frames=None, pcm_format='wav')\",\"Bases: BaseWriter\",\"SoundWriter class.\"]},\"1830\":{\"h\":\"Examples\",\"t\":[\">>> fs = 16000 >>> with SoundWriter('ark,scp:outdir,out.scp') as f: ... f['key'] = fs, array\",\"Initialize Sound writer.\"]},\"1831\":{\"h\":\"espnet2.legacy.transform.spec_augment.SpecAugment\",\"t\":[\"source\"]},\"1832\":{\"h\":\"espnet2.legacy.transform.spectrogram.Spectrogram\",\"t\":[\"source\",\"class espnet2.legacy.transform.spectrogram.Spectrogram(n_fft, n_shift, win_length=None, window='hann')\",\"Bases: object\",\"Spectrogram class.\",\"Initialize Spectrogram.\"]},\"1833\":{\"h\":\"espnet2.legacy.transform.perturb.SpeedPerturbation\",\"t\":[\"source\",\"class espnet2.legacy.transform.perturb.SpeedPerturbation(lower=0.9, upper=1.1, utt2ratio=None, keep_length=True, res_type='kaiser_best', seed=None)\",\"Bases: object\",\"SpeedPerturbation class.\",\"The speed perturbation in kaldi uses sox-speed instead of sox-tempo, and sox-speed just to resample the input, i.e pitch and tempo are changed both.\",\"“Why use speed option instead of tempo -s in SoX for speed perturbation” https://groups.google.com/forum/#!topic/kaldi-help/8OOG7eE4sZ8\",\"WARNING\",\"This function is very slow because of resampling. I recommmend to apply speed-perturb outside the training using sox.\",\"Initialize class.\"]},\"1834\":{\"h\":\"espnet2.legacy.utils.dummy_chainer.StandardUpdater\",\"t\":[\"source\",\"class espnet2.legacy.utils.dummy_chainer.StandardUpdater(*args, **kwargs)\",\"Bases: object\",\"A dummy StandardUpdater wrapper.\",\"Initliaze Dummy StandardUpdater.\",\"update_core(*args, **kwargs)\",\"Update at every step.\"]},\"1835\":{\"h\":\"espnet2.legacy.transform.spectrogram.Stft\",\"t\":[\"source\",\"class espnet2.legacy.transform.spectrogram.Stft(n_fft, n_shift, win_length=None, window='hann', center=True, pad_mode='reflect')\",\"Bases: object\",\"STFT Class.\",\"Initialize Class.\"]},\"1836\":{\"h\":\"espnet2.legacy.transform.spectrogram.Stft2LogMelSpectrogram\",\"t\":[\"source\",\"class espnet2.legacy.transform.spectrogram.Stft2LogMelSpectrogram(fs, n_mels, n_fft, fmin=None, fmax=None, eps=1e-10)\",\"Bases: object\",\"STFT to LogMel Spectrogram Class.\",\"Initialize Class.\"]},\"1837\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding(d_model, dropout_rate, max_len=5000)\",\"Bases: Module\",\"Streaming Positional encoding.\",\"Parameters:\",\"d_model (int) – Embedding dimension.\",\"dropout_rate (float) – Dropout rate.\",\"max_len (int) – Maximum input length.\",\"Construct an PositionalEncoding object.\",\"extend_pe(length, device, dtype)\",\"Reset the positional encodings.\",\"forward(x: Tensor, start_idx: int = 0)\",\"Add positional encoding.\",\"Parameters:x (torch.Tensor) – Input tensor (batch, time, *).\",\"Returns: Encoded tensor (batch, time, *).\",\"Return type: torch.Tensor\"]},\"1838\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.conformer.swish.Swish\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.conformer.swish.Swish(*args, **kwargs)\",\"Bases: Module\",\"Construct an Swish object.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Return Swich activation function.\"]},\"1839\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2Loss\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2Loss(use_masking=True, use_weighted_masking=False, bce_pos_weight=20.0)\",\"Bases: Module\",\"Loss function module for Tacotron2.\",\"Initialize Tactoron2 loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"bce_pos_weight (float) – Weight of positive sample of stop token.\",\"forward(after_outs, before_outs, logits, ys, labels, olens)\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, Lmax, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, Lmax, odim).\",\"logits (Tensor) – Batch of stop logits (B, Lmax).\",\"ys (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"labels (LongTensor) – Batch of the sequences of stop token labels (B, Lmax).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: L1 loss value. Tensor: Mean square error loss value. Tensor: Binary cross entropy loss value.\",\"Return type: Tensor\"]},\"1840\":{\"h\":\"espnet2.legacy.transform.spec_augment.TimeMask\",\"t\":[\"source\"]},\"1841\":{\"h\":\"espnet2.legacy.transform.spec_augment.TimeWarp\",\"t\":[\"source\"]},\"1842\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.subsampling.TooShortUttError\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transformer.subsampling.TooShortUttError(message, actual_size, limit)\",\"Bases: Exception\",\"Raised when the utt is too short for subsampling.\",\"Parameters:\",\"message (str) – Message for error catch\",\"actual_size (int) – the short size that cannot pass the subsampling\",\"limit (int) – the limit size for subsampling\",\"Construct a TooShortUttError for error handler.\"]},\"1843\":{\"h\":\"espnet2.legacy.nets.transducer_decoder_interface.TransducerDecoderInterface\",\"t\":[\"source\",\"class espnet2.legacy.nets.transducer_decoder_interface.TransducerDecoderInterface\",\"Bases: object\",\"Decoder interface for Transducer models.\",\"batch_score(hyps: List[Hypothesis] | List[ExtendedHypothesis], dec_states: Tuple[Tensor, Tensor | None] | List[Tensor | None], cache: Dict[str, Any], use_lm: bool) → Tuple[Tensor, Tuple[Tensor, Tensor | None] | List[Tensor | None], Tensor]\",\"One-step forward hypotheses.\",\"Parameters:\",\"hyps – Hypotheses.\",\"dec_states – Decoder hidden states.\",\"cache – Pairs of (dec_out, dec_states) for each label sequence. (key)\",\"use_lm – Whether to compute label ID sequences for LM.\",\"Returns: Decoder output sequences. dec_states: Decoder hidden states. lm_labels: Label ID sequences for LM.\",\"Return type: dec_out\",\"create_batch_states(states: Tuple[Tensor, Tensor | None] | List[Tensor | None], new_states: List[Tuple[Tensor, Tensor | None] | List[Tensor | None]], l_tokens: List[List[int]]) → Tuple[Tensor, Tensor | None] | List[Tensor | None]\",\"Create decoder hidden states.\",\"Parameters:\",\"batch_states – Batch of decoder states\",\"l_states – List of decoder states\",\"l_tokens – List of token sequences for input batch\",\"Returns: Batch of decoder states\",\"Return type: batch_states\",\"init_state(batch_size: int) → Tuple[Tensor, Tensor | None] | List[Tensor | None]\",\"Initialize decoder states.\",\"Parameters:batch_size – Batch size.\",\"Returns: Initial decoder hidden states.\",\"Return type: state\",\"score(hyp: Hypothesis, cache: Dict[str, Any]) → Tuple[Tensor, Tuple[Tensor, Tensor | None] | List[Tensor | None], Tensor]\",\"One-step forward hypothesis.\",\"Parameters:\",\"hyp – Hypothesis.\",\"cache – Pairs of (dec_out, dec_state) for each token sequence. (key)\",\"Returns: Decoder output sequence. new_state: Decoder hidden states. lm_tokens: Label ID for LM.\",\"Return type: dec_out\",\"select_state(batch_states: Tuple[Tensor, Tensor | None] | List[Tensor], idx: int) → Tuple[Tensor, Tensor | None] | List[Tensor | None]\",\"Get specified ID state from decoder hidden states.\",\"Parameters:\",\"batch_states – Decoder hidden states.\",\"idx – State ID to extract.\",\"Returns: Decoder hidden state for given ID.\",\"Return type: state_idx\"]},\"1844\":{\"h\":\"espnet2.legacy.transform.transform_interface.TransformInterface\",\"t\":[\"source\",\"class espnet2.legacy.transform.transform_interface.TransformInterface\",\"Bases: object\",\"Transform Interface.\",\"classmethod add_arguments(parser)\",\"Add arguments to parser.\"]},\"1845\":{\"h\":\"espnet2.legacy.transform.transformation.Transformation\",\"t\":[\"source\",\"class espnet2.legacy.transform.transformation.Transformation(conffile=None)\",\"Bases: object\",\"Apply some functions to the mini-batch.\"]},\"1846\":{\"h\":\"Examples\",\"t\":[\">>> kwargs = {\\\"process\\\": [{\\\"type\\\": \\\"fbank\\\", ... \\\"n_mels\\\": 80, ... \\\"fs\\\": 16000}, ... {\\\"type\\\": \\\"cmvn\\\", ... \\\"stats\\\": \\\"data/train/cmvn.ark\\\", ... \\\"norm_vars\\\": True}, ... {\\\"type\\\": \\\"delta\\\", \\\"window\\\": 2, \\\"order\\\": 2}]} >>> transform = Transformation(kwargs) >>> bs = 10 >>> xs = [np.random.randn(100, 80).astype(np.float32) ... for _ in range(bs)] >>> xs = transform(xs)\",\"Initialize Transformation.\"]},\"1847\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.transformer_decoder_layer.TransformerDecoderLayer\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transducer.transformer_decoder_layer.TransformerDecoderLayer(hdim: int, self_attention: MultiHeadedAttention, feed_forward: PositionwiseFeedForward, dropout_rate: float)\",\"Bases: Module\",\"Transformer decoder layer module for custom Transducer model.\",\"Parameters:\",\"hdim – Hidden dimension.\",\"self_attention – Self-attention module.\",\"feed_forward – Feed forward module.\",\"dropout_rate – Dropout rate.\",\"Construct an DecoderLayer object.\",\"forward(sequence: Tensor, mask: Tensor, cache: Tensor | None = None)\",\"Compute previous decoder output sequences.\",\"Parameters:\",\"sequence – Transformer input sequences. (B, U, D_dec)\",\"mask – Transformer intput mask sequences. (B, U)\",\"cache – Cached decoder output sequences. (B, (U - 1), D_dec)\",\"Returns: Transformer output sequences. (B, U, D_dec) mask: Transformer output mask sequences. (B, U)\",\"Return type: sequence\"]},\"1848\":{\"h\":\"espnet2.legacy.nets.scorers.uasr.UASRPrefixScorer\",\"t\":[\"source\",\"class espnet2.legacy.nets.scorers.uasr.UASRPrefixScorer(eos: int)\",\"Bases: CTCPrefixScorer\",\"Decoder interface wrapper for CTCPrefixScore.\",\"Initialize class.\",\"batch_init_state(x: Tensor)\",\"Get an initial state for decoding.\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"init_state(x: Tensor)\",\"Get an initial state for decoding.\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\"]},\"1849\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.wavenet.UpSampling\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.wavenet.UpSampling(upsampling_factor, bias=True)\",\"Bases: Module\",\"Upsampling layer with deconvolution.\",\"Parameters:upsampling_factor (int) – Upsampling factor.\",\"Initialize Upsampling class.\",\"forward(x)\",\"Calculate forward propagation.\",\"Parameters:x (Tensor) – Input tensor with the shape (B, C, T)\",\"Returns: Tensor with the shape (B, C, T’) where T’ = T * upsampling_factor.\",\"Return type: Tensor\"]},\"1850\":{\"h\":\"espnet2.legacy.transform.cmvn.UtteranceCMVN\",\"t\":[\"source\",\"class espnet2.legacy.transform.cmvn.UtteranceCMVN(norm_means=True, norm_vars=False, std_floor=1e-20)\",\"Bases: object\",\"Utterance CMVN class.\",\"Initialize class.\"]},\"1851\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.vgg2l.VGG2L\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.transducer.vgg2l.VGG2L(idim: int, odim: int, pos_enc: Module | None = None)\",\"Bases: Module\",\"VGG2L module for custom encoder.\",\"Parameters:\",\"idim – Input dimension.\",\"odim – Output dimension.\",\"pos_enc – Positional encoding class.\",\"Construct a VGG2L object.\",\"create_new_mask(feats_mask: Tensor) → Tensor\",\"Create a subsampled mask of feature sequences.\",\"Parameters:feats_mask – Mask of feature sequences. (B, 1, F)\",\"Returns: Mask of VGG2L output sequences. (B, 1, sub(F))\",\"Return type: vgg_mask\",\"forward(feats: Tensor, feats_mask: Tensor) → Tuple[Tensor, Tensor] | Tuple[Tuple[Tensor, Tensor], Tensor]\",\"Forward VGG2L bottleneck.\",\"Parameters:\",\"feats – Feature sequences. (B, F, D_feats)\",\"feats_mask – Mask of feature sequences. (B, 1, F)\",\"Returns: VGG output sequences. : (B, sub(F), D_out) or ((B, sub(F), D_out), (B, sub(F), D_att))\",\"vgg_mask: Mask of VGG output sequences. (B, 1, sub(F))\",\"Return type: vgg_output\"]},\"1852\":{\"h\":\"espnet2.legacy.transform.perturb.VolumePerturbation\",\"t\":[\"source\",\"class espnet2.legacy.transform.perturb.VolumePerturbation(lower=-1.6, upper=1.6, utt2ratio=None, dbunit=True, seed=None)\",\"Bases: object\",\"Volume Perturbation class.\",\"Initialize class.\"]},\"1853\":{\"h\":\"espnet2.legacy.transform.wpe.WPE\",\"t\":[\"source\",\"class espnet2.legacy.transform.wpe.WPE(taps=10, delay=3, iterations=3, psd_context=0, statistics_mode='full')\",\"Bases: object\",\"WPE Class.\",\"Initialize WPE.\"]},\"1854\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.wavenet.WaveNet\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.wavenet.WaveNet(n_quantize=256, n_aux=28, n_resch=512, n_skipch=256, dilation_depth=10, dilation_repeat=3, kernel_size=2, upsampling_factor=0)\",\"Bases: Module\",\"Conditional wavenet.\",\"Parameters:\",\"n_quantize (int) – Number of quantization.\",\"n_aux (int) – Number of aux feature dimension.\",\"n_resch (int) – Number of filter channels for residual block.\",\"n_skipch (int) – Number of filter channels for skip connection.\",\"dilation_depth (int) – Number of dilation depth (e.g. if set 10, max dilation = 2^(10-1)).\",\"dilation_repeat (int) – Number of dilation repeat.\",\"kernel_size (int) – Filter size of dilated causal convolution.\",\"upsampling_factor (int) – Upsampling factor.\",\"Initialize WaveNet class.\",\"forward(x, h)\",\"Calculate forward propagation.\",\"Parameters:\",\"x (LongTensor) – Quantized input waveform tensor with the shape (B, T).\",\"h (Tensor) – Auxiliary feature tensor with the shape (B, n_aux, T).\",\"Returns: Logits with the shape (B, T, n_quantize).\",\"Return type: Tensor\",\"generate(x, h, n_samples, interval=None, mode='sampling')\",\"Generate a waveform with fast genration algorithm.\",\"This generation based on Fast WaveNet Generation Algorithm.\",\"Parameters:\",\"x (LongTensor) – Initial waveform tensor with the shape (T,).\",\"h (Tensor) – Auxiliary feature tensor with the shape (n_samples + T, n_aux).\",\"n_samples (int) – Number of samples to be generated.\",\"interval (int,optional) – Log interval.\",\"mode (str,optional) – “sampling” or “argmax”.\",\"Returns: Generated quantized waveform (n_samples).\",\"Return type: ndarray\"]},\"1855\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.tacotron2.decoder.ZoneOutCell\",\"t\":[\"source\",\"class espnet2.legacy.nets.pytorch_backend.tacotron2.decoder.ZoneOutCell(cell, zoneout_rate=0.1)\",\"Bases: Module\",\"ZoneOut Cell module.\",\"This is a module of zoneout described in Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations. This code is modified from eladhoffer/seq2seq.pytorch.\"]},\"1856\":{\"h\":\"Examples\",\"t\":[\">>> lstm = torch.nn.LSTMCell(16, 32) >>> lstm = ZoneOutCell(lstm, 0.5)\",\"Initialize zone out cell module.\",\"Parameters:\",\"cell (torch.nn.Module) – Pytorch recurrent cell module e.g. torch.nn.Module.LSTMCell.\",\"zoneout_rate (float,optional) – Probability of zoneout from 0.0 to 1.0.\",\"forward(inputs, hidden)\",\"Calculate forward propagation.\",\"Parameters:\",\"inputs (Tensor) – Batch of input tensor (B, input_size).\",\"hidden (tuple) – \",\"Tensor: Batch of initial hidden states (B, hidden_size).\",\"Tensor: Batch of initial cell states (B, hidden_size).\",\"Returns:\",\"Tensor: Batch of next hidden states (B, hidden_size).\",\"Tensor: Batch of next cell states (B, hidden_size).\",\"Return type: tuple\"]},\"1857\":{\"h\":\"espnet2.legacy.transform.add_deltas.add_deltas\",\"t\":[\"source\",\"espnet2.legacy.transform.add_deltas.add_deltas(x, window=2, order=2)\",\"Append the deltas to the input.\"]},\"1858\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.add_sos_eos.add_sos_eos\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transformer.add_sos_eos.add_sos_eos(ys_pad, sos, eos, ignore_id, repeat=1)\",\"Add <sos> and <eos> labels.\",\"Parameters:\",\"ys_pad (torch.Tensor) – batch of padded target sequences (B, Lmax)\",\"sos (int) – index of <sos>\",\"eos (int) – index of <eos>\",\"ignore_id (int) – index of padding\",\"Returns: padded tensor (B, Lmax)\",\"Return type: torch.Tensor\",\"Returns: padded tensor (B, Lmax)\",\"Return type: torch.Tensor\"]},\"1859\":{\"h\":\"espnet2.legacy.utils.cli_utils.assert_scipy_wav_style\",\"t\":[\"source\",\"espnet2.legacy.utils.cli_utils.assert_scipy_wav_style(value)\",\"Assert if value is in scipy wav style.\"]},\"1860\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.att_for\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.att_for(args, num_att=1, han_mode=False)\",\"Instantiate an attention module given the program arguments.\",\"Parameters:\",\"args (Namespace) – The arguments\",\"num_att (int) – number of attention modules (in multi-speaker case, it can be 2 or more)\",\"han_mode (bool) – switch on/off mode of hierarchical attention network (HAN)\",\":rtype torch.nn.Module :return: The attention module\"]},\"1861\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.att_to_numpy\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.att_to_numpy(att_ws, att)\",\"Convert attention weights to a numpy array given the attention.\",\"Parameters:\",\"att_ws (list) – The attention weights\",\"att (torch.nn.Module) – The attention\",\"Return type: np.ndarray\",\"Returns: The numpy array of the attention weights\"]},\"1862\":{\"h\":\"espnet2.legacy.nets.beam_search.beam_search\",\"t\":[\"source\",\"espnet2.legacy.nets.beam_search.beam_search(x: Tensor, sos: int, eos: int, beam_size: int, vocab_size: int, scorers: Dict[str, ScorerInterface], weights: Dict[str, float], token_list: List[str] | None = None, maxlenratio: float = 0.0, minlenratio: float = 0.0, pre_beam_ratio: float = 1.5, pre_beam_score_key: str = 'full') → list\",\"Perform beam search with scorers.\",\"Parameters:\",\"x (torch.Tensor) – Encoded speech feature (T, D)\",\"sos (int) – Start of sequence id\",\"eos (int) – End of sequence id\",\"beam_size (int) – The number of hypotheses kept during search\",\"vocab_size (int) – The number of vocabulary\",\"scorers (dict *[*str,ScorerInterface]) – Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None\",\"weights (dict *[*str,float]) – Dict of weights for each scorers The scorer will be ignored if its weight is 0\",\"token_list (list *[*str]) – List of tokens for debug log\",\"maxlenratio (float) – Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\",\"minlenratio (float) – Input length ratio to obtain min output length.\",\"pre_beam_score_key (str) – key of scores to perform pre-beam search\",\"pre_beam_ratio (float) – beam size in the pre-beam search will be int(pre_beam_ratio * beam_size)\",\"Returns: N-best decoding results\",\"Return type: list\"]},\"1863\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.build_blocks\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.build_blocks(net_part: str, idim: int, input_layer_type: str, blocks: List[Dict[str, Any]], repeat_block: int = 0, self_attn_type: str = 'self_attn', positional_encoding_type: str = 'abs_pos', positionwise_layer_type: str = 'linear', positionwise_activation_type: str = 'relu', conv_mod_activation_type: str = 'relu', input_layer_dropout_rate: float = 0.0, input_layer_pos_enc_dropout_rate: float = 0.0, padding_idx: int = -1) → Tuple[Conv2dSubsampling | VGG2L | Sequential, MultiSequential, int, int]\",\"Build custom model blocks.\",\"Parameters:\",\"net_part – Network part, either ‘encoder’ or ‘decoder’.\",\"idim – Input dimension.\",\"input_layer – Input layer type.\",\"blocks – Blocks parameters for network part.\",\"repeat_block – Number of times provided blocks are repeated.\",\"positional_encoding_type – Positional encoding layer type.\",\"positionwise_layer_type – Positionwise layer type.\",\"positionwise_activation_type – Positionwise activation type.\",\"conv_mod_activation_type – Convolutional module activation type.\",\"input_layer_dropout_rate – Dropout rate for input layer.\",\"input_layer_pos_enc_dropout_rate – Dropout rate for input layer pos. enc.\",\"padding_idx – Padding symbol ID for embedding layer.\",\"Returns: Input layer all_blocks: Encoder/Decoder network. out_dim: Network output dimension. conv_subsampling_factor: Subsampling factor in frontend CNN.\",\"Return type: in_layer\"]},\"1864\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.build_conformer_block\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.build_conformer_block(block: Dict[str, Any], self_attn_class: str, pw_layer_type: str, pw_activation_type: str, conv_mod_activation_type: str) → EncoderLayer\",\"Build function for conformer block.\",\"Parameters:\",\"block – Conformer block parameters.\",\"self_attn_type – Self-attention module type.\",\"pw_layer_type – Positionwise layer type.\",\"pw_activation_type – Positionwise activation type.\",\"conv_mod_activation_type – Convolutional module activation type.\",\"Returns: Function to create conformer (encoder) block.\"]},\"1865\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.build_conv1d_block\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.build_conv1d_block(block: Dict[str, Any], block_type: str) → CausalConv1d\",\"Build function for causal conv1d block.\",\"Parameters:block – CausalConv1d or Conv1D block parameters.\",\"Returns: Function to create conv1d (encoder) or causal conv1d (decoder) block.\"]},\"1866\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.build_input_layer\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.build_input_layer(block: Dict[str, Any], pos_enc_class: Module, padding_idx: int) → Tuple[Conv2dSubsampling | VGG2L | Sequential, int]\",\"Build input layer.\",\"Parameters:\",\"block – Architecture definition of input layer.\",\"pos_enc_class – Positional encoding class.\",\"padding_idx – Padding symbol ID for embedding layer (if provided).\",\"Returns: Input layer module. subsampling_factor: Subsampling factor.\"]},\"1867\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.build_transformer_block\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.build_transformer_block(net_part: str, block: Dict[str, Any], pw_layer_type: str, pw_activation_type: str) → EncoderLayer | TransformerDecoderLayer\",\"Build function for transformer block.\",\"Parameters:\",\"net_part – Network part, either ‘encoder’ or ‘decoder’.\",\"block – Transformer block parameters.\",\"pw_layer_type – Positionwise layer type.\",\"pw_activation_type – Positionwise activation type.\",\"Returns: Function to create transformer (encoder or decoder) block.\"]},\"1868\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.check_batch_states\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.check_batch_states(states, max_len, pad_id)\",\"Check decoder hidden states and left pad or trim if necessary.\",\"Parameters:\",\"state – Decoder hidden states. [N x (B, ?, D_dec)]\",\"max_len – maximum sequence length.\",\"pad_id – Padding symbol ID.\",\"Returns: Decoder hidden states. [N x (B, max_len, dec_dim)]\",\"Return type: final\"]},\"1869\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.subsampling.check_short_utt\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transformer.subsampling.check_short_utt(ins, size)\",\"Check if the utterance is too short for subsampling.\"]},\"1870\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.check_state\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.check_state(state: List[Tensor | None], max_len: int, pad_id: int) → List[Tensor | None]\",\"Check decoder hidden states and left pad or trim if necessary.\",\"Parameters:\",\"state – Decoder hidden states. [N x (?, D_dec)]\",\"max_len – maximum sequence length.\",\"pad_id – Padding symbol ID.\",\"Returns: Decoder hidden states. [N x (1, max_len, D_dec)]\",\"Return type: final\"]},\"1871\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.create_lm_batch_states\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.create_lm_batch_states(lm_states: List[Any] | Dict[str, Any], lm_layers, is_wordlm: bool) → List[Any] | Dict[str, Any]\",\"Create LM hidden states.\",\"Parameters:\",\"lm_states – LM hidden states.\",\"lm_layers – Number of LM layers.\",\"is_wordlm – Whether provided LM is a word-level LM.\",\"Returns: LM hidden states.\",\"Return type: new_states\"]},\"1872\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.custom_torch_load\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.custom_torch_load(model_path: str, model: Module, training: bool = True)\",\"Load Transducer model with training-only modules and parameters removed.\",\"Parameters:\",\"model_path – Model path.\",\"model – Transducer model.\"]},\"1873\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.wavenet.decode_mu_law\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.wavenet.decode_mu_law(y, mu=256)\",\"Perform mu-law decoding.\",\"Parameters:\",\"x (ndarray) – Quantized audio signal with the range from 0 to mu - 1.\",\"mu (int) – Quantized level.\",\"Returns: Audio signal with the range from -1 to 1.\",\"Return type: ndarray\"]},\"1874\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.tacotron2.decoder.decoder_init\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.tacotron2.decoder.decoder_init(m)\",\"Initialize decoder parameters.\"]},\"1875\":{\"h\":\"espnet2.legacy.transform.add_deltas.delta\",\"t\":[\"source\",\"espnet2.legacy.transform.add_deltas.delta(feat, window)\",\"Process the deta of the feats.\"]},\"1876\":{\"h\":\"espnet2.legacy.utils.dynamic_import.dynamic_import\",\"t\":[\"source\",\"espnet2.legacy.utils.dynamic_import.dynamic_import(import_path, alias={})\",\"Dynamic import module and class.\",\"Parameters:\",\"import_path (str) – syntax ‘module_name:class_name’ e.g., ‘espnet2.legacy.transform.add_deltas:AddDeltas’\",\"alias (dict) – shortcut for registered class\",\"Returns: imported class\"]},\"1877\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.wavenet.encode_mu_law\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.wavenet.encode_mu_law(x, mu=256)\",\"Perform mu-law encoding.\",\"Parameters:\",\"x (ndarray) – Audio signal with the range from -1 to 1.\",\"mu (int) – Quantized level.\",\"Returns: Quantized audio signal with the range from 0 to mu - 1.\",\"Return type: ndarray\"]},\"1878\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.encoders.encoder_for\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.rnn.encoders.encoder_for(args, idim, subsample)\",\"Instantiate an encoder module given the program arguments.\",\"Parameters:\",\"args (Namespace) – The arguments\",\"idim (intorListofinteger) – dimension of input, e.g. 83, or List of dimensions of inputs, e.g. [83,83]\",\"subsample (ListorListofList) –\",\"subsample factors, e.g. [1,2,2,1,1], or List of subsample factors of each encoder.\",\"e.g. [[1,2,2,1,1], [1,2,2,1,1]]\",\":rtype torch.nn.Module :return: The encoder module\"]},\"1879\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.tacotron2.encoder.encoder_init\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.tacotron2.encoder.encoder_init(m)\",\"Initialize encoder parameters.\"]},\"1880\":{\"h\":\"espnet2.legacy.nets.e2e_asr_common.end_detect\",\"t\":[\"source\",\"espnet2.legacy.nets.e2e_asr_common.end_detect(ended_hyps, i, M=3, D_end=np.float64(-10.0))\",\"End detection.\",\"described in Eq. (50) of S. Watanabe et al “Hybrid CTC/Attention Architecture for End-to-End Speech Recognition”\",\"Parameters:\",\"ended_hyps\",\"i\",\"M\",\"D_end\",\"Returns:\"]},\"1881\":{\"h\":\"espnet2.legacy.utils.cli_readers.file_reader_helper\",\"t\":[\"source\",\"espnet2.legacy.utils.cli_readers.file_reader_helper(rspecifier: str, filetype: str = 'mat', return_shape: bool = False, segments: str | None = None)\",\"Read uttid and array in kaldi style.\",\"This function might be a bit confusing as “ark” is used for HDF5 to imitate “kaldi-rspecifier”.\",\"Parameters:\",\"rspecifier – Give as “ark:feats.ark” or “scp:feats.scp”\",\"filetype – “mat” is kaldi-martix, “hdf5”: HDF5\",\"return_shape – Return the shape of the matrix, instead of the matrix. This can reduce IO cost for HDF5.\",\"Return type:Generator[Tuple[str, np.ndarray], None, None]\"]},\"1882\":{\"h\":\"Examples\",\"t\":[\"Read from kaldi-matrix ark file:\",\">>> for u, array in file_reader_helper('ark:feats.ark', 'mat'): ... array\",\"Read from HDF5 file:\",\">>> for u, array in file_reader_helper('ark:feats.h5', 'hdf5'): ... array\"]},\"1883\":{\"h\":\"espnet2.legacy.utils.cli_writers.file_writer_helper\",\"t\":[\"source\",\"espnet2.legacy.utils.cli_writers.file_writer_helper(wspecifier: str, filetype: str = 'mat', write_num_frames: str | None = None, compress: bool = False, compression_method: int = 2, pcm_format: str = 'wav')\",\"Write matrices in kaldi style.\",\"Parameters:\",\"wspecifier – e.g. ark,scp:out.ark,out.scp\",\"filetype – “mat” is kaldi-martix, “hdf5”: HDF5\",\"write_num_frames – e.g. ‘ark,t:num_frames.txt’\",\"compress – Compress or not\",\"compression_method – Specify compression level\",\"Write in kaldi-matrix-ark with “kaldi-scp” file:\",\">>> with file_writer_helper('ark,scp:out.ark,out.scp') as f: >>> f['uttid'] = array\",\"This “scp” has the following format:\",\"uttidA out.ark:1234 uttidB out.ark:2222\",\"where, 1234 and 2222 points the strating byte address of the matrix. (For detail, see official documentation of Kaldi)\",\"Write in HDF5 with “scp” file:\",\">>> with file_writer_helper('ark,scp:out.h5,out.scp', 'hdf5') as f: >>> f['uttid'] = array\",\"This “scp” file is created as:\",\"uttidA out.h5:uttidA uttidB out.h5:uttidB\",\"HDF5 can be, unlike “kaldi-ark”, accessed to any keys, so originally “scp” is not required for random-reading. Nevertheless we create “scp” for HDF5 because it is useful for some use-case. e.g. Concatenation, Splitting.\"]},\"1884\":{\"h\":\"espnet2.legacy.transform.spec_augment.freq_mask\",\"t\":[\"source\"]},\"1885\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.frontends.frontend.frontend_for\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.frontends.frontend.frontend_for(args, idim)\",\"Instantiate an frontend module given the program arguments..\"]},\"1886\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.get_activation\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.get_activation(act)\",\"Return activation function.\"]},\"1887\":{\"h\":\"espnet2.legacy.utils.cli_utils.get_commandline_args\",\"t\":[\"source\",\"espnet2.legacy.utils.cli_utils.get_commandline_args()\",\"Get command line arguments.\"]},\"1888\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.get_decoder_input\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.get_decoder_input(labels: Tensor, blank_id: int, ignore_id: int) → Tensor\",\"Prepare decoder input.\",\"Parameters:labels – Label ID sequences. (B, L)\",\"Returns: Label ID sequences with blank prefix. (B, U)\",\"Return type: decoder_input\"]},\"1889\":{\"h\":\"espnet2.legacy.utils.cli_writers.get_num_frames_writer\",\"t\":[\"source\",\"espnet2.legacy.utils.cli_writers.get_num_frames_writer(write_num_frames: str)\",\"Get number of frames.\"]},\"1890\":{\"h\":\"Examples\",\"t\":[\">>> get_num_frames_writer('ark,t:num_frames.txt')\"]},\"1891\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.get_pos_enc_and_att_class\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.get_pos_enc_and_att_class(net_part: str, pos_enc_type: str, self_attn_type: str) → Tuple[PositionalEncoding | ScaledPositionalEncoding | RelPositionalEncoding, MultiHeadedAttention | RelPositionMultiHeadedAttention]\",\"Get positional encoding and self attention module class.\",\"Parameters:\",\"net_part – Network part, either ‘encoder’ or ‘decoder’.\",\"pos_enc_type – Positional encoding type.\",\"self_attn_type – Self-attention type.\",\"Returns: Positional encoding class. self_attn_class: Self-attention class.\",\"Return type: pos_enc_class\"]},\"1892\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.get_subsample\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.get_subsample(train_args, mode, arch)\",\"Parse the subsampling factors from the args for the specified mode and arch.\",\"Parameters:\",\"train_args – argument Namespace containing options.\",\"mode – one of (‘asr’, ‘mt’, ‘st’)\",\"arch – one of (‘rnn’, ‘rnn-t’, ‘rnn_mix’, ‘rnn_mulenc’, ‘transformer’)\",\"Returns: subsampling factors.\",\"Return type: np.ndarray / List[np.ndarray]\"]},\"1893\":{\"h\":\"espnet2.legacy.nets.e2e_asr_common.get_vgg2l_odim\",\"t\":[\"source\",\"espnet2.legacy.nets.e2e_asr_common.get_vgg2l_odim(idim, in_channel=3, out_channel=128)\",\"Return the output size of the VGG frontend.\",\"Parameters:\",\"in_channel – input channel size\",\"out_channel – output channel size\",\"Returns: output size\",\":rtype int\"]},\"1894\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.init_lm_state\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.init_lm_state(lm_model: Module)\",\"Initialize LM hidden states.\",\"Parameters:lm_model – LM module.\",\"Returns: Initial LM hidden states.\",\"Return type: lm_state\"]},\"1895\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.initial_att\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.rnn.attentions.initial_att(atype, eprojs, dunits, aheads, adim, awin, aconv_chans, aconv_filts, han_mode=False)\",\"Instantiate a single attention module.\",\"Parameters:\",\"atype (str) – attention type\",\"eprojs (int) – # projection-units of encoder\",\"dunits (int) – # units of decoder\",\"aheads (int) – # heads of multi head attention\",\"adim (int) – attention dimension\",\"awin (int) – attention window size\",\"aconv_chans (int) – # channels of attention convolution\",\"aconv_filts (int) – filter size of attention convolution\",\"han_mode (bool) – flag to swith on mode of hierarchical attention\",\"Returns: The attention module\"]},\"1896\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.wavenet.initialize\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.wavenet.initialize(m)\",\"Initilize conv layers with xavier.\",\"Parameters:m (torch.nn.Module) – Torch module.\"]},\"1897\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.is_prefix\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.is_prefix(x: List[int], pref: List[int]) → bool\",\"Check if pref is a prefix of x.\",\"Parameters:\",\"x – Label ID sequence.\",\"pref – Prefix label ID sequence.\",\"Returns: Whether pref is a prefix of x.\"]},\"1898\":{\"h\":\"espnet2.legacy.utils.cli_utils.is_scipy_wav_style\",\"t\":[\"source\",\"espnet2.legacy.utils.cli_utils.is_scipy_wav_style(value)\",\"Check if value is a tuple or not.\"]},\"1899\":{\"h\":\"espnet2.legacy.transform.spectrogram.istft\",\"t\":[\"source\",\"espnet2.legacy.transform.spectrogram.istft(x, n_shift, win_length=None, window='hann', center=True)\",\"Process invert stft.\"]},\"1900\":{\"h\":\"espnet2.legacy.transform.spectrogram.logmelspectrogram\",\"t\":[\"source\",\"espnet2.legacy.transform.spectrogram.logmelspectrogram(x, fs, n_mels, n_fft, n_shift, win_length=None, window='hann', fmin=None, fmax=None, eps=1e-10, pad_mode='reflect')\",\"Obtain Logmel from signal.\"]},\"1901\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.make_non_pad_mask\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.make_non_pad_mask(lengths, xs=None, length_dim=-1)\",\"Make mask tensor containing indices of non-padded part.\",\"Parameters:\",\"lengths (LongTensororList) – Batch of lengths (B,).\",\"xs (Tensor,optional) – The reference tensor. If set, masks will be the same shape as this tensor.\",\"length_dim (int,optional) – Dimension indicator of the above tensor. See the example.\",\"Returns: mask tensor containing indices of padded part. : dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (including 1.2)\",\"Return type: ByteTensor\"]},\"1902\":{\"h\":\"Examples\",\"t\":[\"With only lengths.\",\">>> lengths = [5, 3, 2] >>> make_non_pad_mask(lengths) masks = [[1, 1, 1, 1 ,1], [1, 1, 1, 0, 0], [1, 1, 0, 0, 0]]\",\"With the reference tensor.\",\">>> xs = torch.zeros((3, 2, 4)) >>> make_non_pad_mask(lengths, xs) tensor([[[1, 1, 1, 1], [1, 1, 1, 1]], [[1, 1, 1, 0], [1, 1, 1, 0]], [[1, 1, 0, 0], [1, 1, 0, 0]]], dtype=torch.uint8) >>> xs = torch.zeros((3, 2, 6)) >>> make_non_pad_mask(lengths, xs) tensor([[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0]], [[1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]], [[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\",\"With the reference tensor and dimension indicator.\",\">>> xs = torch.zeros((3, 6, 6)) >>> make_non_pad_mask(lengths, xs, 1) tensor([[[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8) >>> make_non_pad_mask(lengths, xs, 2) tensor([[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0]], [[1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]], [[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\"]},\"1903\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.make_pad_mask\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.make_pad_mask(lengths, xs=None, length_dim=-1, maxlen=None)\",\"Make mask tensor containing indices of padded part.\",\"Parameters:\",\"lengths (LongTensororList) – Batch of lengths (B,).\",\"xs (Tensor,optional) – The reference tensor. If set, masks will be the same shape as this tensor.\",\"length_dim (int,optional) – Dimension indicator of the above tensor. See the example.\",\"Returns: Mask tensor containing indices of padded part. : dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (including 1.2)\",\"Return type: Tensor\"]},\"1904\":{\"h\":\"Examples\",\"t\":[\"With only lengths.\",\">>> lengths = [5, 3, 2] >>> make_pad_mask(lengths) masks = [[0, 0, 0, 0 ,0], [0, 0, 0, 1, 1], [0, 0, 1, 1, 1]]\",\"With the reference tensor.\",\">>> xs = torch.zeros((3, 2, 4)) >>> make_pad_mask(lengths, xs) tensor([[[0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 1], [0, 0, 0, 1]], [[0, 0, 1, 1], [0, 0, 1, 1]]], dtype=torch.uint8) >>> xs = torch.zeros((3, 2, 6)) >>> make_pad_mask(lengths, xs) tensor([[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], [[0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\",\"With the reference tensor and dimension indicator.\",\">>> xs = torch.zeros((3, 6, 6)) >>> make_pad_mask(lengths, xs, 1) tensor([[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1]], [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8) >>> make_pad_mask(lengths, xs, 2) tensor([[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], [[0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\"]},\"1905\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.mask_by_length\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.mask_by_length(xs, lengths, fill=0)\",\"Mask tensor according to length.\",\"Parameters:\",\"xs (Tensor) – Batch of input tensor (B, *).\",\"lengths (LongTensororList) – Batch of lengths (B,).\",\"fill (intorfloat) – Value to fill masked part.\",\"Returns: Batch of masked input tensor (B, *).\",\"Return type: Tensor\"]},\"1906\":{\"h\":\"Examples\",\"t\":[\">>> x = torch.arange(5).repeat(3, 1) + 1 >>> x tensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]) >>> lengths = [5, 3, 2] >>> mask_by_length(x, lengths) tensor([[1, 2, 3, 4, 5], [1, 2, 3, 0, 0], [1, 2, 0, 0, 0]])\"]},\"1907\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.maskctc.add_mask_token.mask_uniform\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.maskctc.add_mask_token.mask_uniform(ys_pad, mask_token, eos, ignore_id)\",\"Replace random tokens with <mask> label and add <eos> label.\",\"The number of <mask> is chosen from a uniform distribution between one and the target sequence’s length. :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax) :param int mask_token: index of <mask> :param int eos: index of <eos> :param int ignore_id: index of padding :return: padded tensor (B, Lmax) :rtype: torch.Tensor :return: padded tensor (B, Lmax) :rtype: torch.Tensor\"]},\"1908\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.pad_list\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.pad_list(xs, pad_value)\",\"Perform padding for the list of tensors.\",\"Parameters:\",\"xs (List) – List of Tensors [(T_1, *), (T_2, *), …, (T_B, *)].\",\"pad_value (float) – Value for padding.\",\"Returns: Padded tensor (B, Tmax, *).\",\"Return type: Tensor\"]},\"1909\":{\"h\":\"Examples\",\"t\":[\">>> x = [torch.ones(4), torch.ones(2), torch.ones(1)] >>> x [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])] >>> pad_list(x, 0) tensor([[1., 1., 1., 1.], [1., 1., 0., 0.], [1., 0., 0., 0.]])\"]},\"1910\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.pad_sequence\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.pad_sequence(labels: List[int], pad_id: int) → List[int]\",\"Left pad label ID sequences.\",\"Parameters:\",\"labels – Label ID sequence.\",\"pad_id – Padding symbol ID.\",\"Returns: Padded label ID sequences.\",\"Return type: final\"]},\"1911\":{\"h\":\"espnet2.legacy.utils.cli_writers.parse_wspecifier\",\"t\":[\"source\",\"espnet2.legacy.utils.cli_writers.parse_wspecifier(wspecifier: str) → Dict[str, str]\",\"Parse wspecifier to dict.\"]},\"1912\":{\"h\":\"Examples\",\"t\":[\">>> parse_wspecifier('ark,scp:out.ark,out.scp') {'ark': 'out.ark', 'scp': 'out.scp'}\"]},\"1913\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.prepare_body_model\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.prepare_body_model(net_part: str, blocks: List[Dict[str, Any]]) → Tuple[int]\",\"Prepare model body blocks.\",\"Parameters:\",\"net_part – Network part, either ‘encoder’ or ‘decoder’.\",\"blocks – Blocks parameters for network part.\",\"Returns: Network output dimension.\"]},\"1914\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.prepare_input_layer\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.prepare_input_layer(input_layer_type: str, feats_dim: int, blocks: List[Dict[str, Any]], dropout_rate: float, pos_enc_dropout_rate: float) → Dict[str, Any]\",\"Prepare input layer arguments.\",\"Parameters:\",\"input_layer_type – Input layer type.\",\"feats_dim – Dimension of input features.\",\"blocks – Blocks parameters for network part.\",\"dropout_rate – Dropout rate for input layer.\",\"pos_enc_dropout_rate – Dropout rate for input layer pos. enc.\",\"Returns: Input block parameters.\",\"Return type: input_block\"]},\"1915\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.recombine_hyps\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.recombine_hyps(hyps: List[Hypothesis]) → List[Hypothesis]\",\"Recombine hypotheses with same label ID sequence.\",\"Parameters:hyps – Hypotheses.\",\"Returns: Recombined hypotheses.\",\"Return type: final\"]},\"1916\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.rename_state_dict\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.rename_state_dict(old_prefix: str, new_prefix: str, state_dict: Dict[str, Tensor])\",\"Replace keys of old prefix with new prefix in state dict.\"]},\"1917\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.repeat.repeat\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transformer.repeat.repeat(N, fn, layer_drop_rate=0.0)\",\"Repeat module N times.\",\"Parameters:\",\"N (int) – Number of repeat time.\",\"fn (Callable) – Function to generate module.\",\"layer_drop_rate (float) – Probability of dropping out each fn (layer).\",\"Returns: Repeated model instance.\",\"Return type:MultiSequential\"]},\"1918\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.rnn.encoders.reset_backward_rnn_state\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.rnn.encoders.reset_backward_rnn_state(states)\",\"Set backward BRNN states to zeroes.\",\"Useful in processing of sliding windows over the inputs\"]},\"1919\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.roll_tensor\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.roll_tensor(x: Tensor, lengths: Tensor, roll_amounts: Tensor | None = None, fixed_intervals: int | None = None) → Tensor\",\"Left-roll tensor x by roll_amounts, only within lengths and optionally quantized.\",\"Parameters:\",\"x – input tensor (B, T, D)\",\"lengths – lengths of each sequence (B,)\",\"roll_amounts – random shift amounts (B,). If None, random shift amounts are generated.\",\"fixed_intervals – if not None, roll_amounts are quantized to multiples of this.\",\"Returns: rolled tensor (B, T, D)\",\"Return type: rolled_x\",\"Useful to apply roll augmentation to the input, while considering the input length for each sample.\"]},\"1920\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.select_k_expansions\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.select_k_expansions(hyps: List[ExtendedHypothesis], topk_idxs: Tensor, topk_logps: Tensor, gamma: float) → List[ExtendedHypothesis]\",\"Return K hypotheses candidates for expansion from a list of hypothesis.\",\"K candidates are selected according to the extended hypotheses probabilities and a prune-by-value method. Where K is equal to beam_size + beta.\",\"Parameters:\",\"hyps – Hypotheses.\",\"topk_idxs – Indices of candidates hypothesis.\",\"topk_logps – Log-probabilities for hypotheses expansions.\",\"gamma – Allowed logp difference for prune-by-value method.\",\"Returns: Best K expansion hypotheses candidates.\",\"Return type: k_expansions\"]},\"1921\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.select_lm_state\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.select_lm_state(lm_states: List[Any] | Dict[str, Any], idx: int, lm_layers: int, is_wordlm: bool) → List[Any] | Dict[str, Any]\",\"Get ID state from LM hidden states.\",\"Parameters:\",\"lm_states – LM hidden states.\",\"idx – LM state ID to extract.\",\"lm_layers – Number of LM layers.\",\"is_wordlm – Whether provided LM is a word-level LM.\",\"Returns: LM hidden state for given ID.\",\"Return type: idx_state\"]},\"1922\":{\"h\":\"espnet2.legacy.transform.spec_augment.spec_augment\",\"t\":[\"source\"]},\"1923\":{\"h\":\"espnet2.legacy.transform.spectrogram.spectrogram\",\"t\":[\"source\",\"espnet2.legacy.transform.spectrogram.spectrogram(x, n_fft, n_shift, win_length=None, window='hann')\",\"Obtain Spectrogram from signal.\"]},\"1924\":{\"h\":\"espnet2.legacy.transform.spectrogram.stft\",\"t\":[\"source\",\"espnet2.legacy.transform.spectrogram.stft(x, n_fft, n_shift, win_length=None, window='hann', center=True, pad_mode='reflect')\",\"Process STFT from signal.\"]},\"1925\":{\"h\":\"espnet2.legacy.transform.spectrogram.stft2logmelspectrogram\",\"t\":[\"source\",\"espnet2.legacy.transform.spectrogram.stft2logmelspectrogram(x_stft, fs, n_mels, n_fft, fmin=None, fmax=None, eps=1e-10)\",\"Convert STFT to LogMel.\"]},\"1926\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transformer.mask.subsequent_mask\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transformer.mask.subsequent_mask(size, device='cpu', dtype=torch.bool)\",\"Create mask for subsequent steps (size, size).\",\"Parameters:\",\"size (int) – size of mask\",\"device (str) – “cpu” or “cuda” or torch.Tensor.device\",\"dtype (torch.dtype) – result dtype\",\"Return type: torch.Tensor\",\">>> subsequent_mask(3) [[1, 0, 0], [1, 1, 0], [1, 1, 1]]\"]},\"1927\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.subtract\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.subtract(x: List[ExtendedHypothesis], subset: List[ExtendedHypothesis]) → List[ExtendedHypothesis]\",\"Remove elements of subset if corresponding label ID sequence already exist in x.\",\"Parameters:\",\"x – Set of hypotheses.\",\"subset – Subset of x.\",\"Returns: New set of hypotheses.\",\"Return type: final\"]},\"1928\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.th_accuracy\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.th_accuracy(pad_outputs, pad_targets, ignore_label)\",\"Calculate accuracy.\",\"Parameters:\",\"pad_outputs (Tensor) – Prediction tensors (B * Lmax, D).\",\"pad_targets (LongTensor) – Target label tensors (B, Lmax, D).\",\"ignore_label (int) – Ignore label id.\",\"Returns: Accuracy value (0.0 - 1.0).\",\"Return type: float\"]},\"1929\":{\"h\":\"espnet2.legacy.transform.spec_augment.time_mask\",\"t\":[\"source\"]},\"1930\":{\"h\":\"espnet2.legacy.transform.spec_augment.time_warp\",\"t\":[\"source\"]},\"1931\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.to_device\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.to_device(m, x)\",\"Send tensor into the device of the module.\",\"Parameters:\",\"m (torch.nn.Module) – Torch module.\",\"x (Tensor) – Torch tensor.\",\"Returns: Torch tensor located in the same place as torch module.\",\"Return type: Tensor\"]},\"1932\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.to_torch_tensor\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.to_torch_tensor(x)\",\"Change to torch.Tensor or ComplexTensor from numpy.ndarray.\",\"Parameters:x – Inputs. It should be one of numpy.ndarray, Tensor, ComplexTensor, and dict.\",\"Returns: Type converted inputs.\",\"Return type: Tensor or ComplexTensor\"]},\"1933\":{\"h\":\"Examples\",\"t\":[\">>> xs = np.ones(3, dtype=np.float32) >>> xs = to_torch_tensor(xs) tensor([1., 1., 1.]) >>> xs = torch.ones(3, 4, 5) >>> assert to_torch_tensor(xs) is xs >>> xs = {'real': xs, 'imag': xs} >>> to_torch_tensor(xs) ComplexTensor( Real: tensor([1., 1., 1.]) Imag; tensor([1., 1., 1.]) )\"]},\"1934\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.trim_by_ctc_posterior\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.trim_by_ctc_posterior(h: Tensor, ctc_probs: Tensor, masks: Tensor, pos_emb: Tensor | None = None)\",\"Trim the encoder hidden output using CTC posterior.\",\"The continuous frames in the tail that confidently represent blank symbols are trimmed.\"]},\"1935\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.nets_utils.triu_onnx\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.nets_utils.triu_onnx(x)\",\"Make TriU for ONNX.\"]},\"1936\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.utils.valid_aux_encoder_output_layers\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.utils.valid_aux_encoder_output_layers(aux_layer_id: List[int], enc_num_layers: int, use_symm_kl_div_loss: bool, subsample: List[int]) → List[int]\",\"Check whether provided auxiliary encoder layer IDs are valid.\",\"Return the valid list sorted with duplicates removed.\",\"Parameters:\",\"aux_layer_id – Auxiliary encoder layer IDs.\",\"enc_num_layers – Number of encoder layers.\",\"use_symm_kl_div_loss – Whether symmetric KL divergence loss is used.\",\"subsample – Subsampling rate per layer.\",\"Returns: Valid list of auxiliary encoder layers.\",\"Return type: valid\"]},\"1937\":{\"h\":\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.verify_block_arguments\",\"t\":[\"source\",\"espnet2.legacy.nets.pytorch_backend.transducer.blocks.verify_block_arguments(net_part: str, block: Dict[str, Any], num_block: int) → Tuple[int, int]\",\"Verify block arguments are valid.\",\"Parameters:\",\"net_part – Network part, either ‘encoder’ or ‘decoder’.\",\"block – Block parameters.\",\"num_block – Block ID.\",\"Returns: Input and output dimension of the block.\",\"Return type: block_io\"]},\"1938\":{\"h\":\"espnet2.lm.abs_model.AbsLM\",\"t\":[\"source\",\"class espnet2.lm.abs_model.AbsLM(*args, **kwargs)\",\"Bases: Module, BatchScorerInterface, ABC\",\"The abstract LM class\",\"To share the loss calculation way among different models, We uses delegate pattern here: The instance of this class should be passed to “LanguageModel”\",\">>> from espnet2.lm.abs_model import AbsLM >>> lm = AbsLM() >>> model = LanguageESPnetModel(lm=lm)\",\"This “model” is one of mediator objects for “Task” class.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, hidden: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1939\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1940\":{\"h\":\"espnet2.lm.espnet_model.ESPnetLanguageModel\",\"t\":[\"source\",\"class espnet2.lm.espnet_model.ESPnetLanguageModel(lm: AbsLM, vocab_size: int, ignore_id: int = 0)\",\"Bases: AbsESPnetModel\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"batchify_nll(text: Tensor, text_lengths: Tensor, batch_size: int = 100) → Tuple[Tensor, Tensor]\",\"Compute negative log likelihood(nll) from transformer language model\",\"To avoid OOM, this fuction seperate the input into batches. Then call nll for each batch and combine and return results. :param text: (Batch, Length) :param text_lengths: (Batch,) :param batch_size: int, samples each batch contain when computing nll,\",\"you may change this to avoid OOM or increase\",\"collect_feats(text: Tensor, text_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1941\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"nll(text: Tensor, text_lengths: Tensor, max_length: int | None = None) → Tuple[Tensor, Tensor]\",\"Compute negative log likelihood(nll)\",\"Normally, this function is called in batchify_nll. :param text: (Batch, Length) :param text_lengths: (Batch,) :param max_lengths: int\"]},\"1942\":{\"h\":\"espnet2.lm.espnet_model_multitask.ESPnetMultitaskLanguageModel\",\"t\":[\"source\",\"class espnet2.lm.espnet_model_multitask.ESPnetMultitaskLanguageModel(lm: AbsLM, vocab_size: int, token_list: Tuple[str, ...] | List[str], ignore_id: int = 0, lsm_weight: float = 0.0, length_normalized_loss: bool = False, sos_syms: List[str] = ['<generatetext>', '<generatespeech>'], eos_sym: str = '<sos/eos>')\",\"Bases: AbsESPnetModel\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"batchify_nll(text: Tensor, text_lengths: Tensor, batch_size: int = 100) → Tuple[Tensor, Tensor]\",\"Compute negative log likelihood(nll) from transformer language model\",\"To avoid OOM, this fuction seperate the input into batches. Then call nll for each batch and combine and return results. :param text: (Batch, Length) :param text_lengths: (Batch,) :param batch_size: int, samples each batch contain when computing nll,\",\"you may change this to avoid OOM or increase\",\"collect_feats(text: Tensor, text_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1943\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"nll(text: Tensor, text_lengths: Tensor, max_length: int | None = None) → Tuple[Tensor, Tensor]\",\"Compute negative log likelihood (nll)\",\"NOTE(yifan): We only use nll to calculate perplexity, : so there is no condition in each sentence.\",\"Normally, this function is called in batchify_nll. :param text: (Batch, Length) :param text_lengths: (Batch,) :param max_lengths: int\"]},\"1944\":{\"h\":\"espnet2.lm.huggingface_pretrained_opt_lm.HuggingfaceOPTModel\",\"t\":[\"source\",\"class espnet2.lm.huggingface_pretrained_opt_lm.HuggingfaceOPTModel(vocab_size: int, opt_name: str)\",\"Bases: AbsLM\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor) → Tuple[Tensor, List[Any]]\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, vocab_size) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(input: Tensor, hidden: None) → Tuple[Tensor, None]\",\"Compute LM loss value from buffer sequences.\",\"Parameters:\",\"input (torch.Tensor) – Input ids. (batch, len)\",\"hidden (torch.Tensor) – Target ids. (batch, len)\",\"reload_pretrained_parameters()\",\"score(y: Tensor, state: Any, x: Tensor) → Tuple[Tensor, Any]\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – encoder feature that generates ys.\",\"Returns: Tuple of : torch.float32 scores for next token (vocab_size) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\"]},\"1945\":{\"h\":\"espnet2.lm.seq_rnn_lm.SequentialRNNLM\",\"t\":[\"source\",\"class espnet2.lm.seq_rnn_lm.SequentialRNNLM(vocab_size: int, unit: int = 650, nhid: int | None = None, nlayers: int = 2, dropout_rate: float = 0.0, tie_weights: bool = False, rnn_type: str = 'lstm', ignore_id: int = 0)\",\"Bases: AbsLM\",\"Sequential RNNLM.\",\"SEE ALSO\",\"https://github.com/pytorch/examples/blob/4581968193699de14b56527296262dd76ab43557/word_language_model/model.py\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: Tensor, xs: Tensor) → Tuple[Tensor, Tensor]\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(input: Tensor, hidden: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1946\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"score(y: Tensor, state: Tensor | Tuple[Tensor, Tensor], x: Tensor) → Tuple[Tensor, Tensor | Tuple[Tensor, Tensor]]\",\"Score new token.\",\"Parameters:\",\"y – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x – 2D encoder feature that generates ys.\",\"Returns: Tuple of : torch.float32 scores for next token (n_vocab) and next state for ys\",\"zero_state()\",\"Initialize LM state filled with zero values.\"]},\"1947\":{\"h\":\"espnet2.lm.transformer_lm.TransformerLM\",\"t\":[\"source\",\"class espnet2.lm.transformer_lm.TransformerLM(vocab_size: int, pos_enc: str | None = None, embed_unit: int = 128, att_unit: int = 256, head: int = 2, unit: int = 1024, layer: int = 4, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1)\",\"Bases: AbsLM\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor) → Tuple[Tensor, List[Any]]\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, vocab_size) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(input: Tensor, hidden: None) → Tuple[Tensor, None]\",\"Compute LM loss value from buffer sequences.\",\"Parameters:\",\"input (torch.Tensor) – Input ids. (batch, len)\",\"hidden (torch.Tensor) – Target ids. (batch, len)\",\"score(y: Tensor, state: Any, x: Tensor) → Tuple[Tensor, Any]\",\"Score new token.\",\"Parameters:\",\"y (torch.Tensor) – 1D torch.int64 prefix tokens.\",\"state – Scorer state for prefix tokens\",\"x (torch.Tensor) – encoder feature that generates ys.\",\"Returns: Tuple of : torch.float32 scores for next token (vocab_size) and next state for ys\",\"Return type: tuple[torch.Tensor, Any]\"]},\"1948\":{\"h\":\"espnet2.main_funcs.pack_funcs.Archiver\",\"t\":[\"source\",\"class espnet2.main_funcs.pack_funcs.Archiver(file, mode='r')\",\"Bases: object\",\"add(filename, arcname=None, recursive: bool = True)\",\"addfile(info, fileobj)\",\"close()\",\"extract(info, path=None)\",\"extractfile(info, mode='r')\",\"generate_info(name, size) → TarInfo | ZipInfo\",\"Generate TarInfo using system information\",\"get_name_from_info(info)\"]},\"1949\":{\"h\":\"espnet2.main_funcs.average_nbest_models.average_nbest_models\",\"t\":[\"source\",\"espnet2.main_funcs.average_nbest_models.average_nbest_models(output_dir: Path, reporter: Reporter, best_model_criterion: Sequence[Sequence[str]], nbest: Collection[int] | int, suffix: str | None = None, use_deepspeed: bool = False) → None\",\"Generate averaged model from n-best models\",\"Parameters:\",\"output_dir – The directory contains the model file for each epoch\",\"reporter – Reporter instance\",\"best_model_criterion – Give criterions to decide the best model. e.g. [(“valid”, “loss”, “min”), (“train”, “acc”, “max”)]\",\"nbest – Number of best model files to be averaged\",\"suffix – A suffix added to the averaged model file name\"]},\"1950\":{\"h\":\"espnet2.main_funcs.calculate_all_attentions.calculate_all_attentions\",\"t\":[\"source\",\"espnet2.main_funcs.calculate_all_attentions.calculate_all_attentions(model: AbsESPnetModel, batch: Dict[str, Tensor]) → Dict[str, List[Tensor]]\",\"Derive the outputs from the all attention layers\",\"Parameters:\",\"model\",\"batch – same as forward\",\"Returns: A dict of a list of tensor. key_names x batch x (D1, D2, …)\",\"Return type: return_dict\"]},\"1951\":{\"h\":\"espnet2.main_funcs.collect_stats.collect_stats\",\"t\":[\"source\",\"espnet2.main_funcs.collect_stats.collect_stats(model: AbsESPnetModel | None, train_iter: Iterable[Tuple[List[str], Dict[str, Tensor]]], valid_iter: Iterable[Tuple[List[str], Dict[str, Tensor]]], output_dir: Path, ngpu: int | None, log_interval: int | None, write_collected_feats: bool) → None\",\"Perform on collect_stats mode.\",\"Running for deriving the shape information from data and gathering statistics. This method is used before executing train().\"]},\"1952\":{\"h\":\"espnet2.main_funcs.pack_funcs.find_path_and_change_it_recursive\",\"t\":[\"source\",\"espnet2.main_funcs.pack_funcs.find_path_and_change_it_recursive(value, src: str, tgt: str)\"]},\"1953\":{\"h\":\"espnet2.main_funcs.pack_funcs.get_dict_from_cache\",\"t\":[\"source\",\"espnet2.main_funcs.pack_funcs.get_dict_from_cache(meta: Path | str) → Dict[str, str] | None\"]},\"1954\":{\"h\":\"espnet2.main_funcs.pack_funcs.pack\",\"t\":[\"source\",\"espnet2.main_funcs.pack_funcs.pack(files: Dict[str, str | Path], yaml_files: Dict[str, str | Path], outpath: str | Path, option: Iterable[str | Path] = ())\"]},\"1955\":{\"h\":\"espnet2.main_funcs.pack_funcs.unpack\",\"t\":[\"source\",\"espnet2.main_funcs.pack_funcs.unpack(input_archive: Path | str, outpath: Path | str, use_cache: bool = True) → Dict[str, str]\",\"Scan all files in the archive file and return as a dict of files.\"]},\"1956\":{\"h\":\"Examples\",\"t\":[\"tarfile: : model.pth some1.file some2.file\",\">>> unpack(\\\"tarfile\\\", \\\"out\\\") {'asr_model_file': 'out/model.pth'}\"]},\"1957\":{\"h\":\"espnet2.mt.frontend.embedding.CodecEmbedding\",\"t\":[\"source\",\"class espnet2.mt.frontend.embedding.CodecEmbedding(input_size, hf_model_tag: str = 'espnet/amuse_encodec_16k', token_bias: int = 2, token_per_frame: int = 8, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, positional_dropout_rate: float = 0.1)\",\"Bases: AbsFrontend\",\"Use codec dequantization process and the input embeddings\",\"Initialize.\",\"Parameters:\",\"hf_model_tag – HuggingFace model tag for Espnet codec models\",\"token_bias – the index of the first codec code\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"positional_dropout_rate – dropout rate after adding positional encoding\",\"forward(input: Tensor, input_lengths: Tensor)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1958\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\",\"Return output length of feature dimension D, i.e. the embedding dim.\"]},\"1959\":{\"h\":\"espnet2.mt.espnet_model.ESPnetMTModel\",\"t\":[\"source\",\"class espnet2.mt.espnet_model.ESPnetMTModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, decoder: AbsDecoder, src_vocab_size: int = 0, src_token_list: Tuple[str, ...] | List[str] = [], ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_bleu: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', patch_size: int = 1, extract_feats_in_collect_stats: bool = True, share_decoder_input_output_embed: bool = False, share_encoder_decoder_input_embed: bool = False)\",\"Bases: AbsESPnetModel\",\"Encoder-Decoder model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(text: Tensor, text_lengths: Tensor, src_text: Tensor, src_text_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"encode(src_text: Tensor, src_text_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Frontend + Encoder. Note that this method is used by mt_inference.py\",\"Parameters:\",\"src_text – (Batch, Length, …)\",\"src_text_lengths – (Batch, )\",\"forward(text: Tensor, text_lengths: Tensor, src_text: Tensor, src_text_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"src_text – (Batch, length)\",\"src_text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\"]},\"1960\":{\"h\":\"espnet2.mt.frontend.embedding.Embedding\",\"t\":[\"source\",\"class espnet2.mt.frontend.embedding.Embedding(input_size: int = 400, embed_dim: int = 400, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, positional_dropout_rate: float = 0.1)\",\"Bases: AbsFrontend\",\"Embedding Frontend for text based inputs.\",\"Initialize.\",\"Parameters:\",\"input_size – Number of input tokens.\",\"embed_dim – Embedding Size.\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"positional_dropout_rate – dropout rate after adding positional encoding\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Apply a sliding window on the input.\",\"Parameters:\",\"input – Input (B, T) or (B, T,D), with D.\",\"input_lengths – Input lengths within batch.\",\"Returns: Output with dimensions (B, T, D). Tensor: Output lengths within batch.\",\"Return type: Tensor\",\"output_size() → int\",\"Return output length of feature dimension D, i.e. the embedding dim.\"]},\"1961\":{\"h\":\"espnet2.mt.frontend.embedding.PatchEmbedding\",\"t\":[\"source\",\"class espnet2.mt.frontend.embedding.PatchEmbedding(input_size: int = 400, embed_dim: int = 400, token_per_frame: int = 1, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, positional_dropout_rate: float = 0.1)\",\"Bases: AbsFrontend\",\"Embedding Frontend for text based inputs.\",\"Initialize.\",\"Parameters:\",\"input_size – Number of input tokens.\",\"embed_dim – Embedding Size.\",\"token_per_frame – number of tokens per frame in the input\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"positional_dropout_rate – dropout rate after adding positional encoding\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Apply a sliding window on the input.\",\"Parameters:\",\"input – Input (B, T)\",\"input_lengths – Input lengths within batch.\",\"Returns: Output with dimensions (B, T // token_per_frame, D). Tensor: Output lengths within batch, devided by token_per_frame\",\"Return type: Tensor\",\"output_size() → int\",\"Return output length of feature dimension D, i.e. the embedding dim.\"]},\"1962\":{\"h\":\"espnet2.optimizers.sgd.SGD\",\"t\":[\"source\",\"class espnet2.optimizers.sgd.SGD(params, lr: float = 0.1, momentum: float = 0.0, dampening: float = 0.0, weight_decay: float = 0.0, nesterov: bool = False)\",\"Bases: SGD\",\"Thin inheritance of torch.optim.SGD to bind the required arguments, ‘lr’\",\"Note that the arguments of the optimizer invoked by AbsTask.main() must have default value except for ‘param’.\",\"I can’t understand why only SGD.lr doesn’t have the default value.\"]},\"1963\":{\"h\":\"espnet2.optimizers.optim_groups.add_optimizer_hooks\",\"t\":[\"source\",\"espnet2.optimizers.optim_groups.add_optimizer_hooks(model, bias_weight_decay=False, normalization_weight_decay=False)\",\"Set zero weight decay for some params\",\"Set weight_decay=0.0 for parameters in model.no_weight_decay, for parameters with attribute _no_weight_decayTrue, for bias parameters if bias_weight_decayFalse, for normalization parameters if normalization_weight_decay==False\",\"See: https://discuss.pytorch.org/t/weight-decay-only-for-weights-of-nn-linear-and-nn-conv/114348 # noqa\"]},\"1964\":{\"h\":\"espnet2.optimizers.optim_groups.configure_optimizer\",\"t\":[\"source\",\"espnet2.optimizers.optim_groups.configure_optimizer(model, optim_class, optim_conf, weight_decay_conf)\"]},\"1965\":{\"h\":\"espnet2.ps2st.espnet_model.ESPnetQwen2AudioModel\",\"t\":[\"source\",\"class espnet2.ps2st.espnet_model.ESPnetQwen2AudioModel(model_name: str = 'Qwen/Qwen2-Audio-7B-Instruct', vocab_size: int = 50000, token_list: Tuple[str, ...] | List[str] = (), ignore_id: int = -1, decode_config_path: str | None = None, pytest_mode: bool | None = False)\",\"Bases: AbsESPnetModel\",\"ESPnet model integrating Qwen2-Audio from transformers\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"Collect features for statistics computation\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Forward pass required by AbsESPnetModel interface\",\"Returns: Scalar tensor (dummy for inference-only) stats: Dictionary of statistics for logging weight: Batch size for normalization\",\"Return type: loss\",\"inference(input_ids: Tensor, attention_mask: Tensor, input_features: Tensor, feature_attention_mask: Tensor, **kwargs) → str\",\"Custom inference method using Qwen2-Audio\"]},\"1966\":{\"h\":\"espnet2.ps2st.qwen2_scorer.Qwen2HFScorer\",\"t\":[\"source\",\"class espnet2.ps2st.qwen2_scorer.Qwen2HFScorer(model, input_ids, attention_mask, input_features=None, feature_attention_mask=None)\",\"Bases: ScorerInterface\",\"final_score(states)\",\"Score eos (optional).\",\"Parameters:state – Scorer state for prefix tokens\",\"Returns: final score\",\"Return type: float\",\"init_state(xs)\",\"Get an initial state for decoding (optional).\",\"Parameters:x (torch.Tensor) – The encoded feature tensor\",\"Returns: initial state\",\"score(ys, state, xs)\",\"Forward scoring function.\",\"ys: 1D LongTensor of generated tokens so far state: dict with ‘past_kv’ and ‘step’\",\"select_state(states, idx)\",\"Select state with relative ids in the main beam search.\",\"Parameters:\",\"state – Decoder state for prefix tokens\",\"i (int) – Index to select a state in the main beam search\",\"new_id (int) – New label index to select a state if necessary\",\"Returns: pruned state\",\"Return type: state\"]},\"1967\":{\"h\":\"espnet2.s2st.aux_attention.abs_aux_attention.AbsS2STAuxAttention\",\"t\":[\"source\",\"class espnet2.s2st.aux_attention.abs_aux_attention.AbsS2STAuxAttention(*args, **kwargs)\",\"Bases: Module, ABC\",\"Base class for all S2ST auxiliary attention modules.\",\"Refer to https://arxiv.org/abs/2107.08661\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward() → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1968\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"property name : str\"]},\"1969\":{\"h\":\"espnet2.s2st.losses.abs_loss.AbsS2STLoss\",\"t\":[\"source\",\"class espnet2.s2st.losses.abs_loss.AbsS2STLoss(*args, **kwargs)\",\"Bases: Module, ABC\",\"Base class for all S2ST loss modules.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward() → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1970\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"property name : str\"]},\"1971\":{\"h\":\"espnet2.s2st.synthesizer.abs_synthesizer.AbsSynthesizer\",\"t\":[\"source\",\"class espnet2.s2st.synthesizer.abs_synthesizer.AbsSynthesizer(*args, **kwargs)\",\"Bases: Module, ABC\",\"TTS abstract class.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input_states: Tensor, input_states_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate outputs and return the loss tensor.\",\"abstract inference(input_states: Tensor, **kwargs) → Dict[str, Tensor]\",\"Return predicted output as a dict.\",\"property require_raw_speech\",\"Return whether or not raw_speech is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\"]},\"1972\":{\"h\":\"espnet2.s2st.tgt_feats_extract.abs_tgt_feats_extract.AbsTgtFeatsExtract\",\"t\":[\"source\",\"class espnet2.s2st.tgt_feats_extract.abs_tgt_feats_extract.AbsTgtFeatsExtract(*args, **kwargs)\",\"Bases: AbsFeatsExtract, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1973\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract get_parameters() → Dict[str, Any]\",\"abstract output_size() → int\",\"abstract spectrogram() → bool\"]},\"1974\":{\"h\":\"espnet2.s2st.synthesizer.translatotron2.DurationPredictor\",\"t\":[\"source\",\"class espnet2.s2st.synthesizer.translatotron2.DurationPredictor(cfg)\",\"Bases: Module\",\"Non-Attentive Tacotron (NAT) Duration Predictor module.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(encoder_outputs, input_lengths=None)\",\"Forward Duration Predictor\",\"Parameters:\",\"encoder_outputs – [batch_size, hidden_length, encoder_lstm_dim]\",\"input_lengths – [batch_size, hidden_length]\",\"Returns: [batch_size, hidden_length]\"]},\"1975\":{\"h\":\"espnet2.s2st.espnet_model.ESPnetS2STModel\",\"t\":[\"source\",\"class espnet2.s2st.espnet_model.ESPnetS2STModel(s2st_type: str, frontend: AbsFrontend | None, tgt_feats_extract: AbsTgtFeatsExtract | None, specaug: AbsSpecAug | None, src_normalize: AbsNormalize | None, tgt_normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, asr_decoder: AbsDecoder | None, st_decoder: AbsDecoder | None, aux_attention: AbsS2STAuxAttention | None, unit_encoder: AbsEncoder | None, synthesizer: AbsSynthesizer | None, asr_ctc: CTC | None, st_ctc: CTC | None, losses: Dict[str, AbsS2STLoss], tgt_vocab_size: int | None, tgt_token_list: Tuple[str, ...] | List[str] | None, src_vocab_size: int | None, src_token_list: Tuple[str, ...] | List[str] | None, unit_vocab_size: int | None, unit_token_list: Tuple[str, ...] | List[str] | None, ignore_id: int = -1, report_cer: bool = True, report_wer: bool = True, report_bleu: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', extract_feats_in_collect_stats: bool = True)\",\"Bases: AbsESPnetModel\",\"ESPnet speech-to-speech translation model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(src_speech: Tensor, src_speech_lengths: Tensor, tgt_speech: Tensor, tgt_speech_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"encode(speech: Tensor, speech_lengths: Tensor, return_all_hs: bool = False, **kwargs) → Tuple[Tensor, Tensor]\",\"Frontend + Encoder. Note that this method is used by st_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"forward(src_speech: Tensor, src_speech_lengths: Tensor, tgt_speech: Tensor, tgt_speech_lengths: Tensor, tgt_text: Tensor | None = None, tgt_text_lengths: Tensor | None = None, src_text: Tensor | None = None, src_text_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1976\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"inference(src_speech: Tensor, src_speech_lengths: Tensor | None = None, tgt_speech: Tensor | None = None, tgt_speech_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, threshold: float = 0.5, minlenratio: float = 0.0, maxlenratio: float = 10.0, use_att_constraint: bool = False, backward_window: int = 1, forward_window: int = 3, use_teacher_forcing: bool = False) → Dict[str, Tensor]\",\"property require_vocoder\",\"Return whether or not vocoder is required.\"]},\"1977\":{\"h\":\"espnet2.s2st.synthesizer.translatotron2.GaussianUpsampling\",\"t\":[\"source\",\"class espnet2.s2st.synthesizer.translatotron2.GaussianUpsampling\",\"Bases: Module\",\"Gaussian Upsample.\",\"Non-attention Tacotron: : - https://arxiv.org/abs/2010.04301\",\"this source code is implemenation of the ExpressiveTacotron from BridgetteSong : - https://github.com/BridgetteSong/ExpressiveTacotron/\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(encoder_outputs, durations, vars, input_lengths=None)\",\"Gaussian upsampling.\",\"Parameters:\",\"encoder_outputs – encoder outputs [batch_size, hidden_length, dim]\",\"durations – phoneme durations [batch_size, hidden_length]\",\"vars – phoneme attended ranges [batch_size, hidden_length]\",\"input_lengths – [batch_size]\",\"Returns: upsampled encoder_output : [batch_size, frame_length, dim]\",\"Return type: encoder_upsampling_outputs\",\"get_mask_from_lengths(lengths, max_len=None)\"]},\"1978\":{\"h\":\"espnet2.s2st.tgt_feats_extract.linear_spectrogram.LinearSpectrogram\",\"t\":[\"source\",\"class espnet2.s2st.tgt_feats_extract.linear_spectrogram.LinearSpectrogram(n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True)\",\"Bases: AbsTgtFeatsExtract\",\"Linear amplitude spectrogram.\",\"Stft -> amplitude-spec\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1979\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters() → Dict[str, Any]\",\"Return the parameters required by Vocoder.\",\"output_size() → int\",\"spectrogram() → bool\"]},\"1980\":{\"h\":\"espnet2.s2st.tgt_feats_extract.log_mel_fbank.LogMelFbank\",\"t\":[\"source\",\"class espnet2.s2st.tgt_feats_extract.log_mel_fbank.LogMelFbank(fs: int | str = 16000, n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True, n_mels: int = 80, fmin: int | None = 80, fmax: int | None = 7600, htk: bool = False, log_base: float | None = 10.0)\",\"Bases: AbsTgtFeatsExtract\",\"Conventional frontend structure for TTS.\",\"Stft -> amplitude-spec -> Log-Mel-Fbank\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1981\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters() → Dict[str, Any]\",\"Return the parameters required by Vocoder\",\"output_size() → int\",\"spectrogram() → bool\"]},\"1982\":{\"h\":\"espnet2.s2st.tgt_feats_extract.log_spectrogram.LogSpectrogram\",\"t\":[\"source\",\"class espnet2.s2st.tgt_feats_extract.log_spectrogram.LogSpectrogram(n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True)\",\"Bases: AbsTgtFeatsExtract\",\"Conventional frontend structure for ASR\",\"Stft -> log-amplitude-spec\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1983\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters() → Dict[str, Any]\",\"Return the parameters required by Vocoder\",\"output_size() → int\",\"spectrogram() → bool\"]},\"1984\":{\"h\":\"espnet2.s2st.aux_attention.multihead.MultiHeadAttention\",\"t\":[\"source\",\"class espnet2.s2st.aux_attention.multihead.MultiHeadAttention(n_head: int = 4, n_feat: int = 512, dropout_rate: float = 0.0)\",\"Bases: AbsS2STAuxAttention\",\"Multihead Attention for S2ST.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(query: Tensor, key: Tensor, value: Tensor, mask: Tensor)\",\"Forward.\",\"Parameters:\",\"query (torch.Tensor) – Query tensor (#batch, time1, size).\",\"key (torch.Tensor) – Key tensor (#batch, time2, size).\",\"value (torch.Tensor) – Value tensor (#batch, time2, size).\",\"mask (torch.Tensor) – Mask tensor (#batch, 1, time2) or (#batch, time1, time2).\",\"Returns: Output tensor (#batch, time1, d_model).\",\"Return type: torch.Tensor\"]},\"1985\":{\"h\":\"espnet2.s2st.synthesizer.translatotron2.Prenet\",\"t\":[\"source\",\"class espnet2.s2st.synthesizer.translatotron2.Prenet(idim, units=128, num_layers=2, dropout=0.5)\",\"Bases: Module\",\"Non-Attentive Tacotron (NAT) Prenet.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1986\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1987\":{\"h\":\"espnet2.s2st.losses.attention_loss.S2STAttentionLoss\",\"t\":[\"source\",\"class espnet2.s2st.losses.attention_loss.S2STAttentionLoss(vocab_size: int, padding_idx: int = -1, weight: float = 1.0, smoothing: float = 0.0, normalize_length: str2bool = False, criterion: Module = KLDivLoss())\",\"Bases: AbsS2STLoss\",\"attention-based label smoothing loss for S2ST.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(dense_y: Tensor, token_y: Tensor)\",\"Forward.\",\"Args:\"]},\"1988\":{\"h\":\"espnet2.s2st.losses.ctc_loss.S2STCTCLoss\",\"t\":[\"source\",\"class espnet2.s2st.losses.ctc_loss.S2STCTCLoss(weight: float = 1.0)\",\"Bases: AbsS2STLoss\",\"CTC-based loss for S2ST.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward()\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"1989\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"1990\":{\"h\":\"espnet2.s2st.losses.guided_attention_loss.S2STGuidedAttentionLoss\",\"t\":[\"source\",\"class espnet2.s2st.losses.guided_attention_loss.S2STGuidedAttentionLoss(weight: float = 1.0, sigma: float = 0.4, alpha: float = 1.0)\",\"Bases: AbsS2STLoss\",\"Tacotron-based loss for S2ST.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(att_ws: Tensor, ilens: Tensor, olens_in: Tensor)\",\"Forward.\",\"Args:\",\"Returns: guided attention loss\",\"Return type: Tensor\"]},\"1991\":{\"h\":\"espnet2.s2st.losses.tacotron_loss.S2STTacotron2Loss\",\"t\":[\"source\",\"class espnet2.s2st.losses.tacotron_loss.S2STTacotron2Loss(weight: float = 1.0, loss_type: str = 'L1+L2', use_masking: str2bool = True, use_weighted_masking: str2bool = False, bce_pos_weight: float = 20.0)\",\"Bases: AbsS2STLoss\",\"Tacotron-based loss for S2ST.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(after_outs: Tensor, before_outs: Tensor, logits: Tensor, ys: Tensor, labels: Tensor, olens: Tensor)\",\"Forward.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, Lmax, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, Lmax, odim).\",\"logits (Tensor) – Batch of stop logits (B, Lmax).\",\"ys (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"labels (LongTensor) – Batch of the sequences of stop token labels (B, Lmax).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: L1 loss value. Tensor: Mean square error loss value. Tensor: Binary cross entropy loss value.\",\"Return type: Tensor\"]},\"1992\":{\"h\":\"espnet2.s2st.synthesizer.discrete_synthesizer.TransformerDiscreteSynthesizer\",\"t\":[\"source\",\"class espnet2.s2st.synthesizer.discrete_synthesizer.TransformerDiscreteSynthesizer(odim: int, idim: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, layer_drop_rate: float = 0.0, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat')\",\"Bases: AbsSynthesizer, BatchScorerInterface\",\"Discrete unit Synthesizer related modules for speech-to-speech translation.\",\"This is a module of discrete unit prediction network in discrete-unit described in Direct speech-to-speech translation with discrete units, which converts the sequence of hidden states into the sequence of discrete unit (from SSLs).\",\"Transfomer decoder for discrete unit module.\",\"Parameters:\",\"vocab_size – output dim\",\"encoder_output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of decoder blocks\",\"dropout_rate – dropout rate\",\"self_attention_dropout_rate – dropout rate for attention\",\"input_layer – input layer type\",\"use_output_layer – whether to use output layer\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"batch_score(ys: Tensor, states: List[Any], xs: Tensor) → Tuple[Tensor, List[Any]]\",\"Score new token batch.\",\"Parameters:\",\"ys (torch.Tensor) – torch.int64 prefix tokens (n_batch, ylen).\",\"states (List *[*Any]) – Scorer states for prefix tokens.\",\"xs (torch.Tensor) – The encoder feature that generates ys (n_batch, xlen, n_feat).\",\"Returns: Tuple of : batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys.\",\"Return type: tuple[torch.Tensor, List[Any]]\",\"forward(enc_outputs: Tensor, enc_outputs_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, return_hs: bool = False, return_all_hs: bool = False) → Tuple[Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"enc_outputs (LongTensor) – Batch of padded character ids (B, T, idim).\",\"enc_outputs_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"Returns: hs hlens\",\"forward_one_step(tgt: Tensor, tgt_mask: Tensor, memory: Tensor, cache: List[Tensor] | None = None, **kwargs) → Tuple[Tensor, List[Tensor]]\",\"Forward one step.\",\"Parameters:\",\"tgt – input token ids, int64 (batch, maxlen_out)\",\"tgt_mask – input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2)\",\"memory – encoded memory, float32 (batch, maxlen_in, feat)\",\"cache – cached output list of (batch, max_time_out-1, size)\",\"Returns: NN output value and cache per self.decoders. y.shape` is (batch, maxlen_out, token)\",\"Return type: y, cache\",\"inference()\",\"Return predicted output as a dict.\",\"score(ys, state, x)\",\"Score.\"]},\"1993\":{\"h\":\"espnet2.s2st.synthesizer.translatotron.Translatotron\",\"t\":[\"source\",\"class espnet2.s2st.synthesizer.translatotron.Translatotron(idim: int, odim: int, embed_dim: int = 512, atype: str = 'multihead', adim: int = 512, aheads: int = 4, aconv_chans: int = 32, aconv_filts: int = 15, cumulate_att_w: bool = True, dlayers: int = 4, dunits: int = 1024, prenet_layers: int = 2, prenet_units: int = 32, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, output_activation: str | None = None, use_batch_norm: bool = True, use_concate: bool = True, use_residual: bool = False, reduction_factor: int = 2, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat', dropout_rate: float = 0.5, zoneout_rate: float = 0.1)\",\"Bases: AbsSynthesizer\",\"TTranslatotron Synthesizer related modules for speech-to-speech translation.\",\"This is a module of Spectrogram prediction network in Translatotron described in Direct speech-to-speech translation with a sequence-to-sequence model, which converts the sequence of hidden states into the sequence of Mel-filterbanks.\",\"Initialize Tacotron2 module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim – (int) Dimension of the outputs.\",\"adim (int) – Number of dimension of mlp in attention.\",\"atype (str) – type of attention\",\"aconv_chans (int) – Number of attention conv filter channels.\",\"aconv_filts (int) – Number of attention conv filter size.\",\"embed_dim (int) – Dimension of the token embedding.\",\"dlayers (int) – Number of decoder lstm layers.\",\"dunits (int) – Number of decoder lstm units.\",\"prenet_layers (int) – Number of prenet layers.\",\"prenet_units (int) – Number of prenet units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_filts (int) – Number of postnet filter size.\",\"postnet_chans (int) – Number of postnet filter channels.\",\"output_activation (str) – Name of activation function for outputs.\",\"cumulate_att_w (bool) – Whether to cumulate previous attention weight.\",\"use_batch_norm (bool) – Whether to use batch normalization.\",\"use_concate (bool) – Whether to concat enc outputs w/ dec lstm outputs.\",\"reduction_factor (int) – Reduction factor.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"dropout_rate (float) – Dropout rate.\",\"zoneout_rate (float) – Zoneout rate.\",\"forward(enc_outputs: Tensor, enc_outputs_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"enc_outputs (LongTensor) – Batch of padded character ids (B, T, idim).\",\"enc_outputs_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"Returns: after_outs (TODO(jiatong) add full comment) before_outs (TODO(jiatong) add full comments) logits att_ws ys stop_labels olens\",\"inference(enc_outputs: Tensor, feats: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, threshold: float = 0.5, minlenratio: float = 0.0, maxlenratio: float = 10.0, use_att_constraint: bool = False, backward_window: int = 1, forward_window: int = 3, use_teacher_forcing: bool = False) → Dict[str, Tensor]\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"enc_outputs (LongTensor) – Input sequence of characters (N, idim).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style (N, odim).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"threshold (float) – Threshold in inference.\",\"minlenratio (float) – Minimum length ratio in inference.\",\"maxlenratio (float) – Maximum length ratio in inference.\",\"use_att_constraint (bool) – Whether to apply attention constraint.\",\"backward_window (int) – Backward window in attention constraint.\",\"forward_window (int) – Forward window in attention constraint.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"prob (Tensor): Output sequence of stop probabilities (T_feats,).\",\"att_w (Tensor): Attention weights (T_feats, T).\",\"Return type: Dict[str, Tensor]\"]},\"1994\":{\"h\":\"espnet2.s2st.synthesizer.translatotron2.Translatotron2\",\"t\":[\"source\",\"class espnet2.s2st.synthesizer.translatotron2.Translatotron2(idim: int, odim: int, synthesizer_type: str = 'rnn', layers: int = 2, units: int = 1024, prenet_layers: int = 2, prenet_units: int = 128, prenet_dropout_rate: float = 0.5, postnet_layers: int = 5, postnet_chans: int = 512, postnet_dropout_rate: float = 0.5, adim: int = 384, aheads: int = 4, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, duration_predictor_layers: int = 2, duration_predictor_type: str = 'rnn', duration_predictor_units: int = 128, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False)\",\"Bases: AbsSynthesizer\",\"Translatotron2 module.\",\"This is a module of the synthesizer in Translatotron2 described in\",\"`Translatotron 2: High-quality direct speech-to-speech translation with voice preservation`_\",\".\",\"High-quality direct speech-to-speech translation with voice preservation`: : https://arxiv.org/pdf/2107.08661v5.pdf\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"]},\"1995\":{\"h\":\"espnet2.s2st.synthesizer.unity_synthesizer.UnitYSynthesizer\",\"t\":[\"source\",\"class espnet2.s2st.synthesizer.unity_synthesizer.UnitYSynthesizer(vocab_size: int, encoder_output_size: int, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, self_attention_dropout_rate: float = 0.0, src_attention_dropout_rate: float = 0.0, input_layer: str = 'embed', use_output_layer: bool = True, pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, layer_drop_rate: float = 0.0, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat')\",\"Bases: AbsSynthesizer\",\"UnitY Synthesizer related modules for speech-to-speech translation.\",\"This is a module of discrete unit prediction network in discrete-unit described in\",\"`Direct speech-to-speech translation with discrete units`_\",\", which converts the sequence of hidden states into the sequence of discrete unit (from SSLs).\",\"Transfomer decoder for discrete unit module.\",\"Parameters:\",\"vocab_size – output dim\",\"encoder_output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of decoder blocks\",\"dropout_rate – dropout rate\",\"self_attention_dropout_rate – dropout rate for attention\",\"input_layer – input layer type\",\"use_output_layer – whether to use output layer\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"forward(enc_outputs: Tensor, enc_outputs_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, return_last_hidden: bool = False, return_all_hiddens: bool = False) → Tuple[Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"enc_outputs (LongTensor) – Batch of padded character ids (B, T, idim).\",\"enc_outputs_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"Returns: hs hlens\"]},\"1996\":{\"h\":\"espnet2.s2t.espnet_ctc_model.ESPnetS2TCTCModel\",\"t\":[\"source\",\"class espnet2.s2t.espnet_ctc_model.ESPnetS2TCTCModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, encoder: AbsEncoder, prompt_encoder: AbsEncoder, ctc: CTC, interctc_weight: float = 0.0, ignore_id: int = -1, report_cer: bool = True, report_wer: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', sym_sos: str = '<sos>', sym_eos: str = '<eos>', sym_sop: str = '<sop>', sym_na: str = '<na>', extract_feats_in_collect_stats: bool = True, ctc_asr_only: List[bool] = [False])\",\"Bases: AbsESPnetModel\",\"OWSM-CTC model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, text_prev: Tensor, text_prev_lengths: Tensor, text_ctc: Tensor, text_ctc_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"encode(speech: Tensor, speech_lengths: Tensor, text_prev: Tensor, text_prev_lengths: Tensor, prefix: Tensor, prefix_lengths: Tensor)\",\"Encode input speech.\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, text_prev: Tensor, text_prev_lengths: Tensor, text_ctc: Tensor, text_ctc_lengths: Tensor, prefix: Tensor, prefix_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"text_prev – (Batch, Length)\",\"text_prev_lengths – (Batch,)\",\"text_ctc – (Batch, Length)\",\"text_ctc_lengths – (Batch,)\",\"prefix – (Batch, Length=2), <lang> and <task>\",\"prefix_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\"]},\"1997\":{\"h\":\"espnet2.s2t.espnet_model.ESPnetS2TModel\",\"t\":[\"source\",\"class espnet2.s2t.espnet_model.ESPnetS2TModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, decoder: AbsDecoder | None, ctc: CTC, ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', sym_sos: str = '<sos>', sym_eos: str = '<eos>', sym_sop: str = '<sop>', sym_na: str = '<na>', extract_feats_in_collect_stats: bool = True)\",\"Bases: AbsESPnetModel\",\"CTC-attention hybrid Encoder-Decoder model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, text_prev: Tensor, text_prev_lengths: Tensor, text_ctc: Tensor, text_ctc_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"encode(speech: Tensor, speech_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Frontend + Encoder. Note that this method is used by s2t_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"forced_align(speech, speech_lengths, text, text_lengths)\",\"Calculate frame-wise alignment from CTC probabilities.\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch,)\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"Returns: Tuple(tensor, tensor): : - Label for each time step in the alignment path <br/> computed using forced alignment. \",\"Log probability scores of the labels for each time step.\",\"Return type: alignments\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, text_prev: Tensor, text_prev_lengths: Tensor, text_ctc: Tensor, text_ctc_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"text_prev – (Batch, Length)\",\"text_prev_lengths – (Batch,)\",\"text_ctc – (Batch, Length)\",\"text_ctc_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\"]},\"1998\":{\"h\":\"espnet2.samplers.abs_sampler.AbsSampler\",\"t\":[\"source\",\"class espnet2.samplers.abs_sampler.AbsSampler(data_source: Sized | None = None)\",\"Bases: Sampler, ABC\",\"generate(seed)\"]},\"1999\":{\"h\":\"espnet2.samplers.category_balanced_sampler.CategoryBalancedSampler\",\"t\":[\"source\",\"class espnet2.samplers.category_balanced_sampler.CategoryBalancedSampler(batch_size: int, min_batch_size: int = 1, drop_last: bool = False, category2utt_file: str | None = None, epoch: int = 1, **kwargs)\",\"Bases: AbsSampler\"]},\"2000\":{\"h\":\"espnet2.samplers.category_power_sampler.CategoryDatasetPowerSampler\",\"t\":[\"source\",\"class espnet2.samplers.category_power_sampler.CategoryDatasetPowerSampler(batch_bins: int, shape_files: Tuple[str, ...] | List[str], min_batch_size: int = 1, max_batch_size: int | None = None, category_upsampling_factor: float = 1.0, dataset_upsampling_factor: float = 1.0, dataset_scaling_factor: float = 1.2, drop_last: bool = False, category2utt_file: str | None = None, dataset2utt_file: str | None = None, utt2dataset_file: str | None = None, epoch: int = 1, **kwargs)\",\"Bases: AbsSampler\",\"A category- and dataset-balanced batch sampler with power-law sampling.\",\"Reference: : Scaling Speech Technology to 1,000+ Languages https://arxiv.org/pdf/2305.13516\",\"This sampler is designed for multi-category, multi-dataset training where both category imbalance and dataset imbalance exist. It performs hierarchical sampling: (1) balancing categories (e.g., languages) within each dataset, and (2) balancing datasets themselves.\",\"Sampling Strategy:\",\"Let:\",\"d ∈ {1, 2, …, D} denote the dataset index\",\"l ∈ {1, 2, …, L_d} denote the category index in dataset d\",\"n_ld: total duration (number of bins) of category l in dataset d\",\"k_ld: the number of utterances in category l in dataset d\",\"N_d = ∑_l n_ld: total duration (number of bins) of all categories\",\"in dataset d\",\"M = ∑_d N_d: total duration (number of bins) of all categories across : all datasets\",\"Step 1 — Category-level sampling within each dataset: : P(l | d) ∝ (n_ld / N_d)^β_L\",\"where β_L (category_upsampling_factor) controls how strongly to upsample low-resource languages within each dataset. The normalized probability becomes:\",\"P(l | d) = [(n_ld / N_d)^β_L] / ∑_l’[(n_l’d / N_d)^β_L]\",\"Step 2 — Dataset-level sampling based on resampled language distributions:\",\"For each dataset d, the resampled number of bins for category l is: : n_ld’ = N_d × P(l | d)\",\"Since the category probabilities sum to 1 within each dataset (∑_l P(l | d) = 1), the total resampled bins (N_d’) for dataset d is:\",\"N_d’ = ∑_l n_ld’ = N_d\",\"The probability of sampling dataset d is then: : P(d) = [(N_d / M)^β_D] / ∑_d[(N_d / M)^β_D]\",\"where:\",\"β_D is dataset_upsampling_factor\",\"Final utterance sampling probability: : P(x) = P(d) × P(l | d) × P(x | l, d), where P(x | l, d) = 1 / k_ld\",\"Note:\",\"Batches are constructed based on batch_bins, similar to LengthBatchSampler.\",\"Set batch_type=catpow_balance_dataset to enable this sampler.\",\"This sampler is particularly useful when combining heterogeneous datasets (e.g., FLEURS + VoxLingua107 + BABEL) with highly imbalanced language and size distributions.\",\"Parameters:\",\"batch_bins – The approximate maximum number of bins (e.g., audio samples) in a batch.\",\"shape_files – A list or tuple of shape file paths. Only one shape file is supported, but the list format is retained for compatibility with other samplers.\",\"min_batch_size – Minimum number of utterances in a batch.\",\"max_batch_size – Maximum number of utterances in a batch (recommended for memory safety).\",\"category_upsampling_factor – β_L in the formula; controls per-dataset category balancing.\",\"dataset_upsampling_factor – β_D in the formula; controls balancing between datasets.\",\"dataset_scaling_factor – A multiplier that determines the total number of utterances sampled. Values > 1 simulate more frequent use of low-resource utterances across batches. Must be ≥ 1.\",\"drop_last – Whether to drop the final batch.\",\"category2utt_file – Path to a file mapping each category to utterance ID.\",\"dataset2utt_file – Path to a file mapping each dataset to utterance ID.\",\"utt2dataset_file – Path to a file mapping each utterance ID to its corresponding dataset label.\",\"epoch – Random seed is set using the epoch to ensure reproducibility with variation across epochs.\"]},\"2001\":{\"h\":\"espnet2.samplers.category_power_sampler.CategoryPowerSampler\",\"t\":[\"source\",\"class espnet2.samplers.category_power_sampler.CategoryPowerSampler(batch_bins: int, shape_files: Tuple[str, ...] | List[str], min_batch_size: int = 1, max_batch_size: int | None = None, upsampling_factor: float = 1.0, dataset_scaling_factor: float = 1.2, drop_last: bool = False, category2utt_file: str | None = None, epoch: int = 1, **kwargs)\",\"Bases: AbsSampler\",\"A category-balanced batch sampler with power-law sampling.\",\"Reference: : Scaling Speech Technology to 1,000+ Languages https://arxiv.org/pdf/2305.13516\",\"This sampler constructs mini-batches by balancing samples across categories (e.g., language IDs), using a power-law distribution to control the sampling frequency. Originally developed for language identification, it can be applied to any dataset that provides a mapping from category (e.g., language) to utterances.\",\"Sampling Strategy:\",\"Given:\",\"l ∈ {1, 2, …, L}, the set of category labels\",\"n_l: total duration (number of bins) of category l\",\"N: total duration (number of bins) of all categories in the dataset\",\"β: upsampling factor\",\"k_l: the number of utterances in category l\",\"We define:\",\"Category-level sampling probability:\",\"P(l) = (n_l / N)^β\",\"Utterance-level conditional sampling: : P(x | l) = 1 / k_l\",\"Combined sampling probability: : P(x) = P(l) * P(x | l) = (n_l / N)^β * (1 / k_l)\",\"Where β ∈ [0, 1] is the upsampling_factor:\",\"β → 0 emphasizes low-resource categories (strong upsampling)\",\"β → 1 approximates uniform sampling over all utterances\",\"Note:\",\"Batches are constructed based on batch_bins, similar to LengthBatchSampler.\",\"Set batch_type=catpow in your configuration to use this sampler.\",\"Parameters:\",\"batch_bins – The approximate maximum number of bins (e.g., audio samples) in a batch.\",\"shape_files – A list or tuple of shape file paths. Only one shape file is supported, but the list format is retained for compatibility with other samplers.\",\"min_batch_size – Minimum number of utterances in a batch.\",\"max_batch_size – Maximum number of utterances in a batch (recommended for memory safety).\",\"upsampling_factor – β in the sampling formula; controls how strongly to upsample low-resource categories.\",\"dataset_scaling_factor – A multiplier that determines the total number of utterances sampled. Values > 1 simulate more frequent use of low-resource utterances across batches. Must be ≥ 1.\",\"drop_last – Whether to drop the final batch.\",\"category2utt_file – Path to a file mapping each category to utterance ID.\",\"epoch – Random seed is set using the epoch to ensure reproducibility with variation across epochs.\"]},\"2002\":{\"h\":\"espnet2.samplers.folded_batch_sampler.FoldedBatchSampler\",\"t\":[\"source\",\"class espnet2.samplers.folded_batch_sampler.FoldedBatchSampler(batch_size: int, shape_files: Tuple[str, ...] | List[str], fold_lengths: Sequence[int], min_batch_size: int = 1, sort_in_batch: str = 'descending', sort_batch: str = 'ascending', drop_last: bool = False, utt2category_file: str | None = None)\",\"Bases: AbsSampler\"]},\"2003\":{\"h\":\"espnet2.samplers.length_batch_sampler.LengthBatchSampler\",\"t\":[\"source\",\"class espnet2.samplers.length_batch_sampler.LengthBatchSampler(batch_bins: int, shape_files: Tuple[str, ...] | List[str], min_batch_size: int = 1, sort_in_batch: str = 'descending', sort_batch: str = 'ascending', drop_last: bool = False, padding: bool = True)\",\"Bases: AbsSampler\"]},\"2004\":{\"h\":\"espnet2.samplers.num_elements_batch_sampler.NumElementsBatchSampler\",\"t\":[\"source\",\"class espnet2.samplers.num_elements_batch_sampler.NumElementsBatchSampler(batch_bins: int, shape_files: Tuple[str, ...] | List[str], min_batch_size: int = 1, sort_in_batch: str = 'descending', sort_batch: str = 'ascending', drop_last: bool = False, padding: bool = True)\",\"Bases: AbsSampler\"]},\"2005\":{\"h\":\"espnet2.samplers.sorted_batch_sampler.SortedBatchSampler\",\"t\":[\"source\",\"class espnet2.samplers.sorted_batch_sampler.SortedBatchSampler(batch_size: int, shape_file: str, sort_in_batch: str = 'descending', sort_batch: str = 'ascending', drop_last: bool = False)\",\"Bases: AbsSampler\",\"BatchSampler with sorted samples by length.\",\"Parameters:\",\"batch_size\",\"shape_file\",\"sort_in_batch – ‘descending’, ‘ascending’ or None.\",\"sort_batch\"]},\"2006\":{\"h\":\"espnet2.samplers.unsorted_batch_sampler.UnsortedBatchSampler\",\"t\":[\"source\",\"class espnet2.samplers.unsorted_batch_sampler.UnsortedBatchSampler(batch_size: int, key_file: str, drop_last: bool = False, utt2category_file: str | None = None)\",\"Bases: AbsSampler\",\"BatchSampler with constant batch-size.\",\"Any sorting is not done in this class, so no length information is required, This class is convenient for decoding mode, or not seq2seq learning e.g. classification.\",\"Parameters:\",\"batch_size\",\"key_file\"]},\"2007\":{\"h\":\"espnet2.samplers.build_batch_sampler.build_batch_sampler\",\"t\":[\"source\",\"espnet2.samplers.build_batch_sampler.build_batch_sampler(type: str, batch_size: int, batch_bins: int, shape_files: Tuple[str, ...] | List[str], sort_in_batch: str = 'descending', sort_batch: str = 'ascending', drop_last: bool = False, min_batch_size: int = 1, fold_lengths: Sequence[int] = (), padding: bool = True, utt2category_file: str | None = None) → AbsSampler\",\"Helper function to instantiate BatchSampler.\",\"Parameters:\",\"type – mini-batch type. “unsorted”, “sorted”, “folded”, “numel”, “length”, or “catbel”\",\"batch_size – The mini-batch size. Used for “unsorted”, “sorted”, “folded”, “catbel” mode\",\"batch_bins – Used for “numel” model\",\"shape_files – Text files describing the length and dimension of each features. e.g. uttA 1330,80\",\"sort_in_batch\",\"sort_batch\",\"drop_last\",\"min_batch_size – Used for “numel” or “folded” mode\",\"fold_lengths – Used for “folded” mode\",\"padding – Whether sequences are input as a padded tensor or not. used for “numel” mode\"]},\"2008\":{\"h\":\"espnet2.samplers.build_batch_sampler.build_category_batch_sampler\",\"t\":[\"source\",\"espnet2.samplers.build_batch_sampler.build_category_batch_sampler(type: str, batch_size: int | None = None, batch_bins: int | None = None, shape_files: Tuple[str, ...] | List[str] | None = None, min_batch_size: int | None = None, max_batch_size: int | None = None, upsampling_factor: float | None = None, category_upsampling_factor: float | None = None, dataset_upsampling_factor: float | None = None, dataset_scaling_factor: float | None = None, drop_last: bool = False, category2utt_file: str | None = None, dataset2utt_parent_dir: str | None = None, epoch: int = 1, num_batches: int | None = None, distributed: bool = False) → Tuple[AbsSampler, dict]\"]},\"2009\":{\"h\":\"espnet2.samplers.category_balanced_sampler.round_down\",\"t\":[\"source\",\"espnet2.samplers.category_balanced_sampler.round_down(num, divisor)\"]},\"2010\":{\"h\":\"espnet2.schedulers.abs_scheduler.AbsBatchStepScheduler\",\"t\":[\"source\",\"class espnet2.schedulers.abs_scheduler.AbsBatchStepScheduler\",\"Bases: AbsScheduler\",\"abstract load_state_dict(state)\",\"abstract state_dict()\",\"abstract step(epoch: int | None = None)\"]},\"2011\":{\"h\":\"espnet2.schedulers.abs_scheduler.AbsEpochStepScheduler\",\"t\":[\"source\",\"class espnet2.schedulers.abs_scheduler.AbsEpochStepScheduler\",\"Bases: AbsScheduler\",\"abstract load_state_dict(state)\",\"abstract state_dict()\",\"abstract step(epoch: int | None = None)\"]},\"2012\":{\"h\":\"espnet2.schedulers.abs_scheduler.AbsScheduler\",\"t\":[\"source\",\"class espnet2.schedulers.abs_scheduler.AbsScheduler\",\"Bases: ABC\",\"abstract load_state_dict(state)\",\"abstract state_dict()\",\"abstract step(epoch: int | None = None)\"]},\"2013\":{\"h\":\"espnet2.schedulers.abs_scheduler.AbsValEpochStepScheduler\",\"t\":[\"source\",\"class espnet2.schedulers.abs_scheduler.AbsValEpochStepScheduler\",\"Bases: AbsEpochStepScheduler\",\"abstract load_state_dict(state)\",\"abstract state_dict()\",\"abstract step(val, epoch: int | None = None)\"]},\"2014\":{\"h\":\"espnet2.schedulers.cosine_anneal_warmup_restart.CosineAnnealingWarmupRestarts\",\"t\":[\"source\",\"class espnet2.schedulers.cosine_anneal_warmup_restart.CosineAnnealingWarmupRestarts(optimizer: Optimizer, first_cycle_steps: int, cycle_mult: float = 1.0, max_lr: float = 0.1, min_lr: float = 0.001, warmup_steps: int = 0, gamma: float = 1.0, last_epoch: int = -1)\",\"Bases: _LRScheduler, AbsBatchStepScheduler\",\"Cosine Annealing Warmup Restart.\",\"optimizer (Optimizer): Wrapped optimizer. first_cycle_steps (int): First cycle step size. cycle_mult(float): Cycle steps magnification. Default: -1. max_lr(float): First cycle’s max learning rate. Default: 0.1. min_lr(float): Min learning rate. Default: 0.001. warmup_steps(int): Linear warmup step size. Default: 0. gamma(float): Decrease rate of max learning rate by cycle. Default: 1. last_epoch (int): The index of last epoch. Default: -1.\",\"get_lr()\",\"Compute learning rate using chainable form of the scheduler.\",\"init_lr()\",\"step(epoch=None)\",\"Perform a step.\"]},\"2015\":{\"h\":\"espnet2.schedulers.exponential_decay_warmup.ExponentialDecayWarmup\",\"t\":[\"source\",\"class espnet2.schedulers.exponential_decay_warmup.ExponentialDecayWarmup(optimizer: Optimizer, max_lr: float, min_lr: float, total_steps: int, warmup_steps: int = 0, warm_from_zero: bool = False, last_epoch: int = -1)\",\"Bases: _LRScheduler, AbsBatchStepScheduler\",\"Exponential Decay with Warmup.\",\"if step < warmup_steps: : if warm_from_zero: : lr = initial_lr * (step / warmup_steps) else: : lr = initial_lr <br/> else: : decay_factor = (step - warmup_steps) / (total_steps - warmup_steps) lr = initial_lr * exp(decay_factor * log(final_lr / initial_lr))\",\"Parameters:\",\"optimizer (Optimizer) – Wrapped optimizer.\",\"max_lr (float) – Initial learning rate (before decay).\",\"min_lr (float) – Final learning rate (after decay).\",\"total_steps (int) – Total number of steps (epochs * iters per epoch).\",\"warmup_steps (int) – Number of warmup steps. Default: 0.\",\"warm_from_zero (bool) – If True, warmup starts from 0 to initial_lr.\",\"last_epoch (int) – The index of last step. Default: -1.\",\"get_lr()\",\"Compute learning rate using chainable form of the scheduler.\",\"init_lr()\",\"step(epoch: int | None = None)\",\"Perform a step.\"]},\"2016\":{\"h\":\"espnet2.schedulers.noam_lr.NoamLR\",\"t\":[\"source\",\"class espnet2.schedulers.noam_lr.NoamLR(optimizer: Optimizer, model_size: int | float = 320, warmup_steps: int | float = 25000, last_epoch: int = -1)\",\"Bases: _LRScheduler, AbsBatchStepScheduler\",\"The LR scheduler proposed by Noam\",\"Ref: : “Attention Is All You Need”, https://arxiv.org/pdf/1706.03762.pdf\",\"FIXME(kamo): PyTorch doesn’t provide _LRScheduler as public class, : thus the behaviour isn’t guaranteed at forward PyTorch version.\",\"NOTE(kamo): The “model_size” in original implementation is derived from : the model, but in this implementation, this parameter is a constant value. You need to change it if the model is changed.\",\"get_lr()\",\"Compute learning rate using chainable form of the scheduler.\",\"lr_for_WarmupLR(lr: float) → float\"]},\"2017\":{\"h\":\"espnet2.schedulers.piecewise_linear_warmup_lr.PiecewiseLinearWarmupLR\",\"t\":[\"source\",\"class espnet2.schedulers.piecewise_linear_warmup_lr.PiecewiseLinearWarmupLR(optimizer: Optimizer, warmup_steps_list: List[float | int] = [0, 25000], warmup_lr_list: List[float] = [0.0, 0.001], last_epoch: int = -1)\",\"Bases: _LRScheduler, AbsBatchStepScheduler\",\"The PiecewiseLinearWarmupLR scheduler\",\"This scheduler is similar to WarmupLR Scheduler except that the warmup stage is piecewise linear.\",\"get_lr()\",\"Compute learning rate using chainable form of the scheduler.\"]},\"2018\":{\"h\":\"espnet2.schedulers.tristage_lr.TristageLR\",\"t\":[\"source\",\"class espnet2.schedulers.tristage_lr.TristageLR(optimizer: Optimizer, max_steps: int | float = 25000, warmup_ratio: float = 0.1, hold_ratio: float = 0.4, decay_ratio: float = 0.5, init_lr_scale: float = 0.01, final_lr_scale: float = 0.01, last_epoch: int = -1)\",\"Bases: _LRScheduler, AbsBatchStepScheduler\",\"Tri-stage learning rate scheduler with warmup, hold, and exponential decay.\",\"This scheduler adjusts the learning rate in three phases: : 1. Warmup: The learning rate increases linearly from init_lr_scale * base_lr to base_lr over the first warmup_ratio * max_steps steps. 2. Hold: The learning rate is held constant at base_lr for hold_ratio * max_steps steps. 3. Decay: The learning rate decays exponentially from base_lr to final_lr_scale * base_lr over decay_ratio * max_steps steps.\",\"Reference: : Adapted from the tri-stage LR scheduler in fairseq: https://github.com/facebookresearch/fairseq/blob/main/fairseq/ optim/lr_scheduler/tri_stage_lr_scheduler.py\",\"Parameters:\",\"optimizer – Wrapped optimizer.\",\"max_steps – Total number of steps.\",\"warmup_ratio – Fraction of steps for linear warmup.\",\"hold_ratio – Fraction of steps to hold constant.\",\"decay_ratio – Fraction of steps for exponential decay.\",\"init_lr_scale – Initial learning rate is init_lr_scale * base_lr.\",\"final_lr_scale – Final learning rate is final_lr_scale * base_lr.\",\"last_epoch – The index of the last step. Default is -1 (fresh start).\",\"get_lr()\",\"Compute learning rate using chainable form of the scheduler.\"]},\"2019\":{\"h\":\"espnet2.schedulers.warmup_lr.WarmupLR\",\"t\":[\"source\",\"class espnet2.schedulers.warmup_lr.WarmupLR(optimizer: Optimizer, warmup_steps: int | float = 25000, last_epoch: int = -1)\",\"Bases: _LRScheduler, AbsBatchStepScheduler\",\"The WarmupLR scheduler\",\"This scheduler is almost same as NoamLR Scheduler except for following difference:\",\"NoamLR: : lr = optimizer.lr * model_size ** -0.5 : * min(step ** -0.5, step * warmup_step ** -1.5)\",\"WarmupLR: : lr = optimizer.lr * warmup_step ** 0.5 : * min(step ** -0.5, step * warmup_step ** -1.5)\",\"Note that the maximum lr equals to optimizer.lr in this scheduler.\",\"get_lr()\",\"Compute learning rate using chainable form of the scheduler.\"]},\"2020\":{\"h\":\"espnet2.schedulers.warmup_reducelronplateau.WarmupReduceLROnPlateau\",\"t\":[\"source\",\"class espnet2.schedulers.warmup_reducelronplateau.WarmupReduceLROnPlateau(optimizer: Optimizer, warmup_steps: int | float = 25000, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\",\"Bases: AbsBatchStepScheduler, AbsValEpochStepScheduler\",\"The WarmupReduceLROnPlateau scheduler.\",\"This scheduler is the combination of WarmupLR and ReduceLROnPlateau:\",\"WarmupLR: : lr = optimizer.lr * warmup_step ** 0.5 : * min(step ** -0.5, step * warmup_step ** -1.5)\",\"WarmupReduceLROnPlateau: : if step <= warmup_step: : lr = optimizer.lr * warmup_step ** 0.5 : * min(step ** -0.5, step * warmup_step ** -1.5) else: : lr = ( : optimizer.lr * factor if no improvement for a ‘patience’ number of epochs else optimizer.lr <br/> )\",\"Note that the maximum lr equals to optimizer.lr in this scheduler.\",\"property in_cooldown\",\"is_better(a, best)\",\"load_state_dict(state_dict)\",\"state_dict()\",\"step(metrics=None, epoch=None)\"]},\"2021\":{\"h\":\"espnet2.schedulers.warmup_step_lr.WarmupStepLR\",\"t\":[\"source\",\"class espnet2.schedulers.warmup_step_lr.WarmupStepLR(optimizer: Optimizer, warmup_steps: int | float = 25000, steps_per_epoch: int = 10000, step_size: int = 1, gamma: float = 0.1, last_epoch: int = -1)\",\"Bases: _LRScheduler, AbsBatchStepScheduler\",\"The WarmupStepLR scheduler.\",\"This scheduler is the combination of WarmupLR and StepLR:\",\"WarmupLR: : lr = optimizer.lr * warmup_step ** 0.5 : * min(step ** -0.5, step * warmup_step ** -1.5)\",\"WarmupStepLR: : if step <= warmup_step: : lr = optimizer.lr * warmup_step ** 0.5 : * min(step ** -0.5, step * warmup_step ** -1.5) else: : lr = optimizer.lr * (gamma ** (epoch//step_size))\",\"Note that the maximum lr equals to optimizer.lr in this scheduler.\",\"get_lr()\",\"Compute learning rate using chainable form of the scheduler.\"]},\"2022\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.A1_A2\",\"t\":[\"source\"]},\"2023\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.A1_A2_batch\",\"t\":[\"source\"]},\"2024\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.A1_T1\",\"t\":[\"source\"]},\"2025\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.A1_T2\",\"t\":[\"source\"]},\"2026\":{\"h\":\"espnet2.sds.asr.abs_asr.AbsASR\",\"t\":[\"source\",\"class espnet2.sds.asr.abs_asr.AbsASR(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(xs_pad)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2027\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract warmup()\"]},\"2028\":{\"h\":\"espnet2.sds.end_to_end.abs_e2e.AbsE2E\",\"t\":[\"source\",\"class espnet2.sds.end_to_end.abs_e2e.AbsE2E(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(xs_pad)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2029\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract warmup()\"]},\"2030\":{\"h\":\"espnet2.sds.llm.abs_llm.AbsLLM\",\"t\":[\"source\",\"class espnet2.sds.llm.abs_llm.AbsLLM(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(text: str)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2031\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract warmup()\"]},\"2032\":{\"h\":\"espnet2.sds.tts.abs_tts.AbsTTS\",\"t\":[\"source\",\"class espnet2.sds.tts.abs_tts.AbsTTS(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(text: str)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2033\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract warmup()\"]},\"2034\":{\"h\":\"espnet2.sds.vad.abs_vad.AbsVAD\",\"t\":[\"source\",\"class espnet2.sds.vad.abs_vad.AbsVAD(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(speech, sample_rate)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2035\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract warmup()\"]},\"2036\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.Block\",\"t\":[\"source\"]},\"2037\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.CLI\",\"t\":[\"source\"]},\"2038\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.CausalSelfAttention\",\"t\":[\"source\"]},\"2039\":{\"h\":\"espnet2.sds.utils.chat.Chat\",\"t\":[\"source\",\"class espnet2.sds.utils.chat.Chat(size: int)\",\"Bases: object\",\"Handles the chat to avoid OOM issues.\",\"size\",\"The maximum size of the buffer for chat pairs.\",\"Type: int\",\"init_chat_message\",\"The initial message to prepend to the chat history, if any.\",\"Type: Union[Dict[str, str], None]\",\"buffer\",\"A list of chat messages in the form of dictionaries, maintaining the chat history.\",\"Type: List[Dict[str, str]]\",\"Initializes the Chat object with a buffer size.\",\"Parameters:size (int) – The maximum number of chat pairs (prompt-response) to retain in the buffer.\",\"append(item: Dict[str, str]) → None\",\"Adds a new message to the chat buffer.\",\"Removes the oldest prompt-response pair if the buffer exceeds the maximum allowed size.\",\"Parameters:item (Dict *[*str,str]) – The chat message to be added to the buffer, typically with keys like “role” (e.g., “user”, “assistant”) and “content”.\",\"init_chat(init_chat_message: Dict[str, str]) → None\",\"Sets the initial chat message, which will be prepended to the chat history.\",\"Parameters:init_chat_message (Dict *[*str,str]) – The initial message to set, typically with keys like “role” and “content”.\",\"to_list() → List[Dict[str, str]]\",\"Returns the complete chat history as a list,\",\"including the initial message if set.\",\"Returns: The chat history as a list of dictionaries, starting with the initial message if it is set, followed by the buffer contents.\",\"Return type: List[Dict[str, str]]\"]},\"2040\":{\"h\":\"espnet2.sds.tts.chat_tts.ChatTTSModel\",\"t\":[\"source\",\"class espnet2.sds.tts.chat_tts.ChatTTSModel\",\"Bases: AbsTTS\",\"ChaTTS Model\",\"Initializes the ChatTTSModel class.\",\"Ensures that the ChatTTS library is properly installed and initializes the TTS engine.\",\"forward(transcript: str) → Tuple[int, ndarray]\",\"Converts a text transcript into an audio waveform\",\"using the ChatTTS system.\",\"Parameters:transcript (str) – The input text to be converted into speech.\",\"Returns: A tuple containing: \",\"The sample rate of the audio (int).\",\"The generated audio waveform as a NumPy array of type int16.\",\"Return type: Tuple[int, np.ndarray]\",\"warmup()\",\"Perform a single forward pass with dummy input to\",\"pre-load and warm up the model.\"]},\"2041\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.config.Config\",\"t\":[\"source\"]},\"2042\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.CycleIterator\",\"t\":[\"source\"]},\"2043\":{\"h\":\"espnet2.sds.asr.espnet_asr.ESPnetASRModel\",\"t\":[\"source\",\"class espnet2.sds.asr.espnet_asr.ESPnetASRModel(tag: str = 'espnet/simpleoier_librispeech_asr_train_asr_conformer7_wavlm_large_raw_en_bpe5000_sp', device: str = 'cuda', dtype: str = 'float16')\",\"Bases: AbsASR\",\"ESPnet ASR\",\"Initializer method.\",\"Args: tag (str, optional):\",\"The pre-trained model tag (on Hugging Face). Defaults to: “espnet/\",\"simpleoier_librispeech_asr_train_asr_\",\"conformer7_wavlm_large_raw_en_bpe5000_sp”.\",\"device (str, optional): : The computation device for running inference. Defaults to “cuda”. Common options include “cuda” or “cpu”.\",\"dtype (str, optional): : The floating-point precision to use. Defaults to “float16”.\",\"forward(array: ndarray) → str\",\"Perform a forward pass on the given audio data,\",\"returning the transcribed text prompt.\",\"Parameters:array (np.ndarray) – The input audio data to be transcribed. Typically a NumPy array.\",\"Returns: The transcribed text from the audio input, as returned by the speech-to-text model.\",\"Return type: str\",\"warmup()\",\"Perform a single forward pass with dummy input to\",\"pre-load and warm up the model.\"]},\"2044\":{\"h\":\"espnet2.sds.espnet_model.ESPnetSDSModelInterface\",\"t\":[\"source\",\"class espnet2.sds.espnet_model.ESPnetSDSModelInterface(ASR_option: str, LLM_option: str, TTS_option: str, type_option: str, access_token: str)\",\"Bases: AbsESPnetModel\",\"Web Interface for Spoken Dialog System models\",\"This class provides a unified interface to integrate ASR, TTS, and LLM modules for cascaded spoken dialog systems as well as also supports E2E spoken dialog systems. It supports real-time interactions, including VAD (Voice Activity Detection) based conversation management.\",\"Initializer method.\",\"Parameters:\",\"ASR_option (str) – The selected ASR model option to use for speech-to-text processing.\",\"LLM_option (str) – The selected LLM model option for generating text responses.\",\"TTS_option (str) – The selected TTS model option for text-to-speech synthesis.\",\"type_option (str) – The type of SDS interaction to perform (e.g., cascaded or E2E).\",\"access_token (str) – The access token for accessing models hosted on Hugging Face.\",\"collect_feats()\",\"forward(y: ndarray, sr: int, stream: ndarray, asr_output_str: str | None, text_str: str | None, audio_output: Tuple[int, ndarray] | None, audio_output1: Tuple[int, ndarray] | None, latency_ASR: float, latency_LM: float, latency_TTS: float)\",\"Processes audio input to generate ASR, LLM, and TTS outputs\",\"while calculating latencies.\",\"This method handles both Cascaded and End-to-End setups.\",\"Parameters:\",\"y – Input audio array.\",\"sr – Sampling rate of the input audio.\",\"stream – The current audio stream buffer.\",\"asr_output_str – Previously generated ASR output string.\",\"text_str – Previously generated LLM text response.\",\"audio_output – Previously generated TTS audio output.\",\"(****) (audio_output1) – Placeholder for audio stream.\",\"latency_ASR (float) – Latency for ASR processing.\",\"latency_LM (float) – Latency for LLM processing.\",\"latency_TTS (float) – Latency for TTS processing.\",\"Returns: Tuple[str, str, Optional[Tuple[int, np.ndarray]], Optional[Tuple[int, np.ndarray]], float, float, float, bool]: \",\"Updated ASR output string.\",\"Updated LLM-generated text.\",\"Updated TTS audio output.\",\"Updated user audio stream output.\",\"ASR latency.\",\"LLM latency.\",\"TTS latency.\",\"Update audio stream\",\"Change flag indicating if output was updated.\",\"handle_ASR_selection(option: str)\",\"Handles the selection and initialization of ASR model.\",\"This method dynamically loads the selected ASR based on the provided option. If the selected model is already active, it avoids reloading to save resources. The method temporarily removes the visibility of Gradio outputs during the initialization process to indicate progress.\",\"Parameters:option (str) – The name of the ASR to load.\",\"handle_E2E_selection()\",\"Handles the selection and initialization of E2E model Mini-Omni.\",\"This method dynamically loads the E2E spoken dialog model. If the model is already active, it avoids reloading to save resources.\",\"handle_LLM_selection(option: str)\",\"Handles the selection and initialization of a LLM.\",\"This method dynamically loads the selected LLM based on the provided option. If the selected model is already active, it avoids reloading to save resources. The method temporarily removes the visibility of Gradio outputs during the initialization process to indicate progress.\",\"Parameters:option (str) – The name of the LLM to load.\",\"handle_TTS_selection(option: str)\",\"Handles the selection and initialization of a Text-to-Speech (TTS) model.\",\"This method dynamically loads the selected TTS model based on the provided option. If the selected model is already active, it avoids reloading to save resources. The method temporarily removes the visibility of Gradio outputs during the initialization process to indicate progress.\",\"Parameters:option (str) – The name of the TTS model to load.\",\"handle_type_selection(option: str, TTS_radio: str, ASR_radio: str, LLM_radio: str)\",\"Handles the selection of the spoken dialogue model type (Cascaded or E2E)\",\"and dynamically updates the interface based on the selected option.\",\"This method manages the initialization of ASR, TTS, and LLM models for Cascaded systems or switches to an End-to-End system. The Gradio interface components are updated accordingly.\",\"Parameters:\",\"option (str) – The selected spoken dialogue system.\",\"TTS_radio (str) – The selected TTS model for the Cascaded system.\",\"ASR_radio (str) – The selected ASR model for the Cascaded system.\",\"LLM_radio (str) – The selected LLM model for the Cascaded system.\"]},\"2045\":{\"h\":\"espnet2.sds.tts.espnet_tts.ESPnetTTSModel\",\"t\":[\"source\",\"class espnet2.sds.tts.espnet_tts.ESPnetTTSModel(tag: str = 'kan-bayashi/ljspeech_vits', device: str = 'cuda')\",\"Bases: AbsTTS\",\"ESPnet TTS.\",\"A class to initialize and manage a ESPnet’s\",\"pre-trained text-to-speech (TTS) model.\",\"This class:\",\"Downloads and sets up a pre-trained TTS model using the ESPnet Model Zoo.\",\"Supports various TTS configurations, including multi speaker TTS using speaker embeddings and speaker IDs.\",\"Parameters:\",\"tag (str,optional) – The model tag for the pre-trained TTS model. Defaults to “kan-bayashi/ljspeech_vits”.\",\"device (str,optional) – The computation device for running inference. Defaults to “cuda”.\",\"Raises:ImportError – If the required espnet_model_zoo library is not installed.\",\"forward(transcript: str) → Tuple[int, ndarray]\",\"Converts a text transcript into an audio waveform\",\"using a pre-trained ESPnet-TTS model.\",\"Parameters:transcript (str) – The input text to be converted into speech.\",\"Returns: A tuple containing: \",\"The sample rate of the audio (int).\",\"The generated audio waveform as a NumPy array of type int16.\",\"Return type: Tuple[int, np.ndarray]\",\"warmup()\",\"Perform a single forward pass with dummy input to\",\"pre-load and warm up the model.\"]},\"2046\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.GPT\",\"t\":[\"source\"]},\"2047\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.GemmaMLP\",\"t\":[\"source\"]},\"2048\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.GptNeoxMLP\",\"t\":[\"source\"]},\"2049\":{\"h\":\"espnet2.sds.llm.hugging_face_llm.HuggingFaceLLM\",\"t\":[\"source\",\"class espnet2.sds.llm.hugging_face_llm.HuggingFaceLLM(access_token: str, tag: str = 'meta-llama/Llama-3.2-1B-Instruct', device: str = 'cuda', dtype: str = 'float16')\",\"Bases: AbsLLM\",\"Hugging Face LLM\",\"A class for initializing a text response generator\",\"using the Transformers library.\",\"Parameters:\",\"access_token (str) – The access token required for downloading models from Hugging Face.\",\"tag (str,optional) – The model tag for the pre-trained language model. Defaults to “meta-llama/Llama-3.2-1B-Instruct”.\",\"device (str,optional) – The device to run the inference on. Defaults to “cuda”.\",\"dtype (str,optional) – The data type for model computation. Defaults to “float16”.\",\"Raises:ImportError – If the transformers library is not installed.\",\"forward(chat_messages: List[dict]) → str\",\"Generate a response from the language model based on\",\"the provided chat messages.\",\"Parameters:chat_messages (List *[*dict]) – A list of chat messages, where each message is a dictionary containing the conversation history. Each dictionary should have keys like “role” (e.g., “user”, “assistant”) and “content” (the message text).\",\"Returns: The generated response text from the language model.\",\"Return type: str\",\"Notes\",\"The model generates a response with a maximum of 64\",\"new tokens and a deterministic sampling strategy (temperature set to 0 and do_sample set to False).\",\"warmup()\",\"Perform a single forward pass with dummy input to\",\"pre-load and warm up the model.\"]},\"2050\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.IncrementalPyTorchPickler\",\"t\":[\"source\"]},\"2051\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.KVCache\",\"t\":[\"source\"]},\"2052\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.LLaMAMLP\",\"t\":[\"source\"]},\"2053\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.LLaMAMoE\",\"t\":[\"source\"]},\"2054\":{\"h\":\"espnet2.sds.end_to_end.mini_omni_e2e.MiniOmniE2EModel\",\"t\":[\"source\",\"class espnet2.sds.end_to_end.mini_omni_e2e.MiniOmniE2EModel(device: str = 'cuda', dtype: str = 'float16')\",\"Bases: AbsE2E\",\"Mini-OMNI E2E\",\"A class to initialize and manage the OmniInference client\",\"for end-to-end dialogue systems.\",\"Parameters:device (Literal[\\\"cuda\\\",\\\"cpu\\\"],optional) – The device to run the inference on. Defaults to “cuda”.\",\"Raises:ImportError – If required dependencies (Pydub, Huggingface Hub, or OmniInference) are not installed.\",\"forward(array: ndarray, orig_sr: int) → Tuple[str, bytes]\",\"Processes audio input to generate synthesized speech\",\"and the corresponding text response.\",\"Parameters:\",\"array (np.ndarray) – The input audio array to be processed.\",\"orig_sr (int) – The sample rate of the input audio.\",\"Returns: A tuple containing: \",\"text_str (str): The generated text response.\",\"audio_output (bytes): The synthesized speech as an MP3 byte stream.\",\"Return type: Tuple[str, bytes]\",\"warmup()\",\"Perform a single forward pass with dummy input to\",\"pre-load and warm up the model.\"]},\"2055\":{\"h\":\"espnet2.sds.asr.owsm_ctc_asr.OWSMCTCModel\",\"t\":[\"source\",\"class espnet2.sds.asr.owsm_ctc_asr.OWSMCTCModel(tag: str = 'pyf98/owsm_ctc_v3.1_1B', device: str = 'cuda', dtype: str = 'float16')\",\"Bases: AbsASR\",\"OWSM CTC ASR\",\"Initializer method.\",\"Args: tag (str, optional):\",\"The pre-trained model tag (on Hugging Face). Defaults to: “pyf98/owsm_ctc_v3.1_1B”.\",\"device (str, optional): : The computation device for running inference. Defaults to “cuda”. Common options include “cuda” or “cpu”.\",\"dtype (str, optional): : The floating-point precision to use. Defaults to “float16”.\",\"forward(array: ndarray) → str\",\"Perform a forward pass on the given audio data,\",\"returning the transcribed text prompt.\",\"Parameters:array (np.ndarray) – The input audio data to be transcribed. Typically a NumPy array.\",\"Returns: The transcribed text from the audio input, as returned by the OWSM ASR model.\",\"Return type: str\",\"warmup()\",\"Perform a single forward pass with dummy input to\",\"pre-load and warm up the model.\"]},\"2056\":{\"h\":\"espnet2.sds.asr.owsm_asr.OWSMModel\",\"t\":[\"source\",\"class espnet2.sds.asr.owsm_asr.OWSMModel(tag: str = 'espnet/owsm_v3.1_ebf', device: str = 'cuda', dtype: str = 'float16')\",\"Bases: AbsASR\",\"OWSM ASR\",\"Initializer method.\",\"Args: tag (str, optional):\",\"The pre-trained model tag (on Hugging Face). Defaults to: “espnet/owsm_v3.1_ebf”.\",\"device (str, optional): : The computation device for running inference. Defaults to “cuda”. Common options include “cuda” or “cpu”.\",\"dtype (str, optional): : The floating-point precision to use. Defaults to “float16”.\",\"forward(array: ndarray) → str\",\"Perform a forward pass on the given audio data,\",\"returning the transcribed text prompt.\",\"Parameters:array (np.ndarray) – The input audio data to be transcribed. Typically a NumPy array.\",\"Returns: The transcribed text from the audio input, as returned by the OWSM ASR model.\",\"Return type: str\",\"warmup()\",\"Perform a single forward pass with dummy input to\",\"pre-load and warm up the model.\"]},\"2057\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.OmniInference\",\"t\":[\"source\"]},\"2058\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.RMSNorm\",\"t\":[\"source\"]},\"2059\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.SavingProxyForStorage\",\"t\":[\"source\"]},\"2060\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.SavingProxyForTensor\",\"t\":[\"source\"]},\"2061\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.SnacConfig\",\"t\":[\"source\",\"class espnet2.sds.end_to_end.mini_omni.utils.snac_utils.SnacConfig\",\"Bases: object\",\"audio_vocab_size = 4096\",\"end_of_audio = 4097\",\"padded_vocab_size = 4160\"]},\"2062\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.T1_A2\",\"t\":[\"source\"]},\"2063\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.T1_T2\",\"t\":[\"source\"]},\"2064\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.tokenizer.Tokenizer\",\"t\":[\"source\"]},\"2065\":{\"h\":\"espnet2.sds.vad.webrtc_vad.WebrtcVADModel\",\"t\":[\"source\",\"class espnet2.sds.vad.webrtc_vad.WebrtcVADModel(speakup_threshold: int = 12, continue_threshold: int = 10, min_speech_ms: int = 500, max_speech_ms: float = inf, target_sr: int = 16000)\",\"Bases: AbsVAD\",\"Webrtc VAD Model\",\"This class uses WebRTC VAD to detect speech in an audio stream.\",\"Parameters:\",\"speakup_threshold (int,optional) – The threshold for detecting the start of speech.\",\"continue_threshold (int,optional) – The threshold for continuing speech detection.\",\"min_speech_ms (int,optional) – The minimum duration (in milliseconds) for a valid speech segment. Defaults to 500 ms.\",\"max_speech_ms (float,optional) – The maximum duration (in milliseconds) for a valid speech segment. Defaults to infinity.\",\"target_sr (int,optional) – The target sampling rate for resampling the input audio. Defaults to 16000 Hz.\",\"vad_output\",\"Stores the speech segments detected as floating-point tensors.\",\"Type: Optional[list]\",\"vad_bin_output\",\"Stores the speech segments detected as binary audio.\",\"Type: Optional[list]\",\"Raises:ImportError – If the required webrtcvad library is not installed.\",\"forward(speech: ndarray, sample_rate: int, binary: bool = False) → ndarray | None\",\"Process an audio stream and detect speech using WebRTC VAD.\",\"Parameters:\",\"speech – The raw audio stream in 16-bit PCM format.\",\"sample_rate (int) – The sampling rate of the input audio.\",\"binary (bool,optional) – If True, returns the binary audio output instead of the resampled float array. Defaults to False.\",\"Returns: The detected speech segment as a NumPy array (float or binary audio), or None if no valid segment is found.\",\"Return type: Optional[np.ndarray]\",\"warmup()\"]},\"2066\":{\"h\":\"espnet2.sds.asr.whisper_asr.WhisperASRModel\",\"t\":[\"source\",\"class espnet2.sds.asr.whisper_asr.WhisperASRModel(tag: str = 'large', device: str = 'cuda', dtype: str = 'float16')\",\"Bases: AbsASR\",\"Whisper ASR\",\"Initializer method.\",\"Args: tag (str, optional):\",\"The Whisper model tag\",\"device (str, optional): : The computation device for running inference. Defaults to “cuda”. Common options include “cuda” or “cpu”.\",\"dtype (str, optional): : The floating-point precision to use. Defaults to “float16”.\",\"forward(array: ndarray) → str\",\"Perform a forward pass on the given audio data,\",\"returning the transcribed text prompt.\",\"Parameters:array (np.ndarray) – The input audio data to be transcribed. Typically a NumPy array.\",\"Returns: The transcribed text from the audio input, as returned by the Whisper ASR model.\",\"Return type: str\",\"warmup()\",\"Perform a single forward pass with dummy input to\",\"pre-load and warm up the model.\"]},\"2067\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.apply_rope\",\"t\":[\"source\"]},\"2068\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.build_mask_cache\",\"t\":[\"source\"]},\"2069\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.build_rope_cache\",\"t\":[\"source\"]},\"2070\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.capture_hparams\",\"t\":[\"source\"]},\"2071\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.check_valid_checkpoint_dir\",\"t\":[\"source\"]},\"2072\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.choose_logger\",\"t\":[\"source\"]},\"2073\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.chunked_cross_entropy\",\"t\":[\"source\"]},\"2074\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.copy_config_files\",\"t\":[\"source\"]},\"2075\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.download_model\",\"t\":[\"source\"]},\"2076\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.estimate_flops\",\"t\":[\"source\"]},\"2077\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.extend_checkpoint_dir\",\"t\":[\"source\"]},\"2078\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.find_multiple\",\"t\":[\"source\"]},\"2079\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.find_resume_path\",\"t\":[\"source\"]},\"2080\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.flops_per_param\",\"t\":[\"source\"]},\"2081\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.generate\",\"t\":[\"source\"]},\"2082\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.generate_AA\",\"t\":[\"source\"]},\"2083\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.generate_ASR\",\"t\":[\"source\"]},\"2084\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.generate_AT\",\"t\":[\"source\"]},\"2085\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.generate_TA\",\"t\":[\"source\"]},\"2086\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.generate_TA_BATCH\",\"t\":[\"source\"]},\"2087\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.generate_TT\",\"t\":[\"source\"]},\"2088\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.generate_audio_data\",\"t\":[\"source\",\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.generate_audio_data(snac_tokens, snacmodel, device=None)\"]},\"2089\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.get_argument_names\",\"t\":[\"source\"]},\"2090\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.get_default_supported_precision\",\"t\":[\"source\"]},\"2091\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.get_input_ids_TA\",\"t\":[\"source\"]},\"2092\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.get_input_ids_TT\",\"t\":[\"source\"]},\"2093\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.get_input_ids_whisper\",\"t\":[\"source\"]},\"2094\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.get_input_ids_whisper_ATBatch\",\"t\":[\"source\"]},\"2095\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.get_snac\",\"t\":[\"source\",\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.get_snac(list_output, index, nums_generate)\"]},\"2096\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.get_time_str\",\"t\":[\"source\",\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.get_time_str()\"]},\"2097\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.incremental_save\",\"t\":[\"source\"]},\"2098\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.init_out_dir\",\"t\":[\"source\"]},\"2099\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.instantiate_bnb_optimizer\",\"t\":[\"source\"]},\"2100\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.instantiate_torch_optimizer\",\"t\":[\"source\"]},\"2101\":{\"h\":\"espnet2.sds.utils.utils.int2float\",\"t\":[\"source\",\"espnet2.sds.utils.utils.int2float(sound: ndarray | list) → ndarray\",\"Converts an integer PCM audio signal to a floating-point representation.\",\"This function scales an integer PCM audio waveform (typically int16) to a float32 format, normalizing the values to the range [-1.0, 1.0].\",\"Parameters:sound (Union *[*np.ndarray,list]) – The input audio signal in integer format. Typically a NumPy array or a list of integers.\",\"Returns: The audio signal converted to float32 format and normalized.\",\"Return type: np.ndarray\",\"Taken from https://github.com/snakers4/silero-vad\"]},\"2102\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.layershift\",\"t\":[\"source\",\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.layershift(input_id, layer, stride=4160, shift=152000)\"]},\"2103\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.load_audio\",\"t\":[\"source\"]},\"2104\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.load_checkpoint\",\"t\":[\"source\"]},\"2105\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.load_model\",\"t\":[\"source\"]},\"2106\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.map_old_state_dict_weights\",\"t\":[\"source\"]},\"2107\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.multinomial_num_samples_1\",\"t\":[\"source\"]},\"2108\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.next_token\",\"t\":[\"source\"]},\"2109\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.next_token_A1T1\",\"t\":[\"source\"]},\"2110\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.next_token_A1T2\",\"t\":[\"source\"]},\"2111\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.next_token_asr\",\"t\":[\"source\"]},\"2112\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.next_token_batch\",\"t\":[\"source\"]},\"2113\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.num_parameters\",\"t\":[\"source\"]},\"2114\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.parse_devices\",\"t\":[\"source\"]},\"2115\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.reconscruct_snac\",\"t\":[\"source\",\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.reconscruct_snac(output_list)\"]},\"2116\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.reconstruct_tensors\",\"t\":[\"source\",\"espnet2.sds.end_to_end.mini_omni.utils.snac_utils.reconstruct_tensors(flattened_output, device=None)\",\"Reconstructs the list of tensors from the flattened output.\"]},\"2117\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.reset_parameters\",\"t\":[\"source\"]},\"2118\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.sample\",\"t\":[\"source\"]},\"2119\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.generate.base.sample_top_p\",\"t\":[\"source\"]},\"2120\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.save_config\",\"t\":[\"source\"]},\"2121\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.utils.save_hyperparameters\",\"t\":[\"source\"]},\"2122\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.inference.test_infer\",\"t\":[\"source\"]},\"2123\":{\"h\":\"espnet2.sds.end_to_end.mini_omni.litgpt.model.whisperMLP\",\"t\":[\"source\"]},\"2124\":{\"h\":\"espnet2.slu.postdecoder.abs_postdecoder.AbsPostDecoder\",\"t\":[\"source\",\"class espnet2.slu.postdecoder.abs_postdecoder.AbsPostDecoder(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract convert_examples_to_features(data: list, max_seq_length: int, output_size: int)\",\"abstract forward(transcript_input_ids: LongTensor, transcript_attention_mask: LongTensor, transcript_token_type_ids: LongTensor, transcript_position_ids: LongTensor) → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2125\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size() → int\"]},\"2126\":{\"h\":\"espnet2.slu.postencoder.conformer_postencoder.ConformerPostEncoder\",\"t\":[\"source\",\"class espnet2.slu.postencoder.conformer_postencoder.ConformerPostEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str = 'linear', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'rel_pos', selfattention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, padding_idx: int = -1)\",\"Bases: AbsPostEncoder\",\"Hugging Face Transformers PostEncoder.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Forward.\",\"output_size() → int\",\"Get the output size.\"]},\"2127\":{\"h\":\"espnet2.slu.espnet_model.ESPnetSLUModel\",\"t\":[\"source\",\"class espnet2.slu.espnet_model.ESPnetSLUModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, postencoder: AbsPostEncoder | None, decoder: AbsDecoder | None, ctc: CTC, joint_network: Module | None, postdecoder: AbsPostDecoder | None = None, deliberationencoder: AbsPostEncoder | None = None, transcript_token_list: Tuple[str, ...] | List[str] | None = None, ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', extract_feats_in_collect_stats: bool = True, two_pass: bool = False, pre_postencoder_norm: bool = False, num_class: int = 0, ssl_input_size: int = 0, superb_setup: bool = False, use_only_last_correct: bool = False)\",\"Bases: ESPnetASRModel\",\"CTC-attention hybrid Encoder-Decoder model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, transcript: Tensor | None = None, transcript_lengths: Tensor | None = None, **kwargs) → Dict[str, Tensor]\",\"encode(speech: Tensor, speech_lengths: Tensor, transcript_pad: Tensor | None = None, transcript_pad_lens: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Frontend + Encoder. Note that this method is used by asr_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, transcript: Tensor | None = None, transcript_lengths: Tensor | None = None, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\"]},\"2128\":{\"h\":\"espnet2.slu.postdecoder.hugging_face_transformers_postdecoder.HuggingFaceTransformersPostDecoder\",\"t\":[\"source\",\"class espnet2.slu.postdecoder.hugging_face_transformers_postdecoder.HuggingFaceTransformersPostDecoder(model_name_or_path: str, output_size=256)\",\"Bases: AbsPostDecoder\",\"Hugging Face Transformers PostEncoder.\",\"Initialize the module.\",\"convert_examples_to_features(data, max_seq_length)\",\"forward(transcript_input_ids: LongTensor, transcript_attention_mask: LongTensor, transcript_token_type_ids: LongTensor, transcript_position_ids: LongTensor) → Tensor\",\"Forward.\",\"output_size() → int\",\"Get the output size.\"]},\"2129\":{\"h\":\"espnet2.slu.postencoder.transformer_postencoder.TransformerPostEncoder\",\"t\":[\"source\",\"class espnet2.slu.postencoder.transformer_postencoder.TransformerPostEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'linear', pos_enc_class=<class 'espnet2.legacy.nets.pytorch_backend.transformer.embedding.PositionalEncoding'>, normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 1, padding_idx: int = -1)\",\"Bases: AbsPostEncoder\",\"Transformer encoder module.\",\"Parameters:\",\"input_size – input dim\",\"output_size – dimension of attention\",\"attention_heads – the number of heads of multi head attention\",\"linear_units – the number of units of position-wise feed forward\",\"num_blocks – the number of decoder blocks\",\"dropout_rate – dropout rate\",\"attention_dropout_rate – dropout rate in attention\",\"positional_dropout_rate – dropout rate after adding positional encoding\",\"input_layer – input layer type\",\"pos_enc_class – PositionalEncoding or ScaledPositionalEncoding\",\"normalize_before – whether to use layer_norm before the first block\",\"concat_after – whether to concat attention layer’s input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x)\",\"positionwise_layer_type – linear of conv1d\",\"positionwise_conv_kernel_size – kernel size of positionwise conv1d layer\",\"padding_idx – padding_idx for input_layer=embed\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(xs_pad: Tensor, ilens: Tensor, prev_states: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor | None]\",\"Embed positions in tensor.\",\"Parameters:\",\"xs_pad – input tensor (B, L, D)\",\"ilens – input length (B)\",\"prev_states – Not to be used now.\",\"Returns: position embedded tensor and mask\",\"output_size() → int\"]},\"2130\":{\"h\":\"espnet2.speechlm.model.speechlm.multimodal_io.abs_io.AbsIO\",\"t\":[\"source\",\"class espnet2.speechlm.model.speechlm.multimodal_io.abs_io.AbsIO(modality: str, is_discrete: bool)\",\"Bases: ABC, Module\",\"Abstract base class for multimodal I/O processing.\",\"This class provides the interface for encoding and decoding different modalities, supporting both discrete (e.g., text tokens, discrete codes) and continuous (e.g., audio features, embeddings) representations.\",\"All methods are optional and can be implemented as needed based on the specific modality and use case. Methods not implemented will raise NotImplementedError.\",\"Key methods: : Data Processing:\",\"preprocess: CPU-based single item preprocessing for data loading\",\"encode_batch: GPU-based batch encoding for model input\",\"decode_batch: GPU-based batch decoding for model output <br/> Utilities:\",\"find_length: CPU-based length statistics collection before training\",\"copy_for_worker: Create lightweight copy for multiprocessing workers <br/> Modality Properties:\",\"feature_dim: Feature dimension for continuous modalities\",\"num_stream: Number of streams for discrete multi-stream modalities\",\"get_vocabulary: Vocabulary for discrete tokenized modalities\",\"get_stream_interval: Token ranges for multi-stream tokenizers\",\"get_stream_weight: Loss weights for multi-stream training\",\"Initialize the multimodal I/O handler.\",\"Parameters:\",\"modality – Type of modality (e.g., “text”, “audio”, “vision”)\",\"is_discrete – True for discrete representations (tokens), False for continuous representations (features)\",\"copy_for_worker() → AbsIO\",\"Create a lightweight copy for multiprocessing data loading workers.\",\"This method creates a deep copy of the object while excluding heavy components like torch models, reducing memory usage and ensuring the object can be safely distributed to multiprocessing data loading workers.\",\"The default implementation performs a shallow copy, which may not be sufficient for all use cases. Subclasses should override this method to properly handle their specific components, especially:\",\"Excluding large torch.nn.Module components\",\"Excluding CUDA tensors\",\"Keeping only necessary CPU-based preprocessing components\",\"Returns: A lightweight copy suitable for multiprocessing workers\",\"decode_batch(batch_encoded: Dict[str, Any]) → List[Any]\",\"Decode GPU-batched tensors back to list of individual items.\",\"This method handles GPU-batched data from encode_batch and converts it back to a list of individual decoded items.\",\"Parameters:batch_encoded – Dictionary of GPU-batched tensors from encode_batch\",\"Returns: List of decoded data items in their individual format\",\"encode_batch(batch_data: List[Any]) → Dict[str, Any]\",\"Encode pre-processed batch data for GPU-based batch processing.\",\"This method handles data that has already been processed into proper shape and is ready for efficient GPU batch computation.\",\"Parameters:batch_data – List of pre-processed data items already in proper shape\",\"Returns:\",\"‘data’: Main encoded tensor [batch, seq_len, …]\",\"’lengths’: Sequence lengths [batch]\",\"Return type: Dictionary containing GPU-ready batched tensors\",\"feature_dim() → int | None\",\"Get the feature dimension for continuous modalities.\",\"Returns: Feature dimension (e.g., 80 for mel-spectrogram, 768 for embeddings), None for discrete modalities\",\"find_length(data: Any) → int\",\"Calculate sequence length for length statistics collection before training.\",\"This CPU-only method is used during the pre-training phase to collect length statistics of the dataset. It efficiently computes the expected sequence length without performing full encoding, allowing for proper batch organization and padding strategies.\",\"Note: This runs on CPU only and is called during length statistics collection phase, not during actual training.\",\"Parameters:data – Single raw input data in modality-specific format\",\"Returns: Expected sequence length after encoding\",\"get_stream_interval() → List[Tuple[int, int]] | None\",\"Get the vocabulary index ranges for all streams.\",\"In multi-stream tokenizers, each stream uses a specific range of vocabulary indices. For example, stream 0 might use indices [0, 1023], stream 1 uses [1024, 2047], etc.\",\"Returns: List of tuples (start, end) for each stream’s vocabulary range, None for continuous modalities\",\"get_stream_weight() → List[float] | None\",\"Get the loss weights for all streams.\",\"Different streams may have different importance during training. For example, semantic streams might be weighted higher than acoustic detail streams.\",\"Returns: List of weight values for each stream (typically between 0.0 and 1.0), None for continuous modalities\",\"get_vocabulary() → List[str] | None\",\"Get the complete vocabulary list for discrete modalities.\",\"For multi-stream tokenizers, this returns the combined vocabulary across all streams.\",\"Returns: List of vocabulary tokens/symbols (e.g., [“<pad>”, “<unk>”, “the”, …]), None for continuous modalities\",\"num_stream() → int | None\",\"Get the number of parallel streams for discrete modalities.\",\"For multi-stream discrete representations, tokens are organized as [T, N] where T is the sequence length and N is the number of parallel streams. Each stream represents a different aspect or level of the signal (e.g., semantic vs acoustic codes in audio).\",\"Returns: Number of parallel streams (e.g., 8 for multi-stream audio codes), None for continuous modalities\",\"preprocess(data: Any) → Tuple[ndarray, Tuple[int, ndarray] | None, ndarray]\",\"Preprocess single data item on CPU for multiprocessing data loading.\",\"This method is called during data loading in multiprocessing workers and performs all CPU-based preprocessing operations on individual data items before they are batched. This includes operations like tokenization, feature extraction, normalization, etc.\",\"Note: This runs on CPU only and processes single items (not batches). Batch processing is handled by encode_batch after data loading.\",\"Parameters:data – Single raw data item in original format\",\"Returns:\",\"seq: np.ndarray of shape [t_len, num_stream] to be placed in training sequence. For continuous features, fill with zeros.\",\"conti_feat: Optional tuple of (length, features) where features is the continuous data with time dimension first. None if discrete.\",\"loss_mask: Float np.ndarray specifying loss weight for each token in seq, same shape as seq.\",\"Return type: Tuple of (seq, conti_feat, loss_mask)\"]},\"2131\":{\"h\":\"espnet2.speechlm.model.abs_job.AbsJobTemplate\",\"t\":[\"source\",\"class espnet2.speechlm.model.abs_job.AbsJobTemplate(config: Dict[str, Any])\",\"Bases: ABC\",\"Abstract base class for training job templates.\",\"Defines the data flow from raw sample dictionaries to model loss computation. Subclasses implement two key methods: build_preprocessor() and build_model().\",\"This abstraction enables support for diverse training paradigms including SpeechLM, diffusion models, and self-supervised learning by customizing the preprocessing pipeline and model architecture.\",\"Initialize the job template with configuration.\",\"Parameters:config – Job configuration containing model architecture, data processing parameters, and training settings.\",\"abstract build_model() → Module\",\"Build and return the model for training.\",\"Constructs the model architecture based on the configuration provided during initialization. The model should implement forward() to compute loss and return training statistics.\",\"Returns: PyTorch model instance ready for training with DeepSpeed.\",\"abstract build_preprocessor() → Callable\",\"Build and return the preprocessor object.\",\"The preprocessor object should implement three key methods:\",\"preprocessing(data_dict: Dict[str, Any]) -> Dict[str, Any]: Converts a single raw data dictionary into a training-ready example. This method handles per-sample transformations and can be used both during training (within collate_fn) and inference stages.\",\"collate_fn(samples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]: Combines multiple training-ready samples into a batch for training. This method typically calls preprocessing() on each sample first, then batches the results together. The collate_fn is passed to PyTorch DataLoader for multi-processing data loading.\",\"find_length(data_dict: Dict[str, Any]) -> int: Quickly computes the sample length from a raw data dictionary. Used for efficient batch construction and data statistics collection without performing full preprocessing.\",\"Note: When using PyTorch DataLoader with num_workers > 0, the collate_fn executes in worker subprocesses. Ensure the preprocessor object is picklable and avoid CUDA operations as workers don’t have GPU access.\",\"Returns: A preprocessor object with preprocessing(), collate_fn(), and find_length() methods.\"]},\"2132\":{\"h\":\"espnet2.speechlm.dataloader.dataset.CombinedDataset\",\"t\":[\"source\",\"class espnet2.speechlm.dataloader.dataset.CombinedDataset(datasets: List[Tuple[str, str]] = [], registered_datasets: List[str] = [], num_worker: int = 1, rank: int = 0, world_size: int = 1)\",\"Bases: Dataset\",\"Combined ESPnet Speech Language Model Dataset.\",\"Combines multiple datasets from both direct paths and registered datasets.\",\"Parameters:\",\"datasets – List of (name, json_path) tuples for direct dataset paths (default: [])\",\"registered_datasets – List of registered dataset names to look up in registry (default: [])\",\"num_worker – Number of parallel workers for loading datasets.\",\"rank – Process rank for distributed training (default: 0)\",\"world_size – Total number of processes (default: 1)\",\"property dataset_names : List[str]\",\"Return list of all dataset names.\",\"get_all_examples() → Dict[str, List[str]]\",\"Return all examples as a dictionary mapping dataset names to sample IDs.\",\"Returns: Dictionary mapping dataset names to lists of sample IDs\",\"verify_subset_entries(task, data_name, required_entries)\",\"Verify that a dataset contains all required entries for a task.\"]},\"2133\":{\"h\":\"espnet2.speechlm.model.speechlm.multimodal_io.audio.ContinuousAudioIO\",\"t\":[\"source\",\"class espnet2.speechlm.model.speechlm.multimodal_io.audio.ContinuousAudioIO(encoder_choice: str = 'huggingface', encoder_hf_model_tag: str = 'Qwen/Qwen2.5-Omni-7B', attn_implementation: str | None = None, dtype: str = 'bfloat16', device: str = 'cpu')\",\"Bases: AbsIO\",\"Continuous audio I/O for feature extraction.\",\"This class handles continuous audio representations using neural encoders that produce dense feature vectors instead of discrete tokens.\",\"Initialize continuous audio encoder.\",\"Parameters:\",\"encoder_choice – Type of encoder (“huggingface”)\",\"encoder_hf_model_tag – HuggingFace model identifier (e.g., “Qwen/Qwen2.5-Omni-7B”)\",\"attn_implementation – Attention implementation type\",\"dtype – Model dtype (“bfloat16”, “float16”, etc.)\",\"device – Device for model (“cpu”, “cuda”, etc.)\",\"copy_for_worker() → ContinuousAudioIO\",\"Create lightweight copy for multiprocessing workers.\",\"For continuous audio, we create a new instance without the model since preprocessing doesn’t require the encoder model itself.\",\"Returns: Lightweight copy suitable for workers\",\"encode_batch(batch_data: Tensor, length: Tensor) → List[Tensor]\",\"Encode batch of audio into continuous features.\",\"Processes audio through the encoder model to extract dense features with proper attention masking based on actual audio lengths.\",\"Parameters:\",\"batch_data – Audio tensor [batch, samples, channels]\",\"length – Frame lengths for each sample [batch]\",\"Returns: List of audio feature tensors, one per sample in batch\",\"feature_dim() → int\",\"Get feature dimension for continuous representation.\",\"Returns: Feature dimension of encoder output\",\"find_length(data: Tuple[ndarray, int]) → int\",\"Calculate frame length after encoding.\",\"We don’t call self.processor as it’s very slow to find the length\",\"Parameters:data – Tuple of (audio_array, sample_rate) where audio_array has shape [num_channels, num_samples]\",\"Returns: Frame length after encoding (number of frames)\",\"preprocess(data: Tuple[ndarray, int]) → Tuple[ndarray, Tuple[int, ndarray], ndarray]\",\"Preprocess audio for continuous feature extraction.\",\"Extracts spectrogram features and prepares them for batch encoding.\",\"Parameters:data – Tuple of (audio_array, sample_rate) where audio_array has shape [num_channels, num_samples]\",\"Returns:\",\"seq: Zero array [after_length, 1] as placeholder\",\"conti_feat: Tuple of (after_length, mel_features)\",\"loss_mask: Zero array [after_length, 1] (no discrete tokens)\",\"Return type: Tuple of (seq, conti_feat, loss_mask)\"]},\"2134\":{\"h\":\"espnet2.speechlm.dataloader.iterator.DataIteratorFactory\",\"t\":[\"source\",\"class espnet2.speechlm.dataloader.iterator.DataIteratorFactory(unregistered_specifier: str, registered_specifier: str, stats_dir: Path | str | None = None, loader_state: Path | None = None, collate_fn: Callable | None = None, batchfy_method: str = 'bucket', batch_size: int = 1000, num_workers: int = 4, rank: int = 0, world_size: int = 1, shuffle: bool = False, sequential_load: bool = False, seed: int = 42)\",\"Bases: object\",\"Factory for creating data iterators for SpeechLM training.\",\"This class manages batching, data sharding across GPUs, and provides DataLoader instances for training with support for endless epochs.\",\"Features: : - Supports multiple tasks and datasets with resampling factors\",\"Bucket or pack batching strategies\",\"Distributed training with automatic batch synchronization\",\"Deterministic shuffling with configurable seeds\",\"State saving/loading for training resumption\",\"Parameters:\",\"unregistered_specifier – Space-separated unregistered data specs. Format: “task:name:data_json[:factor]” Example: “asr:librispeech:train.json:2.0”\",\"registered_specifier – Space-separated registered data specs. Format: “task:name[:factor]” Example: “tts:ljspeech:1.5”\",\"stats_dir – Directory containing statistics files (str or Path). Each file should be named “stats_{task}_{data_name}.jsonl”\",\"collate_fn – Optional collate function for DataLoader.\",\"loader_state – Optional saved state dict to restore from.\",\"batchfy_method – Batching method (“bucket” or “pack”).\",\"batch_size – Maximum tokens per batch.\",\"num_workers – Number of DataLoader workers.\",\"rank – GPU rank for distributed training (0-indexed).\",\"world_size – Total number of GPUs in distributed training.\",\"shuffle – Whether to shuffle batches.\",\"seed – Random seed for reproducibility.\",\"Example\",\">>> factory = DataIteratorFactory( ... unregistered_specifier=\\\"asr:libri:train.json:2.0\\\", ... registered_specifier=\\\"tts:lj:1.0\\\", ... stats_dir=\\\"/path/to/stats\\\", ... batch_size=10000, ... shuffle=True, ... ) >>> loader = factory.get_iterator(global_step=0, length=100) >>> for batch in loader: ... # Training loop ... pass\",\"build_iter(global_step: int = 0, length: int | None = None) → DataLoader\",\"Get a DataLoader for a specific range of batches.\",\"Supports endless epochs by wrapping around when batches are exhausted. If the requested length exceeds remaining batches, it will continue from the beginning.\",\"Parameters:\",\"global_step – Starting batch index (must be non-negative).\",\"length – Number of batches to include (must be positive).\",\"Returns: DataLoader that iterates over the specified batch range.\",\"Raises:ValueError – If validation fails or no batches available.\",\"load_iterator_state(loader_state: str)\",\"Load iterator state from a file.\",\"Parameters:loader_state – Path to the iterator state file\",\"Raises:\",\"FileNotFoundError – If the state file doesn’t exist\",\"KeyError – If required keys are missing in the state file\",\"save_iterator_state(loader_state: str)\",\"Save the current state of the iterator to a file.\",\"Parameters:loader_state – Path to save the iterator state file\"]},\"2135\":{\"h\":\"espnet2.speechlm.trainer.deepspeed_trainer.DeepSpeedTrainer\",\"t\":[\"source\"]},\"2136\":{\"h\":\"espnet2.speechlm.model.speechlm.multimodal_io.audio.DiscreteAudioIO\",\"t\":[\"source\",\"class espnet2.speechlm.model.speechlm.multimodal_io.audio.DiscreteAudioIO(codec_choice: str | None = None, codec_hf_model_tag: str | None = None, codec_max_token_per_frame: int = 8, ssl_choice: str | None = None, ssl_hf_model_tag: str | None = None, stream_weights: List[float] | None = None, delay_interleave: bool = False, device: str = 'cpu')\",\"Bases: AbsIO\",\"Discrete audio I/O using combined codec and SSL tokenizers.\",\"This class handles audio encoding/decoding using both:\",\"Codec tokens (acoustic/low-level features) from neural audio codecs\",\"SSL tokens (semantic/high-level features) from self-supervised models\",\"The tokens from both tokenizers are concatenated frame-by-frame to create a multi-stream representation where semantic and acoustic information are aligned temporally.\",\"Initialize discrete audio I/O handler with combined tokenizers.\",\"Parameters:\",\"codec_choice – Type of codec to use (“ESPnet” or None to disable)\",\"codec_hf_model_tag – HuggingFace model tag for codec tokenizer\",\"codec_max_token_per_frame – Maximum number of codec tokens per frame (default: 8)\",\"ssl_choice – Type of SSL model to use (“ESPnet” or None to disable)\",\"ssl_hf_model_tag – HuggingFace model tag for SSL model (e.g., “espnet/xeus”)\",\"stream_weights – Loss weights for each stream. List of weight values, one for each stream. Order should be [SSL streams, codec streams]. If None, all streams get equal weight (1.0).\",\"delay_interleave – Whether to apply delay interleaving to multi-stream tokens (default: False)\",\"device – Device to run models on (default: “cpu”)\",\"copy_for_worker() → DiscreteAudioIO\",\"Create lightweight copy for multiprocessing workers.\",\"Creates a new instance with the same parameters (loads models) then removes the heavy model components to reduce memory usage in workers while keeping necessary metadata.\",\"Returns: Lightweight copy suitable for workers\",\"decode_batch(codes: Tensor, lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Decode a batch of encoded tokens back to audio.\",\"Note: Only codec tokens are used for audio reconstruction. SSL tokens are discarded as they represent semantic information that cannot be directly converted back to waveforms.\",\"Parameters:\",\"codes – Encoded tokens [batch, time, n_streams]\",\"lengths – Frame lengths [batch]\",\"Returns:\",\"audio: Reconstructed audio [batch, num_channels, num_samples]\",\"audio_lengths: Sample lengths [batch]\",\"Return type: Tuple of\",\"encode_batch(data: Tensor, lengths: Tensor) → Tensor\",\"Encode a batch of audio data into discrete tokens.\",\"Parameters:\",\"data – Audio tensor of shape [batch, samples, num_channel]\",\"lengths – Effective sample lengths [batch]\",\"Returns: Encoded tokens [batch, time, n_streams]\",\"Return type: codes\",\"find_length(data: Tuple[ndarray, int]) → int\",\"Calculate frame length after encoding.\",\"Parameters:data – Tuple of (audio_array, sample_rate) where audio_array has shape [num_channels, num_samples]\",\"Returns: Frame length after encoding (number of frames)\",\"get_stream_interval() → List[Tuple[int, int]] | None\",\"Get vocabulary index ranges for each stream.\",\"SSL streams come first, followed by codec streams. Each tuple represents (start_index, end_index) for that stream.\",\"Returns: List of (start, end) tuples for each stream’s vocabulary range\",\"get_stream_weight() → List[float] | None\",\"Get loss weights for each stream.\",\"Returns: List of weight values for each stream. Order is [SSL streams, codec streams].\",\"get_vocabulary() → List[str] | None\",\"Get the complete vocabulary list across all streams.\",\"Returns: List of all token symbols for SSL and codec combined\",\"num_stream() → int | None\",\"Get number of parallel streams (SSL + codec).\",\"Returns: Total number of streams combining SSL and codec\",\"preprocess(data: Tuple[ndarray, int]) → Tuple[ndarray, Tuple[int, ndarray] | None, ndarray]\",\"Preprocess audio for discrete tokenization.\",\"Since tokenization happens on GPU, this returns placeholder sequences and passes raw audio as continuous features for on-the-fly encoding.\",\"Parameters:data – Tuple of (audio_array, sample_rate) where audio_array has shape [num_channels, num_samples]\",\"Returns:\",\"seq: Zero-filled placeholder array [length, num_stream]\",\"conti_feat: Tuple of (length, transposed_audio) for GPU encoding\",\"loss_mask: Stream weights broadcasted to [length, num_stream]\",\"Return type: Tuple of (seq, conti_feat, loss_mask)\"]},\"2137\":{\"h\":\"espnet2.speechlm.model.speechlm.multimodal_io.text.HuggingFaceTextIO\",\"t\":[\"source\",\"class espnet2.speechlm.model.speechlm.multimodal_io.text.HuggingFaceTextIO(tokenizer_name: str)\",\"Bases: AbsIO\",\"Text I/O using HuggingFace tokenizers.\",\"This class provides text tokenization using HuggingFace’s pretrained tokenizers. Text is discrete with a single stream.\",\"Initialize HuggingFace text tokenizer.\",\"Parameters:tokenizer_name – HuggingFace model name or path (e.g., “bert-base-uncased”, “gpt2”)\",\"copy_for_worker() → HuggingFaceTextIO\",\"Create copy for multiprocessing workers.\",\"Returns: New instance with same tokenizer\",\"decode(tokens: ndarray) → str\",\"Decode a 1D tensor of token IDs to text string.\",\"Parameters:tokens – 1D numpy array of token IDs [seq_len]\",\"Returns: Decoded text string\",\"find_length(data: str) → int\",\"Get token count for length statistics.\",\"Parameters:data – Text string\",\"Returns: Number of tokens after tokenization\",\"get_stream_interval() → List[Tuple[int, int]]\",\"Get vocabulary range for single stream.\",\"Returns: [(0, vocab_size)] for text’s single stream\",\"get_stream_weight() → List[float]\",\"Get loss weight for single stream.\",\"Returns: [1.0] for single text stream\",\"get_vocabulary() → List[str]\",\"Get tokenizer vocabulary.\",\"Returns: List of all tokens, padded to model vocab size\",\"num_stream() → int\",\"Text uses single stream.\",\"preprocess(data: str) → Tuple[ndarray, None, ndarray]\",\"Tokenize single text string for data loading.\",\"Parameters:data – Single text string\",\"Returns:\",\"tokens: Token IDs as numpy array [seq_len, 1]\",\"conti_feat: None (text is discrete)\",\"loss_mask: Loss weights [seq_len, 1], all 1.0\",\"Return type: Tuple of (tokens, conti_feat, loss_mask)\"]},\"2138\":{\"h\":\"espnet2.speechlm.model.speechlm.multimodal_io.audio.KmeansModel\",\"t\":[\"source\",\"class espnet2.speechlm.model.speechlm.multimodal_io.audio.KmeansModel(km_path: str, device: str = 'cpu')\",\"Bases: Module\",\"Apply k-means clustering to quantize SSL features into discrete tokens.\",\"This class loads a pre-trained k-means model and uses it to convert continuous SSL features into discrete cluster indices (tokens).\",\"Initialize k-means quantizer from saved model.\",\"Parameters:\",\"km_path – Path to saved k-means model file\",\"device – Device to place tensors on (default: “cpu”)\"]},\"2139\":{\"h\":\"espnet2.speechlm.dataloader.multimodal_loader.audio_loader.LhotseAudioReader\",\"t\":[\"source\",\"class espnet2.speechlm.dataloader.multimodal_loader.audio_loader.LhotseAudioReader(manifest_dir: str, valid_ids: list | None = None)\",\"Bases: object\",\"Dict-like lazy audio reader using Lhotse manifests.\",\"This reader supports both single-channel and multi-channel audio data:\",\"Single-channel audio (MonoCut): Returns shape [1, num_samples]\",\"Multi-channel audio (MultiCut): Returns shape [num_channels, num_samples]\",\"The output shape is consistent regardless of the input type, always returning a 2D array with shape [num_channels, num_samples].\",\"Parameters:\",\"manifest_dir – Directory containing Lhotse manifest files (recordings.jsonl.gz and optionally cuts.jsonl.gz)\",\"valid_ids – List of valid IDs to keep (optional, keeps all if None)\",\"items()\",\"Return iterator over (id, item) pairs.\",\"keys()\",\"Return iterator over IDs.\",\"values()\",\"Return iterator over items.\"]},\"2140\":{\"h\":\"espnet2.speechlm.model.speechlm.lm.parallel.ParallelHFModel\",\"t\":[\"source\",\"espnet2.speechlm.model.speechlm.lm.parallel.ParallelHFModel(model_hf_tag, **kwargs)\",\"Factory function to create a parallel multimodal LLM from HuggingFace model.\",\"Parameters:\",\"model_hf_tag – HuggingFace model identifier\",\"**kwargs – Additional arguments passed to from_pretrained\",\"Returns: Instantiated parallel LLM model with multimodal capabilities\"]},\"2141\":{\"h\":\"espnet2.speechlm.dataloader.dataset.SingleDataset\",\"t\":[\"source\",\"class espnet2.speechlm.dataloader.dataset.SingleDataset(json_file: str, rank: int = 0, world_size: int = 1)\",\"Bases: Dataset\",\"ESPnet Speech Language Model Dataset.\",\"Parameters:\",\"json_file – Path to dataset JSON created by prepare_dataset_json.py\",\"rank – Process rank for distributed training (default: 0)\",\"world_size – Total number of processes (default: 1)\",\"property entries : List[str]\",\"Return list of all data entry names.\",\"property sample_ids : List[str]\",\"Return list of all sample IDs.\"]},\"2142\":{\"h\":\"espnet2.speechlm.model.speechlm.speechlm_job.SpeechLMJobTemplate\",\"t\":[\"source\",\"class espnet2.speechlm.model.speechlm.speechlm_job.SpeechLMJobTemplate(config: Dict[str, Any])\",\"Bases: AbsJobTemplate\",\"Job template for SpeechLM training tasks.\",\"This class implements the specific model and data processing configurations for speech language modeling tasks.\",\"Initialize the SpeechLM job template.\",\"Parameters:config – Dictionary containing job configuration parameters.\",\"build_model() → Module\",\"Build the SpeechLM model.\",\"Returns: A SpeechLM model instance.\",\"build_preprocessor() → Callable\",\"Build the data collation function for SpeechLM.\",\"Returns: A callable function for collating SpeechLM batch data.\"]},\"2143\":{\"h\":\"espnet2.speechlm.model.speechlm.speechlm_job.SpeechLMPreprocessor\",\"t\":[\"source\",\"class espnet2.speechlm.model.speechlm.speechlm_job.SpeechLMPreprocessor(multimodal_io, vocab, vocab_intervals, audio_input: str = 'continuous_audio', audio_output: str = 'discrete_audio', loss_region: str = 'assistant', batchfy_method: str = 'bucket')\",\"Bases: object\",\"Preprocessor for SpeechLM data handling.\",\"Converts raw data into model-ready format with tokenization, padding, and loss mask generation for multimodal sequences.\",\"collate_fn(data_lst)\",\"Batch multiple samples for training.\",\"Processes each sample, pads sequences to same length, and organizes continuous features by modality. Returns dict ready for model forward.\",\"diagnose(data)\",\"Print human-readable representation of processed data for debugging.\",\"Shows tokens, loss masks, and continuous feature info frame by frame.\",\"find_length(key, data_dict)\",\"Quickly compute sequence length without full preprocessing.\",\"Counts tokens for BOS, role/modality markers, content, and EOS/EOT. Used for efficient batch construction.\",\"preprocessing(key, data_dict)\",\"Convert single raw data dict into training-ready format.\",\"Applies chat template, tokenizes content, adds special tokens, and creates loss masks. Returns dict with sequences and features.\",\"special_mask(value)\",\"Create loss mask for special tokens (1 frame, multi-stream).\",\"Only first stream has the actual value, others are zero.\",\"special_token(token)\",\"Convert special token string to multi-stream token array.\",\"Places token ID in first stream, padding tokens in other streams.\"]},\"2144\":{\"h\":\"espnet2.speechlm.dataloader.multimodal_loader.text_loader.TextReader\",\"t\":[\"source\",\"class espnet2.speechlm.dataloader.multimodal_loader.text_loader.TextReader(text_file: str, valid_ids: list | None = None)\",\"Bases: object\",\"Dict-like text reader supporting plain and JSONL formats.\",\"Plain format: <id> <text content> JSONL format: {“id”: “<id>”, “text”: “<text content>”}\",\"Format is determined by file suffix (.jsonl for JSONL, otherwise plain).\",\"Parameters:\",\"text_file – Path to text file (plain or JSONL format)\",\"valid_ids – List of valid IDs to keep (optional, keeps all if None)\",\"items()\",\"Return iterator over (id, text) pairs.\",\"keys()\",\"Return iterator over IDs.\",\"values()\",\"Return iterator over texts.\"]},\"2145\":{\"h\":\"espnet2.speechlm.dataloader.batch.batchfy\",\"t\":[\"source\",\"espnet2.speechlm.dataloader.batch.batchfy(keys: List[T], key_to_length: Dict[T, int], batch_token: int, batch_method: str) → List[List[T]]\",\"Create batches using the specified batching method.\",\"Parameters:\",\"keys – List of sample keys to batch.\",\"key_to_length – Dictionary mapping each key to its length.\",\"batch_token – Maximum number of tokens allowed per batch.\",\"batch_method – Batching method to use (“bucket” or “pack”).\",\"Returns: List of batches, where each batch is a list of keys.\",\"Raises:ValueError – If batch_method is invalid.\",\"Notes\",\"Samples with length exceeding batch_token are automatically discarded and a warning is logged.\"]},\"2146\":{\"h\":\"espnet2.speechlm.dataloader.batch.batchfy_bucket\",\"t\":[\"source\",\"espnet2.speechlm.dataloader.batch.batchfy_bucket(keys: List[T], key_to_length: Dict[T, int], batch_token: int) → List[List[T]]\",\"Create batches using bucket batching strategy.\",\"Samples are sorted by length and grouped into buckets such that the total tokens (max_length * batch_size) does not exceed the batch_token limit.\",\"Parameters:\",\"keys – List of sample keys to batch.\",\"key_to_length – Dictionary mapping each key to its length.\",\"batch_token – Maximum number of tokens allowed per batch.\",\"Returns: List of buckets, where each bucket is a list of keys.\"]},\"2147\":{\"h\":\"espnet2.speechlm.dataloader.batch.batchfy_pack\",\"t\":[\"source\",\"espnet2.speechlm.dataloader.batch.batchfy_pack(keys: List[T], key_to_length: Dict[T, int], batch_token: int) → List[List[T]]\",\"Create batches using pack batching strategy.\",\"Uses Best Fit Decreasing algorithm to maximize batch utilization. Samples are sorted by length (descending) and packed into batches by finding the batch with minimum remaining space that can fit each sample. Batches at 99% capacity are marked as finished.\",\"Parameters:\",\"keys – List of sample keys to batch.\",\"key_to_length – Dictionary mapping each key to its length.\",\"batch_token – Maximum number of tokens allowed per batch.\",\"Returns: List of batches, where each batch is a list of keys.\"]},\"2148\":{\"h\":\"espnet2.speechlm.model.speechlm.lm.parallel.build_parallel_hf_class\",\"t\":[\"source\",\"espnet2.speechlm.model.speechlm.lm.parallel.build_parallel_hf_class(model_hf_tag)\",\"Dynamically create a parallel LLM class based on HuggingFace architecture.\",\"Creates a subclass of the original HF model with added multimodal support, parallel stream processing, and custom embedding/loss computation.\",\"Parameters:model_hf_tag – HuggingFace model identifier to determine base architecture\",\"Returns: ParallelLLM class inheriting from the original HF architecture\"]},\"2149\":{\"h\":\"espnet2.speechlm.bin.prepare_length_stats.collect_length_stats\",\"t\":[\"source\",\"espnet2.speechlm.bin.prepare_length_stats.collect_length_stats(preprocessor, num_workers: int, spec_type: str, specifier: str) → Dict[Tuple[str, ...], int]\",\"Collect length statistics from all worker processes.\",\"Parameters:\",\"preprocessor – Data preprocessor\",\"num_workers – Number of parallel workers\",\"spec_type – Either “unregistered” or “registered”\",\"specifier – The data specifier string\",\"Returns: Aggregated statistics dictionary\"]},\"2150\":{\"h\":\"espnet2.speechlm.bin.prepare_audio_lhotse.create_cut_from_recording\",\"t\":[\"source\",\"espnet2.speechlm.bin.prepare_audio_lhotse.create_cut_from_recording(cut_id: str, recording: Recording, start: float, duration: float) → MonoCut | MultiCut\",\"Create a cut (MonoCut or MultiCut) from a recording.\",\"Parameters:\",\"cut_id – ID for the cut\",\"recording – Recording object\",\"start – Start time in seconds\",\"duration – Duration in seconds\",\"Returns: MonoCut for single-channel or MultiCut for multi-channel recordings\"]},\"2151\":{\"h\":\"espnet2.speechlm.utils.model_summary.get_human_readable_count\",\"t\":[\"source\",\"espnet2.speechlm.utils.model_summary.get_human_readable_count(number: int) → str\",\"Return human_readable_count\",\"Originated from: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/memory.py\",\"Abbreviates an integer number with K, M, B, T for thousands, millions, billions and trillions, respectively. .. rubric:: Examples\",\">>> get_human_readable_count(123) '123 ' >>> get_human_readable_count(1234) # (one thousand) '1 K' >>> get_human_readable_count(2e6) # (two million) '2 M' >>> get_human_readable_count(3e9) # (three billion) '3 B' >>> get_human_readable_count(4e12) # (four trillion) '4 T' >>> get_human_readable_count(5e15) # (more than trillion) '5,000 T'\",\"Parameters:number – a positive integer number\",\"Returns: A string formatted according to the pattern described above.\"]},\"2152\":{\"h\":\"espnet2.speechlm.bin.prepare_length_stats.get_parser\",\"t\":[\"source\",\"espnet2.speechlm.bin.prepare_length_stats.get_parser() → ArgumentParser\",\"Build argument parser for length statistics preparation.\"]},\"2153\":{\"h\":\"espnet2.speechlm.bin.prepare_length_stats.main\",\"t\":[\"source\",\"espnet2.speechlm.bin.prepare_length_stats.main()\",\"Main entry point for length statistics preparation.\"]},\"2154\":{\"h\":\"espnet2.speechlm.utils.model_summary.model_summary\",\"t\":[\"source\",\"espnet2.speechlm.utils.model_summary.model_summary(model: Module) → str\"]},\"2155\":{\"h\":\"espnet2.speechlm.utils.data.pad_list\",\"t\":[\"source\",\"espnet2.speechlm.utils.data.pad_list(sequences: List[ndarray | Tensor], pad_value: float = 0.0) → Tuple[Tensor, Tensor]\",\"Pad a list of sequences to the same length and stack them.\",\"Uses right padding (padding at the end of sequences). Assumes the FIRST dimension is the time/sequence length.\",\"Parameters:\",\"sequences – List of sequences to pad and stack. Each can be of any shape [seq_len, …] where seq_len is variable\",\"pad_value – Value to use for padding (default: 0.0)\",\"Returns:\",\"Padded and stacked tensor of shape [batch, max_seq_len, …]\",\"Length tensor of shape [batch] with original sequence lengths (dtype=long)\",\"Return type: Tuple of\",\"Raises:\",\"ValueError – If sequence list is empty or sequences have inconsistent shapes\",\"TypeError – If sequences contain non-tensor/non-array types\"]},\"2156\":{\"h\":\"espnet2.speechlm.bin.prepare_audio_lhotse.parse_segments_file\",\"t\":[\"source\",\"espnet2.speechlm.bin.prepare_audio_lhotse.parse_segments_file(segments_path: str, recording_dict: Dict[str, Recording]) → List[Tuple[str, Recording, float, float]]\",\"Parse and validate segments file.\",\"Parameters:\",\"segments_path – Path to segments file\",\"recording_dict – Dictionary mapping recording IDs to Recording objects\",\"Returns: List of validated (segment_id, recording, start, duration) tuples\"]},\"2157\":{\"h\":\"espnet2.speechlm.bin.prepare_audio_lhotse.prepare_audio_lhotse\",\"t\":[\"source\",\"espnet2.speechlm.bin.prepare_audio_lhotse.prepare_audio_lhotse(wav_scp: str, segments: str | None, output_dir: str, num_jobs: int, log_level: str)\",\"Process Kaldi wav.scp and segments to create Lhotse manifests.\",\"Parameters:\",\"wav_scp – Path to Kaldi wav.scp file\",\"segments – Path to Kaldi segments file (optional)\",\"output_dir – Directory to save Lhotse manifest files\",\"num_jobs – Number of parallel jobs for processing\",\"log_level – Logging level\"]},\"2158\":{\"h\":\"espnet2.speechlm.bin.prepare_dataset_json.prepare_dataset_json\",\"t\":[\"source\"]},\"2159\":{\"h\":\"espnet2.speechlm.bin.prepare_audio_lhotse.print_statistics\",\"t\":[\"source\",\"espnet2.speechlm.bin.prepare_audio_lhotse.print_statistics(cut_set: CutSet, recording_set: RecordingSet, multi_channel_count: int)\",\"Print summary statistics for the processed data.\"]},\"2160\":{\"h\":\"espnet2.speechlm.bin.prepare_audio_lhotse.process_recording\",\"t\":[\"source\",\"espnet2.speechlm.bin.prepare_audio_lhotse.process_recording(recording_id: str, audio_path: str) → Recording | None\",\"Process a single recording to extract metadata.\"]},\"2161\":{\"h\":\"espnet2.speechlm.bin.prepare_length_stats.save_stats\",\"t\":[\"source\",\"espnet2.speechlm.bin.prepare_length_stats.save_stats(stats: Dict[Tuple[str, ...], int], output_file: Path) → None\",\"Save statistics dictionary to JSONL file.\"]},\"2162\":{\"h\":\"espnet2.speechlm.dataloader.batch.synchronize_batches\",\"t\":[\"source\",\"espnet2.speechlm.dataloader.batch.synchronize_batches(batches: List[List[T]]) → List[List[T]]\",\"Synchronize batches across all GPU ranks in distributed training.\",\"Ensures all GPU ranks have the same number of batches by duplicating the last few batches on ranks with fewer batches. This is useful for distributed training where each rank may have different numbers of batches due to data sharding.\",\"Parameters:batches – List of batches to synchronize.\",\"Returns: Synchronized list of batches with duplicates added if necessary.\",\"Notes\",\"If torch.distributed is not initialized, returns unchanged\",\"If CUDA is not available, returns batches unchanged\",\"Duplicates are taken from the end of the batch list\"]},\"2163\":{\"h\":\"espnet2.speechlm.utils.model_summary.to_bytes\",\"t\":[\"source\",\"espnet2.speechlm.utils.model_summary.to_bytes(dtype) → int\"]},\"2164\":{\"h\":\"espnet2.speechlm.utils.data.to_device\",\"t\":[\"source\",\"espnet2.speechlm.utils.data.to_device(data, device=None, dtype=None, non_blocking=False, copy=False)\",\"Change the device of object recursively\"]},\"2165\":{\"h\":\"espnet2.speechlm.bin.prepare_dataset_json.validate_triplet\",\"t\":[\"source\"]},\"2166\":{\"h\":\"espnet2.speechlm.bin.prepare_length_stats.worker\",\"t\":[\"source\",\"espnet2.speechlm.bin.prepare_length_stats.worker(preprocessor, rank: int, world_size: int, unregistered_spec: str = '', registered_spec: str = '')\",\"Worker function to collect length statistics for a data shard.\"]},\"2167\":{\"h\":\"espnet2.spk.loss.aamsoftmax.AAMSoftmax\",\"t\":[\"source\",\"class espnet2.spk.loss.aamsoftmax.AAMSoftmax(nout: int, nclasses: int, margin: float = 0.3, scale: int = 15, easy_margin: bool = False, **kwargs)\",\"Bases: AbsLoss\",\"Additive angular margin softmax.\",\"Reference: ArcFace: Additive Angular Margin Loss for Deep Face Recognition https://arxiv.org/pdf/1801.07698\",\"Parameters:\",\"nout – Dimension of input features (embedding size)\",\"nclasses – Number of output classes\",\"margin – Angular margin for positive samples\",\"scale – Feature scaling factor\",\"easy_margin – Whether to use easy margin variant\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, label: Tensor | None = None) → Tuple[Tensor | None, Tensor | None, Tensor]\",\"Forward pass of AAMSoftmax loss.\",\"Parameters:\",\"input – Input embeddings, shape (batch_size, embedding_dim)\",\"label – Ground truth labels, shape (batch_size,)\",\"Returns: Cross-entropy loss with angular margins accuracy: Classification accuracy preds: Predicted class indices\",\"Return type: loss\"]},\"2168\":{\"h\":\"espnet2.spk.layers.rawnet_block.AFMS\",\"t\":[\"source\",\"class espnet2.spk.layers.rawnet_block.AFMS(nb_dim: int)\",\"Bases: Module\",\"Alpha-Feature map scaling, added to the output of each residual block[1,2].\",\"Reference: [1] RawNet2 : https://www.isca-speech.org/archive/Interspeech_2020/pdfs/1011.pdf [2] AMFS : https://www.koreascience.or.kr/article/JAKO202029757857763.page\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2169\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2170\":{\"h\":\"espnet2.spk.loss.abs_loss.AbsLoss\",\"t\":[\"source\",\"class espnet2.spk.loss.abs_loss.AbsLoss(nout: int, **kwargs)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(x: Tensor, label=None) → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2171\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2172\":{\"h\":\"espnet2.spk.pooling.abs_pooling.AbsPooling\",\"t\":[\"source\",\"class espnet2.spk.pooling.abs_pooling.AbsPooling(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor) → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2173\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size() → int\"]},\"2174\":{\"h\":\"espnet2.spk.projector.abs_projector.AbsProjector\",\"t\":[\"source\",\"class espnet2.spk.projector.abs_projector.AbsProjector(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(utt_embd: Tensor) → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2175\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size() → int\"]},\"2176\":{\"h\":\"espnet2.spk.loss.aamsoftmax_subcenter_intertopk.ArcMarginProduct_intertopk_subcenter\",\"t\":[\"source\",\"class espnet2.spk.loss.aamsoftmax_subcenter_intertopk.ArcMarginProduct_intertopk_subcenter(nout: int, nclasses: int, scale: float = 32.0, margin: float = 0.2, easy_margin: bool = False, K: int = 3, mp: float = 0.06, k_top: int = 5, do_lm: bool = False)\",\"Bases: AbsLoss\",\"ArcFace loss (AAMSoftmax loss) with Inter-TopK penalty and Sub-center.\",\"This loss function combines three techniques:\",\"ArcFace: Additive angular margin loss for better feature discrimination\",\"Sub-center: Multiple prototypes per class to handle intra-class variation\",\"Inter-TopK: Additional penalty on hardest negative samples\",\"Reference: : Multi-Query Multi-Head Attention Pooling and Inter-TopK Penalty for Speaker Verification https://arxiv.org/pdf/2110.05042.pdf Sub-Center ArcFace: Boosting Face Recognition by Large-Scale Noisy Web Faces. https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560715.pdf\",\"Parameters:\",\"nout – Dimension of input features (embedding size)\",\"nclasses – Number of output classes\",\"scale – Feature scaling factor\",\"margin – Angular margin for positive samples\",\"easy_margin – Whether to use easy margin variant\",\"K – Number of sub-centers per class\",\"mp – Margin penalty for hard negative samples\",\"k_top – Number of hardest negative samples to penalize\",\"do_lm – Whether to enable Large Margin Fine-tuning mode\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, label: Tensor | None = None) → Tuple[Tensor | None, Tensor | None, Tensor]\",\"Forward pass of ArcFace (AAMSoftmax) with sub-center and inter-topk penalty.\",\"Parameters:\",\"input – Input embeddings, shape (batch_size, embedding_dim)\",\"label – Ground truth labels, shape (batch_size,)\",\"Returns: Cross-entropy loss with angular margins accuracy: Classification accuracy pred_lids: Predicted class indices\",\"Return type: loss\",\"update(margin: float = 0.2)\",\"Update margin and related trigonometric values during training.\"]},\"2177\":{\"h\":\"espnet2.spk.layers.resnet_block.BasicBlock\",\"t\":[\"source\",\"class espnet2.spk.layers.resnet_block.BasicBlock(in_planes, planes, stride=1)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"expansion = 1\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2178\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2179\":{\"h\":\"espnet2.spk.layers.rawnet_block.Bottle2neck\",\"t\":[\"source\",\"class espnet2.spk.layers.rawnet_block.Bottle2neck(inplanes, planes, kernel_size=None, dilation=None, scale=4, pool=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2180\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2181\":{\"h\":\"espnet2.spk.layers.resnet_block.Bottleneck\",\"t\":[\"source\",\"class espnet2.spk.layers.resnet_block.Bottleneck(in_planes, planes, stride=1)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"expansion = 4\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2182\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2183\":{\"h\":\"espnet2.spk.pooling.chn_attn_stat_pooling.ChnAttnStatPooling\",\"t\":[\"source\",\"class espnet2.spk.pooling.chn_attn_stat_pooling.ChnAttnStatPooling(input_size: int = 1536)\",\"Bases: AbsPooling\",\"Aggregates frame-level features to single utterance-level feature.\",\"Reference: ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification https://arxiv.org/pdf/2005.07143\",\"Parameters:input_size – Dimension of the input frame-level embeddings. The output dimensionality will be 2 × input_size after concatenating mean and std.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, feat_lengths: Tensor | None = None) → Tensor\",\"Forward pass of channel-attentive statistical pooling.\",\"Parameters:\",\"x – Input feature tensor of shape (batch_size, feature_dim, seq_len)\",\"feat_lengths – Optional tensor of shape (batch_size,) containing the valid length of each sequence before padding\",\"Returns: Utterance-level embeddings of shape (batch_size, 2 × feature_dim)\",\"Return type: x\",\"output_size()\"]},\"2184\":{\"h\":\"espnet2.spk.espnet_model.ESPnetSpeakerModel\",\"t\":[\"source\",\"class espnet2.spk.espnet_model.ESPnetSpeakerModel(frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, encoder: AbsEncoder | None, pooling: AbsPooling | None, projector: AbsProjector | None, loss: AbsLoss | None)\",\"Bases: AbsESPnetModel\",\"Speaker embedding extraction model.\",\"Core model for diverse speaker-related tasks (e.g., verification, open-set identification, diarization)\",\"The model architecture comprises mainly ‘encoder’, ‘pooling’, and ‘projector’. In common speaker recognition field, the combination of three would be usually named as ‘speaker_encoder’ (or speaker embedding extractor). We splitted it into three for flexibility in future extensions:\",\"‘encoder’ : extract frame-level speaker embeddings.\",\"‘pooling’ : aggregate into single utterance-level embedding.\",\"‘projector’ : connected layer) to derive speaker embedding.\",\"Possibly, in the future, ‘pooling’ and/or ‘projector’ can be integrated as a ‘decoder’, depending on the extension for joint usage of different tasks (e.g., ASR, SE, target speaker extraction).\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"aggregate(frame_level_feats: Tensor) → Tensor\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, spk_labels: Tensor | None = None, **kwargs) → Dict[str, Tensor]\",\"encode_frame(feats: Tensor) → Tensor\",\"extract_feats(speech: Tensor, speech_lengths: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"forward(speech: Tensor, speech_lengths: Tensor | None = None, spk_labels: Tensor | None = None, task_tokens: Tensor | None = None, extract_embd: bool = False, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor] | Tensor\",\"Feed-forward pass of the speaker model.\",\"Parameters:\",\"speech – (Batch, samples), Input speech tensor\",\"speech_lengths – (Batch,), optional, Length of each speech sequence in the batch. Required when speech is padded to ensure proper pooling over unpadded frames.\",\"extract_embd – bool, If True, returns speaker embeddings without computing loss. Used for inference or feature extraction.\",\"spk_labels – (Batch,), Speaker labels for training. One-hot encoded speaker identifiers used during the training phase.\",\"task_tokens – (Batch,), Task-specific tokens for multi-task learning scenarios.\",\"project_spk_embd(utt_level_feat: Tensor) → Tensor\"]},\"2185\":{\"h\":\"espnet2.spk.layers.ecapa_block.EcapaBlock\",\"t\":[\"source\",\"class espnet2.spk.layers.ecapa_block.EcapaBlock(inplanes, planes, kernel_size=None, dilation=None, scale=8)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2186\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2187\":{\"h\":\"espnet2.spk.encoder.ecapa_tdnn_encoder.EcapaTdnnEncoder\",\"t\":[\"source\",\"class espnet2.spk.encoder.ecapa_tdnn_encoder.EcapaTdnnEncoder(input_size: int, block: str = 'EcapaBlock', model_scale: int = 8, ndim: int = 1024, output_size: int = 1536, **kwargs)\",\"Bases: AbsEncoder\",\"ECAPA-TDNN encoder. Extracts frame-level ECAPA-TDNN embeddings from\",\"mel-filterbank energy or MFCC features. Paper: B Desplanques at el.,\",\"``\",\"ECAPA-TDNN: Emphasized Channel Attention,\",\"Propagation and Aggregation in TDNN Based Speaker Verification,’’ in Proc. INTERSPEECH, 2020.\",\"Parameters:\",\"input_size – input feature dimension.\",\"block – type of encoder block class to use.\",\"model_scale – scale value of the Res2Net architecture.\",\"ndim – dimensionality of the hidden representation.\",\"output_size – output embedding dimension.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Calculate forward propagation.\",\"Parameters:x (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"Returns: Output tensor (#batch, L, output_size).\",\"Return type: torch.Tensor\",\"output_size() → int\"]},\"2188\":{\"h\":\"espnet2.spk.encoder.identity_encoder.IdentityEncoder\",\"t\":[\"source\",\"class espnet2.spk.encoder.identity_encoder.IdentityEncoder(input_size: int)\",\"Bases: AbsEncoder\",\"Identity encoder. Does nothing, just passes frontend feature to the pooling.\",\"Expected to be used for cases when frontend already has a good representation (e.g., SSL features).\",\"Parameters:input_size – input feature dimension.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2189\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\"]},\"2190\":{\"h\":\"espnet2.spk.pooling.mean_pooling.MeanPooling\",\"t\":[\"source\",\"class espnet2.spk.pooling.mean_pooling.MeanPooling(input_size: int = 1536)\",\"Bases: AbsPooling\",\"Average frame-level features to a single utterance-level feature.\",\"Parameters:input_size – Dimension of the input frame-level embeddings.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, feat_lengths: Tensor | None = None) → Tensor\",\"Forward pass of mean pooling.\",\"Parameters:\",\"x – Input feature tensor of shape (batch_size, feature_dim, seq_len)\",\"feat_lengths – Optional tensor of shape (batch_size,) containing the valid length of each sequence before padding\",\"Returns: Utterance-level embeddings of shape (batch_size, feature_dim)\",\"Return type: x\",\"output_size()\"]},\"2191\":{\"h\":\"espnet2.spk.encoder.conformer_encoder.MfaConformerEncoder\",\"t\":[\"source\",\"class espnet2.spk.encoder.conformer_encoder.MfaConformerEncoder(input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str | None = 'conv2d2', normalize_before: bool = True, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'rel_pos', selfattention_layer_type: str = 'rel_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, stochastic_depth_rate: float | List[float] = 0.0, layer_drop_rate: float = 0.0, max_pos_emb_len: int = 5000, padding_idx: int | None = None)\",\"Bases: AbsEncoder\",\"Conformer encoder module for MFA-Conformer.\",\"Paper: Y. Zhang et al.,\",\"``\",\"Mfa-conformer: Multi-scale feature aggregation conformer for automatic speaker verification,’’ in Proc. INTERSPEECH, 2022.\",\"Parameters:\",\"input_size (int) – Input dimension.\",\"output_size (int) – Dimension of attention.\",\"attention_heads (int) – The number of heads of multi head attention.\",\"linear_units (int) – The number of units of position-wise feed forward.\",\"num_blocks (int) – The number of encoder blocks.\",\"dropout_rate (float) – Dropout rate.\",\"attention_dropout_rate (float) – Dropout rate in attention.\",\"positional_dropout_rate (float) – Dropout rate after adding positional encoding.\",\"input_layer (Union *[*str,torch.nn.Module]) – Input layer type.\",\"normalize_before (bool) – Whether to use layer_norm before the first block.\",\"positionwise_layer_type (str) – “linear”, “conv1d”, or “conv1d-linear”.\",\"positionwise_conv_kernel_size (int) – Kernel size of positionwise conv1d layer.\",\"rel_pos_type (str) – Whether to use the latest relative positional encoding or the legacy one. The legacy relative positional encoding will be deprecated in the future. More Details can be found in https://github.com/espnet/espnet/pull/2816.\",\"encoder_pos_enc_layer_type (str) – Encoder positional encoding layer type.\",\"encoder_attn_layer_type (str) – Encoder attention layer type.\",\"activation_type (str) – Encoder activation function type.\",\"macaron_style (bool) – Whether to use macaron style for positionwise layer.\",\"use_cnn_module (bool) – Whether to use convolution module.\",\"zero_triu (bool) – Whether to zero the upper triangular part of attention matrix.\",\"cnn_module_kernel (int) – Kernerl size of convolution module.\",\"padding_idx (int) – Padding idx for input_layer=embed.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor) → Tuple[Tensor, Tensor, Tensor | None]\",\"Calculate forward propagation.\",\"Parameters:x (torch.Tensor) – Input tensor (#batch, L, input_size).\",\"Returns: Output tensor (#batch, L, output_size).\",\"Return type: torch.Tensor\",\"output_size() → int\"]},\"2192\":{\"h\":\"espnet2.spk.encoder.rawnet3_encoder.RawNet3Encoder\",\"t\":[\"source\",\"class espnet2.spk.encoder.rawnet3_encoder.RawNet3Encoder(input_size: int, block: str = 'Bottle2neck', model_scale: int = 8, ndim: int = 1024, output_size: int = 1536, **kwargs)\",\"Bases: AbsEncoder\",\"RawNet3 encoder. Extracts frame-level RawNet embeddings from raw waveform.\",\"paper: J. Jung et al., “Pushing the limits of raw waveform speaker : recognition”, in Proc. INTERSPEECH, 2022.\",\"Parameters:\",\"input_size – input feature dimension.\",\"block – type of encoder block class to use.\",\"model_scale – scale value of the Res2Net architecture.\",\"ndim – dimensionality of the hidden representation.\",\"output_size – ouptut embedding dimension.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2193\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\"]},\"2194\":{\"h\":\"espnet2.spk.projector.rawnet3_projector.RawNet3Projector\",\"t\":[\"source\",\"class espnet2.spk.projector.rawnet3_projector.RawNet3Projector(input_size, output_size=192)\",\"Bases: AbsProjector\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2195\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\"]},\"2196\":{\"h\":\"espnet2.spk.encoder.ska_tdnn_encoder.ResBlock\",\"t\":[\"source\",\"class espnet2.spk.encoder.ska_tdnn_encoder.ResBlock(inplanes: int, planes: int, stride: int = 1, reduction: int = 8, skfwse_freq: int = 40, skcwse_channel: int = 128)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2197\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2198\":{\"h\":\"espnet2.spk.encoder.resnet_encoder.ResNetEncoder\",\"t\":[\"source\",\"class espnet2.spk.encoder.resnet_encoder.ResNetEncoder(input_size: int, block: ~typing.Type[~torch.nn.modules.module.Module] = <class 'espnet2.spk.layers.resnet_block.BasicBlock'>, num_blocks: tuple = (2, 2, 2, 2), m_channels: int = 32, resnet_type: str | None = None)\",\"Bases: AbsEncoder\",\"ResNet Encoder. Extracts frame-level ResNet embeddings from\",\"mel-filterbank energy or MFCC features. Paper: K. He et al., “Deep Residual Learning for Image Recognition”, Adapted from https://github.com/wenet-e2e/wespeaker/blob/master/we -speaker/models/resnet.py\",\"Parameters:\",\"input_size – input feature dimension.\",\"block – type of encoder block class, either BasicBlock or Bottleneck.\",\"num_blocks – number of blocks in each layer.\",\"m_channels – number of channels in the first convolution layer.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, **kwargs) → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2199\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\"]},\"2200\":{\"h\":\"espnet2.spk.layers.ecapa_block.SEModule\",\"t\":[\"source\",\"class espnet2.spk.layers.ecapa_block.SEModule(channels: int, bottleneck: int = 128)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2201\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2202\":{\"h\":\"espnet2.spk.encoder.ska_tdnn_encoder.SKAttentionModule\",\"t\":[\"source\",\"class espnet2.spk.encoder.ska_tdnn_encoder.SKAttentionModule(channel=128, reduction=4, L=16, num_kernels=2)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x, convs)\",\"Forward function.\",\"Input: [B, C, T] Split: [K, B, C, T] Fues: [B, C, T] Attention weight: [B, C, 1] Output: [B, C, T]\"]},\"2203\":{\"h\":\"espnet2.spk.encoder.ska_tdnn_encoder.SkaTdnnEncoder\",\"t\":[\"source\",\"class espnet2.spk.encoder.ska_tdnn_encoder.SkaTdnnEncoder(input_size: int, block: str = 'Bottle2neck', ndim: int = 1024, model_scale: int = 8, skablock: str = 'ResBlock', ska_dim: int = 128, output_size: int = 1536, **kwargs)\",\"Bases: AbsEncoder\",\"SKA-TDNN encoder. Extracts frame-level SKA-TDNN embeddings from features.\",\"Paper: S. Mun, J. Jung et al., “Frequency and Multi-Scale Selective Kernel : Attention for Speaker Verification,’ in Proc. IEEE SLT 2022.\",\"Parameters:\",\"input_size – input feature dimension.\",\"block – type of encoder block class to use.\",\"model_scale – scale value of the Res2Net architecture.\",\"ndim – dimensionality of the hidden representation.\",\"output_size – ouptut embedding dimension.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2204\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\"]},\"2205\":{\"h\":\"espnet2.spk.projector.ska_tdnn_projector.SkaTdnnProjector\",\"t\":[\"source\",\"class espnet2.spk.projector.ska_tdnn_projector.SkaTdnnProjector(input_size, output_size)\",\"Bases: AbsProjector\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2206\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\"]},\"2207\":{\"h\":\"espnet2.spk.loss.softmax.Softmax\",\"t\":[\"source\",\"class espnet2.spk.loss.softmax.Softmax(nout: int, nclasses: int)\",\"Bases: AbsLoss\",\"Softmax loss\",\"Parameters:\",\"nout – Dimension of input features (embedding size)\",\"nclasses – Number of output classes\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, label: Tensor | None = None) → Tuple[Tensor | None, Tensor | None, Tensor]\",\"Forward pass of Softmax loss.\",\"Parameters:\",\"input – Input embeddings, shape (batch_size, embedding_dim)\",\"label – Ground truth labels, shape (batch_size,)\",\"Returns: Cross-entropy loss accuracy: Classification accuracy preds: Predicted class indices\",\"Return type: loss\"]},\"2208\":{\"h\":\"espnet2.spk.pooling.stat_pooling.StatsPooling\",\"t\":[\"source\",\"class espnet2.spk.pooling.stat_pooling.StatsPooling(input_size: int = 1536)\",\"Bases: AbsPooling\",\"Aggregates frame-level features to single utterance-level feature.\",\"Reference: X-Vectors: Robust DNN Embeddings for Speaker Recognition https://www.danielpovey.com/files/2018_icassp_xvectors.pdf\",\"Parameters:input_size – Dimension of the input frame-level embeddings.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, feat_lengths: Tensor | None = None) → Tensor\",\"Forward pass of statistics pooling.\",\"Parameters:\",\"x – Input feature tensor of shape (batch_size, feature_dim, seq_len)\",\"feat_lengths – Optional tensor of shape (batch_size,) containing the valid length of each sequence before padding\",\"Returns: Utterance-level embeddings of shape (batch_size, 2 × feature_dim)\",\"Return type: x\",\"output_size()\"]},\"2209\":{\"h\":\"espnet2.spk.encoder.xvector_encoder.XvectorEncoder\",\"t\":[\"source\",\"class espnet2.spk.encoder.xvector_encoder.XvectorEncoder(input_size: int, ndim: int = 512, output_size: int = 1500, kernel_sizes: List = [5, 3, 3, 1, 1], paddings: List = [2, 1, 1, 0, 0], dilations: List = [1, 2, 3, 1, 1], **kwargs)\",\"Bases: AbsEncoder\",\"X-vector encoder. Extracts frame-level x-vector embeddings from features.\",\"Paper: D. Snyder et al., “X-vectors: Robust dnn embeddings for speaker recognition,” in Proc. IEEE ICASSP, 2018.\",\"Parameters:\",\"input_size – input feature dimension.\",\"ndim – dimensionality of the hidden representation.\",\"output_size – ouptut embedding dimension.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2210\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size() → int\"]},\"2211\":{\"h\":\"espnet2.spk.projector.xvector_projector.XvectorProjector\",\"t\":[\"source\",\"class espnet2.spk.projector.xvector_projector.XvectorProjector(input_size, output_size)\",\"Bases: AbsProjector\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2212\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\"]},\"2213\":{\"h\":\"espnet2.spk.encoder.ska_tdnn_encoder.cwSKAttention\",\"t\":[\"source\",\"class espnet2.spk.encoder.ska_tdnn_encoder.cwSKAttention(freq=40, channel=128, kernels=[3, 5], receptive=[3, 5], dilations=[1, 1], reduction=8, groups=1, L=16)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward Function.\",\"Input: [B, C, F, T] Split: [K, B, C, F, T] Fuse: [B, C, F, T] Attention weight: [K, B, C, 1, 1] Output: [B, C, F, T]\"]},\"2214\":{\"h\":\"espnet2.spk.encoder.ska_tdnn_encoder.fwSKAttention\",\"t\":[\"source\",\"class espnet2.spk.encoder.ska_tdnn_encoder.fwSKAttention(freq=40, channel=128, kernels=[3, 5], receptive=[3, 5], dilations=[1, 1], reduction=8, groups=1, L=16)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Forward function.\",\"Input: [B, C, F, T] Split: [K, B, C, F, T] Fues: [B, C, F, T] Attention weight: [K, B, 1, F, 1] Output: [B, C, F, T]\"]},\"2215\":{\"h\":\"espnet2.ssl.loss.abs_loss.AbsSSLLoss\",\"t\":[\"source\",\"class espnet2.ssl.loss.abs_loss.AbsSSLLoss(*args, **kwargs)\",\"Bases: Module, ABC\",\"Abstract loss class for encoder-only SSL model.\",\"Each loss class must contain 2 attributes:\",\"self.util_attributes (List): functions that need to be : performed on encoder input (masking, noise, etc).\",\"self.required_inputs(List): data names needed to perform : loss calculation.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(encoder_output: List, encoder_output_lengths: Tensor | None = None) → Tuple[Tensor, Dict]\",\"Forward for an SSL objective\",\"Parameters:\",\"encoder_output (List) – List of encoded sequences (B, T, D) from each layer\",\"encoder_output_lengths (Tensor) – Lengths of batched encoder sequences (B,),\"]},\"2216\":{\"h\":\"espnet2.ssl.espnet_model.ESPnetSSLModel\",\"t\":[\"source\",\"class espnet2.ssl.espnet_model.ESPnetSSLModel(frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, losses: List[AbsSSLLoss], util_attributes: Set[str], required_inputs: Set[str], util_modules: ModuleDict, token_list: Tuple[str, ...] | List[str] | None = None, extract_feats_in_collect_stats: bool = True, **kwargs)\",\"Bases: AbsESPnetModel\",\"An encoder-only SSL model.\",\"We currently/will support the following SSL objectives:\",\"HuBERT\",\"Data2Vec (in development)\",\"DinoSR (in development)\",\"wav2vec 2.0 (TODO)\",\"w2v-BERT (TODO)\",\"BEST-RQ (TODO)\",\"Flow Matching (TODO)\",\"Models can be trained with multiple objectives by adding multiple entries under loss_conf in the training configuration.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, **kwargs) → Dict[str, Tensor]\",\"encode(speech: Tensor, speech_lengths: Tensor, text: Tensor | None = None, text_lengths: Tensor | None = None, use_final_output: bool = True) → Dict\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor | None = None, text_lengths: Tensor | None = None, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2217\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"inference_encode(speech: Tensor, speech_lengths: Tensor, use_mask: bool = False, use_final_output: bool = True) → Tuple[List[Tensor], Tensor]\"]},\"2218\":{\"h\":\"espnet2.ssl.loss.hubert.HuBERTDecoder\",\"t\":[\"source\",\"class espnet2.ssl.loss.hubert.HuBERTDecoder(encoder_embed_dim: int, num_classes: int, final_dim: int)\",\"Bases: Module\",\"Generate the logits of masked and unmasked inputs.\",\"Parameters:\",\"encoder_embed_dim (int) – The dimension of the transformer embedding output.\",\"num_classes (int) – The number of classes in the labels.\",\"final_dim (int) – Project final representations and targets to final_dim.\",\"forward(x: Tensor, mask_m: Tensor, mask_u: Tensor) → Tuple[Tensor, Tensor]\",\"HuBERTDecoder forward.\",\"Parameters:\",\"x (Tensor) – The feature representation of the last transformer layer.\",\"mask_m (Tensor) – The masked indices of dimension [batch, frame].\",\"mask_u (Tensor) – The unmasked indices of dimension [batch, frame].\",\"Returns: The logits of masked frames. [masked_frame, final_dim]. Tensor: The logits of unmasked frames. [unmasked_frame, final_dim].\",\"Return type: Tensor\"]},\"2219\":{\"h\":\"espnet2.ssl.loss.hubert.HuBERTLoss\",\"t\":[\"source\",\"class espnet2.ssl.loss.hubert.HuBERTLoss(encoder_output_size: int, num_classes: int, final_dim: int, loss_type: str = 'cross_entropy', layers: List = [-1], loss_weights: List = [1.0])\",\"Bases: AbsSSLLoss\",\"HuBERT MLM Loss\",\"Parameters:\",\"encoder_output_size (int) – input dimension\",\"num_classes (int) – vocab size\",\"final_dim (int) – final projection dim\",\"loss_type (str) – TODO, unused for now\",\"layers (List) – encoder output layers for loss\",\"loss_weights (List) – weight of each layer for loss\",\"forward(encoder_output: List, encoder_output_lengths: Tensor | None = None, text: Tensor | None = None, text_lengths: Tensor | None = None, mask_info: Dict | None = None) → Tuple[Tensor, Dict]\",\"HuBERT forward\",\"Parameters:\",\"encoder_output (List) – List of encoded sequences (B, T, D) from each layer.\",\"encoder_output_lengths (Tensor) – Lengths of batched encoder sequences (B,).\",\"text (Tensor) – text targets (B, T)\",\"text_lengths (Tensor) – Lengths of text targets (B,).\",\"mask_info (Dict) – Contains masked/unmasked indices\"]},\"2220\":{\"h\":\"espnet2.ssl.utils.mask.Masking\",\"t\":[\"source\",\"class espnet2.ssl.utils.mask.Masking(encoder_embed_dim: int, mask_prob: float = 0.8, mask_selection: str = 'static', mask_other: float = 0.0, mask_length: int = 10, no_mask_overlap: bool = False, mask_min_space: int = 0, mask_channel_prob: float = 0.0, mask_channel_selection: str = 'static', mask_channel_other: float = 0.0, mask_channel_length: int = 10, no_mask_channel_overlap: bool = False, mask_channel_min_space: int = 0)\",\"Bases: Module\",\"Generate the masks for masked prediction.\",\"Parameters:\",\"encoder_embed_dim (int) – The dimension of the transformer embedding output.\",\"mask_prob (float) – Prob for each token to be the start of a masked span. Will be multiplied by num of timesteps divided by len of mask span to mask approx this % of all elements. However due to overlaps, the actual number will be smaller (unless no_overlap is True).\",\"mask_selection (str) – How to choose the mask length. Options: [static, uniform, normal, poisson].\",\"mask_other (float) – Secondary mask argument (used for more complex distributions).\",\"mask_length (int) – The lengths of the mask.\",\"no_mask_overlap (bool) – Whether to allow masks to overlap.\",\"mask_min_space (int) – Minimum space between spans (if no overlap).\",\"mask_channel_prob (float) – The probability of replacing a feature with 0.\",\"mask_channel_selection (str) – How to choose mask length for channel mask. Options: [static, uniform, normal, poisson].\",\"mask_channel_other (float) – Secondary mask argument for channel masking (used for more complex distributions).\",\"mask_channel_length (int) – Minimum space between spans (if no overlap is enabled) for channel masking.\",\"no_mask_channel_overlap (bool) – Whether to allow channel masks to overlap.\",\"mask_channel_min_space (int) – Minimum space between spans for channel masking (if no overlap is enabled).\",\"forward(x: Tensor, padding_mask: Tensor | None) → Tensor\",\"Masking forward.\",\"Parameters:\",\"x (Tensor) – The encoded representations after feature extraction module.\",\"padding_mask (TensororNone) – The padding mask which will prevent masking padded elements.\",\"Returns: The feature representations after masking. Tensor: The generated mask indices.\",\"Return type: Tensor\"]},\"2221\":{\"h\":\"espnet2.st.espnet_model.ESPnetSTModel\",\"t\":[\"source\",\"class espnet2.st.espnet_model.ESPnetSTModel(vocab_size: int, token_list: Tuple[str, ...] | List[str], frontend: AbsFrontend | None, specaug: AbsSpecAug | None, normalize: AbsNormalize | None, preencoder: AbsPreEncoder | None, encoder: AbsEncoder, hier_encoder: AbsEncoder | None, md_encoder: AbsEncoder | None, extra_mt_encoder: AbsEncoder | None, postencoder: AbsPostEncoder | None, decoder: AbsDecoder, extra_asr_decoder: AbsDecoder | None, extra_mt_decoder: AbsDecoder | None, ctc: CTC | None, st_ctc: CTC | None, st_joint_network: Module | None, src_vocab_size: int | None, src_token_list: Tuple[str, ...] | List[str] | None, asr_weight: float = 0.0, mt_weight: float = 0.0, mtlalpha: float = 0.0, st_mtlalpha: float = 0.0, ignore_id: int = -1, tgt_ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, report_bleu: bool = True, sym_space: str = '<space>', sym_blank: str = '<blank>', tgt_sym_space: str = '<space>', tgt_sym_blank: str = '<blank>', extract_feats_in_collect_stats: bool = True, ctc_sample_rate: float = 0.0, tgt_sym_sos: str = '<sos/eos>', tgt_sym_eos: str = '<sos/eos>', lang_token_id: int = -1)\",\"Bases: AbsESPnetModel\",\"CTC-attention hybrid Encoder-Decoder model\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, src_text: Tensor | None = None, src_text_lengths: Tensor | None = None, **kwargs) → Dict[str, Tensor]\",\"encode(speech: Tensor, speech_lengths: Tensor, return_int_enc: bool = False) → Tuple[Tensor, Tensor]\",\"Frontend + Encoder. Note that this method is used by st_inference.py\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch, )\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor, text_lengths: Tensor, src_text: Tensor | None = None, src_text_lengths: Tensor | None = None, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Encoder + Decoder + Calc loss\",\"Parameters:\",\"speech – (Batch, Length, …)\",\"speech_lengths – (Batch,)\",\"text – (Batch, Length)\",\"text_lengths – (Batch,)\",\"src_text – (Batch, length)\",\"src_text_lengths – (Batch,)\",\"kwargs – “utt_id” is among the input.\"]},\"2222\":{\"h\":\"espnet2.svs.abs_svs.AbsSVS\",\"t\":[\"source\",\"class espnet2.svs.abs_svs.AbsSVS(*args, **kwargs)\",\"Bases: Module, ABC\",\"SVS abstract class.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate outputs and return the loss tensor.\",\"abstract inference(text: Tensor, **kwargs) → Dict[str, Tensor]\",\"Return predicted output as a dict.\",\"property require_raw_singing\",\"Return whether or not raw_singing is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\"]},\"2223\":{\"h\":\"espnet2.svs.singing_tacotron.decoder.Decoder\",\"t\":[\"source\",\"class espnet2.svs.singing_tacotron.decoder.Decoder(idim, odim, att, dlayers=2, dunits=1024, prenet_layers=2, prenet_units=256, postnet_layers=5, postnet_chans=512, postnet_filts=5, output_activation_fn=None, cumulate_att_w=True, use_batch_norm=True, use_concate=True, dropout_rate=0.5, zoneout_rate=0.1, reduction_factor=1)\",\"Bases: Module\",\"Decoder module of Spectrogram prediction network.\",\"This is a module of decoder of Spectrogram prediction network in Singing Tacotron, which described in\",\"`https://arxiv.org/pdf/2202.07907v1.pdf`_\",\". The decoder generates the sequence of features from the sequence of the hidden states.\",\"Filter for End-to-end Singing Voice Synthesis`: : https://arxiv.org/pdf/2202.07907v1.pdf\",\"Initialize Singing Tacotron decoder module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"att (torch.nn.Module) – Instance of attention class.\",\"dlayers (int,optional) – The number of decoder lstm layers.\",\"dunits (int,optional) – The number of decoder lstm units.\",\"prenet_layers (int,optional) – The number of prenet layers.\",\"prenet_units (int,optional) – The number of prenet units.\",\"postnet_layers (int,optional) – The number of postnet layers.\",\"postnet_filts (int,optional) – The number of postnet filter size.\",\"postnet_chans (int,optional) – The number of postnet filter channels.\",\"output_activation_fn (torch.nn.Module,optional) – Activation function for outputs.\",\"cumulate_att_w (bool,optional) – Whether to cumulate previous attention weight.\",\"use_batch_norm (bool,optional) – Whether to use batch normalization.\",\"use_concate (bool,optional) – Whether to concatenate encoder embedding with decoder lstm outputs.\",\"dropout_rate (float,optional) – Dropout rate.\",\"zoneout_rate (float,optional) – Zoneout rate.\",\"reduction_factor (int,optional) – Reduction factor.\",\"forward(hs, hlens, trans_token, ys)\",\"Calculate forward propagation.\",\"Parameters:\",\"hs (Tensor) – Batch of the sequences of padded hidden states (B, Tmax, idim).\",\"hlens (LongTensor) – Batch of lengths of each input batch (B,).\",\"trans_token (Tensor) – Global transition token for duration (B x Tmax x 1)\",\"ys (Tensor) – Batch of the sequences of padded target features (B, Lmax, odim).\",\"Returns: Batch of output tensors after postnet (B, Lmax, odim). Tensor: Batch of output tensors before postnet (B, Lmax, odim). Tensor: Batch of logits of stop prediction (B, Lmax). Tensor: Batch of attention weights (B, Lmax, Tmax).\",\"Return type: Tensor\"]},\"2224\":{\"h\":\"NOTE\",\"t\":[\"This computation is performed in teacher-forcing manner.\",\"inference(h, trans_token, threshold=0.5, minlenratio=0.0, maxlenratio=30.0, use_att_constraint=False, use_dynamic_filter=True, backward_window=1, forward_window=3)\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"h (Tensor) – Input sequence of encoder hidden states (T, C).\",\"trans_token (Tensor) – Global transition token for duration.\",\"threshold (float,optional) – Threshold to stop generation.\",\"minlenratio (float,optional) – Minimum length ratio. If set to 1.0 and the length of input is 10, the minimum length of outputs will be 10 * 1 = 10.\",\"minlenratio – Minimum length ratio. If set to 10 and the length of input is 10, the maximum length of outputs will be 10 * 10 = 100.\",\"use_att_constraint (bool) – Whether to apply attention constraint introduced in Deep Voice 3.\",\"use_dynamic_filter (bool) – Whether to apply dynamic filter introduced in\",\"`Singing Tacotron`_\",\".\",\"backward_window (int) – Backward window size in attention constraint.\",\"forward_window (int) – Forward window size in attention constraint.\",\"Returns: Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T).\",\"Return type: Tensor\"]},\"2225\":{\"h\":\"NOTE\",\"t\":[\"This computation is performed in auto-regressive manner.\"]},\"2226\":{\"h\":\"espnet2.svs.discrete.loss.DiscreteLoss\",\"t\":[\"source\",\"class espnet2.svs.discrete.loss.DiscreteLoss(use_masking=True, use_weighted_masking=False, predict_pitch=False)\",\"Bases: Module\",\"Loss function module for feed-forward Transformer.\",\"Initialize feed-forward Transformer loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"predict_pitch (bool) – Whether to predict pitch and calculate pitch loss.\",\"forward(after_outs: Tensor, before_outs: Tensor, d_outs: Tensor, ys: Tensor, ds: Tensor, ilens: Tensor, olens: Tensor, p_outs: Tensor | None = None, ps: Tensor | None = None, plens: Tensor | None = None)\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, Lmax, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, Lmax, odim).\",\"d_outs (Tensor) – Batch of outputs of duration predictor (B, Tmax).\",\"ys (Tensor) – Batch of target features (B, Lmax, odim).\",\"ds (Tensor) – Batch of durations (B, Tmax).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"p_outs (Tensor) – Batch of outputs of log_f0 (B, T_text, 1).\",\"ps (Tensor) – Batch of target log_f0 (B, T_text, 1).\",\"Returns: L1 loss value. Tensor: Duration predictor loss value. Tensor: Pitch loss value.\",\"Return type: Tensor\"]},\"2227\":{\"h\":\"espnet2.svs.singing_tacotron.encoder.Duration_Encoder\",\"t\":[\"source\",\"class espnet2.svs.singing_tacotron.encoder.Duration_Encoder(idim, embed_dim=512, dropout_rate=0.5, padding_idx=0)\",\"Bases: Module\",\"Duration_Encoder module of Spectrogram prediction network.\",\"This is a module of encoder of Spectrogram prediction network in Singing-Tacotron, This is the encoder which converts the sequence of durations and tempo features into a transition token.\",\"END-TO-END SINGING VOICE SYNTHESIS`: : https://arxiv.org/abs/2202.07907\",\"Initialize Singing-Tacotron encoder module.\",\"Parameters:\",\"idim (int)\",\"embed_dim (int,optional)\",\"dropout_rate (float,optional)\",\"forward(xs)\",\"Calculate forward propagation.\",\"Parameters:xs (Tensor) – Batch of the duration sequence.(B, Tmax, feature_len)\",\"Returns: Batch of the sequences of transition token (B, Tmax, 1). LongTensor: Batch of lengths of each sequence (B,)\",\"Return type: Tensor\",\"inference(x)\",\"Inference.\"]},\"2228\":{\"h\":\"espnet2.svs.discrete_svs_espnet_model.ESPnetDiscreteSVSModel\",\"t\":[\"source\",\"class espnet2.svs.discrete_svs_espnet_model.ESPnetDiscreteSVSModel(text_extract: AbsFeatsExtract | None, feats_extract: AbsFeatsExtract | None, score_feats_extract: AbsFeatsExtract | None, label_extract: AbsFeatsExtract | None, pitch_extract: AbsFeatsExtract | None, ying_extract: AbsFeatsExtract | None, duration_extract: AbsFeatsExtract | None, energy_extract: AbsFeatsExtract | None, normalize: InversibleInterface | None, pitch_normalize: InversibleInterface | None, energy_normalize: InversibleInterface | None, svs: AbsSVS, discrete_token_layers: int = 1)\",\"Bases: ESPnetSVSModel\",\"ESPnet model for singing voice synthesis task.\",\"Initialize ESPnetSVSModel module.\",\"collect_feats(text: Tensor, text_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, label: Tensor | None = None, label_lengths: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration_phn: Tensor | None = None, duration_phn_lengths: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_ruled_phn_lengths: Tensor | None = None, duration_syb: Tensor | None = None, duration_syb_lengths: Tensor | None = None, slur: Tensor | None = None, slur_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, ying: Tensor | None = None, ying_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, discrete_token: Tensor | None = None, discrete_token_lengths: Tensor | None = None, **kwargs) → Dict[str, Tensor]\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"label (Option *[*Tensor]) – Label tensor (B, T_label).\",\"label_lengths (Optional *[*Tensor]) – Label lrngth tensor (B,).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (B, T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (B, T_label).\",\"midi_lengths (Optional *[*Tensor]) – Midi lrngth tensor (B,).\",\"---- ( ---- duration* is duration in time_shift)\",\"duration_phn (Optional *[*Tensor]) – duration tensor (B, T_label).\",\"duration_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (B, T_phone).\",\"duration_ruled_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (B, T_syb).\",\"duration_syb_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"slur (Optional *[*Tensor]) – slur tensor (B, T_slur).\",\"slur_lengths (Optional *[*Tensor]) – slur length tensor (B,).\",\"pitch (Optional *[*Tensor]) – Pitch tensor (B, T_frame). - f0 sequence\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"discrete_token (Optional *[*Tensor]) – Discrete tokens tensor (B, T_frame)\",\"discrete_token_lengths (Optional *[*Tensor]) – Discrete tokens lengths tensor (B,)\",\"Returns: Dict of features.\",\"Return type: Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, feats: Tensor | None = None, feats_lengths: Tensor | None = None, label: Tensor | None = None, label_lengths: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration_phn: Tensor | None = None, duration_phn_lengths: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_ruled_phn_lengths: Tensor | None = None, duration_syb: Tensor | None = None, duration_syb_lengths: Tensor | None = None, slur: Tensor | None = None, slur_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, ying: Tensor | None = None, ying_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, discrete_token: Tensor | None = None, discrete_token_lengths: Tensor | None = None, flag_IsValid=False, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Caclualte outputs and return the loss tensor.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"label (Option *[*Tensor]) – Label tensor (B, T_label).\",\"label_lengths (Optional *[*Tensor]) – Label lrngth tensor (B,).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (B, T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (B, T_label).\",\"midi_lengths (Optional *[*Tensor]) – Midi lrngth tensor (B,).\",\"duration_phn (Optional *[*Tensor]) – duration tensor (B, T_label).\",\"duration_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (B, T_phone).\",\"duration_ruled_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (B, T_syllable).\",\"duration_syb_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"slur (Optional *[*Tensor]) – slur tensor (B, T_slur).\",\"slur_lengths (Optional *[*Tensor]) – slur length tensor (B,).\",\"pitch (Optional *[*Tensor]) – Pitch tensor (B, T_frame). - f0 sequence\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor]) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor]) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"discrete_token (Optional *[*Tensor]) – Discrete token tensor (B, T_frame).\",\"discrete_token_lengths (Optional *[*Tensor]) – Discrete token length tensor (B,).\",\"kwargs – “utt_id” is among the input.\",\"Returns: Loss scalar tensor. Dict[str, float]: Statistics to be monitored. Tensor: Weight tensor to summarize losses.\",\"Return type: Tensor\",\"inference(text: Tensor, singing: Tensor | None = None, label: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, duration_phn: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_syb: Tensor | None = None, slur: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, discrete_token: Tensor | None = None, **decode_config) → Dict[str, Tensor]\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (T_text).\",\"singing (Tensor) – Singing waveform tensor (T_wav).\",\"label (Option *[*Tensor]) – Label tensor (T_label).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (T_l abel).\",\"duration_phn (Optional *[*Tensor]) – duration tensor (T_label).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (T_phone).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (T_phone).\",\"slur (Optional *[*Tensor]) – slur tensor (T_phone).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (D,).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (1,).\",\"lids (Optional *[*Tensor]) – Language ID tensor (1,).\",\"pitch (Optional *[*Tensor) – Pitch tensor (T_frame).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"discrete_token (Optional *[*Tensor]) – Discrete tokens (T_frame)\",\"Returns: Dict of outputs.\",\"Return type: Dict[str, Tensor]\"]},\"2229\":{\"h\":\"espnet2.svs.espnet_model.ESPnetSVSModel\",\"t\":[\"source\",\"class espnet2.svs.espnet_model.ESPnetSVSModel(text_extract: AbsFeatsExtract | None, feats_extract: AbsFeatsExtract | None, score_feats_extract: AbsFeatsExtract | None, label_extract: AbsFeatsExtract | None, pitch_extract: AbsFeatsExtract | None, ying_extract: AbsFeatsExtract | None, duration_extract: AbsFeatsExtract | None, energy_extract: AbsFeatsExtract | None, normalize: InversibleInterface | None, pitch_normalize: InversibleInterface | None, energy_normalize: InversibleInterface | None, svs: AbsSVS)\",\"Bases: AbsESPnetModel\",\"ESPnet model for singing voice synthesis task.\",\"Initialize ESPnetSVSModel module.\",\"collect_feats(text: Tensor, text_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, label: Tensor | None = None, label_lengths: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration_phn: Tensor | None = None, duration_phn_lengths: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_ruled_phn_lengths: Tensor | None = None, duration_syb: Tensor | None = None, duration_syb_lengths: Tensor | None = None, slur: Tensor | None = None, slur_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, ying: Tensor | None = None, ying_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs) → Dict[str, Tensor]\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"label (Option *[*Tensor]) – Label tensor (B, T_label).\",\"label_lengths (Optional *[*Tensor]) – Label lrngth tensor (B,).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (B, T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (B, T_label).\",\"midi_lengths (Optional *[*Tensor]) – Midi lrngth tensor (B,).\",\"---- ( ---- duration* is duration in time_shift)\",\"duration_phn (Optional *[*Tensor]) – duration tensor (B, T_label).\",\"duration_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (B, T_phone).\",\"duration_ruled_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (B, T_syb).\",\"duration_syb_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"slur (Optional *[*Tensor]) – slur tensor (B, T_slur).\",\"slur_lengths (Optional *[*Tensor]) – slur length tensor (B,).\",\"pitch (Optional *[*Tensor]) – Pitch tensor (B, T_wav). - f0 sequence\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"Returns: Dict of features.\",\"Return type: Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, singing: Tensor, singing_lengths: Tensor, feats: Tensor | None = None, feats_lengths: Tensor | None = None, label: Tensor | None = None, label_lengths: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration_phn: Tensor | None = None, duration_phn_lengths: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_ruled_phn_lengths: Tensor | None = None, duration_syb: Tensor | None = None, duration_syb_lengths: Tensor | None = None, slur: Tensor | None = None, slur_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, ying: Tensor | None = None, ying_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, flag_IsValid=False, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Caclualte outputs and return the loss tensor.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"singing (Tensor) – Singing waveform tensor (B, T_wav).\",\"singing_lengths (Tensor) – Singing length tensor (B,).\",\"label (Option *[*Tensor]) – Label tensor (B, T_label).\",\"label_lengths (Optional *[*Tensor]) – Label lrngth tensor (B,).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (B, T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (B, T_label).\",\"midi_lengths (Optional *[*Tensor]) – Midi lrngth tensor (B,).\",\"duration_phn (Optional *[*Tensor]) – duration tensor (B, T_label).\",\"duration_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (B, T_phone).\",\"duration_ruled_phn_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (B, T_syllable).\",\"duration_syb_lengths (Optional *[*Tensor]) – duration length tensor (B,).\",\"slur (Optional *[*Tensor]) – slur tensor (B, T_slur).\",\"slur_lengths (Optional *[*Tensor]) – slur length tensor (B,).\",\"pitch (Optional *[*Tensor]) – Pitch tensor (B, T_wav). - f0 sequence\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor]) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor]) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"kwargs – “utt_id” is among the input.\",\"Returns: Loss scalar tensor. Dict[str, float]: Statistics to be monitored. Tensor: Weight tensor to summarize losses.\",\"Return type: Tensor\",\"inference(text: Tensor, singing: Tensor | None = None, label: Tensor | None = None, phn_cnt: Tensor | None = None, midi: Tensor | None = None, duration_phn: Tensor | None = None, duration_ruled_phn: Tensor | None = None, duration_syb: Tensor | None = None, slur: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **decode_config) → Dict[str, Tensor]\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (T_text).\",\"singing (Tensor) – Singing waveform tensor (T_wav).\",\"label (Option *[*Tensor]) – Label tensor (T_label).\",\"phn_cnt (Optional *[*Tensor]) – Number of phones in each syllable (T_syb)\",\"midi (Option *[*Tensor]) – Midi tensor (T_l abel).\",\"duration_phn (Optional *[*Tensor]) – duration tensor (T_label).\",\"duration_ruled_phn (Optional *[*Tensor]) – duration tensor (T_phone).\",\"duration_syb (Optional *[*Tensor]) – duration tensor (T_phone).\",\"slur (Optional *[*Tensor]) – slur tensor (T_phone).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (D,).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (1,).\",\"lids (Optional *[*Tensor]) – Language ID tensor (1,).\",\"pitch (Optional *[*Tensor) – Pitch tensor (T_wav).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"Returns: Dict of outputs.\",\"Return type: Dict[str, Tensor]\"]},\"2230\":{\"h\":\"espnet2.svs.discrete.frontend.EnCodecFrontend\",\"t\":[\"source\"]},\"2231\":{\"h\":\"espnet2.svs.singing_tacotron.encoder.Encoder\",\"t\":[\"source\",\"class espnet2.svs.singing_tacotron.encoder.Encoder(idim, input_layer='embed', embed_dim=512, elayers=1, eunits=512, econv_layers=3, econv_chans=512, econv_filts=5, use_batch_norm=True, use_residual=False, dropout_rate=0.5, padding_idx=0)\",\"Bases: Module\",\"Encoder module of Spectrogram prediction network.\",\"This is a module of encoder of Spectrogram prediction network in Singing Tacotron, which described in\",\"`Singing-Tacotron: Global Duration Control Attention and Dynamic Filter for End-to-end Singing Voice Synthesis`_\",\". This is the encoder which converts either a sequence of characters or acoustic features into the sequence of hidden states.\",\"Filter for End-to-end Singing Voice Synthesis`: : https://arxiv.org/abs/2202.07907\",\"Initialize Singing Tacotron encoder module.\",\"Parameters:\",\"idim (int)\",\"input_layer (str) – Input layer type.\",\"embed_dim (int,optional)\",\"elayers (int,optional)\",\"eunits (int,optional)\",\"econv_layers (int,optional)\",\"econv_filts (int,optional)\",\"econv_chans (int,optional)\",\"use_batch_norm (bool,optional)\",\"use_residual (bool,optional)\",\"dropout_rate (float,optional)\",\"forward(xs, ilens=None)\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of the padded sequence. Either character ids (B, Tmax) or acoustic feature (B, Tmax, idim * encoder_reduction_factor). Padded value should be 0.\",\"ilens (LongTensor) – Batch of lengths of each input batch (B,).\",\"Returns: Batch of the sequences of encoder states(B, Tmax, eunits). LongTensor: Batch of lengths of each sequence (B,)\",\"Return type: Tensor\",\"inference(x, ilens)\",\"Inference.\",\"Parameters:x (Tensor) – The sequeunce of character ids (T,) or acoustic feature (T, idim * encoder_reduction_factor).\",\"Returns: The sequences of encoder states(T, eunits).\",\"Return type: Tensor\"]},\"2232\":{\"h\":\"espnet2.svs.feats_extract.score_feats_extract.FrameScoreFeats\",\"t\":[\"source\",\"class espnet2.svs.feats_extract.score_feats_extract.FrameScoreFeats(fs: int | str = 22050, n_fft: int = 1024, win_length: int = 512, hop_length: int = 128, window: str = 'hann', center: bool = True)\",\"Bases: AbsFeatsExtract\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(label: Tensor | None = None, label_lengths: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration: Tensor | None = None, duration_lengths: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]\",\"FrameScoreFeats forward function.\",\"Parameters:\",\"label – (Batch, Nsamples)\",\"label_lengths – (Batch)\",\"midi – (Batch, Nsamples)\",\"midi_lengths – (Batch)\",\"duration – (Batch, Nsamples)\",\"duration_lengths – (Batch)\",\"Returns: (Batch, Frames)\",\"Return type: output\",\"get_parameters() → Dict[str, Any]\",\"label_aggregate(input: Tensor, input_lengths: Tensor | None = None) → Tuple[Tensor, Tensor | None]\",\"lage_aggregate function.\",\"Parameters:\",\"input – (Batch, Nsamples, Label_dim)\",\"input_lengths – (Batch)\",\"Returns: (Batch, Frames, Label_dim)\",\"Return type: output\",\"output_size() → int\"]},\"2233\":{\"h\":\"espnet2.svs.feats_extract.score_feats_extract.ListsToTensor\",\"t\":[\"source\",\"espnet2.svs.feats_extract.score_feats_extract.ListsToTensor(xs)\"]},\"2234\":{\"h\":\"espnet2.svs.discrete.frontend.MERTFrontend\",\"t\":[\"source\"]},\"2235\":{\"h\":\"espnet2.svs.naive_rnn.naive_rnn.NaiveRNN\",\"t\":[\"source\",\"class espnet2.svs.naive_rnn.naive_rnn.NaiveRNN(idim: int, odim: int, midi_dim: int = 129, embed_dim: int = 512, eprenet_conv_layers: int = 3, eprenet_conv_chans: int = 256, eprenet_conv_filts: int = 5, elayers: int = 3, eunits: int = 1024, ebidirectional: bool = True, midi_embed_integration_type: str = 'add', dlayers: int = 3, dunits: int = 1024, dbidirectional: bool = True, postnet_layers: int = 5, postnet_chans: int = 256, postnet_filts: int = 5, use_batch_norm: bool = True, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', eprenet_dropout_rate: float = 0.5, edropout_rate: float = 0.1, ddropout_rate: float = 0.1, postnet_dropout_rate: float = 0.5, init_type: str = 'xavier_uniform', use_masking: bool = False, use_weighted_masking: bool = False, loss_type: str = 'L1')\",\"Bases: AbsSVS\",\"NaiveRNN-SVS module.\",\"This is an implementation of naive RNN for singing voice synthesis The features are processed directly over time-domain from music score and predict the singing voice features\",\"Initialize NaiveRNN module.\",\"Parameters:\",\"idim (int) – Dimension of the label inputs.\",\"odim (int) – Dimension of the outputs.\",\"midi_dim (int) – Dimension of the midi inputs.\",\"embed_dim (int) – Dimension of the token embedding.\",\"eprenet_conv_layers (int) – Number of prenet conv layers.\",\"eprenet_conv_filts (int) – Number of prenet conv filter size.\",\"eprenet_conv_chans (int) – Number of prenet conv filter channels.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"ebidirectional (bool) – If bidirectional in encoder.\",\"midi_embed_integration_type (str) – how to integrate midi information, (“add” or “cat”).\",\"dlayers (int) – Number of decoder lstm layers.\",\"dunits (int) – Number of decoder lstm units.\",\"dbidirectional (bool) – if bidirectional in decoder.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_filts (int) – Number of postnet filter size.\",\"postnet_chans (int) – Number of postnet filter channels.\",\"use_batch_norm (bool) – Whether to use batch normalization.\",\"reduction_factor (int) – Reduction factor.\",\"related ( # extra embedding)\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"eprenet_dropout_rate (float) – Prenet dropout rate.\",\"edropout_rate (float) – Encoder dropout rate.\",\"ddropout_rate (float) – Decoder dropout rate.\",\"postnet_dropout_rate (float) – Postnet dropout_rate.\",\"init_type (str) – How to initialize transformer parameters.\",\"use_masking (bool) – Whether to mask padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"loss_type (str) – Loss function type (“L1”, “L2”, or “L1+L2”).\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, melody_lengths: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, duration: Dict[str, Tensor] | None = None, duration_lengths: Dict[str, Tensor] | None = None, slur: LongTensor | None = None, slur_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, flag_IsValid=False) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, Tmax).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, Tmax).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"melody_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded melody (B, ).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"pitch_lengths (LongTensor) – Batch of the lengths of padded f0 (B, ).\",\"duration (Optional *[*Dict]) – key is “lab”, “score”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"duration_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded duration (B, ).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"slur_lengths (LongTensor) – Batch of the lengths of padded slur (B, ).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"GS Fix: : arguements from forward func. V.S. <br/>\",\"**\",\"<br/> batch from espnet_model.py label == durations ｜ phone sequence melody -> pitch sequence\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, use_teacher_forcing: Tensor = False) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (Tmax).\",\"feats (Tensor) – Batch of padded target features (Lmax, odim).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (Tmax).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (Tmax).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"duration (Optional *[*Dict]) – key is “lab”, “score”; value (LongTensor): Batch of padded duration (Tmax).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (1).\",\"Returns: Output dict including the following items: * feat_gen (Tensor): Output sequence of features (T_feats, odim).\",\"Return type: Dict[str, Tensor]\"]},\"2236\":{\"h\":\"espnet2.svs.naive_rnn.naive_rnn_dp.NaiveRNNDP\",\"t\":[\"source\",\"class espnet2.svs.naive_rnn.naive_rnn_dp.NaiveRNNDP(idim: int, odim: int, midi_dim: int = 129, embed_dim: int = 512, duration_dim: int = 500, eprenet_conv_layers: int = 3, eprenet_conv_chans: int = 256, eprenet_conv_filts: int = 5, elayers: int = 3, eunits: int = 1024, ebidirectional: bool = True, midi_embed_integration_type: str = 'add', dlayers: int = 3, dunits: int = 1024, dbidirectional: bool = True, postnet_layers: int = 5, postnet_chans: int = 256, postnet_filts: int = 5, use_batch_norm: bool = True, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', eprenet_dropout_rate: float = 0.5, edropout_rate: float = 0.1, ddropout_rate: float = 0.1, postnet_dropout_rate: float = 0.5, init_type: str = 'xavier_uniform', use_masking: bool = False, use_weighted_masking: bool = False)\",\"Bases: AbsSVS\",\"NaiveRNNDP-SVS module.\",\"This is an implementation of naive RNN with duration prediction for singing voice synthesis The features are processed directly over time-domain from music score and predict the singing voice features\",\"Initialize NaiveRNNDP module.\",\"Parameters:\",\"idim (int) – Dimension of the label inputs.\",\"odim (int) – Dimension of the outputs.\",\"midi_dim (int) – Dimension of the midi inputs.\",\"embed_dim (int) – Dimension of the token embedding.\",\"eprenet_conv_layers (int) – Number of prenet conv layers.\",\"eprenet_conv_filts (int) – Number of prenet conv filter size.\",\"eprenet_conv_chans (int) – Number of prenet conv filter channels.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"ebidirectional (bool) – If bidirectional in encoder.\",\"midi_embed_integration_type (str) – how to integrate midi information, (“add” or “cat”).\",\"dlayers (int) – Number of decoder lstm layers.\",\"dunits (int) – Number of decoder lstm units.\",\"dbidirectional (bool) – if bidirectional in decoder.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_filts (int) – Number of postnet filter size.\",\"postnet_chans (int) – Number of postnet filter channels.\",\"use_batch_norm (bool) – Whether to use batch normalization.\",\"reduction_factor (int) – Reduction factor.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"related ( # extra embedding)\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"eprenet_dropout_rate (float) – Prenet dropout rate.\",\"edropout_rate (float) – Encoder dropout rate.\",\"ddropout_rate (float) – Decoder dropout rate.\",\"postnet_dropout_rate (float) – Postnet dropout_rate.\",\"init_type (str) – How to initialize transformer parameters.\",\"use_masking (bool) – Whether to mask padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, melody_lengths: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, duration: Dict[str, Tensor] | None = None, duration_lengths: Dict[str, Tensor] | None = None, slur: LongTensor | None = None, slur_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False, flag_IsValid=False) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, Tmax).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, Tmax).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"melody_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded melody (B, ).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"pitch_lengths (LongTensor) – Batch of the lengths of padded f0 (B, ).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"duration_length (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of the lengths of padded duration (B, ).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"slur_lengths (LongTensor) – Batch of the lengths of padded slur (B, ).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"GS Fix: : arguements from forward func. V.S. <br/>\",\"**\",\"<br/> batch from espnet_model.py label == durations | phone sequence melody -> pitch sequence\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False, use_teacher_forcing: Tensor = False) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (Tmax).\",\"feats (Tensor) – Batch of padded target features (Lmax, odim).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (Tmax).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (Tmax).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (Tmax).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (1).\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim).\",\"Return type: Dict[str, Tensor]\"]},\"2237\":{\"h\":\"espnet2.svs.naive_rnn.naive_rnn.NaiveRNNLoss\",\"t\":[\"source\",\"class espnet2.svs.naive_rnn.naive_rnn.NaiveRNNLoss(use_masking=True, use_weighted_masking=False)\",\"Bases: Module\",\"Loss function module for Tacotron2.\",\"Initialize Tactoron2 loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"forward(after_outs, before_outs, ys, olens)\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, Lmax, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, Lmax, odim).\",\"ys (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: L1 loss value. Tensor: Mean square error loss value.\",\"Return type: Tensor\"]},\"2238\":{\"h\":\"espnet2.svs.feats_extract.score_feats_extract.SyllableScoreFeats\",\"t\":[\"source\",\"class espnet2.svs.feats_extract.score_feats_extract.SyllableScoreFeats(fs: int | str = 22050, n_fft: int = 1024, win_length: int = 512, hop_length: int = 128, window: str = 'hann', center: bool = True)\",\"Bases: AbsFeatsExtract\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"extra_repr()\",\"Set the extra representation of the module.\",\"To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\",\"forward(label: Tensor | None = None, label_lengths: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration: Tensor | None = None, duration_lengths: Tensor | None = None) → Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]\",\"SyllableScoreFeats forward function.\",\"Parameters:\",\"label – (Batch, Nsamples)\",\"label_lengths – (Batch)\",\"midi – (Batch, Nsamples)\",\"midi_lengths – (Batch)\",\"duration – (Batch, Nsamples)\",\"duration_lengths – (Batch)\",\"Returns: (Batch, Frames)\",\"Return type: output\",\"get_parameters() → Dict[str, Any]\",\"get_segments(label: Tensor | None = None, label_lengths: Tensor | None = None, midi: Tensor | None = None, midi_lengths: Tensor | None = None, duration: Tensor | None = None, duration_lengths: Tensor | None = None)\",\"output_size() → int\"]},\"2239\":{\"h\":\"espnet2.svs.discrete.toksing.TokSing\",\"t\":[\"source\",\"class espnet2.svs.discrete.toksing.TokSing(idim: int, odim: int, midi_dim: int = 129, duration_dim: int = 500, adim: int = 384, aheads: int = 4, elayers: int = 6, eunits: int = 1536, dlayers: int = 6, dunits: int = 1536, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, postnet_dropout_rate: float = 0.5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'transformer', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', global_channels: int = -1, text_encoder_attention_heads: int = 2, text_encoder_ffn_expand: int = 4, text_encoder_blocks: int = 6, text_encoder_positionwise_layer_type: str = 'conv1d', text_encoder_positionwise_conv_kernel_size: int = 1, text_encoder_positional_encoding_layer_type: str = 'rel_pos', text_encoder_self_attention_layer_type: str = 'rel_selfattn', text_encoder_activation_type: str = 'swish', text_encoder_normalize_before: bool = True, text_encoder_dropout_rate: float = 0.1, text_encoder_positional_dropout_rate: float = 0.0, text_encoder_attention_dropout_rate: float = 0.0, text_encoder_conformer_kernel_size: int = 7, use_macaron_style_in_text_encoder: bool = True, use_conformer_conv_in_text_encoder: bool = True, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, loss_function: str = 'XiaoiceSing2', loss_type: str = 'L1', lambda_out: float = 1, lambda_dur: float = 0.1, lambda_pitch: float = 0.01, lambda_vuv: float = 0.01, use_discrete_token: bool = False, discrete_token_layers: int = 1, predict_pitch: bool = False, codec_codebook: int = 0)\",\"Bases: AbsSVS\",\"TokSing: Singing Voice Synthesis based on Discrete Tokens\",\"paper link: https://arxiv.org/abs/2406.08416\",\"Initialize TokSing module.\",\"Parameters:\",\"idim (int) – Dimension of the label inputs.\",\"odim (int) – Dimension of the outputs.\",\"midi_dim (int) – Dimension of the midi inputs.\",\"duration_dim (int) – Dimension of the duration inputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Kernel size of postnet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"loss_function (str) – Loss functions (“FastSpeech1” or “XiaoiceSing2”)\",\"loss_type (str) – Mel loss type (“L1” (MAE), “L2” (MSE) or “L1+L2”)\",\"lambda_out (float) – Loss scaling coefficient for Mel or discrete token loss.\",\"lambda_dur (float) – Loss scaling coefficient for duration loss.\",\"lambda_pitch (float) – Loss scaling coefficient for pitch loss.\",\"lambda_vuv (float) – Loss scaling coefficient for VUV loss.\",\"use_discrete_token (bool) – Whether to use discrete tokens as targets.\",\"predict_pitch (bool) – Whether to predict pitch when use_discrete_token.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, melody_lengths: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, duration: Dict[str, Tensor] | None = None, duration_lengths: Dict[str, Tensor] | None = None, slur: LongTensor | None = None, slur_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False, discrete_token: Tensor | None = None, discrete_token_lengths: Tensor | None = None, discrete_token_lengths_frame: Tensor | None = None, flag_IsValid: bool = False, flag_RL: bool = False, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_frame, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, Tmax).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"melody_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded melody (B, ).\",\"pitch (FloatTensor) – Batch of padded f0 (B, T_frame).\",\"pitch_lengths (LongTensor) – Batch of the lengths of padded f0 (B, ).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"duration_length (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of the lengths of padded duration (B, ).\",\"slur (LongTensor) – Batch of padded slur (B, T_frame).\",\"slur_lengths (LongTensor) – Batch of the lengths of padded slur (B, ).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"discrete_token (LongTensor) – Batch of padded discrete tokens (B, T_frame).\",\"discrete_token_lengths (LongTensor) – Batch of the lengths of padded discrete tokens (B, ).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"flag_IsValid (bool) – Whether it is valid set.\",\"falg_RL (bool) – Whether to perform reinforcement learning. (RL will use model in infer mode.)\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, discrete_token: Tensor | None = None, use_teacher_forcing: Tensor = False, joint_training: bool = False) → Dict[str, Tensor]\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style (T_frame, idim).\",\"durations (Optional *[*LongTensor]) – Groundtruth of duration (T_text + 1,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (Tmax).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (T_frame).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (Tmax).\",\"slur (LongTensor) – Batch of padded slur (T_frame).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"discrete_token (Optional *[*Tensor]) – Batch of discrete tokens (T_frame).\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"duration (Tensor): Duration sequence (T_text + 1,).\",\"Return type: Dict[str, Tensor]\"]},\"2240\":{\"h\":\"espnet2.svs.xiaoice.XiaoiceSing.XiaoiceSing\",\"t\":[\"source\",\"class espnet2.svs.xiaoice.XiaoiceSing.XiaoiceSing(idim: int, odim: int, midi_dim: int = 129, duration_dim: int = 500, adim: int = 384, aheads: int = 4, elayers: int = 6, eunits: int = 1536, dlayers: int = 6, dunits: int = 1536, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, postnet_dropout_rate: float = 0.5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'transformer', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, loss_function: str = 'XiaoiceSing2', loss_type: str = 'L1', lambda_mel: float = 1, lambda_dur: float = 0.1, lambda_pitch: float = 0.01, lambda_vuv: float = 0.01)\",\"Bases: AbsSVS\",\"XiaoiceSing module for Singing Voice Synthesis.\",\"This is a module of XiaoiceSing. A high-quality singing voice synthesis system which employs an integrated network for spectrum, F0 and duration modeling. It follows the main architecture of FastSpeech while proposing some singing-specific design:\",\"Add features from musical score (e.g.note pitch and length)\",\"Add a residual connection in F0 prediction to attenuate off-key issues\",\"The duration of all the phonemes in a musical note is accumulated to calculate the syllable duration loss for rhythm enhancement (syllable loss)\",\"Initialize XiaoiceSing module.\",\"Parameters:\",\"idim (int) – Dimension of the label inputs.\",\"odim (int) – Dimension of the outputs.\",\"midi_dim (int) – Dimension of the midi inputs.\",\"duration_dim (int) – Dimension of the duration inputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Kernel size of postnet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"loss_function (str) – Loss functions (“FastSpeech1” or “XiaoiceSing2”)\",\"loss_type (str) – Mel loss type (“L1” (MAE), “L2” (MSE) or “L1+L2”)\",\"lambda_mel (float) – Loss scaling coefficient for Mel loss.\",\"lambda_dur (float) – Loss scaling coefficient for duration loss.\",\"lambda_pitch (float) – Loss scaling coefficient for pitch loss.\",\"lambda_vuv (float) – Loss scaling coefficient for VUV loss.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, melody_lengths: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, duration: Dict[str, Tensor] | None = None, duration_lengths: Dict[str, Tensor] | None = None, slur: LongTensor | None = None, slur_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False, flag_IsValid=False) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (B, Tmax).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"melody_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded melody (B, ).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"pitch_lengths (LongTensor) – Batch of the lengths of padded f0 (B, ).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"duration_length (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of the lengths of padded duration (B, ).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"slur_lengths (LongTensor) – Batch of the lengths of padded slur (B, ).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, use_teacher_forcing: Tensor = False, joint_training: bool = False) → Dict[str, Tensor]\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style (N, idim).\",\"durations (Optional *[*LongTensor]) – Groundtruth of duration (T_text + 1,).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (Tmax).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (Tmax).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"alpha (float) – Alpha to control the speed.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"duration (Tensor): Duration sequence (T_text + 1,).\",\"Return type: Dict[str, Tensor]\"]},\"2241\":{\"h\":\"espnet2.svs.xiaoice.loss.XiaoiceSing2Loss\",\"t\":[\"source\",\"class espnet2.svs.xiaoice.loss.XiaoiceSing2Loss(use_masking: bool = True, use_weighted_masking: bool = False)\",\"Bases: Module\",\"Loss function module for FastSpeech2.\",\"Initialize feed-forward Transformer loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"forward(after_outs: Tensor, before_outs: Tensor, d_outs: Tensor, p_outs: Tensor, v_outs: Tensor, ys: Tensor, ds: Tensor, ps: Tensor, vs: Tensor, ilens: Tensor, olens: Tensor, loss_type: str = 'L1') → Tuple[Tensor, Tensor, Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, T_feats, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, T_feats, odim).\",\"d_outs (LongTensor) – Batch of outputs of duration predictor (B, T_text).\",\"p_outs (Tensor) – Batch of outputs of log_f0 (B, T_text, 1).\",\"v_outs (Tensor) – Batch of outputs of VUV (B, T_text, 1).\",\"ys (Tensor) – Batch of target features (B, T_feats, odim).\",\"ds (LongTensor) – Batch of durations (B, T_text).\",\"ps (Tensor) – Batch of target log_f0 (B, T_text, 1).\",\"vs (Tensor) – Batch of target VUV (B, T_text, 1).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"loss_type (str) – Mel loss type (“L1” (MAE), “L2” (MSE) or “L1+L2”)\",\"Returns: Mel loss value. Tensor: Duration predictor loss value. Tensor: Pitch predictor loss value. Tensor: VUV predictor loss value.\",\"Return type: Tensor\"]},\"2242\":{\"h\":\"espnet2.svs.singing_tacotron.decoder.decoder_init\",\"t\":[\"source\",\"espnet2.svs.singing_tacotron.decoder.decoder_init(m)\",\"Initialize decoder parameters.\"]},\"2243\":{\"h\":\"espnet2.svs.singing_tacotron.encoder.encoder_init\",\"t\":[\"source\",\"espnet2.svs.singing_tacotron.encoder.encoder_init(m)\",\"Initialize encoder parameters.\"]},\"2244\":{\"h\":\"espnet2.svs.feats_extract.score_feats_extract.expand_to_frame\",\"t\":[\"source\",\"espnet2.svs.feats_extract.score_feats_extract.expand_to_frame(expand_len, len_size, label, midi, duration)\"]},\"2245\":{\"h\":\"espnet2.svs.singing_tacotron.singing_tacotron.singing_tacotron\",\"t\":[\"source\",\"class espnet2.svs.singing_tacotron.singing_tacotron.singing_tacotron(idim: int, odim: int, midi_dim: int = 129, duration_dim: int = 500, embed_dim: int = 512, elayers: int = 1, eunits: int = 512, econv_layers: int = 3, econv_chans: int = 512, econv_filts: int = 5, atype: str = 'GDCA', adim: int = 512, aconv_chans: int = 32, aconv_filts: int = 15, cumulate_att_w: bool = True, dlayers: int = 2, dunits: int = 1024, prenet_layers: int = 2, prenet_units: int = 256, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, output_activation: str | None = None, use_batch_norm: bool = True, use_concate: bool = True, use_residual: bool = False, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, dropout_rate: float = 0.5, zoneout_rate: float = 0.1, use_masking: bool = True, use_weighted_masking: bool = False, bce_pos_weight: float = 5.0, loss_type: str = 'L1', use_guided_attn_loss: bool = True, guided_attn_loss_sigma: float = 0.4, guided_attn_loss_lambda: float = 1.0)\",\"Bases: AbsSVS\",\"singing_Tacotron module for end-to-end singing-voice-synthesis.\",\"This is a module of Spectrogram prediction network in Singing Tacotron described in\",\"`Singing-Tacotron: Global Duration Control Attention and Dynamic Filter for End-to-end Singing Voice Synthesis`_\",\", which learn accurate alignment information automatically.\",\"Filter for End-to-end Singing Voice Synthesis`: : https://arxiv.org/pdf/2202.07907v1.pdf\",\"Initialize Singing Tacotron module.\",\"Parameters:\",\"idim (int) – Dimension of the label inputs.\",\"odim – (int) Dimension of the outputs.\",\"embed_dim (int) – Dimension of the token embedding.\",\"elayers (int) – Number of encoder blstm layers.\",\"eunits (int) – Number of encoder blstm units.\",\"econv_layers (int) – Number of encoder conv layers.\",\"econv_filts (int) – Number of encoder conv filter size.\",\"econv_chans (int) – Number of encoder conv filter channels.\",\"dlayers (int) – Number of decoder lstm layers.\",\"dunits (int) – Number of decoder lstm units.\",\"prenet_layers (int) – Number of prenet layers.\",\"prenet_units (int) – Number of prenet units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_filts (int) – Number of postnet filter size.\",\"postnet_chans (int) – Number of postnet filter channels.\",\"output_activation (str) – Name of activation function for outputs.\",\"adim (int) – Number of dimension of mlp in attention.\",\"aconv_chans (int) – Number of attention conv filter channels.\",\"aconv_filts (int) – Number of attention conv filter size.\",\"cumulate_att_w (bool) – Whether to cumulate previous attention weight.\",\"use_batch_norm (bool) – Whether to use batch normalization.\",\"use_concate (bool) – Whether to concat enc outputs w/ dec lstm outputs.\",\"reduction_factor (int) – Reduction factor.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – Number of GST embeddings.\",\"gst_heads (int) – Number of heads in GST multihead attention.\",\"gst_conv_layers (int) – Number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – Number of GRU layers in GST.\",\"gst_gru_units (int) – Number of GRU units in GST.\",\"dropout_rate (float) – Dropout rate.\",\"zoneout_rate (float) – Zoneout rate.\",\"use_masking (bool) – Whether to mask padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"bce_pos_weight (float) – Weight of positive sample of stop token (only for use_masking=True).\",\"loss_type (str) – Loss function type (“L1”, “L2”, or “L1+L2”).\",\"use_guided_attn_loss (bool) – Whether to use guided attention loss.\",\"guided_attn_loss_sigma (float) – Sigma in guided attention loss.\",\"guided_attn_loss_lambda (float) – Lambda in guided attention loss.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, label: Dict[str, Tensor] | None = None, label_lengths: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, melody_lengths: Dict[str, Tensor] | None = None, duration: Dict[str, Tensor] | None = None, duration_lengths: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, slur: LongTensor | None = None, slur_lengths: Tensor | None = None, ying: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False, flag_IsValid=False) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) –\",\"Batch of the lengths of each target (B,). : label (Optional[Dict]): key is “lab” or “score”;\",\"value (LongTensor): Batch of padded label ids (B, Tmax).\",\"label_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded label ids (B, ).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (B, Tmax).\",\"melody_lengths (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of the lengths of padded melody (B, ).\",\"pitch (FloatTensor) – Batch of padded f0 (B, Tmax).\",\"pitch_lengths (LongTensor) – Batch of the lengths of padded f0 (B, ).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (B, Tmax).\",\"duration_length (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of the lengths of padded duration (B, ).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"slur_lengths (LongTensor) – Batch of the lengths of padded slur (B, ).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, label: Dict[str, Tensor] | None = None, melody: Dict[str, Tensor] | None = None, duration: Dict[str, Tensor] | None = None, slur: Dict[str, Tensor] | None = None, pitch: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, threshold: float = 0.5, minlenratio: float = 0.0, maxlenratio: float = 30.0, use_att_constraint: bool = False, use_dynamic_filter: bool = False, backward_window: int = 1, forward_window: int = 3, use_teacher_forcing: bool = False) → Dict[str, Tensor]\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style (N, idim).\",\"label (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded label ids (Tmax).\",\"melody (Optional *[*Dict]) – key is “lab” or “score”; value (LongTensor): Batch of padded melody (Tmax).\",\"pitch (FloatTensor) – Batch of padded f0 (Tmax).\",\"duration (Optional *[*Dict]) – key is “lab”, “score_phn” or “score_syb”; value (LongTensor): Batch of padded duration (Tmax).\",\"slur (LongTensor) – Batch of padded slur (B, Tmax).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"threshold (float) – Threshold in inference.\",\"minlenratio (float) – Minimum length ratio in inference.\",\"maxlenratio (float) – Maximum length ratio in inference.\",\"use_att_constraint (bool) – Whether to apply attention constraint.\",\"use_dynamic_filter (bool) – Whether to apply dynamic filter.\",\"backward_window (int) – Backward window in attention constraint or dynamic filter.\",\"forward_window (int) – Forward window in attention constraint or dynamic filter.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"prob (Tensor): Output sequence of stop probabilities (T_feats,).\",\"att_w (Tensor): Attention weights (T_feats, T).\",\"Return type: Dict[str, Tensor]\"]},\"2246\":{\"h\":\"espnet2.tasks.asr.ASRTask\",\"t\":[\"source\",\"class espnet2.tasks.asr.ASRTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetASRModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2247\":{\"h\":\"espnet2.tasks.asr_transducer.ASRTransducerTask\",\"t\":[\"source\",\"class espnet2.tasks.asr_transducer.ASRTransducerTask\",\"Bases: AbsTask\",\"ASR Transducer Task definition.\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"Add Transducer task arguments.\",\"Parameters:\",\"cls – ASRTransducerTask object.\",\"parser – Transducer arguments parser.\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Build collate function.\",\"Parameters:\",\"cls – ASRTransducerTask object.\",\"args – Task arguments.\",\"train – Training mode.\",\"Returns: Callable collate function.\",\"classmethod build_model(args: Namespace) → ESPnetASRTransducerModel\",\"Required data depending on task mode.\",\"Parameters:\",\"cls – ASRTransducerTask object.\",\"args – Task arguments.\",\"Returns: ASR Transducer model.\",\"Return type: model\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"Build pre-processing function.\",\"Parameters:\",\"cls – ASRTransducerTask object.\",\"args – Task arguments.\",\"train – Training mode.\",\"Returns: Callable pre-processing function.\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Optional data depending on task mode.\",\"Parameters:\",\"cls – ASRTransducerTask object.\",\"train – Training mode.\",\"inference – Inference mode.\",\"Returns: Optional task data.\",\"Return type: retval\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Required data depending on task mode.\",\"Parameters:\",\"cls – ASRTransducerTask object.\",\"train – Training mode.\",\"inference – Inference mode.\",\"Returns: Required task data.\",\"Return type: retval\",\"trainer\",\"alias of Trainer\"]},\"2248\":{\"h\":\"espnet2.tasks.asvspoof.ASVSpoofTask\",\"t\":[\"source\",\"class espnet2.tasks.asvspoof.ASVSpoofTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetASVSpoofModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2249\":{\"h\":\"espnet2.tasks.abs_task.AbsTask\",\"t\":[\"source\",\"class espnet2.tasks.abs_task.AbsTask\",\"Bases: ABC\",\"abstract classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_category_chunk_iter_factory(args: Namespace, iter_options: IteratorOptions, mode: str) → AbsIterFactory\",\"classmethod build_category_iter_factory(args: Namespace, iter_options: IteratorOptions, mode: str) → AbsIterFactory\",\"classmethod build_chunk_iter_factory(args: Namespace, iter_options: IteratorOptions, mode: str) → AbsIterFactory\",\"abstract classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Sequence[Dict[str, ndarray]]], Dict[str, Tensor]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_dataset(args: Namespace, iter_options: IteratorOptions, keys_to_load: Set[str | int] | None = None) → AbsDataset\",\"classmethod build_iter_factory(args: Namespace, distributed_option: DistributedOption, mode: str, kwargs: dict | None = None) → AbsIterFactory\",\"Build a factory object of mini-batch iterator.\",\"This object is invoked at every epochs to build the iterator for each epoch as following:\",\">>> iter_factory = cls.build_iter_factory(...) >>> for epoch in range(1, max_epoch): ... for keys, batch in iter_fatory.build_iter(epoch): ... model(**batch)\",\"The mini-batches for each epochs are fully controlled by this class. Note that the random seed used for shuffling is decided as “seed + epoch” and the generated mini-batches can be reproduces when resuming.\",\"Note that the definition of “epoch” doesn’t always indicate to run out of the whole training corpus. “–num_iters_per_epoch” option restricts the number of iterations for each epoch and the rest of samples for the originally epoch are left for the next epoch. e.g. If The number of mini-batches equals to 4, the following two are same:\",\"1 epoch without “–num_iters_per_epoch”\",\"4 epoch with “–num_iters_per_epoch” == 1\",\"classmethod build_iter_options(args: Namespace, distributed_option: DistributedOption, mode: str)\",\"abstract classmethod build_model(args: Namespace) → AbsESPnetModel\",\"classmethod build_model_from_file(config_file: Path | str | None = None, model_file: Path | str | None = None, device: str = 'cpu') → Tuple[AbsESPnetModel, Namespace]\",\"Build model from the files.\",\"This method is used for inference or fine-tuning.\",\"Parameters:\",\"config_file – The yaml file saved when training.\",\"model_file – The model file saved when training.\",\"device – Device type, “cpu”, “mps”, “cuda”, or “cuda:N”.\",\"classmethod build_multiple_iter_factory(args: Namespace, distributed_option: DistributedOption, mode: str)\",\"classmethod build_optimizers(args: Namespace, model: Module) → List[Optimizer]\",\"abstract classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"classmethod build_sequence_iter_factory(args: Namespace, iter_options: IteratorOptions, mode: str) → AbsIterFactory\",\"classmethod build_streaming_iterator(data_path_and_name_and_type, preprocess_fn, collate_fn, key_file: str | None = None, batch_size: int = 1, dtype: ~typing.Any | None = <class 'numpy.float32'>, num_workers: int = 1, allow_variable_data_keys: bool = False, ngpu: int = 0, inference: bool = False, mode: str | None = None, multi_task_dataset: bool = False) → DataLoader\",\"Build DataLoader using iterable dataset\",\"classmethod build_task_iter_factory(args: Namespace, iter_options: IteratorOptions, mode: str) → AbsIterFactory\",\"Build task specific iterator factory\",\"Example\",\">>> class YourTask(AbsTask): ... @classmethod ... def add_task_arguments(cls, parser: argparse.ArgumentParser): ... parser.set_defaults(iterator_type=\\\"task\\\") ... ... @classmethod ... def build_task_iter_factory( ... cls, ... args: argparse.Namespace, ... iter_options: IteratorOptions, ... mode: str, ... ): ... return FooIterFactory(...) ... ... @classmethod ... def build_iter_options( .... args: argparse.Namespace, ... distributed_option: DistributedOption, ... mode: str ... ): ... # if you need to customize options object\",\"classmethod check_required_command_args(args: Namespace)\",\"classmethod check_task_requirements(dataset: AbsDataset | IterableESPnetDataset, allow_variable_data_keys: bool, train: bool, inference: bool = False) → None\",\"Check if the dataset satisfy the requirement of current Task\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= []\",\"classmethod exclude_opts() → Tuple[str, ...]\",\"The options not to be shown by –print_config\",\"classmethod get_default_config() → Dict[str, Any]\",\"Return the configuration as dict.\",\"This method is used by print_config()\",\"classmethod get_parser() → ArgumentParser\",\"classmethod main(args: Namespace | None = None, cmd: Sequence[str] | None = None)\",\"classmethod main_worker(args: Namespace)\",\"num_optimizers : int= 1\",\"abstract classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod print_config(file=<_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>) → None\",\"abstract classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2250\":{\"h\":\"espnet2.tasks.cls.CLSTask\",\"t\":[\"source\",\"class espnet2.tasks.cls.CLSTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetClassificationModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2251\":{\"h\":\"espnet2.tasks.diar.DiarizationTask\",\"t\":[\"source\",\"class espnet2.tasks.diar.DiarizationTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetDiarizationModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2252\":{\"h\":\"espnet2.tasks.enh_s2t.EnhS2TTask\",\"t\":[\"source\",\"class espnet2.tasks.enh_s2t.EnhS2TTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetEnhS2TModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2253\":{\"h\":\"espnet2.tasks.enh.EnhancementTask\",\"t\":[\"source\",\"class espnet2.tasks.enh.EnhancementTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_iter_factory(args: Namespace, distributed_option: DistributedOption, mode: str, kwargs: dict | None = None) → AbsIterFactory\",\"Build a factory object of mini-batch iterator.\",\"This object is invoked at every epochs to build the iterator for each epoch as following:\",\">>> iter_factory = cls.build_iter_factory(...) >>> for epoch in range(1, max_epoch): ... for keys, batch in iter_fatory.build_iter(epoch): ... model(**batch)\",\"The mini-batches for each epochs are fully controlled by this class. Note that the random seed used for shuffling is decided as “seed + epoch” and the generated mini-batches can be reproduces when resuming.\",\"Note that the definition of “epoch” doesn’t always indicate to run out of the whole training corpus. “–num_iters_per_epoch” option restricts the number of iterations for each epoch and the rest of samples for the originally epoch are left for the next epoch. e.g. If The number of mini-batches equals to 4, the following two are same:\",\"1 epoch without “–num_iters_per_epoch”\",\"4 epoch with “–num_iters_per_epoch” == 1\",\"classmethod build_model(args: Namespace) → ESPnetEnhancementModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2254\":{\"h\":\"espnet2.tasks.gan_codec.GANCodecTask\",\"t\":[\"source\",\"class espnet2.tasks.gan_codec.GANCodecTask\",\"Bases: AbsTask\",\"GAN-based neural codec task.\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetGANCodecModel\",\"classmethod build_optimizers(args: Namespace, model: ESPnetGANCodecModel) → List[Optimizer]\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 2\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of GANTrainer\"]},\"2255\":{\"h\":\"espnet2.tasks.gan_svs.GANSVSTask\",\"t\":[\"source\",\"class espnet2.tasks.gan_svs.GANSVSTask\",\"Bases: AbsTask\",\"GAN-based Singing-voice-synthesis task.\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetGANSVSModel\",\"classmethod build_optimizers(args: Namespace, model: ESPnetGANSVSModel) → List[Optimizer]\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 2\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of GANTrainer\"]},\"2256\":{\"h\":\"espnet2.tasks.gan_tts.GANTTSTask\",\"t\":[\"source\",\"class espnet2.tasks.gan_tts.GANTTSTask\",\"Bases: AbsTask\",\"GAN-based text-to-speech task.\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetGANTTSModel\",\"classmethod build_optimizers(args: Namespace, model: ESPnetGANTTSModel) → List[Optimizer]\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 2\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of GANTrainer\"]},\"2257\":{\"h\":\"espnet2.tasks.hubert.HubertTask\",\"t\":[\"source\",\"class espnet2.tasks.hubert.HubertTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → HubertPretrainModel | TorchAudioHubertPretrainModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2258\":{\"h\":\"espnet2.tasks.abs_task.IteratorOptions\",\"t\":[\"source\",\"class espnet2.tasks.abs_task.IteratorOptions(preprocess_fn: <built-in function callable>, collate_fn: <built-in function callable>, data_path_and_name_and_type: list, shape_files: list, batch_size: int, batch_bins: int, batch_type: str, max_cache_size: float, max_cache_fd: int, allow_multi_rates: bool, distributed: bool, num_batches: Optional[int], num_iters_per_epoch: Optional[int], train: bool)\",\"Bases: object\",\"allow_multi_rates : bool\",\"batch_bins : int\",\"batch_size : int\",\"batch_type : str\",\"collate_fn : callable\",\"data_path_and_name_and_type : list\",\"distributed : bool\",\"max_cache_fd : int\",\"max_cache_size : float\",\"num_batches : int | None\",\"num_iters_per_epoch : int | None\",\"preprocess_fn : callable\",\"shape_files : list\",\"train : bool\"]},\"2259\":{\"h\":\"espnet2.tasks.lid.LIDTask\",\"t\":[\"source\",\"class espnet2.tasks.lid.LIDTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetLIDModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of LIDTrainer\"]},\"2260\":{\"h\":\"espnet2.tasks.lm.LMTask\",\"t\":[\"source\",\"class espnet2.tasks.lm.LMTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetLanguageModel | ESPnetMultitaskLanguageModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2261\":{\"h\":\"espnet2.tasks.mt.MTTask\",\"t\":[\"source\",\"class espnet2.tasks.mt.MTTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetMTModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2262\":{\"h\":\"espnet2.tasks.ps2st.PS2STTask\",\"t\":[\"source\",\"class espnet2.tasks.ps2st.PS2STTask\",\"Bases: AbsTask\",\"PS2ST refers to the prompt-based speech-to-speech/text task.\",\"The prompt is a text that serves as an instruction for the model to do a specific task such as ASR, IC, ST, etc. The output can be a text sequence or speech, depending on the task. For example, transcriptions for ASR, textual labels for classification, or synthesized speech for speech generation tasks.\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"Add task-specific arguments\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable\",\"Build collate function\",\"classmethod build_model(args: Namespace) → ESPnetQwen2AudioModel\",\"Build the Qwen2-Audio model\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable | None\",\"Build preprocessing function\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define optional data names\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define required data names\",\"trainer\",\"alias of Trainer\"]},\"2263\":{\"h\":\"espnet2.tasks.s2st.S2STTask\",\"t\":[\"source\",\"class espnet2.tasks.s2st.S2STTask\",\"Bases: STTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetS2STModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"classmethod build_vocoder_from_file(vocoder_config_file: Path | str | None = None, vocoder_file: Path | str | None = None, model: ESPnetS2STModel | None = None, device: str = 'cpu')\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2264\":{\"h\":\"espnet2.tasks.s2t_ctc.S2TTask\",\"t\":[\"source\",\"class espnet2.tasks.s2t_ctc.S2TTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace)\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2265\":{\"h\":\"espnet2.tasks.slu.SLUTask\",\"t\":[\"source\",\"class espnet2.tasks.slu.SLUTask\",\"Bases: ASRTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_model(args: Namespace) → ESPnetSLUModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2266\":{\"h\":\"espnet2.tasks.ssl.SSLTask\",\"t\":[\"source\",\"class espnet2.tasks.ssl.SSLTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetSSLModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2267\":{\"h\":\"espnet2.tasks.st.STTask\",\"t\":[\"source\",\"class espnet2.tasks.st.STTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetSTModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2268\":{\"h\":\"espnet2.tasks.svs.SVSTask\",\"t\":[\"source\",\"class espnet2.tasks.svs.SVSTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetSVSModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"classmethod build_vocoder_from_file(vocoder_config_file: Path | str | None = None, vocoder_file: Path | str | None = None, model: ESPnetSVSModel | None = None, device: str = 'cpu')\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2269\":{\"h\":\"espnet2.tasks.spk.SpeakerTask\",\"t\":[\"source\",\"class espnet2.tasks.spk.SpeakerTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetSpeakerModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of SpkTrainer\"]},\"2270\":{\"h\":\"espnet2.tasks.tts2.TTS2Task\",\"t\":[\"source\",\"class espnet2.tasks.tts2.TTS2Task\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetTTS2Model\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"classmethod build_vocoder_from_file(vocoder_config_file: Path | str | None = None, vocoder_file: Path | str | None = None, model: ESPnetTTS2Model | None = None, device: str = 'cpu')\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2271\":{\"h\":\"espnet2.tasks.tts.TTSTask\",\"t\":[\"source\",\"class espnet2.tasks.tts.TTSTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetTTSModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"classmethod build_vocoder_from_file(vocoder_config_file: Path | str | None = None, vocoder_file: Path | str | None = None, model: ESPnetTTSModel | None = None, device: str = 'cpu')\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2272\":{\"h\":\"espnet2.tasks.enh_tse.TargetSpeakerExtractionTask\",\"t\":[\"source\",\"class espnet2.tasks.enh_tse.TargetSpeakerExtractionTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetExtractionModel\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 1\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of Trainer\"]},\"2273\":{\"h\":\"espnet2.tasks.uasr.UASRTask\",\"t\":[\"source\",\"class espnet2.tasks.uasr.UASRTask\",\"Bases: AbsTask\",\"classmethod add_task_arguments(parser: ArgumentParser)\",\"classmethod build_collate_fn(args: Namespace, train: bool) → Callable[[Collection[Tuple[str, Dict[str, ndarray]]]], Tuple[List[str], Dict[str, Tensor]]]\",\"Return “collate_fn”, which is a callable object and given to DataLoader.\",\">>> from torch.utils.data import DataLoader >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args, train=True), ...)\",\"In many cases, you can use our common collate_fn.\",\"classmethod build_model(args: Namespace) → ESPnetUASRModel\",\"classmethod build_optimizers(args: Namespace, model: ESPnetUASRModel) → List[Optimizer]\",\"classmethod build_preprocess_fn(args: Namespace, train: bool) → Callable[[str, Dict[str, array]], Dict[str, ndarray]] | None\",\"class_choices_list : List[[ClassChoices](../train/ClassChoices.md#espnet2.train.class_choices.ClassChoices)]= [<espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>, <espnet2.train.class_choices.ClassChoices object>]\",\"num_optimizers : int= 2\",\"classmethod optional_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the optional names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as follows,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “optional_data_names” should be as\",\">>> optional_data_names = ('opt',)\",\"classmethod required_data_names(train: bool = True, inference: bool = False) → Tuple[str, ...]\",\"Define the required names by Task\",\"This function is used by\",\"cls.check_task_requirements() If your model is defined as following,\",\">>> from espnet2.train.abs_espnet_model import AbsESPnetModel >>> class Model(AbsESPnetModel): ... def forward(self, input, output, opt=None): pass\",\"then “required_data_names” should be as\",\">>> required_data_names = ('input', 'output')\",\"trainer\",\"alias of UASRTrainer\"]},\"2274\":{\"h\":\"espnet2.text.abs_tokenizer.AbsTokenizer\",\"t\":[\"source\",\"class espnet2.text.abs_tokenizer.AbsTokenizer\",\"Bases: ABC\",\"abstract text2tokens(line: str) → List[str]\",\"abstract tokens2text(tokens: Iterable[str]) → str\"]},\"2275\":{\"h\":\"espnet2.text.char_tokenizer.CharTokenizer\",\"t\":[\"source\",\"class espnet2.text.char_tokenizer.CharTokenizer(non_linguistic_symbols: Path | str | Iterable[str] | None = None, space_symbol: str = '<space>', remove_non_linguistic_symbols: bool = False, nonsplit_symbols: Iterable[str] | None = None)\",\"Bases: AbsTokenizer\",\"text2tokens(line: str) → List[str]\",\"tokens2text(tokens: Iterable[str]) → str\"]},\"2276\":{\"h\":\"espnet2.text.phoneme_tokenizer.G2p_en\",\"t\":[\"source\",\"class espnet2.text.phoneme_tokenizer.G2p_en(no_space: bool = False)\",\"Bases: object\",\"On behalf of g2p_en.G2p.\",\"g2p_en.G2p isn’t pickalable and it can’t be copied to the other processes via multiprocessing module. As a workaround, g2p_en.G2p is instantiated upon calling this class.\"]},\"2277\":{\"h\":\"espnet2.text.phoneme_tokenizer.G2pk\",\"t\":[\"source\",\"class espnet2.text.phoneme_tokenizer.G2pk(descritive=False, group_vowels=False, to_syl=False, no_space=False, explicit_space=False, space_symbol='<space>')\",\"Bases: object\",\"On behalf of g2pk.G2p.\",\"g2pk.G2p isn’t pickalable and it can’t be copied to the other processes via multiprocessing module. As a workaround, g2pk.G2p is instantiated upon calling this class.\"]},\"2278\":{\"h\":\"espnet2.text.hugging_face_token_id_converter.HuggingFaceTokenIDConverter\",\"t\":[\"source\",\"class espnet2.text.hugging_face_token_id_converter.HuggingFaceTokenIDConverter(model_name_or_path: str)\",\"Bases: object\",\"get_num_vocabulary_size() → int\",\"ids2tokens(integers: ndarray | Iterable[int]) → List[str]\",\"tokens2ids(tokens: Iterable[str]) → List[int]\"]},\"2279\":{\"h\":\"espnet2.text.hugging_face_tokenizer.HuggingFaceTokenizer\",\"t\":[\"source\",\"class espnet2.text.hugging_face_tokenizer.HuggingFaceTokenizer(model: Path | str)\",\"Bases: AbsTokenizer\",\"text2tokens(line: str) → List[str]\",\"tokens2text(tokens: Iterable[str]) → str\"]},\"2280\":{\"h\":\"espnet2.text.phoneme_tokenizer.IsG2p\",\"t\":[\"source\",\"class espnet2.text.phoneme_tokenizer.IsG2p(dialect: str = 'standard', syllabify: bool = True, word_sep: str = ',', use_dict: bool = True)\",\"Bases: object\",\"Minimal wrapper for https://github.com/grammatek/ice-g2p\",\"The g2p module uses a Bi-LSTM model along with a pronunciation dictionary to generate phonemization Unfortunately does not support multi-thread phonemization as of yet\"]},\"2281\":{\"h\":\"espnet2.text.phoneme_tokenizer.Jaso\",\"t\":[\"source\",\"class espnet2.text.phoneme_tokenizer.Jaso(space_symbol=' ', no_space=False)\",\"Bases: object\",\"JAMO_LEADS = 'ᄀᄁᄂᄃᄄᄅᄆᄇᄈᄉᄊᄋᄌᄍᄎᄏᄐᄑᄒ'\",\"JAMO_TAILS = 'ᆨᆩᆪᆫᆬᆭᆮᆯᆰᆱᆲᆳᆴᆵᆶᆷᆸᆹᆺᆻᆼᆽᆾᆿᇀᇁᇂ'\",\"JAMO_VOWELS = 'ᅡᅢᅣᅤᅥᅦᅧᅨᅩᅪᅫᅬᅭᅮᅯᅰᅱᅲᅳᅴᅵ'\",\"PUNC = \\\"!'(),-.:;?\\\"\",\"SPACE = ' '\",\"VALID_CHARS = \\\"ᄀᄁᄂᄃᄄᄅᄆᄇᄈᄉᄊᄋᄌᄍᄎᄏᄐᄑ하ᅢᅣᅤᅥᅦᅧᅨᅩᅪᅫᅬᅭᅮᅯᅰᅱᅲᅳᅴᅵᆨᆩᆪᆫᆬᆭᆮᆯᆰᆱᆲᆳᆴᆵᆶᆷᆸᆹᆺᆻᆼᆽᆾᆿᇀᇁᇂ!'(),-.:;? \\\"\"]},\"2282\":{\"h\":\"espnet2.text.korean_cleaner.KoreanCleaner\",\"t\":[\"source\",\"class espnet2.text.korean_cleaner.KoreanCleaner\",\"Bases: object\",\"classmethod normalize_text(text)\"]},\"2283\":{\"h\":\"espnet2.text.whisper_token_id_converter.OpenAIWhisperTokenIDConverter\",\"t\":[\"source\",\"class espnet2.text.whisper_token_id_converter.OpenAIWhisperTokenIDConverter(model_type: str, language: str | None = 'en', task: str = 'transcribe', added_tokens_txt: str | None = None, sot: bool = False, speaker_change_symbol: str = '<sc>')\",\"Bases: object\",\"get_num_vocabulary_size() → int\",\"ids2tokens(integers: ndarray | Iterable[int]) → List[str]\",\"tokens2ids(tokens: Iterable[str]) → List[int]\"]},\"2284\":{\"h\":\"espnet2.text.whisper_tokenizer.OpenAIWhisperTokenizer\",\"t\":[\"source\",\"class espnet2.text.whisper_tokenizer.OpenAIWhisperTokenizer(model_type: str, language: str = 'en', task: str = 'transcribe', sot: bool = False, speaker_change_symbol: str = '<sc>', added_tokens_txt: str | None = None)\",\"Bases: AbsTokenizer\",\"text2tokens(line: str) → List[str]\",\"tokens2text(tokens: Iterable[str]) → str\"]},\"2285\":{\"h\":\"espnet2.text.phoneme_tokenizer.PhonemeTokenizer\",\"t\":[\"source\",\"class espnet2.text.phoneme_tokenizer.PhonemeTokenizer(g2p_type: None | str, non_linguistic_symbols: Path | str | Iterable[str] | None = None, space_symbol: str = '<space>', remove_non_linguistic_symbols: bool = False)\",\"Bases: AbsTokenizer\",\"text2tokens(line: str) → List[str]\",\"text2tokens_svs(syllable: str) → List[str]\",\"tokens2text(tokens: Iterable[str]) → str\"]},\"2286\":{\"h\":\"espnet2.text.phoneme_tokenizer.Phonemizer\",\"t\":[\"source\",\"class espnet2.text.phoneme_tokenizer.Phonemizer(backend, word_separator: str | None = None, syllable_separator: str | None = None, phone_separator: str | None = ' ', strip=False, split_by_single_token: bool = False, **phonemizer_kwargs)\",\"Bases: object\",\"Phonemizer module for various languages.\",\"This is wrapper module of https://github.com/bootphon/phonemizer. You can define various g2p modules by specifying options for phonemizer.\",\"See available options: : https://github.com/bootphon/phonemizer/blob/master/phonemizer/phonemize.py#L32\"]},\"2287\":{\"h\":\"espnet2.text.qwen2audio_tokenizer.Qwen2AudioTokenizer\",\"t\":[\"source\",\"class espnet2.text.qwen2audio_tokenizer.Qwen2AudioTokenizer(model_name: str = 'Qwen/Qwen2-Audio-7B-Instruct')\",\"Bases: AbsTokenizer\",\"Qwen2-Audio tokenizer that handles both text and audio inputs\",\"create_multimodal_query(text_input: str, audio_input: Tuple[List[ndarray], int] | None = None) → Dict\",\"Create query with both text and audio inputs for Qwen2-Audio.\",\"This is the core tokenization process from the original example.\",\"text2tokens(line: str) → List[str]\",\"Convert text to tokens using Qwen2-Audio processor\",\"tokens2text(tokens: Iterable[str]) → str\",\"Convert tokens back to text\"]},\"2288\":{\"h\":\"espnet2.text.sentencepiece_tokenizer.SentencepiecesTokenizer\",\"t\":[\"source\",\"class espnet2.text.sentencepiece_tokenizer.SentencepiecesTokenizer(model: Path | str, encode_kwargs: Dict = {})\",\"Bases: AbsTokenizer\",\"text2tokens(line: str) → List[str]\",\"tokens2text(tokens: Iterable[str]) → str\"]},\"2289\":{\"h\":\"espnet2.text.cleaner.TextCleaner\",\"t\":[\"source\",\"class espnet2.text.cleaner.TextCleaner(cleaner_types: Collection[str] | None = None)\",\"Bases: object\",\"Text cleaner.\"]},\"2290\":{\"h\":\"Examples\",\"t\":[\">>> cleaner = TextCleaner(\\\"tacotron\\\") >>> cleaner(\\\"(Hello-World); & jr. & dr.\\\") 'HELLO WORLD, AND JUNIOR AND DOCTOR'\"]},\"2291\":{\"h\":\"espnet2.text.token_id_converter.TokenIDConverter\",\"t\":[\"source\",\"class espnet2.text.token_id_converter.TokenIDConverter(token_list: Path | str | Iterable[str], unk_symbol: str = '<unk>')\",\"Bases: object\",\"get_num_vocabulary_size() → int\",\"ids2tokens(integers: ndarray | Iterable[int]) → List[str]\",\"tokens2ids(tokens: Iterable[str]) → List[int]\"]},\"2292\":{\"h\":\"espnet2.text.word_tokenizer.WordTokenizer\",\"t\":[\"source\",\"class espnet2.text.word_tokenizer.WordTokenizer(delimiter: str | None = None, non_linguistic_symbols: Path | str | Iterable[str] | None = None, remove_non_linguistic_symbols: bool = False)\",\"Bases: AbsTokenizer\",\"text2tokens(line: str) → List[str]\",\"tokens2text(tokens: Iterable[str]) → str\"]},\"2293\":{\"h\":\"espnet2.text.build_tokenizer.build_tokenizer\",\"t\":[\"source\",\"espnet2.text.build_tokenizer.build_tokenizer(token_type: str, bpemodel: Path | str | Iterable[str] | None = None, non_linguistic_symbols: Path | str | Iterable[str] | None = None, remove_non_linguistic_symbols: bool = False, space_symbol: str = '<space>', delimiter: str | None = None, g2p_type: str | None = None, nonsplit_symbol: Iterable[str] | None = None, encode_kwargs: Dict | None = None, whisper_language: str | None = None, whisper_task: str | None = None, sot_asr: bool = False) → AbsTokenizer\",\"A helper function to instantiate Tokenizer\"]},\"2294\":{\"h\":\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p\",\"t\":[\"source\",\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p(text) → List[str]\"]},\"2295\":{\"h\":\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_accent\",\"t\":[\"source\",\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_accent(text) → List[str]\"]},\"2296\":{\"h\":\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_accent_with_pause\",\"t\":[\"source\",\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_accent_with_pause(text) → List[str]\"]},\"2297\":{\"h\":\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_kana\",\"t\":[\"source\",\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_kana(text) → List[str]\"]},\"2298\":{\"h\":\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_prosody\",\"t\":[\"source\",\"espnet2.text.phoneme_tokenizer.pyopenjtalk_g2p_prosody(text: str, drop_unvoiced_vowels: bool = True) → List[str]\",\"Extract phoneme + prosoody symbol sequence from input full-context labels.\",\"The algorithm is based on Prosodic features control by symbols as input of sequence-to-sequence acoustic modeling for neural TTS with some r9y9’s tweaks.\",\"Parameters:\",\"text (str) – Input text.\",\"drop_unvoiced_vowels (bool) – whether to drop unvoiced vowels.\",\"Returns: List of phoneme + prosody symbols.\",\"Return type: List[str]\"]},\"2299\":{\"h\":\"Examples\",\"t\":[\">>> from espnet2.text.phoneme_tokenizer import pyopenjtalk_g2p_prosody >>> pyopenjtalk_g2p_prosody(\\\"こんにちは。\\\") ['^', 'k', 'o', '[', 'N', 'n', 'i', 'ch', 'i', 'w', 'a', '$']\"]},\"2300\":{\"h\":\"espnet2.text.phoneme_tokenizer.pypinyin_g2p\",\"t\":[\"source\",\"espnet2.text.phoneme_tokenizer.pypinyin_g2p(text) → List[str]\"]},\"2301\":{\"h\":\"espnet2.text.phoneme_tokenizer.pypinyin_g2p_phone\",\"t\":[\"source\",\"espnet2.text.phoneme_tokenizer.pypinyin_g2p_phone(text) → List[str]\"]},\"2302\":{\"h\":\"espnet2.text.phoneme_tokenizer.pypinyin_g2p_phone_without_prosody\",\"t\":[\"source\",\"espnet2.text.phoneme_tokenizer.pypinyin_g2p_phone_without_prosody(text) → List[str]\"]},\"2303\":{\"h\":\"espnet2.text.phoneme_tokenizer.split_by_space\",\"t\":[\"source\",\"espnet2.text.phoneme_tokenizer.split_by_space(text) → List[str]\"]},\"2304\":{\"h\":\"espnet2.torch_utils.forward_adaptor.ForwardAdaptor\",\"t\":[\"source\",\"class espnet2.torch_utils.forward_adaptor.ForwardAdaptor(module: Module, name: str)\",\"Bases: Module\",\"Wrapped module to parallelize specified method\",\"torch.nn.DataParallel parallelizes only “forward()” and, maybe, the method having the other name can’t be applied except for wrapping the module just like this class.\"]},\"2305\":{\"h\":\"Examples\",\"t\":[\">>> class A(torch.nn.Module): ... def foo(self, x): ... ... >>> model = A() >>> model = ForwardAdaptor(model, \\\"foo\\\") >>> model = torch.nn.DataParallel(model, device_ids=[0, 1]) >>> x = torch.randn(2, 10) >>> model(x)\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(*args, **kwargs)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2306\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2307\":{\"h\":\"espnet2.torch_utils.add_gradient_noise.add_gradient_noise\",\"t\":[\"source\",\"espnet2.torch_utils.add_gradient_noise.add_gradient_noise(model: Module, iteration: int, duration: float = 100, eta: float = 1.0, scale_factor: float = 0.55)\",\"Adds noise from a standard normal distribution to the gradients.\",\"The standard deviation (sigma) is controlled by the three hyper-parameters below. sigma goes to zero (no noise) with more iterations.\",\"Parameters:\",\"model – Model.\",\"iteration – Number of iterations.\",\"duration – {100, 1000}: Number of durations to control the interval of the sigma change.\",\"eta – {0.01, 0.3, 1.0}: The magnitude of sigma.\",\"scale_factor – {0.55}: The scale of sigma.\"]},\"2308\":{\"h\":\"espnet2.torch_utils.load_pretrained_model.filter_state_dict\",\"t\":[\"source\",\"espnet2.torch_utils.load_pretrained_model.filter_state_dict(dst_state: Dict[str, float | Tensor], src_state: Dict[str, float | Tensor])\",\"Filter name, size mismatch instances between dicts.\",\"Parameters:\",\"dst_state – reference state dict for filtering\",\"src_state – target state dict for filtering\"]},\"2309\":{\"h\":\"espnet2.torch_utils.device_funcs.force_gatherable\",\"t\":[\"source\",\"espnet2.torch_utils.device_funcs.force_gatherable(data, device)\",\"Change object to gatherable in torch.nn.DataParallel recursively\",\"The difference from to_device() is changing to torch.Tensor if float or int value is found.\",\"The restriction to the returned value in DataParallel: : The object must be\",\"torch.cuda.Tensor\",\"1 or more dimension. 0-dimension-tensor sends warning. or a list, tuple, dict.\"]},\"2310\":{\"h\":\"espnet2.torch_utils.model_summary.get_human_readable_count\",\"t\":[\"source\",\"espnet2.torch_utils.model_summary.get_human_readable_count(number: int) → str\",\"Return human_readable_count\",\"Originated from: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/memory.py\",\"Abbreviates an integer number with K, M, B, T for thousands, millions, billions and trillions, respectively. .. rubric:: Examples\",\">>> get_human_readable_count(123) '123 ' >>> get_human_readable_count(1234) # (one thousand) '1 K' >>> get_human_readable_count(2e6) # (two million) '2 M' >>> get_human_readable_count(3e9) # (three billion) '3 B' >>> get_human_readable_count(4e12) # (four trillion) '4 T' >>> get_human_readable_count(5e15) # (more than trillion) '5,000 T'\",\"Parameters:number – a positive integer number\",\"Returns: A string formatted according to the pattern described above.\"]},\"2311\":{\"h\":\"espnet2.torch_utils.get_layer_from_string.get_layer\",\"t\":[\"source\",\"espnet2.torch_utils.get_layer_from_string.get_layer(l_name, library=<module 'torch.nn' from '/home/runner/work/espnet/espnet/tools/venv/lib/python3.10/site-packages/torch/nn/_init_.py'>)\",\"Return layer object handler from library e.g. from torch.nn\",\"E.g. if l_name==”elu”, returns torch.nn.ELU.\",\"Parameters:\",\"l_name (string) – Case insensitive name for layer in library (e.g. .’elu’).\",\"library (module) – Name of library/module where to search for object handler\",\"\\\"torch.nn\\\". (with l_name e.g.)\",\"Returns: handler for the requested layer e.g. (torch.nn.ELU)\",\"Return type: layer_handler (object)\"]},\"2312\":{\"h\":\"espnet2.torch_utils.initialize.initialize\",\"t\":[\"source\",\"espnet2.torch_utils.initialize.initialize(model: Module, init: str)\",\"Initialize weights of a neural network module.\",\"Parameters are initialized using the given method or distribution.\",\"Custom initialization routines can be implemented into submodules as function espnet_initialization_fn within the custom module.\",\"Parameters:\",\"model – Target.\",\"init – Method of initialization.\"]},\"2313\":{\"h\":\"espnet2.torch_utils.get_flash_attn_compatability.is_flash_attn_supported\",\"t\":[\"source\",\"espnet2.torch_utils.get_flash_attn_compatability.is_flash_attn_supported() → bool\"]},\"2314\":{\"h\":\"espnet2.torch_utils.load_pretrained_model.load_pretrained_model\",\"t\":[\"source\",\"espnet2.torch_utils.load_pretrained_model.load_pretrained_model(init_param: str, model: Module, ignore_init_mismatch: bool, map_location: str = 'cpu')\",\"Load a model state and set it to the model.\",\"Parameters:init_param – <file_path>:<src_key>:<dst_key>:<exclude_Keys>\"]},\"2315\":{\"h\":\"Examples\",\"t\":[\">>> load_pretrained_model(\\\"somewhere/model.pth\\\", model) >>> load_pretrained_model(\\\"somewhere/model.pth:decoder:decoder\\\", model) >>> load_pretrained_model(\\\"somewhere/model.pth:decoder:decoder:\\\", model) >>> load_pretrained_model( ... \\\"somewhere/model.pth:decoder:decoder:decoder.embed\\\", model ... ) >>> load_pretrained_model(\\\"somewhere/decoder.pth::decoder\\\", model)\"]},\"2316\":{\"h\":\"espnet2.torch_utils.model_summary.model_summary\",\"t\":[\"source\",\"espnet2.torch_utils.model_summary.model_summary(model: Module) → str\"]},\"2317\":{\"h\":\"espnet2.torch_utils.pytorch_version.pytorch_cudnn_version\",\"t\":[\"source\",\"espnet2.torch_utils.pytorch_version.pytorch_cudnn_version() → str\"]},\"2318\":{\"h\":\"espnet2.torch_utils.recursive_op.recursive_average\",\"t\":[\"source\",\"espnet2.torch_utils.recursive_op.recursive_average(obj, weight: Tensor, distributed: bool = False)\"]},\"2319\":{\"h\":\"espnet2.torch_utils.recursive_op.recursive_divide\",\"t\":[\"source\",\"espnet2.torch_utils.recursive_op.recursive_divide(a, b: Tensor)\"]},\"2320\":{\"h\":\"espnet2.torch_utils.recursive_op.recursive_sum\",\"t\":[\"source\",\"espnet2.torch_utils.recursive_op.recursive_sum(obj, weight: Tensor, distributed: bool = False)\"]},\"2321\":{\"h\":\"espnet2.torch_utils.set_all_random_seed.set_all_random_seed\",\"t\":[\"source\",\"espnet2.torch_utils.set_all_random_seed.set_all_random_seed(seed: int)\"]},\"2322\":{\"h\":\"espnet2.torch_utils.model_summary.to_bytes\",\"t\":[\"source\",\"espnet2.torch_utils.model_summary.to_bytes(dtype) → int\"]},\"2323\":{\"h\":\"espnet2.torch_utils.device_funcs.to_device\",\"t\":[\"source\",\"espnet2.torch_utils.device_funcs.to_device(data, device=None, dtype=None, non_blocking=False, copy=False)\",\"Change the device of object recursively\"]},\"2324\":{\"h\":\"espnet2.train.dataset.AbsDataset\",\"t\":[\"source\",\"class espnet2.train.dataset.AbsDataset\",\"Bases: Dataset, ABC\",\"abstract has_name(name) → bool\",\"abstract names() → Tuple[str, ...]\"]},\"2325\":{\"h\":\"espnet2.train.abs_espnet_model.AbsESPnetModel\",\"t\":[\"source\",\"class espnet2.train.abs_espnet_model.AbsESPnetModel(*args, **kwargs)\",\"Bases: Module, ABC\",\"The common abstract class among each tasks\",\"“ESPnetModel” is referred to a class which inherits torch.nn.Module, and makes the dnn-models forward as its member field, a.k.a delegate pattern, and defines “loss”, “stats”, and “weight” for the task.\",\"If you intend to implement new task in ESPNet, the model must inherit this class. In other words, the “mediator” objects between our training system and the your task class are just only these three values, loss, stats, and weight.\",\"Example\",\">>> from espnet2.tasks.abs_task import AbsTask >>> class YourESPnetModel(AbsESPnetModel): ... def forward(self, input, input_lengths): ... ... ... return loss, stats, weight >>> class YourTask(AbsTask): ... @classmethod ... def build_model(cls, args: argparse.Namespace) -> YourESPnetModel:\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract collect_feats(**batch: Tensor) → Dict[str, Tensor]\",\"abstract forward(**batch: Tensor) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2326\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2327\":{\"h\":\"espnet2.train.abs_gan_espnet_model.AbsGANESPnetModel\",\"t\":[\"source\",\"class espnet2.train.abs_gan_espnet_model.AbsGANESPnetModel(*args, **kwargs)\",\"Bases: AbsESPnetModel, Module, ABC\",\"The common abstract class among each GAN-based task.\",\"“ESPnetModel” is referred to a class which inherits torch.nn.Module, and makes the dnn-models “forward” as its member field, a.k.a delegate pattern. And “forward” must accept the argument “forward_generator” and Return the dict of “loss”, “stats”, “weight”, and “optim_idx”. “optim_idx” for generator must be 0 and that for discriminator must be 1.\",\"Example\",\">>> from espnet2.tasks.abs_task import AbsTask >>> class YourESPnetModel(AbsGANESPnetModel): ... def forward(self, input, input_lengths, forward_generator=True): ... ... ... if forward_generator: ... # return loss for the generator ... # optim idx 0 indicates generator optimizer ... return dict(loss=loss, stats=stats, weight=weight, optim_idx=0) ... else: ... # return loss for the discriminator ... # optim idx 1 indicates discriminator optimizer ... return dict(loss=loss, stats=stats, weight=weight, optim_idx=1) >>> class YourTask(AbsTask): ... @classmethod ... def build_model(cls, args: argparse.Namespace) -> YourESPnetModel:\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract collect_feats(**batch: Tensor) → Dict[str, Tensor]\",\"abstract forward(forward_generator: bool = True, **batch: Tensor) → Dict[str, Tensor | Dict[str, Tensor] | int]\",\"Return the generator loss or the discrimiantor loss.\",\"This method must have an argument “forward_generator” to switch the generator loss calculation and the discrimiantor loss calculation. If forward_generator is true, return the generator loss with optim_idx 0. If forward_generator is false, return the discrimiantor loss with optim_idx 1.\",\"Parameters:forward_generator (bool) – Whether to return the generator loss or the discrimiantor loss. This must have the default value.\",\"Returns:\",\"loss (Tensor): Loss scalar tensor.\",\"stats (Dict[str, float]): Statistics to be monitored.\",\"weight (Tensor): Weight tensor to summarize losses.\",\"optim_idx (int): Optimizer index (0 for G and 1 for D).\",\"Return type: Dict[str, Any]\"]},\"2328\":{\"h\":\"espnet2.train.preprocessor.AbsPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.AbsPreprocessor(train: bool)\",\"Bases: ABC\"]},\"2329\":{\"h\":\"espnet2.train.dataset.AdapterForLabelScpReader\",\"t\":[\"source\",\"class espnet2.train.dataset.AdapterForLabelScpReader(loader: Dict[str, List[List[str | float | int]]])\",\"Bases: Mapping\",\"keys() → a set-like object providing a view on D's keys\"]},\"2330\":{\"h\":\"espnet2.train.dataset.AdapterForSingingScoreScpReader\",\"t\":[\"source\",\"class espnet2.train.dataset.AdapterForSingingScoreScpReader(loader)\",\"Bases: Mapping\",\"keys() → a set-like object providing a view on D's keys\"]},\"2331\":{\"h\":\"espnet2.train.dataset.AdapterForSoundScpReader\",\"t\":[\"source\",\"class espnet2.train.dataset.AdapterForSoundScpReader(loader, dtype: str | None = None, allow_multi_rates: bool = False)\",\"Bases: Mapping\",\"keys() → a set-like object providing a view on D's keys\"]},\"2332\":{\"h\":\"espnet2.train.reporter.Average\",\"t\":[\"source\",\"class espnet2.train.reporter.Average(value: float | int | complex | torch.Tensor | numpy.ndarray)\",\"Bases: ReportedValue\",\"value : float | int | complex | Tensor | ndarray\"]},\"2333\":{\"h\":\"espnet2.train.lightning_callbacks.AverageCheckpointsCallback\",\"t\":[\"source\",\"class espnet2.train.lightning_callbacks.AverageCheckpointsCallback(output_dir, best_ckpt_callbacks)\",\"Bases: Callback\",\"on_fit_end(trainer, pl_module)\",\"Called when fit ends.\"]},\"2334\":{\"h\":\"espnet2.train.class_choices.ClassChoices\",\"t\":[\"source\",\"class espnet2.train.class_choices.ClassChoices(name: str, classes: Mapping[str, Type], type_check: Type | None = None, default: str | None = None, optional: bool = False)\",\"Bases: object\",\"Helper class to manage the options for variable objects and its configuration.\",\"Example:\",\">>> class A: ... def __init__(self, foo=3): pass >>> class B: ... def __init__(self, bar=\\\"aaaa\\\"): pass >>> choices = ClassChoices(\\\"var\\\", dict(a=A, b=B), default=\\\"a\\\") >>> import argparse >>> parser = argparse.ArgumentParser() >>> choices.add_arguments(parser) >>> args = parser.parse_args([\\\"--var\\\", \\\"a\\\", \\\"--var_conf\\\", \\\"foo=4\\\") >>> args.var a >>> args.var_conf {\\\"foo\\\": 4} >>> class_obj = choices.get_class(args.var) >>> a_object = class_obj(**args.var_conf)\",\"add_arguments(parser)\",\"choices() → Tuple[str | None, ...]\",\"get_class(name: str | None) → type | None\"]},\"2335\":{\"h\":\"espnet2.train.collate_fn.CommonCollateFn\",\"t\":[\"source\",\"class espnet2.train.collate_fn.CommonCollateFn(float_pad_value: float | int = 0.0, int_pad_value: int = -32768, not_sequence: Collection[str] = ())\",\"Bases: object\",\"Functor class of common_collate_fn()\"]},\"2336\":{\"h\":\"espnet2.train.preprocessor.CommonPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.CommonPreprocessor(train: bool, use_lang_prompt: bool = False, use_nlp_prompt: bool = False, token_type: str | None = None, token_list: Path | str | Iterable[str] | None = None, bpemodel: Path | str | Iterable[str] | None = None, text_cleaner: Collection[str] | None = None, g2p_type: str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, force_single_channel: bool = False, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, aux_task_names: Collection[str] | None = None, speech_volume_normalize: float | None = None, speech_name: str = 'speech', text_name: str = 'text', fs: int = 0, nonsplit_symbol: Iterable[str] | None = None, data_aug_effects: List | None = None, data_aug_num: List[int] = [1, 1], data_aug_prob: float = 0.0, min_sample_size: int = -1, audio_pad_value: float | int = 0.0, whisper_language: str | None = None, whisper_task: str | None = None)\",\"Bases: AbsPreprocessor\"]},\"2337\":{\"h\":\"espnet2.train.preprocessor.CommonPreprocessor_multi\",\"t\":[\"source\",\"class espnet2.train.preprocessor.CommonPreprocessor_multi(train: bool, use_lang_prompt: bool = False, use_nlp_prompt: bool = False, token_type: str | None = None, token_list: Path | str | Iterable[str] | None = None, bpemodel: Path | str | Iterable[str] | None = None, text_cleaner: Collection[str] | None = None, g2p_type: str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, aux_task_names: Collection[str] | None = None, speech_volume_normalize: float | None = None, speech_name: str = 'speech', text_name: List[str] = ['text'], fs: int = 0, speaker_change_symbol: Iterable[str] | None = None, data_aug_effects: List | None = None, data_aug_num: List[int] = [1, 1], data_aug_prob: float = 0.0, whisper_language: str | None = None, whisper_task: str | None = None)\",\"Bases: CommonPreprocessor\"]},\"2338\":{\"h\":\"espnet2.train.deepspeed_trainer.DeepSpeedTrainer\",\"t\":[\"source\",\"class espnet2.train.deepspeed_trainer.DeepSpeedTrainer\",\"Bases: Trainer\",\"classmethod build_options(args: Namespace) → DeepSpeedTrainerOptions\",\"Build options consumed by train(), eval(), and plot_attention()\",\"static resume(model: None, reporter: Reporter, output_dir: Path)\",\"classmethod run(model: AbsESPnetModel | None, train_iter_factory: AbsIterFactory, valid_iter_factory: AbsIterFactory, trainer_options: DeepSpeedTrainerOptions, **kwargs) → None\",\"Perform training. This method performs the main process of training.\",\"classmethod setup_data_dtype(deepspeed_config: Dict)\",\"classmethod train_one_epoch(model, iterator: Iterable[Tuple[List[str], Dict[str, Tensor]]], reporter: SubReporter, options: DeepSpeedTrainerOptions) → None\",\"classmethod valid_one_epoch(model, iterator: Iterable[Tuple[List[str], Dict[str, Tensor]]], reporter: SubReporter, options: DeepSpeedTrainerOptions) → None\"]},\"2339\":{\"h\":\"espnet2.train.deepspeed_trainer.DeepSpeedTrainerOptions\",\"t\":[\"source\",\"class espnet2.train.deepspeed_trainer.DeepSpeedTrainerOptions(resume: bool, seed: int, train_dtype: str | torch.dtype, log_interval: int | None, output_dir: pathlib.Path | str, max_epoch: int, deepspeed_config: pathlib.Path | str, best_model_criterion: Sequence[Sequence[str]], keep_nbest_models: int | List[int])\",\"Bases: object\",\"best_model_criterion : Sequence[Sequence[str]]\",\"deepspeed_config : Path | str\",\"keep_nbest_models : int | List[int]\",\"log_interval : int | None\",\"max_epoch : int\",\"output_dir : Path | str\",\"resume : bool\",\"seed : int\",\"train_dtype : str | dtype\"]},\"2340\":{\"h\":\"espnet2.train.distributed_utils.DistributedOption\",\"t\":[\"source\",\"class espnet2.train.distributed_utils.DistributedOption(distributed: bool = False, dist_backend: str = 'nccl', dist_init_method: str = 'env://', dist_world_size: int | None = None, dist_rank: int | None = None, local_rank: int | None = None, ngpu: int = 0, dist_master_addr: str | None = None, dist_master_port: int | None = None, dist_launcher: str | None = None, multiprocessing_distributed: bool = True)\",\"Bases: object\",\"dist_backend : str= 'nccl'\",\"dist_init_method : str= 'env://'\",\"dist_launcher : str | None= None\",\"dist_master_addr : str | None= None\",\"dist_master_port : int | None= None\",\"dist_rank : int | None= None\",\"dist_world_size : int | None= None\",\"distributed : bool= False\",\"init_deepspeed()\",\"init_options()\",\"init_torch_distributed()\",\"local_rank : int | None= None\",\"multiprocessing_distributed : bool= True\",\"ngpu : int= 0\"]},\"2341\":{\"h\":\"espnet2.train.preprocessor.DynamicMixingPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.DynamicMixingPreprocessor(train: bool, source_scp: str | None = None, ref_num: int = 2, dynamic_mixing_gain_db: float = 0.0, speech_name: str = 'speech_mix', speech_ref_name_prefix: str = 'speech_ref', mixture_source_name: str | None = None, utt2spk: str | None = None, categories: List | None = None)\",\"Bases: AbsPreprocessor\"]},\"2342\":{\"h\":\"espnet2.train.dataset.ESPnetDataset\",\"t\":[\"source\",\"class espnet2.train.dataset.ESPnetDataset(path_name_type_list: Collection[Tuple[str, str, str]], preprocess: Callable[[str, Dict[str, ndarray]], Dict[str, ndarray]] | None = None, float_dtype: str = 'float32', int_dtype: str = 'long', max_cache_size: float | int | str = 0.0, max_cache_fd: int = 0, allow_multi_rates: bool = False, keys_to_load: Set[str | int] | None = None)\",\"Bases: AbsDataset\",\"Pytorch Dataset class for ESPNet.\"]},\"2343\":{\"h\":\"Examples\",\"t\":[\">>> dataset = ESPnetDataset([('wav.scp', 'input', 'sound'), ... ('token_int', 'output', 'text_int')], ... ) ... uttid, data = dataset['uttid'] {'input': per_utt_array, 'output': per_utt_array}\",\"has_name(name) → bool\",\"names() → Tuple[str, ...]\"]},\"2344\":{\"h\":\"espnet2.train.dataset.ESPnetMultiTaskDataset\",\"t\":[\"source\",\"class espnet2.train.dataset.ESPnetMultiTaskDataset(path_name_type_list: Collection[Tuple[str, str, str]], key_file: str | None = None, **kwargs)\",\"Bases: AbsDataset\",\"ESPnet Multi Task Dataset.\",\"The top-level Dataset object that can manage multiple ESPnetSpeechLMDataset objects, each of which serves a specific task and dataset. This object will query all these ESPnetSpeechLMDataset and combine examples from different tasks for multi-task training. Typically, this dataset is used in ESPnet SpeechLM models See details in: <espnet>/egs2/TEMPLATE/speechlm1#data-loading-and-preprocessing\",\"has_name(name) → bool\",\"names() → Tuple[str, ...]\"]},\"2345\":{\"h\":\"espnet2.train.dataset.ESPnetSpeechLMDataset\",\"t\":[\"source\",\"class espnet2.train.dataset.ESPnetSpeechLMDataset(example_list: List, task: str, **kwargs)\",\"Bases: ESPnetDataset\",\"ESPnet Speech LM Dataset.\",\"Dataset object that is specifically designed for SpeechLM. It will allows dataset-level operations (e.g., on-the-fly speaker prompt sampling). It is task-specific and can be queried by ESPnetMultiTaskDataset.\",\"install_speaker_prompt(uid: str, data: Dict)\",\"Assume the names are utt2spk and wav.scp. Hard code here.\"]},\"2346\":{\"h\":\"espnet2.train.preprocessor.EnhPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.EnhPreprocessor(train: bool, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, speech_volume_normalize: float | None = None, speech_name: str = 'speech_mix', speech_ref_name_prefix: str = 'speech_ref', noise_ref_name_prefix: str = 'noise_ref', dereverb_ref_name_prefix: str = 'dereverb_ref', use_reverberant_ref: bool = False, num_spk: int = 1, num_noise_type: int = 1, sample_rate: int = 8000, force_single_channel: bool = False, channel_reordering: bool = False, categories: List | None = None, data_aug_effects: List | None = None, data_aug_num: List[int] = [1, 1], data_aug_prob: float = 0.0, speech_segment: int | None = None, avoid_allzero_segment: bool = True, flexible_numspk: bool = False)\",\"Bases: CommonPreprocessor\",\"Preprocessor for Speech Enhancement (Enh) task.\"]},\"2347\":{\"h\":\"espnet2.train.gan_trainer.GANTrainer\",\"t\":[\"source\",\"class espnet2.train.gan_trainer.GANTrainer\",\"Bases: Trainer\",\"Trainer for GAN-based training.\",\"classmethod add_arguments(parser: ArgumentParser)\",\"Add additional arguments for GAN-trainer.\",\"classmethod build_options(args: Namespace) → TrainerOptions\",\"Build options consumed by train(), eval(), and plot_attention().\",\"classmethod train_one_epoch(model: Module, iterator: Iterable[Tuple[List[str], Dict[str, Tensor]]], optimizers: Sequence[Optimizer], schedulers: Sequence[AbsScheduler | None], scaler: GradScaler | None, reporter: SubReporter, summary_writer, options: GANTrainerOptions, distributed_option: DistributedOption) → bool\",\"Train one epoch.\",\"classmethod validate_one_epoch(model: Module, iterator: Iterable[Dict[str, Tensor]], reporter: SubReporter, options: GANTrainerOptions, distributed_option: DistributedOption) → None\",\"Validate one epoch.\"]},\"2348\":{\"h\":\"espnet2.train.gan_trainer.GANTrainerOptions\",\"t\":[\"source\",\"class espnet2.train.gan_trainer.GANTrainerOptions(ngpu: int, resume: bool, use_amp: bool, train_dtype: str, grad_noise: bool, accum_grad: int, grad_clip: float, grad_clip_type: float, log_interval: int | None, no_forward_run: bool, use_matplotlib: bool, use_tensorboard: bool, use_wandb: bool, adapter: str, use_adapter: bool, save_strategy: str, output_dir: Path | str, max_epoch: int, seed: int, sharded_ddp: bool, patience: int | None, keep_nbest_models: int | List[int], nbest_averaging_interval: int, early_stopping_criterion: Sequence[str], best_model_criterion: Sequence[Sequence[str]], val_scheduler_criterion: Sequence[str], unused_parameters: bool, wandb_model_log_interval: int, create_graph_in_tensorboard: bool, gradient_as_bucket_view: bool, ddp_comm_hook: str | None, generator_first: bool, skip_discriminator_prob: float)\",\"Bases: TrainerOptions\",\"Trainer option dataclass for GANTrainer.\",\"generator_first : bool\",\"skip_discriminator_prob : float\"]},\"2349\":{\"h\":\"espnet2.train.dataset.H5FileWrapper\",\"t\":[\"source\",\"class espnet2.train.dataset.H5FileWrapper(path: str)\",\"Bases: object\"]},\"2350\":{\"h\":\"espnet2.train.collate_fn.HuBERTCollateFn\",\"t\":[\"source\",\"class espnet2.train.collate_fn.HuBERTCollateFn(float_pad_value: float | int = 0.0, int_pad_value: int = -32768, label_downsampling: int = 1, pad: bool = False, rand_crop: bool = True, crop_audio: bool = True, not_sequence: Collection[str] = (), window_size: float = 25, window_shift: float = 20, sample_rate: float = 16, noise_scp: str = 'data/noise/wav.scp', noise_apply_prob: float = 1.0, noise_db_range: str = '-5_20', dynamic_mixing_gain_db: float = 5.0, dynamic_mixing_prob=0.1, mix_speech: bool = False, reverb_speech: bool = False, rir_scp: str = 'data/rirs/wav.scp', rir_apply_prob: float = 0.3, train: bool = True)\",\"Bases: CommonCollateFn\",\"Functor class of common_collate_fn()\"]},\"2351\":{\"h\":\"espnet2.train.iterable_dataset.IterableESPnetDataset\",\"t\":[\"source\",\"class espnet2.train.iterable_dataset.IterableESPnetDataset(path_name_type_list: Collection[Tuple[str, str, str]], preprocess: Callable[[str, Dict[str, ndarray]], Dict[str, ndarray]] | None = None, float_dtype: str = 'float32', int_dtype: str = 'long', key_file: str | List | None = None, preprocess_prefix: str | None = None)\",\"Bases: IterableDataset\",\"Pytorch Dataset class for ESPNet.\"]},\"2352\":{\"h\":\"Examples\",\"t\":[\">>> dataset = IterableESPnetDataset([('wav.scp', 'input', 'sound'), ... ('token_int', 'output', 'text_int')], ... ) >>> for uid, data in dataset: ... data {'input': per_utt_array, 'output': per_utt_array}\",\"has_name(name) → bool\",\"names() → Tuple[str, ...]\"]},\"2353\":{\"h\":\"espnet2.train.preprocessor.LIDPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.LIDPreprocessor(train: bool, lang2utt: str | None = None, fix_duration: bool = True, target_duration: float | None = None, sample_rate: int = 16000, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_info: List[Tuple[float, str, Tuple[int, int], Tuple[float, float]]] | None = None, noise_apply_prob: float = 1.0, short_noise_thres: float = 0.5)\",\"Bases: CommonPreprocessor\",\"Preprocessor for LID tasks.\",\"Parameters:\",\"train (bool) – Whether to use in training mode.\",\"lang2utt (str) – Path to the lang2utt file.\",\"target_duration (float) – Target duration in seconds, if\",\"fix_duration (bool)\",\"duration. (clip to this)\",\"fix_duration – Whether to fix the duration of the audio.\",\"sample_rate (int) – Sampling rate.\",\"rir_scp (str) – Path to the RIR scp file.\",\"rir_apply_prob (float) – Probability of applying RIR.\",\"noise_info (List *[*Tuple *[*float,str,Tuple *[*int,int],Tuple *[*float,float]]]) –\",\"List of tuples of noise information. Each tuple represents a noise type. Each tuple consists of (prob, noise_scp, num_to_mix, db_range).\",\"prob (float) is the probability of applying the noise type.\",\"noise_scp (str) is the path to the noise scp file.\",\"num_to_mix (Tuple[int, int]) is the range of the number of noises : to be mixed.\",\"db_range (Tuple[float, float]) is the range of noise levels in dB.\",\"noise_apply_prob (float) – Probability of applying noise.\",\"short_noise_thres (float) – Threshold of short noise.\"]},\"2354\":{\"h\":\"espnet2.train.lid_trainer.LIDTrainer\",\"t\":[\"source\",\"class espnet2.train.lid_trainer.LIDTrainer\",\"Bases: Trainer\",\"Trainer designed for LID, adapted from spk_trainer.py\",\"classmethod extract_embed_lid(model: Module, iterator: Iterable[Dict[str, Tensor]], reporter: SubReporter, options: TrainerOptions, distributed_option: DistributedOption, output_dir: str, custom_bs: int, idx2lang: Dict[int, str], extract_embd: bool = False, checkpoint_interval: int = 1000, resume: bool = True, lang_to_embds_dic: Dict[str, List[ndarray]] = None, save_embd_per_utt: bool = False, max_num_utt_per_lang: int | None = None, lang_counter_dic: Dict[str, int] | None = None) → None\",\"Extract LIDs and language embeddings for each utterance in the dataset.\",\"By default, this method performs language identification (LID) for each utterance. If extract_embd=True, it also extracts normalized language embeddings.\",\"lang_embd_dic: {utt_id: lang_embd}, the language embedding for a specific utterance, this is used for temporary saving the language embedding of each utterance, and will be written to the disk every checkpoint_interval utterances. lang_to_embds_dic: {lang: [utt1 embd, utt1 embd …]}, the language embedding for the utterances corresponding to each language, if set extract_embd to True, this will be defaultly used, this will not be written to the dist, but will be (in bin/lid_inference.py) used for calculating the average language embedding for each language, and plotting the tsne plot.\",\"Saved results:\",\"lang_id_dic: {utt_id: predicted_lang}, mapping from utterance ID to\",\"predicted language ID.\",\"lang_embd_dic (optional): {utt_id: lang_embd}, temporary in-memory : storage of per-utterance language embeddings. Saved to disk every checkpoint_interval utterances if save_embd_per_utt=True.\",\"lang_to_embds_dic (optional): {lang: [embd_utt1, embd_utt2, …]}, : mapping from language ID to a list of embeddings from all utterances predicted or labeled with that language. This is not written to disk by this function, but is used downstream (e.g., in bin/lid_inference.py) for computing language-level average embeddings or generating t-SNE visualizations.\",\"Notes:\",\"All extracted embeddings are L2-normalized.\",\"The function supports distributed inference using torch.distributed.\",\"Supports resume functionality by skipping already processed utterances\",\"based on existing output files.\",\"Limits the number of utterances per language if max_num_utt_per_lang is specified.\"]},\"2355\":{\"h\":\"espnet2.train.lightning_espnet_model.LitESPnetModel\",\"t\":[\"source\",\"class espnet2.train.lightning_espnet_model.LitESPnetModel(args)\",\"Bases: LightningModule\",\"configure_optimizers()\",\"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you’d need one. But in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only works in the manual optimization mode.\",\"Returns: Any of these 6 options. \",\"Single optimizer.\",\"List or Tuple of optimizers.\",\"Two lists - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple lr_scheduler_config).\",\"Dictionary, with an \\\"optimizer\\\" key, and (optionally) a \\\"lr_scheduler\\\" key whose value is a single LR scheduler or lr_scheduler_config.\",\"None - Fit will run without any optimizer.\",\"The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below.\",\"lr_scheduler_config = { # REQUIRED: The scheduler instance \\\"scheduler\\\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \\\"interval\\\": \\\"epoch\\\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \\\"frequency\\\": 1, # Metric to monitor for schedulers like `ReduceLROnPlateau` \\\"monitor\\\": \\\"val_loss\\\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \\\"strict\\\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \\\"name\\\": None, }\",\"When there are schedulers in which the .step() method is conditioned on a value, such as the torch.optim.lr_scheduler.ReduceLROnPlateau scheduler, Lightning requires that the lr_scheduler_config contains the keyword \\\"monitor\\\" set to the metric name that the scheduler should be conditioned on.\",\"Metrics can be made available to monitor by simply logging it using self.log('metric_to_track', metric_val) in your LightningModule.\",\"########### NOTE Some things to know:\",\"Lightning calls .backward() and .step() automatically in case of automatic optimization.\",\"If a learning rate scheduler is specified in configure_optimizers() with key \\\"interval\\\" (default “epoch”) in the scheduler configuration, Lightning will call the scheduler’s .step() method automatically in case of automatic optimization.\",\"If you use 16-bit precision (precision=16), Lightning will automatically handle the optimizer.\",\"If you use torch.optim.LBFGS, Lightning handles the closure function automatically for you.\",\"If you use multiple optimizers, you will have to switch to ‘manual optimization’ mode and step them yourself.\",\"If you need to control how often the optimizer steps, override the optimizer_step() hook.\",\"train_dataloader()\",\"An iterable or collection of iterables specifying training samples.\",\"For more information about multiple dataloaders, see this section.\",\"The dataloader you return will not be reloaded unless you set\",\":paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs`\",\"to a positive integer.\",\"For data processing use the following pattern:\",\"download in prepare_data()\",\"process and split in setup()\",\"However, the above are only necessary for distributed processing.\",\"WARNING\",\"do not assign state in prepare_data\",\"fit()\",\"prepare_data()\",\"setup()\",\"########### NOTE Lightning tries to add the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself.\",\"training_step(batch, batch_idx)\",\"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger.\",\"Parameters:\",\"batch – The output of your data iterable, normally a DataLoader.\",\"batch_idx – The index of this batch.\",\"dataloader_idx – The index of the dataloader that produced this batch. (only if multiple dataloaders used)\",\"Returns:\",\"Tensor - The loss tensor\",\"dict - A dictionary which can include any keys, but must include the key 'loss' in the case of automatic optimization.\",\"None - In automatic optimization, this will skip to the next batch (but is not supported for multi-GPU, TPU, or DeepSpeed). For manual optimization, this has no special meaning, as returning the loss is not required.\",\"In this step you’d normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific.\",\"Example:\",\"def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss\",\"To use multiple optimizers, you can switch to ‘manual optimization’ and control their stepping:\",\"def __init__(self): super().__init__() self.automatic_optimization = False # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx): opt1, opt2 = self.optimizers() # do training_step with encoder ... opt1.step() # do training_step with decoder ... opt2.step()\",\"########### NOTE When accumulate_grad_batches > 1, the loss returned here will be automatically normalized by accumulate_grad_batches internally.\",\"val_dataloader()\",\"An iterable or collection of iterables specifying validation samples.\",\"For more information about multiple dataloaders, see this section.\",\"The dataloader you return will not be reloaded unless you set\",\":paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs`\",\"to a positive integer.\",\"It’s recommended that all data downloads and preparation happen in prepare_data().\",\"fit()\",\"validate()\",\"prepare_data()\",\"setup()\",\"########### NOTE Lightning tries to add the correct sampler for distributed and arbitrary hardware There is no need to set it yourself.\",\"########### NOTE If you don’t need a validation dataset and a validation_step(), you don’t need to implement this method.\",\"validation_step(batch, batch_idx)\",\"Operates on a single batch of data from the validation set. In this step you’d might generate examples or calculate anything of interest like accuracy.\",\"Parameters:\",\"batch – The output of your data iterable, normally a DataLoader.\",\"batch_idx – The index of this batch.\",\"dataloader_idx – The index of the dataloader that produced this batch. (only if multiple dataloaders used)\",\"Returns:\",\"Tensor - The loss tensor\",\"dict - A dictionary. Can include any keys, but must include the key 'loss'.\",\"None - Skip to the next batch.\",\"# if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx=0): ...\",\"Examples:\",\"# CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc})\",\"If you pass in multiple val dataloaders, validation_step() will have an additional argument. We recommend setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.\",\"# CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx=0): # dataloader_idx tells you which dataset this is. x, y = batch # implement your own out = self(x) if dataloader_idx == 0: loss = self.loss0(out, y) else: loss = self.loss1(out, y) # calculate acc labels_hat = torch.argmax(out, dim=1) acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs separately for each dataloader self.log_dict({f\\\"val_loss_{dataloader_idx}\\\": loss, f\\\"val_acc_{dataloader_idx}\\\": acc})\",\"########### NOTE If you don’t need to validate you don’t need to implement this method.\",\"########### NOTE When the validation_step() is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.\"]},\"2356\":{\"h\":\"espnet2.train.preprocessor.MutliTokenizerCommonPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.MutliTokenizerCommonPreprocessor(train: bool, token_type: List[str] = [None], token_list: List[Path | str | Iterable[str]] = [None], bpemodel: List[Path | str | Iterable[str]] = [None], text_cleaner: Collection[str] | None = None, g2p_type: List[str] | str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, speech_volume_normalize: float | None = None, speech_name: str = 'speech', text_name: List[str] = ['text'], tokenizer_encode_conf: List[Dict] = [{}, {}], fs: int = 0, data_aug_effects: List | None = None, data_aug_num: List[int] = [1, 1], data_aug_prob: float = 0.0, whisper_language: List[str] | None = None, whisper_task: str | None = None)\",\"Bases: CommonPreprocessor\"]},\"2357\":{\"h\":\"espnet2.train.preprocessor.Qwen2AudioPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.Qwen2AudioPreprocessor(sampling_rate: int = 16000)\",\"Bases: AbsPreprocessor\",\"Preprocessor specifically for Qwen2Audio models\",\"Initialize the Qwen2AudioPreprocessor.\",\"This method sets up the tokenizer for Qwen2Audio models and defines the default sampling rate for audio processing.\"]},\"2358\":{\"h\":\"espnet2.train.reporter.ReportedValue\",\"t\":[\"source\",\"class espnet2.train.reporter.ReportedValue\",\"Bases: object\"]},\"2359\":{\"h\":\"espnet2.train.reporter.Reporter\",\"t\":[\"source\",\"class espnet2.train.reporter.Reporter(epoch: int = 0)\",\"Bases: object\",\"Reporter class.\",\"##\",\"Example s\",\">>> reporter = Reporter() >>> with reporter.observe('train') as sub_reporter: ... for batch in iterator: ... stats = dict(loss=0.2) ... sub_reporter.register(stats)\",\"check_early_stopping(patience: int, key1: str, key2: str, mode: str, epoch: int | None = None, logger=None) → bool\",\"finish_epoch(sub_reporter: SubReporter) → None\",\"get_all_keys(epoch: int | None = None) → Tuple[Tuple[str, str], ...]\",\"get_best_epoch(key: str, key2: str, mode: str, nbest: int = 0) → int\",\"get_epoch() → int\",\"get_keys(epoch: int | None = None) → Tuple[str, ...]\",\"Returns keys1 e.g. train,eval.\",\"get_keys2(key: str, epoch: int | None = None) → Tuple[str, ...]\",\"Returns keys2 e.g. loss,acc.\",\"get_value(key: str, key2: str, epoch: int | None = None)\",\"has(key: str, key2: str, epoch: int | None = None) → bool\",\"load_state_dict(state_dict: dict)\",\"log_message(epoch: int | None = None) → str\",\"matplotlib_plot(output_dir: str | Path)\",\"Plot stats using Matplotlib and save images.\",\"observe(key: str, epoch: int = None) → ContextManager[SubReporter]\",\"set_epoch(epoch: int) → None\",\"sort_epochs(key: str, key2: str, mode: str) → List[int]\",\"sort_epochs_and_values(key: str, key2: str, mode: str) → List[Tuple[int, float]]\",\"Return the epoch which resulted the best value.\",\"Example\",\">>> val = reporter.sort_epochs_and_values('eval', 'loss', 'min') >>> e_1best, v_1best = val[0] >>> e_2best, v_2best = val[1]\",\"sort_values(key: str, key2: str, mode: str) → List[float]\",\"start_epoch(key: str, epoch: int | None = None) → SubReporter\",\"state_dict()\",\"tensorboard_add_scalar(summary_writer, epoch: int | None = None, key1: str | None = None)\",\"wandb_log(epoch: int | None = None)\"]},\"2360\":{\"h\":\"espnet2.train.preprocessor.S2TCTCPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.S2TCTCPreprocessor(train: bool, token_type: str | None = None, token_list: Path | str | Iterable[str] | None = None, bpemodel: Path | str | Iterable[str] | None = None, text_cleaner: Collection[str] | None = None, g2p_type: str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, speech_volume_normalize: float | None = None, speech_name: str = 'speech', text_name: str = 'text', text_prev_name: str = 'text_prev', text_ctc_name: str = 'text_ctc', fs: int = 16000, na_symbol: str = '<na>', speech_length: float = 30, speech_init_silence: float = 1.0, text_prev_apply_prob: float = 0.5, lang_apply_prob: float = 0.5, nolang_symbol: str = '<nolang>')\",\"Bases: CommonPreprocessor\",\"Preprocessor for OWSM-CTC.\"]},\"2361\":{\"h\":\"espnet2.train.preprocessor.S2TPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.S2TPreprocessor(train: bool, token_type: str | None = None, token_list: Path | str | Iterable[str] | None = None, bpemodel: Path | str | Iterable[str] | None = None, text_cleaner: Collection[str] | None = None, g2p_type: str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, speech_volume_normalize: float | None = None, speech_name: str = 'speech', text_name: str = 'text', text_prev_name: str = 'text_prev', text_ctc_name: str = 'text_ctc', fs: int = 16000, na_symbol: str = '<na>', speech_length: float = 30, speech_resolution: float = 0.02, speech_init_silence: float = 1.0, text_prev_apply_prob: float = 0.5, time_apply_prob: float = 0.5, notime_symbol: str = '<notimestamps>', first_time_symbol: str = '<0.00>', last_time_symbol: str = '<30.00>')\",\"Bases: CommonPreprocessor\"]},\"2362\":{\"h\":\"espnet2.train.preprocessor.SLUPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.SLUPreprocessor(train: bool, token_type: str | None = None, token_list: Path | str | Iterable[str] | None = None, transcript_token_list: Path | str | Iterable[str] | None = None, bpemodel: Path | str | Iterable[str] | None = None, text_cleaner: Collection[str] | None = None, g2p_type: str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, speech_volume_normalize: float | None = None, speech_name: str = 'speech', text_name: str = 'text', fs: int = 0, data_aug_effects: List | None = None, data_aug_num: List[int] = [1, 1], data_aug_prob: float = 0.0)\",\"Bases: CommonPreprocessor\"]},\"2363\":{\"h\":\"espnet2.train.preprocessor.SVSPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.SVSPreprocessor(train: bool, token_type: str | None = None, token_list: Path | str | Iterable[str] | None = None, bpemodel: Path | str | Iterable[str] | None = None, text_cleaner: Collection[str] | None = None, g2p_type: str | None = None, unk_symbol: str = '<unk>', space_symbol: str = '<space>', non_linguistic_symbols: Path | str | Iterable[str] | None = None, delimiter: str | None = None, singing_volume_normalize: float | None = None, singing_name: str = 'singing', text_name: str = 'text', label_name: str = 'label', midi_name: str = 'score', fs: int32 = 0, hop_length: int32 = 256, phn_seg: dict = {1: [1], 2: [0.25, 1], 3: [0.1, 0.5, 1], 4: [0.05, 0.1, 0.5, 1]}, discrete_token_name: str = 'discrete_token', pos_sample_name: str = 'pos_idx', neg_sample_name: str = 'neg_idx')\",\"Bases: AbsPreprocessor\",\"Preprocessor for Sing Voice Sythesis (SVS) task.\"]},\"2364\":{\"h\":\"espnet2.train.preprocessor.SpkPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.SpkPreprocessor(train: bool, target_duration: float, spk2utt: str | None = None, sample_rate: int = 16000, num_eval: int = 10, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_info: List[Tuple[float, str, Tuple[int, int], Tuple[float, float]]] | None = None, noise_apply_prob: float = 1.0, short_noise_thres: float = 0.5)\",\"Bases: CommonPreprocessor\",\"Preprocessor for Speaker tasks.\",\"Parameters:\",\"train (bool) – Whether to use in training mode.\",\"spk2utt (str) – Path to the spk2utt file.\",\"target_duration (float) – Target duration in seconds.\",\"sample_rate (int) – Sampling rate.\",\"num_eval (int) – Number of utterances to be used for evaluation.\",\"rir_scp (str) – Path to the RIR scp file.\",\"rir_apply_prob (float) – Probability of applying RIR.\",\"noise_info (List *[*Tuple *[*float,str,Tuple *[*int,int],Tuple *[*float,float]]]) –\",\"List of tuples of noise information. Each tuple represents a noise type. Each tuple consists of (prob, noise_scp, num_to_mix, db_range).\",\"prob (float) is the probability of applying the noise type.\",\"noise_scp (str) is the path to the noise scp file.\",\"num_to_mix (Tuple[int, int]) is the range of the number of noises : to be mixed.\",\"db_range (Tuple[float, float]) is the range of noise levels in dB.\",\"noise_apply_prob (float) – Probability of applying noise.\",\"short_noise_thres (float) – Threshold of short noise.\"]},\"2365\":{\"h\":\"espnet2.train.spk_trainer.SpkTrainer\",\"t\":[\"source\",\"class espnet2.train.spk_trainer.SpkTrainer\",\"Bases: Trainer\",\"Trainer designed for speaker recognition.\",\"Training will be done as closed set classification. Validation will be open set EER calculation.\",\"classmethod extract_embed(model: Module, iterator: Iterable[Dict[str, Tensor]], reporter: SubReporter, options: TrainerOptions, distributed_option: DistributedOption, output_dir: str, custom_bs: int, average: bool = False) → None\",\"classmethod validate_one_epoch(model: Module, iterator: Iterable[Dict[str, Tensor]], reporter: SubReporter, options: TrainerOptions, distributed_option: DistributedOption) → None\"]},\"2366\":{\"h\":\"espnet2.train.iterable_dataset.SplicedIterableESPnetDataset\",\"t\":[\"source\",\"class espnet2.train.iterable_dataset.SplicedIterableESPnetDataset(path_name_type_list: Collection[Tuple[str, str, str]], preprocess: Callable[[str, Dict[str, ndarray]], Dict[str, ndarray]] | None = None, key_file: str | None = None, **kwargs)\",\"Bases: IterableDataset\",\"A data iterator that is spliced from multiple IterableESPnetDataset\",\"has_name(name) → bool\",\"names() → Tuple[str, ...]\",\"post_process(data: Dict, iterator: IterableESPnetDataset)\"]},\"2367\":{\"h\":\"espnet2.train.reporter.SubReporter\",\"t\":[\"source\",\"class espnet2.train.reporter.SubReporter(key: str, epoch: int, total_count: int)\",\"Bases: object\",\"This class is used in Reporter.\",\"See the docstring of Reporter for the usage.\",\"finished() → None\",\"get_epoch() → int\",\"get_total_count() → int\",\"Returns the number of iterations over all epochs.\",\"log_message(start: int | None = None, end: int | None = None) → str\",\"measure_iter_time(iterable, name: str)\",\"measure_time(name: str)\",\"next()\",\"Close up this step and reset state for the next step\",\"register(stats: Dict[str, float | int | complex | Tensor | ndarray | Dict[str, float | int | complex | Tensor | ndarray] | None], weight: float | int | complex | Tensor | ndarray | None = None) → None\",\"tensorboard_add_scalar(summary_writer, start: int | None = None)\",\"wandb_log(start: int | None = None)\"]},\"2368\":{\"h\":\"espnet2.train.preprocessor.TSEPreprocessor\",\"t\":[\"source\",\"class espnet2.train.preprocessor.TSEPreprocessor(train: bool, train_spk2enroll: str | None = None, enroll_segment: int | None = None, load_spk_embedding: bool = False, load_all_speakers: bool = False, rir_scp: str | None = None, rir_apply_prob: float = 1.0, noise_scp: str | None = None, noise_apply_prob: float = 1.0, noise_db_range: str = '3_10', short_noise_thres: float = 0.5, speech_volume_normalize: float | None = None, speech_name: str = 'speech_mix', speech_ref_name_prefix: str = 'speech_ref', noise_ref_name_prefix: str = 'noise_ref', dereverb_ref_name_prefix: str = 'dereverb_ref', use_reverberant_ref: bool = False, num_spk: int = 1, num_noise_type: int = 1, sample_rate: int = 8000, force_single_channel: bool = False, channel_reordering: bool = False, categories: List | None = None, data_aug_effects: List | None = None, data_aug_num: List[int] = [1, 1], data_aug_prob: float = 0.0, speech_segment: int | None = None, avoid_allzero_segment: bool = True, flexible_numspk: bool = False)\",\"Bases: EnhPreprocessor\",\"Preprocessor for Target Speaker Extraction.\"]},\"2369\":{\"h\":\"espnet2.train.trainer.Trainer\",\"t\":[\"source\",\"class espnet2.train.trainer.Trainer\",\"Bases: object\",\"Trainer having a optimizer.\",\"If you’d like to use multiple optimizers, then inherit this class and override the methods if necessary - at least “train_one_epoch()”\",\">>> class TwoOptimizerTrainer(Trainer): ... @classmethod ... def add_arguments(cls, parser): ... ... ... ... @classmethod ... def train_one_epoch(cls, model, optimizers, ...): ... loss1 = model.model1(...) ... loss1.backward() ... optimizers[0].step() ... ... loss2 = model.model2(...) ... loss2.backward() ... optimizers[1].step()\",\"classmethod add_arguments(parser: ArgumentParser)\",\"Reserved for future development of another Trainer\",\"classmethod build_options(args: Namespace) → TrainerOptions\",\"Build options consumed by train(), eval(), and plot_attention()\",\"classmethod plot_attention(model: Module, output_dir: Path | None, summary_writer, iterator: Iterable[Tuple[List[str], Dict[str, Tensor]]], reporter: SubReporter, options: TrainerOptions) → None\",\"static resume(checkpoint: str | Path, model: Module, reporter: Reporter, optimizers: Sequence[Optimizer], schedulers: Sequence[AbsScheduler | None], scaler: GradScaler | None, ngpu: int = 0, strict: bool = True)\",\"classmethod run(model: AbsESPnetModel, optimizers: Sequence[Optimizer], schedulers: Sequence[AbsScheduler | None], train_iter_factory: AbsIterFactory, valid_iter_factory: AbsIterFactory, plot_attention_iter_factory: AbsIterFactory | None, trainer_options, distributed_option: DistributedOption) → None\",\"Perform training. This method performs the main process of training.\",\"classmethod train_one_epoch(model: Module, iterator: Iterable[Tuple[List[str], Dict[str, Tensor]]], optimizers: Sequence[Optimizer], schedulers: Sequence[AbsScheduler | None], scaler: GradScaler | None, reporter: SubReporter, summary_writer, options: TrainerOptions, distributed_option: DistributedOption) → bool\",\"classmethod validate_one_epoch(model: Module, iterator: Iterable[Dict[str, Tensor]], reporter: SubReporter, options: TrainerOptions, distributed_option: DistributedOption) → None\"]},\"2370\":{\"h\":\"espnet2.train.trainer.TrainerOptions\",\"t\":[\"source\",\"class espnet2.train.trainer.TrainerOptions(ngpu: int, resume: bool, use_amp: bool, train_dtype: str, grad_noise: bool, accum_grad: int, grad_clip: float, grad_clip_type: float, log_interval: int | None, no_forward_run: bool, use_matplotlib: bool, use_tensorboard: bool, use_wandb: bool, adapter: str, use_adapter: bool, save_strategy: str, output_dir: pathlib.Path | str, max_epoch: int, seed: int, sharded_ddp: bool, patience: int | None, keep_nbest_models: int | List[int], nbest_averaging_interval: int, early_stopping_criterion: Sequence[str], best_model_criterion: Sequence[Sequence[str]], val_scheduler_criterion: Sequence[str], unused_parameters: bool, wandb_model_log_interval: int, create_graph_in_tensorboard: bool, gradient_as_bucket_view: bool, ddp_comm_hook: str | None)\",\"Bases: object\",\"accum_grad : int\",\"adapter : str\",\"best_model_criterion : Sequence[Sequence[str]]\",\"create_graph_in_tensorboard : bool\",\"ddp_comm_hook : str | None\",\"early_stopping_criterion : Sequence[str]\",\"grad_clip : float\",\"grad_clip_type : float\",\"grad_noise : bool\",\"gradient_as_bucket_view : bool\",\"keep_nbest_models : int | List[int]\",\"log_interval : int | None\",\"max_epoch : int\",\"nbest_averaging_interval : int\",\"ngpu : int\",\"no_forward_run : bool\",\"output_dir : Path | str\",\"patience : int | None\",\"resume : bool\",\"save_strategy : str\",\"seed : int\",\"sharded_ddp : bool\",\"train_dtype : str\",\"unused_parameters : bool\",\"use_adapter : bool\",\"use_amp : bool\",\"use_matplotlib : bool\",\"use_tensorboard : bool\",\"use_wandb : bool\",\"val_scheduler_criterion : Sequence[str]\",\"wandb_model_log_interval : int\"]},\"2371\":{\"h\":\"espnet2.train.uasr_trainer.UASRTrainer\",\"t\":[\"source\",\"class espnet2.train.uasr_trainer.UASRTrainer\",\"Bases: Trainer\",\"Trainer for GAN-based UASR training.\",\"classmethod add_arguments(parser: ArgumentParser)\",\"Add additional arguments for GAN-trainer.\",\"classmethod build_options(args: Namespace) → TrainerOptions\",\"Build options consumed by train(), eval(), and plot_attention().\",\"classmethod train_one_epoch(model: Module, iterator: Iterable[Tuple[List[str], Dict[str, Tensor]]], optimizers: Sequence[Optimizer], schedulers: Sequence[AbsScheduler | None], scaler: GradScaler | None, reporter: SubReporter, summary_writer, options: UASRTrainerOptions, distributed_option: DistributedOption) → bool\",\"Train one epoch for UASR.\",\"classmethod validate_one_epoch(model: Module, iterator: Iterable[Dict[str, Tensor]], reporter: SubReporter, options: UASRTrainerOptions, distributed_option: DistributedOption) → None\",\"Validate one epoch.\"]},\"2372\":{\"h\":\"espnet2.train.uasr_trainer.UASRTrainerOptions\",\"t\":[\"source\",\"class espnet2.train.uasr_trainer.UASRTrainerOptions(ngpu: int, resume: bool, use_amp: bool, train_dtype: str, grad_noise: bool, accum_grad: int, grad_clip: float, grad_clip_type: float, log_interval: int | None, no_forward_run: bool, use_matplotlib: bool, use_tensorboard: bool, use_wandb: bool, adapter: str, use_adapter: bool, save_strategy: str, output_dir: Path | str, max_epoch: int, seed: int, sharded_ddp: bool, patience: int | None, keep_nbest_models: int | List[int], nbest_averaging_interval: int, early_stopping_criterion: Sequence[str], best_model_criterion: Sequence[Sequence[str]], val_scheduler_criterion: Sequence[str], unused_parameters: bool, wandb_model_log_interval: int, create_graph_in_tensorboard: bool, gradient_as_bucket_view: bool, ddp_comm_hook: str | None, generator_first: bool, max_num_warning: int)\",\"Bases: TrainerOptions\",\"Trainer option dataclass for UASRTrainer.\",\"generator_first : bool\",\"max_num_warning : int\"]},\"2373\":{\"h\":\"espnet2.train.reporter.WeightedAverage\",\"t\":[\"source\",\"class espnet2.train.reporter.WeightedAverage(value: Tuple[float | int | complex | torch.Tensor | numpy.ndarray, float | int | complex | torch.Tensor | numpy.ndarray], weight: float | int | complex | torch.Tensor | numpy.ndarray)\",\"Bases: ReportedValue\",\"value : Tuple[float | int | complex | Tensor | ndarray, float | int | complex | Tensor | ndarray]\",\"weight : float | int | complex | Tensor | ndarray\"]},\"2374\":{\"h\":\"espnet2.train.reporter.aggregate\",\"t\":[\"source\",\"espnet2.train.reporter.aggregate(values: Sequence[ReportedValue]) → float | int | complex | Tensor | ndarray\"]},\"2375\":{\"h\":\"espnet2.train.preprocessor.any_allzero\",\"t\":[\"source\",\"espnet2.train.preprocessor.any_allzero(signal)\"]},\"2376\":{\"h\":\"espnet2.train.collate_fn.common_collate_fn\",\"t\":[\"source\",\"espnet2.train.collate_fn.common_collate_fn(data: Collection[Tuple[str, Dict[str, ndarray]]], float_pad_value: float | int = 0.0, int_pad_value: int = -32768, not_sequence: Collection[str] = ()) → Tuple[List[str], Dict[str, Tensor]]\",\"Concatenate ndarray-list to an array and convert to torch.Tensor.\"]},\"2377\":{\"h\":\"Examples\",\"t\":[\">>> from espnet2.samplers.constant_batch_sampler import ConstantBatchSampler, >>> import espnet2.tasks.abs_task >>> from espnet2.train.dataset import ESPnetDataset >>> sampler = ConstantBatchSampler(...) >>> dataset = ESPnetDataset(...) >>> keys = next(iter(sampler) >>> batch = [dataset[key] for key in keys] >>> batch = common_collate_fn(batch) >>> model(**batch)\",\"Note that the dict-keys of batch are propagated from that of the dataset as they are.\"]},\"2378\":{\"h\":\"espnet2.train.preprocessor.detect_non_silence\",\"t\":[\"source\",\"espnet2.train.preprocessor.detect_non_silence(x: ndarray, threshold: float = 0.01, frame_length: int = 1024, frame_shift: int = 512, window: str = 'boxcar') → ndarray\",\"Power based voice activity detection.\",\"Parameters:x – (Channel, Time)\",\">>> x = np.random.randn(1000) >>> detect = detect_non_silence(x) >>> assert x.shape == detect.shape >>> assert detect.dtype == np.bool\"]},\"2379\":{\"h\":\"espnet2.train.preprocessor.framing\",\"t\":[\"source\",\"espnet2.train.preprocessor.framing(x, frame_length: int = 512, frame_shift: int = 256, centered: bool = True, padded: bool = True)\"]},\"2380\":{\"h\":\"espnet2.train.distributed_utils.free_port\",\"t\":[\"source\",\"espnet2.train.distributed_utils.free_port()\",\"Find free port using bind().\",\"There are some interval between finding this port and using it and the other process might catch the port by that time. Thus it is not guaranteed that the port is really empty.\"]},\"2381\":{\"h\":\"espnet2.train.distributed_utils.get_local_rank\",\"t\":[\"source\",\"espnet2.train.distributed_utils.get_local_rank(prior=None, launcher: str | None = None) → int | None\"]},\"2382\":{\"h\":\"espnet2.train.distributed_utils.get_master_addr\",\"t\":[\"source\",\"espnet2.train.distributed_utils.get_master_addr(prior=None, launcher: str | None = None) → str | None\"]},\"2383\":{\"h\":\"espnet2.train.distributed_utils.get_master_port\",\"t\":[\"source\",\"espnet2.train.distributed_utils.get_master_port(prior=None) → int | None\"]},\"2384\":{\"h\":\"espnet2.train.distributed_utils.get_node_rank\",\"t\":[\"source\",\"espnet2.train.distributed_utils.get_node_rank(prior=None, launcher: str | None = None) → int | None\",\"Get Node Rank.\",\"Use for “multiprocessing distributed” mode. The initial RANK equals to the Node id in this case and the real Rank is set as (nGPU * NodeID) + LOCAL_RANK in torch.distributed.\"]},\"2385\":{\"h\":\"espnet2.train.distributed_utils.get_num_nodes\",\"t\":[\"source\",\"espnet2.train.distributed_utils.get_num_nodes(prior=None, launcher: str | None = None) → int | None\",\"Get the number of nodes.\",\"Use for “multiprocessing distributed” mode. RANK equals to the Node id in this case and the real Rank is set as (nGPU * NodeID) + LOCAL_RANK in torch.distributed.\"]},\"2386\":{\"h\":\"espnet2.train.distributed_utils.get_rank\",\"t\":[\"source\",\"espnet2.train.distributed_utils.get_rank(prior=None, launcher: str | None = None) → int | None\"]},\"2387\":{\"h\":\"espnet2.train.distributed_utils.get_world_size\",\"t\":[\"source\",\"espnet2.train.distributed_utils.get_world_size(prior=None, launcher: str | None = None) → int\"]},\"2388\":{\"h\":\"espnet2.train.distributed_utils.is_in_slurm_job\",\"t\":[\"source\",\"espnet2.train.distributed_utils.is_in_slurm_job() → bool\"]},\"2389\":{\"h\":\"espnet2.train.distributed_utils.is_in_slurm_step\",\"t\":[\"source\",\"espnet2.train.distributed_utils.is_in_slurm_step() → bool\"]},\"2390\":{\"h\":\"espnet2.train.dataset.kaldi_loader\",\"t\":[\"source\",\"espnet2.train.dataset.kaldi_loader(path, float_dtype=None, max_cache_fd: int = 0, allow_multi_rates=False)\"]},\"2391\":{\"h\":\"espnet2.train.dataset.label_loader\",\"t\":[\"source\",\"espnet2.train.dataset.label_loader(path)\"]},\"2392\":{\"h\":\"espnet2.train.iterable_dataset.load_kaldi\",\"t\":[\"source\",\"espnet2.train.iterable_dataset.load_kaldi(input)\"]},\"2393\":{\"h\":\"espnet2.train.dataset.multi_columns_sound_loader\",\"t\":[\"source\",\"espnet2.train.dataset.multi_columns_sound_loader(path, float_dtype=None, allow_multi_rates=False)\"]},\"2394\":{\"h\":\"espnet2.train.dataset.rand_int_loader\",\"t\":[\"source\",\"espnet2.train.dataset.rand_int_loader(filepath, loader_type)\"]},\"2395\":{\"h\":\"espnet2.train.distributed_utils.resolve_distributed_mode\",\"t\":[\"source\",\"espnet2.train.distributed_utils.resolve_distributed_mode(args)\"]},\"2396\":{\"h\":\"espnet2.train.dataset.score_loader\",\"t\":[\"source\",\"espnet2.train.dataset.score_loader(path)\"]},\"2397\":{\"h\":\"espnet2.train.dataset.sound_loader\",\"t\":[\"source\",\"espnet2.train.dataset.sound_loader(path, float_dtype=None, multi_columns=False, allow_multi_rates=False)\"]},\"2398\":{\"h\":\"espnet2.train.reporter.to_reported_value\",\"t\":[\"source\",\"espnet2.train.reporter.to_reported_value(v: float | int | complex | Tensor | ndarray, weight: float | int | complex | Tensor | ndarray | None = None) → ReportedValue\"]},\"2399\":{\"h\":\"espnet2.train.dataset.variable_columns_sound_loader\",\"t\":[\"source\",\"espnet2.train.dataset.variable_columns_sound_loader(path, float_dtype=None, allow_multi_rates=False)\"]},\"2400\":{\"h\":\"espnet2.train.reporter.wandb_get_prefix\",\"t\":[\"source\",\"espnet2.train.reporter.wandb_get_prefix(key: str)\"]},\"2401\":{\"h\":\"espnet2.tts.feats_extract.abs_feats_extract.AbsFeatsExtract\",\"t\":[\"source\",\"class espnet2.tts.feats_extract.abs_feats_extract.AbsFeatsExtract(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2402\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract get_parameters() → Dict[str, Any]\",\"abstract output_size() → int\"]},\"2403\":{\"h\":\"espnet2.tts.abs_tts.AbsTTS\",\"t\":[\"source\",\"class espnet2.tts.abs_tts.AbsTTS(*args, **kwargs)\",\"Bases: Module, ABC\",\"TTS abstract class.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate outputs and return the loss tensor.\",\"abstract inference(text: Tensor, **kwargs) → Dict[str, Tensor]\",\"Return predicted output as a dict.\",\"property require_raw_speech\",\"Return whether or not raw_speech is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\"]},\"2404\":{\"h\":\"espnet2.tts.feats_extract.dio.Dio\",\"t\":[\"source\",\"class espnet2.tts.feats_extract.dio.Dio(fs: int | str = 22050, n_fft: int = 1024, hop_length: int = 256, f0min: int = 80, f0max: int = 400, use_token_averaged_f0: bool = True, use_continuous_f0: bool = True, use_log_f0: bool = True, reduction_factor: int_or_none | None = None)\",\"Bases: AbsFeatsExtract\",\"F0 estimation with dio + stonemask algorithm.\",\"This is f0 extractor based on dio + stonmask algorithm introduced in WORLD: a vocoder-based high-quality speech synthesis system for real-time applications.\"]},\"2405\":{\"h\":\"NOTE\",\"t\":[\"This module is based on NumPy implementation. Therefore, the computational graph is not connected.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None, feats_lengths: Tensor | None = None, durations: Tensor | None = None, durations_lengths: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2406\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters() → Dict[str, Any]\",\"output_size() → int\"]},\"2407\":{\"h\":\"espnet2.tts.utils.duration_calculator.DurationCalculator\",\"t\":[\"source\",\"class espnet2.tts.utils.duration_calculator.DurationCalculator(*args, **kwargs)\",\"Bases: Module\",\"Duration calculator module.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(att_ws: Tensor) → Tuple[Tensor, Tensor]\",\"Convert attention weight to durations.\",\"Parameters:att_ws (Tesnor) – Attention weight tensor (T_feats, T_text) or (#layers, #heads, T_feats, T_text).\",\"Returns: Duration of each input (T_text,). Tensor: Focus rate value.\",\"Return type: LongTensor\"]},\"2408\":{\"h\":\"espnet2.tts.espnet_model.ESPnetTTSModel\",\"t\":[\"source\",\"class espnet2.tts.espnet_model.ESPnetTTSModel(feats_extract: AbsFeatsExtract | None, pitch_extract: AbsFeatsExtract | None, energy_extract: AbsFeatsExtract | None, normalize: InversibleInterface | None, pitch_normalize: InversibleInterface | None, energy_normalize: InversibleInterface | None, tts: AbsTTS)\",\"Bases: AbsESPnetModel\",\"ESPnet model for text-to-speech task.\",\"Initialize ESPnetTTSModel module.\",\"collect_feats(text: Tensor, text_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, durations: Tensor | None = None, durations_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs) → Dict[str, Tensor]\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"durations (Optional *[*Tensor) – Duration tensor.\",\"durations_lengths (Optional *[*Tensor) – Duration length tensor (B,).\",\"pitch (Optional *[*Tensor) – Pitch tensor.\",\"pitch_lengths (Optional *[*Tensor) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"Returns: Dict of features.\",\"Return type: Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, durations: Tensor | None = None, durations_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Caclualte outputs and return the loss tensor.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"duration (Optional *[*Tensor]) – Duration tensor.\",\"duration_lengths (Optional *[*Tensor]) – Duration length tensor (B,).\",\"pitch (Optional *[*Tensor]) – Pitch tensor.\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor]) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor]) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"kwargs – “utt_id” is among the input.\",\"Returns: Loss scalar tensor. Dict[str, float]: Statistics to be monitored. Tensor: Weight tensor to summarize losses.\",\"Return type: Tensor\",\"inference(text: Tensor, speech: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, durations: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, **decode_config) → Dict[str, Tensor]\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (T_text).\",\"speech (Tensor) – Speech waveform tensor (T_wav).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (D,).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (1,).\",\"lids (Optional *[*Tensor]) – Language ID tensor (1,).\",\"durations (Optional *[*Tensor) – Duration tensor.\",\"pitch (Optional *[*Tensor) – Pitch tensor.\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"Returns: Dict of outputs.\",\"Return type: Dict[str, Tensor]\"]},\"2409\":{\"h\":\"espnet2.tts.feats_extract.energy.Energy\",\"t\":[\"source\",\"class espnet2.tts.feats_extract.energy.Energy(fs: int | str = 22050, n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True, use_token_averaged_energy: bool = True, reduction_factor: int | None = None)\",\"Bases: AbsFeatsExtract\",\"Energy extractor.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None, feats_lengths: Tensor | None = None, durations: Tensor | None = None, durations_lengths: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2410\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters() → Dict[str, Any]\",\"output_size() → int\"]},\"2411\":{\"h\":\"espnet2.tts.fastspeech.fastspeech.FastSpeech\",\"t\":[\"source\",\"class espnet2.tts.fastspeech.fastspeech.FastSpeech(idim: int, odim: int, adim: int = 384, aheads: int = 4, elayers: int = 6, eunits: int = 1536, dlayers: int = 6, dunits: int = 1536, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, postnet_dropout_rate: float = 0.5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'transformer', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, zero_triu: bool = False, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False)\",\"Bases: AbsTTS\",\"FastSpeech module for end-to-end text-to-speech.\",\"This is a module of FastSpeech, feed-forward Transformer with duration predictor described in FastSpeech: Fast, Robust and Controllable Text to Speech, which does not require any auto-regressive processing during inference, resulting in fast decoding compared with auto-regressive Transformer.\",\"Initialize FastSpeech module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Kernel size of postnet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"conformer_rel_pos_type (str) – Relative pos encoding type in conformer.\",\"conformer_pos_enc_layer_type (str) – Pos encoding layer type in conformer.\",\"conformer_self_attn_layer_type (str) – Self-attention layer type in conformer\",\"conformer_activation_type (str) – Activation function type in conformer.\",\"use_macaron_style_in_conformer – Whether to use macaron style FFN.\",\"use_cnn_in_conformer – Whether to use CNN in conformer.\",\"conformer_enc_kernel_size – Kernel size of encoder conformer.\",\"conformer_dec_kernel_size – Kernel size of decoder conformer.\",\"zero_triu – Whether to use zero triu in relative self-attention module.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – The number of GST embeddings.\",\"gst_heads (int) – The number of heads in GST multihead attention.\",\"gst_conv_layers (int) – The number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – The number of GRU layers in GST.\",\"gst_gru_units (int) – The number of GRU units in GST.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, durations: Tensor, durations_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"durations (LongTensor) – Batch of padded durations (B, T_text + 1).\",\"durations_lengths (LongTensor) – Batch of duration lengths (B, T_text + 1).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, durations: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, alpha: float = 1.0, use_teacher_forcing: bool = False) → Dict[str, Tensor]\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style (N, idim).\",\"durations (Optional *[*LongTensor]) – Groundtruth of duration (T_text + 1,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"alpha (float) – Alpha to control the speed.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing. If true, groundtruth of duration, pitch and energy will be used.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"duration (Tensor): Duration sequence (T_text + 1,).\",\"Return type: Dict[str, Tensor]\"]},\"2412\":{\"h\":\"espnet2.tts.fastspeech2.fastspeech2.FastSpeech2\",\"t\":[\"source\",\"class espnet2.tts.fastspeech2.fastspeech2.FastSpeech2(idim: int, odim: int, adim: int = 384, aheads: int = 4, elayers: int = 6, eunits: int = 1536, dlayers: int = 6, dunits: int = 1536, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, postnet_dropout_rate: float = 0.5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'transformer', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, energy_predictor_layers: int = 2, energy_predictor_chans: int = 384, energy_predictor_kernel_size: int = 3, energy_predictor_dropout: float = 0.5, energy_embed_kernel_size: int = 9, energy_embed_dropout: float = 0.5, stop_gradient_from_energy_predictor: bool = False, pitch_predictor_layers: int = 2, pitch_predictor_chans: int = 384, pitch_predictor_kernel_size: int = 3, pitch_predictor_dropout: float = 0.5, pitch_embed_kernel_size: int = 9, pitch_embed_dropout: float = 0.5, stop_gradient_from_pitch_predictor: bool = False, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False)\",\"Bases: AbsTTS\",\"FastSpeech2 module.\",\"This is a module of FastSpeech2 described in FastSpeech 2: Fast and High-Quality End-to-End Text to Speech. Instead of quantized pitch and energy, we use token-averaged value introduced in FastPitch: Parallel Text-to-speech with Pitch Prediction.\",\"Initialize FastSpeech2 module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Kernel size of postnet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"conformer_rel_pos_type (str) – Relative pos encoding type in conformer.\",\"conformer_pos_enc_layer_type (str) – Pos encoding layer type in conformer.\",\"conformer_self_attn_layer_type (str) – Self-attention layer type in conformer\",\"conformer_activation_type (str) – Activation function type in conformer.\",\"use_macaron_style_in_conformer – Whether to use macaron style FFN.\",\"use_cnn_in_conformer – Whether to use CNN in conformer.\",\"zero_triu – Whether to use zero triu in relative self-attention module.\",\"conformer_enc_kernel_size – Kernel size of encoder conformer.\",\"conformer_dec_kernel_size – Kernel size of decoder conformer.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"pitch_predictor_layers (int) – Number of pitch predictor layers.\",\"pitch_predictor_chans (int) – Number of pitch predictor channels.\",\"pitch_predictor_kernel_size (int) – Kernel size of pitch predictor.\",\"pitch_predictor_dropout_rate (float) – Dropout rate in pitch predictor.\",\"pitch_embed_kernel_size (float) – Kernel size of pitch embedding.\",\"pitch_embed_dropout_rate (float) – Dropout rate for pitch embedding.\",\"stop_gradient_from_pitch_predictor – Whether to stop gradient from pitch predictor to encoder.\",\"energy_predictor_layers (int) – Number of energy predictor layers.\",\"energy_predictor_chans (int) – Number of energy predictor channels.\",\"energy_predictor_kernel_size (int) – Kernel size of energy predictor.\",\"energy_predictor_dropout_rate (float) – Dropout rate in energy predictor.\",\"energy_embed_kernel_size (float) – Kernel size of energy embedding.\",\"energy_embed_dropout_rate (float) – Dropout rate for energy embedding.\",\"stop_gradient_from_energy_predictor – Whether to stop gradient from energy predictor to encoder.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – The number of GST embeddings.\",\"gst_heads (int) – The number of heads in GST multihead attention.\",\"gst_conv_layers (int) – The number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – The number of GRU layers in GST.\",\"gst_gru_units (int) – The number of GRU units in GST.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, durations: Tensor, durations_lengths: Tensor, pitch: Tensor, pitch_lengths: Tensor, energy: Tensor, energy_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded token ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"durations (LongTensor) – Batch of padded durations (B, T_text + 1).\",\"durations_lengths (LongTensor) – Batch of duration lengths (B, T_text + 1).\",\"pitch (Tensor) – Batch of padded token-averaged pitch (B, T_text + 1, 1).\",\"pitch_lengths (LongTensor) – Batch of pitch lengths (B, T_text + 1).\",\"energy (Tensor) – Batch of padded token-averaged energy (B, T_text + 1, 1).\",\"energy_lengths (LongTensor) – Batch of energy lengths (B, T_text + 1).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, durations: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, alpha: float = 1.0, use_teacher_forcing: bool = False) → Dict[str, Tensor]\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor) – Feature sequence to extract style (N, idim).\",\"durations (Optional *[*Tensor) – Groundtruth of duration (T_text + 1,).\",\"spembs (Optional *[*Tensor) – Speaker embedding vector (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"pitch (Optional *[*Tensor]) – Groundtruth of token-avg pitch (T_text + 1, 1).\",\"energy (Optional *[*Tensor]) – Groundtruth of token-avg energy (T_text + 1, 1).\",\"alpha (float) – Alpha to control the speed.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing. If true, groundtruth of duration, pitch and energy will be used.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"duration (Tensor): Duration sequence (T_text + 1,).\",\"pitch (Tensor): Pitch sequence (T_text + 1,).\",\"energy (Tensor): Energy sequence (T_text + 1,).\",\"Return type: Dict[str, Tensor]\"]},\"2413\":{\"h\":\"espnet2.tts.fastspeech2.loss.FastSpeech2Loss\",\"t\":[\"source\",\"class espnet2.tts.fastspeech2.loss.FastSpeech2Loss(use_masking: bool = True, use_weighted_masking: bool = False)\",\"Bases: Module\",\"Loss function module for FastSpeech2.\",\"Initialize feed-forward Transformer loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"forward(after_outs: Tensor, before_outs: Tensor, d_outs: Tensor, p_outs: Tensor, e_outs: Tensor, ys: Tensor, ds: Tensor, ps: Tensor, es: Tensor, ilens: Tensor, olens: Tensor) → Tuple[Tensor, Tensor, Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, T_feats, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, T_feats, odim).\",\"d_outs (LongTensor) – Batch of outputs of duration predictor (B, T_text).\",\"p_outs (Tensor) – Batch of outputs of pitch predictor (B, T_text, 1).\",\"e_outs (Tensor) – Batch of outputs of energy predictor (B, T_text, 1).\",\"ys (Tensor) – Batch of target features (B, T_feats, odim).\",\"ds (LongTensor) – Batch of durations (B, T_text).\",\"ps (Tensor) – Batch of target token-averaged pitch (B, T_text, 1).\",\"es (Tensor) – Batch of target token-averaged energy (B, T_text, 1).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: L1 loss value. Tensor: Duration predictor loss value. Tensor: Pitch predictor loss value. Tensor: Energy predictor loss value.\",\"Return type: Tensor\"]},\"2414\":{\"h\":\"espnet2.tts.feats_extract.linear_spectrogram.LinearSpectrogram\",\"t\":[\"source\",\"class espnet2.tts.feats_extract.linear_spectrogram.LinearSpectrogram(n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True)\",\"Bases: AbsFeatsExtract\",\"Linear amplitude spectrogram.\",\"Stft -> amplitude-spec\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2415\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters() → Dict[str, Any]\",\"Return the parameters required by Vocoder.\",\"output_size() → int\"]},\"2416\":{\"h\":\"espnet2.tts.feats_extract.log_mel_fbank.LogMelFbank\",\"t\":[\"source\",\"class espnet2.tts.feats_extract.log_mel_fbank.LogMelFbank(fs: int | str = 16000, n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True, n_mels: int = 80, fmin: int | None = 80, fmax: int | None = 7600, htk: bool = False, log_base: float | None = 10.0)\",\"Bases: AbsFeatsExtract\",\"Conventional frontend structure for TTS.\",\"Stft -> amplitude-spec -> Log-Mel-Fbank\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2417\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters() → Dict[str, Any]\",\"Return the parameters required by Vocoder\",\"output_size() → int\"]},\"2418\":{\"h\":\"espnet2.tts.feats_extract.log_spectrogram.LogSpectrogram\",\"t\":[\"source\",\"class espnet2.tts.feats_extract.log_spectrogram.LogSpectrogram(n_fft: int = 1024, win_length: int | None = None, hop_length: int = 256, window: str | None = 'hann', center: bool = True, normalized: bool = False, onesided: bool = True)\",\"Bases: AbsFeatsExtract\",\"Conventional frontend structure for ASR\",\"Stft -> log-amplitude-spec\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2419\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters() → Dict[str, Any]\",\"Return the parameters required by Vocoder\",\"output_size() → int\"]},\"2420\":{\"h\":\"espnet2.tts.prodiff.denoiser.Mish\",\"t\":[\"source\",\"class espnet2.tts.prodiff.denoiser.Mish(*args, **kwargs)\",\"Bases: Module\",\"Mish Activation Function.\",\"Introduced in\",\"`Mish: A Self Regularized Non-Monotonic Activation Function`_\",\".\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:x (torch.Tensor) – Input tensor.\",\"Returns: Output tensor.\",\"Return type: torch.Tensor\"]},\"2421\":{\"h\":\"espnet2.tts.gst.style_encoder.MultiHeadedAttention\",\"t\":[\"source\",\"class espnet2.tts.gst.style_encoder.MultiHeadedAttention(q_dim, k_dim, v_dim, n_head, n_feat, dropout_rate=0.0)\",\"Bases: MultiHeadedAttention\",\"Multi head attention module with different input dimension.\",\"Initialize multi head attention module.\"]},\"2422\":{\"h\":\"espnet2.tts.utils.parallel_wavegan_pretrained_vocoder.ParallelWaveGANPretrainedVocoder\",\"t\":[\"source\",\"class espnet2.tts.utils.parallel_wavegan_pretrained_vocoder.ParallelWaveGANPretrainedVocoder(model_file: Path | str, config_file: Path | str | None = None)\",\"Bases: Module\",\"Wrapper class to load the vocoder trained with parallel_wavegan repo.\",\"Initialize ParallelWaveGANPretrainedVocoder module.\",\"forward(feats: Tensor, f0: Tensor = None) → Tensor\",\"Generate waveform with pretrained vocoder.\",\"Parameters:\",\"feats (Tensor) – Feature tensor (T_feats, #mels).\",\"f0 (Tensor) – f0 information (T_feats)\",\"Returns: Generated waveform tensor (T_wav).\",\"Return type: Tensor\"]},\"2423\":{\"h\":\"espnet2.tts.prodiff.prodiff.ProDiff\",\"t\":[\"source\",\"class espnet2.tts.prodiff.prodiff.ProDiff(idim: int, odim: int, adim: int = 384, aheads: int = 4, elayers: int = 6, eunits: int = 1536, postnet_layers: int = 0, postnet_chans: int = 512, postnet_filts: int = 5, postnet_dropout_rate: float = 0.5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, encoder_concat_after: bool = False, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'diffusion', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, denoiser_layers: int = 20, denoiser_channels: int = 256, diffusion_steps: int = 1000, diffusion_timescale: int = 1, diffusion_beta: float = 40.0, diffusion_scheduler: str = 'vpsde', diffusion_cycle_ln: int = 1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, energy_predictor_layers: int = 2, energy_predictor_chans: int = 384, energy_predictor_kernel_size: int = 3, energy_predictor_dropout: float = 0.5, energy_embed_kernel_size: int = 9, energy_embed_dropout: float = 0.5, stop_gradient_from_energy_predictor: bool = False, pitch_predictor_layers: int = 2, pitch_predictor_chans: int = 384, pitch_predictor_kernel_size: int = 3, pitch_predictor_dropout: float = 0.5, pitch_embed_kernel_size: int = 9, pitch_embed_dropout: float = 0.5, stop_gradient_from_pitch_predictor: bool = False, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False)\",\"Bases: AbsTTS\",\"ProDiff module.\",\"This is a module of ProDiff described in ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech.\",\"Initialize ProDiff module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Kernel size of postnet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"conformer_rel_pos_type (str) – Relative pos encoding type in conformer.\",\"conformer_pos_enc_layer_type (str) – Pos encoding layer type in conformer.\",\"conformer_self_attn_layer_type (str) – Self-attention layer type in conformer\",\"conformer_activation_type (str) – Activation function type in conformer.\",\"use_macaron_style_in_conformer – Whether to use macaron style FFN.\",\"use_cnn_in_conformer – Whether to use CNN in conformer.\",\"zero_triu – Whether to use zero triu in relative self-attention module.\",\"conformer_enc_kernel_size – Kernel size of encoder conformer.\",\"conformer_dec_kernel_size – Kernel size of decoder conformer.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"pitch_predictor_layers (int) – Number of pitch predictor layers.\",\"pitch_predictor_chans (int) – Number of pitch predictor channels.\",\"pitch_predictor_kernel_size (int) – Kernel size of pitch predictor.\",\"pitch_predictor_dropout_rate (float) – Dropout rate in pitch predictor.\",\"pitch_embed_kernel_size (float) – Kernel size of pitch embedding.\",\"pitch_embed_dropout_rate (float) – Dropout rate for pitch embedding.\",\"stop_gradient_from_pitch_predictor – Whether to stop gradient from pitch predictor to encoder.\",\"energy_predictor_layers (int) – Number of energy predictor layers.\",\"energy_predictor_chans (int) – Number of energy predictor channels.\",\"energy_predictor_kernel_size (int) – Kernel size of energy predictor.\",\"energy_predictor_dropout_rate (float) – Dropout rate in energy predictor.\",\"energy_embed_kernel_size (float) – Kernel size of energy embedding.\",\"energy_embed_dropout_rate (float) – Dropout rate for energy embedding.\",\"stop_gradient_from_energy_predictor – Whether to stop gradient from energy predictor to encoder.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – The number of GST embeddings.\",\"gst_heads (int) – The number of heads in GST multihead attention.\",\"gst_conv_layers (int) – The number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – The number of GRU layers in GST.\",\"gst_gru_units (int) – The number of GRU units in GST.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, durations: Tensor, durations_lengths: Tensor, pitch: Tensor, pitch_lengths: Tensor, energy: Tensor, energy_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded token ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"durations (LongTensor) – Batch of padded durations (B, T_text + 1).\",\"durations_lengths (LongTensor) – Batch of duration lengths (B, T_text + 1).\",\"pitch (Tensor) – Batch of padded token-averaged pitch (B, T_text + 1, 1).\",\"pitch_lengths (LongTensor) – Batch of pitch lengths (B, T_text + 1).\",\"energy (Tensor) – Batch of padded token-averaged energy (B, T_text + 1, 1).\",\"energy_lengths (LongTensor) – Batch of energy lengths (B, T_text + 1).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, durations: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, alpha: float = 1.0, use_teacher_forcing: bool = False) → Dict[str, Tensor]\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor) – Feature sequence to extract style (N, idim).\",\"durations (Optional *[*Tensor) – Groundtruth of duration (T_text + 1,).\",\"spembs (Optional *[*Tensor) – Speaker embedding vector (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"pitch (Optional *[*Tensor]) – Groundtruth of token-avg pitch (T_text + 1, 1).\",\"energy (Optional *[*Tensor]) – Groundtruth of token-avg energy (T_text + 1, 1).\",\"alpha (float) – Alpha to control the speed.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing. If true, groundtruth of duration, pitch and energy will be used.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"duration (Tensor): Duration sequence (T_text + 1,).\",\"pitch (Tensor): Pitch sequence (T_text + 1,).\",\"energy (Tensor): Energy sequence (T_text + 1,).\",\"Return type: Dict[str, Tensor]\"]},\"2424\":{\"h\":\"espnet2.tts.prodiff.loss.ProDiffLoss\",\"t\":[\"source\",\"class espnet2.tts.prodiff.loss.ProDiffLoss(use_masking: bool = True, use_weighted_masking: bool = False)\",\"Bases: Module\",\"Loss function module for ProDiffLoss.\",\"Initialize feed-forward Transformer loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"forward(after_outs: Tensor, before_outs: Tensor, d_outs: Tensor, p_outs: Tensor, e_outs: Tensor, ys: Tensor, ds: Tensor, ps: Tensor, es: Tensor, ilens: Tensor, olens: Tensor) → Tuple[Tensor, Tensor, Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, T_feats, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, T_feats, odim).\",\"d_outs (LongTensor) – Batch of outputs of duration predictor (B, T_text).\",\"p_outs (Tensor) – Batch of outputs of pitch predictor (B, T_text, 1).\",\"e_outs (Tensor) – Batch of outputs of energy predictor (B, T_text, 1).\",\"ys (Tensor) – Batch of target features (B, T_feats, odim).\",\"ds (LongTensor) – Batch of durations (B, T_text).\",\"ps (Tensor) – Batch of target token-averaged pitch (B, T_text, 1).\",\"es (Tensor) – Batch of target token-averaged energy (B, T_text, 1).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: L1 loss value. Tensor: Duration predictor loss value. Tensor: Pitch predictor loss value. Tensor: Energy predictor loss value.\",\"Return type: Tensor\"]},\"2425\":{\"h\":\"espnet2.tts.gst.style_encoder.ReferenceEncoder\",\"t\":[\"source\",\"class espnet2.tts.gst.style_encoder.ReferenceEncoder(idim=80, conv_layers: int = 6, conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), conv_kernel_size: int = 3, conv_stride: int = 2, gru_layers: int = 1, gru_units: int = 128)\",\"Bases: Module\",\"Reference encoder module.\",\"This module is reference encoder introduced in Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis.\",\"Parameters:\",\"idim (int,optional) – Dimension of the input mel-spectrogram.\",\"conv_layers (int,optional) – The number of conv layers in the reference encoder.\",\"conv_chans_list – (Sequence[int], optional): List of the number of channels of conv layers in the referece encoder.\",\"conv_kernel_size (int,optional) – Kernel size of conv layers in the reference encoder.\",\"conv_stride (int,optional) – Stride size of conv layers in the reference encoder.\",\"gru_layers (int,optional) – The number of GRU layers in the reference encoder.\",\"gru_units (int,optional) – The number of GRU units in the reference encoder.\",\"Initilize reference encoder module.\",\"forward(speech: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:speech (Tensor) – Batch of padded target features (B, Lmax, idim).\",\"Returns: Reference embedding (B, gru_units)\",\"Return type: Tensor\"]},\"2426\":{\"h\":\"espnet2.tts.prodiff.denoiser.ResidualBlock\",\"t\":[\"source\",\"class espnet2.tts.prodiff.denoiser.ResidualBlock(adim: int, channels: int, dilation: int)\",\"Bases: Module\",\"Residual Block for Diffusion Denoiser.\",\"Initialization.\",\"Parameters:\",\"adim (int) – Size of dimensions.\",\"channels (int) – Number of channels.\",\"dilation (int) – Size of dilations.\",\"forward(x: Tensor, condition: Tensor, step: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"x (torch.Tensor) – Input tensor.\",\"condition (torch.Tensor) – Conditioning tensor.\",\"step (torch.Tensor) – Number of diffusion step.\",\"Returns: Output tensor.\",\"Return type: Union[torch.Tensor, torch.Tensor]\"]},\"2427\":{\"h\":\"espnet2.tts.prodiff.loss.SSimLoss\",\"t\":[\"source\",\"class espnet2.tts.prodiff.loss.SSimLoss(bias: float = 6.0, window_size: int = 11, channels: int = 1, reduction: str = 'none')\",\"Bases: Module\",\"SSimLoss.\",\"This is an implementation of structural similarity (SSIM) loss. This code is modified from https://github.com/Po-Hsun-Su/pytorch-ssim.\",\"Initialization.\",\"Parameters:\",\"bias (float,optional) – value of the bias. Defaults to 6.0.\",\"window_size (int,optional) – Window size. Defaults to 11.\",\"channels (int,optional) – Number of channels. Defaults to 1.\",\"reduction (str,optional) – Type of reduction during the loss calculation. Defaults to “none”.\",\"forward(outputs: Tensor, target: Tensor)\",\"Calculate forward propagation.\",\"Parameters:\",\"outputs (torch.Tensor) – Batch of output sequences generated by the model (batch, time, mels).\",\"target (torch.Tensor) – Batch of sequences with true states (batch, time, mels).\",\"Returns: Loss scalar value.\",\"Return type: Tensor\",\"ssim(tensor1: Tensor, tensor2: Tensor)\",\"Calculate SSIM loss.\",\"Parameters:\",\"tensor1 (torch.Tensor) – Generated output.\",\"tensor2 (torch.Tensor) – Groundtruth output.\",\"Returns: Loss scalar value.\",\"Return type: Tensor\"]},\"2428\":{\"h\":\"espnet2.tts.prodiff.denoiser.SpectogramDenoiser\",\"t\":[\"source\",\"class espnet2.tts.prodiff.denoiser.SpectogramDenoiser(idim: int, adim: int = 256, layers: int = 20, channels: int = 256, cycle_length: int = 1, timesteps: int = 200, timescale: int = 1, max_beta: float = 40.0, scheduler: str = 'vpsde', dropout_rate: float = 0.05)\",\"Bases: Module\",\"Spectogram Denoiser.\",\"Ref: https://arxiv.org/pdf/2207.06389.pdf.\",\"Initialization.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"adim (int,optional) – Dimension of the hidden states. Defaults to 256.\",\"layers (int,optional) – Number of layers. Defaults to 20.\",\"channels (int,optional) – Number of channels of each layer. Defaults to 256.\",\"cycle_length (int,optional) – Cycle length of the diffusion. Defaults to 1.\",\"timesteps (int,optional) – Number of timesteps of the diffusion. Defaults to 200.\",\"timescale (int,optional) – Number of timescale. Defaults to 1.\",\"max_beta (float,optional) – Maximum beta value for schedueler. Defaults to 40.\",\"scheduler (str,optional) – Type of noise scheduler. Defaults to “vpsde”.\",\"dropout_rate (float,optional) – Dropout rate. Defaults to 0.05.\",\"diffusion(xs_ref: Tensor, steps: Tensor, noise: Tensor | None = None) → Tensor\",\"Calculate diffusion process during training.\",\"Parameters:\",\"xs_ref (torch.Tensor) – Input tensor.\",\"steps (torch.Tensor) – Number of step.\",\"noise (Optional *[*torch.Tensor],optional) – Noise tensor. Defaults to None.\",\"Returns: Output tensor.\",\"Return type: torch.Tensor\",\"forward(xs: Tensor, ys: Tensor | None = None, masks: Tensor | None = None, is_inference: bool = False) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (torch.Tensor) – Phoneme-encoded tensor (#batch, time, dims)\",\"ys (Optional *[*torch.Tensor],optional) – Mel-based reference tensor (#batch, time, mels). Defaults to None.\",\"masks (Optional *[*torch.Tensor],optional) – Mask tensor (#batch, time). Defaults to None.\",\"Returns: Output tensor (#batch, time, dims).\",\"Return type: torch.Tensor\",\"forward_denoise(xs_noisy: Tensor, step: Tensor, condition: Tensor) → Tensor\",\"Calculate forward for denoising diffusion.\",\"Parameters:\",\"xs_noisy (torch.Tensor) – Input tensor.\",\"step (torch.Tensor) – Number of step.\",\"condition (torch.Tensor) – Conditioning tensor.\",\"Returns: Denoised tensor.\",\"Return type: torch.Tensor\",\"inference(condition: Tensor) → Tensor\",\"Calculate forward during inference.\",\"Parameters:condition (torch.Tensor) – Conditioning tensor (batch, time, dims).\",\"Returns: Output tensor.\",\"Return type: torch.Tensor\"]},\"2429\":{\"h\":\"espnet2.tts.gst.style_encoder.StyleEncoder\",\"t\":[\"source\",\"class espnet2.tts.gst.style_encoder.StyleEncoder(idim: int = 80, gst_tokens: int = 10, gst_token_dim: int = 256, gst_heads: int = 4, conv_layers: int = 6, conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), conv_kernel_size: int = 3, conv_stride: int = 2, gru_layers: int = 1, gru_units: int = 128)\",\"Bases: Module\",\"Style encoder.\",\"This module is style encoder introduced in Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis.\",\"Parameters:\",\"idim (int,optional) – Dimension of the input mel-spectrogram.\",\"gst_tokens (int,optional) – The number of GST embeddings.\",\"gst_token_dim (int,optional) – Dimension of each GST embedding.\",\"gst_heads (int,optional) – The number of heads in GST multihead attention.\",\"conv_layers (int,optional) – The number of conv layers in the reference encoder.\",\"conv_chans_list – (Sequence[int], optional): List of the number of channels of conv layers in the referece encoder.\",\"conv_kernel_size (int,optional) – Kernel size of conv layers in the reference encoder.\",\"conv_stride (int,optional) – Stride size of conv layers in the reference encoder.\",\"gru_layers (int,optional) – The number of GRU layers in the reference encoder.\",\"gru_units (int,optional) – The number of GRU units in the reference encoder.\",\"Initilize global style encoder module.\",\"forward(speech: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:speech (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"Returns: Style token embeddings (B, token_dim).\",\"Return type: Tensor\"]},\"2430\":{\"h\":\"espnet2.tts.gst.style_encoder.StyleTokenLayer\",\"t\":[\"source\",\"class espnet2.tts.gst.style_encoder.StyleTokenLayer(ref_embed_dim: int = 128, gst_tokens: int = 10, gst_token_dim: int = 256, gst_heads: int = 4, dropout_rate: float = 0.0)\",\"Bases: Module\",\"Style token layer module.\",\"This module is style token layer introduced in Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis.\",\"Parameters:\",\"ref_embed_dim (int,optional) – Dimension of the input reference embedding.\",\"gst_tokens (int,optional) – The number of GST embeddings.\",\"gst_token_dim (int,optional) – Dimension of each GST embedding.\",\"gst_heads (int,optional) – The number of heads in GST multihead attention.\",\"dropout_rate (float,optional) – Dropout rate in multi-head attention.\",\"Initilize style token layer module.\",\"forward(ref_embs: Tensor) → Tensor\",\"Calculate forward propagation.\",\"Parameters:ref_embs (Tensor) – Reference embeddings (B, ref_embed_dim).\",\"Returns: Style token embeddings (B, gst_token_dim).\",\"Return type: Tensor\"]},\"2431\":{\"h\":\"espnet2.tts.tacotron2.tacotron2.Tacotron2\",\"t\":[\"source\",\"class espnet2.tts.tacotron2.tacotron2.Tacotron2(idim: int, odim: int, embed_dim: int = 512, elayers: int = 1, eunits: int = 512, econv_layers: int = 3, econv_chans: int = 512, econv_filts: int = 5, atype: str = 'location', adim: int = 512, aconv_chans: int = 32, aconv_filts: int = 15, cumulate_att_w: bool = True, dlayers: int = 2, dunits: int = 1024, prenet_layers: int = 2, prenet_units: int = 256, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, output_activation: str | None = None, use_batch_norm: bool = True, use_concate: bool = True, use_residual: bool = False, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'concat', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, dropout_rate: float = 0.5, zoneout_rate: float = 0.1, use_masking: bool = True, use_weighted_masking: bool = False, bce_pos_weight: float = 5.0, loss_type: str = 'L1+L2', use_guided_attn_loss: bool = True, guided_attn_loss_sigma: float = 0.4, guided_attn_loss_lambda: float = 1.0)\",\"Bases: AbsTTS\",\"Tacotron2 module for end-to-end text-to-speech.\",\"This is a module of Spectrogram prediction network in Tacotron2 described in Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions, which converts the sequence of characters into the sequence of Mel-filterbanks.\",\"Initialize Tacotron2 module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim – (int) Dimension of the outputs.\",\"embed_dim (int) – Dimension of the token embedding.\",\"elayers (int) – Number of encoder blstm layers.\",\"eunits (int) – Number of encoder blstm units.\",\"econv_layers (int) – Number of encoder conv layers.\",\"econv_filts (int) – Number of encoder conv filter size.\",\"econv_chans (int) – Number of encoder conv filter channels.\",\"dlayers (int) – Number of decoder lstm layers.\",\"dunits (int) – Number of decoder lstm units.\",\"prenet_layers (int) – Number of prenet layers.\",\"prenet_units (int) – Number of prenet units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_filts (int) – Number of postnet filter size.\",\"postnet_chans (int) – Number of postnet filter channels.\",\"output_activation (str) – Name of activation function for outputs.\",\"adim (int) – Number of dimension of mlp in attention.\",\"aconv_chans (int) – Number of attention conv filter channels.\",\"aconv_filts (int) – Number of attention conv filter size.\",\"cumulate_att_w (bool) – Whether to cumulate previous attention weight.\",\"use_batch_norm (bool) – Whether to use batch normalization.\",\"use_concate (bool) – Whether to concat enc outputs w/ dec lstm outputs.\",\"reduction_factor (int) – Reduction factor.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – Number of GST embeddings.\",\"gst_heads (int) – Number of heads in GST multihead attention.\",\"gst_conv_layers (int) – Number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – Number of GRU layers in GST.\",\"gst_gru_units (int) – Number of GRU units in GST.\",\"dropout_rate (float) – Dropout rate.\",\"zoneout_rate (float) – Zoneout rate.\",\"use_masking (bool) – Whether to mask padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"bce_pos_weight (float) – Weight of positive sample of stop token (only for use_masking=True).\",\"loss_type (str) – Loss function type (“L1”, “L2”, or “L1+L2”).\",\"use_guided_attn_loss (bool) – Whether to use guided attention loss.\",\"guided_attn_loss_sigma (float) – Sigma in guided attention loss.\",\"guided_attn_loss_lambda (float) – Lambda in guided attention loss.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, T_feats, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, threshold: float = 0.5, minlenratio: float = 0.0, maxlenratio: float = 10.0, use_att_constraint: bool = False, backward_window: int = 1, forward_window: int = 3, use_teacher_forcing: bool = False) → Dict[str, Tensor]\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style (N, idim).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"threshold (float) – Threshold in inference.\",\"minlenratio (float) – Minimum length ratio in inference.\",\"maxlenratio (float) – Maximum length ratio in inference.\",\"use_att_constraint (bool) – Whether to apply attention constraint.\",\"backward_window (int) – Backward window in attention constraint.\",\"forward_window (int) – Forward window in attention constraint.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"prob (Tensor): Output sequence of stop probabilities (T_feats,).\",\"att_w (Tensor): Attention weights (T_feats, T).\",\"Return type: Dict[str, Tensor]\"]},\"2432\":{\"h\":\"espnet2.tts.transformer.transformer.Transformer\",\"t\":[\"source\",\"class espnet2.tts.transformer.transformer.Transformer(idim: int, odim: int, embed_dim: int = 512, eprenet_conv_layers: int = 3, eprenet_conv_chans: int = 256, eprenet_conv_filts: int = 5, dprenet_layers: int = 2, dprenet_units: int = 256, elayers: int = 6, eunits: int = 1024, adim: int = 512, aheads: int = 4, dlayers: int = 6, dunits: int = 1024, postnet_layers: int = 5, postnet_chans: int = 256, postnet_filts: int = 5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, reduction_factor: int = 1, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', use_gst: bool = False, gst_tokens: int = 10, gst_heads: int = 4, gst_conv_layers: int = 6, gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128), gst_conv_kernel_size: int = 3, gst_conv_stride: int = 2, gst_gru_layers: int = 1, gst_gru_units: int = 128, transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, transformer_enc_dec_attn_dropout_rate: float = 0.1, eprenet_dropout_rate: float = 0.5, dprenet_dropout_rate: float = 0.5, postnet_dropout_rate: float = 0.5, init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, bce_pos_weight: float = 5.0, loss_type: str = 'L1', use_guided_attn_loss: bool = True, num_heads_applied_guided_attn: int = 2, num_layers_applied_guided_attn: int = 2, modules_applied_guided_attn: Sequence[str] = 'encoder-decoder', guided_attn_loss_sigma: float = 0.4, guided_attn_loss_lambda: float = 1.0)\",\"Bases: AbsTTS\",\"Transformer-TTS module.\",\"This is a module of text-to-speech Transformer described in Neural Speech Synthesis with Transformer Network, which convert the sequence of tokens into the sequence of Mel-filterbanks.\",\"Initialize Transformer module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"embed_dim (int) – Dimension of character embedding.\",\"eprenet_conv_layers (int) – Number of encoder prenet convolution layers.\",\"eprenet_conv_chans (int) – Number of encoder prenet convolution channels.\",\"eprenet_conv_filts (int) – Filter size of encoder prenet convolution.\",\"dprenet_layers (int) – Number of decoder prenet layers.\",\"dprenet_units (int) – Number of decoder prenet hidden units.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"adim (int) – Number of attention transformation dimensions.\",\"aheads (int) – Number of heads for multi head attention.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Filter size of postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"positionwise_layer_type (str) – Position-wise operation type.\",\"positionwise_conv_kernel_size (int) – Kernel size in position wise conv 1d.\",\"reduction_factor (int) – Reduction factor.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type (str) – How to integrate speaker embedding.\",\"use_gst (str) – Whether to use global style token.\",\"gst_tokens (int) – Number of GST embeddings.\",\"gst_heads (int) – Number of heads in GST multihead attention.\",\"gst_conv_layers (int) – Number of conv layers in GST.\",\"gst_conv_chans_list – (Sequence[int]): List of the number of channels of conv layers in GST.\",\"gst_conv_kernel_size (int) – Kernel size of conv layers in GST.\",\"gst_conv_stride (int) – Stride size of conv layers in GST.\",\"gst_gru_layers (int) – Number of GRU layers in GST.\",\"gst_gru_units (int) – Number of GRU units in GST.\",\"transformer_lr (float) – Initial value of learning rate.\",\"transformer_warmup_steps (int) – Optimizer warmup steps.\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"transformer_enc_dec_attn_dropout_rate (float) – Dropout rate in source attention module.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"eprenet_dropout_rate (float) – Dropout rate in encoder prenet.\",\"dprenet_dropout_rate (float) – Dropout rate in decoder prenet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"bce_pos_weight (float) – Positive sample weight in bce calculation (only for use_masking=true).\",\"loss_type (str) – How to calculate loss.\",\"use_guided_attn_loss (bool) – Whether to use guided attention loss.\",\"num_heads_applied_guided_attn (int) – Number of heads in each layer to apply guided attention loss.\",\"num_layers_applied_guided_attn (int) – Number of layers to apply guided attention loss.\",\"modules_applied_guided_attn (Sequence *[*str]) – List of module names to apply guided attention loss.\",\"guided_attn_loss_sigma (float)\",\"guided_attn_loss_lambda (float) – Lambda in guided attention loss.\",\"forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded character ids (B, Tmax).\",\"text_lengths (LongTensor) – Batch of lengths of each input batch (B,).\",\"feats (Tensor) – Batch of padded target features (B, Lmax, odim).\",\"feats_lengths (LongTensor) – Batch of the lengths of each target (B,).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, feats: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, threshold: float = 0.5, minlenratio: float = 0.0, maxlenratio: float = 10.0, use_teacher_forcing: bool = False) → Dict[str, Tensor]\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"feats (Optional *[*Tensor]) – Feature sequence to extract style embedding (T_feats’, idim).\",\"spembs (Optional *[*Tensor]) – Speaker embedding (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"threshold (float) – Threshold in inference.\",\"minlenratio (float) – Minimum length ratio in inference.\",\"maxlenratio (float) – Maximum length ratio in inference.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"prob (Tensor): Output sequence of stop probabilities (T_feats,).\",\"att_w (Tensor): Source attn weight (#layers, #heads, T_feats, T_text).\",\"Return type: Dict[str, Tensor]\"]},\"2433\":{\"h\":\"espnet2.tts.fastspeech2.variance_predictor.VariancePredictor\",\"t\":[\"source\",\"class espnet2.tts.fastspeech2.variance_predictor.VariancePredictor(idim: int, n_layers: int = 2, n_chans: int = 384, kernel_size: int = 3, bias: bool = True, dropout_rate: float = 0.5)\",\"Bases: Module\",\"Variance predictor module.\",\"This is a module of variacne predictor described in FastSpeech 2: Fast and High-Quality End-to-End Text to Speech.\",\"Initilize duration predictor module.\",\"Parameters:\",\"idim (int) – Input dimension.\",\"n_layers (int) – Number of convolutional layers.\",\"n_chans (int) – Number of channels of convolutional layers.\",\"kernel_size (int) – Kernel size of convolutional layers.\",\"dropout_rate (float) – Dropout rate.\",\"forward(xs: Tensor, x_masks: Tensor | None = None) → Tensor\",\"Calculate forward propagation.\",\"Parameters:\",\"xs (Tensor) – Batch of input sequences (B, Tmax, idim).\",\"x_masks (ByteTensor) – Batch of masks indicating padded part (B, Tmax).\",\"Returns: Batch of predicted sequences (B, Tmax, 1).\",\"Return type: Tensor\"]},\"2434\":{\"h\":\"espnet2.tts.feats_extract.ying.Ying\",\"t\":[\"source\",\"class espnet2.tts.feats_extract.ying.Ying(fs: int = 22050, w_step: int = 256, W: int = 2048, tau_max: int = 2048, midi_start: int = -5, midi_end: int = 75, octave_range: int = 24, use_token_averaged_ying: bool = False)\",\"Bases: AbsFeatsExtract\",\"Extact Ying-based Features.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"crop_scope(x, yin_start, scope_shift)\",\"forward(input: Tensor, input_lengths: Tensor | None = None, feats_lengths: Tensor | None = None, durations: Tensor | None = None, durations_lengths: Tensor | None = None) → Tuple[Tensor, Tensor]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2435\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"get_parameters() → Dict[str, Any]\",\"midi_to_lag(m: int, octave_range: float = 12)\",\"converts midi-to-lag, eq. (4)\",\"Parameters:\",\"m – midi\",\"fs – sample_rate\",\"octave_range\",\"Returns: time lag(tau, c(m)) calculated from midi, eq. (4)\",\"Return type: lag\",\"output_size() → int\",\"yingram(x: Tensor)\",\"calculates yingram from raw audio (multi segment)\",\"Parameters:\",\"x – raw audio, torch.Tensor of shape (t)\",\"W – yingram Window Size\",\"tau_max\",\"fs – sampling rate\",\"w_step – yingram bin step size\",\"Returns: yingram. torch.Tensor of shape (80 x t’)\",\"Return type: yingram\",\"yingram_from_cmndf(cmndfs: Tensor) → Tensor\",\"yingram calculator from cMNDFs.\",\"(cumulative Mean Normalized Difference Functions)\",\"Parameters:\",\"cmndfs – torch.Tensor calculated cumulative mean normalized difference function for details, see models/yin.py or eq. (1) and (2)\",\"ms – list of midi(int)\",\"fs – sampling rate\",\"Returns: calculated batch yingram\",\"Return type: y\"]},\"2436\":{\"h\":\"espnet2.tts.feats_extract.yin.cumulativeMeanNormalizedDifferenceFunction\",\"t\":[\"source\",\"espnet2.tts.feats_extract.yin.cumulativeMeanNormalizedDifferenceFunction(df, N, eps=1e-08)\",\"Compute cumulative mean normalized difference function (CMND).\",\"This corresponds to equation (8) in [1]\",\"Parameters:\",\"df – Difference function\",\"N – length of data\",\"Returns: cumulative mean normalized difference function\",\"Return type: list\"]},\"2437\":{\"h\":\"espnet2.tts.feats_extract.yin.cumulativeMeanNormalizedDifferenceFunctionTorch\",\"t\":[\"source\",\"espnet2.tts.feats_extract.yin.cumulativeMeanNormalizedDifferenceFunctionTorch(dfs: Tensor, N, eps=1e-08) → Tensor\"]},\"2438\":{\"h\":\"espnet2.tts.feats_extract.yin.differenceFunction\",\"t\":[\"source\",\"espnet2.tts.feats_extract.yin.differenceFunction(x, N, tau_max)\",\"Compute difference function of data x. This corresponds to equation (6) in [1]\",\"This solution is implemented directly with torch rfft.\",\"Parameters:\",\"x – audio data (Tensor)\",\"N – length of data\",\"tau_max – integration window size\",\"Returns: difference function\",\"Return type: list\"]},\"2439\":{\"h\":\"espnet2.tts.feats_extract.yin.differenceFunctionTorch\",\"t\":[\"source\",\"espnet2.tts.feats_extract.yin.differenceFunctionTorch(xs: Tensor, N, tau_max) → Tensor\",\"pytorch backend batch-wise differenceFunction\",\"has 1e-4 level error with input shape of (32, 22050*1.5) :param xs: :param N: :param tau_max:\",\"Returns:\"]},\"2440\":{\"h\":\"espnet2.tts.feats_extract.yin.differenceFunction_np\",\"t\":[\"source\",\"espnet2.tts.feats_extract.yin.differenceFunction_np(x, N, tau_max)\",\"Compute difference function of data x. This corresponds to equation (6) in [1]\",\"This solution is implemented directly with Numpy fft.\",\"Parameters:\",\"x – audio data\",\"N – length of data\",\"tau_max – integration window size\",\"Returns: difference function\",\"Return type: list\"]},\"2441\":{\"h\":\"espnet2.tts.prodiff.loss.gaussian\",\"t\":[\"source\",\"espnet2.tts.prodiff.loss.gaussian(window_size: int, sigma: float) → Tensor\",\"Gaussian Noise.\",\"Parameters:\",\"window_size (int) – Window size.\",\"sigma (float) – Noise sigma.\",\"Returns: Noise.\",\"Return type: torch.Tensor\"]},\"2442\":{\"h\":\"espnet2.tts.prodiff.denoiser.noise_scheduler\",\"t\":[\"source\",\"espnet2.tts.prodiff.denoiser.noise_scheduler(sched_type: str, timesteps: int, min_beta: float = 0.0, max_beta: float = 0.01, s: float = 0.008) → Tensor\",\"Noise Scheduler.\",\"Parameters:\",\"sched_type (str) – type of scheduler.\",\"timesteps (int) – numbern of time steps.\",\"min_beta (float,optional) – Minimum beta. Defaults to 0.0.\",\"max_beta (float,optional) – Maximum beta. Defaults to 0.01.\",\"s (float,optional) – Scheduler intersection. Defaults to 0.008.\",\"Returns: Noise.\",\"Return type: tensor\"]},\"2443\":{\"h\":\"espnet2.tts2.feats_extract.abs_feats_extract.AbsFeatsExtractDiscrete\",\"t\":[\"source\",\"class espnet2.tts2.feats_extract.abs_feats_extract.AbsFeatsExtractDiscrete(*args, **kwargs)\",\"Bases: Module, ABC\",\"Parse the discrete token sequence\",\"into structured data format for predicting. E.g., (1) keep as sequence (2) resize as a matrix (3) multi-resolution …\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(input: Tensor, input_lengths: Tensor) → Tuple[Any, Dict]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2444\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2445\":{\"h\":\"espnet2.tts2.abs_tts2.AbsTTS2\",\"t\":[\"source\",\"class espnet2.tts2.abs_tts2.AbsTTS2(*args, **kwargs)\",\"Bases: Module, ABC\",\"TTS2 (Discrete Unit-Based TTS) abstract class.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(text: Tensor, text_lengths: Tensor, feats: Tensor, feats_lengths: Tensor, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate outputs and return the loss tensor.\",\"abstract inference(text: Tensor, **kwargs) → Dict[str, Tensor]\",\"Return predicted output as a dict.\",\"property require_raw_speech\",\"Return whether or not raw_speech is required.\",\"property require_vocoder\",\"Return whether or not vocoder is required.\"]},\"2446\":{\"h\":\"espnet2.tts2.espnet_model.ESPnetTTS2Model\",\"t\":[\"source\",\"class espnet2.tts2.espnet_model.ESPnetTTS2Model(discrete_feats_extract: AbsFeatsExtractDiscrete, pitch_extract: AbsFeatsExtract | None, energy_extract: AbsFeatsExtract | None, pitch_normalize: InversibleInterface | None, energy_normalize: InversibleInterface | None, tts: AbsTTS2)\",\"Bases: AbsESPnetModel\",\"ESPnet model for text-to-speech task.\",\"Initialize ESPnetTTSModel module.\",\"collect_feats(text: Tensor, text_lengths: Tensor, discrete_speech: Tensor, discrete_speech_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, durations: Tensor | None = None, durations_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs) → Dict[str, Tensor]\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"discrete_speech (Tensor) – Discrete speech tensor (B, T_token).\",\"discrete_speech_lengths (Tensor) – Discrete speech length tensor (B,).\",\"durations (Optional *[*Tensor) – Duration tensor.\",\"durations_lengths (Optional *[*Tensor) – Duration length tensor (B,).\",\"pitch (Optional *[*Tensor) – Pitch tensor.\",\"pitch_lengths (Optional *[*Tensor) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"Returns: Dict of features.\",\"Return type: Dict[str, Tensor]\",\"forward(text: Tensor, text_lengths: Tensor, discrete_speech: Tensor, discrete_speech_lengths: Tensor, speech: Tensor, speech_lengths: Tensor, durations: Tensor | None = None, durations_lengths: Tensor | None = None, pitch: Tensor | None = None, pitch_lengths: Tensor | None = None, energy: Tensor | None = None, energy_lengths: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Caclualte outputs and return the loss tensor.\",\"Parameters:\",\"text (Tensor) – Text index tensor (B, T_text).\",\"text_lengths (Tensor) – Text length tensor (B,).\",\"speech (Tensor) – Speech waveform tensor (B, T_wav).\",\"speech_lengths (Tensor) – Speech length tensor (B,).\",\"discrete_speech (Tensor) – Discrete speech tensor (B, T_token).\",\"discrete_speech_lengths (Tensor) – Discrete speech length tensor (B,).\",\"duration (Optional *[*Tensor]) – Duration tensor.\",\"duration_lengths (Optional *[*Tensor]) – Duration length tensor (B,).\",\"pitch (Optional *[*Tensor]) – Pitch tensor.\",\"pitch_lengths (Optional *[*Tensor]) – Pitch length tensor (B,).\",\"energy (Optional *[*Tensor]) – Energy tensor.\",\"energy_lengths (Optional *[*Tensor]) – Energy length tensor (B,).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (B, D).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (B, 1).\",\"lids (Optional *[*Tensor]) – Language ID tensor (B, 1).\",\"kwargs – “utt_id” is among the input.\",\"Returns: Loss scalar tensor. Dict[str, float]: Statistics to be monitored. Tensor: Weight tensor to summarize losses.\",\"Return type: Tensor\",\"inference(text: Tensor, speech: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, durations: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, **decode_config) → Dict[str, Tensor]\",\"Caclualte features and return them as a dict.\",\"Parameters:\",\"text (Tensor) – Text index tensor (T_text).\",\"speech (Tensor) – Speech waveform tensor (T_wav).\",\"spembs (Optional *[*Tensor]) – Speaker embedding tensor (D,).\",\"sids (Optional *[*Tensor]) – Speaker ID tensor (1,).\",\"lids (Optional *[*Tensor]) – Language ID tensor (1,).\",\"durations (Optional *[*Tensor) – Duration tensor.\",\"pitch (Optional *[*Tensor) – Pitch tensor.\",\"energy (Optional *[*Tensor) – Energy tensor.\",\"Returns: Dict of outputs.\",\"Return type: Dict[str, Tensor]\"]},\"2447\":{\"h\":\"espnet2.tts2.fastspeech2.fastspeech2_discrete.FastSpeech2Discrete\",\"t\":[\"source\",\"class espnet2.tts2.fastspeech2.fastspeech2_discrete.FastSpeech2Discrete(idim: int, odim: int, adim: int = 384, aheads: int = 4, elayers: int = 6, eunits: int = 1536, dlayers: int = 6, dunits: int = 1536, postnet_layers: int = 5, postnet_chans: int = 512, postnet_filts: int = 5, postnet_dropout_rate: float = 0.5, positionwise_layer_type: str = 'conv1d', positionwise_conv_kernel_size: int = 1, use_scaled_pos_enc: bool = True, use_batch_norm: bool = True, encoder_normalize_before: bool = True, decoder_normalize_before: bool = True, encoder_concat_after: bool = False, decoder_concat_after: bool = False, reduction_factor: int = 1, encoder_type: str = 'transformer', decoder_type: str = 'transformer', transformer_enc_dropout_rate: float = 0.1, transformer_enc_positional_dropout_rate: float = 0.1, transformer_enc_attn_dropout_rate: float = 0.1, transformer_dec_dropout_rate: float = 0.1, transformer_dec_positional_dropout_rate: float = 0.1, transformer_dec_attn_dropout_rate: float = 0.1, conformer_rel_pos_type: str = 'legacy', conformer_pos_enc_layer_type: str = 'rel_pos', conformer_self_attn_layer_type: str = 'rel_selfattn', conformer_activation_type: str = 'swish', use_macaron_style_in_conformer: bool = True, use_cnn_in_conformer: bool = True, zero_triu: bool = False, conformer_enc_kernel_size: int = 7, conformer_dec_kernel_size: int = 31, duration_predictor_layers: int = 2, duration_predictor_chans: int = 384, duration_predictor_kernel_size: int = 3, duration_predictor_dropout_rate: float = 0.1, energy_predictor_layers: int = 2, energy_predictor_chans: int = 384, energy_predictor_kernel_size: int = 3, energy_predictor_dropout: float = 0.5, energy_embed_kernel_size: int = 9, energy_embed_dropout: float = 0.5, stop_gradient_from_energy_predictor: bool = False, pitch_predictor_layers: int = 2, pitch_predictor_chans: int = 384, pitch_predictor_kernel_size: int = 3, pitch_predictor_dropout: float = 0.5, pitch_embed_kernel_size: int = 9, pitch_embed_dropout: float = 0.5, stop_gradient_from_pitch_predictor: bool = False, spks: int | None = None, langs: int | None = None, spk_embed_dim: int | None = None, spk_embed_integration_type: str = 'add', init_type: str = 'xavier_uniform', init_enc_alpha: float = 1.0, init_dec_alpha: float = 1.0, use_masking: bool = False, use_weighted_masking: bool = False, ignore_id: int = 0, discrete_token_layers: int = 1)\",\"Bases: AbsTTS2\",\"FastSpeech2 module with discrete output.\",\"This is a module of discrete-output Fastspeech2: it uses the same Fastspeech2 architecture as tts1, but with discrete token as output.\",\"Initialize FastSpeech2 module.\",\"Parameters:\",\"idim (int) – Dimension of the inputs.\",\"odim (int) – Dimension of the outputs.\",\"elayers (int) – Number of encoder layers.\",\"eunits (int) – Number of encoder hidden units.\",\"dlayers (int) – Number of decoder layers.\",\"dunits (int) – Number of decoder hidden units.\",\"postnet_layers (int) – Number of postnet layers.\",\"postnet_chans (int) – Number of postnet channels.\",\"postnet_filts (int) – Kernel size of postnet.\",\"postnet_dropout_rate (float) – Dropout rate in postnet.\",\"use_scaled_pos_enc (bool) – Whether to use trainable scaled pos encoding.\",\"use_batch_norm (bool) – Whether to use batch normalization in encoder prenet.\",\"encoder_normalize_before (bool) – Whether to apply layernorm layer before encoder block.\",\"decoder_normalize_before (bool) – Whether to apply layernorm layer before decoder block.\",\"encoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in encoder.\",\"decoder_concat_after (bool) – Whether to concatenate attention layer’s input and output in decoder.\",\"reduction_factor (int) – Reduction factor.\",\"encoder_type (str) – Encoder type (“transformer” or “conformer”).\",\"decoder_type (str) – Decoder type (“transformer” or “conformer”).\",\"transformer_enc_dropout_rate (float) – Dropout rate in encoder except attention and positional encoding.\",\"transformer_enc_positional_dropout_rate (float) – Dropout rate after encoder positional encoding.\",\"transformer_enc_attn_dropout_rate (float) – Dropout rate in encoder self-attention module.\",\"transformer_dec_dropout_rate (float) – Dropout rate in decoder except attention & positional encoding.\",\"transformer_dec_positional_dropout_rate (float) – Dropout rate after decoder positional encoding.\",\"transformer_dec_attn_dropout_rate (float) – Dropout rate in decoder self-attention module.\",\"conformer_rel_pos_type (str) – Relative pos encoding type in conformer.\",\"conformer_pos_enc_layer_type (str) – Pos encoding layer type in conformer.\",\"conformer_self_attn_layer_type (str) – Self-attention layer type in conformer\",\"conformer_activation_type (str) – Activation function type in conformer.\",\"use_macaron_style_in_conformer – Whether to use macaron style FFN.\",\"use_cnn_in_conformer – Whether to use CNN in conformer.\",\"zero_triu – Whether to use zero triu in relative self-attention module.\",\"conformer_enc_kernel_size – Kernel size of encoder conformer.\",\"conformer_dec_kernel_size – Kernel size of decoder conformer.\",\"duration_predictor_layers (int) – Number of duration predictor layers.\",\"duration_predictor_chans (int) – Number of duration predictor channels.\",\"duration_predictor_kernel_size (int) – Kernel size of duration predictor.\",\"duration_predictor_dropout_rate (float) – Dropout rate in duration predictor.\",\"pitch_predictor_layers (int) – Number of pitch predictor layers.\",\"pitch_predictor_chans (int) – Number of pitch predictor channels.\",\"pitch_predictor_kernel_size (int) – Kernel size of pitch predictor.\",\"pitch_predictor_dropout_rate (float) – Dropout rate in pitch predictor.\",\"pitch_embed_kernel_size (float) – Kernel size of pitch embedding.\",\"pitch_embed_dropout_rate (float) – Dropout rate for pitch embedding.\",\"stop_gradient_from_pitch_predictor – Whether to stop gradient from pitch predictor to encoder.\",\"energy_predictor_layers (int) – Number of energy predictor layers.\",\"energy_predictor_chans (int) – Number of energy predictor channels.\",\"energy_predictor_kernel_size (int) – Kernel size of energy predictor.\",\"energy_predictor_dropout_rate (float) – Dropout rate in energy predictor.\",\"energy_embed_kernel_size (float) – Kernel size of energy embedding.\",\"energy_embed_dropout_rate (float) – Dropout rate for energy embedding.\",\"stop_gradient_from_energy_predictor – Whether to stop gradient from energy predictor to encoder.\",\"spks (Optional *[*int]) – Number of speakers. If set to > 1, assume that the sids will be provided as the input and use sid embedding layer.\",\"langs (Optional *[*int]) – Number of languages. If set to > 1, assume that the lids will be provided as the input and use sid embedding layer.\",\"spk_embed_dim (Optional *[*int]) – Speaker embedding dimension. If set to > 0, assume that spembs will be provided as the input.\",\"spk_embed_integration_type – How to integrate speaker embedding.\",\"init_type (str) – How to initialize transformer parameters.\",\"init_enc_alpha (float) – Initial value of alpha in scaled pos encoding of the encoder.\",\"init_dec_alpha (float) – Initial value of alpha in scaled pos encoding of the decoder.\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to apply weighted masking in loss calculation.\",\"forward(text: Tensor, text_lengths: Tensor, discrete_feats: Tensor, discrete_feats_lengths: Tensor, durations: Tensor, durations_lengths: Tensor, pitch: Tensor, pitch_lengths: Tensor, energy: Tensor, energy_lengths: Tensor, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, joint_training: bool = False) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"text (LongTensor) – Batch of padded token ids (B, T_text).\",\"text_lengths (LongTensor) – Batch of lengths of each input (B,).\",\"discrete_feats (Tensor) – Discrete speech tensor (B, T_token).\",\"discrete_feats_lengths (LongTensor) – Discrete speech length tensor (B,).\",\"durations (LongTensor) – Batch of padded durations (B, T_text + 1).\",\"durations_lengths (LongTensor) – Batch of duration lengths (B, T_text + 1).\",\"pitch (Tensor) – Batch of padded token-averaged pitch (B, T_text + 1, 1).\",\"pitch_lengths (LongTensor) – Batch of pitch lengths (B, T_text + 1).\",\"energy (Tensor) – Batch of padded token-averaged energy (B, T_text + 1, 1).\",\"energy_lengths (LongTensor) – Batch of energy lengths (B, T_text + 1).\",\"spembs (Optional *[*Tensor]) – Batch of speaker embeddings (B, spk_embed_dim).\",\"sids (Optional *[*Tensor]) – Batch of speaker IDs (B, 1).\",\"lids (Optional *[*Tensor]) – Batch of language IDs (B, 1).\",\"joint_training (bool) – Whether to perform joint training with vocoder.\",\"Returns: Loss scalar value. Dict: Statistics to be monitored. Tensor: Weight value if not joint training else model outputs.\",\"Return type: Tensor\",\"inference(text: Tensor, durations: Tensor | None = None, spembs: Tensor | None = None, sids: Tensor | None = None, lids: Tensor | None = None, pitch: Tensor | None = None, energy: Tensor | None = None, alpha: float = 1.0, use_teacher_forcing: bool = False) → Dict[str, Tensor]\",\"Generate the sequence of features given the sequences of characters.\",\"Parameters:\",\"text (LongTensor) – Input sequence of characters (T_text,).\",\"durations (Optional *[*Tensor) – Groundtruth of duration (T_text + 1,).\",\"spembs (Optional *[*Tensor) – Speaker embedding vector (spk_embed_dim,).\",\"sids (Optional *[*Tensor]) – Speaker ID (1,).\",\"lids (Optional *[*Tensor]) – Language ID (1,).\",\"pitch (Optional *[*Tensor]) – Groundtruth of token-avg pitch (T_text + 1, 1).\",\"energy (Optional *[*Tensor]) – Groundtruth of token-avg energy (T_text + 1, 1).\",\"alpha (float) – Alpha to control the speed.\",\"use_teacher_forcing (bool) – Whether to use teacher forcing. If true, groundtruth of duration, pitch and energy will be used.\",\"Returns: Output dict including the following items: : * feat_gen (Tensor): Output sequence of features (T_feats, odim). \",\"duration (Tensor): Duration sequence (T_text + 1,).\",\"pitch (Tensor): Pitch sequence (T_text + 1,).\",\"energy (Tensor): Energy sequence (T_text + 1,).\",\"Return type: Dict[str, Tensor]\"]},\"2448\":{\"h\":\"espnet2.tts2.fastspeech2.loss.FastSpeech2LossDiscrete\",\"t\":[\"source\",\"class espnet2.tts2.fastspeech2.loss.FastSpeech2LossDiscrete(use_masking: bool = True, use_weighted_masking: bool = False, ignore_id: int = -1)\",\"Bases: Module\",\"Loss function module for FastSpeech2.\",\"Initialize feed-forward Transformer loss module.\",\"Parameters:\",\"use_masking (bool) – Whether to apply masking for padded part in loss calculation.\",\"use_weighted_masking (bool) – Whether to weighted masking in loss calculation.\",\"forward(after_outs: Tensor, before_outs: Tensor, d_outs: Tensor, p_outs: Tensor, e_outs: Tensor, ys: Tensor, ds: Tensor, ps: Tensor, es: Tensor, ilens: Tensor, olens: Tensor) → Tuple[Tensor, Tensor, Tensor, Tensor]\",\"Calculate forward propagation.\",\"Parameters:\",\"after_outs (Tensor) – Batch of outputs after postnets (B, T_feats, odim).\",\"before_outs (Tensor) – Batch of outputs before postnets (B, T_feats, odim).\",\"d_outs (LongTensor) – Batch of outputs of duration predictor (B, T_text).\",\"p_outs (Tensor) – Batch of outputs of pitch predictor (B, T_text, 1).\",\"e_outs (Tensor) – Batch of outputs of energy predictor (B, T_text, 1).\",\"ys (Tensor) – Batch of target features in discrete space (B, T_feats).\",\"ds (LongTensor) – Batch of durations (B, T_text).\",\"ps (Tensor) – Batch of target token-averaged pitch (B, T_text, 1).\",\"es (Tensor) – Batch of target token-averaged energy (B, T_text, 1).\",\"ilens (LongTensor) – Batch of the lengths of each input (B,).\",\"olens (LongTensor) – Batch of the lengths of each target (B,).\",\"Returns: CrossEntropy loss value. Tensor: Duration predictor loss value. Tensor: Pitch predictor loss value. Tensor: Energy predictor loss value.\",\"Return type: Tensor\"]},\"2449\":{\"h\":\"espnet2.tts2.feats_extract.identity.IdentityFeatureExtract\",\"t\":[\"source\",\"class espnet2.tts2.feats_extract.identity.IdentityFeatureExtract\",\"Bases: AbsFeatsExtractDiscrete\",\"Keep the input discrete sequence as-is\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(input: Tensor, input_lengths: Tensor) → Tuple[Any, Dict]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2450\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2451\":{\"h\":\"espnet2.uasr.discriminator.abs_discriminator.AbsDiscriminator\",\"t\":[\"source\",\"class espnet2.uasr.discriminator.abs_discriminator.AbsDiscriminator(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(xs_pad: Tensor, padding_mask: Tensor) → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2452\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2453\":{\"h\":\"espnet2.uasr.generator.abs_generator.AbsGenerator\",\"t\":[\"source\",\"class espnet2.uasr.generator.abs_generator.AbsGenerator(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward(xs_pad: Tensor, ilens: Tensor) → Tuple[Tensor, Tensor, Tensor | None]\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2454\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"abstract output_size() → int\"]},\"2455\":{\"h\":\"espnet2.uasr.segmenter.abs_segmenter.AbsSegmenter\",\"t\":[\"source\",\"class espnet2.uasr.segmenter.abs_segmenter.AbsSegmenter(*args, **kwargs)\",\"Bases: Module, ABC\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract logit_segment(xs_pad: Tensor, ilens: Tensor) → Tensor\",\"abstract pre_segment(xs_pad: Tensor, ilens: Tensor) → Tensor\"]},\"2456\":{\"h\":\"espnet2.uasr.loss.abs_loss.AbsUASRLoss\",\"t\":[\"source\",\"class espnet2.uasr.loss.abs_loss.AbsUASRLoss(*args, **kwargs)\",\"Bases: Module, ABC\",\"Base class for all Diarization loss modules.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"abstract forward() → Tensor\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2457\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"property name : str\"]},\"2458\":{\"h\":\"espnet2.uasr.discriminator.conv_discriminator.ConvDiscriminator\",\"t\":[\"source\",\"class espnet2.uasr.discriminator.conv_discriminator.ConvDiscriminator(input_dim: int, cfg: Dict | None = None, conv_channels: int = 384, conv_kernel: int = 8, conv_dilation: int = 1, conv_depth: int = 2, linear_emb: str2bool = False, causal: str2bool = True, max_pool: str2bool = False, act_after_linear: str2bool = False, dropout: float = 0.0, spectral_norm: str2bool = False, weight_norm: str2bool = False)\",\"Bases: AbsDiscriminator\",\"convolutional discriminator for UASR.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x: Tensor, padding_mask: Tensor | None)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2459\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2460\":{\"h\":\"espnet2.uasr.generator.conv_generator.ConvGenerator\",\"t\":[\"source\",\"class espnet2.uasr.generator.conv_generator.ConvGenerator(input_dim: int, output_dim: int, cfg: Dict | None = None, conv_kernel: int = 3, conv_dilation: int = 1, conv_stride: int = 9, pad: int = -1, bias: str2bool = False, dropout: float = 0.0, batch_norm: str2bool = True, batch_norm_weight: float = 30.0, residual: str2bool = True)\",\"Bases: AbsGenerator\",\"convolutional generator for UASR.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"bn_padded_data(feature: Tensor, padding_mask: Tensor)\",\"forward(feats: Tensor, text: Tensor | None, feats_padding_mask: Tensor)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2461\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\",\"output_size()\"]},\"2462\":{\"h\":\"espnet2.uasr.espnet_model.ESPnetUASRModel\",\"t\":[\"source\",\"class espnet2.uasr.espnet_model.ESPnetUASRModel(frontend: AbsFrontend | None, segmenter: AbsSegmenter | None, generator: AbsGenerator, discriminator: AbsDiscriminator, losses: Dict[str, AbsUASRLoss], kenlm_path: str | None, token_list: list | None, max_epoch: int | None, vocab_size: int, cfg: Dict | None = None, pad: int = 1, sil_token: str = '<SIL>', sos_token: str = '<s>', eos_token: str = '</s>', skip_softmax: str2bool = False, use_gumbel: str2bool = False, use_hard_gumbel: str2bool = True, min_temperature: float = 0.1, max_temperature: float = 2.0, decay_temperature: float = 0.99995, use_collected_training_feats: str2bool = False)\",\"Bases: AbsESPnetModel\",\"Unsupervised ASR model.\",\"The source code is from FAIRSEQ: https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec/unsupervised\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"collect_feats(speech: Tensor, speech_lengths: Tensor, text: Tensor | None = None, text_lengths: Tensor | None = None, **kwargs) → Dict[str, Tensor]\",\"encode(speech: Tensor, speech_lengths: Tensor) → Tuple[Tensor, Tensor]\",\"forward(speech: Tensor, speech_lengths: Tensor, text: Tensor | None = None, text_lengths: Tensor | None = None, pseudo_labels: Tensor | None = None, pseudo_labels_lengths: Tensor | None = None, do_validation: str2bool | None = False, print_hyp: str2bool | None = False, **kwargs) → Tuple[Tensor, Dict[str, Tensor], Tensor]\",\"Frontend + Segmenter + Generator + Discriminator + Calc Loss\",\"Args:\",\"get_optim_index()\",\"inference(speech: Tensor, speech_lengths: Tensor)\",\"is_discriminative_step()\",\"property number_updates\"]},\"2463\":{\"h\":\"espnet2.uasr.segmenter.join_segmenter.JoinSegmenter\",\"t\":[\"source\",\"class espnet2.uasr.segmenter.join_segmenter.JoinSegmenter(cfg: Dict | None = None, subsample_rate: float = 0.25, mean_pool: str2bool = True, mean_join_pool: str2bool = False, remove_zeros: str2bool = False)\",\"Bases: AbsSegmenter\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"logit_segment(logits: Tensor, padding_mask: Tensor) → Tensor\",\"pre_segment(xs_pad: Tensor, padding_mask: Tensor) → Tensor\"]},\"2464\":{\"h\":\"espnet2.uasr.segmenter.random_segmenter.RandomSegmenter\",\"t\":[\"source\",\"class espnet2.uasr.segmenter.random_segmenter.RandomSegmenter(subsample_rate: float = 0.25, mean_pool: str2bool = True, mean_join_pool: str2bool = False, remove_zeros: str2bool = False)\",\"Bases: AbsSegmenter\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"logit_segment(xs_pad: Tensor, padding_mask: Tensor) → Tensor\",\"pre_segment(xs_pad: Tensor, padding_mask: Tensor) → Tensor\"]},\"2465\":{\"h\":\"espnet2.uasr.generator.conv_generator.SamePad\",\"t\":[\"source\",\"class espnet2.uasr.generator.conv_generator.SamePad(kernel_size, causal=False)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2466\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2467\":{\"h\":\"espnet2.uasr.generator.conv_generator.TransposeLast\",\"t\":[\"source\",\"class espnet2.uasr.generator.conv_generator.TransposeLast(deconstruct_idx=None)\",\"Bases: Module\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(x)\",\"Define the computation performed at every call.\",\"Should be overridden by all subclasses.\"]},\"2468\":{\"h\":\"NOTE\",\"t\":[\"Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\"]},\"2469\":{\"h\":\"espnet2.uasr.loss.discriminator_loss.UASRDiscriminatorLoss\",\"t\":[\"source\",\"class espnet2.uasr.loss.discriminator_loss.UASRDiscriminatorLoss(weight: float = 1.0, smoothing: float = 0.0, smoothing_one_side: str2bool = False, reduction: str = 'sum')\",\"Bases: AbsUASRLoss\",\"discriminator loss for UASR.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(dense_y: Tensor, token_y: Tensor, is_discriminative_step: str2bool)\",\"Forward.\",\"Parameters:\",\"dense_y – predicted logits of generated samples\",\"token_y – predicted logits of real samples\"]},\"2470\":{\"h\":\"espnet2.uasr.loss.gradient_penalty.UASRGradientPenalty\",\"t\":[\"source\",\"class espnet2.uasr.loss.gradient_penalty.UASRGradientPenalty(discriminator: AbsDiscriminator, weight: float = 1.0, probabilistic_grad_penalty_slicing: str2bool = False, reduction: str = 'sum')\",\"Bases: AbsUASRLoss\",\"gradient penalty for UASR.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(fake_sample: Tensor, real_sample: Tensor, is_training: str2bool, is_discrimininative_step: str2bool)\",\"Forward.\",\"Parameters:\",\"fake_sample – generated sample from generator\",\"real_sample – real sample\",\"is_training – whether is at training step\",\"is_discriminative_step – whether is training discriminator\"]},\"2471\":{\"h\":\"espnet2.uasr.loss.phoneme_diversity_loss.UASRPhonemeDiversityLoss\",\"t\":[\"source\",\"class espnet2.uasr.loss.phoneme_diversity_loss.UASRPhonemeDiversityLoss(weight: float = 1.0)\",\"Bases: AbsUASRLoss\",\"phoneme diversity loss for UASR.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(dense_x: Tensor, sample_size: int, is_discriminative_step: str2bool)\",\"Forward.\",\"Parameters:\",\"dense_x – predicted logits of generated samples\",\"sample_size – batch size\",\"is_dicriminative_step – whether is training discriminator\"]},\"2472\":{\"h\":\"espnet2.uasr.loss.pseudo_label_loss.UASRPseudoLabelLoss\",\"t\":[\"source\",\"class espnet2.uasr.loss.pseudo_label_loss.UASRPseudoLabelLoss(weight: float = 1.0, input_dim: int = 128, output_dim: int = 64, downsample_rate: int = 2, ignore_index: int = -1, reduction: str = 'none')\",\"Bases: AbsUASRLoss\",\"auxiliary pseudo label loss for UASR.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(inter_x: Tensor, pseudo_labels: Tensor, is_discriminative_step: str2bool)\",\"Forward.\",\"Args:\"]},\"2473\":{\"h\":\"espnet2.uasr.loss.smoothness_penalty.UASRSmoothnessPenalty\",\"t\":[\"source\",\"class espnet2.uasr.loss.smoothness_penalty.UASRSmoothnessPenalty(weight: float = 1.0, reduction: str = 'none')\",\"Bases: AbsUASRLoss\",\"smoothness penalty for UASR.\",\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\",\"forward(dense_logits: Tensor, dense_padding_mask: Tensor, sample_size: int, is_discriminative_step: bool)\",\"Forward.\",\"Parameters:\",\"dense_logits – output logits of generator\",\"dense_padding_mask – padding mask of logits\",\"sample_size – batch size\",\"is_discriminative_step – Whether is training discriminator\"]},\"2474\":{\"h\":\"espnet2.utils.config_argparse.ArgumentParser\",\"t\":[\"source\",\"class espnet2.utils.config_argparse.ArgumentParser(*args, **kwargs)\",\"Bases: ArgumentParser\",\"Simple implementation of ArgumentParser supporting config file\",\"This class is originated from https://github.com/bw2/ConfigArgParse, but this class is lack of some features that it has.\",\"Not supporting multiple config files\",\"Automatically adding “–config” as an option.\",\"Not supporting any formats other than yaml\",\"Not checking argument type\",\"parse_known_args(args=None, namespace=None)\"]},\"2475\":{\"h\":\"espnet2.utils.eer.ComputeErrorRates\",\"t\":[\"source\",\"espnet2.utils.eer.ComputeErrorRates(scores, labels)\"]},\"2476\":{\"h\":\"espnet2.utils.eer.ComputeMinDcf\",\"t\":[\"source\",\"espnet2.utils.eer.ComputeMinDcf(fnrs, fprs, thresholds, p_target, c_miss, c_fa)\"]},\"2477\":{\"h\":\"espnet2.utils.get_default_kwargs.Invalid\",\"t\":[\"source\",\"class espnet2.utils.get_default_kwargs.Invalid\",\"Bases: object\",\"Marker object for not serializable-object\"]},\"2478\":{\"h\":\"espnet2.utils.nested_dict_action.NestedDictAction\",\"t\":[\"source\",\"class espnet2.utils.nested_dict_action.NestedDictAction(option_strings, dest, nargs=None, default=None, choices=None, required=False, help=None, metavar=None)\",\"Bases: Action\",\"Action class to append items to dict object.\"]},\"2479\":{\"h\":\"Examples\",\"t\":[\">>> parser = argparse.ArgumentParser() >>> _ = parser.add_argument('--conf', action=NestedDictAction, ... default={'a': 4}) >>> parser.parse_args(['--conf', 'a=3', '--conf', 'c=4']) Namespace(conf={'a': 3, 'c': 4}) >>> parser.parse_args(['--conf', 'c.d=4']) Namespace(conf={'a': 4, 'c': {'d': 4}}) >>> parser.parse_args(['--conf', 'c.d=4', '--conf', 'c=2']) Namespace(conf={'a': 4, 'c': 2}) >>> parser.parse_args(['--conf', '{d: 5, e: 9}']) Namespace(conf={'d': 5, 'e': 9})\"]},\"2480\":{\"h\":\"espnet2.utils.yaml_no_alias_safe_dump.NoAliasSafeDumper\",\"t\":[\"source\",\"class espnet2.utils.yaml_no_alias_safe_dump.NoAliasSafeDumper(stream, default_style=None, default_flow_style=False, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None, encoding=None, explicit_start=None, explicit_end=None, version=None, tags=None, sort_keys=True)\",\"Bases: SafeDumper\",\"A custom YAML SafeDumper that disables the use of anchors and aliases.\",\"This dumper overrides the ignore_aliases method to always return True, ensuring that YAML output does not contain anchors (&) or aliases (*), which can make the output less readable or “ugly” in certain contexts.\",\"Example usage: : yaml.dump(data, Dumper=NoAliasSafeDumper)\",\"ignore_aliases(data)\",\"Disable anchor/alias in yaml because looks ugly.\"]},\"2481\":{\"h\":\"espnet2.utils.sized_dict.SizedDict\",\"t\":[\"source\",\"class espnet2.utils.sized_dict.SizedDict(shared: bool = False, data: dict | None = None)\",\"Bases: MutableMapping\"]},\"2482\":{\"h\":\"espnet2.utils.griffin_lim.Spectrogram2Waveform\",\"t\":[\"source\",\"class espnet2.utils.griffin_lim.Spectrogram2Waveform(n_fft: int, n_shift: int, fs: int | None = None, n_mels: int | None = None, win_length: int | None = None, window: str | None = 'hann', fmin: int | None = None, fmax: int | None = None, griffin_lim_iters: int | None = 8)\",\"Bases: object\",\"Spectrogram to waveform conversion module.\",\"Initialize module.\",\"Parameters:\",\"fs – Sampling frequency.\",\"n_fft – The number of FFT points.\",\"n_shift – Shift size in points.\",\"n_mels – The number of mel basis.\",\"win_length – Window length in points.\",\"window – Window function type.\",\"f_min – Minimum frequency to analyze.\",\"f_max – Maximum frequency to analyze.\",\"griffin_lim_iters – The number of iterations.\"]},\"2483\":{\"h\":\"espnet2.utils.build_dataclass.build_dataclass\",\"t\":[\"source\",\"espnet2.utils.build_dataclass.build_dataclass(dataclass, args: Namespace)\",\"Helper function to build dataclass from ‘args’.\"]},\"2484\":{\"h\":\"espnet2.utils.types.float_or_none\",\"t\":[\"source\",\"espnet2.utils.types.float_or_none(value: str) → float | None\",\"float_or_none.\"]},\"2485\":{\"h\":\"Examples\",\"t\":[\">>> import argparse >>> parser = argparse.ArgumentParser() >>> _ = parser.add_argument('--foo', type=float_or_none) >>> parser.parse_args(['--foo', '4.5']) Namespace(foo=4.5) >>> parser.parse_args(['--foo', 'none']) Namespace(foo=None) >>> parser.parse_args(['--foo', 'null']) Namespace(foo=None) >>> parser.parse_args(['--foo', 'nil']) Namespace(foo=None)\"]},\"2486\":{\"h\":\"espnet2.utils.kwargs2args.func\",\"t\":[\"source\",\"espnet2.utils.kwargs2args.func(a: int, b, *, c, **kwargs)\"]},\"2487\":{\"h\":\"espnet2.utils.get_default_kwargs.get_default_kwargs\",\"t\":[\"source\",\"espnet2.utils.get_default_kwargs.get_default_kwargs(func)\",\"Get the default values of the input function.\"]},\"2488\":{\"h\":\"Examples\",\"t\":[\">>> def func(a, b=3): pass >>> get_default_kwargs(func) {'b': 3}\"]},\"2489\":{\"h\":\"espnet2.utils.sized_dict.get_size\",\"t\":[\"source\",\"espnet2.utils.sized_dict.get_size(obj, seen=None)\",\"Recursively finds size of objects\",\"Taken from https://github.com/bosswissam/pysize\"]},\"2490\":{\"h\":\"espnet2.utils.griffin_lim.griffin_lim\",\"t\":[\"source\",\"espnet2.utils.griffin_lim.griffin_lim(spc: ndarray, n_fft: int, n_shift: int, win_length: int | None = None, window: str | None = 'hann', n_iter: int | None = 32) → ndarray\",\"Convert linear spectrogram into waveform using Griffin-Lim.\",\"Parameters:\",\"spc – Linear spectrogram (T, n_fft // 2 + 1).\",\"n_fft – The number of FFT points.\",\"n_shift – Shift size in points.\",\"win_length – Window length in points.\",\"window – Window function type.\",\"n_iter – The number of iterations.\",\"Returns: Reconstructed waveform (N,).\"]},\"2491\":{\"h\":\"espnet2.utils.types.humanfriendly_parse_size_or_none\",\"t\":[\"source\",\"espnet2.utils.types.humanfriendly_parse_size_or_none(value) → float | None\"]},\"2492\":{\"h\":\"espnet2.utils.types.int_or_none\",\"t\":[\"source\",\"espnet2.utils.types.int_or_none(value: str) → int | None\",\"int_or_none.\"]},\"2493\":{\"h\":\"Examples\",\"t\":[\">>> import argparse >>> parser = argparse.ArgumentParser() >>> _ = parser.add_argument('--foo', type=int_or_none) >>> parser.parse_args(['--foo', '456']) Namespace(foo=456) >>> parser.parse_args(['--foo', 'none']) Namespace(foo=None) >>> parser.parse_args(['--foo', 'null']) Namespace(foo=None) >>> parser.parse_args(['--foo', 'nil']) Namespace(foo=None)\"]},\"2494\":{\"h\":\"espnet2.utils.kwargs2args.kwargs2args\",\"t\":[\"source\",\"espnet2.utils.kwargs2args.kwargs2args(func, kwargs)\"]},\"2495\":{\"h\":\"espnet2.utils.griffin_lim.logmel2linear\",\"t\":[\"source\",\"espnet2.utils.griffin_lim.logmel2linear(lmspc: ndarray, fs: int, n_fft: int, n_mels: int, fmin: int | None = None, fmax: int | None = None) → ndarray\",\"Convert log Mel filterbank to linear spectrogram.\",\"Parameters:\",\"lmspc – Log Mel filterbank (T, n_mels).\",\"fs – Sampling frequency.\",\"n_fft – The number of FFT points.\",\"n_mels – The number of mel basis.\",\"f_min – Minimum frequency to analyze.\",\"f_max – Maximum frequency to analyze.\",\"Returns: Linear spectrogram (T, n_fft // 2 + 1).\"]},\"2496\":{\"h\":\"espnet2.utils.types.remove_parenthesis\",\"t\":[\"source\",\"espnet2.utils.types.remove_parenthesis(value: str)\"]},\"2497\":{\"h\":\"espnet2.utils.types.remove_quotes\",\"t\":[\"source\",\"espnet2.utils.types.remove_quotes(value: str)\"]},\"2498\":{\"h\":\"espnet2.utils.types.str2bool\",\"t\":[\"source\",\"espnet2.utils.types.str2bool(value: str) → bool\"]},\"2499\":{\"h\":\"espnet2.utils.types.str2pair_str\",\"t\":[\"source\",\"espnet2.utils.types.str2pair_str(value: str) → Tuple[str, str]\",\"str2pair_str.\"]},\"2500\":{\"h\":\"Examples\",\"t\":[\">>> import argparse >>> str2pair_str('abc,def ') ('abc', 'def') >>> parser = argparse.ArgumentParser() >>> _ = parser.add_argument('--foo', type=str2pair_str) >>> parser.parse_args(['--foo', 'abc,def']) Namespace(foo=('abc', 'def'))\"]},\"2501\":{\"h\":\"espnet2.utils.types.str2triple_str\",\"t\":[\"source\",\"espnet2.utils.types.str2triple_str(value: str) → Tuple[str, str, str]\",\"str2triple_str.\"]},\"2502\":{\"h\":\"Examples\",\"t\":[\">>> str2triple_str('abc,def ,ghi') ('abc', 'def', 'ghi')\"]},\"2503\":{\"h\":\"espnet2.utils.types.str_or_int\",\"t\":[\"source\",\"espnet2.utils.types.str_or_int(value: str) → str | int\"]},\"2504\":{\"h\":\"espnet2.utils.types.str_or_none\",\"t\":[\"source\",\"espnet2.utils.types.str_or_none(value: str) → str | None\",\"str_or_none.\"]},\"2505\":{\"h\":\"Examples\",\"t\":[\">>> import argparse >>> parser = argparse.ArgumentParser() >>> _ = parser.add_argument('--foo', type=str_or_none) >>> parser.parse_args(['--foo', 'aaa']) Namespace(foo='aaa') >>> parser.parse_args(['--foo', 'none']) Namespace(foo=None) >>> parser.parse_args(['--foo', 'null']) Namespace(foo=None) >>> parser.parse_args(['--foo', 'nil']) Namespace(foo=None)\"]},\"2506\":{\"h\":\"espnet2.utils.eer.tuneThresholdfromScore\",\"t\":[\"source\",\"espnet2.utils.eer.tuneThresholdfromScore(scores, labels, target_fa, target_fr=None)\"]},\"2507\":{\"h\":\"espnet2.utils.yaml_no_alias_safe_dump.yaml_no_alias_safe_dump\",\"t\":[\"source\",\"espnet2.utils.yaml_no_alias_safe_dump.yaml_no_alias_safe_dump(data, stream=None, **kwargs)\",\"Safe-dump in yaml with no anchor/alias.\"]},\"2508\":{\"h\":\"\",\"t\":[\"404 Not Found\"]},\"2509\":{\"h\":\"Espnet2 Bin\"},\"2510\":{\"h\":\"Tools\"},\"2511\":{\"h\":\"Spm\"},\"2512\":{\"h\":\"Utils\"},\"2513\":{\"h\":\"Utils Py\"},\"2514\":{\"h\":\"Asr Transducer\"},\"2515\":{\"h\":\"Espnet2\"},\"2516\":{\"h\":\"Guide\"},\"2517\":{\"h\":\"Asr\"},\"2518\":{\"h\":\"Asvspoof\"},\"2519\":{\"h\":\"Cls\"},\"2520\":{\"h\":\"Diar\"},\"2521\":{\"h\":\"Fileio\"},\"2522\":{\"h\":\"Enh\"},\"2523\":{\"h\":\"Fst\"},\"2524\":{\"h\":\"Gan Codec\"},\"2525\":{\"h\":\"Gan Svs\"},\"2526\":{\"h\":\"Gan Tts\"},\"2527\":{\"h\":\"Hubert\"},\"2528\":{\"h\":\"Iterators\"},\"2529\":{\"h\":\"Layers\"},\"2530\":{\"h\":\"Lid\"},\"2531\":{\"h\":\"Legacy\"},\"2532\":{\"h\":\"Lm\"},\"2533\":{\"h\":\"Main Funcs\"},\"2534\":{\"h\":\"Mt\"},\"2535\":{\"h\":\"Optimizers\"},\"2536\":{\"h\":\"Ps2st\"},\"2537\":{\"h\":\"S2st\"},\"2538\":{\"h\":\"S2t\"},\"2539\":{\"h\":\"Samplers\"},\"2540\":{\"h\":\"Schedulers\"},\"2541\":{\"h\":\"Sds\"},\"2542\":{\"h\":\"Slu\"},\"2543\":{\"h\":\"Speechlm\"},\"2544\":{\"h\":\"Spk\"},\"2545\":{\"h\":\"Ssl\"},\"2546\":{\"h\":\"St\"},\"2547\":{\"h\":\"Svs\"},\"2548\":{\"h\":\"Tasks\"},\"2549\":{\"h\":\"Text\"},\"2550\":{\"h\":\"Torch Utils\"},\"2551\":{\"h\":\"Train\"},\"2552\":{\"h\":\"Tts\"},\"2553\":{\"h\":\"Tts2\"},\"2554\":{\"h\":\"Uasr\"},\"2555\":{\"h\":\"Utils\"}},\"dirtCount\":0,\"index\":[[\"｜\",{\"1\":{\"2235\":1}}],[\"≥\",{\"1\":{\"2000\":1,\"2001\":1}}],[\"×\",{\"1\":{\"2000\":3,\"2183\":2,\"2208\":1}}],[\"β\",{\"1\":{\"2000\":4,\"2001\":5}}],[\"∝\",{\"1\":{\"2000\":1}}],[\"∑\",{\"1\":{\"2000\":6}}],[\"∈\",{\"1\":{\"2000\":2,\"2001\":2}}],[\"啊\",{\"1\":{\"1025\":1}}],[\"→\",{\"1\":{\"614\":7,\"615\":1,\"616\":9,\"617\":3,\"618\":3,\"619\":1,\"620\":5,\"621\":1,\"622\":1,\"623\":1,\"624\":3,\"625\":3,\"626\":3,\"627\":3,\"629\":1,\"630\":2,\"632\":1,\"633\":3,\"634\":9,\"635\":1,\"636\":3,\"637\":6,\"638\":2,\"639\":4,\"640\":1,\"641\":8,\"642\":1,\"643\":8,\"644\":6,\"645\":2,\"646\":2,\"647\":4,\"648\":1,\"649\":3,\"650\":1,\"651\":7,\"652\":1,\"654\":2,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"663\":1,\"664\":1,\"665\":1,\"666\":1,\"667\":1,\"668\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":1,\"673\":1,\"675\":2,\"676\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"691\":2,\"692\":5,\"696\":9,\"697\":8,\"699\":4,\"700\":2,\"702\":2,\"703\":1,\"706\":1,\"709\":2,\"710\":4,\"711\":4,\"712\":1,\"720\":1,\"721\":1,\"733\":2,\"734\":2,\"736\":1,\"737\":2,\"738\":1,\"739\":1,\"740\":3,\"745\":2,\"746\":3,\"747\":2,\"748\":2,\"752\":1,\"753\":1,\"755\":1,\"759\":2,\"760\":2,\"761\":2,\"762\":2,\"765\":2,\"768\":2,\"770\":2,\"771\":2,\"772\":2,\"774\":2,\"775\":1,\"776\":1,\"777\":2,\"778\":1,\"779\":1,\"780\":2,\"785\":1,\"787\":1,\"790\":3,\"791\":1,\"792\":4,\"795\":1,\"798\":1,\"799\":1,\"815\":1,\"816\":1,\"820\":2,\"831\":2,\"832\":1,\"846\":2,\"847\":7,\"849\":2,\"850\":3,\"854\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":3,\"900\":1,\"907\":1,\"908\":1,\"929\":1,\"950\":1,\"951\":1,\"952\":1,\"954\":3,\"956\":1,\"958\":4,\"959\":2,\"962\":1,\"963\":1,\"965\":1,\"967\":1,\"968\":1,\"969\":1,\"974\":3,\"978\":1,\"980\":1,\"992\":1,\"994\":1,\"997\":1,\"1000\":1,\"1002\":1,\"1004\":1,\"1008\":1,\"1012\":1,\"1016\":1,\"1019\":1,\"1021\":1,\"1022\":1,\"1024\":1,\"1026\":1,\"1028\":1,\"1030\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1039\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1053\":1,\"1054\":1,\"1062\":1,\"1064\":1,\"1066\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1119\":1,\"1125\":1,\"1126\":2,\"1127\":2,\"1130\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1155\":3,\"1156\":5,\"1157\":4,\"1158\":4,\"1162\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1175\":1,\"1199\":1,\"1202\":1,\"1217\":1,\"1232\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1252\":1,\"1261\":1,\"1262\":1,\"1267\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1275\":1,\"1277\":1,\"1278\":1,\"1280\":1,\"1283\":1,\"1290\":1,\"1291\":1,\"1298\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1314\":1,\"1315\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1322\":1,\"1325\":1,\"1334\":1,\"1337\":1,\"1342\":1,\"1351\":1,\"1352\":1,\"1356\":1,\"1376\":1,\"1377\":1,\"1381\":4,\"1389\":5,\"1390\":1,\"1395\":3,\"1397\":1,\"1401\":5,\"1402\":1,\"1406\":1,\"1408\":5,\"1409\":1,\"1419\":1,\"1439\":2,\"1441\":4,\"1466\":5,\"1467\":1,\"1476\":1,\"1480\":1,\"1484\":1,\"1485\":1,\"1508\":1,\"1509\":1,\"1511\":1,\"1513\":1,\"1521\":3,\"1522\":1,\"1523\":1,\"1526\":2,\"1533\":1,\"1539\":1,\"1540\":1,\"1546\":1,\"1551\":1,\"1552\":2,\"1553\":2,\"1558\":1,\"1576\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":2,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1591\":1,\"1592\":2,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":2,\"1599\":2,\"1600\":2,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":2,\"1606\":1,\"1607\":1,\"1608\":2,\"1609\":1,\"1610\":2,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":2,\"1620\":1,\"1621\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1625\":2,\"1626\":2,\"1627\":1,\"1628\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1640\":3,\"1641\":3,\"1642\":1,\"1644\":1,\"1645\":1,\"1647\":1,\"1648\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1656\":2,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1666\":1,\"1668\":5,\"1669\":2,\"1671\":1,\"1700\":1,\"1702\":5,\"1719\":9,\"1720\":3,\"1721\":2,\"1723\":1,\"1724\":2,\"1725\":11,\"1726\":4,\"1727\":4,\"1736\":3,\"1749\":6,\"1766\":1,\"1779\":1,\"1787\":1,\"1805\":1,\"1806\":6,\"1807\":1,\"1815\":7,\"1822\":4,\"1843\":5,\"1851\":2,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1870\":1,\"1871\":1,\"1888\":1,\"1891\":1,\"1897\":1,\"1910\":1,\"1911\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1927\":1,\"1936\":1,\"1937\":1,\"1938\":1,\"1940\":3,\"1941\":1,\"1942\":3,\"1943\":1,\"1944\":3,\"1945\":2,\"1946\":1,\"1947\":3,\"1948\":1,\"1949\":1,\"1950\":1,\"1951\":1,\"1953\":1,\"1955\":1,\"1958\":1,\"1959\":3,\"1960\":2,\"1961\":2,\"1965\":3,\"1967\":1,\"1969\":1,\"1971\":2,\"1972\":1,\"1973\":3,\"1975\":3,\"1976\":1,\"1978\":1,\"1979\":3,\"1980\":1,\"1981\":3,\"1982\":1,\"1983\":3,\"1992\":3,\"1993\":2,\"1995\":1,\"1996\":2,\"1997\":3,\"2001\":2,\"2007\":1,\"2008\":1,\"2016\":1,\"2039\":3,\"2040\":1,\"2043\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2065\":1,\"2066\":1,\"2101\":1,\"2124\":1,\"2125\":1,\"2126\":2,\"2127\":3,\"2128\":2,\"2129\":2,\"2130\":10,\"2131\":2,\"2132\":1,\"2133\":5,\"2134\":1,\"2136\":9,\"2137\":8,\"2142\":2,\"2145\":1,\"2146\":1,\"2147\":1,\"2149\":1,\"2150\":1,\"2151\":1,\"2152\":1,\"2154\":1,\"2155\":1,\"2156\":1,\"2160\":1,\"2161\":1,\"2162\":1,\"2163\":1,\"2167\":1,\"2170\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2175\":1,\"2176\":1,\"2183\":1,\"2184\":6,\"2187\":1,\"2189\":1,\"2190\":1,\"2191\":2,\"2193\":1,\"2198\":1,\"2199\":1,\"2204\":1,\"2207\":1,\"2208\":1,\"2210\":1,\"2215\":1,\"2216\":3,\"2217\":1,\"2218\":1,\"2219\":1,\"2220\":1,\"2221\":3,\"2222\":2,\"2228\":3,\"2229\":3,\"2232\":4,\"2235\":2,\"2236\":2,\"2238\":3,\"2239\":2,\"2240\":2,\"2241\":1,\"2245\":2,\"2246\":5,\"2247\":5,\"2248\":5,\"2249\":20,\"2250\":5,\"2251\":5,\"2252\":5,\"2253\":6,\"2254\":6,\"2255\":6,\"2256\":6,\"2257\":5,\"2259\":5,\"2260\":5,\"2261\":5,\"2262\":5,\"2263\":5,\"2264\":4,\"2265\":4,\"2266\":5,\"2267\":5,\"2268\":5,\"2269\":5,\"2270\":5,\"2271\":5,\"2272\":5,\"2273\":6,\"2274\":2,\"2275\":2,\"2278\":3,\"2279\":2,\"2283\":3,\"2284\":2,\"2285\":3,\"2287\":3,\"2288\":2,\"2291\":3,\"2292\":2,\"2293\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":1,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2310\":1,\"2313\":1,\"2316\":1,\"2317\":1,\"2322\":1,\"2324\":2,\"2325\":2,\"2327\":2,\"2329\":1,\"2330\":1,\"2331\":1,\"2334\":2,\"2338\":4,\"2343\":2,\"2344\":2,\"2347\":3,\"2352\":2,\"2354\":1,\"2359\":15,\"2365\":2,\"2366\":2,\"2367\":5,\"2369\":5,\"2371\":3,\"2374\":1,\"2376\":1,\"2378\":1,\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2388\":1,\"2389\":1,\"2398\":1,\"2401\":1,\"2402\":2,\"2403\":2,\"2405\":1,\"2406\":2,\"2407\":1,\"2408\":3,\"2409\":1,\"2410\":2,\"2411\":2,\"2412\":2,\"2413\":1,\"2414\":1,\"2415\":2,\"2416\":1,\"2417\":2,\"2418\":1,\"2419\":2,\"2420\":1,\"2422\":1,\"2423\":2,\"2424\":1,\"2425\":1,\"2426\":1,\"2428\":4,\"2429\":1,\"2430\":1,\"2431\":2,\"2432\":2,\"2433\":1,\"2434\":1,\"2435\":3,\"2437\":1,\"2439\":1,\"2441\":1,\"2442\":1,\"2443\":1,\"2445\":2,\"2446\":3,\"2447\":2,\"2448\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2454\":1,\"2455\":2,\"2456\":1,\"2462\":3,\"2463\":2,\"2464\":2,\"2484\":1,\"2490\":1,\"2491\":1,\"2492\":1,\"2495\":1,\"2498\":1,\"2499\":1,\"2501\":1,\"2503\":1,\"2504\":1}}],[\"世界\",{\"1\":{\"290\":1}}],[\"ー\",{\"1\":{\"288\":1}}],[\"ねじ曲げたのだ\",{\"1\":{\"288\":2}}],[\"ほうへ\",{\"1\":{\"288\":2}}],[\"自分の\",{\"1\":{\"288\":2}}],[\"すべて\",{\"1\":{\"288\":2}}],[\"現実を\",{\"1\":{\"288\":2}}],[\"あらゆる\",{\"1\":{\"288\":2}}],[\"ᆸ\",{\"1\":{\"287\":2}}],[\"ᆨᆩᆪᆫᆬᆭᆮᆯᆰᆱᆲᆳᆴᆵᆶᆷᆸᆹᆺᆻᆼᆽᆾᆿᇀᇁᇂ\",{\"1\":{\"2281\":1}}],[\"ᆨ\",{\"1\":{\"287\":2}}],[\"ᅳ\",{\"1\":{\"287\":2}}],[\"갑니다\",{\"1\":{\"287\":2}}],[\"학교에\",{\"1\":{\"287\":2}}],[\"나는\",{\"1\":{\"287\":2}}],[\"ᄃ\",{\"1\":{\"287\":5}}],[\"ᆷ\",{\"1\":{\"287\":3}}],[\"ᅵ\",{\"1\":{\"287\":8}}],[\"ᅨ\",{\"1\":{\"287\":3}}],[\"ᄀᄁᄂᄃᄄᄅᄆᄇᄈᄉᄊᄋᄌᄍᄎᄏᄐᄑ하ᅢᅣᅤᅥᅦᅧᅨᅩᅪᅫᅬᅭᅮᅯᅰᅱᅲᅳᅴᅵᆨᆩᆪᆫᆬᆭᆮᆯᆰᆱᆲᆳᆴᆵᆶᆷᆸᆹᆺᆻᆼᆽᆾᆿᇀᇁᇂ\",{\"1\":{\"2281\":1}}],[\"ᄀᄁᄂᄃᄄᄅᄆᄇᄈᄉᄊᄋᄌᄍᄎᄏᄐᄑᄒ\",{\"1\":{\"2281\":1}}],[\"ᄀ\",{\"1\":{\"287\":7}}],[\"ᅭ\",{\"1\":{\"287\":5}}],[\"ᅦ\",{\"1\":{\"287\":8}}],[\"ᄉ\",{\"1\":{\"287\":6}}],[\"ᄒ\",{\"1\":{\"287\":5}}],[\"ᆼ\",{\"1\":{\"287\":3}}],[\"ᅧ\",{\"1\":{\"287\":3}}],[\"ᄂ\",{\"1\":{\"287\":12}}],[\"ᆫ\",{\"1\":{\"287\":5}}],[\"ᅡᅢᅣᅤᅥᅦᅧᅨᅩᅪᅫᅬᅭᅮᅯᅰᅱᅲᅳᅴᅵ\",{\"1\":{\"2281\":1}}],[\"ᅡ\",{\"1\":{\"287\":17}}],[\"ᄋ\",{\"1\":{\"287\":11}}],[\"세계입니다\",{\"1\":{\"287\":3}}],[\"안녕하세요\",{\"1\":{\"287\":3}}],[\"ː\",{\"1\":{\"287\":1}}],[\"ɜ\",{\"1\":{\"287\":1}}],[\"ʊ\",{\"1\":{\"287\":1}}],[\"ɛ\",{\"1\":{\"287\":1}}],[\"ɕ\",{\"1\":{\"287\":1}}],[\"świecie\",{\"1\":{\"287\":1}}],[\"ˌaː\",{\"1\":{\"287\":1}}],[\"दुनिया\",{\"1\":{\"287\":1}}],[\"नमस्ते\",{\"1\":{\"287\":1}}],[\"ʋ\",{\"1\":{\"287\":1}}],[\"κόσμε\",{\"1\":{\"287\":1}}],[\"σου\",{\"1\":{\"287\":1}}],[\"γειά\",{\"1\":{\"287\":1}}],[\"мир\",{\"1\":{\"287\":1}}],[\"привет\",{\"1\":{\"287\":1}}],[\"ə\",{\"1\":{\"287\":4}}],[\"ʁ\",{\"1\":{\"287\":1}}],[\"ʒ\",{\"1\":{\"287\":1}}],[\"ɔ̃\",{\"1\":{\"287\":1}}],[\"ɡ\",{\"1\":{\"287\":2}}],[\"ç\",{\"1\":{\"287\":1}}],[\"ɪ\",{\"1\":{\"287\":3}}],[\"ɾ\",{\"1\":{\"287\":1}}],[\"ˈ\",{\"1\":{\"287\":2}}],[\"ˈʊ\",{\"1\":{\"287\":1}}],[\"ˈʌ\",{\"1\":{\"287\":1}}],[\"ˈɪː\",{\"1\":{\"287\":1}}],[\"ˈɑ\",{\"1\":{\"287\":1}}],[\"ˈɛ\",{\"1\":{\"287\":2}}],[\"ˈi\",{\"1\":{\"287\":3}}],[\"ˈiː\",{\"1\":{\"287\":1}}],[\"ˈei\",{\"1\":{\"287\":1}}],[\"ˈe\",{\"1\":{\"287\":1}}],[\"ˈo\",{\"1\":{\"287\":3}}],[\"ˈɔ̃\",{\"1\":{\"287\":1}}],[\"ˈu\",{\"1\":{\"287\":2}}],[\"ˈuː\",{\"1\":{\"287\":1}}],[\"ˈœ\",{\"1\":{\"287\":1}}],[\"ˈaː\",{\"1\":{\"287\":1}}],[\"ˈa\",{\"1\":{\"287\":4}}],[\"ʕ\",{\"1\":{\"287\":1}}],[\"ʔ\",{\"1\":{\"287\":1}}],[\"عليكم\",{\"1\":{\"287\":1}}],[\"السلام\",{\"1\":{\"287\":1}}],[\"卡尔普陪外孙玩滑梯\",{\"1\":{\"287\":2}}],[\"^β\",{\"1\":{\"2000\":5,\"2001\":2}}],[\"^t\",{\"1\":{\"1224\":1,\"1309\":1}}],[\"^\",{\"1\":{\"287\":1,\"705\":1,\"804\":1,\"932\":1,\"934\":1,\"1319\":1,\"1330\":1,\"2299\":1}}],[\"^virbr\",{\"1\":{\"67\":1}}],[\"ワ\",{\"1\":{\"287\":1}}],[\"チ\",{\"1\":{\"287\":1}}],[\"ニ\",{\"1\":{\"287\":1}}],[\"ン\",{\"1\":{\"287\":1}}],[\"コ\",{\"1\":{\"287\":2}}],[\"こんにちは\",{\"1\":{\"271\":1,\"280\":1,\"287\":5,\"290\":1,\"2299\":1}}],[\"こ\",{\"1\":{\"271\":1,\"280\":1,\"287\":5}}],[\"情意深重爱恨两重\",{\"1\":{\"269\":1,\"278\":1}}],[\"重\",{\"1\":{\"269\":1,\"278\":1}}],[\"ヴぁ\",{\"1\":{\"269\":3,\"278\":3}}],[\"└──\",{\"1\":{\"213\":1,\"218\":9,\"223\":25,\"224\":1,\"227\":13,\"228\":6,\"267\":11,\"268\":2,\"276\":14,\"277\":2,\"286\":11}}],[\"│\",{\"1\":{\"213\":6,\"218\":17,\"223\":99,\"227\":23,\"228\":7,\"267\":30,\"268\":10,\"276\":44,\"277\":10,\"286\":32}}],[\"├──\",{\"1\":{\"213\":7,\"218\":21,\"223\":55,\"224\":3,\"227\":20,\"228\":7,\"267\":36,\"268\":10,\"276\":40,\"277\":10,\"286\":37}}],[\"~espnet2\",{\"1\":{\"1726\":1,\"1727\":1}}],[\"~wang\",{\"1\":{\"1125\":1}}],[\"~pathlib\",{\"1\":{\"991\":1,\"1003\":1,\"1015\":1}}],[\"~typing\",{\"1\":{\"617\":1,\"618\":1,\"622\":1,\"624\":1,\"636\":2,\"642\":1,\"692\":1,\"731\":1,\"732\":1,\"766\":1,\"767\":1,\"848\":1,\"1726\":2,\"1727\":2,\"2198\":1,\"2249\":1}}],[\"~torch\",{\"1\":{\"617\":3,\"618\":5,\"622\":1,\"624\":6,\"633\":2,\"636\":2,\"638\":2,\"642\":1,\"1475\":1,\"2198\":1}}],[\"~\",{\"1\":{\"211\":2,\"266\":2,\"275\":2,\"706\":1}}],[\"~1\",{\"1\":{\"196\":1,\"268\":1,\"277\":1}}],[\"~55\",{\"1\":{\"71\":1}}],[\"`mish\",{\"1\":{\"2420\":1}}],[\"`~lightning\",{\"1\":{\"2355\":2}}],[\"`learningratemonitor`\",{\"1\":{\"2355\":1}}],[\"`false`\",{\"1\":{\"2355\":1}}],[\"`true`\",{\"1\":{\"2355\":1}}],[\"`translatotron\",{\"1\":{\"1994\":1}}],[\"`reducelronplateau`\",{\"1\":{\"2355\":1}}],[\"`scheduler\",{\"1\":{\"2355\":1}}],[\"`singing\",{\"1\":{\"2224\":1,\"2231\":1,\"2245\":1}}],[\"`short\",{\"1\":{\"147\":2}}],[\"`https\",{\"1\":{\"2223\":1}}],[\"`direct\",{\"1\":{\"1995\":1}}],[\"`data`\",{\"1\":{\"195\":2}}],[\"`\",{\"1\":{\"960\":2,\"1301\":2,\"1306\":2,\"1371\":2,\"1372\":2,\"1598\":1,\"1725\":1,\"2355\":1}}],[\"``\",{\"1\":{\"922\":1,\"960\":2,\"2187\":1,\"2191\":1}}],[\"`parallel\",{\"1\":{\"267\":1,\"276\":1,\"286\":1}}],[\"`batch\",{\"1\":{\"195\":2}}],[\"`validate\",{\"1\":{\"150\":1}}],[\"λ\",{\"1\":{\"144\":3}}],[\"||rtf||\",{\"1\":{\"1328\":1}}],[\"|c<out\",{\"1\":{\"74\":1}}],[\"|\",{\"1\":{\"74\":2,\"75\":2,\"213\":1,\"227\":13,\"285\":50,\"286\":1,\"374\":1,\"536\":189,\"545\":1,\"614\":25,\"616\":1,\"617\":1,\"618\":1,\"619\":2,\"620\":6,\"621\":3,\"622\":2,\"623\":2,\"624\":1,\"625\":4,\"628\":6,\"630\":2,\"631\":8,\"633\":9,\"634\":1,\"636\":2,\"637\":5,\"641\":10,\"642\":2,\"643\":2,\"644\":2,\"649\":2,\"651\":4,\"660\":1,\"661\":2,\"665\":1,\"666\":2,\"669\":1,\"674\":2,\"678\":2,\"686\":2,\"692\":3,\"696\":4,\"697\":4,\"699\":9,\"700\":6,\"701\":2,\"702\":3,\"706\":1,\"709\":6,\"710\":7,\"711\":7,\"712\":3,\"720\":6,\"733\":6,\"734\":6,\"735\":2,\"736\":8,\"737\":7,\"738\":3,\"743\":8,\"745\":2,\"746\":2,\"747\":2,\"748\":2,\"752\":1,\"755\":1,\"759\":2,\"760\":3,\"763\":11,\"765\":2,\"768\":2,\"770\":2,\"771\":3,\"774\":3,\"777\":7,\"778\":1,\"780\":5,\"785\":1,\"786\":2,\"787\":9,\"790\":2,\"791\":4,\"792\":2,\"794\":1,\"795\":6,\"798\":2,\"800\":2,\"815\":3,\"824\":2,\"831\":1,\"832\":2,\"833\":5,\"846\":7,\"847\":12,\"849\":3,\"850\":7,\"851\":2,\"854\":2,\"862\":1,\"864\":2,\"865\":2,\"908\":1,\"921\":2,\"935\":2,\"954\":6,\"955\":1,\"958\":6,\"974\":9,\"978\":2,\"980\":1,\"985\":1,\"987\":2,\"989\":3,\"991\":1,\"996\":1,\"998\":2,\"1003\":1,\"1005\":2,\"1009\":3,\"1013\":1,\"1015\":1,\"1017\":2,\"1019\":2,\"1021\":1,\"1022\":3,\"1024\":3,\"1026\":2,\"1028\":2,\"1030\":1,\"1031\":1,\"1034\":1,\"1040\":1,\"1044\":1,\"1053\":3,\"1054\":1,\"1062\":5,\"1065\":1,\"1107\":3,\"1112\":2,\"1113\":1,\"1117\":3,\"1118\":4,\"1125\":3,\"1126\":5,\"1127\":4,\"1130\":3,\"1131\":3,\"1136\":3,\"1141\":3,\"1155\":1,\"1156\":3,\"1157\":12,\"1158\":2,\"1162\":1,\"1199\":1,\"1204\":2,\"1217\":3,\"1222\":1,\"1223\":1,\"1232\":3,\"1250\":2,\"1251\":2,\"1252\":1,\"1261\":3,\"1267\":3,\"1268\":3,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":3,\"1280\":7,\"1283\":5,\"1291\":3,\"1294\":1,\"1298\":1,\"1308\":6,\"1309\":3,\"1310\":3,\"1311\":6,\"1314\":3,\"1315\":2,\"1317\":3,\"1318\":4,\"1319\":5,\"1322\":6,\"1323\":1,\"1327\":1,\"1328\":1,\"1330\":1,\"1334\":3,\"1337\":2,\"1342\":3,\"1348\":1,\"1351\":3,\"1352\":3,\"1355\":1,\"1356\":3,\"1357\":2,\"1359\":1,\"1367\":1,\"1376\":2,\"1377\":2,\"1381\":2,\"1387\":2,\"1391\":3,\"1403\":5,\"1406\":2,\"1410\":1,\"1419\":3,\"1432\":2,\"1439\":3,\"1441\":4,\"1444\":3,\"1448\":3,\"1450\":2,\"1452\":2,\"1468\":3,\"1469\":1,\"1483\":1,\"1508\":2,\"1513\":1,\"1521\":69,\"1526\":19,\"1533\":2,\"1539\":4,\"1546\":1,\"1548\":1,\"1551\":1,\"1552\":29,\"1553\":23,\"1558\":1,\"1576\":2,\"1581\":2,\"1582\":1,\"1583\":1,\"1584\":4,\"1585\":24,\"1586\":1,\"1587\":2,\"1588\":1,\"1591\":2,\"1592\":2,\"1598\":6,\"1599\":13,\"1603\":1,\"1607\":5,\"1610\":2,\"1611\":1,\"1612\":1,\"1613\":2,\"1616\":2,\"1619\":1,\"1624\":1,\"1625\":9,\"1626\":13,\"1628\":3,\"1640\":5,\"1641\":5,\"1642\":1,\"1643\":6,\"1644\":1,\"1645\":4,\"1646\":6,\"1647\":1,\"1648\":1,\"1650\":3,\"1652\":1,\"1655\":1,\"1656\":3,\"1659\":1,\"1660\":2,\"1662\":4,\"1664\":3,\"1665\":3,\"1668\":1,\"1669\":7,\"1670\":1,\"1671\":1,\"1677\":3,\"1680\":2,\"1683\":1,\"1692\":2,\"1698\":2,\"1700\":1,\"1702\":12,\"1719\":6,\"1720\":1,\"1721\":3,\"1725\":7,\"1736\":6,\"1749\":11,\"1762\":8,\"1766\":3,\"1775\":11,\"1806\":1,\"1807\":3,\"1815\":12,\"1822\":1,\"1843\":27,\"1847\":1,\"1851\":2,\"1862\":1,\"1863\":2,\"1866\":2,\"1867\":1,\"1870\":2,\"1871\":2,\"1881\":1,\"1883\":1,\"1891\":3,\"1919\":2,\"1921\":2,\"1934\":1,\"1941\":1,\"1942\":1,\"1943\":1,\"1945\":1,\"1946\":2,\"1947\":1,\"1948\":1,\"1949\":2,\"1951\":3,\"1953\":2,\"1954\":4,\"1955\":2,\"1959\":5,\"1965\":3,\"1975\":30,\"1976\":6,\"1978\":3,\"1980\":7,\"1982\":3,\"1992\":7,\"1993\":11,\"1994\":3,\"1995\":6,\"1996\":4,\"1997\":7,\"1998\":1,\"1999\":1,\"2000\":12,\"2001\":5,\"2002\":2,\"2003\":1,\"2004\":1,\"2006\":1,\"2007\":2,\"2008\":13,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2015\":1,\"2016\":2,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2044\":4,\"2065\":1,\"2101\":1,\"2127\":18,\"2129\":3,\"2130\":6,\"2133\":1,\"2134\":5,\"2136\":10,\"2139\":1,\"2144\":1,\"2150\":1,\"2155\":1,\"2157\":1,\"2160\":1,\"2167\":3,\"2176\":3,\"2183\":1,\"2184\":13,\"2190\":1,\"2191\":4,\"2198\":1,\"2207\":3,\"2208\":1,\"2215\":1,\"2216\":10,\"2219\":4,\"2220\":1,\"2221\":21,\"2226\":3,\"2228\":75,\"2229\":70,\"2232\":9,\"2235\":25,\"2236\":26,\"2238\":13,\"2239\":29,\"2240\":25,\"2245\":27,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":14,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":2,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":2,\"2258\":2,\"2259\":1,\"2260\":2,\"2261\":1,\"2262\":1,\"2263\":6,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":6,\"2269\":1,\"2270\":6,\"2271\":6,\"2272\":1,\"2273\":1,\"2275\":4,\"2278\":1,\"2279\":1,\"2283\":3,\"2284\":1,\"2285\":4,\"2286\":3,\"2287\":1,\"2288\":1,\"2289\":1,\"2291\":3,\"2292\":4,\"2293\":12,\"2308\":2,\"2327\":2,\"2329\":2,\"2331\":1,\"2332\":8,\"2334\":5,\"2335\":1,\"2336\":22,\"2337\":21,\"2338\":1,\"2339\":10,\"2340\":12,\"2341\":4,\"2342\":5,\"2344\":1,\"2346\":6,\"2347\":2,\"2348\":5,\"2350\":1,\"2351\":4,\"2353\":4,\"2354\":2,\"2356\":17,\"2359\":12,\"2360\":16,\"2361\":16,\"2362\":20,\"2363\":14,\"2364\":3,\"2366\":2,\"2367\":19,\"2368\":8,\"2369\":8,\"2370\":10,\"2371\":2,\"2372\":5,\"2373\":24,\"2374\":4,\"2376\":1,\"2381\":2,\"2382\":2,\"2383\":1,\"2384\":2,\"2385\":2,\"2386\":2,\"2387\":1,\"2398\":9,\"2404\":2,\"2405\":4,\"2408\":31,\"2409\":7,\"2411\":11,\"2412\":13,\"2414\":3,\"2416\":7,\"2418\":3,\"2422\":3,\"2423\":13,\"2428\":3,\"2431\":11,\"2432\":10,\"2433\":1,\"2434\":4,\"2446\":29,\"2447\":12,\"2453\":1,\"2458\":2,\"2460\":2,\"2462\":14,\"2463\":1,\"2481\":1,\"2482\":7,\"2484\":1,\"2490\":3,\"2491\":1,\"2492\":1,\"2495\":2,\"2503\":1,\"2504\":1}}],[\"qwen\",{\"1\":{\"1965\":1,\"2133\":2,\"2287\":1}}],[\"qwen2audiopreprocessor\",{\"0\":{\"2357\":1},\"1\":{\"2357\":2}}],[\"qwen2audiotokenizer\",{\"0\":{\"2287\":1},\"1\":{\"2287\":1}}],[\"qwen2audio\",{\"0\":{\"2287\":1},\"1\":{\"2287\":1,\"2357\":2}}],[\"qwen2hfscorer\",{\"0\":{\"1966\":1},\"1\":{\"1966\":1}}],[\"qwen2\",{\"0\":{\"1966\":1},\"1\":{\"404\":1,\"1965\":3,\"1966\":1,\"2133\":2,\"2262\":1,\"2287\":4}}],[\"q^\",{\"1\":{\"924\":1}}],[\"qn\",{\"1\":{\"787\":1}}],[\"qmf\",{\"1\":{\"254\":2,\"449\":4,\"1608\":1}}],[\"q\",{\"1\":{\"168\":11,\"637\":2,\"644\":1,\"787\":1,\"924\":2,\"1182\":2,\"1183\":2,\"1184\":2,\"1269\":2,\"1270\":2,\"1271\":2,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1406\":1,\"1408\":1,\"1410\":1,\"1439\":3,\"1441\":3,\"1466\":1,\"1468\":1,\"1601\":2,\"1602\":4,\"1655\":3,\"1672\":4,\"1673\":3,\"1687\":3,\"1689\":3,\"1690\":3,\"1794\":1,\"2421\":1}}],[\"qsub\",{\"1\":{\"168\":4}}],[\"quotes\",{\"0\":{\"2497\":1},\"1\":{\"2497\":1}}],[\"quantile\",{\"1\":{\"1676\":4}}],[\"quantize=256\",{\"1\":{\"1854\":1}}],[\"quantizers\",{\"1\":{\"1439\":1,\"1441\":2}}],[\"quantizer\",{\"0\":{\"1400\":1,\"1432\":1,\"1439\":1,\"1441\":1,\"1469\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1501\":1,\"1502\":1,\"1504\":1,\"1507\":1},\"1\":{\"1389\":8,\"1391\":8,\"1396\":7,\"1400\":1,\"1401\":8,\"1403\":8,\"1406\":7,\"1408\":7,\"1410\":7,\"1432\":1,\"1439\":1,\"1441\":7,\"1466\":7,\"1468\":7,\"1469\":2,\"1473\":1,\"1474\":1,\"1475\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1501\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"2138\":1}}],[\"quantizedresult\",{\"0\":{\"1432\":1},\"1\":{\"1406\":1,\"1432\":1,\"1441\":2}}],[\"quantized\",{\"1\":{\"699\":1,\"1400\":1,\"1432\":2,\"1441\":3,\"1854\":2,\"1873\":2,\"1877\":2,\"1919\":2,\"2412\":1}}],[\"quantize\",{\"1\":{\"301\":7,\"321\":5,\"389\":5,\"421\":7,\"429\":7,\"436\":5,\"442\":7,\"498\":7,\"927\":1,\"1400\":1,\"1854\":2,\"2138\":1}}],[\"quantization\",{\"0\":{\"305\":1,\"393\":1,\"425\":1,\"433\":1,\"440\":1,\"446\":1,\"502\":1},\"1\":{\"276\":1,\"927\":7,\"1389\":1,\"1391\":1,\"1395\":1,\"1396\":1,\"1400\":1,\"1401\":1,\"1403\":1,\"1408\":1,\"1410\":1,\"1439\":1,\"1441\":1,\"1466\":1,\"1468\":1,\"1469\":1,\"1779\":3,\"1854\":1}}],[\"quant\",{\"0\":{\"927\":1},\"1\":{\"927\":1}}],[\"quadratic\",{\"0\":{\"1634\":1,\"1635\":1,\"1636\":1},\"1\":{\"262\":1,\"1634\":1,\"1635\":1,\"1636\":1}}],[\"quality\",{\"1\":{\"205\":1,\"246\":2,\"247\":1,\"254\":1,\"262\":3,\"267\":3,\"269\":1,\"276\":3,\"278\":1,\"285\":1,\"286\":3,\"1994\":2,\"2240\":1,\"2404\":1,\"2412\":1,\"2423\":1,\"2433\":1}}],[\"quite\",{\"1\":{\"232\":1,\"258\":1}}],[\"quickly\",{\"1\":{\"290\":1,\"2131\":1,\"2143\":1,\"2355\":1}}],[\"quickstart\",{\"1\":{\"92\":1}}],[\"quick\",{\"0\":{\"69\":1},\"1\":{\"220\":1}}],[\"queried\",{\"1\":{\"2345\":1}}],[\"quert\",{\"1\":{\"1756\":1}}],[\"query\",{\"1\":{\"142\":1,\"633\":4,\"634\":4,\"644\":14,\"784\":9,\"787\":2,\"1029\":1,\"1064\":1,\"1070\":1,\"1071\":1,\"1235\":1,\"1262\":1,\"1271\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1290\":1,\"1683\":2,\"1756\":3,\"1757\":4,\"1785\":3,\"1789\":4,\"1790\":4,\"1794\":8,\"1817\":3,\"1984\":3,\"2176\":1,\"2287\":2,\"2344\":1}}],[\"queue\",{\"1\":{\"166\":3,\"167\":1,\"168\":6,\"516\":2,\"520\":2,\"521\":2,\"523\":2,\"524\":2,\"525\":2}}],[\"qkv\",{\"1\":{\"644\":1,\"784\":1,\"1029\":2,\"1064\":2,\"1235\":2,\"1262\":2,\"1280\":2,\"1281\":2,\"1282\":2,\"1290\":2,\"1794\":1}}],[\"qk\",{\"1\":{\"142\":2,\"633\":4,\"634\":3,\"700\":1,\"709\":1,\"734\":1,\"848\":1,\"1029\":3,\"1064\":3,\"1182\":1,\"1183\":1,\"1184\":1,\"1235\":3,\"1262\":3,\"1269\":2,\"1270\":2,\"1271\":2,\"1280\":3,\"1281\":3,\"1282\":3,\"1290\":3,\"1794\":2}}],[\"qint8\",{\"1\":{\"301\":1,\"321\":1,\"389\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"498\":1}}],[\"qing\",{\"1\":{\"269\":1,\"278\":1}}],[\"qin\",{\"1\":{\"13\":1}}],[\"qian\",{\"1\":{\"11\":1,\"13\":1,\"1279\":1,\"1280\":2,\"1281\":2,\"1282\":2,\"1283\":1}}],[\"qiu\",{\"1\":{\"11\":1}}],[\"○\",{\"1\":{\"53\":1}}],[\"◎\",{\"1\":{\"53\":3}}],[\"+1\",{\"1\":{\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1}}],[\"+\",{\"0\":{\"341\":1},\"1\":{\"43\":1,\"45\":1,\"48\":2,\"98\":1,\"141\":2,\"144\":2,\"150\":1,\"218\":3,\"267\":3,\"276\":3,\"286\":10,\"535\":1,\"616\":1,\"629\":1,\"650\":1,\"674\":4,\"691\":1,\"692\":2,\"709\":2,\"710\":2,\"711\":2,\"736\":3,\"737\":4,\"774\":2,\"777\":3,\"780\":2,\"785\":2,\"819\":1,\"824\":1,\"849\":2,\"882\":2,\"883\":2,\"884\":4,\"919\":1,\"922\":4,\"928\":1,\"954\":5,\"974\":5,\"979\":3,\"1051\":3,\"1107\":2,\"1108\":2,\"1155\":4,\"1156\":5,\"1157\":4,\"1158\":3,\"1210\":1,\"1224\":1,\"1225\":2,\"1245\":2,\"1278\":2,\"1309\":1,\"1315\":1,\"1316\":2,\"1320\":1,\"1327\":1,\"1330\":1,\"1350\":2,\"1351\":1,\"1356\":1,\"1519\":1,\"1536\":1,\"1551\":1,\"1553\":1,\"1582\":1,\"1595\":2,\"1598\":1,\"1607\":1,\"1625\":1,\"1631\":1,\"1640\":3,\"1641\":3,\"1661\":1,\"1686\":1,\"1716\":1,\"1725\":1,\"1735\":2,\"1737\":1,\"1751\":2,\"1759\":2,\"1854\":1,\"1906\":1,\"1920\":1,\"1959\":4,\"1975\":1,\"1992\":2,\"1995\":2,\"1996\":2,\"1997\":4,\"2000\":2,\"2127\":4,\"2129\":2,\"2136\":1,\"2221\":4,\"2239\":2,\"2240\":2,\"2249\":1,\"2253\":1,\"2298\":2,\"2384\":1,\"2385\":1,\"2404\":2,\"2411\":4,\"2412\":12,\"2423\":12,\"2447\":12,\"2462\":4,\"2490\":1,\"2495\":1}}],[\"=>\",{\"1\":{\"1308\":2,\"1716\":1}}],[\"==\",{\"1\":{\"78\":1,\"137\":1,\"692\":1,\"775\":1,\"790\":1,\"820\":1,\"850\":1,\"988\":2,\"990\":2,\"994\":1,\"1157\":1,\"1270\":1,\"1280\":1,\"2235\":1,\"2236\":1,\"2249\":1,\"2253\":1,\"2355\":3,\"2378\":2}}],[\"=n\",{\"1\":{\"54\":2}}],[\"=\",{\"1\":{\"43\":21,\"44\":9,\"45\":1,\"46\":1,\"78\":2,\"79\":1,\"81\":5,\"82\":5,\"91\":2,\"99\":1,\"100\":1,\"102\":1,\"127\":1,\"139\":6,\"141\":57,\"142\":22,\"143\":2,\"144\":6,\"145\":5,\"147\":4,\"148\":2,\"156\":7,\"163\":1,\"202\":10,\"220\":2,\"225\":3,\"243\":1,\"286\":2,\"290\":18,\"614\":1,\"615\":1,\"616\":11,\"617\":5,\"618\":5,\"619\":5,\"620\":11,\"621\":4,\"622\":6,\"623\":3,\"624\":5,\"625\":16,\"626\":3,\"627\":3,\"628\":4,\"629\":5,\"630\":1,\"631\":2,\"632\":4,\"633\":21,\"634\":22,\"635\":4,\"636\":5,\"637\":9,\"638\":5,\"639\":1,\"640\":2,\"641\":7,\"642\":5,\"643\":13,\"644\":8,\"645\":4,\"646\":2,\"647\":3,\"648\":1,\"649\":1,\"650\":5,\"651\":5,\"652\":4,\"661\":14,\"664\":10,\"665\":1,\"666\":2,\"667\":2,\"669\":2,\"674\":66,\"678\":1,\"686\":1,\"691\":5,\"692\":13,\"693\":1,\"696\":15,\"697\":15,\"699\":16,\"700\":25,\"701\":3,\"702\":5,\"706\":8,\"709\":33,\"710\":27,\"711\":22,\"720\":14,\"730\":1,\"731\":14,\"732\":14,\"733\":30,\"734\":33,\"735\":1,\"736\":15,\"737\":14,\"738\":8,\"740\":2,\"743\":4,\"745\":5,\"746\":27,\"747\":20,\"748\":14,\"749\":2,\"752\":4,\"756\":1,\"759\":3,\"760\":7,\"761\":1,\"762\":2,\"763\":2,\"764\":2,\"765\":5,\"766\":14,\"767\":14,\"768\":10,\"770\":4,\"771\":6,\"772\":1,\"773\":1,\"774\":29,\"775\":11,\"777\":13,\"778\":11,\"780\":34,\"786\":3,\"787\":8,\"788\":1,\"790\":4,\"791\":8,\"792\":3,\"794\":1,\"795\":9,\"796\":9,\"798\":9,\"800\":2,\"801\":2,\"802\":2,\"815\":5,\"820\":8,\"831\":4,\"832\":2,\"833\":11,\"846\":38,\"847\":7,\"848\":15,\"849\":16,\"850\":21,\"851\":18,\"854\":3,\"862\":9,\"864\":4,\"865\":2,\"866\":1,\"867\":1,\"869\":14,\"921\":2,\"924\":2,\"935\":2,\"939\":1,\"947\":4,\"948\":1,\"949\":5,\"954\":2,\"958\":4,\"959\":2,\"962\":1,\"974\":7,\"976\":3,\"977\":1,\"978\":2,\"979\":4,\"980\":7,\"986\":3,\"987\":2,\"988\":3,\"989\":3,\"990\":3,\"991\":2,\"992\":2,\"993\":1,\"994\":2,\"997\":2,\"999\":3,\"1002\":2,\"1003\":1,\"1004\":2,\"1006\":3,\"1007\":2,\"1008\":4,\"1009\":4,\"1010\":5,\"1012\":2,\"1014\":3,\"1015\":1,\"1016\":2,\"1018\":3,\"1019\":1,\"1020\":1,\"1022\":1,\"1025\":1,\"1026\":1,\"1028\":5,\"1030\":1,\"1031\":1,\"1034\":1,\"1040\":2,\"1043\":1,\"1044\":1,\"1051\":1,\"1053\":5,\"1054\":1,\"1062\":10,\"1065\":4,\"1107\":23,\"1112\":2,\"1113\":1,\"1117\":8,\"1118\":12,\"1119\":11,\"1124\":1,\"1125\":20,\"1126\":24,\"1127\":19,\"1130\":12,\"1131\":8,\"1133\":4,\"1136\":10,\"1141\":14,\"1153\":2,\"1155\":3,\"1156\":6,\"1157\":18,\"1158\":6,\"1162\":4,\"1204\":2,\"1208\":2,\"1217\":36,\"1222\":1,\"1223\":1,\"1224\":2,\"1225\":2,\"1232\":8,\"1245\":1,\"1250\":11,\"1251\":12,\"1252\":6,\"1261\":11,\"1267\":13,\"1268\":17,\"1269\":1,\"1270\":4,\"1271\":4,\"1274\":3,\"1278\":16,\"1280\":31,\"1283\":21,\"1301\":1,\"1308\":5,\"1309\":4,\"1310\":3,\"1311\":6,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":6,\"1319\":5,\"1321\":4,\"1322\":6,\"1323\":4,\"1326\":1,\"1327\":9,\"1328\":7,\"1329\":5,\"1330\":9,\"1334\":1,\"1350\":2,\"1356\":3,\"1357\":1,\"1361\":2,\"1372\":1,\"1377\":4,\"1382\":4,\"1385\":9,\"1389\":18,\"1390\":2,\"1391\":33,\"1392\":14,\"1395\":1,\"1396\":19,\"1397\":1,\"1400\":5,\"1401\":18,\"1402\":8,\"1403\":37,\"1406\":2,\"1408\":20,\"1409\":8,\"1410\":17,\"1413\":4,\"1417\":2,\"1419\":12,\"1420\":6,\"1422\":6,\"1424\":3,\"1426\":2,\"1428\":3,\"1430\":3,\"1432\":1,\"1439\":3,\"1441\":13,\"1442\":8,\"1444\":8,\"1446\":5,\"1448\":7,\"1450\":21,\"1452\":24,\"1454\":18,\"1456\":20,\"1458\":10,\"1460\":11,\"1462\":2,\"1466\":20,\"1467\":6,\"1468\":31,\"1469\":8,\"1476\":1,\"1478\":1,\"1482\":1,\"1483\":1,\"1484\":4,\"1485\":2,\"1491\":1,\"1493\":2,\"1494\":2,\"1509\":4,\"1511\":6,\"1513\":17,\"1519\":18,\"1521\":58,\"1524\":7,\"1525\":7,\"1526\":45,\"1533\":5,\"1534\":1,\"1535\":17,\"1536\":18,\"1539\":6,\"1545\":5,\"1546\":17,\"1548\":4,\"1549\":8,\"1551\":16,\"1552\":85,\"1553\":47,\"1558\":1,\"1581\":4,\"1582\":7,\"1583\":3,\"1584\":2,\"1585\":19,\"1586\":1,\"1587\":3,\"1588\":1,\"1589\":1,\"1591\":2,\"1592\":16,\"1593\":2,\"1594\":5,\"1595\":7,\"1596\":12,\"1597\":12,\"1598\":25,\"1599\":93,\"1600\":22,\"1603\":2,\"1604\":13,\"1605\":14,\"1606\":15,\"1607\":14,\"1608\":5,\"1609\":11,\"1610\":18,\"1611\":12,\"1612\":13,\"1613\":13,\"1614\":7,\"1615\":8,\"1616\":10,\"1617\":1,\"1618\":5,\"1619\":15,\"1620\":6,\"1621\":8,\"1622\":16,\"1624\":4,\"1625\":31,\"1626\":61,\"1627\":2,\"1628\":21,\"1631\":3,\"1638\":3,\"1640\":10,\"1641\":1,\"1642\":1,\"1643\":11,\"1644\":6,\"1645\":8,\"1646\":11,\"1647\":6,\"1648\":3,\"1650\":7,\"1652\":1,\"1655\":1,\"1656\":5,\"1659\":1,\"1660\":4,\"1662\":9,\"1664\":5,\"1665\":6,\"1668\":6,\"1669\":9,\"1670\":3,\"1671\":4,\"1672\":4,\"1673\":2,\"1676\":2,\"1677\":3,\"1679\":3,\"1680\":5,\"1682\":2,\"1683\":5,\"1686\":2,\"1687\":3,\"1689\":2,\"1690\":2,\"1691\":4,\"1692\":5,\"1694\":2,\"1698\":4,\"1699\":2,\"1700\":4,\"1702\":4,\"1716\":2,\"1719\":9,\"1720\":4,\"1721\":8,\"1722\":6,\"1725\":12,\"1726\":7,\"1727\":12,\"1736\":7,\"1738\":2,\"1739\":2,\"1740\":2,\"1741\":2,\"1742\":2,\"1743\":2,\"1744\":2,\"1745\":2,\"1746\":2,\"1748\":6,\"1749\":9,\"1750\":2,\"1762\":4,\"1766\":17,\"1774\":1,\"1775\":2,\"1779\":2,\"1806\":2,\"1807\":4,\"1815\":4,\"1822\":1,\"1824\":4,\"1827\":2,\"1830\":2,\"1837\":1,\"1846\":5,\"1847\":1,\"1849\":1,\"1851\":1,\"1854\":1,\"1856\":2,\"1862\":5,\"1863\":9,\"1872\":1,\"1881\":3,\"1883\":7,\"1902\":5,\"1904\":5,\"1906\":2,\"1909\":1,\"1919\":2,\"1933\":4,\"1934\":1,\"1938\":2,\"1940\":2,\"1941\":1,\"1942\":6,\"1943\":1,\"1945\":7,\"1947\":9,\"1948\":1,\"1949\":2,\"1954\":1,\"1955\":1,\"1957\":4,\"1959\":12,\"1960\":3,\"1961\":4,\"1962\":5,\"1965\":6,\"1975\":15,\"1976\":13,\"1978\":8,\"1980\":14,\"1982\":8,\"1984\":3,\"1987\":5,\"1988\":1,\"1990\":3,\"1991\":5,\"1992\":22,\"1993\":39,\"1994\":32,\"1995\":21,\"1996\":12,\"1997\":14,\"1998\":1,\"1999\":4,\"2000\":20,\"2001\":11,\"2002\":5,\"2003\":5,\"2004\":5,\"2005\":3,\"2006\":2,\"2007\":7,\"2008\":15,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":6,\"2015\":8,\"2016\":3,\"2017\":3,\"2018\":7,\"2019\":4,\"2020\":4,\"2021\":8,\"2043\":3,\"2045\":2,\"2049\":3,\"2054\":2,\"2055\":3,\"2056\":3,\"2061\":3,\"2065\":6,\"2066\":3,\"2126\":21,\"2127\":25,\"2129\":14,\"2132\":5,\"2133\":5,\"2134\":15,\"2136\":8,\"2138\":1,\"2139\":1,\"2141\":2,\"2143\":4,\"2144\":1,\"2155\":1,\"2166\":2,\"2167\":4,\"2176\":9,\"2177\":1,\"2181\":1,\"2183\":2,\"2184\":6,\"2187\":4,\"2190\":2,\"2191\":23,\"2192\":4,\"2196\":4,\"2198\":4,\"2200\":1,\"2203\":6,\"2207\":1,\"2208\":2,\"2209\":5,\"2215\":1,\"2216\":7,\"2217\":2,\"2219\":7,\"2220\":12,\"2221\":25,\"2224\":2,\"2226\":3,\"2228\":65,\"2229\":59,\"2232\":13,\"2235\":52,\"2236\":58,\"2238\":18,\"2239\":108,\"2240\":82,\"2241\":3,\"2245\":79,\"2246\":8,\"2247\":5,\"2248\":8,\"2249\":26,\"2250\":8,\"2251\":8,\"2252\":8,\"2253\":10,\"2254\":8,\"2255\":8,\"2256\":8,\"2257\":8,\"2259\":8,\"2260\":8,\"2261\":8,\"2262\":5,\"2263\":12,\"2264\":8,\"2265\":7,\"2266\":8,\"2267\":8,\"2268\":12,\"2269\":8,\"2270\":12,\"2271\":12,\"2272\":8,\"2273\":8,\"2275\":4,\"2276\":1,\"2280\":4,\"2281\":6,\"2283\":5,\"2284\":5,\"2285\":3,\"2286\":4,\"2287\":2,\"2288\":1,\"2289\":1,\"2290\":1,\"2291\":1,\"2292\":3,\"2293\":11,\"2298\":1,\"2305\":4,\"2307\":3,\"2314\":1,\"2318\":1,\"2320\":1,\"2327\":1,\"2331\":2,\"2334\":8,\"2335\":3,\"2336\":31,\"2337\":28,\"2340\":11,\"2341\":8,\"2342\":7,\"2343\":2,\"2344\":1,\"2346\":24,\"2350\":19,\"2351\":5,\"2352\":1,\"2353\":9,\"2354\":7,\"2355\":19,\"2356\":25,\"2357\":1,\"2359\":19,\"2360\":27,\"2361\":30,\"2362\":23,\"2363\":20,\"2364\":8,\"2365\":1,\"2366\":2,\"2367\":5,\"2368\":28,\"2369\":4,\"2376\":3,\"2377\":5,\"2378\":6,\"2379\":4,\"2381\":1,\"2382\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2390\":1,\"2398\":1,\"2404\":9,\"2405\":4,\"2408\":25,\"2409\":14,\"2411\":69,\"2412\":85,\"2413\":2,\"2414\":8,\"2416\":14,\"2418\":8,\"2422\":2,\"2423\":84,\"2424\":2,\"2425\":6,\"2427\":4,\"2428\":13,\"2429\":10,\"2430\":5,\"2431\":60,\"2432\":72,\"2433\":6,\"2434\":12,\"2435\":1,\"2442\":3,\"2446\":25,\"2447\":77,\"2448\":3,\"2458\":12,\"2460\":10,\"2462\":20,\"2463\":5,\"2464\":4,\"2469\":4,\"2470\":3,\"2471\":1,\"2472\":6,\"2473\":2,\"2479\":2,\"2481\":2,\"2482\":7,\"2485\":2,\"2490\":3,\"2493\":2,\"2495\":2,\"2500\":2,\"2505\":2}}],[\"5e15\",{\"1\":{\"2151\":1,\"2310\":1}}],[\"55\",{\"1\":{\"2307\":2}}],[\"55582\",{\"1\":{\"699\":1}}],[\"5590277314186096\",{\"1\":{\"211\":1}}],[\"59c43ac0d40b121060bd71dd418f5ece\",{\"1\":{\"286\":1}}],[\"5best\",{\"1\":{\"218\":1,\"243\":2,\"286\":2}}],[\"5th\",{\"1\":{\"174\":1,\"1002\":1}}],[\"52581\",{\"1\":{\"136\":1}}],[\"5268\",{\"1\":{\"39\":1}}],[\"58\",{\"1\":{\"113\":2}}],[\"5gpus\",{\"0\":{\"63\":1}}],[\"5\",{\"1\":{\"41\":1,\"43\":1,\"44\":1,\"94\":1,\"139\":2,\"174\":1,\"199\":1,\"200\":2,\"201\":1,\"204\":1,\"205\":1,\"210\":1,\"211\":2,\"216\":1,\"222\":1,\"223\":1,\"227\":1,\"228\":2,\"234\":1,\"240\":1,\"243\":4,\"253\":1,\"259\":1,\"265\":1,\"266\":1,\"267\":8,\"274\":1,\"275\":1,\"276\":2,\"284\":1,\"285\":1,\"286\":2,\"287\":18,\"290\":1,\"527\":1,\"630\":2,\"642\":2,\"643\":10,\"649\":1,\"674\":4,\"700\":1,\"701\":1,\"702\":1,\"718\":1,\"720\":1,\"730\":1,\"736\":1,\"737\":1,\"738\":1,\"746\":1,\"747\":1,\"777\":1,\"795\":1,\"796\":2,\"815\":1,\"833\":1,\"846\":1,\"869\":2,\"947\":1,\"949\":1,\"1020\":1,\"1029\":1,\"1118\":2,\"1125\":1,\"1126\":1,\"1127\":1,\"1130\":1,\"1155\":1,\"1157\":1,\"1210\":1,\"1217\":1,\"1224\":2,\"1235\":1,\"1250\":1,\"1251\":1,\"1253\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1389\":6,\"1390\":4,\"1391\":2,\"1396\":2,\"1401\":5,\"1402\":3,\"1403\":1,\"1408\":8,\"1409\":3,\"1410\":5,\"1413\":2,\"1420\":5,\"1433\":1,\"1450\":1,\"1454\":1,\"1466\":3,\"1467\":1,\"1468\":2,\"1484\":3,\"1509\":50,\"1511\":50,\"1513\":4,\"1526\":9,\"1539\":1,\"1548\":4,\"1549\":3,\"1551\":3,\"1552\":6,\"1553\":59,\"1581\":1,\"1592\":3,\"1593\":2,\"1594\":1,\"1595\":3,\"1596\":1,\"1597\":1,\"1598\":12,\"1599\":7,\"1600\":15,\"1604\":4,\"1606\":1,\"1611\":1,\"1612\":1,\"1613\":2,\"1614\":1,\"1616\":1,\"1618\":1,\"1625\":9,\"1626\":6,\"1643\":1,\"1646\":1,\"1680\":1,\"1717\":1,\"1719\":1,\"1721\":1,\"1722\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1750\":2,\"1758\":1,\"1766\":1,\"1802\":1,\"1810\":1,\"1811\":1,\"1856\":1,\"1862\":1,\"1902\":1,\"1904\":1,\"1906\":6,\"1933\":1,\"1976\":1,\"1985\":1,\"1993\":4,\"1994\":3,\"1997\":1,\"2018\":1,\"2019\":6,\"2020\":6,\"2021\":6,\"2127\":1,\"2133\":2,\"2134\":1,\"2151\":1,\"2176\":1,\"2209\":1,\"2213\":2,\"2214\":2,\"2223\":1,\"2224\":1,\"2227\":1,\"2231\":1,\"2235\":5,\"2236\":5,\"2239\":3,\"2240\":3,\"2245\":6,\"2310\":1,\"2336\":1,\"2337\":1,\"2346\":1,\"2350\":2,\"2353\":1,\"2356\":1,\"2360\":3,\"2361\":3,\"2362\":1,\"2363\":2,\"2364\":1,\"2368\":1,\"2411\":3,\"2412\":7,\"2423\":6,\"2431\":6,\"2432\":8,\"2433\":1,\"2434\":1,\"2439\":1,\"2447\":7,\"2479\":2,\"2485\":2}}],[\"540\",{\"1\":{\"650\":1}}],[\"5414980\",{\"1\":{\"290\":1}}],[\"54\",{\"1\":{\"39\":1,\"113\":1}}],[\"513\",{\"1\":{\"1552\":1,\"1611\":1,\"1626\":1}}],[\"512\",{\"1\":{\"142\":1,\"243\":1,\"267\":2,\"286\":4,\"633\":1,\"634\":1,\"643\":1,\"674\":6,\"702\":7,\"720\":1,\"778\":1,\"846\":7,\"976\":1,\"979\":1,\"980\":1,\"1117\":1,\"1130\":1,\"1131\":1,\"1136\":1,\"1210\":1,\"1232\":1,\"1246\":1,\"1250\":1,\"1251\":1,\"1261\":1,\"1267\":1,\"1268\":1,\"1389\":1,\"1390\":1,\"1396\":3,\"1397\":3,\"1408\":4,\"1409\":3,\"1410\":1,\"1420\":2,\"1422\":3,\"1441\":1,\"1511\":2,\"1513\":1,\"1514\":1,\"1526\":2,\"1534\":3,\"1549\":2,\"1551\":1,\"1552\":1,\"1553\":1,\"1592\":1,\"1598\":1,\"1599\":1,\"1600\":2,\"1605\":1,\"1614\":1,\"1618\":2,\"1625\":1,\"1626\":1,\"1660\":1,\"1662\":1,\"1669\":1,\"1984\":1,\"1993\":3,\"1994\":1,\"2209\":1,\"2232\":1,\"2235\":1,\"2236\":1,\"2238\":1,\"2239\":1,\"2240\":1,\"2245\":5,\"2378\":1,\"2379\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":5,\"2432\":2,\"2447\":1}}],[\"51<51890>\",{\"1\":{\"66\":1}}],[\"51\",{\"1\":{\"39\":1}}],[\"5186\",{\"1\":{\"39\":1}}],[\"5678\",{\"1\":{\"80\":1}}],[\"56\",{\"1\":{\"39\":1,\"73\":1,\"113\":1,\"196\":1}}],[\"5089420\",{\"1\":{\"1321\":1,\"1322\":1}}],[\"500\",{\"1\":{\"1002\":1,\"1130\":1,\"1526\":1,\"2065\":2,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1}}],[\"500epoch\",{\"1\":{\"267\":1}}],[\"500clusters\",{\"1\":{\"259\":1}}],[\"50000\",{\"1\":{\"1965\":1}}],[\"5000\",{\"1\":{\"98\":2,\"141\":1,\"243\":1,\"645\":1,\"661\":1,\"709\":1,\"733\":1,\"734\":1,\"780\":1,\"1012\":1,\"1014\":1,\"1748\":1,\"2191\":1}}],[\"50\",{\"1\":{\"39\":1,\"145\":1,\"616\":1,\"696\":1,\"697\":1,\"1259\":1,\"1261\":1,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1408\":1,\"1410\":1,\"1441\":1,\"1466\":1,\"1468\":1,\"1469\":1,\"1716\":1,\"1880\":1}}],[\"506\",{\"1\":{\"39\":1,\"113\":1}}],[\"99\",{\"1\":{\"1389\":1,\"1391\":1,\"1396\":2,\"1400\":1,\"1401\":1,\"1403\":1,\"1408\":2,\"1410\":1,\"1441\":1,\"1466\":2,\"1468\":1,\"1469\":1,\"2147\":1}}],[\"99995\",{\"1\":{\"2462\":1}}],[\"999995\",{\"1\":{\"674\":2}}],[\"999\",{\"1\":{\"84\":1,\"1382\":1}}],[\"9205\",{\"1\":{\"1210\":1,\"1264\":1,\"1334\":1}}],[\"9201\",{\"1\":{\"1210\":1,\"1264\":1,\"1334\":1}}],[\"9292\",{\"1\":{\"650\":1}}],[\"9250505\",{\"1\":{\"616\":1,\"696\":1,\"697\":1}}],[\"96b2fd08d4fd9276aabd7ad41ec5e02a88b30958\",{\"1\":{\"290\":1}}],[\"962\",{\"1\":{\"113\":1}}],[\"9+\",{\"1\":{\"159\":1}}],[\"9\",{\"1\":{\"84\":1,\"199\":1,\"204\":1,\"210\":1,\"216\":1,\"217\":1,\"222\":1,\"223\":1,\"227\":1,\"228\":2,\"234\":1,\"235\":2,\"240\":1,\"242\":1,\"243\":1,\"253\":1,\"254\":1,\"259\":1,\"261\":1,\"265\":1,\"266\":1,\"274\":1,\"275\":1,\"284\":1,\"285\":2,\"535\":1,\"768\":1,\"1012\":1,\"1014\":1,\"1392\":1,\"1509\":1,\"1511\":1,\"1526\":1,\"1553\":1,\"1599\":2,\"1600\":1,\"1608\":1,\"1618\":3,\"1619\":1,\"1620\":1,\"1621\":1,\"1631\":1,\"1655\":1,\"1676\":1,\"1697\":1,\"1698\":1,\"1833\":1,\"2412\":2,\"2423\":2,\"2447\":2,\"2460\":1,\"2479\":2}}],[\"91300\",{\"1\":{\"39\":1}}],[\"970010\",{\"1\":{\"66\":1}}],[\"97\",{\"1\":{\"39\":1,\"691\":1,\"1686\":1,\"1694\":1}}],[\"90\",{\"1\":{\"1697\":1,\"1698\":1}}],[\"907\",{\"1\":{\"1309\":1,\"1311\":1}}],[\"903\",{\"1\":{\"1309\":1,\"1311\":1}}],[\"9053040\",{\"1\":{\"616\":2,\"696\":2,\"697\":2}}],[\"9054224\",{\"1\":{\"207\":2}}],[\"900\",{\"1\":{\"48\":2}}],[\"90000\",{\"1\":{\"39\":1}}],[\"90200\",{\"1\":{\"39\":1}}],[\"90100\",{\"1\":{\"39\":1}}],[\"98\",{\"1\":{\"73\":1,\"196\":1,\"243\":1}}],[\"9823\",{\"1\":{\"39\":1}}],[\"9897\",{\"1\":{\"39\":1}}],[\"945\",{\"1\":{\"113\":1}}],[\"9459\",{\"1\":{\"39\":1}}],[\"94\",{\"1\":{\"39\":2}}],[\"93\",{\"1\":{\"39\":1}}],[\"7b\",{\"1\":{\"1965\":1,\"2133\":2,\"2287\":1}}],[\"707\",{\"1\":{\"1655\":3,\"1672\":1,\"1673\":1,\"1687\":1,\"1689\":1,\"1690\":1}}],[\"77\",{\"1\":{\"1125\":1}}],[\"776\",{\"1\":{\"113\":1}}],[\"7600\",{\"1\":{\"536\":10,\"778\":1,\"1980\":1,\"2416\":1}}],[\"7698\",{\"1\":{\"207\":1}}],[\"7694\",{\"1\":{\"207\":1}}],[\"768\",{\"1\":{\"39\":1,\"674\":4,\"846\":1,\"851\":1,\"1519\":1,\"1535\":1,\"1536\":2,\"1546\":1,\"1622\":1,\"2130\":1}}],[\"7+\",{\"1\":{\"159\":1}}],[\"75\",{\"1\":{\"147\":1,\"661\":1,\"747\":1,\"1389\":2,\"1390\":2,\"1413\":2,\"1420\":2,\"1679\":1,\"1716\":1,\"1717\":1,\"2434\":1}}],[\"7471631\",{\"1\":{\"1131\":1,\"1172\":1}}],[\"74\",{\"1\":{\"39\":2}}],[\"740617\",{\"1\":{\"39\":1}}],[\"7\",{\"1\":{\"39\":4,\"43\":1,\"113\":1,\"136\":1,\"161\":1,\"199\":1,\"204\":1,\"210\":1,\"212\":2,\"216\":1,\"222\":1,\"223\":2,\"227\":1,\"228\":1,\"234\":1,\"240\":1,\"253\":1,\"261\":1,\"265\":1,\"267\":3,\"274\":1,\"284\":1,\"285\":1,\"286\":8,\"780\":1,\"1012\":1,\"1014\":1,\"1107\":1,\"1262\":2,\"1268\":1,\"1274\":1,\"1319\":1,\"1389\":5,\"1390\":1,\"1391\":4,\"1396\":4,\"1401\":5,\"1402\":1,\"1403\":4,\"1408\":3,\"1409\":1,\"1410\":2,\"1420\":2,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1466\":4,\"1468\":4,\"1509\":27,\"1511\":27,\"1513\":3,\"1519\":1,\"1526\":4,\"1535\":1,\"1536\":2,\"1546\":1,\"1548\":2,\"1549\":1,\"1551\":2,\"1552\":4,\"1553\":32,\"1592\":2,\"1593\":1,\"1595\":1,\"1598\":4,\"1599\":3,\"1600\":4,\"1605\":1,\"1622\":1,\"1625\":4,\"1626\":3,\"1994\":1,\"2239\":2,\"2240\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2447\":1}}],[\"731593\",{\"1\":{\"39\":1}}],[\"731425\",{\"1\":{\"39\":1}}],[\"733486\",{\"1\":{\"39\":1}}],[\"730052\",{\"1\":{\"39\":1}}],[\"72476\",{\"1\":{\"39\":1}}],[\"72\",{\"1\":{\"39\":2}}],[\"72843\",{\"1\":{\"39\":1}}],[\"718948801\",{\"1\":{\"1411\":1}}],[\"71428\",{\"1\":{\"39\":1}}],[\"71\",{\"1\":{\"39\":1}}],[\"7861\",{\"1\":{\"39\":1}}],[\"8oog7ee4sz8\",{\"1\":{\"1833\":1}}],[\"8691481\",{\"1\":{\"1309\":1,\"1311\":1}}],[\"8k\",{\"1\":{\"1155\":2,\"1157\":2}}],[\"8khz\",{\"1\":{\"70\":1}}],[\"873\",{\"1\":{\"113\":1}}],[\"8\",{\"1\":{\"66\":1,\"71\":1,\"86\":2,\"141\":1,\"162\":2,\"175\":2,\"199\":1,\"200\":1,\"204\":1,\"205\":1,\"210\":1,\"216\":1,\"218\":2,\"222\":1,\"223\":1,\"227\":1,\"228\":1,\"234\":1,\"240\":1,\"253\":1,\"261\":2,\"265\":1,\"267\":1,\"274\":1,\"276\":3,\"284\":1,\"285\":1,\"286\":7,\"290\":2,\"786\":1,\"846\":1,\"851\":1,\"866\":1,\"921\":1,\"922\":1,\"980\":1,\"1029\":1,\"1125\":1,\"1235\":1,\"1252\":1,\"1267\":1,\"1268\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1385\":2,\"1389\":2,\"1391\":3,\"1396\":2,\"1401\":4,\"1402\":2,\"1403\":2,\"1408\":3,\"1410\":3,\"1441\":1,\"1450\":1,\"1454\":1,\"1466\":4,\"1467\":2,\"1468\":2,\"1511\":2,\"1513\":2,\"1514\":2,\"1526\":3,\"1534\":2,\"1548\":4,\"1549\":2,\"1551\":2,\"1552\":5,\"1553\":3,\"1592\":2,\"1598\":2,\"1599\":2,\"1600\":2,\"1605\":2,\"1609\":1,\"1618\":1,\"1625\":3,\"1626\":3,\"1681\":2,\"1683\":4,\"1745\":3,\"1957\":1,\"2130\":1,\"2136\":2,\"2187\":1,\"2192\":1,\"2196\":1,\"2203\":1,\"2220\":1,\"2249\":1,\"2436\":1,\"2458\":1,\"2482\":1}}],[\"808\",{\"1\":{\"285\":1}}],[\"801\",{\"1\":{\"113\":1}}],[\"8000\",{\"1\":{\"2346\":1,\"2368\":1}}],[\"800\",{\"1\":{\"48\":4}}],[\"80\",{\"1\":{\"43\":1,\"97\":3,\"128\":1,\"536\":10,\"720\":1,\"778\":1,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1419\":1,\"1466\":1,\"1513\":1,\"1526\":1,\"1551\":1,\"1553\":1,\"1582\":1,\"1592\":1,\"1598\":1,\"1600\":1,\"1605\":1,\"1607\":1,\"1610\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1625\":1,\"1662\":1,\"1670\":1,\"1699\":1,\"1846\":2,\"1980\":2,\"2007\":1,\"2130\":1,\"2404\":1,\"2416\":2,\"2429\":1,\"2435\":1}}],[\"8041\",{\"1\":{\"39\":1}}],[\"8192\",{\"1\":{\"1509\":1,\"1511\":1}}],[\"81\",{\"1\":{\"39\":1}}],[\"84\",{\"1\":{\"39\":2}}],[\"890\",{\"1\":{\"97\":2}}],[\"89900\",{\"1\":{\"39\":1}}],[\"89800\",{\"1\":{\"39\":1}}],[\"89700\",{\"1\":{\"39\":1}}],[\"835\",{\"1\":{\"285\":1}}],[\"83\",{\"1\":{\"39\":1,\"988\":4,\"990\":4,\"1878\":3}}],[\"3e9\",{\"1\":{\"2151\":1,\"2310\":1}}],[\"3x3\",{\"1\":{\"1304\":1,\"1347\":1}}],[\"3d\",{\"1\":{\"706\":5,\"730\":1,\"1119\":1,\"1264\":1,\"1334\":2}}],[\"3829\",{\"1\":{\"1730\":1}}],[\"3825\",{\"1\":{\"1730\":1}}],[\"38\",{\"1\":{\"268\":1,\"277\":1}}],[\"384\",{\"1\":{\"43\":2,\"1107\":1,\"1278\":1,\"1526\":2,\"1536\":2,\"1598\":3,\"1599\":3,\"1600\":4,\"1994\":1,\"2236\":1,\"2239\":2,\"2240\":2,\"2411\":2,\"2412\":4,\"2423\":4,\"2433\":1,\"2447\":4,\"2458\":1}}],[\"3382\",{\"1\":{\"290\":1}}],[\"33\",{\"1\":{\"268\":1,\"277\":1}}],[\"3711\",{\"1\":{\"616\":1,\"696\":2,\"697\":2}}],[\"37\",{\"1\":{\"261\":1}}],[\"3773\",{\"1\":{\"39\":1}}],[\"36787944117144233\",{\"1\":{\"1589\":1}}],[\"36\",{\"1\":{\"242\":1,\"285\":1}}],[\"369\",{\"1\":{\"113\":1}}],[\"3023\",{\"1\":{\"1002\":1}}],[\"3072\",{\"1\":{\"674\":4,\"846\":1,\"851\":1}}],[\"30\",{\"1\":{\"243\":3,\"1000\":1,\"1610\":1,\"1628\":1,\"1664\":1,\"1691\":1,\"2245\":1,\"2360\":1,\"2361\":1,\"2460\":1}}],[\"30s\",{\"1\":{\"242\":1}}],[\"3000\",{\"1\":{\"1655\":1,\"1672\":1,\"1673\":1,\"1689\":1}}],[\"300\",{\"1\":{\"98\":1,\"267\":5,\"536\":8,\"720\":2,\"738\":2,\"815\":2,\"1126\":1,\"1127\":1,\"1217\":2,\"1539\":2,\"1766\":2}}],[\"3rd\",{\"1\":{\"174\":1,\"276\":1}}],[\"31\",{\"1\":{\"141\":1,\"243\":2,\"700\":1,\"709\":1,\"710\":1,\"733\":1,\"734\":1,\"774\":1,\"780\":2,\"1526\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1994\":1,\"2126\":1,\"2191\":1,\"2239\":1,\"2240\":1,\"2411\":1,\"2412\":1,\"2447\":1}}],[\"315\",{\"1\":{\"113\":1}}],[\"3nodes\",{\"0\":{\"63\":1}}],[\"32768\",{\"1\":{\"2335\":1,\"2350\":1,\"2376\":1}}],[\"320\",{\"1\":{\"44\":1,\"276\":6,\"720\":3,\"738\":3,\"796\":3,\"798\":2,\"815\":3,\"847\":1,\"862\":2,\"869\":2,\"1126\":2,\"1127\":1,\"1217\":3,\"1539\":3,\"1766\":3,\"2016\":1}}],[\"32\",{\"1\":{\"43\":4,\"48\":1,\"148\":1,\"243\":1,\"548\":1,\"551\":1,\"606\":1,\"626\":1,\"1118\":1,\"1124\":1,\"1125\":1,\"1385\":1,\"1389\":3,\"1390\":2,\"1391\":1,\"1396\":3,\"1397\":2,\"1401\":3,\"1402\":2,\"1403\":1,\"1408\":3,\"1409\":3,\"1413\":1,\"1420\":2,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1466\":2,\"1467\":1,\"1468\":1,\"1509\":1,\"1511\":2,\"1514\":2,\"1526\":2,\"1534\":2,\"1549\":2,\"1552\":1,\"1553\":3,\"1593\":1,\"1595\":1,\"1596\":1,\"1598\":3,\"1599\":2,\"1600\":4,\"1615\":1,\"1625\":2,\"1626\":1,\"1682\":1,\"1856\":1,\"1993\":2,\"2176\":1,\"2198\":1,\"2245\":3,\"2411\":2,\"2412\":2,\"2423\":2,\"2425\":2,\"2429\":2,\"2431\":3,\"2432\":2,\"2439\":1,\"2490\":1}}],[\"3463\",{\"1\":{\"290\":1}}],[\"34\",{\"1\":{\"39\":1,\"73\":1,\"113\":3,\"261\":1,\"988\":2,\"990\":2,\"1020\":1}}],[\"35\",{\"1\":{\"39\":1}}],[\"3\",{\"0\":{\"163\":1},\"1\":{\"24\":1,\"29\":1,\"41\":2,\"43\":3,\"51\":1,\"53\":1,\"61\":3,\"80\":1,\"85\":2,\"93\":1,\"126\":1,\"141\":1,\"145\":2,\"159\":1,\"162\":3,\"167\":2,\"174\":1,\"175\":7,\"199\":1,\"204\":1,\"210\":1,\"216\":1,\"222\":3,\"223\":1,\"224\":1,\"225\":1,\"227\":1,\"228\":1,\"234\":1,\"240\":2,\"243\":4,\"253\":1,\"261\":2,\"265\":2,\"267\":5,\"274\":2,\"276\":12,\"284\":1,\"285\":1,\"286\":9,\"287\":2,\"616\":2,\"649\":2,\"674\":2,\"696\":1,\"697\":1,\"702\":4,\"709\":1,\"710\":1,\"720\":3,\"733\":1,\"734\":1,\"735\":1,\"738\":3,\"774\":1,\"780\":1,\"815\":3,\"846\":4,\"911\":1,\"978\":1,\"980\":2,\"994\":1,\"1002\":1,\"1012\":1,\"1014\":1,\"1020\":2,\"1108\":2,\"1110\":2,\"1117\":1,\"1124\":3,\"1125\":2,\"1126\":2,\"1127\":2,\"1130\":1,\"1131\":1,\"1136\":2,\"1141\":1,\"1145\":2,\"1147\":1,\"1211\":2,\"1217\":3,\"1232\":2,\"1238\":2,\"1261\":2,\"1264\":2,\"1267\":2,\"1268\":2,\"1283\":1,\"1311\":1,\"1318\":1,\"1322\":1,\"1327\":1,\"1328\":2,\"1329\":1,\"1330\":2,\"1334\":2,\"1377\":1,\"1389\":6,\"1390\":6,\"1392\":1,\"1396\":1,\"1397\":1,\"1401\":8,\"1402\":8,\"1408\":12,\"1409\":8,\"1410\":4,\"1420\":7,\"1433\":1,\"1435\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":2,\"1466\":2,\"1467\":2,\"1484\":4,\"1509\":42,\"1511\":42,\"1513\":4,\"1519\":1,\"1524\":1,\"1525\":1,\"1526\":12,\"1535\":1,\"1536\":2,\"1539\":3,\"1546\":1,\"1548\":5,\"1549\":7,\"1551\":4,\"1552\":4,\"1553\":53,\"1592\":4,\"1593\":6,\"1594\":2,\"1595\":8,\"1596\":6,\"1597\":1,\"1598\":14,\"1599\":8,\"1600\":14,\"1604\":4,\"1605\":2,\"1606\":2,\"1609\":1,\"1610\":2,\"1614\":2,\"1615\":1,\"1616\":2,\"1618\":1,\"1622\":1,\"1625\":13,\"1626\":6,\"1628\":2,\"1655\":2,\"1698\":1,\"1701\":1,\"1722\":1,\"1740\":1,\"1750\":1,\"1766\":3,\"1807\":1,\"1820\":1,\"1902\":4,\"1904\":4,\"1906\":7,\"1926\":1,\"1933\":2,\"1976\":1,\"1993\":1,\"2018\":1,\"2049\":2,\"2126\":1,\"2151\":1,\"2167\":1,\"2176\":1,\"2191\":1,\"2209\":3,\"2213\":2,\"2214\":2,\"2224\":1,\"2235\":3,\"2236\":4,\"2239\":1,\"2240\":1,\"2245\":3,\"2307\":1,\"2310\":1,\"2336\":1,\"2337\":1,\"2346\":1,\"2350\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2368\":1,\"2411\":2,\"2412\":4,\"2423\":4,\"2425\":1,\"2429\":1,\"2431\":3,\"2432\":2,\"2433\":1,\"2443\":1,\"2447\":3,\"2460\":1,\"2479\":1,\"2488\":1}}],[\"<30\",{\"1\":{\"2361\":1}}],[\"<0\",{\"1\":{\"2361\":1}}],[\"<text\",{\"1\":{\"2144\":2}}],[\"<task>\",{\"1\":{\"107\":1,\"108\":1,\"1996\":1}}],[\"<id>\",{\"1\":{\"2144\":2}}],[\"<int>\",{\"1\":{\"102\":1}}],[\"<in\",{\"1\":{\"74\":1}}],[\"<lang>\",{\"1\":{\"1996\":1}}],[\"<generatespeech>\",{\"1\":{\"1942\":1}}],[\"<generatetext>\",{\"1\":{\"1942\":1}}],[\"<notimestamps>\",{\"1\":{\"2361\":1}}],[\"<nolang>\",{\"1\":{\"2360\":1}}],[\"<na>\",{\"1\":{\"1002\":14,\"1996\":1,\"1997\":1,\"2360\":1,\"2361\":1}}],[\"<num\",{\"1\":{\"161\":2}}],[\"<num>\",{\"1\":{\"74\":1}}],[\"<num>c|\",{\"1\":{\"74\":1}}],[\"<built\",{\"1\":{\"2258\":2}}],[\"<br\",{\"1\":{\"706\":1,\"833\":1,\"994\":1,\"1000\":2,\"1281\":1,\"1282\":1,\"1319\":1,\"1322\":1,\"1350\":2,\"1397\":2,\"1655\":2,\"1717\":1,\"1997\":1,\"2015\":1,\"2020\":1,\"2130\":2,\"2235\":2,\"2236\":2}}],[\"<blank>\",{\"1\":{\"625\":1,\"736\":1,\"737\":1,\"777\":1,\"1640\":1,\"1959\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2221\":2}}],[\"<=\",{\"1\":{\"650\":2,\"2020\":1,\"2021\":1}}],[\"<\",{\"1\":{\"629\":2,\"652\":1,\"1063\":1,\"2015\":1,\"2462\":1}}],[\"<queue\",{\"1\":{\"535\":1}}],[\"<class\",{\"1\":{\"617\":1,\"618\":1,\"622\":1,\"624\":1,\"633\":1,\"636\":1,\"638\":2,\"642\":1,\"991\":1,\"1003\":1,\"1015\":1,\"2198\":1,\"2249\":1}}],[\"<cuda\",{\"1\":{\"163\":1}}],[\"<conda\",{\"1\":{\"31\":2}}],[\"<kaldi\",{\"1\":{\"161\":4,\"162\":1}}],[\"<key\",{\"1\":{\"79\":1}}],[\"<pad>\",{\"1\":{\"2130\":1}}],[\"<packing\",{\"1\":{\"127\":1}}],[\"<param\",{\"1\":{\"114\":1}}],[\"<repo\",{\"1\":{\"127\":1}}],[\"<rank0\",{\"1\":{\"60\":2}}],[\"<unk>\",{\"1\":{\"2130\":1,\"2291\":1,\"2336\":1,\"2337\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1}}],[\"<user\",{\"1\":{\"127\":1}}],[\"<utterance\",{\"1\":{\"73\":1}}],[\"<mask>\",{\"1\":{\"777\":1,\"1907\":3}}],[\"<my\",{\"1\":{\"127\":2}}],[\"<model\",{\"1\":{\"125\":1,\"286\":1}}],[\"<your\",{\"1\":{\"124\":3}}],[\"<dataset>\",{\"1\":{\"107\":1,\"108\":1}}],[\"<dst\",{\"1\":{\"88\":1,\"2314\":1}}],[\"<dir\",{\"1\":{\"24\":1}}],[\"<file\",{\"1\":{\"79\":2,\"88\":1,\"2314\":1}}],[\"<out\",{\"1\":{\"74\":1}}],[\"<s>\",{\"1\":{\"2462\":1}}],[\"<sil>\",{\"1\":{\"2462\":1}}],[\"<sc>\",{\"1\":{\"2283\":1,\"2284\":1}}],[\"<stdout>\",{\"1\":{\"2249\":1}}],[\"<start\",{\"1\":{\"73\":1}}],[\"<sop>\",{\"1\":{\"1996\":1,\"1997\":1}}],[\"<sos>\",{\"1\":{\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"1858\":2,\"1996\":1,\"1997\":1}}],[\"<sos\",{\"1\":{\"736\":2,\"1942\":1,\"2221\":2}}],[\"<some\",{\"1\":{\"108\":2,\"153\":2}}],[\"<space>\",{\"1\":{\"625\":1,\"736\":1,\"737\":1,\"777\":1,\"1640\":1,\"1959\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2221\":2,\"2275\":1,\"2277\":1,\"2285\":1,\"2293\":1,\"2336\":1,\"2337\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1}}],[\"<sample\",{\"1\":{\"114\":1}}],[\"<src\",{\"1\":{\"88\":1,\"2314\":1}}],[\"<shell\",{\"1\":{\"3\":1}}],[\"<wav\",{\"1\":{\"73\":1}}],[\"<any\",{\"1\":{\"58\":2,\"60\":2,\"61\":3,\"161\":1,\"162\":1}}],[\"<arg\",{\"1\":{\"24\":2}}],[\"<eos>\",{\"1\":{\"1858\":2,\"1907\":2,\"1996\":1,\"1997\":1}}],[\"<exclude\",{\"1\":{\"88\":1,\"2314\":1}}],[\"<example>\",{\"1\":{\"24\":1}}],[\"<end\",{\"1\":{\"73\":1}}],[\"<espnet>\",{\"1\":{\"2344\":1}}],[\"<espnet2\",{\"1\":{\"2246\":9,\"2247\":4,\"2248\":6,\"2250\":7,\"2251\":7,\"2252\":24,\"2253\":6,\"2254\":1,\"2255\":10,\"2256\":7,\"2257\":6,\"2259\":12,\"2260\":2,\"2261\":7,\"2262\":1,\"2263\":14,\"2264\":7,\"2265\":10,\"2266\":6,\"2267\":13,\"2268\":10,\"2269\":8,\"2270\":6,\"2271\":7,\"2272\":4,\"2273\":5}}],[\"<espnet\",{\"1\":{\"31\":2,\"33\":1,\"154\":1,\"162\":15,\"163\":4,\"164\":1}}],[\"0db\",{\"1\":{\"1672\":1}}],[\"0s\",{\"1\":{\"1484\":1}}],[\"0>\",{\"1\":{\"1475\":1}}],[\"09685\",{\"1\":{\"1683\":1}}],[\"09792https\",{\"1\":{\"1411\":1}}],[\"09921\",{\"1\":{\"974\":1}}],[\"09058\",{\"1\":{\"699\":1}}],[\"03762\",{\"1\":{\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"2016\":1}}],[\"03718\",{\"1\":{\"780\":1}}],[\"032\",{\"1\":{\"1680\":1,\"1692\":1,\"1698\":1}}],[\"03\",{\"1\":{\"1509\":1,\"1511\":1,\"1553\":1}}],[\"03312\",{\"1\":{\"1385\":1,\"1439\":1}}],[\"03952\",{\"1\":{\"1269\":1}}],[\"03577\",{\"1\":{\"696\":1,\"697\":1}}],[\"03541\",{\"1\":{\"696\":1,\"785\":1,\"786\":1,\"866\":2,\"882\":1,\"883\":1,\"884\":1,\"921\":2,\"922\":2}}],[\"01634449\",{\"1\":{\"1327\":1,\"1330\":1}}],[\"01531\",{\"1\":{\"1252\":1}}],[\"01\",{\"1\":{\"537\":1,\"664\":1,\"1487\":1,\"1526\":2,\"2018\":2,\"2239\":2,\"2240\":2,\"2307\":1,\"2378\":1,\"2442\":2}}],[\"013\",{\"1\":{\"268\":1,\"277\":1}}],[\"0+1+2\",{\"1\":{\"276\":1}}],[\"02795\",{\"1\":{\"1784\":2}}],[\"02860\",{\"1\":{\"1785\":1,\"1786\":1,\"1817\":1,\"1818\":1}}],[\"028\",{\"1\":{\"1330\":1}}],[\"02\",{\"1\":{\"243\":1,\"633\":1,\"638\":1,\"646\":1,\"647\":1,\"2361\":1}}],[\"02971\",{\"1\":{\"617\":1}}],[\"02973\",{\"1\":{\"202\":1}}],[\"0296\",{\"1\":{\"39\":1}}],[\"06389\",{\"1\":{\"2428\":1}}],[\"06736\",{\"1\":{\"1708\":1,\"1709\":1}}],[\"06247\",{\"1\":{\"629\":1,\"974\":1}}],[\"0625\",{\"1\":{\"135\":1,\"136\":1}}],[\"06\",{\"1\":{\"243\":1,\"691\":1,\"1054\":1,\"1126\":2,\"1127\":2,\"1210\":1,\"1217\":2,\"1308\":1,\"1354\":1,\"1490\":1,\"2176\":1}}],[\"068\",{\"1\":{\"113\":1}}],[\"0=1\",{\"1\":{\"113\":2}}],[\"07143\",{\"1\":{\"2183\":1}}],[\"07145\",{\"1\":{\"652\":1}}],[\"07907v1\",{\"1\":{\"2223\":2,\"2245\":1}}],[\"07907\",{\"1\":{\"1768\":1,\"2227\":1,\"2231\":1}}],[\"07949\",{\"1\":{\"1618\":1}}],[\"07503\",{\"1\":{\"1710\":1}}],[\"07597\",{\"1\":{\"768\":1,\"1668\":1}}],[\"07093\",{\"1\":{\"1530\":1}}],[\"07\",{\"1\":{\"1126\":1,\"1127\":1,\"1217\":2,\"1248\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1318\":1,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1329\":1,\"1330\":2}}],[\"07467\",{\"1\":{\"640\":1}}],[\"07204\",{\"1\":{\"711\":1}}],[\"072\",{\"1\":{\"113\":1}}],[\"07698\",{\"1\":{\"2167\":1}}],[\"076\",{\"1\":{\"113\":1}}],[\"07840\",{\"1\":{\"9\":1}}],[\"05042\",{\"1\":{\"2176\":1}}],[\"05941v1\",{\"1\":{\"652\":1}}],[\"05895\",{\"1\":{\"648\":1}}],[\"05420\",{\"1\":{\"616\":1}}],[\"05k\",{\"1\":{\"536\":12}}],[\"055\",{\"1\":{\"113\":1}}],[\"05\",{\"1\":{\"106\":1,\"113\":4,\"141\":7,\"243\":1,\"267\":1,\"286\":7,\"625\":1,\"640\":1,\"648\":1,\"722\":1,\"746\":1,\"786\":1,\"846\":1,\"854\":1,\"866\":1,\"887\":1,\"921\":1,\"1029\":1,\"1046\":1,\"1048\":1,\"1065\":1,\"1070\":1,\"1071\":1,\"1073\":1,\"1078\":1,\"1130\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1224\":1,\"1235\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1400\":1,\"1469\":1,\"1491\":1,\"1527\":1,\"1583\":1,\"1603\":1,\"1665\":1,\"2363\":1,\"2428\":2}}],[\"00>\",{\"1\":{\"2361\":2}}],[\"008\",{\"1\":{\"1680\":1,\"1692\":1,\"1698\":1,\"2442\":2}}],[\"003\",{\"1\":{\"1545\":3}}],[\"00\",{\"1\":{\"242\":1,\"243\":2}}],[\"004\",{\"1\":{\"136\":1}}],[\"000+\",{\"1\":{\"2000\":1,\"2001\":1}}],[\"000hz\",{\"1\":{\"1717\":1}}],[\"00077\",{\"1\":{\"624\":1}}],[\"0007\",{\"1\":{\"268\":1,\"277\":1}}],[\"0000\",{\"1\":{\"1012\":3,\"1014\":3}}],[\"00000000210000000030\",{\"1\":{\"1000\":1}}],[\"00000000110000000020\",{\"1\":{\"1000\":1}}],[\"00000000000000000010\",{\"1\":{\"1000\":1}}],[\"0000000000000000hato\",{\"1\":{\"268\":4,\"277\":4}}],[\"000098\",{\"1\":{\"196\":1}}],[\"00023992260685190558\",{\"1\":{\"211\":1}}],[\"000\",{\"1\":{\"113\":2,\"1717\":1,\"2151\":1,\"2310\":1}}],[\"00012396479723975062\",{\"1\":{\"211\":1}}],[\"0001\",{\"1\":{\"84\":1,\"2020\":1}}],[\"006\",{\"1\":{\"113\":3}}],[\"001156\",{\"1\":{\"196\":1}}],[\"001\",{\"1\":{\"84\":1,\"136\":3,\"821\":2,\"824\":1,\"1634\":3,\"1635\":3,\"1636\":3,\"2014\":2,\"2017\":1}}],[\"0e\",{\"1\":{\"84\":2,\"243\":2}}],[\"08416\",{\"1\":{\"2239\":1}}],[\"08661v5\",{\"1\":{\"1994\":1}}],[\"08661\",{\"1\":{\"1967\":1}}],[\"08681\",{\"1\":{\"635\":1}}],[\"08895\",{\"1\":{\"1820\":1}}],[\"082\",{\"1\":{\"136\":1}}],[\"08\",{\"1\":{\"39\":6,\"84\":2,\"1210\":1,\"1293\":1,\"1309\":1,\"1310\":1,\"1318\":1,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1329\":1,\"1330\":1,\"1334\":1,\"1361\":2,\"1479\":1,\"2020\":1,\"2436\":1,\"2437\":1}}],[\"04368\",{\"1\":{\"1705\":1}}],[\"04301\",{\"1\":{\"1590\":1,\"1977\":1}}],[\"046\",{\"1\":{\"113\":1}}],[\"0475\",{\"1\":{\"39\":1}}],[\"04\",{\"0\":{\"29\":1},\"1\":{\"26\":1,\"27\":2,\"113\":3,\"160\":1,\"243\":1}}],[\"0\",{\"1\":{\"22\":1,\"23\":1,\"24\":3,\"25\":1,\"26\":5,\"29\":3,\"31\":1,\"39\":10,\"41\":1,\"43\":18,\"44\":11,\"45\":1,\"46\":2,\"48\":2,\"50\":1,\"51\":1,\"58\":1,\"61\":1,\"69\":1,\"73\":1,\"80\":1,\"82\":1,\"84\":8,\"86\":2,\"100\":1,\"126\":1,\"128\":1,\"135\":2,\"136\":1,\"139\":7,\"141\":42,\"142\":11,\"144\":9,\"147\":2,\"161\":1,\"175\":20,\"196\":1,\"205\":1,\"211\":7,\"235\":2,\"242\":1,\"243\":24,\"267\":6,\"276\":10,\"281\":1,\"286\":11,\"287\":8,\"290\":1,\"521\":1,\"527\":3,\"535\":2,\"614\":1,\"615\":1,\"616\":1,\"617\":3,\"618\":3,\"620\":3,\"622\":2,\"624\":3,\"625\":10,\"629\":5,\"633\":9,\"634\":12,\"635\":2,\"636\":3,\"637\":4,\"638\":5,\"640\":2,\"641\":5,\"642\":4,\"643\":8,\"644\":6,\"645\":4,\"646\":3,\"647\":3,\"650\":6,\"651\":3,\"652\":1,\"661\":6,\"664\":10,\"667\":1,\"669\":1,\"674\":60,\"689\":1,\"691\":1,\"692\":2,\"696\":1,\"697\":4,\"700\":9,\"701\":6,\"703\":2,\"706\":7,\"709\":8,\"710\":4,\"711\":4,\"717\":1,\"720\":4,\"730\":1,\"731\":6,\"732\":6,\"733\":6,\"734\":6,\"736\":5,\"737\":6,\"738\":6,\"744\":1,\"745\":5,\"746\":14,\"747\":14,\"748\":6,\"749\":1,\"755\":2,\"756\":1,\"762\":1,\"764\":1,\"765\":2,\"766\":6,\"767\":6,\"768\":1,\"770\":2,\"771\":1,\"772\":2,\"773\":1,\"774\":4,\"775\":6,\"777\":5,\"780\":8,\"784\":1,\"785\":2,\"786\":7,\"787\":2,\"790\":2,\"791\":2,\"793\":1,\"796\":4,\"798\":2,\"800\":5,\"801\":1,\"802\":1,\"809\":2,\"815\":4,\"817\":1,\"819\":1,\"820\":5,\"821\":1,\"823\":1,\"824\":1,\"828\":3,\"830\":2,\"832\":1,\"833\":1,\"846\":16,\"847\":5,\"848\":8,\"849\":4,\"850\":6,\"851\":6,\"862\":2,\"866\":2,\"867\":1,\"881\":2,\"884\":2,\"895\":2,\"921\":5,\"922\":2,\"929\":1,\"935\":5,\"936\":2,\"937\":2,\"947\":2,\"948\":1,\"949\":4,\"958\":4,\"959\":2,\"962\":2,\"974\":2,\"979\":1,\"994\":1,\"1000\":1,\"1002\":1,\"1012\":1,\"1014\":1,\"1025\":2,\"1028\":1,\"1029\":8,\"1043\":1,\"1054\":1,\"1057\":1,\"1059\":1,\"1064\":4,\"1065\":1,\"1080\":2,\"1082\":4,\"1103\":1,\"1107\":3,\"1108\":1,\"1117\":3,\"1119\":2,\"1120\":2,\"1124\":4,\"1125\":4,\"1126\":3,\"1127\":2,\"1130\":4,\"1131\":3,\"1132\":1,\"1133\":4,\"1134\":1,\"1136\":3,\"1137\":1,\"1139\":1,\"1141\":3,\"1147\":4,\"1153\":2,\"1156\":1,\"1162\":3,\"1164\":1,\"1167\":1,\"1177\":1,\"1181\":2,\"1185\":2,\"1202\":2,\"1204\":2,\"1205\":1,\"1208\":4,\"1209\":2,\"1211\":2,\"1217\":4,\"1228\":2,\"1232\":3,\"1235\":7,\"1236\":1,\"1238\":1,\"1242\":1,\"1245\":1,\"1250\":2,\"1251\":2,\"1255\":2,\"1257\":1,\"1259\":2,\"1261\":3,\"1262\":6,\"1278\":3,\"1279\":2,\"1280\":8,\"1281\":5,\"1282\":5,\"1283\":3,\"1290\":3,\"1301\":2,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1309\":2,\"1318\":1,\"1327\":2,\"1328\":1,\"1329\":1,\"1330\":1,\"1334\":1,\"1346\":1,\"1347\":1,\"1356\":1,\"1369\":2,\"1371\":1,\"1372\":2,\"1382\":2,\"1389\":23,\"1390\":13,\"1391\":2,\"1392\":1,\"1395\":1,\"1396\":12,\"1397\":1,\"1400\":1,\"1401\":13,\"1402\":2,\"1403\":3,\"1408\":13,\"1409\":3,\"1410\":1,\"1413\":13,\"1419\":2,\"1420\":13,\"1441\":1,\"1446\":1,\"1448\":2,\"1450\":3,\"1452\":6,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":13,\"1467\":1,\"1468\":3,\"1469\":2,\"1477\":1,\"1478\":1,\"1484\":9,\"1487\":1,\"1493\":3,\"1494\":3,\"1509\":13,\"1511\":13,\"1513\":3,\"1519\":5,\"1521\":1,\"1524\":1,\"1525\":1,\"1526\":27,\"1529\":1,\"1533\":3,\"1535\":5,\"1536\":12,\"1539\":4,\"1545\":10,\"1546\":5,\"1548\":3,\"1549\":2,\"1551\":1,\"1552\":16,\"1553\":39,\"1556\":1,\"1581\":1,\"1582\":1,\"1583\":2,\"1585\":1,\"1589\":1,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":2,\"1596\":1,\"1597\":1,\"1598\":23,\"1599\":15,\"1600\":25,\"1604\":1,\"1605\":1,\"1606\":1,\"1607\":2,\"1608\":2,\"1609\":1,\"1610\":4,\"1611\":2,\"1612\":2,\"1613\":2,\"1614\":1,\"1615\":1,\"1616\":2,\"1618\":7,\"1619\":1,\"1622\":5,\"1625\":22,\"1626\":14,\"1628\":4,\"1631\":2,\"1634\":1,\"1635\":4,\"1636\":1,\"1638\":4,\"1640\":7,\"1643\":3,\"1645\":2,\"1646\":3,\"1648\":1,\"1650\":2,\"1655\":11,\"1662\":6,\"1664\":1,\"1665\":3,\"1668\":1,\"1672\":1,\"1673\":1,\"1676\":3,\"1679\":3,\"1680\":3,\"1683\":5,\"1686\":4,\"1687\":3,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":2,\"1694\":4,\"1697\":1,\"1698\":3,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1715\":1,\"1717\":1,\"1719\":1,\"1720\":5,\"1721\":6,\"1722\":1,\"1725\":8,\"1726\":6,\"1727\":7,\"1730\":1,\"1736\":2,\"1749\":4,\"1750\":3,\"1752\":1,\"1754\":1,\"1758\":1,\"1759\":1,\"1766\":3,\"1768\":1,\"1770\":1,\"1771\":1,\"1782\":2,\"1784\":2,\"1788\":2,\"1796\":1,\"1806\":2,\"1807\":1,\"1815\":5,\"1824\":1,\"1837\":1,\"1839\":1,\"1856\":4,\"1862\":6,\"1863\":5,\"1873\":1,\"1877\":1,\"1880\":1,\"1902\":123,\"1904\":168,\"1906\":5,\"1909\":6,\"1917\":1,\"1926\":3,\"1928\":3,\"1940\":1,\"1942\":3,\"1945\":3,\"1947\":3,\"1957\":1,\"1959\":3,\"1960\":1,\"1961\":1,\"1962\":7,\"1963\":1,\"1976\":4,\"1980\":1,\"1984\":2,\"1987\":3,\"1988\":1,\"1990\":3,\"1991\":2,\"1992\":9,\"1993\":7,\"1994\":4,\"1995\":9,\"1996\":2,\"1997\":5,\"2000\":2,\"2001\":3,\"2014\":8,\"2015\":3,\"2017\":4,\"2018\":5,\"2019\":4,\"2020\":4,\"2021\":5,\"2049\":1,\"2101\":2,\"2126\":4,\"2127\":7,\"2129\":4,\"2130\":5,\"2131\":1,\"2132\":2,\"2134\":6,\"2136\":1,\"2137\":3,\"2141\":2,\"2155\":4,\"2167\":1,\"2176\":4,\"2191\":8,\"2209\":2,\"2216\":1,\"2219\":1,\"2220\":10,\"2221\":12,\"2224\":3,\"2231\":1,\"2235\":5,\"2236\":6,\"2239\":20,\"2240\":14,\"2245\":10,\"2249\":1,\"2305\":1,\"2307\":6,\"2309\":1,\"2327\":4,\"2335\":2,\"2336\":8,\"2337\":6,\"2340\":2,\"2341\":2,\"2342\":3,\"2346\":5,\"2350\":5,\"2353\":3,\"2355\":5,\"2356\":6,\"2359\":3,\"2360\":6,\"2361\":7,\"2362\":6,\"2363\":7,\"2364\":3,\"2368\":5,\"2369\":2,\"2376\":2,\"2378\":1,\"2390\":1,\"2411\":12,\"2412\":16,\"2416\":1,\"2421\":1,\"2423\":15,\"2427\":2,\"2428\":3,\"2430\":2,\"2431\":10,\"2432\":20,\"2433\":1,\"2442\":8,\"2447\":17,\"2458\":2,\"2460\":3,\"2462\":3,\"2463\":1,\"2464\":1,\"2469\":3,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1}}],[\">>\",{\"1\":{\"536\":1}}],[\">>>\",{\"1\":{\"154\":1,\"986\":1,\"992\":2,\"994\":3,\"997\":2,\"999\":3,\"1002\":2,\"1004\":2,\"1006\":3,\"1008\":4,\"1010\":6,\"1012\":2,\"1014\":3,\"1016\":2,\"1018\":3,\"1020\":2,\"1023\":1,\"1025\":2,\"1027\":1,\"1644\":6,\"1647\":6,\"1774\":1,\"1824\":4,\"1827\":2,\"1830\":2,\"1846\":5,\"1856\":2,\"1882\":2,\"1883\":4,\"1890\":1,\"1902\":9,\"1904\":9,\"1906\":4,\"1909\":3,\"1912\":1,\"1926\":1,\"1933\":6,\"1938\":3,\"1956\":1,\"2134\":3,\"2151\":6,\"2246\":8,\"2248\":8,\"2249\":11,\"2250\":8,\"2251\":8,\"2252\":8,\"2253\":10,\"2254\":8,\"2255\":8,\"2256\":8,\"2257\":8,\"2259\":8,\"2260\":8,\"2261\":8,\"2263\":8,\"2264\":8,\"2265\":6,\"2266\":8,\"2267\":8,\"2268\":8,\"2269\":8,\"2270\":8,\"2271\":8,\"2272\":8,\"2273\":8,\"2290\":2,\"2299\":2,\"2305\":6,\"2310\":6,\"2315\":5,\"2325\":3,\"2327\":3,\"2334\":11,\"2343\":1,\"2352\":2,\"2359\":5,\"2369\":1,\"2377\":9,\"2378\":4,\"2479\":6,\"2485\":7,\"2488\":2,\"2493\":7,\"2500\":5,\"2502\":1,\"2505\":7}}],[\">hello\",{\"1\":{\"288\":1}}],[\">=\",{\"1\":{\"161\":1,\"635\":2,\"652\":1,\"703\":1,\"755\":1,\"785\":1,\"786\":1,\"800\":1,\"881\":1,\"884\":1,\"922\":1,\"936\":1,\"937\":1,\"978\":1,\"1662\":2}}],[\">1\",{\"1\":{\"45\":5}}],[\">\",{\"1\":{\"19\":1,\"44\":1,\"45\":2,\"50\":3,\"51\":3,\"68\":4,\"144\":1,\"145\":1,\"167\":4,\"196\":3,\"197\":1,\"213\":2,\"227\":2,\"268\":3,\"269\":2,\"271\":2,\"277\":3,\"278\":2,\"280\":2,\"286\":1,\"287\":28,\"288\":1,\"289\":2,\"290\":1,\"527\":1,\"535\":1,\"536\":2,\"617\":1,\"618\":1,\"622\":1,\"624\":1,\"633\":1,\"635\":2,\"636\":1,\"638\":2,\"642\":1,\"650\":2,\"686\":4,\"692\":3,\"699\":1,\"706\":1,\"709\":2,\"710\":3,\"711\":3,\"720\":4,\"724\":2,\"725\":2,\"728\":2,\"729\":2,\"731\":1,\"732\":1,\"744\":2,\"756\":3,\"766\":1,\"767\":1,\"768\":5,\"773\":3,\"774\":2,\"775\":1,\"780\":2,\"784\":2,\"828\":3,\"829\":2,\"830\":3,\"833\":1,\"848\":1,\"849\":3,\"850\":1,\"991\":1,\"994\":1,\"1000\":2,\"1003\":1,\"1011\":1,\"1015\":1,\"1064\":1,\"1103\":1,\"1107\":2,\"1108\":1,\"1145\":1,\"1168\":1,\"1205\":1,\"1236\":1,\"1262\":2,\"1264\":1,\"1265\":1,\"1278\":2,\"1281\":1,\"1282\":1,\"1319\":1,\"1322\":1,\"1350\":2,\"1397\":2,\"1552\":3,\"1599\":3,\"1626\":3,\"1655\":2,\"1662\":3,\"1701\":1,\"1717\":1,\"1726\":1,\"1727\":1,\"1735\":2,\"1748\":1,\"1751\":2,\"1759\":2,\"1957\":1,\"1960\":1,\"1961\":1,\"1978\":1,\"1980\":2,\"1982\":1,\"1992\":6,\"1993\":3,\"1995\":6,\"1997\":1,\"2000\":1,\"2001\":1,\"2015\":1,\"2020\":1,\"2129\":3,\"2130\":2,\"2131\":4,\"2198\":1,\"2235\":6,\"2236\":6,\"2239\":3,\"2240\":3,\"2245\":3,\"2249\":2,\"2311\":1,\"2325\":1,\"2327\":1,\"2355\":1,\"2411\":3,\"2412\":3,\"2414\":1,\"2416\":2,\"2418\":1,\"2423\":3,\"2431\":3,\"2432\":3,\"2447\":3}}],[\"zone\",{\"1\":{\"1856\":1}}],[\"zoneoutcell\",{\"0\":{\"1855\":1},\"1\":{\"1855\":1,\"1856\":1}}],[\"zoneout\",{\"1\":{\"1750\":3,\"1855\":4,\"1856\":2,\"1993\":3,\"2223\":3,\"2245\":3,\"2431\":3}}],[\"zoo\",{\"1\":{\"125\":1,\"200\":1,\"205\":1,\"242\":1,\"243\":1,\"286\":1,\"290\":1,\"2045\":2}}],[\"zwicker\",{\"1\":{\"1654\":1}}],[\"z\",{\"1\":{\"287\":1,\"797\":2,\"1063\":1,\"1210\":1,\"1224\":2,\"1225\":2,\"1245\":3,\"1264\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1327\":1,\"1330\":1,\"1334\":1,\"1395\":2,\"1450\":1,\"1452\":1,\"1556\":8,\"1601\":4,\"1610\":4,\"1619\":2,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1768\":2,\"1801\":2,\"2355\":1}}],[\"zenkaku\",{\"1\":{\"290\":1}}],[\"zenodo\",{\"1\":{\"222\":1,\"223\":2,\"290\":2}}],[\"zeroes\",{\"1\":{\"939\":2,\"1918\":1}}],[\"zeroed\",{\"1\":{\"939\":2}}],[\"zero=false\",{\"1\":{\"807\":1}}],[\"zeros\",{\"1\":{\"220\":1,\"979\":1,\"1306\":1,\"1310\":1,\"1371\":1,\"1680\":1,\"1692\":1,\"1698\":1,\"1902\":3,\"1904\":3,\"2130\":1,\"2463\":1,\"2464\":1}}],[\"zero\",{\"0\":{\"929\":1},\"1\":{\"82\":1,\"102\":1,\"211\":1,\"700\":1,\"706\":4,\"709\":3,\"733\":1,\"734\":1,\"755\":3,\"769\":1,\"774\":3,\"780\":3,\"785\":2,\"786\":1,\"787\":1,\"792\":1,\"797\":1,\"800\":1,\"833\":1,\"867\":1,\"878\":2,\"879\":2,\"881\":1,\"882\":2,\"883\":2,\"884\":1,\"921\":1,\"922\":2,\"929\":1,\"935\":1,\"936\":2,\"937\":2,\"1163\":1,\"1164\":2,\"1246\":3,\"1247\":2,\"1306\":1,\"1309\":1,\"1333\":1,\"1371\":1,\"1376\":1,\"1377\":1,\"1493\":1,\"1494\":1,\"1505\":1,\"1506\":1,\"1526\":1,\"1598\":1,\"1599\":3,\"1600\":1,\"1664\":1,\"1665\":1,\"1691\":1,\"1700\":1,\"1785\":3,\"1817\":3,\"1946\":2,\"1963\":1,\"1994\":1,\"2015\":3,\"2126\":1,\"2133\":2,\"2136\":1,\"2143\":1,\"2191\":3,\"2239\":1,\"2240\":1,\"2307\":1,\"2411\":3,\"2412\":3,\"2423\":3,\"2447\":3}}],[\"zipinfo\",{\"1\":{\"1948\":1}}],[\"zip\",{\"1\":{\"127\":1,\"201\":1,\"235\":1,\"290\":1,\"518\":1}}],[\"zh\",{\"1\":{\"276\":1,\"536\":4}}],[\"zhong\",{\"1\":{\"11\":1,\"269\":2,\"278\":2}}],[\"zhuo\",{\"1\":{\"11\":1,\"1117\":1}}],[\"zhang\",{\"1\":{\"8\":1,\"9\":1,\"10\":1,\"11\":2,\"12\":1,\"146\":1,\"1210\":1,\"1264\":1,\"1279\":1,\"1280\":2,\"1281\":2,\"1282\":2,\"1283\":1,\"1334\":1,\"2191\":1}}],[\"zhaoheng\",{\"1\":{\"7\":1,\"10\":1,\"11\":1}}],[\"zakaria\",{\"1\":{\"8\":1}}],[\"2e6\",{\"1\":{\"2151\":1,\"2310\":1}}],[\"2epoch\",{\"1\":{\"113\":2}}],[\"2^\",{\"1\":{\"1854\":1}}],[\"2tuple\",{\"0\":{\"1362\":1},\"1\":{\"1362\":1}}],[\"2columns\",{\"0\":{\"1022\":1},\"1\":{\"1022\":1,\"1023\":1}}],[\"2ch\",{\"1\":{\"70\":1}}],[\"2best\",{\"1\":{\"267\":1,\"276\":1,\"2359\":2}}],[\"27\",{\"1\":{\"243\":1,\"261\":2}}],[\"278\",{\"1\":{\"113\":1}}],[\"267\",{\"1\":{\"1618\":1}}],[\"26\",{\"1\":{\"242\":1,\"1309\":1,\"1311\":1}}],[\"2mix\",{\"1\":{\"223\":1,\"224\":2}}],[\"2816\",{\"1\":{\"709\":1,\"774\":1,\"780\":1,\"1785\":1,\"1786\":1,\"1817\":1,\"1818\":1,\"2191\":1}}],[\"288\",{\"1\":{\"615\":1}}],[\"28\",{\"1\":{\"261\":2}}],[\"28518\",{\"1\":{\"202\":1}}],[\"28492\",{\"1\":{\"202\":1}}],[\"280e\",{\"1\":{\"113\":1}}],[\"2pass\",{\"1\":{\"180\":1}}],[\"2+\",{\"1\":{\"159\":1,\"692\":2,\"790\":1,\"850\":1,\"1901\":1,\"1903\":1,\"1992\":1}}],[\"2911179\",{\"1\":{\"1309\":1,\"1311\":1}}],[\"29430212\",{\"1\":{\"1117\":1}}],[\"29\",{\"1\":{\"202\":1}}],[\"2958\",{\"1\":{\"156\":1}}],[\"2985\",{\"1\":{\"39\":1}}],[\"21\",{\"1\":{\"1000\":1,\"1509\":4,\"1511\":4,\"1553\":4}}],[\"2107\",{\"1\":{\"1385\":1,\"1439\":1,\"1967\":1,\"1994\":1}}],[\"2106\",{\"1\":{\"974\":1,\"1683\":1,\"1784\":2}}],[\"2108\",{\"1\":{\"652\":1,\"1577\":1}}],[\"21437\",{\"1\":{\"156\":1}}],[\"2110\",{\"1\":{\"9\":1,\"2176\":1}}],[\"2222\",{\"1\":{\"1883\":2}}],[\"2212\",{\"1\":{\"699\":1}}],[\"2210\",{\"1\":{\"624\":1,\"1396\":1}}],[\"2211\",{\"1\":{\"156\":1,\"696\":1,\"785\":1,\"786\":1,\"866\":2,\"882\":1,\"883\":1,\"884\":1,\"921\":2,\"922\":2,\"1269\":1}}],[\"22k\",{\"1\":{\"286\":6}}],[\"2202\",{\"1\":{\"1768\":1,\"2223\":2,\"2227\":1,\"2231\":1,\"2245\":1}}],[\"2206\",{\"1\":{\"1515\":1,\"1541\":1}}],[\"2209\",{\"1\":{\"634\":1,\"1269\":1}}],[\"2201\",{\"1\":{\"616\":1}}],[\"22050\",{\"1\":{\"286\":3,\"1419\":1,\"1511\":1,\"1524\":1,\"1526\":2,\"1534\":1,\"1549\":1,\"1552\":1,\"1553\":2,\"1598\":2,\"1600\":2,\"1607\":1,\"1625\":2,\"2232\":1,\"2238\":1,\"2404\":1,\"2409\":1,\"2434\":1,\"2439\":1}}],[\"2207\",{\"1\":{\"156\":1,\"617\":1,\"2428\":1}}],[\"22\",{\"1\":{\"261\":2,\"267\":1,\"286\":7,\"536\":12}}],[\"2d\",{\"0\":{\"1301\":1,\"1306\":1,\"1344\":1,\"1345\":1,\"1371\":1,\"1372\":1,\"1430\":1,\"1437\":1,\"1444\":1,\"1448\":1,\"1452\":1,\"1456\":1,\"1460\":1,\"1482\":1,\"1483\":1,\"1494\":1,\"1503\":1,\"1506\":1},\"1\":{\"141\":1,\"706\":1,\"730\":1,\"806\":1,\"993\":1,\"1007\":1,\"1028\":1,\"1119\":1,\"1168\":1,\"1301\":2,\"1306\":3,\"1344\":1,\"1345\":1,\"1371\":3,\"1372\":2,\"1374\":2,\"1375\":2,\"1430\":1,\"1437\":1,\"1444\":1,\"1448\":1,\"1452\":1,\"1456\":1,\"1460\":1,\"1482\":1,\"1483\":1,\"1494\":1,\"1503\":1,\"1506\":1,\"1711\":1,\"1719\":1,\"1731\":2,\"1741\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1787\":1,\"1946\":1,\"2139\":1}}],[\"2nodes\",{\"1\":{\"121\":1}}],[\"257\",{\"1\":{\"1127\":1}}],[\"251\",{\"1\":{\"691\":1}}],[\"25000\",{\"1\":{\"625\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1}}],[\"2500000steps\",{\"1\":{\"286\":2}}],[\"250k\",{\"1\":{\"267\":1}}],[\"25\",{\"1\":{\"141\":6,\"147\":1,\"261\":1,\"286\":2,\"615\":1,\"661\":1,\"820\":1,\"1389\":3,\"1390\":3,\"1413\":4,\"1420\":3,\"1509\":1,\"1511\":1,\"1553\":1,\"1716\":1,\"2350\":1,\"2363\":1,\"2463\":1,\"2464\":1}}],[\"255\",{\"1\":{\"71\":1}}],[\"256\",{\"1\":{\"43\":4,\"101\":1,\"141\":4,\"142\":6,\"143\":1,\"286\":3,\"536\":15,\"632\":1,\"641\":2,\"651\":1,\"691\":1,\"700\":1,\"709\":1,\"710\":1,\"711\":1,\"733\":1,\"734\":1,\"745\":1,\"747\":1,\"768\":1,\"771\":1,\"774\":1,\"780\":1,\"846\":1,\"849\":1,\"1118\":4,\"1124\":1,\"1125\":1,\"1141\":1,\"1235\":1,\"1268\":1,\"1280\":2,\"1282\":1,\"1283\":2,\"1385\":1,\"1392\":1,\"1396\":3,\"1397\":3,\"1401\":1,\"1402\":1,\"1408\":4,\"1409\":3,\"1422\":1,\"1441\":1,\"1466\":1,\"1467\":1,\"1509\":18,\"1511\":19,\"1524\":1,\"1525\":1,\"1526\":1,\"1534\":2,\"1549\":1,\"1552\":1,\"1553\":19,\"1598\":2,\"1599\":1,\"1600\":1,\"1607\":1,\"1625\":1,\"1947\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"2126\":1,\"2129\":1,\"2191\":1,\"2235\":2,\"2236\":2,\"2245\":1,\"2363\":1,\"2379\":1,\"2404\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2423\":1,\"2428\":4,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":3,\"2434\":1}}],[\"2gpus\",{\"0\":{\"58\":1,\"61\":1,\"62\":1,\"64\":1},\"1\":{\"121\":1}}],[\"2hosts\",{\"0\":{\"61\":1,\"62\":1,\"64\":1}}],[\"2host\",{\"0\":{\"58\":1}}],[\"2406\",{\"1\":{\"2239\":1}}],[\"2407\",{\"1\":{\"780\":1}}],[\"24000\",{\"1\":{\"267\":11,\"1389\":4,\"1390\":2,\"1391\":1,\"1396\":2,\"1401\":2,\"1403\":1,\"1420\":1,\"1466\":2,\"1468\":1,\"1533\":1,\"1539\":1}}],[\"2444\",{\"1\":{\"696\":1,\"697\":1}}],[\"24k\",{\"1\":{\"267\":2,\"536\":11}}],[\"24\",{\"1\":{\"47\":1,\"242\":1,\"261\":2,\"267\":2,\"286\":1,\"548\":1,\"551\":1,\"606\":1,\"746\":1,\"2434\":1}}],[\"2x\",{\"1\":{\"43\":1,\"141\":1,\"1051\":1}}],[\"23rd\",{\"1\":{\"276\":1}}],[\"2309\",{\"1\":{\"244\":1}}],[\"2305\",{\"1\":{\"207\":1,\"643\":1,\"2000\":1,\"2001\":1}}],[\"2310\",{\"1\":{\"202\":1}}],[\"231\",{\"1\":{\"136\":1}}],[\"23\",{\"1\":{\"39\":1,\"202\":1,\"261\":2,\"276\":5,\"780\":1}}],[\"2047\",{\"1\":{\"2130\":1}}],[\"2048\",{\"1\":{\"142\":2,\"267\":5,\"286\":2,\"536\":8,\"633\":1,\"634\":1,\"647\":1,\"674\":2,\"700\":1,\"709\":1,\"710\":1,\"711\":1,\"731\":1,\"732\":1,\"733\":2,\"734\":2,\"766\":1,\"767\":1,\"774\":1,\"775\":1,\"780\":2,\"848\":1,\"849\":1,\"850\":1,\"1389\":1,\"1390\":1,\"1396\":2,\"1397\":2,\"1408\":2,\"1409\":2,\"1420\":2,\"1422\":2,\"1618\":1,\"1992\":1,\"1995\":1,\"2126\":1,\"2129\":1,\"2191\":1,\"2434\":2}}],[\"2079\",{\"1\":{\"650\":1}}],[\"2014\",{\"1\":{\"1327\":1,\"1330\":1}}],[\"2016\",{\"1\":{\"1130\":2,\"1131\":1,\"1172\":1}}],[\"2017\",{\"1\":{\"1117\":1,\"1126\":1,\"1149\":1}}],[\"2011\",{\"1\":{\"1066\":1}}],[\"2010\",{\"1\":{\"768\":1,\"1321\":1,\"1322\":1,\"1590\":1,\"1668\":1,\"1977\":1}}],[\"2018\",{\"1\":{\"156\":2,\"1176\":1,\"1327\":1,\"1330\":1,\"1717\":1,\"2208\":1,\"2209\":1}}],[\"2012\",{\"1\":{\"145\":1}}],[\"2019\",{\"1\":{\"106\":1,\"1309\":2,\"1311\":2,\"1730\":1}}],[\"20ms\",{\"1\":{\"128\":1,\"242\":1}}],[\"20batch\",{\"1\":{\"113\":1}}],[\"203\",{\"1\":{\"113\":1}}],[\"2006\",{\"1\":{\"1720\":1,\"1721\":1,\"1731\":2}}],[\"2004\",{\"1\":{\"1319\":1,\"1330\":2}}],[\"2007\",{\"1\":{\"1318\":1}}],[\"2003\",{\"1\":{\"1252\":1}}],[\"2008\",{\"1\":{\"1170\":1}}],[\"2000\",{\"1\":{\"1012\":1,\"1014\":1}}],[\"200000\",{\"1\":{\"100\":1}}],[\"2005\",{\"1\":{\"974\":1,\"2183\":1}}],[\"2002\",{\"1\":{\"696\":1,\"697\":1}}],[\"200\",{\"1\":{\"94\":1,\"2428\":2}}],[\"20\",{\"1\":{\"39\":1,\"94\":2,\"98\":2,\"141\":2,\"160\":1,\"243\":1,\"262\":1,\"635\":2,\"664\":1,\"778\":1,\"833\":1,\"947\":1,\"949\":1,\"1000\":1,\"1136\":1,\"1141\":1,\"1252\":2,\"1261\":1,\"1280\":1,\"1283\":1,\"1509\":4,\"1511\":4,\"1553\":4,\"1656\":1,\"1671\":1,\"1700\":1,\"1728\":1,\"1802\":1,\"1850\":1,\"1991\":1,\"2350\":2,\"2423\":1,\"2428\":2}}],[\"202\",{\"1\":{\"202\":1}}],[\"2022\",{\"1\":{\"11\":1,\"12\":1,\"13\":1,\"16\":1,\"139\":1,\"256\":1,\"285\":1,\"750\":1,\"1210\":1,\"1264\":1,\"1269\":2,\"1334\":1,\"2191\":1,\"2192\":1,\"2203\":1}}],[\"2021\",{\"1\":{\"9\":1,\"11\":1,\"45\":1,\"46\":1,\"139\":1,\"145\":2,\"146\":1,\"179\":1,\"1066\":1,\"1131\":2,\"1172\":2}}],[\"2020\",{\"1\":{\"9\":1,\"10\":1,\"45\":3,\"113\":3,\"145\":2,\"207\":4,\"691\":1,\"1125\":1,\"1170\":1,\"1185\":1,\"1252\":1,\"2168\":1,\"2176\":1,\"2187\":1}}],[\"2023\",{\"1\":{\"6\":1,\"7\":1,\"10\":1,\"14\":1,\"15\":1,\"202\":2,\"207\":1,\"240\":1,\"243\":1,\"244\":1,\"260\":1,\"1061\":3,\"1062\":3,\"1270\":2,\"1271\":2,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1}}],[\"2024\",{\"1\":{\"5\":1,\"6\":2,\"8\":1,\"267\":1,\"1280\":1,\"1281\":1,\"1282\":1}}],[\"2>\",{\"1\":{\"24\":1}}],[\"2\",{\"0\":{\"23\":1,\"162\":1},\"1\":{\"6\":1,\"23\":1,\"24\":2,\"39\":1,\"40\":2,\"41\":1,\"43\":3,\"44\":2,\"50\":1,\"51\":1,\"58\":4,\"61\":1,\"62\":2,\"64\":2,\"71\":1,\"80\":3,\"93\":2,\"94\":2,\"102\":1,\"113\":1,\"117\":2,\"118\":1,\"121\":2,\"126\":1,\"141\":2,\"142\":2,\"144\":1,\"145\":4,\"160\":1,\"167\":2,\"168\":1,\"173\":1,\"175\":1,\"188\":1,\"195\":1,\"199\":1,\"204\":1,\"210\":1,\"212\":2,\"216\":1,\"218\":2,\"222\":3,\"223\":3,\"224\":2,\"225\":1,\"227\":1,\"228\":1,\"234\":1,\"240\":2,\"243\":1,\"253\":1,\"261\":2,\"265\":4,\"267\":12,\"269\":1,\"272\":1,\"274\":3,\"276\":11,\"278\":1,\"284\":1,\"285\":3,\"286\":14,\"287\":4,\"289\":1,\"290\":1,\"522\":2,\"616\":3,\"617\":7,\"618\":7,\"619\":1,\"620\":9,\"622\":1,\"623\":1,\"624\":7,\"625\":1,\"627\":1,\"629\":1,\"635\":1,\"636\":4,\"637\":1,\"644\":19,\"645\":1,\"647\":2,\"664\":1,\"674\":10,\"675\":1,\"692\":4,\"696\":3,\"697\":3,\"699\":1,\"702\":8,\"705\":1,\"715\":2,\"720\":1,\"738\":1,\"756\":1,\"773\":1,\"783\":2,\"786\":2,\"790\":2,\"798\":2,\"800\":1,\"804\":1,\"815\":1,\"833\":2,\"846\":8,\"850\":2,\"866\":1,\"867\":1,\"911\":1,\"921\":2,\"922\":1,\"932\":1,\"934\":1,\"935\":1,\"948\":2,\"949\":1,\"977\":1,\"1002\":1,\"1010\":1,\"1020\":2,\"1022\":1,\"1025\":4,\"1026\":1,\"1054\":1,\"1061\":3,\"1062\":2,\"1063\":1,\"1107\":1,\"1108\":1,\"1117\":2,\"1118\":3,\"1119\":1,\"1124\":3,\"1125\":5,\"1126\":1,\"1130\":2,\"1131\":3,\"1136\":1,\"1141\":1,\"1147\":1,\"1155\":1,\"1158\":1,\"1172\":1,\"1198\":2,\"1210\":1,\"1211\":5,\"1217\":1,\"1224\":1,\"1225\":2,\"1232\":1,\"1252\":2,\"1261\":1,\"1267\":1,\"1268\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1274\":1,\"1278\":1,\"1280\":5,\"1281\":1,\"1282\":1,\"1283\":3,\"1301\":1,\"1306\":1,\"1316\":2,\"1320\":1,\"1325\":1,\"1327\":1,\"1328\":2,\"1330\":2,\"1334\":1,\"1350\":1,\"1356\":1,\"1371\":1,\"1372\":1,\"1385\":10,\"1389\":7,\"1390\":1,\"1391\":5,\"1392\":3,\"1396\":6,\"1400\":1,\"1401\":21,\"1402\":15,\"1403\":5,\"1408\":9,\"1409\":6,\"1410\":2,\"1419\":2,\"1420\":2,\"1441\":1,\"1450\":4,\"1452\":4,\"1454\":4,\"1456\":4,\"1458\":1,\"1460\":1,\"1462\":1,\"1466\":20,\"1467\":14,\"1468\":5,\"1469\":1,\"1484\":5,\"1509\":14,\"1511\":18,\"1513\":3,\"1514\":8,\"1519\":4,\"1520\":1,\"1526\":9,\"1533\":1,\"1534\":8,\"1535\":2,\"1536\":3,\"1539\":1,\"1546\":1,\"1548\":4,\"1549\":9,\"1551\":3,\"1552\":5,\"1553\":23,\"1592\":2,\"1593\":1,\"1594\":4,\"1595\":5,\"1597\":2,\"1598\":13,\"1599\":7,\"1600\":12,\"1604\":1,\"1605\":3,\"1606\":2,\"1607\":1,\"1609\":3,\"1610\":1,\"1615\":1,\"1618\":3,\"1619\":13,\"1620\":1,\"1621\":2,\"1622\":1,\"1625\":9,\"1626\":3,\"1628\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":3,\"1677\":1,\"1691\":1,\"1722\":1,\"1729\":1,\"1730\":1,\"1736\":4,\"1738\":2,\"1739\":3,\"1740\":2,\"1742\":1,\"1743\":3,\"1757\":3,\"1766\":1,\"1790\":3,\"1807\":1,\"1814\":1,\"1816\":1,\"1817\":2,\"1820\":1,\"1846\":2,\"1860\":1,\"1878\":6,\"1883\":1,\"1901\":2,\"1902\":4,\"1903\":2,\"1904\":4,\"1906\":7,\"1908\":1,\"1909\":1,\"1945\":1,\"1947\":1,\"1957\":1,\"1992\":2,\"1993\":2,\"1994\":4,\"2000\":5,\"2001\":2,\"2018\":1,\"2049\":2,\"2134\":2,\"2151\":1,\"2168\":2,\"2176\":2,\"2183\":2,\"2198\":4,\"2208\":1,\"2209\":2,\"2215\":1,\"2216\":1,\"2236\":1,\"2239\":2,\"2240\":1,\"2245\":3,\"2254\":1,\"2255\":1,\"2256\":1,\"2273\":1,\"2305\":1,\"2310\":1,\"2341\":1,\"2355\":1,\"2359\":1,\"2363\":1,\"2411\":2,\"2412\":5,\"2423\":4,\"2425\":1,\"2429\":1,\"2431\":3,\"2432\":4,\"2433\":2,\"2435\":1,\"2443\":1,\"2447\":3,\"2458\":1,\"2462\":1,\"2472\":1,\"2479\":1,\"2490\":1,\"2495\":1}}],[\"1best\",{\"1\":{\"2359\":2}}],[\"1b\",{\"1\":{\"2049\":2,\"2055\":2}}],[\"1ch\",{\"1\":{\"1155\":2,\"1157\":2,\"1734\":1}}],[\"1c|c0=c0\",{\"1\":{\"74\":2}}],[\"1$\",{\"1\":{\"1119\":2}}],[\"1x1\",{\"1\":{\"978\":3,\"1273\":1,\"1274\":1,\"1303\":1,\"1346\":1,\"1579\":2}}],[\"1s\",{\"1\":{\"787\":1}}],[\"1+2\",{\"1\":{\"265\":1,\"267\":1}}],[\"1706\",{\"1\":{\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"2016\":1}}],[\"1704\",{\"1\":{\"1705\":1}}],[\"1705\",{\"1\":{\"1411\":1}}],[\"1710\",{\"1\":{\"652\":1}}],[\"17\",{\"1\":{\"205\":1,\"228\":3}}],[\"1796\",{\"1\":{\"156\":1}}],[\"173\",{\"1\":{\"113\":1}}],[\"14941\",{\"1\":{\"1720\":1,\"1721\":1,\"1731\":2}}],[\"142\",{\"1\":{\"1526\":1,\"1600\":1,\"1608\":1,\"1618\":1,\"1631\":1}}],[\"1456\",{\"1\":{\"156\":1}}],[\"14\",{\"1\":{\"141\":1,\"199\":1,\"201\":1,\"204\":1,\"228\":2,\"240\":1,\"242\":2,\"1731\":2}}],[\"14x\",{\"1\":{\"141\":1}}],[\"19\",{\"1\":{\"1701\":1}}],[\"1980\",{\"1\":{\"1654\":1}}],[\"195\",{\"1\":{\"1608\":1}}],[\"1901\",{\"1\":{\"1785\":1,\"1786\":1,\"1817\":1,\"1818\":1}}],[\"1904\",{\"1\":{\"1748\":1}}],[\"1909\",{\"1\":{\"974\":1}}],[\"1908\",{\"1\":{\"635\":1}}],[\"1910\",{\"1\":{\"640\":1,\"648\":1,\"711\":1}}],[\"19452\",{\"1\":{\"136\":1}}],[\"1920928955078125e\",{\"1\":{\"1248\":1}}],[\"192khz\",{\"1\":{\"71\":2}}],[\"192\",{\"1\":{\"66\":1,\"113\":1,\"1509\":1,\"1511\":1,\"1519\":2,\"1524\":1,\"1525\":1,\"1535\":2,\"1536\":2,\"1546\":1,\"1552\":1,\"1553\":2,\"1611\":2,\"1612\":2,\"1613\":2,\"1616\":1,\"1622\":1,\"1625\":1,\"1626\":1}}],[\"114348\",{\"1\":{\"1963\":1}}],[\"11660\",{\"1\":{\"1748\":1}}],[\"11273\",{\"1\":{\"1170\":1}}],[\"1109\",{\"1\":{\"207\":1,\"1309\":1,\"1311\":1}}],[\"11\",{\"1\":{\"73\":1,\"113\":1,\"196\":1,\"199\":1,\"201\":1,\"204\":1,\"222\":1,\"223\":3,\"228\":2,\"240\":1,\"243\":2,\"284\":1,\"650\":1,\"731\":6,\"732\":6,\"766\":6,\"767\":6,\"1000\":2,\"1389\":2,\"1390\":1,\"1396\":1,\"1401\":2,\"1402\":1,\"1408\":4,\"1409\":1,\"1410\":2,\"1419\":1,\"1420\":2,\"1466\":1,\"1509\":11,\"1511\":11,\"1513\":2,\"1526\":2,\"1548\":2,\"1549\":1,\"1551\":1,\"1552\":2,\"1553\":14,\"1592\":1,\"1593\":1,\"1595\":1,\"1598\":2,\"1599\":1,\"1600\":2,\"1619\":1,\"1625\":2,\"1626\":1,\"2427\":2}}],[\"1khz\",{\"1\":{\"70\":1}}],[\"1gpu\",{\"0\":{\"61\":1}}],[\"129\",{\"1\":{\"1526\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1}}],[\"12943\",{\"1\":{\"652\":1}}],[\"12433\",{\"1\":{\"1269\":1}}],[\"1211\",{\"1\":{\"616\":1,\"696\":2,\"697\":2}}],[\"1200\",{\"1\":{\"267\":5,\"536\":8}}],[\"128\",{\"1\":{\"141\":1,\"142\":2,\"276\":17,\"286\":2,\"633\":1,\"634\":1,\"674\":2,\"720\":1,\"846\":1,\"947\":1,\"949\":1,\"976\":1,\"978\":1,\"980\":1,\"1118\":2,\"1119\":1,\"1124\":1,\"1125\":1,\"1250\":1,\"1251\":1,\"1252\":2,\"1267\":1,\"1268\":3,\"1274\":1,\"1280\":1,\"1283\":1,\"1389\":1,\"1391\":1,\"1396\":4,\"1397\":3,\"1401\":2,\"1402\":1,\"1403\":1,\"1408\":4,\"1409\":4,\"1410\":1,\"1422\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1466\":2,\"1467\":1,\"1468\":1,\"1509\":6,\"1511\":6,\"1526\":1,\"1533\":2,\"1534\":1,\"1549\":1,\"1553\":7,\"1594\":1,\"1595\":1,\"1597\":1,\"1598\":4,\"1599\":3,\"1600\":4,\"1610\":1,\"1619\":1,\"1625\":1,\"1628\":1,\"1644\":1,\"1647\":1,\"1660\":1,\"1669\":1,\"1748\":1,\"1947\":1,\"1994\":2,\"2196\":1,\"2200\":1,\"2203\":1,\"2232\":1,\"2238\":1,\"2245\":3,\"2411\":3,\"2412\":3,\"2423\":3,\"2425\":3,\"2429\":3,\"2430\":1,\"2431\":3,\"2432\":3,\"2472\":1}}],[\"123560715\",{\"1\":{\"2176\":1}}],[\"123\",{\"1\":{\"988\":2,\"990\":2,\"2151\":2,\"2310\":2}}],[\"1230\",{\"1\":{\"97\":2}}],[\"1234\",{\"1\":{\"80\":1,\"1883\":2,\"2151\":1,\"2310\":1}}],[\"12\",{\"1\":{\"53\":1,\"60\":1,\"62\":1,\"63\":1,\"64\":1,\"73\":1,\"106\":1,\"131\":1,\"136\":1,\"137\":1,\"199\":1,\"201\":2,\"204\":1,\"222\":1,\"223\":2,\"228\":2,\"240\":1,\"242\":1,\"243\":2,\"674\":4,\"700\":1,\"733\":1,\"734\":1,\"748\":2,\"846\":2,\"1350\":1,\"1382\":1,\"1692\":1,\"2065\":1,\"2435\":1}}],[\"156\",{\"1\":{\"97\":2}}],[\"15\",{\"1\":{\"73\":1,\"200\":1,\"204\":1,\"205\":1,\"228\":2,\"240\":1,\"242\":2,\"699\":1,\"768\":1,\"780\":1,\"832\":1,\"1250\":1,\"1251\":1,\"1311\":1,\"1326\":1,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":2,\"1402\":1,\"1403\":1,\"1408\":2,\"1409\":1,\"1410\":1,\"1466\":2,\"1467\":1,\"1468\":1,\"1509\":1,\"1511\":1,\"1526\":1,\"1549\":1,\"1553\":2,\"1594\":1,\"1595\":1,\"1597\":1,\"1598\":1,\"1600\":1,\"1604\":1,\"1625\":1,\"1993\":1,\"2167\":1,\"2245\":1,\"2431\":1}}],[\"1500\",{\"1\":{\"2209\":1}}],[\"15003\",{\"1\":{\"1066\":1}}],[\"1506\",{\"1\":{\"1710\":1}}],[\"150\",{\"1\":{\"48\":2}}],[\"1536\",{\"1\":{\"43\":1,\"1107\":1,\"1278\":1,\"1526\":2,\"1600\":2,\"2183\":1,\"2187\":1,\"2190\":1,\"2192\":1,\"2203\":1,\"2208\":1,\"2239\":2,\"2240\":2,\"2411\":2,\"2412\":2,\"2423\":1,\"2447\":2}}],[\"1d\",{\"0\":{\"886\":1},\"1\":{\"43\":1,\"141\":3,\"760\":1,\"797\":1,\"820\":1,\"886\":1,\"1119\":1,\"1505\":1,\"1506\":1,\"1725\":1,\"1731\":2,\"1733\":1,\"1736\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1787\":1,\"1805\":1,\"1822\":1,\"1944\":1,\"1946\":1,\"1947\":1,\"1966\":1,\"2137\":2,\"2432\":1}}],[\"1330\",{\"1\":{\"2007\":1}}],[\"13516\",{\"1\":{\"2000\":1,\"2001\":1}}],[\"13404\",{\"1\":{\"1515\":1,\"1541\":1}}],[\"13438\",{\"1\":{\"1396\":1}}],[\"130\",{\"1\":{\"1029\":1,\"1235\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1698\":1}}],[\"13048\",{\"1\":{\"643\":1}}],[\"13\",{\"1\":{\"199\":1,\"200\":1,\"201\":2,\"204\":1,\"222\":1,\"223\":2,\"228\":2,\"240\":1,\"1509\":1,\"1511\":1,\"1553\":1}}],[\"137762\",{\"1\":{\"136\":1}}],[\"137936\",{\"1\":{\"39\":1}}],[\"13876\",{\"1\":{\"244\":1}}],[\"138175\",{\"1\":{\"39\":1}}],[\"138049\",{\"1\":{\"39\":1}}],[\"1386\",{\"1\":{\"39\":1}}],[\"136473\",{\"1\":{\"39\":1}}],[\"136320\",{\"1\":{\"39\":1}}],[\"136184\",{\"1\":{\"39\":1}}],[\"1e\",{\"1\":{\"39\":6,\"141\":7,\"640\":1,\"648\":1,\"691\":1,\"854\":1,\"1065\":1,\"1126\":3,\"1127\":3,\"1130\":1,\"1217\":4,\"1280\":1,\"1283\":1,\"1309\":2,\"1310\":2,\"1311\":2,\"1317\":1,\"1318\":2,\"1319\":2,\"1321\":2,\"1322\":2,\"1323\":2,\"1326\":1,\"1327\":2,\"1329\":2,\"1330\":2,\"1361\":2,\"1377\":1,\"1382\":1,\"1400\":1,\"1469\":1,\"1491\":1,\"1583\":1,\"1603\":1,\"1656\":1,\"1671\":1,\"1700\":1,\"2439\":1}}],[\"1809\",{\"1\":{\"1820\":1}}],[\"1807\",{\"1\":{\"1708\":1,\"1709\":1}}],[\"1801\",{\"1\":{\"652\":1,\"2167\":1}}],[\"1812\",{\"1\":{\"629\":1}}],[\"18108\",{\"1\":{\"207\":1}}],[\"18\",{\"0\":{\"29\":1},\"1\":{\"26\":1,\"204\":1}}],[\"1011\",{\"1\":{\"2168\":1}}],[\"1016\",{\"1\":{\"1330\":1}}],[\"1023\",{\"1\":{\"1002\":1,\"2130\":1}}],[\"1024\",{\"1\":{\"71\":1,\"128\":1,\"141\":1,\"142\":5,\"243\":1,\"276\":1,\"286\":3,\"536\":15,\"633\":1,\"634\":2,\"643\":1,\"746\":1,\"748\":2,\"1385\":2,\"1389\":3,\"1390\":2,\"1391\":1,\"1392\":3,\"1396\":3,\"1397\":2,\"1401\":5,\"1402\":4,\"1403\":1,\"1408\":5,\"1409\":4,\"1410\":1,\"1420\":3,\"1422\":2,\"1441\":1,\"1466\":4,\"1467\":3,\"1468\":1,\"1509\":9,\"1511\":9,\"1525\":2,\"1526\":3,\"1549\":2,\"1552\":2,\"1553\":12,\"1593\":1,\"1594\":1,\"1595\":2,\"1596\":1,\"1597\":1,\"1598\":5,\"1599\":2,\"1600\":3,\"1604\":1,\"1606\":1,\"1607\":1,\"1618\":1,\"1625\":3,\"1643\":1,\"1646\":1,\"1947\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1993\":1,\"1994\":1,\"2130\":1,\"2187\":1,\"2192\":1,\"2203\":1,\"2232\":1,\"2235\":2,\"2236\":2,\"2238\":1,\"2245\":1,\"2378\":1,\"2404\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2431\":1,\"2432\":2}}],[\"10447\",{\"1\":{\"1577\":1}}],[\"104\",{\"1\":{\"746\":1}}],[\"10ms\",{\"1\":{\"135\":2}}],[\"10batch\",{\"1\":{\"113\":1}}],[\"10096020\",{\"1\":{\"1061\":1,\"1062\":1}}],[\"100k\",{\"1\":{\"286\":1}}],[\"10000\",{\"1\":{\"99\":1,\"2021\":1}}],[\"1000000\",{\"1\":{\"523\":1}}],[\"10000000\",{\"1\":{\"243\":1}}],[\"100000\",{\"1\":{\"48\":1}}],[\"1000\",{\"1\":{\"91\":1,\"135\":1,\"1644\":1,\"1647\":1,\"1655\":2,\"1687\":1,\"1690\":1,\"2134\":1,\"2307\":1,\"2354\":1,\"2378\":1,\"2423\":1}}],[\"100\",{\"1\":{\"48\":3,\"90\":1,\"184\":1,\"243\":2,\"748\":1,\"752\":1,\"774\":6,\"777\":1,\"796\":2,\"846\":1,\"869\":2,\"1010\":1,\"1156\":1,\"1171\":1,\"1547\":1,\"1679\":1,\"1716\":1,\"1750\":1,\"1824\":2,\"1846\":1,\"1940\":1,\"1942\":1,\"2224\":1,\"2307\":2}}],[\"10654\",{\"1\":{\"974\":1}}],[\"10655\",{\"1\":{\"634\":1}}],[\"106\",{\"1\":{\"39\":1}}],[\"10\",{\"1\":{\"26\":4,\"29\":2,\"31\":1,\"39\":1,\"80\":1,\"84\":1,\"94\":4,\"96\":1,\"97\":1,\"120\":2,\"126\":1,\"135\":1,\"156\":1,\"159\":1,\"161\":1,\"162\":2,\"175\":6,\"199\":1,\"200\":1,\"204\":1,\"207\":1,\"210\":1,\"211\":1,\"222\":1,\"223\":1,\"228\":2,\"234\":1,\"235\":2,\"240\":1,\"243\":4,\"253\":1,\"254\":1,\"262\":1,\"267\":2,\"274\":1,\"276\":2,\"284\":1,\"285\":1,\"286\":3,\"635\":1,\"674\":8,\"702\":1,\"747\":1,\"795\":1,\"796\":1,\"846\":3,\"869\":1,\"1000\":1,\"1029\":1,\"1235\":1,\"1246\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1309\":1,\"1311\":1,\"1317\":1,\"1330\":1,\"1377\":2,\"1400\":1,\"1419\":1,\"1509\":7,\"1511\":7,\"1553\":8,\"1581\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1607\":1,\"1609\":1,\"1638\":1,\"1701\":2,\"1750\":7,\"1791\":1,\"1836\":1,\"1846\":1,\"1854\":2,\"1880\":1,\"1900\":1,\"1925\":1,\"1976\":1,\"1980\":1,\"1993\":1,\"2065\":1,\"2220\":2,\"2224\":7,\"2245\":1,\"2305\":1,\"2311\":1,\"2336\":1,\"2337\":1,\"2346\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2364\":1,\"2368\":1,\"2411\":1,\"2412\":1,\"2416\":1,\"2423\":1,\"2429\":1,\"2430\":1,\"2431\":2,\"2432\":2}}],[\"16k\",{\"1\":{\"1957\":1}}],[\"16khz\",{\"1\":{\"70\":2}}],[\"1609\",{\"1\":{\"1530\":1}}],[\"1600\",{\"1\":{\"699\":1}}],[\"16000\",{\"1\":{\"135\":1,\"220\":1,\"243\":1,\"276\":6,\"527\":1,\"639\":1,\"674\":2,\"702\":1,\"720\":1,\"738\":1,\"748\":1,\"752\":1,\"759\":1,\"768\":1,\"815\":1,\"864\":1,\"1010\":3,\"1162\":1,\"1250\":1,\"1251\":1,\"1408\":2,\"1410\":1,\"1533\":1,\"1539\":1,\"1662\":1,\"1668\":1,\"1679\":1,\"1824\":1,\"1827\":1,\"1830\":1,\"1846\":1,\"1980\":1,\"2065\":2,\"2353\":1,\"2357\":1,\"2360\":1,\"2361\":1,\"2364\":1,\"2416\":1}}],[\"160\",{\"1\":{\"243\":1,\"778\":1,\"831\":1}}],[\"16xlarge\",{\"1\":{\"66\":1}}],[\"168\",{\"1\":{\"66\":1}}],[\"1616\",{\"1\":{\"39\":1}}],[\"16\",{\"1\":{\"26\":1,\"27\":2,\"39\":1,\"113\":3,\"130\":2,\"199\":1,\"204\":1,\"228\":2,\"242\":1,\"243\":3,\"267\":1,\"286\":1,\"548\":1,\"551\":1,\"606\":1,\"674\":2,\"691\":1,\"710\":2,\"711\":2,\"746\":1,\"820\":1,\"846\":1,\"1062\":1,\"1124\":1,\"1125\":1,\"1211\":1,\"1401\":1,\"1402\":1,\"1408\":2,\"1409\":1,\"1410\":1,\"1466\":1,\"1467\":1,\"1509\":8,\"1511\":10,\"1513\":2,\"1514\":2,\"1526\":3,\"1534\":2,\"1548\":4,\"1549\":3,\"1551\":2,\"1552\":5,\"1553\":12,\"1592\":2,\"1594\":1,\"1595\":1,\"1597\":1,\"1598\":3,\"1599\":2,\"1600\":3,\"1604\":1,\"1606\":1,\"1611\":1,\"1618\":1,\"1625\":4,\"1626\":3,\"1748\":1,\"1856\":1,\"2065\":1,\"2350\":1,\"2355\":1}}],[\"1>\",{\"1\":{\"24\":1}}],[\"1\",{\"0\":{\"161\":1,\"887\":1,\"2107\":1},\"1\":{\"6\":2,\"22\":3,\"24\":3,\"25\":1,\"29\":6,\"39\":1,\"41\":5,\"43\":9,\"45\":3,\"50\":2,\"51\":1,\"54\":2,\"58\":1,\"61\":4,\"63\":2,\"66\":1,\"75\":2,\"80\":1,\"82\":1,\"84\":3,\"86\":2,\"98\":1,\"100\":1,\"113\":1,\"119\":2,\"135\":1,\"141\":21,\"142\":9,\"144\":1,\"145\":1,\"162\":3,\"167\":2,\"175\":5,\"195\":1,\"197\":1,\"199\":1,\"204\":1,\"210\":2,\"211\":3,\"212\":2,\"216\":1,\"218\":2,\"222\":3,\"223\":4,\"224\":3,\"225\":1,\"227\":1,\"228\":2,\"234\":1,\"235\":3,\"240\":2,\"243\":19,\"253\":1,\"259\":3,\"262\":1,\"265\":3,\"266\":2,\"267\":20,\"269\":3,\"274\":3,\"275\":2,\"276\":16,\"278\":3,\"284\":1,\"285\":3,\"286\":17,\"287\":8,\"522\":1,\"527\":2,\"535\":3,\"548\":1,\"551\":1,\"606\":1,\"616\":2,\"617\":4,\"618\":4,\"619\":2,\"620\":10,\"621\":2,\"622\":1,\"623\":1,\"624\":4,\"625\":2,\"626\":2,\"630\":2,\"632\":2,\"633\":9,\"634\":2,\"635\":1,\"636\":3,\"637\":8,\"639\":7,\"641\":10,\"643\":13,\"644\":24,\"645\":1,\"649\":5,\"650\":7,\"651\":1,\"652\":4,\"664\":6,\"667\":1,\"674\":20,\"675\":1,\"688\":1,\"692\":11,\"693\":1,\"696\":6,\"697\":5,\"699\":4,\"700\":3,\"701\":4,\"706\":2,\"709\":3,\"710\":3,\"711\":4,\"720\":1,\"731\":2,\"732\":2,\"733\":3,\"734\":3,\"735\":3,\"736\":4,\"737\":2,\"738\":2,\"746\":7,\"747\":2,\"748\":2,\"749\":2,\"756\":1,\"761\":1,\"762\":1,\"764\":1,\"765\":1,\"766\":2,\"767\":2,\"768\":2,\"771\":2,\"773\":1,\"774\":9,\"775\":2,\"777\":1,\"780\":3,\"784\":2,\"785\":1,\"786\":1,\"790\":4,\"791\":1,\"792\":1,\"795\":1,\"796\":5,\"798\":2,\"800\":1,\"801\":1,\"802\":1,\"815\":2,\"817\":1,\"821\":2,\"824\":1,\"831\":1,\"846\":6,\"847\":9,\"848\":2,\"849\":5,\"850\":8,\"851\":3,\"852\":1,\"856\":1,\"862\":1,\"869\":3,\"882\":1,\"883\":1,\"884\":2,\"887\":1,\"911\":1,\"922\":3,\"947\":2,\"948\":1,\"949\":2,\"958\":1,\"962\":1,\"974\":2,\"978\":2,\"979\":6,\"982\":2,\"1002\":1,\"1012\":1,\"1014\":1,\"1020\":2,\"1025\":7,\"1028\":1,\"1043\":1,\"1061\":1,\"1062\":2,\"1065\":1,\"1078\":1,\"1080\":4,\"1082\":4,\"1107\":5,\"1108\":2,\"1110\":2,\"1118\":3,\"1119\":3,\"1124\":10,\"1125\":9,\"1126\":3,\"1127\":2,\"1131\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1147\":5,\"1155\":2,\"1158\":2,\"1172\":1,\"1180\":1,\"1181\":1,\"1185\":1,\"1204\":2,\"1209\":1,\"1211\":4,\"1213\":1,\"1217\":2,\"1225\":2,\"1228\":1,\"1238\":3,\"1240\":1,\"1242\":1,\"1250\":2,\"1251\":1,\"1269\":3,\"1270\":2,\"1271\":2,\"1273\":2,\"1274\":2,\"1278\":4,\"1279\":1,\"1280\":2,\"1281\":1,\"1282\":1,\"1283\":2,\"1298\":1,\"1299\":1,\"1301\":2,\"1306\":2,\"1308\":1,\"1309\":3,\"1311\":2,\"1315\":1,\"1316\":1,\"1319\":3,\"1320\":1,\"1321\":2,\"1322\":2,\"1323\":1,\"1327\":7,\"1328\":3,\"1330\":5,\"1334\":1,\"1350\":2,\"1351\":1,\"1371\":2,\"1372\":2,\"1382\":1,\"1385\":6,\"1389\":15,\"1390\":8,\"1391\":5,\"1392\":5,\"1395\":5,\"1396\":10,\"1397\":3,\"1401\":26,\"1402\":14,\"1403\":9,\"1406\":1,\"1408\":19,\"1409\":11,\"1410\":6,\"1413\":3,\"1417\":1,\"1419\":2,\"1420\":7,\"1422\":2,\"1433\":1,\"1435\":1,\"1439\":1,\"1442\":3,\"1444\":3,\"1446\":2,\"1448\":3,\"1450\":5,\"1452\":11,\"1454\":3,\"1456\":7,\"1458\":4,\"1460\":8,\"1466\":18,\"1467\":10,\"1468\":6,\"1469\":1,\"1482\":2,\"1484\":4,\"1490\":1,\"1509\":63,\"1511\":69,\"1513\":9,\"1514\":12,\"1515\":2,\"1519\":5,\"1520\":2,\"1521\":7,\"1524\":4,\"1525\":4,\"1526\":37,\"1534\":13,\"1535\":1,\"1536\":4,\"1539\":2,\"1545\":4,\"1546\":2,\"1548\":9,\"1549\":15,\"1551\":7,\"1552\":23,\"1553\":92,\"1556\":3,\"1558\":1,\"1559\":2,\"1581\":1,\"1582\":1,\"1583\":2,\"1585\":6,\"1590\":1,\"1592\":8,\"1593\":5,\"1594\":5,\"1595\":9,\"1596\":4,\"1597\":5,\"1598\":39,\"1599\":33,\"1600\":35,\"1601\":1,\"1603\":1,\"1604\":3,\"1605\":2,\"1606\":4,\"1607\":3,\"1608\":2,\"1609\":5,\"1610\":4,\"1611\":5,\"1612\":3,\"1613\":4,\"1614\":2,\"1615\":1,\"1616\":6,\"1618\":4,\"1619\":3,\"1622\":2,\"1624\":1,\"1625\":28,\"1626\":20,\"1627\":4,\"1628\":7,\"1631\":1,\"1638\":1,\"1640\":2,\"1641\":1,\"1655\":8,\"1661\":1,\"1668\":2,\"1677\":1,\"1686\":2,\"1691\":1,\"1694\":2,\"1698\":1,\"1716\":1,\"1717\":2,\"1719\":1,\"1721\":1,\"1722\":1,\"1725\":1,\"1726\":2,\"1727\":1,\"1735\":6,\"1736\":11,\"1738\":2,\"1739\":3,\"1740\":3,\"1741\":3,\"1742\":2,\"1743\":3,\"1744\":3,\"1745\":3,\"1746\":2,\"1748\":2,\"1749\":5,\"1750\":3,\"1751\":2,\"1752\":1,\"1759\":3,\"1766\":1,\"1768\":1,\"1779\":2,\"1783\":1,\"1785\":1,\"1786\":1,\"1794\":2,\"1807\":1,\"1815\":8,\"1817\":3,\"1833\":1,\"1847\":1,\"1851\":4,\"1852\":1,\"1854\":1,\"1855\":1,\"1856\":1,\"1862\":1,\"1863\":1,\"1870\":1,\"1873\":3,\"1877\":3,\"1878\":9,\"1901\":4,\"1902\":169,\"1903\":4,\"1904\":124,\"1906\":8,\"1908\":1,\"1909\":15,\"1926\":6,\"1928\":1,\"1933\":9,\"1947\":3,\"1957\":1,\"1959\":2,\"1960\":1,\"1961\":2,\"1962\":1,\"1965\":1,\"1975\":1,\"1976\":1,\"1984\":1,\"1987\":2,\"1988\":1,\"1990\":2,\"1991\":1,\"1992\":10,\"1993\":8,\"1994\":2,\"1995\":6,\"1996\":1,\"1997\":1,\"1999\":2,\"2000\":15,\"2001\":12,\"2002\":1,\"2003\":1,\"2004\":1,\"2007\":1,\"2008\":1,\"2014\":8,\"2015\":2,\"2016\":1,\"2017\":1,\"2018\":4,\"2019\":3,\"2020\":3,\"2021\":5,\"2055\":2,\"2056\":2,\"2101\":2,\"2126\":3,\"2127\":1,\"2129\":4,\"2130\":2,\"2132\":3,\"2133\":2,\"2134\":3,\"2136\":1,\"2137\":4,\"2139\":1,\"2141\":2,\"2143\":1,\"2151\":1,\"2168\":2,\"2177\":1,\"2191\":2,\"2196\":1,\"2202\":1,\"2209\":7,\"2213\":4,\"2214\":4,\"2219\":2,\"2221\":3,\"2223\":2,\"2224\":2,\"2226\":2,\"2227\":1,\"2228\":7,\"2229\":6,\"2235\":9,\"2236\":10,\"2239\":25,\"2240\":21,\"2241\":4,\"2245\":12,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":6,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":4,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2305\":1,\"2307\":2,\"2309\":1,\"2310\":1,\"2327\":4,\"2336\":5,\"2337\":4,\"2346\":6,\"2350\":3,\"2353\":2,\"2355\":6,\"2356\":4,\"2359\":1,\"2360\":3,\"2361\":3,\"2362\":4,\"2363\":7,\"2364\":2,\"2368\":6,\"2369\":1,\"2408\":6,\"2411\":23,\"2412\":35,\"2413\":4,\"2423\":34,\"2424\":4,\"2425\":1,\"2427\":2,\"2428\":4,\"2429\":1,\"2431\":12,\"2432\":19,\"2433\":1,\"2435\":1,\"2436\":1,\"2438\":1,\"2439\":1,\"2440\":1,\"2443\":1,\"2446\":6,\"2447\":35,\"2448\":5,\"2458\":1,\"2460\":2,\"2462\":2,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":2,\"2473\":1,\"2490\":1,\"2495\":1}}],[\"xeus\",{\"1\":{\"2136\":1}}],[\"xp\",{\"1\":{\"1729\":1}}],[\"x64\",{\"1\":{\"1701\":1}}],[\"xavier\",{\"1\":{\"1526\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1896\":1,\"1994\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2432\":1,\"2447\":1}}],[\"x1\",{\"1\":{\"1370\":1}}],[\"x^t\",{\"1\":{\"1309\":1}}],[\"x|args\",{\"1\":{\"1224\":2,\"1225\":2,\"1245\":2}}],[\"x0\",{\"1\":{\"1224\":1,\"1225\":1,\"1370\":1}}],[\"xla\",{\"0\":{\"915\":1},\"1\":{\"915\":1}}],[\"xl\",{\"1\":{\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"744\":1,\"828\":1,\"829\":1,\"830\":1,\"859\":1}}],[\"xlens\",{\"1\":{\"1730\":2}}],[\"xlen\",{\"1\":{\"692\":1,\"760\":1,\"790\":1,\"820\":1,\"850\":1,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"1723\":1,\"1724\":1,\"1787\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1992\":1}}],[\"xs=none\",{\"1\":{\"1901\":1,\"1903\":1}}],[\"xs\",{\"1\":{\"678\":1,\"692\":3,\"699\":2,\"700\":2,\"709\":2,\"710\":6,\"711\":6,\"733\":2,\"734\":2,\"745\":2,\"746\":4,\"747\":2,\"748\":2,\"749\":2,\"760\":2,\"771\":2,\"774\":2,\"780\":2,\"790\":2,\"791\":1,\"798\":1,\"820\":2,\"846\":2,\"849\":2,\"850\":2,\"862\":1,\"941\":1,\"1089\":1,\"1093\":1,\"1196\":1,\"1199\":2,\"1233\":1,\"1326\":2,\"1629\":2,\"1668\":3,\"1723\":2,\"1724\":2,\"1725\":5,\"1753\":4,\"1758\":2,\"1787\":2,\"1788\":2,\"1810\":2,\"1814\":2,\"1816\":2,\"1846\":3,\"1901\":1,\"1902\":7,\"1903\":1,\"1904\":7,\"1905\":2,\"1908\":2,\"1933\":10,\"1944\":2,\"1945\":2,\"1947\":2,\"1966\":2,\"1992\":2,\"2026\":1,\"2028\":1,\"2129\":2,\"2227\":2,\"2231\":2,\"2233\":1,\"2428\":6,\"2433\":2,\"2439\":2,\"2451\":1,\"2453\":1,\"2455\":2,\"2463\":1,\"2464\":2}}],[\"xvectorprojector\",{\"0\":{\"2211\":1},\"1\":{\"2211\":1}}],[\"xvectorencoder\",{\"0\":{\"2209\":1},\"1\":{\"2209\":1}}],[\"xvectors\",{\"1\":{\"2208\":1}}],[\"xvector\",{\"0\":{\"2209\":1,\"2211\":1},\"1\":{\"285\":1,\"286\":5,\"2209\":1,\"2211\":1}}],[\"xmlscpwriter\",{\"1\":{\"1018\":1}}],[\"xmlscpreader\",{\"1\":{\"992\":1,\"1016\":1}}],[\"xmlwriter\",{\"0\":{\"1017\":1},\"1\":{\"1017\":1}}],[\"xmlreader\",{\"0\":{\"1015\":1},\"1\":{\"269\":2,\"278\":2,\"1015\":1}}],[\"xml\",{\"1\":{\"269\":2,\"278\":2,\"1015\":1,\"1016\":5,\"1018\":3}}],[\"xx\",{\"1\":{\"200\":1,\"205\":1,\"242\":1,\"1309\":2}}],[\"xxxxxxsteps\",{\"1\":{\"267\":2,\"276\":2,\"286\":1}}],[\"xxx\",{\"1\":{\"48\":1,\"243\":1,\"286\":1,\"290\":2}}],[\"x\",{\"1\":{\"48\":5,\"121\":1,\"126\":5,\"135\":1,\"141\":1,\"144\":3,\"243\":1,\"259\":1,\"286\":2,\"289\":2,\"615\":2,\"617\":6,\"618\":6,\"619\":3,\"620\":6,\"621\":3,\"622\":3,\"623\":3,\"624\":6,\"626\":11,\"629\":5,\"630\":5,\"633\":3,\"634\":8,\"635\":4,\"636\":6,\"637\":5,\"638\":3,\"640\":3,\"641\":4,\"642\":6,\"643\":11,\"644\":3,\"645\":4,\"647\":3,\"648\":2,\"649\":6,\"650\":4,\"651\":1,\"652\":4,\"661\":3,\"686\":2,\"689\":1,\"692\":9,\"693\":1,\"701\":2,\"702\":2,\"704\":1,\"705\":1,\"709\":7,\"710\":7,\"711\":7,\"712\":2,\"713\":1,\"715\":2,\"718\":1,\"722\":1,\"724\":2,\"725\":2,\"726\":3,\"727\":1,\"728\":2,\"729\":3,\"730\":2,\"735\":2,\"744\":2,\"749\":2,\"754\":1,\"756\":1,\"757\":1,\"760\":3,\"770\":4,\"774\":7,\"775\":1,\"780\":7,\"781\":1,\"783\":2,\"786\":3,\"787\":2,\"788\":1,\"789\":1,\"790\":2,\"797\":4,\"800\":3,\"805\":1,\"806\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"820\":4,\"825\":1,\"827\":1,\"828\":1,\"829\":2,\"830\":2,\"832\":1,\"833\":2,\"835\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"847\":1,\"849\":7,\"850\":2,\"851\":1,\"852\":1,\"856\":1,\"859\":3,\"860\":1,\"861\":1,\"867\":3,\"868\":1,\"887\":1,\"888\":1,\"892\":1,\"897\":1,\"899\":1,\"900\":1,\"901\":1,\"909\":1,\"913\":1,\"914\":1,\"920\":1,\"921\":3,\"923\":1,\"935\":3,\"942\":1,\"943\":1,\"945\":1,\"959\":4,\"972\":2,\"973\":2,\"981\":2,\"982\":2,\"1029\":3,\"1046\":1,\"1048\":1,\"1050\":4,\"1051\":1,\"1055\":1,\"1057\":1,\"1061\":2,\"1063\":2,\"1064\":5,\"1068\":1,\"1070\":2,\"1071\":2,\"1073\":2,\"1075\":2,\"1076\":1,\"1084\":1,\"1087\":1,\"1091\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1107\":7,\"1110\":1,\"1116\":4,\"1119\":2,\"1120\":1,\"1122\":1,\"1124\":2,\"1132\":1,\"1140\":2,\"1141\":2,\"1148\":2,\"1151\":1,\"1153\":1,\"1161\":4,\"1164\":4,\"1165\":1,\"1167\":1,\"1176\":2,\"1177\":1,\"1180\":2,\"1181\":2,\"1182\":2,\"1183\":2,\"1184\":2,\"1185\":1,\"1187\":1,\"1189\":4,\"1190\":1,\"1192\":1,\"1194\":1,\"1198\":2,\"1204\":2,\"1205\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1218\":4,\"1219\":1,\"1221\":4,\"1224\":2,\"1225\":2,\"1226\":1,\"1228\":1,\"1229\":5,\"1230\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1244\":4,\"1245\":8,\"1253\":1,\"1254\":1,\"1262\":3,\"1264\":1,\"1272\":2,\"1273\":2,\"1274\":2,\"1278\":7,\"1284\":1,\"1288\":1,\"1290\":2,\"1292\":2,\"1299\":2,\"1301\":4,\"1306\":2,\"1334\":1,\"1335\":1,\"1336\":1,\"1344\":1,\"1345\":1,\"1362\":1,\"1370\":2,\"1371\":2,\"1372\":4,\"1374\":3,\"1375\":1,\"1376\":1,\"1377\":1,\"1383\":1,\"1385\":2,\"1387\":1,\"1389\":6,\"1390\":2,\"1391\":4,\"1392\":1,\"1397\":2,\"1398\":1,\"1400\":5,\"1401\":6,\"1402\":2,\"1403\":6,\"1404\":1,\"1406\":1,\"1408\":6,\"1409\":2,\"1410\":4,\"1411\":1,\"1413\":1,\"1414\":1,\"1415\":1,\"1417\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":2,\"1441\":3,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1466\":6,\"1467\":2,\"1468\":4,\"1469\":2,\"1473\":1,\"1474\":1,\"1479\":1,\"1484\":1,\"1491\":1,\"1493\":1,\"1494\":1,\"1503\":1,\"1505\":2,\"1506\":2,\"1514\":2,\"1516\":2,\"1517\":1,\"1519\":4,\"1520\":4,\"1525\":2,\"1527\":1,\"1529\":4,\"1530\":1,\"1534\":2,\"1535\":4,\"1536\":4,\"1537\":2,\"1543\":1,\"1547\":2,\"1549\":1,\"1554\":2,\"1556\":2,\"1558\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1577\":2,\"1581\":4,\"1583\":4,\"1586\":4,\"1588\":2,\"1593\":2,\"1594\":2,\"1595\":2,\"1596\":1,\"1597\":2,\"1603\":4,\"1604\":2,\"1606\":2,\"1608\":4,\"1609\":2,\"1611\":4,\"1612\":4,\"1613\":4,\"1614\":2,\"1616\":4,\"1617\":6,\"1618\":2,\"1620\":2,\"1621\":2,\"1622\":4,\"1623\":1,\"1628\":4,\"1632\":4,\"1633\":2,\"1654\":1,\"1656\":3,\"1657\":1,\"1661\":3,\"1666\":1,\"1668\":3,\"1670\":4,\"1671\":2,\"1686\":1,\"1694\":2,\"1699\":2,\"1700\":2,\"1704\":4,\"1705\":3,\"1706\":3,\"1707\":3,\"1708\":4,\"1710\":5,\"1711\":7,\"1712\":3,\"1713\":4,\"1714\":5,\"1715\":6,\"1716\":6,\"1719\":19,\"1720\":8,\"1721\":4,\"1724\":2,\"1725\":26,\"1726\":1,\"1727\":1,\"1729\":1,\"1730\":4,\"1731\":10,\"1733\":2,\"1735\":12,\"1737\":2,\"1738\":5,\"1739\":5,\"1740\":5,\"1741\":5,\"1742\":5,\"1743\":5,\"1744\":5,\"1745\":5,\"1746\":5,\"1747\":2,\"1748\":2,\"1749\":10,\"1751\":9,\"1753\":4,\"1756\":1,\"1757\":1,\"1758\":2,\"1759\":9,\"1766\":1,\"1768\":7,\"1782\":3,\"1783\":2,\"1784\":3,\"1785\":2,\"1786\":2,\"1787\":2,\"1789\":1,\"1790\":1,\"1794\":1,\"1795\":2,\"1798\":2,\"1799\":2,\"1800\":3,\"1803\":2,\"1805\":2,\"1806\":9,\"1808\":3,\"1809\":1,\"1812\":2,\"1815\":1,\"1817\":2,\"1818\":3,\"1820\":2,\"1822\":4,\"1837\":2,\"1838\":1,\"1848\":4,\"1849\":2,\"1854\":4,\"1857\":1,\"1862\":2,\"1868\":2,\"1870\":2,\"1873\":1,\"1877\":2,\"1897\":4,\"1899\":1,\"1900\":1,\"1906\":3,\"1909\":3,\"1919\":4,\"1923\":1,\"1924\":1,\"1925\":1,\"1927\":4,\"1931\":2,\"1932\":2,\"1935\":1,\"1944\":2,\"1946\":2,\"1947\":2,\"1950\":2,\"1966\":1,\"1985\":1,\"1992\":8,\"1995\":7,\"2000\":3,\"2001\":3,\"2129\":7,\"2168\":1,\"2170\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":3,\"2185\":1,\"2187\":2,\"2188\":1,\"2190\":3,\"2191\":2,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2208\":4,\"2209\":4,\"2211\":1,\"2213\":1,\"2214\":1,\"2218\":2,\"2220\":2,\"2223\":2,\"2227\":1,\"2231\":2,\"2305\":3,\"2355\":8,\"2378\":5,\"2379\":1,\"2420\":2,\"2426\":2,\"2433\":2,\"2434\":1,\"2435\":3,\"2438\":3,\"2440\":3,\"2458\":1,\"2465\":1,\"2467\":1,\"2471\":2,\"2472\":1}}],[\"xin\",{\"1\":{\"1406\":2}}],[\"xinjian\",{\"1\":{\"6\":1,\"244\":1}}],[\"xiaobin\",{\"1\":{\"1316\":1}}],[\"xiaoice\",{\"0\":{\"2240\":1,\"2241\":1},\"1\":{\"267\":1,\"276\":1,\"1526\":1,\"2240\":1,\"2241\":1}}],[\"xiaoicesing2loss\",{\"0\":{\"2241\":1},\"1\":{\"2241\":1}}],[\"xiaoicesing2\",{\"1\":{\"1526\":1,\"2239\":2,\"2240\":2}}],[\"xiaoicesing\",{\"0\":{\"2240\":2},\"1\":{\"265\":1,\"267\":5,\"272\":1,\"274\":1,\"276\":3,\"282\":1,\"2240\":5}}],[\"xiaohui\",{\"1\":{\"10\":1}}],[\"xu\",{\"1\":{\"9\":1,\"13\":1,\"202\":1}}],[\"xuankai\",{\"1\":{\"6\":2,\"7\":1,\"11\":2,\"12\":1,\"13\":1,\"207\":1,\"244\":1}}],[\"jhu\",{\"1\":{\"1717\":1}}],[\"jvp\",{\"1\":{\"756\":1,\"773\":1}}],[\"jvs\",{\"1\":{\"290\":1}}],[\"jr\",{\"1\":{\"288\":1,\"2290\":1}}],[\"jdongian\",{\"1\":{\"287\":2}}],[\"jetsgenerator\",{\"0\":{\"1599\":1},\"1\":{\"1599\":1}}],[\"jets\",{\"0\":{\"1577\":1,\"1589\":1,\"1590\":1,\"1598\":3,\"1599\":1,\"1627\":1,\"1629\":1,\"1637\":1},\"1\":{\"286\":1,\"289\":1,\"1577\":1,\"1589\":1,\"1590\":1,\"1598\":9,\"1599\":3,\"1627\":2,\"1629\":1,\"1637\":1}}],[\"jeeweon\",{\"1\":{\"691\":1}}],[\"jee\",{\"1\":{\"8\":1,\"202\":1,\"256\":1}}],[\"jp\",{\"1\":{\"267\":3,\"276\":2,\"536\":4}}],[\"jako202029757857763\",{\"1\":{\"2168\":1}}],[\"jax\",{\"1\":{\"1373\":1}}],[\"jaconv\",{\"1\":{\"288\":2,\"290\":1,\"481\":1}}],[\"jamo\",{\"1\":{\"287\":2,\"2281\":3}}],[\"jaso\",{\"0\":{\"2281\":1},\"1\":{\"287\":2,\"481\":2,\"2281\":1}}],[\"jaywalnut310\",{\"1\":{\"287\":1}}],[\"japanese\",{\"1\":{\"163\":1,\"269\":1,\"278\":1,\"287\":2}}],[\"jahn\",{\"1\":{\"156\":1}}],[\"j\",{\"1\":{\"161\":3,\"168\":2,\"287\":3,\"1061\":2,\"1062\":2,\"1210\":1,\"1264\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1330\":1,\"1334\":1,\"2192\":1,\"2203\":1}}],[\"joon\",{\"1\":{\"256\":1}}],[\"joinsegmenter\",{\"0\":{\"2463\":1},\"1\":{\"2463\":1}}],[\"join\",{\"0\":{\"2463\":1},\"1\":{\"243\":1,\"2463\":2,\"2464\":1}}],[\"jointtext2wav\",{\"0\":{\"1600\":1},\"1\":{\"1600\":2}}],[\"jointscore2wav\",{\"0\":{\"1526\":1},\"1\":{\"1526\":2}}],[\"jointnetwork\",{\"0\":{\"632\":1,\"1779\":1},\"1\":{\"616\":1,\"625\":1,\"627\":1,\"632\":2,\"696\":1,\"697\":1,\"1779\":1}}],[\"joint\",{\"0\":{\"143\":1,\"632\":1,\"1526\":2,\"1600\":2,\"1779\":1},\"1\":{\"140\":1,\"143\":7,\"226\":2,\"228\":4,\"262\":2,\"284\":1,\"286\":10,\"290\":2,\"616\":3,\"625\":3,\"627\":3,\"632\":11,\"696\":3,\"697\":3,\"736\":1,\"740\":1,\"777\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1526\":2,\"1600\":2,\"1720\":1,\"1726\":2,\"1727\":2,\"1749\":3,\"1779\":15,\"2127\":1,\"2184\":1,\"2221\":1,\"2235\":1,\"2236\":5,\"2239\":5,\"2240\":5,\"2245\":4,\"2411\":4,\"2412\":4,\"2423\":4,\"2431\":4,\"2432\":4,\"2447\":4}}],[\"jointly\",{\"1\":{\"44\":1,\"223\":1,\"247\":1,\"286\":2,\"1526\":1,\"1598\":1,\"1600\":1}}],[\"jonathan\",{\"1\":{\"202\":1}}],[\"jong\",{\"1\":{\"202\":1}}],[\"job=1\",{\"1\":{\"167\":2}}],[\"jobs\",{\"0\":{\"120\":1},\"1\":{\"54\":3,\"69\":1,\"120\":2,\"165\":1,\"167\":2,\"168\":3,\"173\":2,\"516\":2,\"520\":2,\"521\":2,\"523\":2,\"524\":2,\"525\":2,\"535\":2,\"537\":2,\"2157\":3}}],[\"job\",{\"0\":{\"165\":1,\"166\":1,\"2131\":1,\"2142\":1,\"2143\":1,\"2388\":1},\"1\":{\"37\":1,\"62\":1,\"69\":1,\"109\":1,\"110\":2,\"120\":1,\"121\":1,\"165\":3,\"167\":6,\"168\":1,\"206\":2,\"212\":2,\"218\":2,\"236\":1,\"255\":2,\"267\":2,\"276\":2,\"286\":2,\"2131\":4,\"2142\":4,\"2143\":1,\"2388\":1}}],[\"journal=\",{\"1\":{\"9\":1,\"207\":1,\"244\":1}}],[\"john\",{\"1\":{\"8\":1,\"1131\":1,\"1172\":1,\"1319\":1}}],[\"june\",{\"1\":{\"1309\":1,\"1311\":1}}],[\"junior\",{\"1\":{\"288\":1,\"2290\":1}}],[\"jung2022pushing\",{\"1\":{\"256\":1}}],[\"jung2024espnet\",{\"1\":{\"8\":1}}],[\"jung\",{\"1\":{\"8\":1,\"202\":1,\"256\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"2192\":1,\"2203\":1}}],[\"judgements\",{\"1\":{\"286\":1}}],[\"judgments\",{\"1\":{\"246\":1}}],[\"jul\",{\"1\":{\"202\":1}}],[\"just\",{\"1\":{\"78\":1,\"80\":1,\"81\":1,\"91\":1,\"93\":1,\"96\":1,\"102\":1,\"106\":1,\"197\":1,\"211\":1,\"217\":2,\"223\":1,\"242\":1,\"271\":1,\"280\":1,\"287\":1,\"756\":1,\"773\":1,\"866\":1,\"867\":1,\"943\":1,\"1156\":1,\"1493\":1,\"1494\":1,\"1502\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1,\"1833\":1,\"2188\":1,\"2304\":1,\"2325\":1}}],[\"juan\",{\"1\":{\"10\":1}}],[\"ju\",{\"1\":{\"9\":1,\"11\":1}}],[\"jupyter\",{\"1\":{\"3\":1,\"208\":1}}],[\"jik876\",{\"1\":{\"1513\":1,\"1548\":1,\"1551\":1,\"1592\":1}}],[\"jiro\",{\"1\":{\"156\":1}}],[\"jin\",{\"1\":{\"13\":1,\"256\":2}}],[\"jing\",{\"1\":{\"11\":1}}],[\"jinchuan\",{\"1\":{\"6\":3,\"244\":1}}],[\"jiatong\",{\"1\":{\"5\":1,\"6\":2,\"8\":1,\"9\":1,\"10\":1,\"13\":1,\"14\":1,\"244\":1,\"691\":1,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1466\":1,\"1468\":1,\"1644\":1,\"1993\":2}}],[\"jionghao\",{\"1\":{\"5\":1}}],[\"jsut\",{\"1\":{\"286\":1,\"536\":8}}],[\"jsonl\",{\"1\":{\"2134\":1,\"2139\":2,\"2144\":5,\"2161\":1}}],[\"json2trn\",{\"0\":{\"583\":1,\"586\":1,\"589\":1},\"1\":{\"583\":1,\"586\":1,\"589\":1}}],[\"json2text\",{\"0\":{\"581\":1},\"1\":{\"581\":1}}],[\"json2sctm\",{\"0\":{\"578\":1},\"1\":{\"578\":1}}],[\"jsons\",{\"1\":{\"538\":2,\"554\":2,\"556\":2}}],[\"json\",{\"0\":{\"554\":1,\"930\":1,\"2158\":2,\"2165\":1},\"1\":{\"38\":1,\"106\":4,\"268\":2,\"269\":2,\"277\":2,\"278\":2,\"521\":1,\"538\":1,\"554\":2,\"556\":1,\"578\":2,\"581\":2,\"583\":2,\"586\":2,\"589\":2,\"594\":1,\"598\":1,\"600\":2,\"760\":2,\"930\":3,\"1004\":4,\"1006\":4,\"1798\":1,\"1799\":1,\"1800\":1,\"1807\":1,\"2132\":1,\"2134\":3,\"2141\":4}}],[\"js\",{\"1\":{\"3\":1}}],[\"y1\",{\"1\":{\"1370\":1}}],[\"y0\",{\"1\":{\"1370\":1}}],[\"ylen\",{\"1\":{\"692\":1,\"760\":1,\"790\":1,\"820\":1,\"850\":1,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"1723\":1,\"1724\":1,\"1787\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1992\":1}}],[\"ys\",{\"1\":{\"676\":2,\"692\":11,\"706\":8,\"748\":2,\"760\":13,\"770\":4,\"775\":4,\"777\":8,\"790\":9,\"796\":2,\"797\":2,\"820\":11,\"846\":2,\"850\":11,\"959\":2,\"1156\":8,\"1515\":4,\"1723\":5,\"1724\":4,\"1731\":4,\"1750\":4,\"1760\":6,\"1764\":2,\"1787\":5,\"1798\":1,\"1799\":1,\"1800\":1,\"1805\":2,\"1822\":2,\"1839\":2,\"1858\":2,\"1907\":2,\"1944\":6,\"1945\":4,\"1946\":2,\"1947\":6,\"1966\":2,\"1991\":2,\"1992\":5,\"1993\":1,\"2223\":2,\"2226\":2,\"2237\":2,\"2241\":2,\"2413\":2,\"2424\":2,\"2428\":2,\"2448\":2}}],[\"yseq\",{\"1\":{\"628\":1,\"631\":3,\"692\":1,\"743\":1,\"763\":2,\"797\":1,\"1722\":3,\"1762\":1,\"1775\":2,\"1806\":1,\"1807\":6}}],[\"yyy\",{\"1\":{\"286\":1}}],[\"y\",{\"1\":{\"168\":2,\"689\":1,\"692\":4,\"718\":1,\"757\":1,\"760\":1,\"790\":2,\"797\":1,\"809\":1,\"820\":1,\"850\":2,\"868\":1,\"888\":1,\"920\":1,\"971\":3,\"975\":3,\"1050\":1,\"1061\":2,\"1062\":2,\"1072\":3,\"1074\":3,\"1076\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1116\":1,\"1161\":1,\"1179\":3,\"1189\":1,\"1210\":1,\"1212\":1,\"1218\":1,\"1221\":1,\"1224\":5,\"1225\":5,\"1229\":1,\"1244\":1,\"1245\":1,\"1254\":3,\"1264\":1,\"1269\":4,\"1270\":4,\"1271\":4,\"1279\":1,\"1280\":2,\"1281\":2,\"1282\":2,\"1283\":1,\"1299\":2,\"1308\":3,\"1314\":2,\"1315\":2,\"1334\":1,\"1351\":2,\"1352\":2,\"1370\":2,\"1376\":2,\"1377\":2,\"1419\":4,\"1421\":1,\"1479\":1,\"1509\":2,\"1511\":2,\"1541\":2,\"1607\":4,\"1617\":4,\"1640\":4,\"1641\":4,\"1686\":2,\"1694\":1,\"1731\":6,\"1760\":2,\"1787\":2,\"1798\":2,\"1799\":2,\"1800\":2,\"1805\":3,\"1822\":2,\"1873\":1,\"1944\":2,\"1946\":2,\"1947\":2,\"1987\":2,\"1992\":2,\"2044\":2,\"2191\":1,\"2355\":10,\"2435\":1,\"2469\":4}}],[\"yin\",{\"0\":{\"2436\":1,\"2437\":1,\"2438\":1,\"2439\":1,\"2440\":1},\"1\":{\"1553\":3,\"1556\":38,\"2434\":1,\"2435\":1,\"2436\":1,\"2437\":1,\"2438\":1,\"2439\":1,\"2440\":1}}],[\"yingram\",{\"1\":{\"2435\":9}}],[\"yingdecoder\",{\"0\":{\"1556\":1},\"1\":{\"1556\":2}}],[\"ying\",{\"0\":{\"1556\":1,\"2434\":2},\"1\":{\"1521\":5,\"1552\":5,\"1553\":3,\"1556\":2,\"2228\":5,\"2229\":5,\"2245\":1,\"2434\":4}}],[\"yi\",{\"1\":{\"14\":1,\"269\":1,\"278\":1}}],[\"yifan\",{\"1\":{\"5\":1,\"6\":3,\"7\":1,\"10\":1,\"12\":1,\"202\":1,\"244\":1,\"1943\":1}}],[\"year\",{\"1\":{\"156\":1,\"190\":1,\"202\":1}}],[\"year=\",{\"1\":{\"5\":1,\"6\":3,\"7\":1,\"8\":1,\"9\":2,\"10\":2,\"11\":2,\"12\":1,\"13\":1,\"14\":1,\"15\":1,\"16\":1,\"202\":1,\"207\":2,\"244\":1,\"256\":1}}],[\"yes\",{\"1\":{\"95\":3}}],[\"yet\",{\"1\":{\"46\":1,\"70\":1,\"144\":1,\"831\":1,\"994\":2,\"2280\":1}}],[\"yen\",{\"1\":{\"11\":1}}],[\"yu23b\",{\"1\":{\"1061\":1,\"1062\":1}}],[\"yum\",{\"1\":{\"159\":2}}],[\"yuya\",{\"1\":{\"156\":1,\"207\":1}}],[\"yuekai\",{\"1\":{\"12\":1}}],[\"yusuf\",{\"1\":{\"1130\":1}}],[\"yusuke\",{\"1\":{\"9\":1}}],[\"yushi\",{\"1\":{\"12\":1}}],[\"yuning\",{\"1\":{\"13\":1}}],[\"yun\",{\"1\":{\"10\":1}}],[\"yu\",{\"1\":{\"9\":1,\"11\":1,\"46\":1,\"139\":1,\"1061\":2,\"1062\":2}}],[\"yui\",{\"1\":{\"6\":1}}],[\"yosuke\",{\"1\":{\"16\":1,\"202\":1}}],[\"yoshiki\",{\"1\":{\"11\":1}}],[\"yoshimura\",{\"1\":{\"9\":2}}],[\"yooncheol\",{\"1\":{\"9\":1}}],[\"youngki\",{\"1\":{\"256\":1}}],[\"yourespnetmodel\",{\"1\":{\"2325\":2,\"2327\":2}}],[\"yourtask\",{\"1\":{\"2249\":1,\"2325\":1,\"2327\":1}}],[\"yourself\",{\"1\":{\"125\":1,\"160\":1,\"196\":1,\"197\":1,\"243\":1,\"268\":1,\"277\":1,\"536\":1,\"2355\":3}}],[\"your\",{\"0\":{\"127\":1,\"195\":1},\"1\":{\"1\":1,\"22\":2,\"25\":1,\"31\":1,\"32\":1,\"34\":2,\"41\":1,\"67\":2,\"68\":1,\"70\":1,\"71\":1,\"78\":1,\"92\":1,\"102\":1,\"109\":1,\"110\":4,\"118\":1,\"120\":1,\"121\":1,\"127\":2,\"128\":1,\"139\":2,\"145\":1,\"150\":3,\"152\":1,\"154\":1,\"160\":1,\"162\":2,\"164\":1,\"165\":1,\"168\":2,\"173\":1,\"194\":1,\"195\":1,\"196\":1,\"197\":3,\"211\":1,\"213\":1,\"224\":5,\"225\":8,\"242\":2,\"243\":1,\"247\":4,\"249\":1,\"254\":1,\"259\":3,\"261\":1,\"263\":1,\"267\":15,\"268\":1,\"276\":11,\"277\":1,\"286\":6,\"290\":2,\"768\":3,\"1031\":1,\"1035\":1,\"1064\":1,\"1078\":1,\"1112\":1,\"1113\":1,\"1153\":1,\"1202\":1,\"1250\":1,\"1251\":1,\"1262\":1,\"1290\":1,\"1502\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"2001\":1,\"2232\":1,\"2238\":1,\"2246\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2325\":1,\"2355\":6}}],[\"you\",{\"1\":{\"1\":1,\"18\":3,\"19\":4,\"22\":4,\"23\":2,\"24\":3,\"25\":1,\"31\":3,\"32\":4,\"33\":1,\"36\":1,\"38\":1,\"39\":4,\"40\":1,\"41\":4,\"42\":1,\"47\":3,\"48\":3,\"55\":2,\"56\":2,\"60\":1,\"67\":2,\"68\":1,\"69\":4,\"70\":3,\"71\":4,\"73\":1,\"76\":3,\"78\":1,\"79\":2,\"80\":1,\"81\":1,\"82\":1,\"84\":2,\"85\":1,\"94\":1,\"96\":2,\"97\":1,\"98\":2,\"99\":1,\"100\":1,\"106\":2,\"107\":1,\"108\":1,\"110\":7,\"117\":2,\"118\":1,\"119\":2,\"121\":1,\"123\":2,\"124\":4,\"125\":2,\"127\":2,\"128\":1,\"132\":2,\"138\":1,\"141\":1,\"150\":3,\"152\":3,\"153\":3,\"159\":2,\"160\":1,\"161\":8,\"162\":13,\"163\":3,\"164\":1,\"165\":2,\"168\":6,\"173\":7,\"174\":2,\"175\":4,\"195\":3,\"196\":3,\"197\":12,\"200\":11,\"201\":3,\"205\":3,\"206\":2,\"210\":1,\"211\":5,\"212\":3,\"213\":2,\"217\":7,\"218\":4,\"219\":2,\"224\":2,\"225\":3,\"242\":8,\"243\":2,\"247\":1,\"254\":3,\"255\":2,\"256\":1,\"263\":5,\"265\":1,\"266\":10,\"267\":28,\"268\":3,\"269\":7,\"270\":2,\"271\":2,\"272\":2,\"274\":1,\"275\":10,\"276\":25,\"277\":3,\"278\":7,\"279\":2,\"280\":2,\"282\":2,\"285\":16,\"286\":46,\"287\":2,\"288\":2,\"289\":4,\"290\":15,\"536\":7,\"756\":2,\"773\":2,\"777\":1,\"787\":1,\"819\":2,\"866\":1,\"867\":1,\"960\":1,\"1031\":1,\"1035\":1,\"1064\":1,\"1078\":1,\"1112\":1,\"1113\":1,\"1145\":1,\"1153\":1,\"1156\":1,\"1202\":1,\"1209\":1,\"1228\":1,\"1250\":1,\"1251\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1269\":3,\"1270\":3,\"1271\":3,\"1290\":1,\"1334\":3,\"1484\":1,\"1655\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1940\":1,\"1942\":1,\"2016\":2,\"2232\":1,\"2238\":1,\"2246\":1,\"2248\":1,\"2249\":2,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2286\":1,\"2325\":1,\"2355\":26,\"2369\":1}}],[\"yaml\",{\"0\":{\"545\":1,\"2480\":1,\"2507\":2},\"1\":{\"47\":17,\"49\":1,\"84\":1,\"85\":4,\"86\":1,\"119\":1,\"201\":3,\"218\":2,\"219\":1,\"224\":2,\"225\":1,\"236\":1,\"243\":14,\"267\":23,\"272\":1,\"276\":11,\"282\":1,\"285\":1,\"286\":31,\"289\":1,\"290\":2,\"526\":2,\"527\":1,\"545\":2,\"768\":1,\"1209\":1,\"1228\":1,\"1954\":1,\"2249\":1,\"2474\":1,\"2480\":5,\"2507\":3}}],[\"yamamoto\",{\"1\":{\"9\":2}}],[\"yalta\",{\"1\":{\"10\":1,\"156\":1}}],[\"yasuda\",{\"1\":{\"9\":1}}],[\"yang\",{\"1\":{\"750\":1,\"1211\":1}}],[\"yanmin\",{\"1\":{\"11\":1}}],[\"yan2023espnet\",{\"1\":{\"10\":1}}],[\"yan\",{\"1\":{\"6\":2,\"10\":1,\"11\":1,\"12\":1,\"207\":1,\"244\":1}}],[\"yml\",{\"1\":{\"3\":2,\"267\":1,\"276\":1,\"286\":1}}],[\"667\",{\"1\":{\"1526\":1,\"1552\":1,\"1553\":1,\"1625\":1,\"1626\":1}}],[\"62\",{\"1\":{\"1526\":1,\"1600\":1,\"1608\":1,\"1618\":3,\"1631\":1}}],[\"6730918\",{\"1\":{\"1327\":1,\"1330\":1}}],[\"650\",{\"1\":{\"1945\":1}}],[\"65\",{\"1\":{\"674\":4}}],[\"6+wavlm\",{\"1\":{\"276\":1}}],[\"6th\",{\"1\":{\"276\":3}}],[\"6006\",{\"1\":{\"39\":1}}],[\"6084\",{\"1\":{\"39\":1}}],[\"613215\",{\"1\":{\"39\":1}}],[\"6901\",{\"1\":{\"39\":1}}],[\"6931\",{\"1\":{\"39\":1}}],[\"640\",{\"1\":{\"148\":1}}],[\"6462\",{\"1\":{\"39\":1}}],[\"64\",{\"1\":{\"39\":1,\"168\":2,\"747\":1,\"1029\":1,\"1118\":1,\"1124\":1,\"1125\":1,\"1280\":3,\"1281\":1,\"1283\":2,\"1396\":1,\"1397\":1,\"1408\":1,\"1409\":1,\"1509\":12,\"1511\":12,\"1524\":1,\"1551\":1,\"1552\":1,\"1553\":12,\"1598\":3,\"1599\":3,\"1600\":2,\"1609\":1,\"1610\":2,\"1619\":1,\"1620\":1,\"1621\":1,\"1628\":2,\"2049\":1,\"2245\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2425\":2,\"2429\":2,\"2431\":2,\"2432\":2,\"2472\":1}}],[\"63\",{\"1\":{\"39\":1,\"113\":1}}],[\"6\",{\"1\":{\"3\":1,\"31\":1,\"39\":3,\"80\":1,\"87\":1,\"95\":1,\"118\":1,\"141\":1,\"142\":1,\"199\":1,\"204\":1,\"210\":1,\"211\":2,\"216\":1,\"222\":1,\"223\":1,\"227\":1,\"228\":2,\"234\":1,\"240\":1,\"242\":1,\"253\":1,\"261\":4,\"265\":1,\"266\":2,\"267\":6,\"274\":1,\"275\":2,\"276\":20,\"284\":1,\"285\":1,\"286\":5,\"674\":2,\"699\":1,\"709\":1,\"710\":1,\"711\":1,\"731\":1,\"732\":1,\"766\":1,\"767\":1,\"774\":1,\"775\":1,\"780\":1,\"848\":1,\"849\":2,\"850\":1,\"1020\":1,\"1062\":1,\"1107\":1,\"1141\":1,\"1278\":1,\"1283\":1,\"1309\":1,\"1311\":1,\"1319\":1,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1419\":1,\"1466\":1,\"1509\":1,\"1511\":1,\"1519\":1,\"1526\":2,\"1536\":2,\"1546\":1,\"1552\":1,\"1553\":2,\"1598\":1,\"1599\":1,\"1600\":1,\"1622\":1,\"1625\":1,\"1626\":1,\"1744\":3,\"1852\":2,\"1902\":3,\"1904\":3,\"1992\":1,\"1995\":1,\"2126\":1,\"2129\":1,\"2191\":1,\"2239\":3,\"2240\":2,\"2245\":1,\"2355\":3,\"2411\":3,\"2412\":3,\"2423\":2,\"2425\":1,\"2427\":2,\"2429\":1,\"2431\":1,\"2432\":3,\"2438\":1,\"2440\":1,\"2447\":2}}],[\"4e12\",{\"1\":{\"2151\":1,\"2310\":1}}],[\"42\",{\"1\":{\"2134\":1}}],[\"4251\",{\"1\":{\"223\":1}}],[\"4160\",{\"1\":{\"2061\":1}}],[\"41663\",{\"1\":{\"699\":1}}],[\"41\",{\"1\":{\"1401\":1,\"1402\":1,\"1408\":1,\"1409\":1,\"1466\":1,\"1467\":1,\"1509\":4,\"1511\":4,\"1526\":1,\"1549\":1,\"1553\":5,\"1594\":1,\"1595\":1,\"1597\":1,\"1598\":1,\"1600\":1,\"1625\":1}}],[\"4d\",{\"1\":{\"1264\":1}}],[\"47052\",{\"1\":{\"1411\":1}}],[\"470\",{\"1\":{\"268\":1,\"277\":1}}],[\"471\",{\"1\":{\"113\":1}}],[\"48k\",{\"1\":{\"1155\":1,\"1157\":2}}],[\"48khz\",{\"1\":{\"70\":1,\"71\":1,\"1155\":1,\"1157\":1}}],[\"48000\",{\"1\":{\"1062\":1}}],[\"480\",{\"1\":{\"267\":1}}],[\"48gb\",{\"1\":{\"243\":1}}],[\"481\",{\"1\":{\"136\":1}}],[\"404\",{\"1\":{\"2508\":1}}],[\"4097\",{\"1\":{\"2061\":1}}],[\"4096\",{\"1\":{\"243\":3,\"746\":1,\"1618\":1,\"2061\":1}}],[\"4023\",{\"1\":{\"1002\":2}}],[\"40c71c5ee3ee5dffa1ad2c53b1b089e16d967bb5\",{\"1\":{\"290\":1}}],[\"4000\",{\"1\":{\"1002\":1}}],[\"400\",{\"1\":{\"243\":1,\"778\":1,\"831\":1,\"1960\":2,\"1961\":2,\"2404\":1}}],[\"40th\",{\"1\":{\"202\":1}}],[\"40\",{\"1\":{\"130\":1,\"710\":1,\"711\":1,\"1117\":1,\"1130\":1,\"1131\":1,\"2196\":1,\"2423\":1,\"2428\":2}}],[\"40ms\",{\"1\":{\"128\":1,\"242\":1}}],[\"40epoch\",{\"1\":{\"113\":1}}],[\"456\",{\"1\":{\"2493\":1}}],[\"45\",{\"1\":{\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1526\":1,\"1553\":2,\"1598\":1,\"1600\":1,\"1625\":1}}],[\"4581968193699de14b56527296262dd76ab43557\",{\"1\":{\"1945\":1}}],[\"458\",{\"1\":{\"644\":1}}],[\"4583\",{\"1\":{\"39\":1}}],[\"451458394527435\",{\"1\":{\"211\":1}}],[\"453\",{\"1\":{\"113\":2}}],[\"4gpus\",{\"0\":{\"56\":1},\"1\":{\"121\":1}}],[\"44k\",{\"1\":{\"286\":4}}],[\"44100\",{\"1\":{\"267\":3,\"286\":2,\"1413\":2,\"1417\":1,\"1420\":2}}],[\"44\",{\"1\":{\"39\":1,\"70\":1,\"267\":1,\"286\":5}}],[\"4909\",{\"1\":{\"160\":1}}],[\"4994583\",{\"1\":{\"66\":2}}],[\"49\",{\"1\":{\"39\":2}}],[\"4321\",{\"1\":{\"223\":1}}],[\"43\",{\"1\":{\"39\":1,\"73\":1}}],[\"4\",{\"1\":{\"3\":1,\"38\":1,\"43\":3,\"44\":1,\"48\":2,\"56\":2,\"57\":1,\"80\":2,\"85\":2,\"121\":1,\"141\":6,\"142\":3,\"159\":1,\"161\":1,\"167\":2,\"199\":1,\"201\":1,\"204\":1,\"210\":1,\"211\":2,\"216\":1,\"222\":3,\"223\":1,\"224\":2,\"225\":1,\"227\":1,\"228\":1,\"234\":1,\"240\":1,\"243\":8,\"253\":1,\"254\":1,\"261\":1,\"262\":1,\"265\":2,\"266\":2,\"267\":15,\"274\":2,\"275\":2,\"276\":14,\"284\":1,\"285\":1,\"286\":16,\"287\":4,\"621\":1,\"633\":1,\"634\":2,\"637\":1,\"643\":1,\"650\":1,\"674\":4,\"700\":1,\"709\":1,\"710\":1,\"711\":1,\"731\":2,\"732\":2,\"733\":1,\"734\":1,\"766\":2,\"767\":2,\"774\":1,\"775\":1,\"780\":1,\"786\":1,\"796\":2,\"798\":1,\"848\":1,\"849\":1,\"850\":1,\"862\":1,\"866\":1,\"869\":2,\"921\":1,\"922\":1,\"1012\":1,\"1014\":1,\"1107\":1,\"1124\":1,\"1125\":1,\"1133\":2,\"1141\":1,\"1147\":1,\"1235\":4,\"1252\":2,\"1278\":1,\"1280\":7,\"1282\":4,\"1283\":1,\"1328\":2,\"1385\":2,\"1389\":1,\"1391\":1,\"1392\":1,\"1396\":1,\"1401\":6,\"1402\":5,\"1403\":4,\"1408\":5,\"1409\":3,\"1410\":2,\"1450\":1,\"1452\":4,\"1454\":1,\"1456\":4,\"1466\":6,\"1467\":5,\"1468\":1,\"1484\":5,\"1509\":13,\"1511\":15,\"1513\":2,\"1514\":2,\"1526\":7,\"1534\":2,\"1548\":4,\"1549\":5,\"1551\":2,\"1552\":7,\"1553\":21,\"1592\":2,\"1594\":3,\"1595\":3,\"1597\":2,\"1598\":8,\"1599\":5,\"1600\":9,\"1604\":4,\"1606\":5,\"1608\":2,\"1609\":1,\"1610\":4,\"1612\":2,\"1616\":1,\"1618\":4,\"1625\":9,\"1626\":6,\"1692\":4,\"1717\":1,\"1722\":1,\"1741\":3,\"1742\":2,\"1746\":2,\"1770\":1,\"1771\":1,\"1807\":1,\"1902\":1,\"1904\":1,\"1906\":4,\"1909\":1,\"1933\":1,\"1947\":1,\"1984\":1,\"1990\":1,\"1992\":1,\"1993\":2,\"1994\":1,\"1995\":1,\"2018\":1,\"2126\":1,\"2129\":1,\"2134\":1,\"2151\":1,\"2181\":1,\"2191\":1,\"2239\":2,\"2240\":1,\"2245\":2,\"2249\":2,\"2253\":2,\"2310\":1,\"2334\":1,\"2363\":1,\"2411\":2,\"2412\":2,\"2423\":2,\"2429\":1,\"2430\":1,\"2431\":2,\"2432\":3,\"2435\":2,\"2439\":1,\"2447\":1,\"2479\":5,\"2485\":1}}],[\"lbfgs\",{\"1\":{\"2355\":1}}],[\"lhotse\",{\"0\":{\"2150\":1,\"2156\":1,\"2157\":2,\"2159\":1,\"2160\":1},\"1\":{\"2139\":2,\"2150\":1,\"2156\":1,\"2157\":4,\"2159\":1,\"2160\":1}}],[\"lhotseaudioreader\",{\"0\":{\"2139\":1},\"1\":{\"2139\":1}}],[\"lj\",{\"1\":{\"2134\":1}}],[\"ljseepch\",{\"1\":{\"290\":3}}],[\"ljspeech\",{\"1\":{\"285\":1,\"286\":16,\"289\":1,\"290\":5,\"523\":1,\"536\":28,\"2045\":2,\"2134\":1}}],[\"ld\",{\"1\":{\"2000\":8}}],[\"ldc2004t12\",{\"1\":{\"1002\":1}}],[\"ldc94s13b\",{\"1\":{\"24\":2}}],[\"ldc93s6b\",{\"1\":{\"24\":2}}],[\"ldc\",{\"1\":{\"24\":5,\"1002\":1}}],[\"lv2\",{\"1\":{\"1509\":1,\"1511\":1,\"1553\":1}}],[\"lv1\",{\"1\":{\"1509\":1,\"1511\":1,\"1553\":1}}],[\"l^\",{\"1\":{\"1308\":3}}],[\"l^h\",{\"1\":{\"1308\":1}}],[\"l=16\",{\"1\":{\"2202\":1,\"2213\":1,\"2214\":1}}],[\"l=none\",{\"1\":{\"821\":2,\"823\":1,\"824\":2}}],[\"l=sequence\",{\"1\":{\"819\":1}}],[\"lf\",{\"1\":{\"774\":1}}],[\"lfs\",{\"1\":{\"127\":1}}],[\"l9\",{\"1\":{\"290\":1}}],[\"l62\",{\"1\":{\"290\":1}}],[\"l61\",{\"1\":{\"290\":1}}],[\"l69\",{\"1\":{\"289\":1}}],[\"l71\",{\"1\":{\"289\":1}}],[\"lyrics\",{\"1\":{\"269\":3,\"278\":3}}],[\"lyric\",{\"1\":{\"265\":2,\"269\":7,\"274\":2,\"278\":7,\"995\":1}}],[\"lcmv\",{\"0\":{\"1319\":1},\"1\":{\"1319\":2}}],[\"lc\",{\"1\":{\"224\":1,\"535\":2}}],[\"lt\",{\"1\":{\"196\":7,\"200\":2,\"201\":7,\"211\":1,\"212\":3,\"213\":4,\"224\":5,\"225\":6,\"227\":1,\"242\":41,\"243\":5,\"259\":7,\"267\":18,\"268\":7,\"271\":2,\"276\":18,\"277\":7,\"280\":2,\"285\":2,\"286\":41,\"287\":7,\"290\":4,\"515\":2,\"516\":17,\"517\":2,\"518\":5,\"519\":4,\"520\":3,\"521\":8,\"522\":1,\"523\":13,\"524\":10,\"525\":10,\"526\":12,\"527\":10,\"531\":2,\"533\":1,\"535\":5,\"536\":1,\"537\":10}}],[\"ln\",{\"1\":{\"162\":1,\"243\":2,\"738\":1,\"2423\":1}}],[\"l\",{\"1\":{\"98\":2,\"144\":5,\"168\":7,\"271\":4,\"276\":4,\"280\":4,\"286\":1,\"287\":20,\"625\":2,\"627\":1,\"633\":12,\"634\":2,\"637\":4,\"638\":2,\"641\":1,\"642\":2,\"643\":2,\"646\":2,\"647\":4,\"651\":1,\"667\":1,\"675\":5,\"700\":2,\"709\":2,\"710\":3,\"711\":3,\"724\":2,\"725\":2,\"728\":2,\"729\":3,\"733\":2,\"734\":2,\"740\":1,\"744\":2,\"745\":1,\"746\":2,\"747\":1,\"748\":1,\"759\":1,\"771\":1,\"774\":2,\"780\":2,\"784\":2,\"817\":4,\"819\":4,\"821\":1,\"822\":1,\"823\":4,\"824\":7,\"827\":2,\"828\":2,\"829\":4,\"830\":2,\"846\":1,\"847\":1,\"849\":1,\"852\":2,\"856\":2,\"926\":2,\"928\":2,\"1164\":2,\"1308\":2,\"1319\":1,\"1521\":1,\"1656\":1,\"1671\":1,\"1750\":3,\"1815\":1,\"1843\":3,\"1888\":1,\"2000\":22,\"2001\":14,\"2129\":1,\"2187\":2,\"2191\":2,\"2224\":3,\"2228\":1,\"2229\":1,\"2311\":4}}],[\"lst\",{\"1\":{\"2143\":1}}],[\"lstmcell\",{\"1\":{\"1856\":2}}],[\"lstms\",{\"1\":{\"1176\":2}}],[\"lstm\",{\"1\":{\"43\":2,\"142\":2,\"262\":1,\"641\":1,\"796\":1,\"798\":2,\"847\":1,\"862\":2,\"1029\":1,\"1061\":1,\"1062\":1,\"1117\":1,\"1118\":2,\"1124\":5,\"1125\":5,\"1130\":1,\"1131\":1,\"1133\":1,\"1134\":1,\"1136\":2,\"1137\":1,\"1139\":1,\"1141\":2,\"1176\":8,\"1185\":1,\"1202\":2,\"1208\":1,\"1232\":1,\"1255\":2,\"1257\":1,\"1269\":3,\"1270\":3,\"1271\":3,\"1279\":1,\"1280\":2,\"1281\":1,\"1283\":2,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1450\":3,\"1452\":3,\"1454\":3,\"1456\":3,\"1462\":1,\"1466\":1,\"1468\":1,\"1712\":2,\"1750\":3,\"1811\":1,\"1856\":3,\"1945\":1,\"1974\":1,\"1993\":3,\"2223\":3,\"2235\":2,\"2236\":2,\"2245\":3,\"2280\":1,\"2431\":3}}],[\"lsp\",{\"1\":{\"1309\":1,\"1311\":1}}],[\"lsc\",{\"1\":{\"768\":1}}],[\"lsm\",{\"1\":{\"243\":1,\"736\":1,\"737\":1,\"777\":1,\"958\":1,\"962\":2,\"1640\":1,\"1942\":1,\"1959\":1,\"1997\":1,\"2127\":1,\"2221\":1}}],[\"ls\",{\"1\":{\"85\":1}}],[\"lrscheduler\",{\"1\":{\"2014\":1,\"2015\":1,\"2016\":2,\"2017\":1,\"2018\":1,\"2019\":1,\"2021\":1}}],[\"lrngth\",{\"1\":{\"1521\":4,\"2228\":4,\"2229\":4}}],[\"lr=none\",{\"1\":{\"793\":1,\"821\":1,\"823\":1,\"824\":1}}],[\"lr=0\",{\"1\":{\"86\":1,\"119\":2,\"2020\":1}}],[\"lr\",{\"0\":{\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2021\":1},\"1\":{\"84\":2,\"86\":1,\"113\":2,\"243\":1,\"819\":1,\"821\":1,\"824\":2,\"1529\":1,\"1962\":3,\"2014\":6,\"2015\":15,\"2016\":5,\"2017\":3,\"2018\":20,\"2019\":8,\"2020\":9,\"2021\":10,\"2355\":10,\"2432\":1}}],[\"l5\",{\"1\":{\"44\":1}}],[\"l432\",{\"1\":{\"1320\":1}}],[\"l49\",{\"1\":{\"1316\":1}}],[\"l464\",{\"1\":{\"1308\":1,\"1320\":1}}],[\"l4\",{\"1\":{\"44\":1}}],[\"l32\",{\"1\":{\"2286\":1}}],[\"l3das22\",{\"1\":{\"227\":1,\"1210\":1,\"1264\":1,\"1334\":1}}],[\"l3\",{\"1\":{\"44\":1}}],[\"l2norm\",{\"0\":{\"1490\":1},\"1\":{\"1490\":1}}],[\"l2=true\",{\"1\":{\"718\":1}}],[\"l2\",{\"1\":{\"44\":1,\"1607\":1,\"2235\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2245\":1,\"2354\":1,\"2431\":1}}],[\"l1+l2\",{\"1\":{\"1991\":1,\"2235\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2245\":1,\"2431\":2}}],[\"l169\",{\"1\":{\"1332\":1}}],[\"l11\",{\"1\":{\"1316\":1}}],[\"l1\",{\"1\":{\"44\":1,\"1173\":1,\"1210\":1,\"1275\":1,\"1526\":1,\"1607\":1,\"1764\":1,\"1839\":1,\"1991\":1,\"2226\":1,\"2235\":2,\"2237\":1,\"2239\":2,\"2240\":2,\"2241\":2,\"2245\":2,\"2413\":1,\"2424\":1,\"2431\":1,\"2432\":1}}],[\"lmspc\",{\"1\":{\"2495\":2}}],[\"lmtask\",{\"0\":{\"2260\":1},\"1\":{\"2260\":1}}],[\"lm=lm\",{\"1\":{\"1938\":1}}],[\"lmax\",{\"1\":{\"706\":2,\"1526\":1,\"1552\":2,\"1750\":7,\"1764\":3,\"1839\":5,\"1858\":3,\"1907\":3,\"1928\":2,\"1991\":5,\"2223\":5,\"2226\":3,\"2235\":2,\"2236\":2,\"2237\":3,\"2425\":1,\"2429\":1,\"2432\":1}}],[\"lm1\",{\"1\":{\"237\":1}}],[\"lm\",{\"0\":{\"385\":1,\"389\":1,\"904\":1,\"1378\":2,\"1379\":2,\"1380\":1,\"1871\":1,\"1894\":1,\"1921\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":2,\"1945\":2,\"1947\":2,\"2140\":1,\"2148\":1,\"2260\":1,\"2532\":1},\"1\":{\"44\":8,\"46\":3,\"52\":1,\"78\":1,\"119\":2,\"124\":4,\"136\":6,\"139\":2,\"144\":6,\"195\":1,\"197\":1,\"199\":4,\"200\":13,\"204\":4,\"205\":13,\"228\":2,\"240\":4,\"242\":13,\"243\":2,\"267\":2,\"276\":2,\"286\":3,\"301\":12,\"315\":10,\"321\":6,\"385\":1,\"389\":12,\"396\":10,\"403\":12,\"421\":12,\"429\":12,\"442\":12,\"463\":20,\"469\":10,\"498\":12,\"505\":10,\"526\":3,\"616\":8,\"625\":6,\"628\":5,\"631\":5,\"696\":6,\"697\":6,\"743\":3,\"760\":4,\"763\":2,\"777\":1,\"847\":6,\"904\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1732\":1,\"1749\":7,\"1762\":3,\"1775\":2,\"1815\":6,\"1822\":3,\"1843\":7,\"1862\":1,\"1871\":11,\"1894\":7,\"1921\":12,\"1938\":4,\"1940\":2,\"1942\":2,\"1944\":3,\"1945\":2,\"1946\":1,\"1947\":3,\"2044\":2,\"2140\":1,\"2148\":1,\"2176\":2,\"2260\":1,\"2345\":1}}],[\"llamamoe\",{\"0\":{\"2053\":1}}],[\"llamamlp\",{\"0\":{\"2052\":1}}],[\"llama\",{\"1\":{\"2049\":4}}],[\"llbackward\",{\"1\":{\"879\":3,\"883\":3}}],[\"llforward\",{\"1\":{\"878\":3,\"882\":3}}],[\"llm\",{\"0\":{\"2030\":2,\"2049\":2},\"1\":{\"246\":2,\"247\":3,\"260\":1,\"262\":2,\"2030\":2,\"2044\":17,\"2049\":3,\"2140\":2,\"2148\":1}}],[\"ll60k\",{\"1\":{\"128\":1,\"267\":1,\"276\":2}}],[\"ll\",{\"1\":{\"41\":1,\"124\":1,\"159\":1,\"161\":1,\"162\":2,\"197\":2,\"263\":1}}],[\"louder\",{\"1\":{\"1679\":1}}],[\"loudness\",{\"0\":{\"1560\":1,\"1567\":1},\"1\":{\"1560\":1,\"1567\":1}}],[\"lora\",{\"0\":{\"1683\":1,\"1685\":1},\"1\":{\"377\":6,\"449\":6,\"1681\":1,\"1683\":10,\"1685\":2}}],[\"lot\",{\"1\":{\"173\":1}}],[\"los\",{\"1\":{\"113\":1}}],[\"loss2\",{\"1\":{\"2369\":2}}],[\"loss1\",{\"1\":{\"2355\":1,\"2369\":2}}],[\"loss0\",{\"1\":{\"2355\":1}}],[\"losswrapper\",{\"1\":{\"1209\":1,\"1228\":1}}],[\"loss=0\",{\"1\":{\"2359\":1}}],[\"loss=loss\",{\"1\":{\"2327\":2}}],[\"loss=false\",{\"1\":{\"1066\":2,\"1170\":2,\"1171\":2,\"1172\":2,\"1173\":2,\"1174\":2,\"1175\":2,\"1210\":2,\"1246\":2,\"1247\":2,\"1248\":2,\"1275\":2,\"1276\":2,\"1277\":2}}],[\"loss=44\",{\"1\":{\"113\":1}}],[\"loss=50\",{\"1\":{\"113\":1}}],[\"lossy\",{\"1\":{\"71\":3}}],[\"lossless\",{\"1\":{\"71\":1}}],[\"losses\",{\"0\":{\"1969\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1},\"1\":{\"44\":3,\"144\":1,\"706\":1,\"786\":1,\"800\":1,\"846\":2,\"921\":1,\"935\":1,\"954\":1,\"1155\":2,\"1156\":2,\"1157\":2,\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1521\":1,\"1526\":1,\"1553\":1,\"1585\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1969\":1,\"1975\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"2216\":1,\"2228\":1,\"2229\":1,\"2327\":1,\"2408\":1,\"2446\":1,\"2462\":1}}],[\"loss\",{\"0\":{\"921\":1,\"922\":1,\"935\":1,\"936\":1,\"937\":1,\"947\":2,\"948\":2,\"949\":2,\"950\":2,\"1036\":2,\"1042\":1,\"1066\":1,\"1132\":1,\"1167\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1204\":1,\"1209\":1,\"1210\":1,\"1228\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1382\":2,\"1394\":2,\"1419\":2,\"1584\":1,\"1587\":1,\"1589\":1,\"1591\":1,\"1601\":1,\"1602\":1,\"1607\":1,\"1627\":1,\"1638\":1,\"1782\":1,\"1969\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"2167\":1,\"2170\":2,\"2176\":1,\"2207\":1,\"2215\":2,\"2218\":1,\"2219\":1,\"2226\":1,\"2241\":1,\"2413\":1,\"2424\":1,\"2427\":1,\"2441\":1,\"2448\":1,\"2456\":2,\"2469\":2,\"2470\":1,\"2471\":2,\"2472\":2,\"2473\":1},\"1\":{\"39\":7,\"42\":3,\"44\":39,\"46\":2,\"50\":2,\"71\":1,\"78\":4,\"102\":2,\"113\":3,\"136\":2,\"138\":1,\"139\":13,\"144\":12,\"175\":2,\"218\":2,\"222\":1,\"223\":2,\"225\":5,\"242\":1,\"243\":1,\"259\":1,\"262\":1,\"263\":1,\"267\":2,\"276\":2,\"285\":1,\"286\":9,\"290\":1,\"541\":1,\"625\":20,\"667\":1,\"703\":1,\"706\":3,\"709\":1,\"717\":1,\"736\":2,\"737\":2,\"755\":2,\"774\":1,\"777\":2,\"785\":2,\"786\":1,\"794\":1,\"800\":1,\"866\":1,\"878\":1,\"879\":1,\"882\":2,\"883\":2,\"884\":1,\"921\":2,\"922\":2,\"935\":2,\"936\":2,\"937\":2,\"947\":3,\"948\":3,\"949\":3,\"950\":3,\"954\":1,\"958\":2,\"961\":2,\"974\":4,\"1036\":3,\"1042\":2,\"1053\":3,\"1066\":3,\"1132\":3,\"1155\":14,\"1156\":5,\"1157\":16,\"1158\":3,\"1167\":3,\"1170\":3,\"1171\":3,\"1172\":4,\"1173\":3,\"1174\":4,\"1175\":3,\"1204\":4,\"1209\":7,\"1210\":5,\"1217\":1,\"1228\":5,\"1246\":5,\"1247\":5,\"1248\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1275\":3,\"1276\":4,\"1277\":3,\"1381\":1,\"1382\":2,\"1389\":10,\"1391\":2,\"1394\":2,\"1395\":3,\"1396\":10,\"1401\":10,\"1403\":2,\"1408\":11,\"1410\":2,\"1419\":5,\"1441\":1,\"1466\":11,\"1468\":2,\"1508\":1,\"1521\":3,\"1526\":32,\"1553\":32,\"1576\":1,\"1577\":1,\"1584\":9,\"1585\":3,\"1587\":7,\"1589\":4,\"1591\":8,\"1598\":26,\"1599\":3,\"1600\":30,\"1601\":4,\"1602\":3,\"1607\":6,\"1625\":26,\"1627\":7,\"1637\":1,\"1638\":6,\"1640\":3,\"1641\":1,\"1702\":3,\"1754\":5,\"1764\":6,\"1770\":4,\"1771\":3,\"1782\":5,\"1839\":7,\"1936\":3,\"1938\":1,\"1942\":1,\"1944\":1,\"1947\":1,\"1949\":1,\"1959\":2,\"1965\":1,\"1969\":2,\"1971\":1,\"1987\":2,\"1988\":2,\"1990\":3,\"1991\":6,\"1996\":1,\"1997\":2,\"2127\":2,\"2130\":5,\"2131\":2,\"2133\":2,\"2136\":4,\"2137\":4,\"2143\":5,\"2148\":1,\"2167\":5,\"2170\":2,\"2176\":7,\"2184\":2,\"2207\":5,\"2215\":5,\"2216\":1,\"2218\":1,\"2219\":8,\"2221\":2,\"2222\":1,\"2226\":9,\"2228\":2,\"2229\":2,\"2235\":6,\"2236\":3,\"2237\":6,\"2239\":17,\"2240\":19,\"2241\":12,\"2245\":15,\"2325\":3,\"2327\":13,\"2355\":20,\"2359\":2,\"2403\":1,\"2408\":2,\"2411\":3,\"2412\":3,\"2413\":9,\"2423\":3,\"2424\":9,\"2427\":6,\"2431\":15,\"2432\":17,\"2441\":1,\"2445\":1,\"2446\":2,\"2447\":3,\"2448\":9,\"2456\":3,\"2462\":1,\"2469\":3,\"2470\":1,\"2471\":3,\"2472\":3,\"2473\":1}}],[\"loops\",{\"1\":{\"263\":1}}],[\"loop\",{\"1\":{\"245\":1,\"961\":1,\"2134\":1}}],[\"loopback\",{\"1\":{\"67\":1}}],[\"looking\",{\"1\":{\"787\":1}}],[\"looks\",{\"1\":{\"242\":1,\"994\":1,\"1008\":1,\"2480\":1}}],[\"look\",{\"1\":{\"84\":1,\"130\":2,\"168\":1,\"259\":1,\"286\":1,\"710\":3,\"711\":3,\"1720\":1,\"1721\":5,\"2132\":1}}],[\"longtensororlist\",{\"1\":{\"1901\":1,\"1903\":1,\"1905\":1}}],[\"longtensor\",{\"1\":{\"1054\":2,\"1126\":4,\"1127\":4,\"1199\":2,\"1526\":13,\"1552\":16,\"1553\":13,\"1599\":2,\"1627\":3,\"1750\":2,\"1753\":1,\"1754\":1,\"1758\":2,\"1764\":2,\"1766\":2,\"1770\":2,\"1771\":2,\"1788\":1,\"1803\":1,\"1839\":2,\"1854\":2,\"1928\":1,\"1966\":1,\"1991\":2,\"1992\":3,\"1993\":4,\"1995\":3,\"2124\":4,\"2128\":4,\"2223\":1,\"2226\":2,\"2227\":1,\"2231\":2,\"2235\":18,\"2236\":18,\"2237\":1,\"2239\":21,\"2240\":19,\"2241\":4,\"2245\":18,\"2407\":1,\"2411\":7,\"2412\":8,\"2413\":4,\"2423\":8,\"2424\":4,\"2431\":4,\"2432\":4,\"2447\":8,\"2448\":4}}],[\"longformerattention\",{\"0\":{\"1792\":1}}],[\"longformerencoder\",{\"0\":{\"774\":1},\"1\":{\"774\":1}}],[\"longformer\",{\"0\":{\"774\":1,\"1792\":1},\"1\":{\"774\":6}}],[\"longshortdata\",{\"0\":{\"529\":1},\"1\":{\"529\":1}}],[\"long\",{\"0\":{\"73\":1},\"1\":{\"196\":1,\"197\":1,\"199\":1,\"200\":1,\"204\":1,\"205\":2,\"211\":1,\"216\":1,\"217\":2,\"227\":1,\"235\":1,\"240\":2,\"242\":2,\"266\":1,\"268\":1,\"275\":1,\"277\":1,\"284\":1,\"285\":2,\"286\":1,\"290\":1,\"1035\":1,\"1113\":1,\"1246\":1,\"1251\":1,\"1259\":1,\"1261\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"1803\":1,\"2342\":1,\"2351\":1}}],[\"longer\",{\"1\":{\"27\":1,\"269\":2,\"278\":2,\"756\":1,\"773\":1}}],[\"loads\",{\"1\":{\"2044\":4,\"2136\":1,\"2138\":1}}],[\"loadable\",{\"1\":{\"286\":1}}],[\"loaded\",{\"1\":{\"266\":1,\"275\":1,\"930\":1}}],[\"loaders\",{\"1\":{\"263\":1}}],[\"loader\",{\"0\":{\"82\":1,\"2139\":2,\"2144\":2,\"2390\":1,\"2391\":1,\"2393\":1,\"2394\":1,\"2396\":1,\"2397\":1,\"2399\":1},\"1\":{\"82\":4,\"987\":1,\"989\":1,\"1019\":1,\"2134\":8,\"2139\":2,\"2144\":2,\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2390\":1,\"2391\":1,\"2393\":1,\"2394\":2,\"2396\":1,\"2397\":1,\"2399\":1}}],[\"load\",{\"0\":{\"668\":1,\"1019\":1,\"1021\":1,\"1872\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2308\":1,\"2314\":2,\"2392\":1},\"1\":{\"71\":1,\"88\":5,\"223\":2,\"243\":1,\"247\":1,\"267\":1,\"284\":1,\"286\":1,\"290\":6,\"668\":2,\"699\":1,\"759\":1,\"760\":3,\"790\":1,\"1019\":1,\"1020\":1,\"1021\":1,\"1022\":3,\"1025\":1,\"1246\":2,\"1644\":1,\"1872\":2,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2020\":1,\"2040\":1,\"2043\":1,\"2044\":3,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2134\":3,\"2249\":1,\"2308\":1,\"2314\":3,\"2315\":5,\"2342\":1,\"2359\":1,\"2368\":2,\"2392\":1,\"2422\":1}}],[\"loading\",{\"0\":{\"339\":1,\"347\":1,\"354\":1,\"366\":1},\"1\":{\"71\":1,\"747\":1,\"993\":1,\"1000\":1,\"1126\":1,\"1127\":1,\"1217\":1,\"1309\":2,\"1310\":2,\"1311\":2,\"1318\":2,\"1319\":2,\"1321\":2,\"1322\":2,\"1323\":2,\"1327\":2,\"1329\":1,\"1330\":2,\"1334\":2,\"2130\":6,\"2131\":1,\"2132\":1,\"2134\":1,\"2137\":1,\"2344\":1}}],[\"lo\",{\"1\":{\"67\":4}}],[\"lowest\",{\"1\":{\"1662\":1}}],[\"lower=\",{\"1\":{\"1802\":1,\"1852\":1}}],[\"lower=0\",{\"1\":{\"1717\":1,\"1833\":1}}],[\"lower\",{\"1\":{\"1308\":1,\"1654\":1,\"1676\":1}}],[\"lowpass\",{\"0\":{\"1690\":1},\"1\":{\"1655\":1,\"1690\":2}}],[\"low=0\",{\"1\":{\"990\":1}}],[\"low\",{\"1\":{\"45\":1,\"145\":1,\"173\":1,\"262\":1,\"703\":1,\"755\":1,\"785\":1,\"786\":1,\"800\":1,\"821\":1,\"824\":3,\"867\":1,\"881\":1,\"884\":1,\"922\":1,\"924\":1,\"928\":1,\"936\":1,\"937\":1,\"989\":1,\"1210\":1,\"1264\":1,\"1327\":5,\"1330\":5,\"1334\":1,\"2000\":2,\"2001\":3,\"2136\":1}}],[\"logmel2linear\",{\"0\":{\"2495\":1},\"1\":{\"2495\":1}}],[\"logmelfbank\",{\"0\":{\"1980\":1,\"2416\":1},\"1\":{\"1980\":1,\"2416\":1}}],[\"logmelspectrogram\",{\"0\":{\"1791\":1,\"1900\":1},\"1\":{\"1791\":1,\"1900\":1}}],[\"logmel\",{\"0\":{\"1662\":1},\"1\":{\"1662\":1,\"1791\":2,\"1836\":1,\"1900\":1,\"1925\":1}}],[\"logcompression\",{\"0\":{\"1661\":1},\"1\":{\"1661\":1}}],[\"logflow\",{\"0\":{\"1603\":1},\"1\":{\"1603\":1}}],[\"logfile\",{\"1\":{\"134\":3}}],[\"logll\",{\"1\":{\"703\":2,\"881\":3,\"884\":3}}],[\"loglikelihood\",{\"1\":{\"703\":4}}],[\"logarithmic\",{\"1\":{\"267\":1,\"276\":1}}],[\"logits\",{\"1\":{\"959\":1,\"1385\":3,\"1401\":1,\"1466\":1,\"1640\":1,\"1750\":1,\"1839\":3,\"1854\":1,\"1991\":3,\"1993\":1,\"2218\":3,\"2223\":1,\"2463\":1,\"2469\":2,\"2471\":1,\"2473\":4}}],[\"logit\",{\"1\":{\"674\":2,\"785\":1,\"786\":1,\"866\":1,\"882\":1,\"883\":1,\"884\":1,\"921\":1,\"922\":1,\"2455\":1,\"2463\":1,\"2464\":1}}],[\"logic\",{\"1\":{\"263\":1}}],[\"login\",{\"1\":{\"22\":1,\"92\":1}}],[\"logspectrogram\",{\"0\":{\"1982\":1,\"2418\":1},\"1\":{\"1982\":1,\"2418\":1}}],[\"logsumexp\",{\"1\":{\"918\":1}}],[\"logsoftmaxgradmodification\",{\"0\":{\"773\":1},\"1\":{\"773\":1}}],[\"logs\",{\"1\":{\"135\":1,\"960\":1,\"1601\":4,\"1602\":4}}],[\"logps\",{\"1\":{\"1920\":2}}],[\"logprobs\",{\"1\":{\"703\":2,\"755\":2,\"785\":2,\"878\":4,\"879\":4,\"881\":4,\"882\":4,\"883\":4,\"884\":4,\"919\":5}}],[\"logp\",{\"0\":{\"919\":1},\"1\":{\"45\":1,\"145\":1,\"616\":3,\"696\":1,\"697\":1,\"770\":1,\"919\":1,\"1224\":1,\"1225\":1,\"1245\":1,\"1920\":1}}],[\"logdir\",{\"1\":{\"39\":1,\"115\":1,\"136\":2,\"223\":1,\"267\":1,\"276\":1,\"519\":1}}],[\"logger=none\",{\"1\":{\"2359\":1}}],[\"logger\",{\"0\":{\"907\":1,\"2072\":1},\"1\":{\"907\":3,\"2355\":2}}],[\"logged\",{\"1\":{\"39\":1,\"135\":1,\"2145\":1,\"2355\":1}}],[\"logging\",{\"0\":{\"39\":1,\"90\":1},\"1\":{\"78\":1,\"134\":1,\"1155\":1,\"1157\":1,\"1965\":1,\"2157\":1,\"2355\":1}}],[\"log\",{\"0\":{\"113\":1,\"916\":1,\"917\":1,\"918\":1,\"1572\":1,\"1662\":1,\"1980\":1,\"1982\":1,\"2416\":1,\"2418\":1},\"1\":{\"39\":1,\"90\":1,\"113\":1,\"133\":1,\"134\":6,\"135\":1,\"136\":3,\"167\":7,\"218\":3,\"246\":1,\"263\":2,\"267\":9,\"276\":10,\"286\":10,\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"374\":4,\"377\":3,\"385\":3,\"389\":1,\"396\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":3,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"511\":1,\"516\":3,\"521\":2,\"523\":3,\"524\":3,\"525\":3,\"537\":1,\"541\":2,\"543\":3,\"616\":1,\"628\":1,\"631\":1,\"650\":1,\"691\":3,\"703\":7,\"706\":4,\"717\":4,\"720\":1,\"755\":4,\"770\":1,\"777\":2,\"778\":1,\"785\":2,\"786\":1,\"792\":2,\"800\":1,\"820\":1,\"823\":1,\"824\":1,\"828\":1,\"865\":1,\"867\":1,\"873\":1,\"878\":2,\"879\":2,\"881\":3,\"882\":2,\"883\":2,\"884\":3,\"916\":1,\"918\":1,\"919\":1,\"958\":1,\"1156\":2,\"1224\":4,\"1225\":3,\"1245\":3,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1419\":3,\"1466\":1,\"1526\":1,\"1552\":1,\"1553\":1,\"1572\":1,\"1577\":1,\"1581\":1,\"1586\":1,\"1588\":1,\"1589\":3,\"1598\":1,\"1599\":1,\"1600\":1,\"1603\":3,\"1607\":3,\"1613\":1,\"1616\":2,\"1625\":1,\"1626\":1,\"1637\":4,\"1661\":3,\"1662\":2,\"1668\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"1732\":2,\"1752\":1,\"1753\":3,\"1754\":3,\"1755\":1,\"1854\":1,\"1862\":1,\"1920\":1,\"1940\":1,\"1941\":1,\"1942\":1,\"1943\":1,\"1951\":1,\"1980\":3,\"1982\":2,\"1997\":1,\"2015\":1,\"2157\":2,\"2226\":2,\"2241\":2,\"2339\":2,\"2348\":2,\"2355\":6,\"2359\":2,\"2367\":2,\"2370\":4,\"2372\":2,\"2404\":1,\"2416\":3,\"2418\":2,\"2495\":2}}],[\"location\",{\"1\":{\"53\":1,\"212\":1,\"527\":4,\"796\":1,\"869\":1,\"1706\":2,\"1710\":1,\"1711\":2,\"1712\":2,\"1715\":2,\"1716\":2,\"2314\":1,\"2431\":1}}],[\"located\",{\"1\":{\"3\":1,\"22\":1,\"1931\":1}}],[\"locally\",{\"1\":{\"247\":1,\"249\":1,\"269\":1,\"278\":1}}],[\"localhost\",{\"1\":{\"39\":1}}],[\"local\",{\"0\":{\"26\":1,\"1128\":1,\"2381\":1},\"1\":{\"26\":11,\"108\":1,\"110\":1,\"161\":1,\"163\":1,\"165\":1,\"166\":1,\"197\":7,\"200\":1,\"201\":6,\"205\":1,\"211\":1,\"217\":1,\"224\":5,\"227\":1,\"235\":1,\"242\":2,\"243\":5,\"249\":1,\"254\":1,\"266\":1,\"269\":4,\"275\":1,\"278\":4,\"285\":3,\"290\":6,\"356\":1,\"377\":2,\"449\":2,\"1064\":1,\"1119\":1,\"1128\":1,\"1499\":1,\"1610\":2,\"1628\":2,\"2340\":2,\"2381\":1,\"2384\":1,\"2385\":1}}],[\"lack\",{\"1\":{\"2474\":1}}],[\"lacks\",{\"1\":{\"211\":1}}],[\"lazy\",{\"1\":{\"2139\":1}}],[\"lab\",{\"1\":{\"1526\":7,\"1553\":7,\"2235\":9,\"2236\":9,\"2239\":9,\"2240\":9,\"2245\":9}}],[\"labeled\",{\"1\":{\"2354\":1}}],[\"label=none\",{\"1\":{\"2170\":1}}],[\"labelaggregate\",{\"0\":{\"1660\":1},\"1\":{\"1660\":2}}],[\"labelprocessor\",{\"0\":{\"976\":1},\"1\":{\"976\":1}}],[\"labellength\",{\"1\":{\"786\":1,\"800\":1,\"867\":1,\"921\":1,\"935\":1}}],[\"labelling\",{\"1\":{\"232\":1,\"258\":1}}],[\"labelsmoothingloss\",{\"0\":{\"1782\":1},\"1\":{\"1782\":2}}],[\"labels\",{\"1\":{\"211\":2,\"232\":2,\"233\":1,\"259\":6,\"287\":3,\"614\":2,\"634\":4,\"641\":2,\"643\":4,\"651\":2,\"667\":2,\"703\":5,\"706\":1,\"717\":4,\"755\":5,\"785\":5,\"786\":2,\"800\":2,\"846\":1,\"847\":3,\"866\":1,\"867\":2,\"873\":1,\"921\":2,\"922\":3,\"935\":2,\"936\":3,\"937\":3,\"954\":1,\"960\":1,\"974\":5,\"1702\":4,\"1729\":1,\"1749\":1,\"1815\":3,\"1839\":3,\"1843\":1,\"1858\":1,\"1888\":2,\"1910\":2,\"1991\":3,\"1993\":1,\"1997\":1,\"2001\":1,\"2167\":1,\"2176\":1,\"2184\":4,\"2207\":1,\"2218\":1,\"2262\":1,\"2298\":1,\"2355\":4,\"2462\":2,\"2472\":1,\"2475\":1,\"2506\":1}}],[\"label\",{\"0\":{\"962\":1,\"976\":1,\"1024\":1,\"1660\":1,\"1782\":1,\"2391\":1,\"2472\":1},\"1\":{\"44\":2,\"144\":2,\"175\":1,\"211\":7,\"213\":3,\"259\":1,\"261\":1,\"266\":1,\"268\":2,\"269\":1,\"275\":1,\"277\":2,\"278\":1,\"287\":1,\"614\":5,\"616\":1,\"625\":5,\"627\":3,\"628\":1,\"631\":1,\"634\":5,\"641\":6,\"643\":5,\"651\":5,\"667\":3,\"674\":2,\"703\":3,\"706\":1,\"740\":3,\"748\":4,\"755\":4,\"785\":4,\"786\":4,\"800\":4,\"847\":9,\"866\":1,\"867\":3,\"873\":1,\"921\":4,\"922\":4,\"935\":4,\"936\":4,\"937\":4,\"947\":3,\"948\":3,\"949\":3,\"954\":1,\"958\":4,\"962\":8,\"974\":5,\"976\":4,\"1002\":2,\"1024\":1,\"1025\":1,\"1174\":1,\"1521\":24,\"1526\":9,\"1552\":12,\"1553\":9,\"1660\":3,\"1667\":1,\"1729\":1,\"1730\":4,\"1731\":1,\"1749\":10,\"1782\":2,\"1815\":9,\"1822\":1,\"1843\":4,\"1888\":2,\"1897\":2,\"1907\":2,\"1910\":3,\"1915\":1,\"1927\":1,\"1928\":4,\"1966\":1,\"1987\":1,\"1997\":1,\"2000\":1,\"2167\":2,\"2176\":2,\"2207\":2,\"2228\":24,\"2229\":24,\"2232\":7,\"2235\":11,\"2236\":11,\"2238\":6,\"2239\":10,\"2240\":10,\"2244\":1,\"2245\":10,\"2350\":1,\"2363\":2,\"2391\":1,\"2472\":2}}],[\"laplace\",{\"0\":{\"1491\":1},\"1\":{\"1491\":1}}],[\"lapack\",{\"1\":{\"1308\":1}}],[\"layout\",{\"1\":{\"1462\":2}}],[\"layered\",{\"1\":{\"1795\":1}}],[\"layer=\",{\"1\":{\"1758\":1,\"2231\":1}}],[\"layer=<class\",{\"1\":{\"1064\":1,\"1205\":1,\"1262\":2}}],[\"layer=6\",{\"1\":{\"1061\":1}}],[\"layer=4\",{\"1\":{\"1059\":1}}],[\"layer=embed\",{\"1\":{\"709\":1,\"710\":1,\"711\":1,\"771\":1,\"774\":1,\"780\":1,\"849\":1,\"1107\":1,\"2129\":1,\"2191\":1}}],[\"layer=none\",{\"1\":{\"675\":2,\"820\":1,\"828\":1,\"830\":2}}],[\"layerdrop\",{\"1\":{\"674\":4,\"746\":1,\"747\":1}}],[\"layernormalization4dcf\",{\"0\":{\"1194\":1},\"1\":{\"1194\":1}}],[\"layernormalization4d\",{\"0\":{\"1192\":1},\"1\":{\"1192\":1}}],[\"layernormalization\",{\"0\":{\"1190\":1},\"1\":{\"1190\":1}}],[\"layernorm\",{\"0\":{\"1527\":1,\"1783\":1},\"1\":{\"617\":1,\"618\":1,\"622\":1,\"624\":1,\"633\":1,\"636\":1,\"638\":1,\"642\":1,\"722\":1,\"852\":2,\"854\":1,\"1064\":1,\"1262\":1,\"1387\":2,\"1527\":1,\"1546\":1,\"1599\":2,\"1622\":1,\"1683\":1,\"1783\":3,\"2239\":2,\"2240\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2432\":2,\"2447\":2}}],[\"layer\",{\"0\":{\"887\":1,\"1340\":1,\"1657\":1,\"1735\":1,\"1737\":1,\"1751\":1,\"1759\":1,\"1783\":1,\"1795\":1,\"1847\":1,\"1866\":1,\"1914\":1,\"2311\":2},\"1\":{\"43\":1,\"44\":1,\"50\":2,\"51\":4,\"128\":2,\"141\":3,\"142\":5,\"243\":5,\"252\":2,\"259\":2,\"275\":8,\"276\":24,\"634\":4,\"641\":1,\"642\":2,\"643\":4,\"651\":1,\"661\":2,\"674\":2,\"689\":1,\"691\":2,\"692\":11,\"699\":5,\"700\":3,\"701\":1,\"702\":1,\"709\":17,\"710\":9,\"711\":8,\"712\":1,\"731\":2,\"732\":2,\"733\":9,\"734\":9,\"735\":1,\"738\":4,\"745\":2,\"746\":1,\"747\":2,\"748\":1,\"754\":1,\"760\":1,\"765\":1,\"766\":2,\"767\":2,\"768\":3,\"771\":4,\"774\":18,\"775\":4,\"780\":18,\"784\":1,\"788\":1,\"790\":2,\"798\":1,\"809\":1,\"815\":1,\"820\":13,\"828\":8,\"830\":7,\"846\":18,\"847\":2,\"848\":3,\"849\":8,\"850\":4,\"851\":2,\"854\":1,\"862\":1,\"887\":1,\"911\":2,\"971\":1,\"975\":1,\"978\":1,\"979\":1,\"980\":2,\"1029\":3,\"1061\":2,\"1062\":1,\"1064\":3,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1084\":2,\"1107\":15,\"1110\":1,\"1117\":2,\"1118\":3,\"1124\":8,\"1125\":8,\"1130\":2,\"1131\":2,\"1133\":1,\"1136\":2,\"1137\":1,\"1140\":2,\"1141\":2,\"1144\":1,\"1147\":5,\"1162\":2,\"1164\":1,\"1165\":1,\"1176\":3,\"1179\":1,\"1208\":1,\"1209\":5,\"1232\":2,\"1235\":1,\"1252\":1,\"1257\":1,\"1261\":2,\"1262\":4,\"1267\":2,\"1268\":8,\"1274\":7,\"1278\":5,\"1279\":3,\"1280\":1,\"1281\":3,\"1282\":1,\"1283\":1,\"1331\":1,\"1340\":1,\"1390\":1,\"1397\":2,\"1402\":1,\"1409\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1467\":1,\"1513\":1,\"1514\":4,\"1515\":1,\"1516\":1,\"1519\":13,\"1526\":3,\"1534\":2,\"1535\":10,\"1536\":11,\"1539\":1,\"1546\":10,\"1548\":1,\"1549\":1,\"1551\":1,\"1552\":13,\"1553\":3,\"1582\":2,\"1583\":1,\"1592\":1,\"1593\":1,\"1595\":1,\"1596\":2,\"1597\":3,\"1598\":3,\"1599\":14,\"1600\":3,\"1604\":5,\"1605\":4,\"1606\":4,\"1610\":1,\"1613\":1,\"1614\":2,\"1615\":2,\"1619\":2,\"1620\":1,\"1622\":10,\"1625\":3,\"1626\":13,\"1628\":1,\"1657\":1,\"1683\":1,\"1735\":13,\"1737\":2,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1749\":9,\"1751\":4,\"1756\":3,\"1757\":3,\"1758\":2,\"1759\":6,\"1783\":3,\"1784\":1,\"1785\":1,\"1789\":3,\"1790\":3,\"1794\":1,\"1795\":1,\"1796\":4,\"1809\":1,\"1815\":3,\"1816\":1,\"1817\":1,\"1847\":2,\"1849\":1,\"1863\":16,\"1864\":3,\"1866\":5,\"1867\":3,\"1914\":7,\"1917\":3,\"1936\":5,\"1947\":1,\"1992\":11,\"1993\":2,\"1994\":2,\"1995\":11,\"2102\":1,\"2126\":4,\"2129\":8,\"2184\":1,\"2191\":15,\"2198\":2,\"2215\":1,\"2218\":1,\"2219\":2,\"2231\":2,\"2235\":2,\"2236\":2,\"2239\":12,\"2240\":9,\"2245\":2,\"2311\":6,\"2411\":13,\"2412\":13,\"2423\":13,\"2428\":1,\"2430\":3,\"2431\":2,\"2432\":9,\"2447\":13}}],[\"layershift\",{\"0\":{\"2102\":1},\"1\":{\"2102\":1}}],[\"layerspp\",{\"0\":{\"1057\":1,\"1076\":1,\"1177\":1,\"1238\":1,\"1242\":1},\"1\":{\"1057\":1,\"1076\":1,\"1177\":1,\"1238\":1,\"1242\":1}}],[\"layers>1\",{\"1\":{\"276\":2}}],[\"layers=3\",{\"1\":{\"1758\":1,\"2231\":1}}],[\"layers=6\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1}}],[\"layers=5\",{\"1\":{\"1124\":1,\"1147\":1,\"1750\":1,\"1810\":1,\"2223\":1}}],[\"layers=2\",{\"1\":{\"276\":1,\"1124\":1,\"1176\":1,\"1750\":1,\"1752\":1,\"1811\":1,\"1985\":1,\"2223\":1}}],[\"layers=1\",{\"1\":{\"276\":4,\"828\":1,\"1120\":1,\"1122\":1,\"1134\":1,\"1137\":1,\"1139\":1}}],[\"layers\",{\"0\":{\"713\":1,\"715\":1,\"749\":1,\"781\":1,\"783\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"975\":1,\"978\":1,\"981\":1,\"982\":1,\"983\":1,\"984\":1,\"1029\":1,\"1051\":1,\"1054\":1,\"1055\":2,\"1057\":1,\"1059\":1,\"1061\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1068\":2,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1086\":2,\"1087\":2,\"1089\":2,\"1091\":2,\"1093\":2,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":2,\"1105\":1,\"1108\":1,\"1110\":1,\"1114\":2,\"1119\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1129\":1,\"1133\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1144\":2,\"1145\":1,\"1147\":1,\"1148\":1,\"1149\":1,\"1151\":2,\"1153\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1168\":1,\"1176\":1,\"1177\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1185\":1,\"1187\":1,\"1196\":2,\"1198\":1,\"1199\":1,\"1200\":2,\"1202\":1,\"1205\":1,\"1207\":2,\"1208\":1,\"1211\":1,\"1213\":2,\"1215\":1,\"1219\":1,\"1226\":1,\"1230\":2,\"1233\":2,\"1235\":1,\"1236\":2,\"1238\":1,\"1240\":2,\"1242\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1279\":1,\"1281\":1,\"1282\":1,\"1284\":2,\"1286\":2,\"1288\":1,\"1290\":1,\"1291\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1297\":1,\"1298\":1,\"1299\":2,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":2,\"1304\":2,\"1305\":2,\"1306\":1,\"1307\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1312\":2,\"1313\":1,\"1314\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1330\":1,\"1331\":1,\"1332\":1,\"1333\":1,\"1335\":2,\"1336\":2,\"1337\":1,\"1338\":1,\"1339\":1,\"1340\":2,\"1341\":1,\"1342\":1,\"1343\":1,\"1344\":1,\"1345\":1,\"1346\":2,\"1347\":2,\"1348\":1,\"1349\":1,\"1351\":1,\"1352\":1,\"1353\":1,\"1354\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1,\"1359\":1,\"1360\":1,\"1361\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":2,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1657\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1672\":1,\"1673\":1,\"1674\":1,\"1675\":1,\"1676\":1,\"1677\":1,\"1679\":1,\"1680\":1,\"1681\":1,\"1682\":1,\"1683\":1,\"1684\":1,\"1685\":1,\"1686\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1694\":1,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1699\":1,\"1700\":1,\"1701\":1,\"1936\":1,\"2168\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2185\":1,\"2200\":1,\"2529\":1},\"1\":{\"43\":2,\"44\":1,\"52\":1,\"102\":1,\"128\":1,\"141\":5,\"142\":3,\"143\":1,\"223\":1,\"262\":2,\"276\":11,\"475\":2,\"617\":1,\"624\":1,\"641\":5,\"674\":8,\"692\":1,\"699\":1,\"713\":1,\"715\":1,\"724\":2,\"725\":2,\"726\":2,\"727\":2,\"728\":2,\"734\":1,\"744\":2,\"745\":2,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"750\":2,\"762\":1,\"765\":1,\"781\":1,\"783\":1,\"796\":1,\"798\":3,\"807\":1,\"820\":3,\"828\":4,\"829\":2,\"830\":2,\"846\":6,\"847\":6,\"848\":1,\"851\":1,\"859\":2,\"862\":3,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"975\":1,\"978\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"984\":1,\"1029\":2,\"1051\":1,\"1054\":1,\"1055\":2,\"1057\":1,\"1059\":1,\"1061\":2,\"1062\":3,\"1063\":1,\"1064\":1,\"1065\":1,\"1068\":2,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1086\":2,\"1087\":2,\"1089\":2,\"1091\":2,\"1093\":2,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":3,\"1105\":1,\"1107\":2,\"1108\":1,\"1110\":1,\"1114\":2,\"1117\":2,\"1118\":1,\"1119\":3,\"1120\":1,\"1122\":1,\"1124\":9,\"1125\":10,\"1126\":1,\"1127\":1,\"1128\":1,\"1129\":1,\"1130\":2,\"1131\":2,\"1133\":4,\"1134\":4,\"1136\":2,\"1137\":4,\"1139\":4,\"1141\":2,\"1144\":2,\"1145\":2,\"1147\":6,\"1148\":1,\"1149\":1,\"1151\":2,\"1153\":2,\"1163\":1,\"1164\":1,\"1165\":1,\"1168\":1,\"1176\":3,\"1177\":1,\"1179\":1,\"1180\":2,\"1181\":2,\"1185\":1,\"1187\":1,\"1196\":2,\"1198\":1,\"1199\":2,\"1200\":2,\"1202\":2,\"1205\":1,\"1207\":2,\"1208\":2,\"1209\":1,\"1211\":1,\"1213\":2,\"1215\":1,\"1219\":1,\"1226\":1,\"1230\":2,\"1232\":2,\"1233\":2,\"1235\":1,\"1236\":2,\"1238\":1,\"1240\":2,\"1242\":1,\"1252\":4,\"1255\":2,\"1257\":2,\"1259\":2,\"1262\":1,\"1264\":4,\"1265\":1,\"1267\":1,\"1268\":3,\"1269\":2,\"1270\":2,\"1271\":3,\"1272\":1,\"1273\":1,\"1274\":5,\"1278\":2,\"1279\":2,\"1280\":1,\"1281\":2,\"1282\":1,\"1283\":1,\"1284\":2,\"1286\":2,\"1288\":1,\"1290\":1,\"1291\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1297\":1,\"1298\":1,\"1299\":2,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":2,\"1304\":2,\"1305\":2,\"1306\":1,\"1307\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1312\":2,\"1313\":1,\"1314\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1330\":1,\"1331\":1,\"1332\":1,\"1333\":1,\"1334\":3,\"1335\":2,\"1336\":2,\"1337\":1,\"1338\":1,\"1339\":1,\"1340\":2,\"1341\":1,\"1342\":1,\"1343\":1,\"1344\":1,\"1345\":1,\"1346\":2,\"1347\":2,\"1348\":1,\"1349\":1,\"1351\":1,\"1352\":1,\"1353\":1,\"1354\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1,\"1359\":1,\"1360\":1,\"1361\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":2,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1389\":2,\"1391\":1,\"1396\":2,\"1397\":1,\"1401\":2,\"1403\":1,\"1408\":1,\"1450\":5,\"1452\":5,\"1454\":5,\"1456\":5,\"1462\":1,\"1466\":2,\"1468\":1,\"1513\":6,\"1514\":3,\"1517\":1,\"1524\":1,\"1526\":3,\"1534\":1,\"1546\":3,\"1548\":6,\"1551\":6,\"1552\":8,\"1553\":3,\"1554\":1,\"1556\":4,\"1568\":1,\"1581\":3,\"1583\":3,\"1587\":3,\"1592\":6,\"1596\":7,\"1597\":9,\"1598\":6,\"1599\":24,\"1600\":7,\"1604\":4,\"1605\":5,\"1606\":5,\"1609\":7,\"1610\":6,\"1611\":3,\"1612\":3,\"1613\":3,\"1614\":2,\"1615\":2,\"1616\":3,\"1618\":1,\"1619\":6,\"1622\":3,\"1625\":4,\"1626\":11,\"1628\":8,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1657\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1672\":1,\"1673\":1,\"1674\":1,\"1675\":1,\"1676\":1,\"1677\":1,\"1679\":1,\"1680\":1,\"1681\":1,\"1682\":2,\"1683\":3,\"1684\":1,\"1685\":1,\"1686\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1694\":1,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1699\":1,\"1700\":1,\"1701\":1,\"1735\":1,\"1742\":1,\"1747\":2,\"1748\":4,\"1750\":5,\"1753\":4,\"1758\":1,\"1794\":2,\"1810\":2,\"1812\":2,\"1814\":1,\"1815\":2,\"1816\":1,\"1871\":3,\"1896\":1,\"1921\":3,\"1936\":5,\"1950\":1,\"1993\":7,\"1994\":4,\"2168\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2185\":1,\"2198\":1,\"2200\":1,\"2219\":3,\"2223\":5,\"2228\":1,\"2231\":1,\"2235\":8,\"2236\":11,\"2239\":9,\"2240\":8,\"2245\":20,\"2407\":1,\"2411\":17,\"2412\":23,\"2423\":24,\"2425\":9,\"2428\":3,\"2429\":9,\"2431\":20,\"2432\":24,\"2433\":5,\"2447\":15}}],[\"lag\",{\"1\":{\"2435\":4}}],[\"lage\",{\"1\":{\"2232\":1}}],[\"lagnuageb\",{\"1\":{\"268\":1,\"277\":1}}],[\"lags\",{\"1\":{\"262\":1}}],[\"last10\",{\"1\":{\"526\":1}}],[\"last\",{\"1\":{\"127\":1,\"164\":1,\"224\":1,\"243\":1,\"252\":1,\"287\":1,\"436\":2,\"449\":2,\"665\":3,\"692\":1,\"696\":1,\"699\":1,\"703\":1,\"738\":1,\"745\":2,\"755\":1,\"785\":1,\"856\":1,\"878\":1,\"879\":1,\"881\":1,\"884\":1,\"921\":1,\"1119\":1,\"1124\":18,\"1125\":18,\"1133\":1,\"1147\":16,\"1209\":1,\"1228\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"1387\":1,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1419\":1,\"1444\":2,\"1448\":2,\"1450\":2,\"1452\":3,\"1454\":2,\"1456\":2,\"1466\":1,\"1468\":1,\"1484\":2,\"1597\":1,\"1604\":2,\"1606\":1,\"1628\":4,\"1708\":3,\"1709\":3,\"1710\":3,\"1768\":3,\"1995\":1,\"1999\":1,\"2000\":2,\"2001\":2,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":2,\"2008\":1,\"2014\":3,\"2015\":3,\"2016\":1,\"2017\":1,\"2018\":3,\"2019\":1,\"2021\":1,\"2127\":1,\"2162\":1,\"2218\":1,\"2361\":1}}],[\"latencies\",{\"1\":{\"2044\":1}}],[\"latency\",{\"0\":{\"133\":1},\"1\":{\"133\":2,\"136\":2,\"137\":1,\"246\":1,\"703\":1,\"755\":1,\"785\":1,\"786\":1,\"800\":1,\"867\":1,\"881\":1,\"884\":1,\"922\":1,\"936\":1,\"937\":1,\"2044\":12}}],[\"latent\",{\"1\":{\"674\":2,\"1224\":1,\"1225\":1,\"1245\":1}}],[\"latest\",{\"1\":{\"218\":2,\"243\":1,\"267\":5,\"276\":2,\"286\":7,\"700\":1,\"709\":1,\"733\":1,\"734\":1,\"774\":1,\"780\":1,\"1526\":1,\"1598\":1,\"1600\":1,\"2191\":1}}],[\"later\",{\"1\":{\"97\":1,\"124\":1,\"211\":1,\"212\":1}}],[\"latter\",{\"1\":{\"47\":1,\"269\":1,\"278\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"lawor\",{\"1\":{\"70\":1}}],[\"law\",{\"0\":{\"1873\":1,\"1877\":1},\"1\":{\"70\":1,\"1645\":2,\"1678\":2,\"1873\":2,\"1877\":2,\"2000\":1,\"2001\":2}}],[\"langevincorrector\",{\"0\":{\"1189\":1},\"1\":{\"1189\":1}}],[\"langevin\",{\"1\":{\"1050\":1,\"1253\":1}}],[\"langs\",{\"1\":{\"267\":4,\"276\":4,\"286\":4,\"515\":1,\"517\":1,\"535\":1,\"1526\":1,\"1552\":2,\"1553\":1,\"1598\":1,\"1599\":2,\"1600\":1,\"1625\":1,\"1626\":2,\"1992\":2,\"1993\":2,\"1994\":1,\"1995\":2,\"2235\":2,\"2236\":2,\"2239\":2,\"2240\":2,\"2245\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":2,\"2432\":2,\"2447\":2}}],[\"lang2utt\",{\"1\":{\"235\":1,\"2353\":3}}],[\"lang\",{\"0\":{\"517\":1,\"596\":1},\"1\":{\"200\":2,\"267\":2,\"276\":2,\"286\":2,\"301\":2,\"377\":4,\"421\":2,\"429\":2,\"436\":4,\"517\":2,\"526\":1,\"527\":2,\"536\":2,\"596\":1,\"603\":2,\"736\":1,\"761\":1,\"762\":1,\"1702\":2,\"2221\":1,\"2336\":1,\"2337\":1,\"2354\":14,\"2360\":1}}],[\"languageespnetmodel\",{\"1\":{\"1938\":1}}],[\"languagemodel\",{\"1\":{\"1938\":1}}],[\"languageb\",{\"1\":{\"268\":1,\"277\":1}}],[\"languagea\",{\"1\":{\"268\":2,\"277\":2}}],[\"languages\",{\"1\":{\"3\":1,\"200\":1,\"240\":1,\"267\":2,\"276\":2,\"286\":2,\"290\":1,\"535\":1,\"1552\":1,\"1599\":1,\"1626\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"2000\":3,\"2001\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2286\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"language\",{\"0\":{\"12\":1,\"180\":1,\"186\":1,\"233\":1,\"237\":1,\"250\":1,\"251\":1,\"436\":1},\"1\":{\"3\":2,\"10\":1,\"12\":1,\"126\":1,\"175\":2,\"180\":1,\"185\":1,\"190\":1,\"200\":8,\"202\":1,\"205\":1,\"228\":1,\"233\":3,\"234\":1,\"235\":8,\"240\":1,\"242\":6,\"243\":1,\"247\":2,\"262\":1,\"265\":2,\"266\":1,\"267\":2,\"268\":1,\"274\":2,\"275\":1,\"276\":2,\"277\":1,\"284\":2,\"285\":1,\"286\":2,\"289\":1,\"436\":1,\"511\":2,\"527\":1,\"596\":1,\"733\":1,\"760\":2,\"1521\":3,\"1526\":2,\"1552\":2,\"1553\":2,\"1585\":2,\"1598\":1,\"1599\":2,\"1625\":2,\"1626\":2,\"1702\":4,\"1822\":1,\"1940\":1,\"1942\":1,\"1945\":1,\"1992\":1,\"1993\":2,\"1995\":1,\"2000\":2,\"2001\":3,\"2049\":3,\"2132\":1,\"2141\":1,\"2142\":1,\"2228\":3,\"2229\":3,\"2235\":2,\"2236\":2,\"2239\":2,\"2240\":2,\"2245\":2,\"2283\":1,\"2284\":1,\"2293\":1,\"2336\":1,\"2337\":1,\"2354\":15,\"2356\":1,\"2408\":3,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":2,\"2432\":2,\"2446\":3,\"2447\":2}}],[\"lan\",{\"1\":{\"67\":1}}],[\"lambda\",{\"1\":{\"44\":1,\"46\":1,\"139\":2,\"144\":1,\"625\":3,\"703\":2,\"755\":2,\"785\":2,\"786\":2,\"800\":2,\"866\":1,\"867\":2,\"880\":1,\"881\":2,\"884\":2,\"921\":1,\"922\":2,\"935\":1,\"936\":2,\"937\":2,\"1308\":1,\"1318\":1,\"1389\":6,\"1396\":6,\"1401\":6,\"1408\":6,\"1466\":6,\"1526\":12,\"1553\":16,\"1598\":10,\"1600\":8,\"1625\":10,\"1770\":1,\"1771\":2,\"2239\":8,\"2240\":8,\"2245\":3,\"2431\":3,\"2432\":3}}],[\"launcher\",{\"1\":{\"62\":1,\"63\":1,\"64\":1,\"377\":1,\"449\":1,\"2340\":2,\"2381\":1,\"2382\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1}}],[\"launch\",{\"0\":{\"374\":1},\"1\":{\"19\":4,\"267\":1,\"276\":1,\"286\":1,\"374\":2,\"703\":1,\"755\":3,\"785\":3}}],[\"largest\",{\"1\":{\"1224\":1,\"1225\":1}}],[\"larger\",{\"1\":{\"102\":3,\"704\":1,\"817\":1,\"1327\":1,\"1330\":1}}],[\"large\",{\"1\":{\"15\":1,\"71\":1,\"91\":1,\"106\":2,\"126\":1,\"128\":1,\"202\":1,\"232\":1,\"258\":1,\"262\":2,\"267\":1,\"276\":20,\"290\":2,\"1000\":1,\"1794\":1,\"2043\":2,\"2066\":1,\"2130\":1,\"2176\":2}}],[\"leyered\",{\"1\":{\"1795\":1}}],[\"leq\",{\"0\":{\"1380\":1}}],[\"legnths\",{\"1\":{\"1637\":1}}],[\"legt\",{\"1\":{\"821\":1}}],[\"legs\",{\"1\":{\"821\":3}}],[\"legacyrelpositionalencoding\",{\"0\":{\"1786\":1},\"1\":{\"1786\":1,\"1808\":1}}],[\"legacyrelpositionmultiheadedattention\",{\"0\":{\"1785\":1},\"1\":{\"1785\":1}}],[\"legacy\",{\"0\":{\"193\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1718\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1728\":1,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1733\":1,\"1734\":1,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1761\":1,\"1762\":1,\"1763\":1,\"1764\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1770\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1775\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1779\":1,\"1780\":1,\"1781\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1791\":1,\"1792\":1,\"1793\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1802\":1,\"1803\":1,\"1804\":1,\"1806\":1,\"1807\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1813\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":1,\"1821\":1,\"1823\":1,\"1824\":1,\"1825\":1,\"1826\":1,\"1828\":1,\"1829\":1,\"1831\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":1,\"1876\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1881\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1889\":1,\"1891\":1,\"1892\":1,\"1893\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1898\":1,\"1899\":1,\"1900\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1907\":1,\"1908\":1,\"1910\":1,\"1911\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1922\":1,\"1923\":1,\"1924\":1,\"1925\":1,\"1926\":1,\"1927\":1,\"1928\":1,\"1929\":1,\"1930\":1,\"1931\":1,\"1932\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"1937\":1,\"2531\":1},\"1\":{\"162\":1,\"692\":1,\"709\":3,\"710\":1,\"711\":1,\"731\":1,\"732\":1,\"766\":1,\"767\":1,\"774\":3,\"775\":1,\"780\":3,\"848\":1,\"849\":1,\"850\":1,\"1599\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1718\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":2,\"1727\":2,\"1728\":1,\"1729\":1,\"1730\":1,\"1731\":2,\"1732\":1,\"1733\":1,\"1734\":1,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":2,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1761\":1,\"1762\":1,\"1763\":1,\"1764\":1,\"1766\":1,\"1768\":1,\"1770\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1775\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1779\":1,\"1780\":1,\"1781\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1791\":1,\"1793\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1802\":1,\"1803\":1,\"1804\":1,\"1805\":1,\"1806\":1,\"1807\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1813\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":1,\"1821\":1,\"1822\":6,\"1823\":1,\"1824\":1,\"1825\":1,\"1826\":1,\"1828\":1,\"1829\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":1,\"1876\":2,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1881\":1,\"1883\":1,\"1885\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1889\":1,\"1891\":1,\"1892\":1,\"1893\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1898\":1,\"1899\":1,\"1900\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1907\":1,\"1908\":1,\"1910\":1,\"1911\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1923\":1,\"1924\":1,\"1925\":1,\"1926\":1,\"1927\":1,\"1928\":1,\"1931\":1,\"1932\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"1937\":1,\"1957\":1,\"1960\":1,\"1961\":1,\"1992\":1,\"1994\":1,\"1995\":1,\"2126\":1,\"2129\":1,\"2191\":3,\"2239\":1,\"2240\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2447\":1}}],[\"leo\",{\"1\":{\"750\":1}}],[\"le\",{\"1\":{\"287\":1}}],[\"less\",{\"1\":{\"262\":1,\"290\":1,\"1350\":1,\"1400\":1,\"1441\":1,\"1469\":1,\"1717\":1,\"2480\":1}}],[\"leverage\",{\"1\":{\"262\":1}}],[\"levels\",{\"1\":{\"1177\":1,\"2353\":1,\"2364\":1}}],[\"level=20\",{\"1\":{\"907\":1}}],[\"level\",{\"1\":{\"108\":1,\"235\":1,\"267\":5,\"276\":3,\"286\":1,\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"377\":1,\"385\":1,\"389\":1,\"396\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":1,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"511\":1,\"627\":4,\"691\":1,\"703\":1,\"705\":1,\"740\":4,\"755\":1,\"785\":1,\"786\":1,\"800\":1,\"804\":1,\"867\":1,\"881\":1,\"884\":1,\"922\":1,\"932\":1,\"934\":1,\"936\":1,\"937\":1,\"1011\":1,\"1031\":1,\"1035\":1,\"1112\":1,\"1113\":1,\"1250\":1,\"1251\":1,\"1269\":1,\"1270\":1,\"1547\":2,\"1629\":2,\"1645\":2,\"1702\":1,\"1760\":4,\"1788\":2,\"1871\":1,\"1873\":1,\"1877\":1,\"1883\":1,\"1921\":1,\"2000\":2,\"2001\":2,\"2130\":1,\"2136\":2,\"2157\":3,\"2183\":4,\"2184\":4,\"2187\":1,\"2190\":4,\"2192\":1,\"2198\":1,\"2203\":1,\"2208\":4,\"2209\":1,\"2344\":1,\"2345\":1,\"2354\":1,\"2439\":1}}],[\"leaving\",{\"1\":{\"1717\":1}}],[\"leaky\",{\"1\":{\"1120\":1,\"1122\":1}}],[\"leakyrelu\",{\"1\":{\"141\":2,\"664\":3,\"768\":1,\"1389\":1,\"1390\":1,\"1392\":1,\"1396\":1,\"1397\":1,\"1401\":2,\"1402\":2,\"1408\":3,\"1409\":3,\"1420\":1,\"1466\":1,\"1467\":1,\"1513\":1,\"1526\":3,\"1548\":1,\"1549\":2,\"1551\":1,\"1553\":2,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":2,\"1596\":1,\"1597\":1,\"1598\":3,\"1599\":1,\"1600\":3,\"1604\":1,\"1605\":1,\"1606\":1,\"1609\":1,\"1614\":1,\"1615\":1,\"1618\":1,\"1619\":1,\"1625\":2}}],[\"leads\",{\"1\":{\"1327\":1,\"1330\":1,\"2281\":1}}],[\"lead\",{\"1\":{\"722\":1}}],[\"least\",{\"1\":{\"213\":1,\"224\":1,\"1350\":1,\"1717\":1,\"2369\":1}}],[\"learn\",{\"1\":{\"1811\":1,\"2245\":1}}],[\"learned\",{\"1\":{\"674\":2,\"699\":1,\"701\":2,\"750\":1,\"768\":1,\"1131\":1,\"1132\":1,\"1400\":1}}],[\"learnablefourierposenc\",{\"0\":{\"1784\":1},\"1\":{\"1784\":1}}],[\"learnable\",{\"1\":{\"43\":2,\"141\":1,\"620\":1,\"689\":1,\"1029\":1,\"1064\":1,\"1235\":1,\"1262\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1290\":1,\"1736\":1,\"1784\":1}}],[\"learning\",{\"0\":{\"6\":1,\"7\":1,\"44\":1,\"50\":1,\"88\":1,\"128\":1,\"144\":1,\"231\":1,\"240\":1,\"257\":1},\"1\":{\"7\":1,\"44\":1,\"49\":1,\"50\":2,\"52\":1,\"106\":1,\"119\":1,\"128\":1,\"144\":1,\"178\":1,\"198\":1,\"200\":4,\"201\":1,\"202\":2,\"205\":1,\"207\":1,\"232\":1,\"243\":1,\"258\":1,\"262\":1,\"273\":1,\"276\":1,\"281\":1,\"290\":1,\"793\":1,\"821\":1,\"1125\":1,\"1176\":1,\"1204\":1,\"1209\":1,\"1228\":1,\"1546\":1,\"1553\":1,\"1577\":1,\"1611\":1,\"1612\":1,\"1616\":1,\"1622\":1,\"1625\":1,\"1626\":1,\"2006\":1,\"2014\":4,\"2015\":3,\"2016\":1,\"2017\":1,\"2018\":8,\"2019\":1,\"2021\":1,\"2131\":1,\"2184\":1,\"2198\":1,\"2239\":1,\"2355\":4,\"2432\":1}}],[\"left=0\",{\"1\":{\"1635\":1}}],[\"left\",{\"1\":{\"137\":1,\"147\":4,\"148\":1,\"321\":2,\"616\":1,\"617\":4,\"618\":4,\"620\":4,\"624\":4,\"626\":4,\"636\":4,\"644\":8,\"645\":4,\"661\":3,\"669\":3,\"704\":1,\"1080\":1,\"1868\":1,\"1870\":1,\"1910\":1,\"1919\":1,\"2249\":1,\"2253\":1}}],[\"letters\",{\"1\":{\"1309\":1,\"1311\":1}}],[\"let\",{\"1\":{\"107\":1,\"139\":1,\"269\":1,\"278\":1,\"1308\":1,\"2000\":1}}],[\"lets\",{\"1\":{\"18\":1}}],[\"len=none\",{\"1\":{\"1977\":1}}],[\"len=5000\",{\"1\":{\"1784\":1,\"1786\":1,\"1808\":1,\"1818\":1,\"1820\":1,\"1837\":1}}],[\"len=16\",{\"1\":{\"1164\":2}}],[\"lens\",{\"1\":{\"625\":1,\"667\":2,\"676\":1,\"692\":2,\"699\":1,\"706\":4,\"760\":3,\"770\":1,\"775\":2,\"777\":8,\"786\":4,\"790\":2,\"794\":5,\"796\":1,\"800\":4,\"820\":2,\"850\":3,\"866\":2,\"867\":4,\"921\":4,\"935\":4,\"1156\":8,\"2127\":1}}],[\"lenghts\",{\"1\":{\"626\":1,\"1314\":1}}],[\"lenght\",{\"1\":{\"267\":4}}],[\"lengthbatchsampler\",{\"0\":{\"2003\":1},\"1\":{\"2000\":1,\"2001\":1,\"2003\":1}}],[\"lengthbonus\",{\"0\":{\"1787\":1},\"1\":{\"1787\":1,\"1822\":1}}],[\"lengthofquery\",{\"1\":{\"1817\":1}}],[\"lengthregulator\",{\"0\":{\"1529\":1,\"1788\":1},\"1\":{\"1529\":1,\"1788\":1}}],[\"length=100\",{\"1\":{\"2134\":1}}],[\"length=160\",{\"1\":{\"1128\":1}}],[\"length=2\",{\"1\":{\"1996\":1}}],[\"length=true\",{\"1\":{\"1833\":1}}],[\"length=false\",{\"1\":{\"1782\":1}}],[\"length=none\",{\"1\":{\"1141\":1,\"1574\":1,\"1776\":1,\"1791\":1,\"1832\":1,\"1835\":1,\"1899\":1,\"1900\":1,\"1923\":1,\"1924\":1}}],[\"length=512\",{\"1\":{\"1066\":1,\"1246\":1}}],[\"lengthadaptorpostencoder\",{\"0\":{\"765\":1},\"1\":{\"765\":1}}],[\"length2\",{\"1\":{\"98\":2,\"99\":2}}],[\"lengths=\",{\"1\":{\"1534\":1}}],[\"lengths=none\",{\"1\":{\"819\":1,\"833\":1,\"1974\":1,\"1977\":1}}],[\"lengths\",{\"1\":{\"82\":3,\"134\":3,\"135\":1,\"175\":1,\"262\":1,\"543\":1,\"625\":19,\"626\":2,\"634\":1,\"667\":3,\"670\":5,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"692\":3,\"703\":4,\"706\":4,\"720\":1,\"730\":2,\"736\":4,\"737\":6,\"738\":1,\"752\":1,\"755\":10,\"759\":4,\"761\":1,\"762\":1,\"765\":1,\"768\":1,\"772\":1,\"777\":4,\"785\":10,\"786\":1,\"800\":1,\"815\":1,\"831\":4,\"846\":1,\"864\":1,\"873\":2,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"921\":1,\"922\":6,\"935\":1,\"936\":6,\"937\":6,\"954\":4,\"955\":1,\"958\":9,\"962\":2,\"974\":10,\"977\":1,\"979\":1,\"980\":1,\"994\":1,\"1053\":1,\"1062\":1,\"1072\":1,\"1074\":1,\"1107\":1,\"1112\":1,\"1113\":1,\"1117\":1,\"1118\":1,\"1125\":1,\"1130\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1155\":5,\"1156\":14,\"1157\":6,\"1158\":7,\"1162\":1,\"1217\":2,\"1222\":1,\"1223\":1,\"1232\":1,\"1250\":2,\"1251\":2,\"1252\":1,\"1261\":1,\"1267\":1,\"1268\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":1,\"1280\":1,\"1283\":1,\"1334\":1,\"1396\":2,\"1397\":4,\"1408\":2,\"1409\":2,\"1422\":4,\"1519\":2,\"1521\":36,\"1522\":1,\"1526\":11,\"1534\":2,\"1536\":2,\"1539\":1,\"1546\":2,\"1552\":18,\"1553\":13,\"1577\":4,\"1585\":20,\"1586\":1,\"1589\":2,\"1598\":6,\"1599\":14,\"1600\":6,\"1611\":2,\"1612\":1,\"1613\":1,\"1622\":2,\"1625\":6,\"1626\":8,\"1627\":1,\"1629\":4,\"1632\":2,\"1637\":3,\"1640\":8,\"1641\":8,\"1652\":1,\"1659\":1,\"1664\":1,\"1665\":1,\"1667\":4,\"1670\":2,\"1691\":3,\"1697\":2,\"1702\":5,\"1720\":1,\"1721\":1,\"1725\":1,\"1730\":1,\"1750\":2,\"1751\":1,\"1758\":2,\"1764\":2,\"1770\":2,\"1771\":2,\"1806\":1,\"1814\":1,\"1816\":1,\"1839\":1,\"1862\":1,\"1901\":3,\"1902\":7,\"1903\":3,\"1904\":7,\"1905\":3,\"1906\":2,\"1919\":4,\"1940\":4,\"1941\":3,\"1942\":4,\"1943\":3,\"1957\":1,\"1959\":8,\"1960\":4,\"1961\":4,\"1965\":4,\"1971\":2,\"1972\":1,\"1974\":1,\"1975\":8,\"1976\":2,\"1977\":3,\"1978\":1,\"1980\":1,\"1982\":1,\"1991\":1,\"1992\":6,\"1993\":6,\"1995\":6,\"1996\":17,\"1997\":18,\"2002\":1,\"2007\":2,\"2126\":1,\"2127\":10,\"2130\":2,\"2133\":2,\"2136\":8,\"2155\":1,\"2183\":2,\"2184\":4,\"2190\":2,\"2208\":2,\"2215\":3,\"2216\":6,\"2217\":1,\"2219\":6,\"2220\":1,\"2221\":11,\"2222\":2,\"2223\":1,\"2226\":2,\"2227\":1,\"2228\":48,\"2229\":43,\"2231\":2,\"2232\":8,\"2235\":21,\"2236\":20,\"2237\":1,\"2238\":9,\"2239\":24,\"2240\":20,\"2241\":2,\"2245\":20,\"2325\":1,\"2327\":1,\"2401\":1,\"2403\":2,\"2405\":3,\"2408\":20,\"2409\":3,\"2411\":9,\"2412\":15,\"2413\":2,\"2414\":1,\"2416\":1,\"2418\":1,\"2423\":15,\"2424\":2,\"2431\":6,\"2432\":6,\"2434\":3,\"2443\":1,\"2445\":2,\"2446\":24,\"2447\":14,\"2448\":2,\"2449\":1,\"2462\":7}}],[\"length\",{\"0\":{\"765\":1,\"1529\":1,\"1590\":1,\"1787\":1,\"1788\":1,\"1905\":1,\"2003\":1,\"2149\":2,\"2152\":1,\"2153\":1,\"2161\":1,\"2166\":1},\"1\":{\"45\":5,\"48\":1,\"82\":5,\"95\":4,\"96\":2,\"97\":9,\"98\":13,\"99\":5,\"134\":3,\"136\":1,\"141\":1,\"142\":2,\"145\":3,\"147\":1,\"148\":1,\"175\":2,\"205\":1,\"242\":1,\"243\":4,\"262\":1,\"267\":17,\"276\":1,\"286\":6,\"295\":2,\"301\":2,\"315\":8,\"377\":2,\"396\":2,\"406\":2,\"415\":2,\"421\":2,\"442\":2,\"449\":4,\"463\":4,\"469\":8,\"515\":2,\"516\":3,\"537\":7,\"543\":1,\"548\":2,\"551\":2,\"558\":2,\"606\":4,\"616\":5,\"633\":3,\"634\":3,\"637\":6,\"639\":7,\"645\":1,\"646\":3,\"647\":3,\"661\":1,\"674\":6,\"691\":4,\"696\":7,\"697\":7,\"700\":2,\"702\":4,\"703\":6,\"709\":2,\"710\":3,\"711\":3,\"712\":2,\"716\":2,\"717\":2,\"720\":2,\"724\":2,\"725\":2,\"726\":2,\"727\":2,\"728\":2,\"731\":1,\"732\":1,\"733\":2,\"734\":2,\"736\":3,\"737\":4,\"744\":2,\"745\":1,\"746\":1,\"747\":3,\"748\":2,\"755\":5,\"759\":1,\"762\":1,\"765\":3,\"766\":1,\"767\":1,\"771\":1,\"774\":2,\"777\":7,\"778\":3,\"779\":1,\"780\":2,\"785\":5,\"786\":1,\"787\":2,\"792\":1,\"800\":1,\"817\":1,\"818\":1,\"819\":1,\"820\":1,\"821\":1,\"823\":1,\"824\":3,\"828\":5,\"829\":2,\"830\":5,\"831\":10,\"846\":8,\"849\":1,\"859\":2,\"867\":1,\"878\":4,\"879\":4,\"881\":4,\"882\":4,\"883\":4,\"884\":4,\"919\":2,\"921\":1,\"922\":1,\"935\":1,\"936\":1,\"937\":1,\"941\":1,\"954\":2,\"958\":3,\"962\":2,\"971\":1,\"972\":1,\"974\":6,\"975\":1,\"976\":2,\"984\":1,\"994\":3,\"1029\":1,\"1066\":1,\"1075\":1,\"1119\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1156\":10,\"1162\":2,\"1179\":1,\"1246\":2,\"1250\":3,\"1251\":3,\"1259\":1,\"1296\":1,\"1314\":1,\"1350\":5,\"1356\":8,\"1368\":2,\"1385\":4,\"1392\":5,\"1397\":1,\"1401\":2,\"1402\":2,\"1413\":4,\"1422\":1,\"1466\":2,\"1467\":2,\"1484\":1,\"1511\":1,\"1514\":2,\"1519\":1,\"1521\":11,\"1524\":1,\"1525\":6,\"1526\":4,\"1529\":7,\"1535\":3,\"1536\":1,\"1545\":3,\"1546\":1,\"1549\":1,\"1551\":1,\"1552\":6,\"1553\":5,\"1559\":4,\"1574\":1,\"1577\":2,\"1585\":10,\"1586\":1,\"1590\":1,\"1598\":5,\"1599\":4,\"1600\":5,\"1607\":6,\"1611\":1,\"1612\":1,\"1613\":1,\"1622\":1,\"1625\":6,\"1626\":5,\"1629\":2,\"1632\":1,\"1637\":2,\"1640\":7,\"1641\":6,\"1643\":2,\"1644\":2,\"1646\":2,\"1647\":3,\"1660\":2,\"1664\":1,\"1665\":1,\"1667\":2,\"1669\":2,\"1680\":6,\"1691\":2,\"1692\":6,\"1698\":6,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1719\":6,\"1720\":6,\"1721\":7,\"1722\":3,\"1725\":12,\"1739\":1,\"1740\":1,\"1741\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1750\":6,\"1756\":1,\"1757\":1,\"1768\":1,\"1782\":2,\"1784\":1,\"1786\":1,\"1787\":2,\"1788\":5,\"1789\":1,\"1790\":1,\"1801\":1,\"1806\":8,\"1807\":3,\"1808\":1,\"1818\":1,\"1820\":1,\"1822\":1,\"1837\":2,\"1862\":4,\"1868\":1,\"1870\":1,\"1901\":2,\"1903\":2,\"1905\":2,\"1906\":1,\"1907\":1,\"1919\":1,\"1940\":1,\"1941\":2,\"1942\":2,\"1943\":2,\"1958\":1,\"1959\":4,\"1960\":1,\"1961\":1,\"1974\":3,\"1975\":1,\"1977\":4,\"1978\":2,\"1980\":2,\"1982\":2,\"1987\":1,\"1993\":2,\"1996\":4,\"1997\":8,\"2003\":1,\"2005\":1,\"2006\":1,\"2007\":2,\"2124\":1,\"2127\":4,\"2128\":1,\"2129\":1,\"2130\":11,\"2131\":3,\"2133\":9,\"2134\":3,\"2136\":6,\"2137\":2,\"2143\":3,\"2145\":4,\"2146\":5,\"2147\":4,\"2149\":3,\"2152\":2,\"2153\":2,\"2155\":3,\"2161\":1,\"2166\":2,\"2183\":1,\"2184\":1,\"2190\":1,\"2208\":1,\"2220\":6,\"2221\":5,\"2224\":6,\"2228\":17,\"2229\":16,\"2232\":2,\"2236\":1,\"2238\":2,\"2239\":1,\"2240\":2,\"2245\":3,\"2360\":1,\"2361\":1,\"2363\":1,\"2378\":1,\"2379\":1,\"2404\":1,\"2408\":10,\"2409\":2,\"2414\":2,\"2416\":2,\"2418\":2,\"2428\":3,\"2431\":2,\"2432\":2,\"2436\":1,\"2438\":1,\"2440\":1,\"2446\":12,\"2447\":1,\"2482\":3,\"2490\":3}}],[\"len\",{\"1\":{\"99\":1,\"141\":1,\"301\":2,\"421\":2,\"514\":4,\"626\":5,\"645\":2,\"661\":2,\"667\":2,\"670\":1,\"709\":1,\"733\":1,\"734\":1,\"750\":2,\"780\":1,\"787\":3,\"795\":1,\"974\":1,\"1063\":1,\"1133\":1,\"1134\":1,\"1137\":1,\"1157\":1,\"1162\":4,\"1208\":1,\"1255\":1,\"1257\":1,\"1269\":2,\"1270\":2,\"1271\":1,\"1316\":1,\"1334\":2,\"1526\":2,\"1548\":1,\"1552\":2,\"1553\":2,\"1559\":2,\"1566\":2,\"1625\":2,\"1626\":2,\"1665\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1725\":1,\"1731\":2,\"1745\":1,\"1748\":2,\"1768\":2,\"1784\":1,\"1786\":1,\"1801\":2,\"1805\":1,\"1806\":1,\"1808\":1,\"1818\":1,\"1820\":1,\"1837\":1,\"1868\":3,\"1870\":3,\"1944\":2,\"1947\":2,\"2130\":2,\"2137\":3,\"2155\":3,\"2183\":1,\"2190\":1,\"2191\":1,\"2208\":1,\"2220\":1,\"2227\":1,\"2244\":2,\"2355\":2}}],[\"lee\",{\"1\":{\"14\":1,\"256\":1,\"1269\":2,\"1270\":2,\"1271\":2}}],[\"leibny\",{\"1\":{\"14\":1}}],[\"lucidrains\",{\"1\":{\"1320\":1}}],[\"lug\",{\"1\":{\"1308\":1}}],[\"luo\",{\"1\":{\"1061\":2,\"1062\":2}}],[\"lun\",{\"1\":{\"790\":1}}],[\"lu\",{\"1\":{\"11\":1,\"1210\":1,\"1264\":1,\"1334\":1}}],[\"lu2022espnetsep\",{\"1\":{\"11\":1}}],[\"lightconv2d\",{\"0\":{\"1790\":1},\"1\":{\"1790\":1}}],[\"lightconv\",{\"0\":{\"1789\":1},\"1\":{\"1789\":1}}],[\"lightweight\",{\"1\":{\"768\":4,\"1789\":3,\"1790\":3,\"2130\":3,\"2133\":2,\"2136\":2}}],[\"lightweightsincconvs\",{\"0\":{\"768\":1},\"1\":{\"768\":2}}],[\"lightweightconvolution2d\",{\"0\":{\"1790\":1},\"1\":{\"1790\":1}}],[\"lightweightconvolution2dtransformerdecoder\",{\"0\":{\"766\":1},\"1\":{\"766\":1}}],[\"lightweightconvolution\",{\"0\":{\"1789\":1},\"1\":{\"1789\":1}}],[\"lightweightconvolutiontransformerdecoder\",{\"0\":{\"767\":1},\"1\":{\"767\":1}}],[\"lightningmodule\",{\"1\":{\"2355\":2}}],[\"lightning\",{\"0\":{\"960\":1,\"2333\":1,\"2355\":1},\"1\":{\"79\":1,\"929\":1,\"960\":1,\"2151\":2,\"2310\":2,\"2333\":1,\"2355\":8}}],[\"litespnetmodel\",{\"0\":{\"2355\":1},\"1\":{\"2355\":1}}],[\"literal\",{\"1\":{\"702\":1,\"2054\":1}}],[\"litgpt\",{\"0\":{\"2036\":1,\"2037\":1,\"2038\":1,\"2041\":1,\"2042\":1,\"2046\":1,\"2047\":1,\"2048\":1,\"2050\":1,\"2051\":1,\"2052\":1,\"2053\":1,\"2058\":1,\"2059\":1,\"2060\":1,\"2064\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2070\":1,\"2071\":1,\"2072\":1,\"2073\":1,\"2074\":1,\"2076\":1,\"2077\":1,\"2078\":1,\"2079\":1,\"2080\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2085\":1,\"2086\":1,\"2087\":1,\"2089\":1,\"2090\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2104\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2117\":1,\"2118\":1,\"2119\":1,\"2120\":1,\"2121\":1,\"2123\":1}}],[\"little\",{\"1\":{\"70\":1}}],[\"liang\",{\"1\":{\"269\":1,\"278\":1}}],[\"lives\",{\"1\":{\"242\":1}}],[\"livescu\",{\"1\":{\"6\":1}}],[\"live\",{\"1\":{\"242\":1}}],[\"limit=0\",{\"1\":{\"1720\":2}}],[\"limited\",{\"1\":{\"1674\":1}}],[\"limitation\",{\"0\":{\"1674\":1},\"1\":{\"1674\":2}}],[\"limitations\",{\"0\":{\"137\":1},\"1\":{\"133\":1}}],[\"limit\",{\"1\":{\"286\":1,\"315\":4,\"469\":4,\"653\":3,\"663\":1,\"1842\":3,\"2146\":1}}],[\"limits\",{\"1\":{\"256\":1,\"2192\":1,\"2354\":1}}],[\"lim\",{\"0\":{\"2482\":1,\"2490\":2,\"2495\":1},\"1\":{\"218\":1,\"267\":1,\"276\":1,\"286\":2,\"516\":2,\"558\":1,\"2482\":3,\"2490\":3,\"2495\":1}}],[\"lidpreprocessor\",{\"0\":{\"2353\":1},\"1\":{\"2353\":1}}],[\"lidtrainer\",{\"0\":{\"2354\":1},\"1\":{\"2259\":1,\"2354\":1}}],[\"lidtask\",{\"0\":{\"2259\":1},\"1\":{\"2259\":1}}],[\"lids\",{\"1\":{\"1521\":6,\"1526\":4,\"1552\":5,\"1553\":4,\"1585\":4,\"1598\":2,\"1599\":5,\"1625\":4,\"1626\":5,\"1702\":1,\"1975\":1,\"1976\":1,\"1992\":3,\"1993\":5,\"1995\":3,\"2176\":1,\"2228\":6,\"2229\":6,\"2235\":5,\"2236\":5,\"2239\":5,\"2240\":5,\"2245\":5,\"2354\":1,\"2408\":6,\"2411\":5,\"2412\":5,\"2423\":5,\"2431\":5,\"2432\":5,\"2446\":6,\"2447\":5}}],[\"lid1\",{\"1\":{\"201\":1,\"233\":1,\"235\":1,\"236\":1}}],[\"lid\",{\"0\":{\"377\":1,\"1702\":1,\"2259\":1,\"2354\":1,\"2530\":1},\"1\":{\"200\":1,\"201\":1,\"234\":1,\"235\":4,\"266\":1,\"267\":6,\"275\":1,\"276\":6,\"285\":1,\"286\":6,\"377\":5,\"402\":1,\"403\":2,\"1702\":6,\"1726\":1,\"2259\":1,\"2353\":1,\"2354\":6}}],[\"lib\",{\"1\":{\"1701\":1,\"2311\":1}}],[\"libatlas\",{\"1\":{\"161\":1}}],[\"librosa\",{\"1\":{\"1662\":1}}],[\"libri\",{\"1\":{\"2134\":1}}],[\"libritts\",{\"1\":{\"218\":3,\"219\":1,\"220\":1,\"286\":1,\"289\":1,\"536\":6}}],[\"librispeech\",{\"1\":{\"108\":1,\"136\":6,\"184\":1,\"195\":1,\"204\":1,\"206\":4,\"253\":1,\"527\":2,\"2043\":2,\"2134\":1}}],[\"libraries\",{\"1\":{\"28\":1,\"163\":2}}],[\"library=<module\",{\"1\":{\"2311\":1}}],[\"library\",{\"1\":{\"26\":1,\"161\":1,\"193\":2,\"287\":13,\"750\":1,\"760\":1,\"2040\":1,\"2045\":1,\"2049\":2,\"2065\":1,\"2311\":4}}],[\"libsndfile\",{\"1\":{\"71\":1}}],[\"lin\",{\"1\":{\"632\":1,\"821\":2}}],[\"linguistic\",{\"1\":{\"242\":1,\"481\":4,\"2275\":2,\"2285\":2,\"2292\":2,\"2293\":2,\"2336\":1,\"2337\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1}}],[\"links\",{\"1\":{\"161\":1,\"243\":1,\"261\":1}}],[\"link\",{\"1\":{\"125\":1,\"160\":1,\"224\":1,\"261\":6,\"746\":1,\"780\":1,\"2239\":1}}],[\"linting\",{\"1\":{\"162\":1}}],[\"linters\",{\"1\":{\"162\":1}}],[\"lint\",{\"1\":{\"79\":1}}],[\"linux\",{\"0\":{\"160\":1},\"1\":{\"67\":4,\"160\":2}}],[\"linearspectrogram\",{\"0\":{\"1978\":1,\"2414\":1},\"1\":{\"1978\":1,\"2414\":1}}],[\"linearly\",{\"1\":{\"1319\":1,\"2018\":1}}],[\"linearprojection\",{\"0\":{\"772\":1},\"1\":{\"772\":1}}],[\"linearencoder\",{\"0\":{\"771\":1},\"1\":{\"771\":1}}],[\"lineardecoder\",{\"0\":{\"770\":1,\"955\":1,\"959\":1,\"977\":1},\"1\":{\"770\":1,\"955\":1,\"959\":2,\"977\":1}}],[\"linearactivation\",{\"0\":{\"769\":1},\"1\":{\"769\":1}}],[\"linear\",{\"0\":{\"754\":1,\"770\":1,\"771\":1,\"772\":1,\"955\":1,\"959\":1,\"977\":1,\"1978\":1,\"2017\":1,\"2414\":1},\"1\":{\"68\":1,\"70\":1,\"71\":2,\"128\":1,\"141\":9,\"142\":2,\"143\":1,\"243\":5,\"286\":6,\"617\":3,\"624\":3,\"634\":2,\"635\":1,\"642\":2,\"643\":2,\"649\":1,\"664\":3,\"692\":4,\"700\":2,\"709\":8,\"710\":7,\"711\":7,\"713\":2,\"715\":1,\"718\":1,\"731\":1,\"732\":1,\"733\":4,\"734\":4,\"748\":2,\"752\":1,\"754\":2,\"766\":1,\"767\":1,\"769\":1,\"770\":1,\"771\":3,\"772\":2,\"774\":8,\"775\":1,\"780\":14,\"781\":3,\"783\":2,\"848\":1,\"849\":7,\"850\":1,\"856\":2,\"911\":2,\"927\":1,\"955\":2,\"959\":1,\"977\":2,\"978\":2,\"1029\":1,\"1070\":1,\"1071\":1,\"1084\":2,\"1107\":9,\"1144\":1,\"1180\":1,\"1181\":1,\"1235\":1,\"1267\":2,\"1268\":2,\"1273\":3,\"1274\":2,\"1278\":8,\"1279\":1,\"1281\":1,\"1282\":1,\"1321\":1,\"1322\":1,\"1357\":1,\"1456\":1,\"1519\":3,\"1522\":1,\"1535\":3,\"1536\":3,\"1546\":3,\"1547\":1,\"1552\":1,\"1607\":1,\"1622\":3,\"1626\":1,\"1636\":1,\"1678\":2,\"1735\":3,\"1737\":2,\"1749\":1,\"1751\":3,\"1753\":2,\"1754\":1,\"1755\":1,\"1759\":3,\"1794\":2,\"1863\":1,\"1963\":1,\"1978\":2,\"1992\":5,\"1995\":5,\"2014\":1,\"2017\":2,\"2018\":1,\"2126\":3,\"2129\":8,\"2191\":5,\"2414\":2,\"2458\":2,\"2490\":2,\"2495\":2}}],[\"lines\",{\"1\":{\"41\":1,\"267\":1,\"290\":1}}],[\"line\",{\"0\":{\"117\":1},\"1\":{\"3\":3,\"22\":1,\"23\":1,\"25\":1,\"40\":1,\"79\":1,\"80\":1,\"81\":1,\"84\":2,\"85\":1,\"117\":1,\"167\":1,\"168\":2,\"197\":1,\"224\":1,\"290\":3,\"1008\":2,\"1064\":2,\"1078\":2,\"1153\":2,\"1202\":2,\"1262\":2,\"1290\":2,\"1656\":2,\"1660\":2,\"1662\":2,\"1664\":2,\"1665\":2,\"1669\":2,\"1670\":2,\"1671\":2,\"1678\":1,\"1887\":1,\"2232\":2,\"2238\":2,\"2274\":1,\"2275\":1,\"2279\":1,\"2284\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2292\":1,\"2480\":1}}],[\"likelihood\",{\"1\":{\"755\":1,\"777\":2,\"785\":1,\"878\":2,\"879\":2,\"881\":3,\"882\":2,\"883\":2,\"884\":3,\"1156\":2,\"1224\":1,\"1225\":1,\"1245\":1,\"1552\":1,\"1616\":1,\"1626\":1,\"1940\":1,\"1941\":1,\"1942\":1,\"1943\":1}}],[\"likely\",{\"1\":{\"174\":1,\"262\":1}}],[\"like\",{\"0\":{\"1348\":1},\"1\":{\"48\":1,\"53\":1,\"71\":1,\"76\":1,\"79\":1,\"91\":1,\"102\":1,\"127\":1,\"137\":1,\"162\":1,\"166\":1,\"217\":1,\"242\":1,\"246\":1,\"248\":1,\"259\":2,\"267\":2,\"269\":2,\"276\":2,\"278\":2,\"286\":1,\"287\":1,\"290\":1,\"621\":3,\"665\":1,\"820\":1,\"828\":1,\"830\":2,\"985\":1,\"992\":1,\"994\":2,\"997\":1,\"1000\":1,\"1002\":1,\"1004\":1,\"1008\":2,\"1012\":1,\"1016\":1,\"1145\":1,\"1168\":1,\"1348\":1,\"1668\":1,\"2039\":2,\"2049\":1,\"2130\":2,\"2139\":1,\"2144\":1,\"2304\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2355\":3,\"2369\":1}}],[\"listorlistoflist\",{\"1\":{\"1878\":1}}],[\"listortuple\",{\"1\":{\"1061\":1,\"1062\":1}}],[\"listofint\",{\"1\":{\"1392\":1}}],[\"listconfig\",{\"1\":{\"943\":1}}],[\"liststotensor\",{\"0\":{\"2233\":1},\"1\":{\"2233\":1}}],[\"lists\",{\"1\":{\"942\":1,\"2132\":1,\"2355\":1}}],[\"listed\",{\"1\":{\"197\":1,\"276\":1}}],[\"list=<class\",{\"1\":{\"1726\":1,\"1727\":1}}],[\"list=none\",{\"1\":{\"1515\":1}}],[\"list=\",{\"1\":{\"32\":1}}],[\"list\",{\"0\":{\"869\":1,\"914\":1,\"943\":1,\"1908\":1,\"2155\":1},\"1\":{\"19\":1,\"32\":5,\"44\":2,\"50\":1,\"51\":1,\"67\":4,\"82\":3,\"141\":2,\"199\":1,\"200\":2,\"204\":1,\"205\":2,\"210\":1,\"211\":4,\"212\":1,\"213\":2,\"223\":6,\"228\":1,\"240\":1,\"242\":3,\"243\":7,\"259\":4,\"265\":1,\"266\":3,\"267\":4,\"268\":2,\"274\":1,\"275\":3,\"276\":4,\"277\":2,\"284\":1,\"285\":3,\"286\":3,\"290\":1,\"505\":2,\"614\":21,\"616\":13,\"625\":4,\"626\":1,\"627\":6,\"628\":2,\"630\":2,\"631\":4,\"634\":19,\"636\":3,\"641\":3,\"643\":13,\"649\":2,\"651\":3,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"671\":1,\"692\":14,\"696\":18,\"697\":14,\"700\":3,\"702\":1,\"709\":2,\"734\":1,\"736\":2,\"737\":4,\"740\":6,\"743\":5,\"750\":3,\"752\":1,\"760\":5,\"763\":6,\"774\":5,\"776\":2,\"777\":2,\"780\":3,\"786\":1,\"790\":8,\"795\":4,\"797\":2,\"820\":5,\"832\":2,\"846\":3,\"847\":6,\"848\":1,\"850\":8,\"854\":1,\"866\":1,\"869\":1,\"914\":1,\"921\":1,\"922\":2,\"943\":5,\"958\":2,\"978\":2,\"992\":1,\"994\":2,\"1008\":1,\"1014\":2,\"1016\":1,\"1019\":1,\"1021\":1,\"1024\":2,\"1026\":1,\"1028\":1,\"1031\":1,\"1035\":1,\"1042\":2,\"1053\":1,\"1061\":1,\"1062\":4,\"1086\":4,\"1107\":2,\"1112\":1,\"1113\":1,\"1117\":4,\"1118\":6,\"1124\":2,\"1125\":4,\"1126\":4,\"1127\":4,\"1130\":2,\"1131\":2,\"1132\":3,\"1133\":3,\"1136\":2,\"1141\":2,\"1155\":4,\"1156\":3,\"1157\":15,\"1158\":1,\"1162\":2,\"1167\":2,\"1172\":1,\"1204\":6,\"1207\":4,\"1209\":5,\"1210\":4,\"1217\":2,\"1228\":2,\"1232\":2,\"1250\":1,\"1251\":1,\"1252\":2,\"1261\":2,\"1267\":2,\"1268\":2,\"1269\":3,\"1270\":3,\"1271\":3,\"1278\":2,\"1280\":3,\"1283\":4,\"1334\":3,\"1336\":1,\"1354\":5,\"1356\":1,\"1368\":2,\"1385\":8,\"1387\":1,\"1390\":6,\"1391\":2,\"1392\":1,\"1397\":8,\"1402\":7,\"1403\":3,\"1409\":9,\"1410\":6,\"1413\":2,\"1420\":7,\"1422\":3,\"1448\":1,\"1450\":1,\"1452\":2,\"1454\":1,\"1456\":1,\"1458\":6,\"1460\":6,\"1467\":6,\"1468\":2,\"1509\":3,\"1511\":3,\"1513\":19,\"1514\":8,\"1515\":12,\"1516\":8,\"1534\":10,\"1548\":11,\"1549\":3,\"1551\":15,\"1552\":27,\"1582\":3,\"1584\":18,\"1587\":18,\"1591\":9,\"1592\":15,\"1593\":6,\"1594\":6,\"1595\":9,\"1596\":7,\"1597\":9,\"1598\":1,\"1599\":18,\"1600\":1,\"1604\":9,\"1605\":3,\"1606\":12,\"1614\":3,\"1618\":13,\"1619\":6,\"1624\":3,\"1626\":15,\"1640\":2,\"1641\":2,\"1643\":1,\"1644\":2,\"1646\":1,\"1647\":2,\"1655\":7,\"1675\":1,\"1682\":1,\"1683\":3,\"1704\":1,\"1705\":7,\"1706\":7,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":3,\"1714\":3,\"1715\":4,\"1716\":4,\"1719\":12,\"1720\":4,\"1721\":10,\"1722\":2,\"1723\":2,\"1724\":5,\"1725\":20,\"1726\":2,\"1727\":2,\"1746\":2,\"1749\":16,\"1751\":2,\"1760\":3,\"1762\":5,\"1766\":1,\"1768\":1,\"1775\":6,\"1787\":5,\"1798\":5,\"1799\":5,\"1800\":5,\"1801\":1,\"1802\":1,\"1806\":6,\"1813\":1,\"1815\":6,\"1816\":1,\"1843\":16,\"1861\":1,\"1862\":7,\"1863\":1,\"1870\":2,\"1871\":2,\"1878\":2,\"1892\":1,\"1897\":2,\"1908\":4,\"1909\":1,\"1910\":2,\"1913\":1,\"1914\":1,\"1915\":2,\"1920\":3,\"1921\":2,\"1927\":3,\"1936\":5,\"1942\":3,\"1944\":5,\"1945\":3,\"1947\":5,\"1950\":2,\"1951\":2,\"1959\":4,\"1965\":2,\"1975\":6,\"1992\":8,\"1996\":3,\"1997\":2,\"2000\":3,\"2001\":3,\"2002\":1,\"2003\":1,\"2004\":1,\"2007\":1,\"2008\":1,\"2017\":4,\"2039\":7,\"2049\":3,\"2065\":2,\"2095\":1,\"2101\":3,\"2115\":1,\"2116\":1,\"2124\":1,\"2127\":4,\"2130\":13,\"2131\":1,\"2132\":7,\"2133\":2,\"2136\":9,\"2137\":4,\"2139\":2,\"2141\":4,\"2144\":2,\"2145\":6,\"2146\":6,\"2147\":6,\"2155\":5,\"2156\":2,\"2162\":7,\"2191\":1,\"2209\":3,\"2215\":5,\"2216\":3,\"2217\":1,\"2219\":7,\"2221\":4,\"2245\":3,\"2246\":3,\"2247\":3,\"2248\":3,\"2249\":3,\"2250\":3,\"2251\":3,\"2252\":3,\"2253\":3,\"2254\":4,\"2255\":4,\"2256\":4,\"2257\":3,\"2258\":4,\"2259\":3,\"2260\":3,\"2261\":3,\"2262\":2,\"2263\":3,\"2264\":3,\"2265\":2,\"2266\":3,\"2267\":3,\"2268\":3,\"2269\":3,\"2270\":3,\"2271\":3,\"2272\":3,\"2273\":4,\"2274\":1,\"2275\":1,\"2278\":2,\"2279\":1,\"2283\":2,\"2284\":1,\"2285\":2,\"2287\":2,\"2288\":1,\"2291\":3,\"2292\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":3,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2309\":1,\"2329\":2,\"2336\":3,\"2337\":4,\"2338\":2,\"2339\":2,\"2341\":1,\"2342\":1,\"2344\":1,\"2345\":2,\"2346\":3,\"2347\":1,\"2348\":1,\"2351\":2,\"2353\":3,\"2354\":2,\"2355\":2,\"2356\":10,\"2359\":3,\"2360\":1,\"2361\":1,\"2362\":4,\"2363\":1,\"2364\":3,\"2366\":1,\"2368\":3,\"2369\":2,\"2370\":2,\"2371\":1,\"2372\":1,\"2376\":2,\"2411\":3,\"2412\":3,\"2423\":3,\"2425\":3,\"2429\":3,\"2431\":3,\"2432\":4,\"2435\":1,\"2436\":1,\"2438\":1,\"2440\":1,\"2462\":2}}],[\"li2020espnetse\",{\"1\":{\"11\":1}}],[\"li\",{\"1\":{\"6\":1,\"11\":2,\"13\":1,\"244\":1,\"1170\":1,\"1210\":1,\"1264\":1,\"1334\":1}}],[\"ndim\",{\"1\":{\"2187\":2,\"2192\":2,\"2203\":2,\"2209\":2}}],[\"ndarray\",{\"1\":{\"994\":2,\"1548\":1,\"1631\":2,\"1674\":2,\"1750\":1,\"1766\":1,\"1816\":1,\"1854\":1,\"1861\":1,\"1873\":2,\"1877\":2,\"1881\":1,\"1892\":2,\"1932\":2,\"2040\":2,\"2043\":2,\"2044\":6,\"2045\":2,\"2054\":2,\"2055\":2,\"2056\":2,\"2065\":3,\"2066\":2,\"2101\":4,\"2130\":5,\"2133\":5,\"2136\":5,\"2137\":3,\"2155\":1,\"2246\":2,\"2247\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":1,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2278\":1,\"2283\":1,\"2287\":1,\"2291\":1,\"2332\":2,\"2342\":2,\"2351\":2,\"2354\":1,\"2366\":2,\"2367\":3,\"2373\":6,\"2374\":1,\"2376\":2,\"2378\":2,\"2398\":2,\"2490\":2,\"2495\":2}}],[\"nhid\",{\"1\":{\"1945\":1}}],[\"nhyp\",{\"1\":{\"1156\":1}}],[\"nyquist\",{\"0\":{\"1570\":1},\"1\":{\"1570\":1}}],[\"n=0\",{\"1\":{\"1720\":1}}],[\"n=none\",{\"1\":{\"1254\":2}}],[\"n=30\",{\"1\":{\"1253\":1}}],[\"n=1000\",{\"1\":{\"1224\":1,\"1225\":1}}],[\"n=64\",{\"1\":{\"821\":1}}],[\"nmic2\",{\"1\":{\"1164\":1}}],[\"nmic1\",{\"1\":{\"1164\":1}}],[\"nmask=1\",{\"1\":{\"1199\":1}}],[\"nmask\",{\"1\":{\"1127\":1}}],[\"nref\",{\"1\":{\"1156\":1}}],[\"n$\",{\"1\":{\"1119\":3}}],[\"nf=128\",{\"1\":{\"1211\":1}}],[\"nframe\",{\"1\":{\"1164\":1}}],[\"nfreqs\",{\"1\":{\"1119\":1}}],[\"nfs\",{\"1\":{\"60\":1}}],[\"nclasses\",{\"1\":{\"2167\":2,\"2176\":2,\"2207\":2}}],[\"nclass\",{\"1\":{\"1171\":2}}],[\"nclusters\",{\"1\":{\"276\":7}}],[\"ncbi\",{\"1\":{\"1117\":1}}],[\"ncsnv1\",{\"1\":{\"1346\":1,\"1347\":1}}],[\"ncsnv2\",{\"1\":{\"1050\":1,\"1347\":1}}],[\"ncsn++\",{\"1\":{\"1211\":1}}],[\"ncsnpp\",{\"0\":{\"1055\":1,\"1057\":1,\"1068\":1,\"1076\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1110\":1,\"1114\":1,\"1144\":1,\"1151\":1,\"1177\":1,\"1187\":1,\"1196\":1,\"1200\":1,\"1211\":2,\"1213\":1,\"1219\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1299\":1,\"1301\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1312\":1,\"1324\":1,\"1331\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1},\"1\":{\"1055\":1,\"1057\":1,\"1068\":1,\"1076\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":2,\"1105\":1,\"1110\":1,\"1114\":1,\"1144\":1,\"1151\":1,\"1177\":1,\"1187\":1,\"1196\":1,\"1200\":1,\"1211\":2,\"1213\":1,\"1219\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1299\":1,\"1301\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1312\":1,\"1324\":1,\"1331\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1}}],[\"ncsn\",{\"0\":{\"1346\":1,\"1347\":1},\"1\":{\"1050\":1,\"1346\":1,\"1347\":1}}],[\"ncols\",{\"1\":{\"603\":2}}],[\"nchar\",{\"1\":{\"603\":2}}],[\"nccl\",{\"0\":{\"66\":1},\"1\":{\"22\":1,\"66\":7,\"67\":5,\"104\":1,\"173\":1,\"2340\":2}}],[\"nb\",{\"1\":{\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1678\":1,\"2168\":1}}],[\"nbest\",{\"0\":{\"1379\":1,\"1949\":2},\"1\":{\"243\":1,\"301\":2,\"315\":2,\"321\":2,\"389\":2,\"396\":2,\"406\":4,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"463\":4,\"469\":2,\"498\":2,\"505\":6,\"616\":7,\"696\":8,\"697\":8,\"1949\":4,\"2339\":2,\"2348\":2,\"2359\":1,\"2370\":4,\"2372\":2}}],[\"nbpe\",{\"1\":{\"243\":2}}],[\"nbpe=50000\",{\"1\":{\"243\":1}}],[\"nlayers\",{\"1\":{\"1945\":1}}],[\"nlm\",{\"1\":{\"1117\":1}}],[\"nll\",{\"1\":{\"505\":2,\"777\":7,\"1156\":7,\"1552\":1,\"1581\":1,\"1586\":1,\"1588\":1,\"1603\":1,\"1613\":1,\"1616\":1,\"1626\":1,\"1940\":4,\"1941\":3,\"1942\":4,\"1943\":4}}],[\"nlp\",{\"1\":{\"200\":2,\"301\":2,\"2336\":1,\"2337\":1}}],[\"nlsyms\",{\"1\":{\"200\":1,\"242\":1,\"243\":2}}],[\"nn\",{\"1\":{\"200\":3,\"205\":3,\"242\":3,\"614\":1,\"617\":4,\"618\":6,\"622\":2,\"624\":7,\"629\":1,\"633\":3,\"635\":1,\"636\":3,\"638\":4,\"642\":2,\"650\":1,\"652\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"692\":3,\"693\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"706\":1,\"709\":2,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"715\":1,\"718\":1,\"720\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"741\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"750\":1,\"751\":1,\"752\":1,\"754\":1,\"757\":1,\"759\":1,\"766\":1,\"767\":1,\"768\":1,\"771\":1,\"774\":2,\"775\":1,\"777\":1,\"778\":1,\"780\":2,\"781\":1,\"783\":1,\"786\":1,\"787\":1,\"788\":1,\"790\":2,\"791\":1,\"793\":1,\"794\":1,\"796\":1,\"798\":1,\"800\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"818\":1,\"820\":3,\"823\":1,\"825\":1,\"828\":3,\"829\":1,\"830\":3,\"833\":2,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":2,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"859\":1,\"860\":1,\"862\":1,\"864\":1,\"927\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"979\":1,\"981\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":2,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1063\":1,\"1064\":3,\"1065\":1,\"1066\":1,\"1068\":1,\"1072\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1107\":1,\"1108\":2,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1126\":1,\"1127\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1144\":1,\"1145\":2,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1156\":1,\"1157\":4,\"1158\":1,\"1159\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":2,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1177\":1,\"1179\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":1,\"1205\":2,\"1207\":1,\"1208\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":1,\"1219\":1,\"1222\":1,\"1223\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":2,\"1238\":1,\"1240\":1,\"1242\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":5,\"1264\":2,\"1265\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1301\":1,\"1332\":1,\"1333\":1,\"1334\":1,\"1372\":1,\"1381\":1,\"1383\":1,\"1386\":2,\"1387\":1,\"1392\":1,\"1398\":1,\"1400\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1417\":1,\"1419\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1508\":1,\"1509\":1,\"1511\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1533\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1547\":1,\"1554\":1,\"1576\":1,\"1588\":1,\"1590\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1605\":1,\"1623\":1,\"1638\":1,\"1640\":1,\"1641\":1,\"1652\":1,\"1656\":1,\"1657\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1667\":1,\"1668\":3,\"1669\":1,\"1670\":1,\"1671\":1,\"1681\":1,\"1683\":1,\"1702\":1,\"1731\":1,\"1735\":2,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1750\":2,\"1751\":3,\"1759\":2,\"1782\":1,\"1796\":1,\"1838\":1,\"1856\":3,\"1860\":1,\"1861\":1,\"1878\":1,\"1896\":1,\"1931\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1959\":1,\"1963\":2,\"1965\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1992\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2129\":1,\"2130\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":2,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":2,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2221\":1,\"2222\":1,\"2223\":2,\"2232\":1,\"2238\":1,\"2304\":1,\"2305\":3,\"2309\":1,\"2311\":6,\"2325\":2,\"2327\":2,\"2401\":1,\"2403\":1,\"2405\":1,\"2407\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2434\":1,\"2443\":1,\"2445\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1}}],[\"nulldecoder\",{\"0\":{\"1222\":1},\"1\":{\"1222\":1}}],[\"nullencoder\",{\"0\":{\"1223\":1},\"1\":{\"1053\":1,\"1223\":1}}],[\"null\",{\"0\":{\"1222\":1,\"1223\":1},\"1\":{\"197\":1,\"223\":2,\"286\":5,\"661\":1,\"669\":1,\"1222\":2,\"1223\":2,\"2485\":1,\"2493\":1,\"2505\":1}}],[\"nums\",{\"1\":{\"2095\":1}}],[\"numspk=false\",{\"1\":{\"1228\":1}}],[\"numspk\",{\"1\":{\"356\":2,\"1155\":1,\"1157\":2,\"1158\":1,\"1228\":1,\"2346\":1,\"2368\":1}}],[\"num=0\",{\"1\":{\"1545\":1}}],[\"num=2\",{\"1\":{\"1503\":1}}],[\"num2tuple\",{\"0\":{\"1349\":1},\"1\":{\"1349\":1}}],[\"numgroups\",{\"1\":{\"1301\":1,\"1372\":1}}],[\"numbre\",{\"1\":{\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1}}],[\"numba\",{\"1\":{\"755\":1,\"785\":1,\"786\":1,\"800\":1,\"866\":1}}],[\"numbern\",{\"1\":{\"2442\":1}}],[\"number=\",{\"1\":{\"207\":1}}],[\"numbers\",{\"1\":{\"80\":1,\"102\":1,\"691\":1,\"993\":1,\"1228\":1,\"1730\":1,\"1816\":1,\"2162\":1}}],[\"number\",{\"0\":{\"91\":1,\"94\":1,\"120\":1,\"123\":1},\"1\":{\"43\":6,\"45\":4,\"48\":3,\"55\":2,\"58\":1,\"59\":1,\"62\":1,\"69\":2,\"70\":1,\"90\":1,\"91\":2,\"93\":1,\"94\":3,\"97\":2,\"98\":4,\"99\":1,\"100\":2,\"102\":3,\"120\":2,\"123\":2,\"127\":2,\"135\":1,\"139\":2,\"141\":4,\"142\":3,\"145\":3,\"147\":2,\"148\":1,\"165\":1,\"167\":2,\"173\":3,\"175\":1,\"211\":1,\"223\":2,\"224\":1,\"267\":4,\"276\":5,\"286\":4,\"290\":1,\"515\":2,\"516\":4,\"520\":1,\"521\":1,\"522\":1,\"523\":2,\"524\":1,\"525\":1,\"527\":1,\"535\":1,\"537\":1,\"616\":4,\"617\":2,\"618\":2,\"619\":1,\"620\":3,\"622\":1,\"623\":1,\"624\":2,\"625\":2,\"626\":3,\"627\":1,\"630\":2,\"633\":2,\"634\":3,\"636\":2,\"637\":1,\"641\":1,\"642\":1,\"643\":1,\"644\":5,\"645\":2,\"646\":1,\"647\":2,\"649\":2,\"661\":2,\"669\":2,\"692\":3,\"696\":4,\"697\":4,\"699\":1,\"703\":1,\"706\":1,\"709\":3,\"710\":3,\"711\":3,\"717\":1,\"748\":2,\"755\":1,\"756\":1,\"768\":7,\"771\":1,\"773\":1,\"774\":3,\"780\":5,\"784\":2,\"785\":2,\"798\":4,\"804\":2,\"817\":1,\"820\":1,\"821\":3,\"824\":1,\"828\":1,\"831\":1,\"846\":5,\"847\":2,\"849\":5,\"862\":4,\"882\":1,\"883\":1,\"884\":1,\"922\":1,\"932\":2,\"934\":2,\"936\":1,\"937\":1,\"938\":1,\"960\":1,\"978\":5,\"980\":3,\"982\":5,\"1000\":1,\"1002\":1,\"1019\":1,\"1024\":1,\"1029\":1,\"1053\":1,\"1061\":3,\"1062\":3,\"1064\":3,\"1070\":1,\"1071\":1,\"1107\":4,\"1117\":2,\"1118\":2,\"1124\":9,\"1125\":9,\"1130\":2,\"1131\":2,\"1133\":2,\"1134\":1,\"1136\":2,\"1137\":1,\"1139\":2,\"1141\":3,\"1145\":5,\"1147\":4,\"1155\":2,\"1157\":2,\"1162\":2,\"1163\":1,\"1164\":1,\"1176\":2,\"1180\":2,\"1181\":2,\"1185\":1,\"1224\":1,\"1225\":1,\"1232\":2,\"1235\":1,\"1245\":1,\"1252\":3,\"1253\":2,\"1259\":1,\"1261\":2,\"1262\":2,\"1264\":8,\"1265\":2,\"1267\":4,\"1268\":4,\"1269\":5,\"1270\":5,\"1271\":5,\"1273\":8,\"1274\":7,\"1278\":4,\"1279\":4,\"1280\":8,\"1281\":4,\"1282\":3,\"1283\":8,\"1290\":2,\"1311\":1,\"1316\":2,\"1318\":1,\"1320\":2,\"1322\":1,\"1327\":1,\"1328\":1,\"1330\":1,\"1333\":1,\"1334\":10,\"1354\":1,\"1376\":1,\"1377\":1,\"1385\":1,\"1392\":3,\"1397\":3,\"1400\":1,\"1402\":1,\"1409\":1,\"1416\":1,\"1419\":1,\"1422\":3,\"1423\":1,\"1441\":3,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1467\":1,\"1469\":1,\"1513\":4,\"1514\":5,\"1519\":4,\"1520\":3,\"1521\":3,\"1524\":2,\"1525\":1,\"1533\":2,\"1534\":1,\"1535\":6,\"1536\":4,\"1545\":1,\"1546\":3,\"1548\":4,\"1549\":1,\"1551\":5,\"1552\":15,\"1556\":4,\"1558\":4,\"1581\":4,\"1582\":1,\"1583\":2,\"1584\":1,\"1586\":1,\"1587\":2,\"1591\":1,\"1592\":4,\"1594\":1,\"1595\":1,\"1596\":4,\"1597\":4,\"1599\":21,\"1604\":4,\"1605\":4,\"1606\":5,\"1607\":1,\"1608\":2,\"1609\":5,\"1610\":8,\"1611\":6,\"1612\":6,\"1613\":5,\"1614\":1,\"1615\":1,\"1616\":4,\"1618\":1,\"1619\":4,\"1620\":2,\"1621\":2,\"1622\":3,\"1626\":14,\"1628\":9,\"1631\":1,\"1645\":5,\"1647\":2,\"1650\":3,\"1654\":1,\"1655\":1,\"1662\":3,\"1666\":1,\"1668\":3,\"1683\":1,\"1692\":1,\"1719\":2,\"1721\":2,\"1722\":6,\"1725\":2,\"1735\":3,\"1736\":1,\"1737\":2,\"1747\":1,\"1748\":3,\"1749\":1,\"1750\":7,\"1753\":2,\"1756\":2,\"1757\":2,\"1782\":1,\"1785\":2,\"1787\":1,\"1789\":2,\"1790\":2,\"1794\":2,\"1795\":2,\"1807\":5,\"1809\":1,\"1810\":3,\"1812\":2,\"1814\":3,\"1815\":2,\"1816\":3,\"1817\":2,\"1854\":7,\"1860\":1,\"1862\":2,\"1863\":1,\"1871\":1,\"1889\":1,\"1907\":1,\"1917\":1,\"1921\":1,\"1936\":1,\"1949\":1,\"1960\":1,\"1961\":2,\"1992\":5,\"1993\":12,\"1995\":5,\"2000\":9,\"2001\":7,\"2015\":2,\"2018\":1,\"2020\":1,\"2039\":1,\"2129\":3,\"2130\":4,\"2132\":2,\"2133\":1,\"2134\":3,\"2136\":4,\"2137\":1,\"2141\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2149\":1,\"2151\":4,\"2157\":1,\"2162\":1,\"2167\":1,\"2176\":3,\"2191\":3,\"2198\":2,\"2207\":1,\"2218\":1,\"2220\":1,\"2223\":7,\"2228\":3,\"2229\":3,\"2235\":12,\"2236\":14,\"2239\":10,\"2240\":10,\"2245\":23,\"2249\":2,\"2253\":2,\"2307\":2,\"2310\":4,\"2353\":1,\"2354\":1,\"2364\":2,\"2367\":1,\"2385\":1,\"2411\":16,\"2412\":20,\"2423\":20,\"2425\":4,\"2426\":2,\"2427\":1,\"2428\":6,\"2429\":6,\"2430\":2,\"2431\":23,\"2432\":22,\"2433\":2,\"2447\":14,\"2462\":1,\"2482\":3,\"2490\":2,\"2495\":2}}],[\"numerical\",{\"1\":{\"615\":1,\"640\":1,\"648\":1,\"722\":1,\"1400\":1,\"1469\":1}}],[\"numelementsbatchsampler\",{\"0\":{\"2004\":1},\"1\":{\"2004\":1}}],[\"numel\",{\"1\":{\"95\":1,\"97\":1,\"98\":1,\"100\":3,\"449\":2,\"2007\":4}}],[\"numpy=false\",{\"1\":{\"1353\":1}}],[\"numpy\",{\"0\":{\"1861\":1},\"1\":{\"220\":1,\"570\":1,\"759\":1,\"991\":1,\"996\":1,\"998\":1,\"999\":2,\"1003\":1,\"1010\":4,\"1011\":1,\"1015\":1,\"1750\":1,\"1760\":2,\"1861\":3,\"1932\":2,\"2040\":1,\"2043\":1,\"2045\":1,\"2055\":1,\"2056\":1,\"2065\":1,\"2066\":1,\"2101\":1,\"2137\":2,\"2249\":1,\"2332\":1,\"2373\":3,\"2405\":1,\"2440\":1}}],[\"num\",{\"0\":{\"1019\":1,\"1889\":1,\"2004\":1,\"2107\":1,\"2113\":1,\"2385\":1},\"1\":{\"91\":2,\"121\":1,\"141\":2,\"142\":5,\"147\":2,\"168\":3,\"223\":5,\"224\":4,\"225\":1,\"243\":8,\"259\":3,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":2,\"335\":4,\"342\":2,\"349\":2,\"361\":2,\"368\":2,\"374\":4,\"377\":2,\"385\":2,\"389\":2,\"396\":2,\"404\":2,\"406\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":10,\"457\":2,\"461\":2,\"463\":2,\"469\":2,\"475\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":2,\"505\":4,\"521\":1,\"541\":2,\"543\":2,\"548\":2,\"551\":2,\"561\":2,\"564\":2,\"578\":2,\"583\":2,\"589\":2,\"630\":4,\"633\":2,\"634\":4,\"637\":10,\"641\":2,\"642\":2,\"643\":2,\"644\":2,\"649\":4,\"661\":2,\"669\":2,\"689\":1,\"692\":1,\"700\":1,\"703\":2,\"705\":1,\"709\":2,\"710\":2,\"711\":2,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"736\":2,\"748\":2,\"750\":1,\"755\":2,\"766\":1,\"767\":1,\"774\":2,\"775\":1,\"780\":2,\"785\":5,\"787\":2,\"794\":5,\"796\":4,\"798\":2,\"803\":2,\"804\":2,\"807\":1,\"833\":2,\"846\":6,\"847\":2,\"848\":1,\"849\":6,\"850\":1,\"851\":2,\"862\":2,\"869\":2,\"882\":3,\"883\":3,\"884\":4,\"922\":4,\"936\":2,\"937\":2,\"966\":1,\"969\":1,\"970\":1,\"974\":1,\"977\":2,\"978\":7,\"979\":3,\"980\":1,\"1019\":1,\"1020\":1,\"1025\":1,\"1045\":1,\"1053\":3,\"1059\":2,\"1061\":7,\"1062\":7,\"1064\":2,\"1065\":1,\"1078\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":2,\"1097\":2,\"1099\":2,\"1101\":2,\"1103\":1,\"1105\":2,\"1107\":3,\"1117\":3,\"1118\":6,\"1125\":3,\"1126\":2,\"1130\":3,\"1131\":3,\"1133\":4,\"1134\":2,\"1136\":3,\"1137\":3,\"1139\":2,\"1141\":3,\"1145\":2,\"1155\":1,\"1156\":2,\"1157\":6,\"1158\":1,\"1162\":3,\"1163\":3,\"1164\":3,\"1168\":1,\"1187\":1,\"1198\":3,\"1211\":1,\"1213\":1,\"1217\":2,\"1219\":1,\"1228\":1,\"1232\":3,\"1252\":5,\"1259\":2,\"1261\":3,\"1262\":2,\"1267\":3,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":3,\"1279\":4,\"1280\":7,\"1281\":4,\"1282\":4,\"1283\":7,\"1288\":1,\"1290\":4,\"1319\":2,\"1333\":3,\"1334\":1,\"1349\":1,\"1374\":1,\"1375\":1,\"1398\":1,\"1404\":1,\"1408\":1,\"1410\":1,\"1416\":1,\"1423\":1,\"1439\":1,\"1441\":1,\"1462\":1,\"1489\":2,\"1498\":1,\"1499\":1,\"1545\":2,\"1643\":3,\"1645\":4,\"1646\":3,\"1647\":2,\"1650\":2,\"1664\":1,\"1665\":1,\"1691\":1,\"1702\":1,\"1726\":1,\"1727\":1,\"1735\":2,\"1748\":3,\"1773\":1,\"1781\":1,\"1826\":1,\"1829\":1,\"1860\":2,\"1883\":3,\"1889\":2,\"1890\":2,\"1936\":2,\"1937\":2,\"1985\":1,\"1992\":2,\"1995\":2,\"2004\":1,\"2008\":1,\"2009\":1,\"2126\":1,\"2127\":1,\"2129\":2,\"2130\":3,\"2131\":1,\"2132\":2,\"2133\":4,\"2134\":2,\"2136\":10,\"2137\":1,\"2139\":5,\"2149\":2,\"2157\":2,\"2191\":2,\"2198\":2,\"2202\":1,\"2218\":2,\"2219\":2,\"2220\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":5,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":4,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":4,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2278\":1,\"2283\":1,\"2291\":1,\"2336\":1,\"2337\":1,\"2341\":1,\"2346\":3,\"2353\":2,\"2354\":2,\"2356\":1,\"2362\":1,\"2364\":4,\"2368\":3,\"2372\":2,\"2385\":1,\"2432\":4}}],[\"nvcc\",{\"1\":{\"162\":2}}],[\"nvidia\",{\"1\":{\"66\":1,\"67\":1,\"173\":1,\"705\":1,\"804\":1,\"932\":1,\"934\":1}}],[\"nj=4\",{\"1\":{\"167\":1}}],[\"nj=1\",{\"1\":{\"126\":1}}],[\"nj=10\",{\"1\":{\"69\":1}}],[\"nj\",{\"1\":{\"69\":2,\"120\":2,\"126\":2,\"167\":3,\"223\":2,\"243\":2,\"267\":3,\"276\":3,\"285\":1,\"286\":4,\"516\":2,\"520\":2,\"521\":2,\"523\":2,\"524\":2,\"525\":2,\"535\":2,\"537\":2}}],[\"npsd\",{\"1\":{\"1330\":1}}],[\"npsd^\",{\"1\":{\"1319\":2,\"1321\":2,\"1322\":2,\"1323\":1,\"1327\":2}}],[\"nplr\",{\"0\":{\"924\":1},\"1\":{\"821\":4,\"824\":1,\"924\":2}}],[\"npyscpwriter\",{\"0\":{\"998\":1},\"1\":{\"998\":1,\"999\":1}}],[\"npyscpreader\",{\"0\":{\"996\":1},\"1\":{\"996\":1,\"997\":1}}],[\"npy\",{\"0\":{\"996\":1,\"998\":1},\"1\":{\"80\":2,\"82\":1,\"96\":2,\"97\":4,\"98\":8,\"99\":4,\"100\":4,\"101\":2,\"996\":1,\"997\":5,\"998\":1,\"999\":4,\"1548\":1,\"1656\":1}}],[\"np\",{\"0\":{\"2440\":1},\"1\":{\"64\":1,\"220\":2,\"994\":3,\"1020\":2,\"1025\":1,\"1545\":1,\"1674\":2,\"1816\":1,\"1824\":1,\"1846\":2,\"1861\":1,\"1881\":1,\"1892\":2,\"1933\":1,\"2040\":1,\"2043\":1,\"2044\":2,\"2045\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2065\":1,\"2066\":1,\"2101\":2,\"2130\":2,\"2378\":2,\"2440\":1}}],[\"npm\",{\"1\":{\"33\":1}}],[\"n3\",{\"1\":{\"63\":1}}],[\"n5\",{\"1\":{\"63\":1}}],[\"n2\",{\"1\":{\"62\":1,\"774\":1}}],[\"nsample\",{\"1\":{\"1669\":1}}],[\"nsamples\",{\"1\":{\"976\":1,\"1660\":1,\"1669\":1,\"2232\":4,\"2238\":3}}],[\"nspk=2\",{\"1\":{\"1164\":1}}],[\"ns\",{\"1\":{\"523\":1,\"536\":2}}],[\"nstep\",{\"1\":{\"45\":3,\"139\":2,\"145\":1,\"616\":2,\"625\":2,\"627\":2,\"696\":2,\"697\":2}}],[\"nsc\",{\"1\":{\"45\":2,\"616\":1,\"628\":1,\"696\":5,\"697\":5,\"743\":1,\"1762\":1}}],[\"n\",{\"1\":{\"32\":1,\"45\":2,\"54\":12,\"141\":1,\"142\":1,\"173\":1,\"200\":2,\"205\":2,\"242\":1,\"243\":2,\"267\":18,\"271\":2,\"276\":7,\"280\":2,\"286\":10,\"287\":13,\"290\":1,\"309\":2,\"469\":2,\"514\":6,\"516\":6,\"522\":2,\"523\":4,\"548\":6,\"551\":4,\"558\":6,\"575\":4,\"616\":4,\"630\":2,\"631\":2,\"634\":5,\"636\":2,\"641\":20,\"642\":2,\"643\":11,\"649\":1,\"651\":1,\"692\":11,\"696\":6,\"697\":7,\"715\":3,\"720\":2,\"745\":2,\"749\":3,\"760\":6,\"762\":1,\"765\":1,\"770\":2,\"776\":1,\"778\":2,\"783\":3,\"784\":9,\"790\":5,\"792\":1,\"797\":1,\"817\":1,\"819\":3,\"820\":11,\"821\":5,\"822\":2,\"823\":1,\"824\":8,\"828\":5,\"829\":2,\"847\":21,\"850\":5,\"877\":1,\"895\":1,\"924\":1,\"926\":3,\"928\":1,\"938\":2,\"939\":3,\"944\":1,\"958\":2,\"959\":2,\"971\":3,\"975\":3,\"978\":2,\"980\":1,\"982\":3,\"1029\":3,\"1050\":1,\"1063\":1,\"1068\":1,\"1070\":2,\"1071\":2,\"1072\":3,\"1073\":2,\"1074\":3,\"1087\":1,\"1091\":2,\"1107\":2,\"1116\":1,\"1117\":1,\"1126\":2,\"1128\":1,\"1130\":1,\"1131\":1,\"1132\":1,\"1133\":1,\"1136\":2,\"1141\":2,\"1156\":1,\"1162\":1,\"1167\":1,\"1179\":3,\"1182\":2,\"1183\":2,\"1184\":1,\"1189\":1,\"1198\":1,\"1204\":2,\"1209\":1,\"1224\":1,\"1225\":1,\"1228\":1,\"1230\":2,\"1232\":2,\"1235\":5,\"1245\":2,\"1246\":2,\"1247\":2,\"1250\":1,\"1251\":1,\"1252\":2,\"1253\":1,\"1261\":2,\"1264\":2,\"1267\":2,\"1268\":4,\"1269\":13,\"1270\":13,\"1271\":11,\"1273\":5,\"1274\":5,\"1278\":2,\"1290\":1,\"1301\":4,\"1306\":5,\"1309\":1,\"1310\":1,\"1311\":1,\"1316\":5,\"1318\":1,\"1319\":3,\"1320\":4,\"1321\":3,\"1322\":3,\"1323\":3,\"1327\":1,\"1330\":1,\"1334\":10,\"1354\":2,\"1368\":2,\"1371\":5,\"1372\":4,\"1385\":3,\"1389\":7,\"1391\":3,\"1392\":2,\"1395\":4,\"1396\":5,\"1397\":2,\"1401\":8,\"1402\":1,\"1403\":3,\"1406\":1,\"1408\":6,\"1409\":1,\"1410\":1,\"1419\":2,\"1422\":2,\"1439\":2,\"1441\":3,\"1450\":4,\"1452\":4,\"1454\":4,\"1456\":4,\"1466\":8,\"1467\":1,\"1468\":3,\"1491\":1,\"1517\":1,\"1524\":3,\"1525\":3,\"1526\":2,\"1533\":7,\"1551\":3,\"1552\":4,\"1553\":2,\"1554\":1,\"1555\":1,\"1556\":2,\"1558\":7,\"1560\":1,\"1564\":1,\"1568\":1,\"1598\":2,\"1600\":2,\"1607\":5,\"1625\":2,\"1655\":2,\"1662\":4,\"1667\":2,\"1669\":1,\"1680\":3,\"1692\":6,\"1698\":3,\"1719\":4,\"1720\":2,\"1721\":1,\"1723\":7,\"1724\":5,\"1725\":2,\"1727\":1,\"1749\":9,\"1752\":2,\"1753\":2,\"1756\":2,\"1757\":2,\"1776\":1,\"1785\":4,\"1787\":8,\"1789\":2,\"1790\":2,\"1791\":3,\"1794\":9,\"1798\":2,\"1799\":2,\"1800\":2,\"1806\":3,\"1810\":6,\"1811\":2,\"1812\":2,\"1815\":21,\"1817\":4,\"1822\":1,\"1832\":2,\"1835\":2,\"1836\":2,\"1846\":1,\"1854\":15,\"1862\":1,\"1868\":2,\"1870\":2,\"1899\":1,\"1900\":3,\"1917\":3,\"1923\":2,\"1924\":2,\"1925\":2,\"1944\":4,\"1945\":5,\"1946\":1,\"1947\":4,\"1949\":1,\"1978\":1,\"1980\":2,\"1982\":1,\"1984\":2,\"1992\":5,\"1993\":2,\"2000\":18,\"2001\":6,\"2130\":2,\"2136\":2,\"2232\":1,\"2238\":1,\"2240\":1,\"2245\":1,\"2249\":1,\"2299\":2,\"2355\":2,\"2404\":1,\"2409\":1,\"2411\":1,\"2412\":1,\"2414\":1,\"2416\":2,\"2418\":1,\"2421\":2,\"2423\":1,\"2431\":1,\"2433\":4,\"2436\":2,\"2437\":1,\"2438\":2,\"2439\":2,\"2440\":2,\"2482\":6,\"2490\":8,\"2495\":6}}],[\"ng\",{\"1\":{\"285\":1,\"287\":39,\"481\":14}}],[\"ngrampartscorer\",{\"0\":{\"1799\":1},\"1\":{\"1799\":1}}],[\"ngrambase\",{\"0\":{\"1800\":1},\"1\":{\"1798\":2,\"1799\":2,\"1800\":2}}],[\"ngramfullscorer\",{\"0\":{\"1798\":1},\"1\":{\"1798\":1}}],[\"ngram\",{\"0\":{\"1798\":1,\"1799\":1,\"1800\":1},\"1\":{\"199\":1,\"204\":1,\"228\":1,\"240\":1,\"242\":1,\"301\":4,\"389\":4,\"396\":4,\"421\":4,\"429\":4,\"442\":4,\"463\":8,\"498\":4,\"1798\":5,\"1799\":5,\"1800\":5}}],[\"ngpu=0\",{\"1\":{\"41\":1}}],[\"ngpu=1\",{\"1\":{\"40\":1,\"117\":1}}],[\"ngpu1\",{\"1\":{\"23\":1}}],[\"ngpu\",{\"1\":{\"22\":1,\"24\":2,\"25\":1,\"40\":3,\"41\":7,\"54\":5,\"56\":2,\"57\":1,\"58\":2,\"61\":3,\"62\":1,\"63\":1,\"64\":1,\"93\":2,\"94\":2,\"117\":3,\"121\":2,\"195\":1,\"223\":1,\"243\":1,\"267\":1,\"286\":5,\"295\":2,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"368\":2,\"374\":2,\"377\":2,\"385\":2,\"389\":2,\"396\":2,\"404\":2,\"406\":2,\"415\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":2,\"457\":2,\"463\":2,\"469\":2,\"475\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":2,\"505\":2,\"527\":3,\"1951\":1,\"2249\":1,\"2340\":2,\"2348\":1,\"2369\":1,\"2370\":2,\"2372\":1,\"2384\":1,\"2385\":1}}],[\"ngoc\",{\"1\":{\"12\":1}}],[\"near\",{\"1\":{\"1608\":1}}],[\"nearest\",{\"1\":{\"1371\":1,\"1372\":1,\"1582\":1,\"1617\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1624\":1}}],[\"neurons\",{\"1\":{\"1784\":1}}],[\"neurips\",{\"1\":{\"1605\":1,\"1606\":1}}],[\"neuralbeamformer\",{\"0\":{\"1217\":1},\"1\":{\"223\":1,\"1217\":1}}],[\"neural\",{\"0\":{\"1217\":1},\"1\":{\"26\":1,\"200\":1,\"205\":1,\"223\":1,\"242\":1,\"284\":1,\"286\":1,\"287\":1,\"290\":2,\"768\":1,\"927\":1,\"1086\":1,\"1170\":1,\"1207\":1,\"1217\":1,\"1246\":1,\"1334\":1,\"1381\":1,\"1389\":1,\"1391\":2,\"1395\":1,\"1401\":1,\"1403\":2,\"1406\":2,\"1408\":1,\"1410\":2,\"1466\":1,\"1468\":2,\"1822\":1,\"2133\":1,\"2136\":1,\"2254\":1,\"2298\":1,\"2312\":1,\"2432\":1}}],[\"neighbor\",{\"1\":{\"1371\":1,\"1372\":1}}],[\"neighborhood\",{\"1\":{\"43\":2,\"141\":1}}],[\"nevertheless\",{\"1\":{\"1883\":1}}],[\"never\",{\"1\":{\"819\":1,\"1484\":1}}],[\"nesterov\",{\"1\":{\"1962\":1}}],[\"nesteddictaction\",{\"0\":{\"2478\":1},\"1\":{\"2478\":1}}],[\"nested\",{\"0\":{\"2478\":1},\"1\":{\"86\":1,\"925\":1,\"2478\":1}}],[\"nessasary\",{\"1\":{\"747\":1}}],[\"negate\",{\"0\":{\"923\":1},\"1\":{\"923\":1}}],[\"negative\",{\"1\":{\"141\":1,\"175\":1,\"661\":1,\"664\":1,\"669\":1,\"755\":1,\"777\":2,\"785\":1,\"1156\":2,\"1246\":1,\"1247\":1,\"1389\":1,\"1390\":1,\"1392\":1,\"1396\":1,\"1397\":1,\"1401\":2,\"1402\":2,\"1408\":3,\"1409\":3,\"1420\":1,\"1466\":1,\"1467\":1,\"1513\":1,\"1526\":3,\"1548\":1,\"1549\":2,\"1551\":1,\"1552\":1,\"1553\":2,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":2,\"1596\":1,\"1597\":1,\"1598\":3,\"1599\":1,\"1600\":3,\"1604\":1,\"1605\":1,\"1606\":1,\"1609\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1618\":1,\"1619\":1,\"1625\":2,\"1626\":1,\"1940\":1,\"1941\":1,\"1942\":1,\"1943\":1,\"2134\":1,\"2176\":3}}],[\"neg\",{\"1\":{\"141\":1,\"664\":2,\"2363\":2}}],[\"next\",{\"0\":{\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1},\"1\":{\"87\":1,\"195\":1,\"197\":1,\"243\":1,\"641\":2,\"692\":2,\"760\":4,\"790\":2,\"797\":2,\"820\":4,\"831\":1,\"847\":2,\"850\":2,\"1050\":2,\"1116\":2,\"1161\":2,\"1189\":2,\"1218\":2,\"1221\":2,\"1229\":2,\"1244\":2,\"1723\":3,\"1724\":2,\"1725\":7,\"1731\":7,\"1735\":11,\"1787\":3,\"1798\":2,\"1799\":5,\"1800\":5,\"1804\":1,\"1805\":5,\"1815\":2,\"1822\":2,\"1856\":2,\"1944\":4,\"1945\":2,\"1946\":2,\"1947\":4,\"1992\":2,\"2249\":1,\"2253\":1,\"2355\":2,\"2367\":2,\"2377\":1}}],[\"netlib\",{\"1\":{\"1308\":1}}],[\"net\",{\"1\":{\"102\":1,\"1259\":1,\"1368\":1,\"1589\":1,\"1610\":2,\"1863\":2,\"1867\":2,\"1891\":2,\"1913\":2,\"1937\":2,\"2176\":1}}],[\"nets\",{\"0\":{\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1733\":1,\"1735\":1,\"1736\":2,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1762\":1,\"1764\":1,\"1766\":1,\"1768\":1,\"1769\":1,\"1770\":1,\"1771\":1,\"1775\":1,\"1779\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1792\":1,\"1793\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1803\":1,\"1804\":1,\"1806\":1,\"1807\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1820\":1,\"1821\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1842\":1,\"1843\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1851\":1,\"1854\":1,\"1855\":1,\"1858\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1885\":1,\"1886\":2,\"1888\":1,\"1891\":1,\"1892\":2,\"1893\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1901\":2,\"1903\":2,\"1905\":2,\"1907\":1,\"1908\":2,\"1910\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1916\":2,\"1917\":1,\"1918\":1,\"1919\":2,\"1920\":1,\"1921\":1,\"1926\":1,\"1927\":1,\"1928\":2,\"1931\":2,\"1932\":2,\"1934\":2,\"1935\":2,\"1936\":1,\"1937\":1},\"1\":{\"42\":1,\"46\":2,\"692\":1,\"710\":1,\"711\":1,\"731\":1,\"732\":1,\"766\":1,\"767\":1,\"775\":1,\"848\":1,\"849\":1,\"850\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":2,\"1727\":2,\"1729\":1,\"1730\":1,\"1731\":2,\"1732\":1,\"1733\":1,\"1735\":1,\"1736\":2,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1762\":1,\"1764\":1,\"1766\":1,\"1768\":1,\"1770\":1,\"1771\":1,\"1775\":1,\"1779\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1793\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1803\":1,\"1804\":1,\"1805\":1,\"1806\":1,\"1807\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1820\":1,\"1821\":1,\"1822\":8,\"1837\":1,\"1838\":1,\"1839\":1,\"1842\":1,\"1843\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1851\":1,\"1854\":1,\"1855\":1,\"1858\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1885\":1,\"1886\":2,\"1888\":1,\"1891\":1,\"1892\":2,\"1893\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1901\":2,\"1903\":2,\"1905\":2,\"1907\":1,\"1908\":2,\"1910\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1916\":2,\"1917\":1,\"1918\":1,\"1919\":2,\"1920\":1,\"1921\":1,\"1926\":1,\"1927\":1,\"1928\":2,\"1931\":2,\"1932\":2,\"1934\":2,\"1935\":2,\"1936\":1,\"1937\":1,\"1957\":1,\"1960\":1,\"1961\":1,\"1992\":1,\"1995\":1,\"2129\":1}}],[\"network=none\",{\"1\":{\"1720\":1}}],[\"networks\",{\"1\":{\"927\":1,\"939\":1,\"1176\":1,\"1246\":1,\"1770\":1,\"1822\":1}}],[\"network\",{\"0\":{\"143\":1,\"632\":1,\"905\":1,\"1779\":1},\"1\":{\"26\":1,\"44\":2,\"67\":3,\"140\":1,\"143\":4,\"193\":1,\"200\":1,\"202\":1,\"205\":1,\"242\":1,\"616\":3,\"618\":1,\"621\":1,\"624\":1,\"625\":3,\"627\":3,\"632\":3,\"696\":3,\"697\":3,\"736\":1,\"740\":1,\"768\":1,\"777\":1,\"786\":2,\"800\":2,\"867\":2,\"905\":1,\"921\":2,\"935\":2,\"1086\":1,\"1117\":2,\"1124\":1,\"1125\":1,\"1139\":1,\"1141\":1,\"1162\":1,\"1170\":1,\"1185\":1,\"1207\":1,\"1271\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1582\":1,\"1610\":3,\"1624\":1,\"1705\":1,\"1749\":1,\"1750\":2,\"1758\":2,\"1779\":5,\"1795\":1,\"1810\":2,\"1811\":2,\"1860\":1,\"1863\":4,\"1867\":1,\"1891\":1,\"1913\":3,\"1914\":1,\"1937\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"2127\":1,\"2221\":1,\"2223\":2,\"2227\":2,\"2231\":2,\"2240\":1,\"2245\":1,\"2312\":1,\"2431\":1,\"2432\":1}}],[\"newdatadir\",{\"1\":{\"529\":1}}],[\"newtask\",{\"1\":{\"78\":2,\"81\":1,\"82\":1}}],[\"newmodel\",{\"1\":{\"78\":2}}],[\"new\",{\"0\":{\"150\":1,\"197\":1,\"224\":1,\"225\":1,\"1348\":1,\"1684\":1,\"1685\":1},\"1\":{\"32\":1,\"36\":1,\"47\":2,\"70\":2,\"81\":3,\"82\":1,\"107\":1,\"138\":1,\"150\":4,\"162\":1,\"168\":1,\"190\":1,\"191\":3,\"194\":1,\"197\":1,\"200\":1,\"201\":1,\"208\":1,\"222\":5,\"223\":4,\"224\":2,\"225\":8,\"232\":2,\"243\":3,\"258\":2,\"262\":1,\"263\":2,\"269\":1,\"276\":1,\"278\":1,\"284\":5,\"285\":1,\"290\":8,\"614\":3,\"620\":4,\"634\":5,\"637\":2,\"639\":1,\"641\":2,\"643\":3,\"651\":2,\"675\":1,\"692\":1,\"760\":2,\"790\":1,\"797\":1,\"820\":2,\"847\":3,\"850\":1,\"1157\":1,\"1250\":1,\"1251\":1,\"1348\":1,\"1481\":1,\"1674\":1,\"1684\":2,\"1685\":2,\"1695\":2,\"1719\":10,\"1720\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":14,\"1726\":1,\"1727\":1,\"1731\":5,\"1736\":4,\"1748\":2,\"1749\":3,\"1787\":2,\"1805\":1,\"1806\":5,\"1807\":1,\"1815\":3,\"1817\":1,\"1818\":1,\"1822\":4,\"1843\":2,\"1851\":1,\"1871\":1,\"1916\":2,\"1927\":1,\"1944\":2,\"1945\":1,\"1946\":1,\"1947\":2,\"1966\":2,\"1992\":1,\"2039\":1,\"2049\":1,\"2133\":1,\"2136\":1,\"2137\":1,\"2325\":1}}],[\"needs\",{\"1\":{\"135\":1,\"242\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"756\":4,\"758\":1,\"773\":4,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"821\":1,\"822\":1,\"826\":1,\"830\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"866\":4,\"867\":4,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1668\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"needed\",{\"1\":{\"41\":1,\"49\":1,\"128\":1,\"141\":1,\"162\":1,\"217\":1,\"235\":1,\"286\":1,\"819\":1,\"821\":1,\"1008\":1,\"2130\":1,\"2215\":1}}],[\"need\",{\"1\":{\"19\":1,\"23\":1,\"24\":2,\"47\":1,\"55\":1,\"66\":1,\"67\":1,\"69\":1,\"79\":2,\"82\":1,\"96\":1,\"98\":1,\"99\":1,\"100\":1,\"106\":1,\"110\":4,\"119\":2,\"121\":1,\"123\":1,\"125\":1,\"127\":1,\"128\":2,\"139\":1,\"141\":1,\"150\":3,\"152\":1,\"153\":1,\"161\":2,\"162\":1,\"163\":1,\"168\":1,\"173\":1,\"196\":1,\"197\":2,\"200\":2,\"201\":2,\"217\":1,\"232\":2,\"242\":5,\"243\":4,\"254\":1,\"258\":1,\"259\":1,\"267\":5,\"268\":1,\"269\":1,\"276\":4,\"277\":1,\"278\":1,\"285\":1,\"286\":8,\"290\":1,\"536\":1,\"787\":6,\"819\":1,\"821\":1,\"851\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1799\":1,\"1800\":1,\"1806\":1,\"2016\":2,\"2215\":1,\"2249\":1,\"2355\":8}}],[\"nelson\",{\"1\":{\"10\":1,\"156\":1}}],[\"necessary\",{\"0\":{\"70\":1},\"1\":{\"1\":1,\"3\":2,\"82\":1,\"200\":2,\"205\":1,\"235\":1,\"269\":2,\"278\":2,\"1354\":2,\"1668\":1,\"1731\":1,\"1822\":1,\"1868\":1,\"1870\":1,\"1966\":1,\"2130\":1,\"2136\":1,\"2162\":1,\"2355\":1,\"2369\":1}}],[\"nil\",{\"1\":{\"2485\":1,\"2493\":1,\"2505\":1}}],[\"nin\",{\"0\":{\"1213\":1},\"1\":{\"1213\":1}}],[\"ninputs=1\",{\"1\":{\"1086\":1,\"1207\":1,\"1340\":1}}],[\"nih\",{\"1\":{\"1117\":1}}],[\"nishitoba\",{\"1\":{\"156\":1}}],[\"nist\",{\"0\":{\"75\":1},\"1\":{\"1824\":1}}],[\"nice\",{\"1\":{\"225\":1}}],[\"nic\",{\"1\":{\"66\":2}}],[\"ni\",{\"1\":{\"7\":1,\"10\":1,\"11\":1,\"1210\":1,\"1264\":1,\"1334\":1}}],[\"nolang\",{\"1\":{\"2360\":1}}],[\"nolinear\",{\"1\":{\"983\":1,\"1295\":1}}],[\"noaliassafedumper\",{\"0\":{\"2480\":1},\"1\":{\"2480\":1}}],[\"noamlr\",{\"0\":{\"2016\":1},\"1\":{\"2016\":1,\"2019\":2}}],[\"noam\",{\"0\":{\"2016\":1},\"1\":{\"2016\":2}}],[\"noatt\",{\"0\":{\"1801\":1},\"1\":{\"1801\":2}}],[\"nout\",{\"1\":{\"1783\":2,\"2167\":2,\"2170\":1,\"2176\":2,\"2207\":2}}],[\"nom\",{\"1\":{\"1618\":1}}],[\"nomask\",{\"0\":{\"971\":1,\"972\":1,\"973\":1,\"975\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"984\":1},\"1\":{\"674\":2,\"846\":2,\"971\":1,\"972\":1,\"973\":1,\"975\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"984\":1,\"1638\":2,\"1640\":1}}],[\"noop\",{\"0\":{\"1492\":1},\"1\":{\"1492\":1}}],[\"noop>\",{\"1\":{\"1489\":1}}],[\"nor\",{\"1\":{\"1462\":1}}],[\"normconvtranspose2d\",{\"0\":{\"1430\":1},\"1\":{\"1430\":1}}],[\"normconvtranspose1d\",{\"0\":{\"1428\":1},\"1\":{\"1428\":1}}],[\"normconv2d\",{\"0\":{\"1426\":1},\"1\":{\"1426\":1}}],[\"normconv1d\",{\"0\":{\"1424\":1},\"1\":{\"1424\":1}}],[\"norms\",{\"1\":{\"820\":2,\"828\":2}}],[\"norms=true\",{\"1\":{\"820\":1,\"828\":1}}],[\"norm=\",{\"1\":{\"1185\":1}}],[\"norm=none\",{\"1\":{\"820\":1,\"828\":1,\"830\":1}}],[\"norm=false\",{\"1\":{\"769\":1,\"1515\":1,\"1516\":1,\"1530\":1,\"1541\":1,\"1543\":1,\"1794\":1}}],[\"norm=true\",{\"1\":{\"254\":2,\"726\":1,\"727\":1,\"859\":1,\"1548\":1,\"1750\":1,\"1758\":1,\"1810\":1,\"2223\":1,\"2231\":1}}],[\"norm\",{\"0\":{\"887\":1,\"984\":1,\"1296\":1,\"1298\":1,\"1476\":1,\"1485\":1,\"1783\":1},\"1\":{\"45\":1,\"141\":31,\"142\":1,\"145\":1,\"267\":1,\"276\":1,\"286\":1,\"616\":2,\"617\":4,\"618\":4,\"619\":2,\"620\":2,\"622\":4,\"624\":4,\"634\":1,\"636\":4,\"643\":1,\"661\":10,\"674\":2,\"692\":1,\"696\":4,\"697\":4,\"700\":1,\"702\":2,\"709\":2,\"710\":1,\"711\":1,\"712\":1,\"734\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"771\":1,\"774\":1,\"780\":1,\"813\":1,\"820\":3,\"828\":3,\"830\":1,\"846\":7,\"848\":1,\"849\":1,\"851\":2,\"854\":1,\"887\":1,\"973\":1,\"980\":2,\"981\":1,\"982\":2,\"984\":2,\"1029\":2,\"1061\":2,\"1062\":2,\"1063\":1,\"1064\":2,\"1107\":1,\"1108\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1139\":2,\"1141\":2,\"1148\":1,\"1185\":1,\"1198\":1,\"1202\":2,\"1255\":2,\"1259\":2,\"1262\":2,\"1267\":2,\"1268\":2,\"1272\":1,\"1273\":2,\"1274\":2,\"1278\":1,\"1279\":2,\"1280\":2,\"1281\":2,\"1283\":2,\"1296\":2,\"1298\":1,\"1382\":1,\"1389\":6,\"1390\":3,\"1391\":3,\"1392\":3,\"1396\":5,\"1397\":3,\"1399\":1,\"1401\":6,\"1402\":7,\"1403\":3,\"1405\":1,\"1408\":7,\"1409\":7,\"1420\":2,\"1424\":2,\"1426\":2,\"1428\":2,\"1430\":2,\"1434\":1,\"1436\":1,\"1442\":2,\"1444\":2,\"1446\":2,\"1448\":2,\"1450\":5,\"1452\":5,\"1454\":5,\"1456\":5,\"1458\":5,\"1460\":5,\"1466\":4,\"1467\":5,\"1468\":3,\"1476\":2,\"1485\":3,\"1513\":5,\"1526\":7,\"1548\":4,\"1549\":5,\"1551\":5,\"1552\":7,\"1553\":8,\"1555\":1,\"1558\":2,\"1583\":1,\"1592\":5,\"1593\":2,\"1594\":5,\"1595\":7,\"1596\":8,\"1597\":10,\"1598\":7,\"1599\":5,\"1600\":7,\"1605\":5,\"1606\":5,\"1609\":4,\"1610\":5,\"1611\":3,\"1612\":2,\"1613\":2,\"1618\":2,\"1619\":5,\"1625\":8,\"1626\":7,\"1628\":5,\"1656\":4,\"1671\":2,\"1700\":4,\"1728\":2,\"1735\":1,\"1736\":2,\"1748\":2,\"1750\":1,\"1751\":1,\"1758\":1,\"1759\":1,\"1783\":1,\"1794\":1,\"1810\":1,\"1846\":1,\"1850\":2,\"1992\":1,\"1993\":2,\"1995\":1,\"2127\":1,\"2129\":1,\"2191\":1,\"2223\":1,\"2231\":1,\"2235\":2,\"2236\":2,\"2239\":2,\"2240\":2,\"2245\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":2,\"2432\":2,\"2447\":2,\"2458\":2,\"2460\":2}}],[\"normalizing\",{\"1\":{\"1334\":1,\"2101\":1}}],[\"normalize=false\",{\"1\":{\"895\":1}}],[\"normalizer\",{\"1\":{\"842\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1}}],[\"normalized=false\",{\"1\":{\"1547\":1}}],[\"normalizedpositionfeedforward\",{\"1\":{\"638\":1}}],[\"normalizedpositionwisefeedforward\",{\"0\":{\"638\":1},\"1\":{\"634\":1,\"638\":4}}],[\"normalized\",{\"1\":{\"243\":1,\"615\":2,\"640\":2,\"648\":2,\"720\":1,\"722\":1,\"736\":1,\"737\":1,\"738\":1,\"777\":1,\"854\":1,\"961\":1,\"1250\":1,\"1251\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1293\":1,\"1306\":1,\"1309\":1,\"1310\":1,\"1315\":1,\"1323\":1,\"1334\":2,\"1371\":1,\"1385\":2,\"1387\":1,\"1392\":2,\"1401\":1,\"1402\":1,\"1419\":3,\"1466\":1,\"1467\":1,\"1607\":3,\"1640\":1,\"1669\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"1783\":2,\"1942\":1,\"1959\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1997\":1,\"2000\":1,\"2101\":1,\"2127\":1,\"2221\":1,\"2354\":2,\"2355\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2435\":2,\"2436\":2}}],[\"normalize\",{\"0\":{\"1652\":1},\"1\":{\"128\":1,\"243\":3,\"267\":2,\"286\":5,\"301\":2,\"315\":2,\"335\":4,\"342\":4,\"361\":4,\"396\":2,\"406\":2,\"421\":2,\"442\":2,\"463\":2,\"469\":2,\"537\":1,\"548\":1,\"551\":1,\"606\":1,\"616\":1,\"625\":2,\"674\":2,\"692\":2,\"696\":2,\"697\":2,\"702\":2,\"709\":2,\"710\":2,\"711\":2,\"731\":1,\"732\":1,\"736\":1,\"745\":2,\"747\":2,\"748\":1,\"766\":1,\"767\":1,\"771\":2,\"774\":2,\"775\":1,\"777\":1,\"778\":1,\"780\":2,\"848\":1,\"849\":2,\"850\":1,\"954\":1,\"958\":1,\"974\":1,\"1107\":2,\"1133\":2,\"1155\":7,\"1157\":8,\"1210\":3,\"1252\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":2,\"1334\":1,\"1385\":1,\"1392\":1,\"1403\":1,\"1519\":3,\"1521\":3,\"1526\":2,\"1535\":2,\"1536\":2,\"1546\":2,\"1548\":2,\"1552\":2,\"1553\":1,\"1585\":3,\"1598\":2,\"1599\":4,\"1600\":2,\"1622\":2,\"1625\":1,\"1626\":2,\"1640\":1,\"1641\":1,\"1652\":1,\"1702\":1,\"1719\":2,\"1721\":2,\"1725\":2,\"1735\":2,\"1751\":2,\"1759\":2,\"1782\":3,\"1794\":1,\"1975\":2,\"1987\":1,\"1992\":2,\"1995\":2,\"1996\":1,\"1997\":1,\"2126\":1,\"2127\":1,\"2129\":2,\"2184\":1,\"2191\":2,\"2216\":1,\"2221\":1,\"2228\":3,\"2229\":3,\"2239\":5,\"2240\":4,\"2282\":1,\"2336\":1,\"2337\":1,\"2346\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2368\":1,\"2408\":3,\"2411\":4,\"2412\":4,\"2423\":3,\"2432\":4,\"2446\":2,\"2447\":4}}],[\"normalization=true\",{\"1\":{\"1326\":1}}],[\"normalization=<class\",{\"1\":{\"1103\":1,\"1236\":1}}],[\"normalization\",{\"0\":{\"615\":1,\"640\":1,\"648\":1,\"666\":2,\"788\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1105\":1,\"1187\":1,\"1219\":1,\"1288\":1,\"1293\":1,\"1324\":2},\"1\":{\"43\":2,\"45\":1,\"78\":1,\"141\":15,\"142\":4,\"145\":1,\"211\":1,\"217\":1,\"254\":3,\"266\":1,\"275\":1,\"285\":1,\"615\":2,\"617\":3,\"618\":3,\"619\":1,\"620\":1,\"622\":3,\"624\":3,\"625\":1,\"633\":4,\"634\":6,\"636\":3,\"638\":4,\"639\":3,\"640\":2,\"642\":7,\"643\":6,\"648\":2,\"661\":4,\"666\":8,\"686\":1,\"768\":2,\"785\":1,\"786\":1,\"788\":1,\"820\":1,\"828\":1,\"830\":1,\"866\":1,\"921\":1,\"971\":1,\"975\":1,\"984\":1,\"1029\":2,\"1061\":1,\"1062\":1,\"1064\":2,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1127\":1,\"1139\":1,\"1141\":1,\"1179\":1,\"1185\":1,\"1187\":1,\"1219\":1,\"1235\":1,\"1262\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1279\":2,\"1280\":2,\"1281\":2,\"1282\":1,\"1283\":2,\"1288\":1,\"1293\":2,\"1296\":1,\"1324\":3,\"1326\":1,\"1328\":2,\"1334\":1,\"1387\":1,\"1392\":1,\"1397\":1,\"1424\":2,\"1426\":2,\"1428\":2,\"1430\":2,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1458\":2,\"1460\":2,\"1485\":2,\"1513\":2,\"1535\":1,\"1536\":1,\"1548\":4,\"1549\":2,\"1551\":2,\"1552\":3,\"1558\":1,\"1592\":2,\"1596\":2,\"1597\":4,\"1599\":1,\"1605\":2,\"1606\":2,\"1609\":2,\"1610\":2,\"1612\":1,\"1613\":1,\"1618\":2,\"1619\":2,\"1626\":3,\"1628\":2,\"1656\":3,\"1700\":1,\"1736\":1,\"1750\":1,\"1783\":2,\"1810\":1,\"1963\":3,\"1965\":1,\"1993\":1,\"2130\":1,\"2223\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"normally\",{\"1\":{\"67\":1,\"154\":1,\"777\":1,\"1156\":1,\"1941\":1,\"1943\":1,\"2355\":4}}],[\"normal\",{\"0\":{\"154\":1},\"1\":{\"22\":1,\"24\":1,\"242\":1,\"260\":1,\"269\":1,\"278\":1,\"824\":1,\"846\":2,\"911\":6,\"924\":1,\"928\":1,\"1008\":1,\"1086\":2,\"1207\":2,\"1533\":1,\"2220\":2,\"2307\":1}}],[\"nosie\",{\"1\":{\"1062\":1}}],[\"noqa\",{\"1\":{\"793\":1,\"1963\":1}}],[\"noiseinjection\",{\"0\":{\"1802\":1},\"1\":{\"1802\":1}}],[\"noises\",{\"1\":{\"1329\":1,\"2353\":1,\"2364\":1}}],[\"noise^\",{\"1\":{\"1328\":1}}],[\"noise1\",{\"1\":{\"1217\":1}}],[\"noise=0\",{\"1\":{\"787\":1}}],[\"noise\",{\"0\":{\"927\":1,\"1525\":1,\"2307\":2,\"2442\":1},\"1\":{\"223\":7,\"224\":5,\"225\":2,\"284\":1,\"285\":2,\"290\":2,\"475\":4,\"484\":4,\"490\":4,\"746\":4,\"927\":5,\"1050\":1,\"1062\":2,\"1066\":1,\"1107\":3,\"1116\":1,\"1118\":3,\"1125\":3,\"1126\":3,\"1130\":3,\"1136\":3,\"1141\":3,\"1157\":1,\"1161\":1,\"1162\":3,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":3,\"1175\":1,\"1177\":1,\"1189\":1,\"1210\":1,\"1217\":1,\"1218\":1,\"1221\":1,\"1229\":1,\"1232\":3,\"1244\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1261\":1,\"1267\":3,\"1275\":1,\"1276\":2,\"1277\":1,\"1278\":3,\"1293\":3,\"1311\":3,\"1318\":4,\"1319\":1,\"1321\":2,\"1322\":5,\"1327\":6,\"1328\":4,\"1330\":7,\"1354\":3,\"1390\":1,\"1397\":1,\"1402\":1,\"1409\":1,\"1467\":1,\"1525\":3,\"1526\":6,\"1545\":4,\"1552\":6,\"1553\":6,\"1593\":1,\"1594\":1,\"1595\":1,\"1597\":1,\"1604\":1,\"1606\":1,\"1609\":1,\"1610\":2,\"1616\":3,\"1619\":10,\"1625\":6,\"1626\":6,\"1628\":1,\"1680\":2,\"1802\":1,\"2215\":1,\"2307\":4,\"2336\":4,\"2337\":4,\"2346\":7,\"2348\":1,\"2350\":4,\"2353\":15,\"2356\":4,\"2360\":4,\"2361\":4,\"2362\":4,\"2364\":15,\"2368\":7,\"2370\":2,\"2372\":1,\"2428\":4,\"2441\":3,\"2442\":3}}],[\"noisy\",{\"1\":{\"223\":1,\"224\":1,\"1118\":2,\"1253\":3,\"1327\":1,\"1330\":1,\"2176\":1,\"2428\":2}}],[\"nonlinearity=\",{\"1\":{\"1211\":1}}],[\"nonlinear\",{\"0\":{\"983\":1,\"1295\":1},\"1\":{\"978\":1,\"983\":1,\"1107\":3,\"1117\":3,\"1126\":1,\"1127\":1,\"1130\":3,\"1131\":3,\"1136\":3,\"1141\":3,\"1232\":3,\"1261\":3,\"1267\":5,\"1268\":6,\"1273\":2,\"1274\":2,\"1278\":3,\"1280\":1,\"1283\":1,\"1295\":1,\"1389\":2,\"1390\":2,\"1401\":4,\"1402\":4,\"1408\":4,\"1409\":4,\"1420\":2,\"1466\":2,\"1467\":2,\"1513\":4,\"1526\":6,\"1548\":4,\"1549\":4,\"1551\":4,\"1553\":4,\"1582\":4,\"1592\":4,\"1593\":2,\"1594\":2,\"1595\":4,\"1596\":4,\"1597\":4,\"1598\":6,\"1599\":4,\"1600\":6,\"1604\":4,\"1605\":6,\"1606\":4,\"1609\":6,\"1614\":4,\"1615\":4,\"1618\":2,\"1624\":4,\"1625\":4,\"1811\":1}}],[\"nonlinear=\",{\"1\":{\"978\":1,\"1199\":1,\"1273\":2,\"1274\":2}}],[\"nonpadding\",{\"1\":{\"749\":1}}],[\"nonsplit\",{\"1\":{\"481\":2,\"2275\":1,\"2293\":1,\"2336\":1}}],[\"noneordict\",{\"1\":{\"1268\":1}}],[\"noneorint\",{\"1\":{\"1029\":1,\"1070\":1,\"1071\":1,\"1073\":1,\"1235\":1,\"1279\":2,\"1281\":1,\"1282\":1}}],[\"nonepredictor\",{\"0\":{\"1221\":1},\"1\":{\"1221\":1}}],[\"nonenorm2d\",{\"0\":{\"1219\":1},\"1\":{\"1219\":1}}],[\"nonecorrector\",{\"0\":{\"1218\":1},\"1\":{\"1218\":1}}],[\"none=\",{\"1\":{\"631\":2,\"674\":1,\"1432\":1,\"2340\":6}}],[\"none\",{\"0\":{\"2484\":1,\"2491\":1,\"2492\":1,\"2504\":1},\"1\":{\"142\":2,\"243\":2,\"246\":1,\"267\":6,\"269\":1,\"270\":1,\"271\":1,\"276\":4,\"278\":1,\"279\":1,\"280\":1,\"285\":2,\"286\":5,\"287\":1,\"288\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"377\":1,\"389\":1,\"396\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"442\":1,\"449\":2,\"463\":2,\"469\":1,\"481\":2,\"498\":1,\"536\":28,\"614\":10,\"616\":3,\"617\":3,\"618\":3,\"619\":4,\"620\":5,\"621\":4,\"622\":4,\"623\":4,\"624\":3,\"625\":3,\"626\":1,\"628\":11,\"630\":4,\"631\":10,\"633\":16,\"634\":3,\"636\":5,\"637\":12,\"638\":1,\"639\":4,\"641\":21,\"642\":3,\"643\":5,\"644\":4,\"645\":1,\"646\":1,\"647\":1,\"649\":4,\"651\":21,\"661\":4,\"666\":4,\"668\":1,\"669\":2,\"674\":3,\"678\":3,\"686\":3,\"692\":6,\"696\":4,\"697\":4,\"699\":20,\"700\":4,\"701\":2,\"702\":2,\"706\":2,\"709\":8,\"710\":10,\"711\":10,\"712\":3,\"720\":8,\"733\":10,\"734\":10,\"735\":2,\"736\":7,\"737\":5,\"738\":2,\"743\":10,\"745\":3,\"746\":3,\"747\":3,\"748\":3,\"752\":2,\"755\":1,\"756\":3,\"759\":2,\"760\":4,\"763\":7,\"765\":4,\"770\":5,\"771\":4,\"773\":3,\"774\":5,\"777\":7,\"778\":2,\"780\":5,\"785\":1,\"786\":2,\"787\":14,\"790\":4,\"791\":7,\"792\":4,\"794\":2,\"795\":12,\"796\":1,\"798\":3,\"800\":2,\"815\":3,\"824\":1,\"831\":2,\"832\":2,\"833\":4,\"846\":11,\"847\":12,\"849\":4,\"850\":14,\"851\":4,\"862\":2,\"864\":2,\"865\":4,\"866\":2,\"867\":2,\"908\":1,\"921\":2,\"935\":2,\"954\":8,\"955\":1,\"958\":6,\"959\":1,\"974\":17,\"989\":2,\"1009\":2,\"1022\":3,\"1026\":1,\"1028\":2,\"1030\":2,\"1031\":2,\"1034\":2,\"1040\":2,\"1044\":2,\"1053\":6,\"1061\":2,\"1062\":6,\"1063\":2,\"1064\":1,\"1065\":1,\"1107\":2,\"1112\":4,\"1113\":2,\"1117\":2,\"1118\":2,\"1119\":1,\"1125\":2,\"1126\":5,\"1130\":2,\"1131\":2,\"1136\":2,\"1141\":2,\"1155\":3,\"1156\":5,\"1157\":22,\"1158\":5,\"1162\":2,\"1209\":1,\"1217\":2,\"1222\":2,\"1223\":2,\"1232\":2,\"1250\":6,\"1251\":6,\"1252\":2,\"1253\":2,\"1259\":2,\"1261\":4,\"1262\":1,\"1267\":2,\"1268\":2,\"1269\":4,\"1270\":4,\"1271\":4,\"1273\":1,\"1274\":1,\"1278\":2,\"1279\":2,\"1280\":6,\"1283\":4,\"1290\":2,\"1311\":2,\"1319\":2,\"1322\":2,\"1334\":4,\"1356\":2,\"1389\":4,\"1391\":6,\"1396\":4,\"1401\":4,\"1403\":9,\"1406\":4,\"1408\":2,\"1410\":2,\"1419\":4,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1432\":3,\"1439\":6,\"1441\":8,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":4,\"1452\":4,\"1466\":4,\"1468\":6,\"1469\":2,\"1476\":1,\"1483\":2,\"1485\":1,\"1513\":2,\"1521\":126,\"1525\":1,\"1526\":44,\"1533\":6,\"1539\":3,\"1546\":2,\"1548\":2,\"1551\":2,\"1552\":56,\"1553\":52,\"1556\":1,\"1558\":3,\"1581\":2,\"1582\":2,\"1583\":2,\"1585\":42,\"1592\":4,\"1598\":16,\"1599\":26,\"1600\":4,\"1607\":8,\"1610\":4,\"1611\":2,\"1612\":2,\"1613\":2,\"1616\":4,\"1618\":3,\"1619\":2,\"1624\":2,\"1625\":24,\"1626\":26,\"1628\":6,\"1640\":4,\"1641\":4,\"1642\":2,\"1643\":8,\"1644\":2,\"1645\":6,\"1646\":8,\"1647\":2,\"1648\":2,\"1650\":4,\"1652\":2,\"1656\":4,\"1659\":2,\"1660\":3,\"1662\":9,\"1664\":2,\"1665\":2,\"1668\":2,\"1669\":9,\"1670\":2,\"1671\":2,\"1677\":6,\"1680\":4,\"1683\":4,\"1692\":4,\"1698\":4,\"1700\":2,\"1702\":13,\"1719\":13,\"1720\":2,\"1721\":7,\"1725\":16,\"1748\":2,\"1749\":12,\"1762\":10,\"1766\":1,\"1775\":7,\"1784\":1,\"1787\":1,\"1806\":2,\"1807\":2,\"1815\":12,\"1822\":2,\"1843\":17,\"1847\":2,\"1851\":2,\"1862\":3,\"1870\":2,\"1881\":4,\"1883\":2,\"1919\":6,\"1934\":2,\"1941\":2,\"1943\":2,\"1944\":2,\"1945\":2,\"1947\":4,\"1949\":3,\"1951\":4,\"1953\":1,\"1959\":3,\"1965\":3,\"1975\":34,\"1976\":12,\"1978\":5,\"1980\":8,\"1982\":5,\"1992\":14,\"1993\":22,\"1994\":6,\"1995\":12,\"1996\":3,\"1997\":6,\"1998\":2,\"1999\":2,\"2000\":8,\"2001\":4,\"2002\":2,\"2005\":1,\"2006\":2,\"2007\":2,\"2008\":24,\"2010\":2,\"2011\":2,\"2012\":2,\"2013\":2,\"2015\":2,\"2039\":3,\"2044\":4,\"2065\":2,\"2127\":25,\"2129\":4,\"2130\":12,\"2133\":2,\"2134\":8,\"2136\":18,\"2137\":2,\"2139\":3,\"2144\":3,\"2157\":1,\"2160\":1,\"2161\":1,\"2167\":4,\"2176\":4,\"2183\":2,\"2184\":17,\"2190\":2,\"2191\":4,\"2198\":2,\"2207\":4,\"2208\":2,\"2215\":2,\"2216\":14,\"2219\":8,\"2220\":1,\"2221\":23,\"2226\":6,\"2228\":139,\"2229\":129,\"2232\":15,\"2235\":50,\"2236\":50,\"2238\":24,\"2239\":58,\"2240\":50,\"2245\":54,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":20,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":3,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":2,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":7,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":7,\"2269\":1,\"2270\":7,\"2271\":7,\"2272\":1,\"2273\":1,\"2275\":4,\"2283\":3,\"2284\":2,\"2285\":3,\"2286\":5,\"2287\":2,\"2289\":2,\"2292\":4,\"2293\":16,\"2331\":2,\"2334\":7,\"2336\":30,\"2337\":30,\"2338\":5,\"2339\":2,\"2340\":18,\"2341\":8,\"2342\":4,\"2344\":2,\"2346\":12,\"2347\":3,\"2348\":3,\"2351\":6,\"2353\":8,\"2354\":6,\"2355\":4,\"2356\":23,\"2359\":25,\"2360\":20,\"2361\":20,\"2362\":24,\"2363\":16,\"2364\":6,\"2365\":2,\"2366\":4,\"2367\":13,\"2368\":16,\"2369\":10,\"2370\":6,\"2371\":3,\"2372\":3,\"2381\":3,\"2382\":3,\"2383\":1,\"2384\":3,\"2385\":3,\"2386\":3,\"2387\":2,\"2398\":2,\"2404\":3,\"2405\":8,\"2408\":56,\"2409\":12,\"2411\":22,\"2412\":26,\"2414\":5,\"2416\":8,\"2418\":5,\"2422\":3,\"2423\":26,\"2427\":2,\"2428\":9,\"2431\":22,\"2432\":20,\"2433\":2,\"2434\":8,\"2446\":54,\"2447\":24,\"2453\":1,\"2458\":3,\"2460\":3,\"2462\":21,\"2463\":2,\"2472\":1,\"2473\":1,\"2481\":2,\"2482\":12,\"2484\":3,\"2485\":2,\"2490\":4,\"2491\":2,\"2492\":3,\"2493\":2,\"2495\":4,\"2504\":3,\"2505\":2}}],[\"nonetype\",{\"1\":{\"132\":1}}],[\"non\",{\"0\":{\"74\":1,\"1901\":1,\"2378\":1},\"1\":{\"67\":1,\"104\":2,\"133\":1,\"137\":1,\"163\":1,\"242\":1,\"262\":1,\"284\":1,\"286\":1,\"290\":4,\"481\":4,\"603\":2,\"756\":1,\"773\":1,\"776\":1,\"780\":1,\"781\":1,\"783\":1,\"866\":1,\"867\":1,\"943\":1,\"978\":2,\"982\":1,\"1000\":1,\"1029\":1,\"1070\":1,\"1071\":1,\"1235\":1,\"1267\":1,\"1273\":3,\"1274\":3,\"1279\":2,\"1280\":1,\"1281\":2,\"1282\":1,\"1283\":1,\"1290\":1,\"1374\":1,\"1444\":1,\"1448\":1,\"1804\":1,\"1901\":2,\"1902\":5,\"1974\":1,\"1977\":1,\"1985\":1,\"2134\":1,\"2155\":2,\"2164\":1,\"2275\":2,\"2285\":2,\"2292\":2,\"2293\":2,\"2323\":1,\"2336\":1,\"2337\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2378\":2,\"2420\":1}}],[\"now\",{\"1\":{\"36\":1,\"57\":1,\"107\":1,\"133\":1,\"196\":1,\"213\":1,\"223\":2,\"243\":1,\"268\":1,\"277\":1,\"289\":1,\"700\":2,\"709\":2,\"710\":3,\"711\":3,\"733\":2,\"734\":2,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"771\":1,\"774\":2,\"780\":2,\"790\":1,\"846\":1,\"849\":1,\"1021\":1,\"1062\":1,\"1155\":1,\"1157\":1,\"1334\":1,\"2129\":1,\"2219\":1}}],[\"no\",{\"0\":{\"153\":1,\"2480\":1,\"2507\":2},\"1\":{\"27\":1,\"29\":1,\"53\":2,\"66\":2,\"71\":1,\"79\":1,\"95\":3,\"132\":1,\"196\":3,\"200\":1,\"213\":4,\"218\":5,\"261\":1,\"267\":5,\"268\":9,\"269\":1,\"270\":1,\"276\":7,\"277\":9,\"278\":1,\"279\":1,\"286\":22,\"287\":3,\"288\":1,\"290\":1,\"449\":2,\"481\":3,\"515\":1,\"530\":1,\"532\":1,\"534\":1,\"632\":1,\"674\":8,\"692\":1,\"709\":1,\"710\":1,\"711\":1,\"756\":1,\"773\":1,\"774\":1,\"780\":1,\"786\":1,\"787\":1,\"800\":1,\"809\":1,\"831\":1,\"846\":7,\"849\":1,\"921\":1,\"935\":1,\"1107\":1,\"1278\":1,\"1280\":2,\"1283\":2,\"1309\":1,\"1311\":1,\"1610\":1,\"1628\":1,\"1668\":1,\"1730\":1,\"1735\":1,\"1751\":1,\"1759\":1,\"1801\":2,\"1943\":1,\"1963\":2,\"1992\":1,\"1995\":1,\"2006\":1,\"2020\":1,\"2065\":1,\"2129\":1,\"2133\":1,\"2134\":1,\"2220\":8,\"2276\":1,\"2277\":1,\"2281\":1,\"2307\":1,\"2348\":1,\"2355\":3,\"2370\":2,\"2372\":1,\"2480\":1,\"2507\":3}}],[\"novel\",{\"1\":{\"15\":1}}],[\"nodeid\",{\"1\":{\"2384\":1,\"2385\":1}}],[\"node54\",{\"1\":{\"1308\":1}}],[\"nodes\",{\"0\":{\"2385\":1},\"1\":{\"54\":2,\"121\":1,\"173\":1,\"223\":1,\"243\":1,\"374\":2,\"2385\":2}}],[\"nodev\",{\"1\":{\"50\":1}}],[\"nodejs==22\",{\"1\":{\"31\":1}}],[\"node\",{\"0\":{\"56\":1,\"62\":1,\"64\":1,\"2384\":1},\"1\":{\"3\":1,\"54\":3,\"58\":1,\"106\":1,\"121\":1,\"173\":1,\"2384\":3,\"2385\":1}}],[\"notation\",{\"1\":{\"1126\":1,\"1127\":1}}],[\"notably\",{\"1\":{\"262\":2}}],[\"notable\",{\"1\":{\"84\":1}}],[\"notset\",{\"1\":{\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"377\":1,\"385\":1,\"389\":1,\"396\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":1,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"511\":1}}],[\"notimplementederror\",{\"1\":{\"2130\":1}}],[\"notime\",{\"1\":{\"243\":1,\"2361\":1}}],[\"notimestamps\",{\"1\":{\"242\":1,\"243\":1}}],[\"notice\",{\"1\":{\"276\":1}}],[\"nothing\",{\"1\":{\"96\":1,\"1218\":1,\"1221\":1,\"2188\":1}}],[\"noted\",{\"1\":{\"217\":1,\"269\":1,\"278\":1,\"1031\":1,\"1035\":1,\"1112\":1,\"1113\":1,\"1250\":1,\"1251\":1}}],[\"notes\",{\"0\":{\"46\":1,\"52\":1,\"135\":1},\"1\":{\"104\":1,\"242\":1,\"269\":2,\"278\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1,\"1668\":1,\"2049\":1,\"2145\":1,\"2162\":1,\"2354\":1}}],[\"note\",{\"0\":{\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"705\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"804\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"932\":1,\"934\":1,\"951\":1,\"953\":1,\"957\":1,\"961\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"995\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1678\":1,\"1753\":1,\"1755\":1,\"1812\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2224\":1,\"2225\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2405\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1},\"1\":{\"24\":1,\"32\":1,\"39\":2,\"41\":1,\"44\":1,\"46\":1,\"55\":1,\"57\":1,\"58\":1,\"67\":2,\"68\":1,\"69\":1,\"71\":3,\"74\":1,\"84\":1,\"91\":1,\"95\":1,\"98\":1,\"107\":1,\"110\":1,\"118\":1,\"121\":1,\"128\":2,\"139\":2,\"144\":1,\"145\":2,\"148\":1,\"162\":1,\"164\":1,\"168\":1,\"173\":2,\"175\":1,\"196\":2,\"197\":2,\"201\":1,\"211\":2,\"213\":2,\"232\":1,\"242\":1,\"243\":4,\"263\":1,\"266\":1,\"267\":7,\"268\":4,\"269\":9,\"275\":1,\"276\":2,\"277\":4,\"278\":9,\"285\":1,\"286\":3,\"290\":1,\"516\":1,\"523\":1,\"524\":1,\"525\":1,\"536\":1,\"691\":2,\"706\":1,\"737\":1,\"768\":1,\"790\":1,\"824\":1,\"922\":1,\"980\":1,\"992\":1,\"994\":1,\"995\":1,\"1008\":1,\"1016\":1,\"1021\":1,\"1107\":1,\"1118\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1155\":6,\"1156\":2,\"1157\":6,\"1162\":1,\"1209\":1,\"1217\":1,\"1224\":1,\"1225\":1,\"1228\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1278\":1,\"1328\":1,\"1354\":1,\"1444\":1,\"1448\":1,\"1545\":1,\"1556\":6,\"1640\":1,\"1641\":1,\"1655\":2,\"1679\":1,\"1692\":1,\"1697\":1,\"1698\":1,\"1750\":3,\"1943\":1,\"1959\":1,\"1962\":1,\"1975\":1,\"1997\":1,\"2000\":1,\"2001\":1,\"2016\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2127\":1,\"2130\":2,\"2131\":1,\"2136\":1,\"2221\":1,\"2240\":2,\"2249\":2,\"2253\":2,\"2355\":7,\"2377\":1}}],[\"notebooks\",{\"0\":{\"176\":1}}],[\"notebook\",{\"1\":{\"3\":2,\"208\":1,\"290\":1,\"536\":2}}],[\"notebook2rst\",{\"1\":{\"3\":1}}],[\"not\",{\"1\":{\"3\":1,\"26\":1,\"32\":1,\"39\":1,\"41\":1,\"46\":2,\"51\":1,\"55\":1,\"57\":1,\"63\":1,\"66\":1,\"70\":1,\"81\":1,\"82\":4,\"94\":1,\"96\":2,\"101\":1,\"102\":1,\"104\":1,\"106\":2,\"110\":1,\"123\":1,\"138\":2,\"144\":1,\"150\":1,\"153\":1,\"159\":1,\"163\":1,\"168\":2,\"173\":1,\"175\":2,\"197\":3,\"200\":1,\"210\":1,\"213\":1,\"232\":1,\"242\":4,\"243\":5,\"248\":1,\"259\":1,\"260\":1,\"267\":1,\"269\":1,\"276\":1,\"278\":1,\"285\":1,\"286\":1,\"287\":2,\"290\":4,\"536\":1,\"620\":2,\"691\":1,\"699\":1,\"700\":2,\"703\":4,\"709\":2,\"710\":3,\"711\":3,\"733\":2,\"734\":2,\"738\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"756\":4,\"760\":2,\"770\":2,\"771\":1,\"773\":4,\"774\":2,\"780\":2,\"785\":1,\"798\":1,\"817\":1,\"819\":1,\"821\":1,\"831\":4,\"846\":2,\"849\":1,\"862\":1,\"866\":2,\"867\":2,\"922\":1,\"943\":1,\"959\":2,\"974\":2,\"980\":1,\"1022\":1,\"1051\":1,\"1061\":2,\"1062\":3,\"1063\":1,\"1064\":1,\"1107\":1,\"1112\":1,\"1113\":1,\"1118\":1,\"1126\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1155\":2,\"1156\":1,\"1157\":2,\"1158\":1,\"1162\":2,\"1209\":1,\"1210\":1,\"1217\":1,\"1222\":1,\"1223\":1,\"1224\":1,\"1225\":1,\"1232\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1261\":1,\"1262\":1,\"1267\":1,\"1268\":1,\"1269\":3,\"1270\":3,\"1271\":3,\"1274\":1,\"1278\":2,\"1280\":1,\"1283\":1,\"1290\":1,\"1301\":1,\"1328\":1,\"1334\":2,\"1356\":1,\"1372\":1,\"1376\":1,\"1377\":1,\"1419\":1,\"1420\":1,\"1469\":1,\"1519\":2,\"1526\":3,\"1529\":3,\"1535\":3,\"1549\":1,\"1553\":3,\"1581\":1,\"1586\":1,\"1588\":1,\"1598\":3,\"1600\":3,\"1603\":1,\"1613\":1,\"1616\":1,\"1625\":3,\"1638\":1,\"1668\":1,\"1676\":1,\"1679\":1,\"1683\":1,\"1686\":1,\"1691\":1,\"1694\":1,\"1704\":2,\"1705\":1,\"1706\":1,\"1707\":3,\"1710\":1,\"1711\":2,\"1712\":1,\"1713\":2,\"1714\":2,\"1715\":1,\"1716\":1,\"1756\":4,\"1757\":4,\"1768\":1,\"1784\":1,\"1789\":4,\"1790\":4,\"1794\":1,\"1801\":2,\"1883\":2,\"1898\":1,\"1919\":1,\"1971\":2,\"1976\":1,\"2006\":2,\"2007\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2065\":1,\"2129\":1,\"2130\":4,\"2146\":1,\"2162\":2,\"2222\":2,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2249\":1,\"2280\":1,\"2335\":1,\"2350\":1,\"2354\":2,\"2355\":6,\"2376\":1,\"2380\":1,\"2403\":2,\"2405\":1,\"2411\":2,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2445\":2,\"2447\":1,\"2474\":3,\"2477\":1,\"2480\":1,\"2508\":1}}],[\"nargs=none\",{\"1\":{\"2478\":1}}],[\"nakatani\",{\"1\":{\"1309\":1,\"1311\":1}}],[\"nachmani\",{\"1\":{\"1252\":1}}],[\"nat\",{\"1\":{\"1974\":1,\"1985\":1}}],[\"native\",{\"0\":{\"1370\":1},\"1\":{\"792\":1,\"1370\":1}}],[\"natsume\",{\"1\":{\"269\":3,\"278\":3}}],[\"naturalness\",{\"1\":{\"246\":1,\"247\":2}}],[\"natural\",{\"1\":{\"200\":2,\"246\":1,\"1750\":1,\"1758\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"2431\":1}}],[\"naming\",{\"1\":{\"1009\":2}}],[\"namine\",{\"1\":{\"269\":1,\"278\":1}}],[\"name==\",{\"1\":{\"2311\":1}}],[\"name=none\",{\"1\":{\"1066\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1175\":1,\"1210\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1275\":1,\"1277\":1}}],[\"name=\",{\"1\":{\"907\":1,\"2249\":1}}],[\"name|default=root\",{\"1\":{\"162\":1}}],[\"name|default=venv\",{\"1\":{\"162\":1}}],[\"namespace=none\",{\"1\":{\"2474\":1}}],[\"namespace\",{\"1\":{\"78\":1,\"82\":1,\"1860\":1,\"1878\":1,\"1892\":1,\"2246\":3,\"2247\":3,\"2248\":3,\"2249\":19,\"2250\":3,\"2251\":3,\"2252\":3,\"2253\":4,\"2254\":4,\"2255\":4,\"2256\":4,\"2257\":3,\"2259\":3,\"2260\":3,\"2261\":3,\"2262\":3,\"2263\":3,\"2264\":3,\"2265\":2,\"2266\":3,\"2267\":3,\"2268\":3,\"2269\":3,\"2270\":3,\"2271\":3,\"2272\":3,\"2273\":4,\"2325\":1,\"2327\":1,\"2338\":1,\"2347\":1,\"2369\":1,\"2371\":1,\"2479\":4,\"2483\":1,\"2485\":4,\"2493\":4,\"2500\":1,\"2505\":4}}],[\"names\",{\"0\":{\"2089\":1},\"1\":{\"52\":1,\"67\":1,\"78\":2,\"81\":9,\"243\":1,\"461\":3,\"912\":1,\"1053\":1,\"1719\":1,\"1725\":2,\"1950\":1,\"2132\":5,\"2141\":1,\"2215\":1,\"2246\":8,\"2247\":2,\"2248\":8,\"2249\":8,\"2250\":8,\"2251\":8,\"2252\":8,\"2253\":8,\"2254\":8,\"2255\":8,\"2256\":8,\"2257\":8,\"2259\":8,\"2260\":8,\"2261\":8,\"2262\":4,\"2263\":8,\"2264\":8,\"2265\":8,\"2266\":8,\"2267\":8,\"2268\":8,\"2269\":8,\"2270\":8,\"2271\":8,\"2272\":8,\"2273\":8,\"2324\":1,\"2336\":1,\"2337\":1,\"2343\":1,\"2344\":1,\"2345\":1,\"2352\":1,\"2366\":1,\"2432\":1}}],[\"namedtuple\",{\"1\":{\"1722\":1,\"1807\":1}}],[\"named\",{\"0\":{\"153\":1,\"294\":1,\"296\":1,\"302\":1,\"310\":1,\"316\":1,\"322\":1,\"328\":1,\"332\":1,\"336\":1,\"343\":1,\"350\":1,\"357\":1,\"362\":1,\"369\":1,\"373\":1,\"376\":1,\"378\":1,\"386\":1,\"390\":1,\"397\":1,\"405\":1,\"407\":1,\"416\":1,\"422\":1,\"430\":1,\"437\":1,\"443\":1,\"450\":1,\"458\":1,\"462\":1,\"464\":1,\"470\":1,\"476\":1,\"482\":1,\"485\":1,\"491\":1,\"497\":1,\"499\":1,\"506\":1,\"512\":1,\"540\":1,\"542\":1,\"544\":1,\"547\":1,\"550\":1,\"553\":1,\"560\":1,\"563\":1,\"566\":1,\"569\":1,\"574\":1,\"577\":1,\"580\":1,\"585\":1,\"588\":1,\"591\":1,\"595\":1,\"597\":1,\"599\":1,\"602\":1,\"605\":1,\"608\":1,\"613\":1},\"1\":{\"47\":1,\"69\":1,\"86\":1,\"150\":1,\"197\":1,\"403\":14,\"1247\":1,\"2134\":1,\"2184\":1}}],[\"name\",{\"1\":{\"47\":1,\"50\":1,\"60\":1,\"67\":1,\"69\":1,\"78\":1,\"79\":9,\"81\":6,\"82\":1,\"84\":1,\"96\":4,\"97\":4,\"98\":8,\"99\":4,\"100\":4,\"101\":2,\"124\":1,\"125\":1,\"128\":1,\"130\":1,\"134\":3,\"135\":1,\"136\":1,\"162\":1,\"167\":1,\"168\":1,\"212\":1,\"224\":4,\"225\":1,\"242\":2,\"243\":4,\"246\":1,\"259\":3,\"267\":15,\"269\":1,\"276\":17,\"278\":1,\"286\":37,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"368\":2,\"372\":2,\"377\":4,\"385\":2,\"389\":2,\"396\":2,\"404\":2,\"406\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":4,\"457\":2,\"463\":2,\"469\":2,\"475\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":2,\"505\":2,\"511\":2,\"527\":5,\"536\":2,\"543\":1,\"760\":6,\"761\":1,\"762\":1,\"788\":1,\"793\":1,\"874\":1,\"875\":1,\"876\":1,\"903\":1,\"906\":1,\"912\":1,\"951\":1,\"1009\":4,\"1037\":1,\"1053\":3,\"1155\":2,\"1157\":2,\"1174\":2,\"1204\":1,\"1210\":1,\"1253\":2,\"1254\":2,\"1276\":2,\"1313\":1,\"1402\":1,\"1409\":1,\"1467\":1,\"1513\":1,\"1548\":1,\"1551\":1,\"1582\":1,\"1592\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1604\":2,\"1605\":2,\"1606\":3,\"1614\":1,\"1615\":2,\"1619\":1,\"1624\":1,\"1625\":1,\"1655\":1,\"1681\":1,\"1695\":1,\"1824\":1,\"1876\":2,\"1944\":1,\"1948\":2,\"1949\":1,\"1965\":1,\"1968\":1,\"1970\":1,\"1993\":1,\"2044\":3,\"2128\":1,\"2132\":2,\"2134\":3,\"2137\":3,\"2245\":1,\"2249\":1,\"2258\":2,\"2278\":1,\"2287\":1,\"2304\":2,\"2308\":1,\"2311\":5,\"2324\":2,\"2334\":2,\"2336\":2,\"2337\":2,\"2341\":3,\"2342\":1,\"2343\":2,\"2344\":3,\"2346\":4,\"2351\":1,\"2352\":2,\"2355\":3,\"2356\":2,\"2360\":4,\"2361\":4,\"2362\":2,\"2363\":7,\"2366\":3,\"2367\":2,\"2368\":4,\"2431\":1,\"2457\":1}}],[\"name>\",{\"1\":{\"3\":1,\"79\":1,\"114\":1,\"125\":1,\"127\":2}}],[\"naivernnloss\",{\"0\":{\"2237\":1},\"1\":{\"2237\":1}}],[\"naivernndp\",{\"0\":{\"2236\":1},\"1\":{\"2236\":3}}],[\"naivernn\",{\"0\":{\"2235\":1},\"1\":{\"2235\":3}}],[\"naive\",{\"0\":{\"1344\":1,\"1345\":1,\"2235\":2,\"2236\":2,\"2237\":2},\"1\":{\"265\":2,\"267\":6,\"272\":1,\"274\":1,\"276\":3,\"282\":1,\"1132\":1,\"1167\":1,\"1344\":1,\"1345\":1,\"2235\":3,\"2236\":3,\"2237\":2}}],[\"na\",{\"1\":{\"242\":3,\"243\":4,\"1996\":1,\"1997\":1,\"2360\":1,\"2361\":1}}],[\"nanxin\",{\"1\":{\"156\":1}}],[\"nan\",{\"1\":{\"13\":1,\"706\":2,\"994\":1,\"1753\":1,\"1754\":1}}],[\"naoyuki\",{\"1\":{\"11\":1}}],[\"naviecomplexlstm\",{\"0\":{\"1215\":1},\"1\":{\"1215\":1}}],[\"navigating\",{\"1\":{\"245\":1}}],[\"navigation\",{\"1\":{\"3\":1}}],[\"navbar\",{\"1\":{\"3\":2}}],[\"$p\",{\"1\":{\"1224\":2,\"1225\":2,\"1245\":2}}],[\"$t\",{\"1\":{\"1119\":2}}],[\"$time\",{\"1\":{\"1119\":1}}],[\"$f\",{\"1\":{\"1119\":2}}],[\"$nfreqs\",{\"1\":{\"1119\":1}}],[\"$0\",{\"1\":{\"168\":3}}],[\"$cuda\",{\"1\":{\"163\":1}}],[\"$conda\",{\"1\":{\"31\":1}}],[\"$\",{\"1\":{\"22\":2,\"23\":2,\"24\":6,\"25\":2,\"26\":1,\"31\":8,\"32\":2,\"33\":3,\"38\":3,\"39\":4,\"40\":2,\"41\":4,\"47\":2,\"48\":3,\"59\":2,\"60\":2,\"62\":2,\"63\":2,\"64\":2,\"69\":5,\"117\":2,\"126\":13,\"154\":3,\"159\":5,\"161\":13,\"162\":38,\"167\":5,\"174\":1,\"195\":4,\"197\":5,\"200\":1,\"201\":5,\"205\":1,\"206\":4,\"212\":7,\"218\":7,\"223\":8,\"242\":1,\"243\":9,\"255\":4,\"267\":37,\"276\":29,\"286\":52,\"287\":1,\"290\":1,\"536\":1,\"821\":2,\"1119\":2,\"1224\":2,\"1225\":2,\"1245\":2,\"2299\":1}}],[\"$2\",{\"1\":{\"3\":1,\"286\":1}}],[\"$1\",{\"1\":{\"3\":1,\"286\":1}}],[\"hparams\",{\"0\":{\"2070\":1}}],[\"hdim\",{\"1\":{\"1814\":2,\"1816\":3,\"1847\":2}}],[\"hdf5writer\",{\"0\":{\"1773\":1},\"1\":{\"1773\":2,\"1774\":1}}],[\"hdf5reader\",{\"0\":{\"1772\":1},\"1\":{\"1772\":1}}],[\"hdf5\",{\"1\":{\"521\":1,\"524\":1,\"525\":1,\"548\":1,\"551\":1,\"558\":1,\"561\":4,\"567\":2,\"575\":1,\"1772\":2,\"1774\":1,\"1824\":2,\"1825\":2,\"1827\":1,\"1881\":4,\"1882\":2,\"1883\":6}}],[\"hx\",{\"1\":{\"1712\":1}}],[\"hc\",{\"1\":{\"1202\":3,\"1203\":1,\"1255\":1,\"1259\":2,\"1261\":2}}],[\"h=1\",{\"1\":{\"895\":1}}],[\"h=hidden\",{\"1\":{\"819\":1}}],[\"h2\",{\"1\":{\"724\":2,\"725\":2,\"728\":2,\"729\":2,\"744\":2,\"784\":2,\"828\":2,\"829\":2,\"830\":2}}],[\"h1\",{\"1\":{\"724\":2,\"725\":2,\"728\":2,\"729\":2,\"744\":2,\"784\":2,\"828\":2,\"829\":2,\"830\":2}}],[\"hlens\",{\"1\":{\"676\":1,\"692\":2,\"706\":4,\"760\":3,\"770\":2,\"775\":2,\"790\":2,\"796\":1,\"820\":2,\"850\":2,\"956\":1,\"959\":2,\"1750\":4,\"1992\":1,\"1995\":1,\"2223\":2}}],[\"hsun\",{\"1\":{\"2427\":1}}],[\"hs=1\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1}}],[\"hs=false\",{\"1\":{\"692\":1}}],[\"hs\",{\"1\":{\"676\":1,\"692\":9,\"706\":10,\"709\":2,\"734\":1,\"750\":2,\"760\":2,\"770\":2,\"774\":2,\"775\":2,\"790\":2,\"796\":1,\"797\":1,\"820\":2,\"850\":6,\"956\":1,\"959\":2,\"1182\":1,\"1183\":1,\"1184\":1,\"1199\":1,\"1269\":1,\"1270\":2,\"1271\":1,\"1590\":2,\"1704\":4,\"1705\":4,\"1706\":4,\"1707\":4,\"1708\":4,\"1709\":4,\"1710\":4,\"1711\":4,\"1712\":4,\"1713\":4,\"1714\":4,\"1715\":4,\"1716\":4,\"1719\":2,\"1721\":2,\"1722\":3,\"1725\":2,\"1750\":4,\"1768\":4,\"1801\":4,\"1975\":1,\"1992\":3,\"1995\":1,\"2223\":2}}],[\"hört\",{\"1\":{\"287\":1}}],[\"hh\",{\"1\":{\"271\":2,\"280\":2,\"287\":4}}],[\"h5filewrapper\",{\"0\":{\"2349\":1},\"1\":{\"2349\":1}}],[\"h5\",{\"1\":{\"267\":1,\"276\":1,\"286\":1,\"1548\":1,\"1774\":1,\"1824\":1,\"1827\":1,\"1882\":1,\"1883\":3}}],[\"hybrid\",{\"0\":{\"175\":1},\"1\":{\"175\":7,\"736\":1,\"777\":1,\"1729\":1,\"1730\":1,\"1880\":1,\"1997\":1,\"2127\":1,\"2221\":1}}],[\"hypernetwork\",{\"1\":{\"818\":1}}],[\"hyper\",{\"1\":{\"785\":1,\"786\":1,\"817\":1,\"818\":1,\"866\":1,\"882\":1,\"883\":1,\"884\":1,\"921\":1,\"2307\":1}}],[\"hyperparameter\",{\"0\":{\"384\":1,\"456\":1},\"1\":{\"262\":2}}],[\"hyperparameters\",{\"0\":{\"2121\":1},\"1\":{\"243\":1,\"259\":2,\"793\":1,\"1513\":1,\"1548\":1,\"1551\":1,\"1592\":1,\"1596\":1,\"1597\":1,\"1599\":1,\"1604\":2,\"1605\":2,\"1606\":2,\"1614\":1,\"1615\":2,\"1619\":1}}],[\"hyperparams\",{\"1\":{\"243\":1}}],[\"hyps\",{\"0\":{\"1915\":1},\"1\":{\"578\":2,\"583\":3,\"586\":3,\"589\":3,\"614\":2,\"616\":14,\"634\":2,\"641\":2,\"643\":2,\"651\":2,\"696\":9,\"697\":9,\"847\":2,\"1719\":8,\"1720\":4,\"1721\":2,\"1725\":6,\"1726\":4,\"1727\":4,\"1731\":1,\"1749\":2,\"1806\":6,\"1815\":2,\"1843\":2,\"1880\":2,\"1915\":3,\"1920\":2}}],[\"hypothesis\",{\"0\":{\"631\":1,\"763\":1,\"1775\":1},\"1\":{\"175\":3,\"614\":1,\"616\":18,\"628\":3,\"631\":2,\"634\":2,\"641\":2,\"643\":2,\"651\":2,\"696\":16,\"697\":14,\"743\":2,\"763\":2,\"776\":1,\"795\":1,\"847\":4,\"1719\":15,\"1720\":12,\"1721\":9,\"1722\":1,\"1725\":28,\"1726\":2,\"1727\":2,\"1749\":4,\"1762\":2,\"1775\":2,\"1806\":13,\"1807\":1,\"1815\":4,\"1843\":4,\"1862\":1,\"1915\":2,\"1920\":2}}],[\"hypotheses\",{\"1\":{\"45\":2,\"145\":2,\"321\":2,\"614\":2,\"616\":9,\"634\":2,\"641\":2,\"643\":2,\"651\":2,\"696\":3,\"697\":3,\"847\":2,\"1719\":8,\"1720\":1,\"1721\":2,\"1725\":9,\"1726\":1,\"1727\":1,\"1730\":1,\"1749\":2,\"1806\":7,\"1815\":2,\"1843\":2,\"1862\":1,\"1915\":3,\"1920\":5,\"1927\":2}}],[\"hypo\",{\"1\":{\"134\":3,\"136\":1,\"543\":1}}],[\"hyp\",{\"1\":{\"126\":3,\"243\":1,\"581\":1,\"596\":2,\"696\":2,\"697\":1,\"795\":1,\"847\":2,\"1719\":10,\"1720\":3,\"1721\":1,\"1725\":10,\"1749\":2,\"1806\":6,\"1815\":2,\"1843\":2,\"2462\":1}}],[\"hz\",{\"1\":{\"135\":1,\"267\":6,\"536\":4,\"691\":1,\"1112\":1,\"1113\":1,\"1222\":1,\"1223\":1,\"1250\":1,\"1251\":1,\"1316\":2,\"1320\":1,\"1413\":1,\"1420\":2,\"1545\":1,\"1558\":2,\"1654\":2,\"1662\":2,\"1666\":2,\"1672\":1,\"1673\":1,\"1674\":2,\"1676\":1,\"1678\":1,\"1679\":1,\"1680\":1,\"1686\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1692\":1,\"1694\":1,\"1697\":1,\"1698\":1,\"1717\":1,\"2065\":1}}],[\"h\",{\"1\":{\"134\":2,\"287\":6,\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"374\":1,\"377\":1,\"385\":1,\"389\":1,\"396\":1,\"402\":1,\"403\":14,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":1,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"511\":1,\"513\":1,\"514\":1,\"538\":1,\"541\":1,\"543\":1,\"545\":1,\"548\":1,\"551\":1,\"554\":1,\"556\":1,\"558\":1,\"561\":1,\"564\":1,\"567\":1,\"570\":1,\"572\":1,\"575\":1,\"578\":1,\"581\":1,\"583\":1,\"586\":1,\"589\":1,\"592\":1,\"594\":1,\"596\":1,\"598\":1,\"600\":1,\"603\":1,\"606\":1,\"609\":1,\"611\":1,\"616\":1,\"641\":1,\"644\":15,\"675\":1,\"696\":1,\"697\":1,\"726\":1,\"746\":1,\"819\":9,\"821\":6,\"822\":3,\"823\":3,\"824\":8,\"827\":3,\"847\":1,\"859\":1,\"938\":2,\"972\":2,\"973\":1,\"982\":2,\"1029\":2,\"1061\":1,\"1062\":1,\"1064\":2,\"1075\":2,\"1148\":1,\"1180\":2,\"1181\":2,\"1202\":1,\"1235\":2,\"1259\":1,\"1261\":1,\"1273\":2,\"1274\":2,\"1290\":1,\"1300\":1,\"1301\":4,\"1302\":1,\"1306\":5,\"1308\":2,\"1309\":1,\"1311\":1,\"1318\":2,\"1319\":2,\"1321\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1330\":1,\"1371\":5,\"1372\":4,\"1374\":2,\"1375\":3,\"1509\":1,\"1511\":1,\"1515\":1,\"1516\":1,\"1532\":1,\"1541\":1,\"1552\":6,\"1553\":1,\"1590\":2,\"1601\":4,\"1602\":4,\"1626\":6,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1720\":2,\"1726\":1,\"1727\":1,\"1749\":1,\"1750\":2,\"1768\":1,\"1771\":1,\"1815\":1,\"1854\":4,\"1934\":1,\"2224\":2}}],[\"hf\",{\"0\":{\"2148\":1},\"1\":{\"118\":2,\"127\":2,\"211\":1,\"1053\":1,\"1957\":2,\"2133\":2,\"2136\":4,\"2140\":2,\"2148\":5}}],[\"htk\",{\"1\":{\"720\":1,\"778\":1,\"1662\":3,\"1678\":1,\"1824\":1,\"1980\":1,\"2416\":1}}],[\"htm\",{\"1\":{\"650\":1}}],[\"html\",{\"0\":{\"33\":1},\"1\":{\"31\":1,\"34\":1,\"60\":1,\"67\":1,\"104\":1,\"106\":1,\"196\":1,\"846\":1,\"1061\":1,\"1062\":1,\"1130\":1,\"1131\":1,\"1172\":1,\"1308\":1}}],[\"hts\",{\"1\":{\"269\":1,\"278\":1}}],[\"http\",{\"1\":{\"196\":1,\"1126\":1,\"1717\":1}}],[\"https\",{\"1\":{\"18\":1,\"34\":1,\"60\":1,\"66\":1,\"67\":2,\"69\":1,\"92\":4,\"104\":1,\"106\":1,\"125\":1,\"161\":1,\"162\":1,\"195\":4,\"201\":1,\"213\":1,\"242\":1,\"260\":1,\"268\":1,\"277\":1,\"287\":1,\"289\":1,\"290\":3,\"518\":1,\"536\":2,\"615\":1,\"616\":5,\"617\":1,\"624\":1,\"629\":1,\"634\":1,\"635\":1,\"640\":1,\"643\":1,\"644\":1,\"648\":1,\"650\":1,\"652\":3,\"669\":1,\"670\":1,\"696\":7,\"697\":6,\"699\":1,\"705\":1,\"709\":1,\"711\":1,\"747\":1,\"750\":1,\"768\":1,\"774\":2,\"780\":2,\"785\":1,\"786\":1,\"790\":1,\"791\":1,\"804\":1,\"846\":1,\"864\":1,\"866\":2,\"882\":1,\"883\":1,\"884\":1,\"921\":2,\"922\":3,\"932\":1,\"934\":1,\"936\":1,\"937\":1,\"974\":3,\"1002\":1,\"1053\":3,\"1061\":2,\"1062\":2,\"1066\":1,\"1117\":1,\"1125\":1,\"1130\":1,\"1131\":2,\"1153\":1,\"1170\":1,\"1172\":2,\"1211\":2,\"1252\":1,\"1308\":2,\"1309\":1,\"1311\":1,\"1316\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1327\":2,\"1330\":3,\"1332\":1,\"1350\":1,\"1385\":3,\"1396\":1,\"1411\":1,\"1439\":1,\"1513\":1,\"1515\":1,\"1530\":1,\"1541\":1,\"1548\":1,\"1551\":1,\"1577\":1,\"1589\":1,\"1590\":1,\"1592\":1,\"1605\":1,\"1606\":1,\"1608\":1,\"1668\":2,\"1672\":1,\"1673\":1,\"1683\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1705\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1720\":1,\"1721\":1,\"1731\":2,\"1748\":1,\"1756\":1,\"1757\":1,\"1768\":1,\"1784\":2,\"1785\":2,\"1786\":2,\"1789\":1,\"1790\":1,\"1817\":2,\"1818\":2,\"1820\":1,\"1833\":1,\"1945\":1,\"1963\":1,\"1967\":1,\"1977\":2,\"1994\":1,\"2000\":1,\"2001\":1,\"2016\":1,\"2018\":1,\"2101\":1,\"2151\":1,\"2167\":1,\"2168\":2,\"2176\":2,\"2183\":1,\"2191\":1,\"2198\":1,\"2208\":1,\"2223\":1,\"2227\":1,\"2231\":1,\"2239\":1,\"2245\":1,\"2280\":1,\"2286\":2,\"2310\":1,\"2427\":1,\"2428\":1,\"2462\":1,\"2474\":1,\"2489\":1}}],[\"hier\",{\"1\":{\"2221\":1}}],[\"hierarchical\",{\"1\":{\"260\":1,\"262\":4,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1768\":1,\"1860\":1,\"1895\":1,\"2000\":1}}],[\"hitachi\",{\"1\":{\"1717\":1}}],[\"hinge\",{\"1\":{\"1584\":1,\"1591\":1}}],[\"hindi\",{\"1\":{\"287\":1,\"481\":1}}],[\"history\",{\"1\":{\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"1712\":1,\"2039\":5,\"2049\":1}}],[\"hid\",{\"1\":{\"1124\":2,\"1125\":2,\"1145\":2,\"1147\":2,\"1264\":4,\"1334\":4}}],[\"hiddens\",{\"1\":{\"1995\":1}}],[\"hidden\",{\"1\":{\"43\":6,\"44\":1,\"128\":1,\"141\":7,\"142\":5,\"252\":1,\"614\":9,\"615\":2,\"617\":1,\"619\":4,\"622\":3,\"623\":3,\"624\":1,\"630\":5,\"632\":1,\"633\":2,\"634\":12,\"638\":3,\"640\":2,\"641\":15,\"642\":4,\"643\":14,\"648\":2,\"649\":5,\"651\":11,\"692\":4,\"706\":4,\"709\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"744\":1,\"750\":2,\"774\":1,\"796\":1,\"798\":3,\"820\":1,\"827\":1,\"828\":2,\"829\":4,\"830\":2,\"846\":1,\"847\":15,\"850\":4,\"859\":1,\"862\":3,\"955\":1,\"977\":1,\"979\":1,\"980\":2,\"1029\":4,\"1059\":1,\"1064\":1,\"1117\":1,\"1130\":1,\"1131\":1,\"1133\":3,\"1134\":3,\"1136\":1,\"1137\":3,\"1139\":3,\"1141\":1,\"1145\":1,\"1162\":3,\"1164\":1,\"1176\":5,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":3,\"1199\":1,\"1202\":5,\"1205\":1,\"1208\":3,\"1215\":1,\"1232\":1,\"1235\":1,\"1252\":3,\"1255\":3,\"1257\":3,\"1259\":5,\"1261\":3,\"1262\":1,\"1267\":2,\"1268\":2,\"1269\":3,\"1270\":3,\"1271\":3,\"1279\":3,\"1280\":4,\"1281\":4,\"1282\":1,\"1283\":3,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1406\":1,\"1408\":1,\"1410\":1,\"1462\":1,\"1466\":1,\"1468\":1,\"1511\":1,\"1513\":1,\"1514\":3,\"1517\":1,\"1524\":4,\"1525\":4,\"1534\":3,\"1535\":3,\"1537\":1,\"1546\":3,\"1548\":1,\"1549\":1,\"1551\":1,\"1552\":5,\"1553\":1,\"1554\":1,\"1556\":3,\"1564\":1,\"1568\":1,\"1581\":3,\"1590\":3,\"1592\":1,\"1599\":3,\"1601\":1,\"1611\":4,\"1612\":3,\"1613\":3,\"1622\":1,\"1625\":1,\"1626\":5,\"1704\":3,\"1705\":3,\"1706\":3,\"1707\":2,\"1708\":3,\"1709\":3,\"1710\":3,\"1711\":3,\"1712\":3,\"1713\":3,\"1714\":3,\"1715\":3,\"1716\":3,\"1719\":1,\"1721\":1,\"1725\":1,\"1737\":4,\"1749\":11,\"1750\":4,\"1752\":1,\"1758\":1,\"1768\":3,\"1779\":1,\"1784\":4,\"1795\":4,\"1801\":2,\"1809\":3,\"1814\":1,\"1815\":13,\"1816\":1,\"1843\":8,\"1847\":1,\"1855\":1,\"1856\":8,\"1868\":3,\"1870\":3,\"1871\":3,\"1894\":2,\"1921\":3,\"1934\":1,\"1938\":1,\"1944\":2,\"1945\":1,\"1947\":2,\"1974\":3,\"1977\":3,\"1992\":1,\"1993\":1,\"1995\":2,\"2187\":1,\"2192\":1,\"2203\":1,\"2209\":1,\"2223\":2,\"2224\":1,\"2231\":1,\"2235\":1,\"2236\":1,\"2239\":2,\"2240\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2428\":1,\"2432\":3,\"2447\":2}}],[\"hippo\",{\"1\":{\"821\":1,\"924\":1}}],[\"hifi\",{\"1\":{\"1402\":1,\"1409\":2,\"1467\":1,\"1513\":1,\"1548\":1,\"1551\":1,\"1592\":1,\"1593\":1,\"1594\":2,\"1595\":3,\"1597\":1,\"1598\":1}}],[\"hifiganscalediscriminator\",{\"0\":{\"1597\":1},\"1\":{\"1597\":1}}],[\"hifiganperioddiscriminator\",{\"0\":{\"1596\":1},\"1\":{\"1596\":2}}],[\"hifiganmultiscalemultiperioddiscriminator\",{\"0\":{\"1595\":1},\"1\":{\"1595\":1}}],[\"hifiganmultiscalediscriminator\",{\"0\":{\"1594\":1},\"1\":{\"1594\":1}}],[\"hifiganmultiperioddiscriminator\",{\"0\":{\"1593\":1},\"1\":{\"1593\":2}}],[\"hifigangenerator\",{\"0\":{\"1592\":1},\"1\":{\"1548\":1,\"1551\":1,\"1592\":2}}],[\"hifigan\",{\"0\":{\"1584\":1,\"1587\":1,\"1591\":1,\"1592\":2,\"1593\":2,\"1594\":2,\"1595\":2,\"1596\":2,\"1597\":2,\"1607\":1,\"1614\":1},\"1\":{\"267\":1,\"276\":1,\"286\":17,\"290\":1,\"1526\":2,\"1552\":1,\"1553\":3,\"1584\":1,\"1587\":1,\"1591\":1,\"1592\":3,\"1593\":3,\"1594\":3,\"1595\":3,\"1596\":3,\"1597\":3,\"1598\":1,\"1600\":2,\"1607\":1,\"1614\":2,\"1625\":1}}],[\"hificodecgenerator\",{\"0\":{\"1410\":1},\"1\":{\"1410\":1}}],[\"hificodecdiscriminator\",{\"0\":{\"1409\":1},\"1\":{\"1409\":1}}],[\"hificodec\",{\"0\":{\"1398\":1,\"1404\":1,\"1406\":1,\"1408\":3,\"1409\":2,\"1410\":2,\"1433\":1,\"1435\":1,\"1486\":1,\"1487\":1},\"1\":{\"219\":1,\"1398\":1,\"1404\":1,\"1406\":3,\"1408\":5,\"1409\":4,\"1410\":7,\"1433\":1,\"1435\":1,\"1486\":1,\"1487\":1}}],[\"higher\",{\"1\":{\"2130\":1}}],[\"highest\",{\"1\":{\"1662\":1}}],[\"highpass\",{\"0\":{\"1689\":1},\"1\":{\"1655\":1,\"1689\":2}}],[\"highlight\",{\"1\":{\"1558\":2}}],[\"highly\",{\"1\":{\"246\":1,\"2000\":1}}],[\"high=10\",{\"1\":{\"990\":1}}],[\"highway\",{\"0\":{\"757\":1},\"1\":{\"757\":1}}],[\"high\",{\"1\":{\"45\":1,\"145\":1,\"267\":1,\"276\":1,\"286\":1,\"989\":1,\"1061\":1,\"1062\":1,\"1994\":2,\"2136\":1,\"2240\":1,\"2404\":1,\"2412\":1,\"2423\":1,\"2433\":1}}],[\"higuchi\",{\"1\":{\"8\":1,\"16\":1}}],[\"hira\",{\"1\":{\"10\":1,\"11\":1}}],[\"hirofumi\",{\"1\":{\"10\":2}}],[\"hua2\",{\"1\":{\"287\":1}}],[\"huazhe\",{\"1\":{\"13\":1}}],[\"humanfriendly\",{\"0\":{\"2491\":1},\"1\":{\"2491\":1}}],[\"human\",{\"0\":{\"2151\":1,\"2310\":1},\"1\":{\"245\":1,\"246\":2,\"267\":1,\"276\":1,\"286\":2,\"2143\":1,\"2151\":8,\"2310\":8}}],[\"hugging\",{\"0\":{\"372\":1,\"760\":1,\"761\":1,\"762\":1,\"904\":2,\"905\":2,\"930\":1,\"2049\":1,\"2128\":1,\"2278\":1,\"2279\":1},\"1\":{\"125\":1,\"127\":2,\"200\":2,\"205\":2,\"217\":3,\"222\":1,\"223\":2,\"228\":1,\"235\":1,\"242\":2,\"243\":1,\"284\":1,\"285\":3,\"290\":1,\"301\":4,\"372\":2,\"463\":4,\"469\":2,\"759\":1,\"760\":3,\"761\":2,\"762\":2,\"904\":2,\"905\":2,\"930\":1,\"2043\":1,\"2044\":1,\"2049\":3,\"2055\":1,\"2056\":1,\"2126\":1,\"2128\":2,\"2278\":1,\"2279\":1}}],[\"huggingfacetokenizer\",{\"0\":{\"2279\":1},\"1\":{\"2279\":1}}],[\"huggingfacetokenidconverter\",{\"0\":{\"2278\":1},\"1\":{\"2278\":1}}],[\"huggingfacetextio\",{\"0\":{\"2137\":1},\"1\":{\"2137\":2}}],[\"huggingfacetransformerspostdecoder\",{\"0\":{\"2128\":1},\"1\":{\"2128\":1}}],[\"huggingfacetransformerspostencoder\",{\"0\":{\"762\":1},\"1\":{\"762\":1}}],[\"huggingfacetransformersencoder\",{\"0\":{\"761\":1},\"1\":{\"761\":1}}],[\"huggingfacetransformersdecoder\",{\"0\":{\"760\":1},\"1\":{\"760\":2}}],[\"huggingfacellm\",{\"0\":{\"2049\":1},\"1\":{\"2049\":1}}],[\"huggingfaceoptmodel\",{\"0\":{\"1944\":1},\"1\":{\"1944\":1}}],[\"huggingfacefrontend\",{\"0\":{\"759\":1},\"1\":{\"759\":1}}],[\"huggingface\",{\"0\":{\"759\":1,\"1944\":1},\"1\":{\"118\":1,\"127\":1,\"185\":2,\"210\":1,\"220\":2,\"246\":2,\"247\":1,\"254\":2,\"759\":1,\"1053\":1,\"1153\":1,\"1944\":1,\"1957\":1,\"2054\":1,\"2133\":3,\"2136\":2,\"2137\":4,\"2140\":2,\"2148\":2}}],[\"hub\",{\"1\":{\"26\":1,\"27\":1,\"128\":1,\"235\":1,\"246\":1,\"1053\":1,\"2054\":1}}],[\"hubertcollatefn\",{\"0\":{\"2350\":1},\"1\":{\"2350\":1}}],[\"huberttask\",{\"0\":{\"2257\":1},\"1\":{\"2257\":1}}],[\"hubertloss\",{\"0\":{\"2219\":1},\"1\":{\"2219\":1}}],[\"hubertdecoder\",{\"0\":{\"2218\":1},\"1\":{\"2218\":2}}],[\"hubertpretrainmodel\",{\"0\":{\"1640\":1},\"1\":{\"1640\":1,\"2257\":1}}],[\"hubertpretrainloss\",{\"0\":{\"1638\":1},\"1\":{\"1638\":1}}],[\"hubert1\",{\"1\":{\"232\":3,\"258\":2,\"259\":6}}],[\"hubert11\",{\"1\":{\"231\":1}}],[\"hubert\",{\"0\":{\"259\":1,\"747\":1,\"748\":1,\"846\":1,\"890\":2,\"1638\":2,\"1640\":1,\"1641\":1,\"2218\":1,\"2219\":1,\"2257\":1,\"2527\":1},\"1\":{\"7\":2,\"128\":4,\"231\":1,\"232\":3,\"258\":2,\"259\":6,\"267\":4,\"276\":5,\"281\":1,\"747\":11,\"748\":6,\"846\":6,\"890\":2,\"1552\":3,\"1553\":3,\"1638\":3,\"1640\":2,\"1641\":2,\"1748\":1,\"2216\":1,\"2218\":1,\"2219\":3,\"2257\":1}}],[\"hungarian\",{\"1\":{\"287\":1,\"481\":1}}],[\"hung\",{\"1\":{\"14\":1}}],[\"huo\",{\"1\":{\"13\":1}}],[\"hussen\",{\"1\":{\"8\":1}}],[\"heavy\",{\"1\":{\"1804\":1,\"2130\":1,\"2136\":1}}],[\"heavily\",{\"1\":{\"1668\":1}}],[\"head=4\",{\"1\":{\"1182\":1,\"1183\":1,\"1184\":1,\"1269\":1,\"1270\":1,\"1271\":1}}],[\"headed\",{\"1\":{\"787\":1}}],[\"heads=4\",{\"1\":{\"1029\":1,\"1070\":1,\"1071\":1,\"1139\":1,\"1235\":1,\"1279\":1,\"1281\":1,\"1282\":1}}],[\"heads\",{\"1\":{\"43\":6,\"141\":7,\"142\":3,\"243\":2,\"633\":3,\"634\":3,\"637\":11,\"644\":3,\"674\":4,\"692\":2,\"700\":1,\"709\":3,\"710\":3,\"711\":3,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"746\":1,\"748\":3,\"749\":4,\"766\":1,\"767\":1,\"774\":3,\"775\":1,\"780\":3,\"784\":1,\"787\":3,\"796\":1,\"817\":1,\"821\":1,\"846\":3,\"848\":1,\"849\":3,\"850\":1,\"851\":1,\"869\":1,\"1029\":2,\"1064\":3,\"1070\":2,\"1071\":2,\"1107\":1,\"1139\":3,\"1141\":3,\"1185\":3,\"1235\":2,\"1262\":3,\"1269\":1,\"1270\":2,\"1271\":1,\"1278\":1,\"1279\":3,\"1280\":4,\"1281\":3,\"1282\":3,\"1283\":4,\"1290\":3,\"1519\":3,\"1535\":3,\"1536\":3,\"1546\":3,\"1552\":3,\"1553\":1,\"1598\":1,\"1599\":3,\"1600\":1,\"1622\":3,\"1625\":1,\"1626\":3,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1785\":1,\"1794\":1,\"1817\":1,\"1895\":1,\"1992\":3,\"1995\":3,\"2126\":1,\"2129\":3,\"2191\":3,\"2239\":1,\"2245\":3,\"2407\":1,\"2411\":3,\"2412\":3,\"2423\":3,\"2429\":3,\"2430\":3,\"2431\":3,\"2432\":8}}],[\"head\",{\"0\":{\"637\":1,\"904\":1},\"1\":{\"26\":1,\"43\":2,\"141\":3,\"252\":1,\"637\":1,\"692\":1,\"709\":1,\"710\":1,\"711\":1,\"748\":1,\"760\":3,\"774\":1,\"780\":1,\"784\":8,\"787\":3,\"849\":1,\"904\":1,\"1029\":1,\"1107\":1,\"1235\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1290\":1,\"1713\":6,\"1714\":4,\"1715\":6,\"1716\":9,\"1771\":2,\"1785\":4,\"1794\":8,\"1817\":4,\"1895\":1,\"1947\":1,\"1984\":1,\"1992\":1,\"1995\":1,\"2129\":1,\"2176\":1,\"2191\":1,\"2421\":3,\"2430\":1,\"2432\":1}}],[\"he\",{\"1\":{\"1655\":1,\"2198\":1}}],[\"hermitian\",{\"1\":{\"1308\":3}}],[\"hershey\",{\"1\":{\"1131\":1,\"1172\":1}}],[\"here\",{\"1\":{\"32\":1,\"43\":1,\"126\":1,\"128\":2,\"132\":1,\"138\":1,\"139\":2,\"141\":1,\"144\":1,\"147\":1,\"201\":1,\"206\":1,\"212\":2,\"218\":1,\"242\":2,\"243\":1,\"255\":1,\"260\":1,\"261\":1,\"262\":1,\"267\":4,\"270\":1,\"271\":1,\"276\":2,\"279\":1,\"280\":1,\"286\":3,\"287\":1,\"288\":1,\"290\":1,\"620\":2,\"824\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"922\":1,\"927\":1,\"961\":1,\"986\":1,\"1246\":1,\"1328\":1,\"1397\":1,\"1454\":1,\"1456\":1,\"1938\":1,\"2345\":1,\"2355\":2}}],[\"height=0\",{\"1\":{\"1634\":1,\"1635\":1,\"1636\":1}}],[\"heights\",{\"1\":{\"1634\":1,\"1635\":1,\"1636\":1}}],[\"height\",{\"1\":{\"1290\":1,\"1375\":1}}],[\"heigth\",{\"1\":{\"1064\":1}}],[\"hei\",{\"1\":{\"287\":1}}],[\"hen\",{\"1\":{\"269\":1,\"278\":1}}],[\"hence\",{\"1\":{\"200\":1,\"1454\":1,\"1456\":1}}],[\"hee\",{\"1\":{\"256\":1}}],[\"heo\",{\"1\":{\"256\":1}}],[\"heuristics\",{\"1\":{\"175\":1,\"1822\":1}}],[\"heymann\",{\"1\":{\"156\":1}}],[\"held\",{\"1\":{\"2018\":1}}],[\"helló\",{\"1\":{\"287\":1}}],[\"hello\",{\"1\":{\"80\":1,\"287\":3,\"288\":1,\"290\":8,\"2290\":2}}],[\"help=none\",{\"1\":{\"2478\":1}}],[\"helper\",{\"0\":{\"868\":1,\"880\":1,\"886\":1,\"888\":1,\"897\":1,\"899\":1,\"908\":1,\"909\":1,\"916\":1,\"920\":1,\"923\":1,\"1881\":1,\"1883\":1},\"1\":{\"703\":1,\"755\":1,\"785\":1,\"803\":1,\"868\":1,\"880\":1,\"886\":1,\"888\":1,\"897\":1,\"899\":1,\"908\":1,\"909\":1,\"916\":1,\"920\":1,\"923\":1,\"931\":1,\"933\":1,\"1881\":1,\"1882\":2,\"1883\":3,\"2007\":1,\"2293\":1,\"2334\":1,\"2483\":1}}],[\"helps\",{\"1\":{\"218\":1,\"286\":2,\"290\":1,\"1810\":1,\"1811\":1}}],[\"helpful\",{\"1\":{\"71\":1}}],[\"help\",{\"1\":{\"3\":1,\"80\":2,\"84\":2,\"117\":2,\"134\":2,\"152\":1,\"212\":1,\"223\":1,\"242\":1,\"267\":1,\"276\":1,\"290\":1,\"530\":1,\"532\":1,\"534\":1,\"1246\":1,\"1833\":1}}],[\"heterogeneous\",{\"1\":{\"6\":1,\"2000\":1}}],[\"harmonics\",{\"1\":{\"1524\":1,\"1551\":1}}],[\"harmonic\",{\"0\":{\"1565\":1},\"1\":{\"1524\":9,\"1545\":4,\"1551\":3,\"1552\":3,\"1565\":1}}],[\"harm\",{\"0\":{\"1524\":1},\"1\":{\"1524\":3}}],[\"hardware\",{\"1\":{\"2355\":2}}],[\"hardest\",{\"1\":{\"2176\":2}}],[\"hardness\",{\"1\":{\"1130\":1}}],[\"hard\",{\"1\":{\"262\":2,\"267\":1,\"286\":1,\"2176\":1,\"2345\":1,\"2462\":1}}],[\"hardtanh\",{\"1\":{\"141\":4,\"664\":6}}],[\"haeb\",{\"1\":{\"1318\":1}}],[\"hats\",{\"1\":{\"1509\":1,\"1511\":1,\"1760\":1}}],[\"hat\",{\"1\":{\"1253\":1,\"1419\":2,\"1515\":2,\"1541\":1,\"1584\":2,\"1587\":2,\"1607\":2,\"1760\":4,\"2355\":4}}],[\"hawkaaron\",{\"1\":{\"922\":2,\"936\":2,\"937\":2}}],[\"having\",{\"1\":{\"821\":1,\"980\":1,\"1022\":1,\"1026\":1,\"2304\":1,\"2369\":1}}],[\"have\",{\"0\":{\"61\":1},\"1\":{\"19\":1,\"22\":1,\"31\":1,\"32\":1,\"48\":1,\"52\":1,\"78\":2,\"79\":1,\"84\":1,\"110\":1,\"124\":1,\"128\":1,\"145\":1,\"150\":1,\"152\":1,\"153\":1,\"161\":1,\"162\":4,\"165\":1,\"167\":1,\"175\":2,\"195\":1,\"197\":4,\"200\":2,\"211\":3,\"212\":1,\"213\":1,\"217\":2,\"223\":3,\"224\":1,\"228\":1,\"242\":1,\"243\":1,\"254\":1,\"262\":1,\"263\":2,\"269\":1,\"278\":1,\"285\":1,\"290\":1,\"699\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"744\":1,\"756\":1,\"773\":1,\"784\":1,\"824\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"833\":1,\"866\":1,\"867\":1,\"922\":1,\"927\":1,\"960\":1,\"974\":1,\"994\":1,\"1133\":1,\"1134\":1,\"1137\":1,\"1155\":2,\"1156\":1,\"1157\":2,\"1158\":1,\"1208\":1,\"1255\":1,\"1257\":1,\"1400\":1,\"1441\":1,\"1469\":1,\"1478\":1,\"1962\":2,\"2049\":1,\"2130\":1,\"2131\":1,\"2155\":1,\"2162\":2,\"2327\":2,\"2355\":6}}],[\"hamming\",{\"1\":{\"548\":1,\"551\":1,\"558\":1,\"768\":1,\"778\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1,\"1668\":4}}],[\"happen\",{\"1\":{\"1493\":1,\"1494\":1,\"2355\":1}}],[\"happened\",{\"1\":{\"290\":1}}],[\"happens\",{\"1\":{\"71\":1,\"2136\":1}}],[\"hal\",{\"1\":{\"1327\":2,\"1330\":2}}],[\"halves\",{\"1\":{\"704\":2}}],[\"hallo\",{\"1\":{\"287\":1}}],[\"half\",{\"1\":{\"102\":1,\"704\":1,\"824\":1}}],[\"hack\",{\"1\":{\"232\":1,\"258\":1}}],[\"hayato\",{\"1\":{\"202\":1}}],[\"hayashi2021espnet2\",{\"1\":{\"9\":1}}],[\"hayashi2020espnet\",{\"1\":{\"5\":1,\"9\":1}}],[\"hayashi\",{\"1\":{\"9\":2,\"10\":2,\"11\":1,\"13\":1,\"16\":1,\"156\":1}}],[\"has\",{\"1\":{\"36\":1,\"40\":1,\"43\":1,\"48\":4,\"68\":2,\"69\":1,\"71\":1,\"80\":1,\"82\":1,\"85\":1,\"86\":1,\"96\":1,\"98\":1,\"99\":1,\"100\":1,\"117\":1,\"132\":1,\"133\":2,\"140\":1,\"141\":2,\"147\":1,\"174\":1,\"194\":1,\"196\":2,\"213\":1,\"242\":2,\"243\":1,\"267\":3,\"268\":2,\"276\":2,\"277\":2,\"286\":3,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"738\":1,\"744\":1,\"756\":1,\"760\":2,\"773\":1,\"784\":2,\"785\":1,\"787\":1,\"797\":1,\"820\":1,\"828\":1,\"829\":1,\"830\":1,\"851\":1,\"866\":1,\"867\":1,\"978\":1,\"1002\":1,\"1246\":1,\"1350\":1,\"1654\":1,\"1719\":4,\"1720\":2,\"1723\":1,\"1725\":4,\"1731\":2,\"1784\":1,\"1805\":1,\"1806\":2,\"1822\":1,\"1883\":1,\"2130\":1,\"2133\":2,\"2136\":2,\"2143\":1,\"2188\":1,\"2324\":1,\"2343\":1,\"2344\":1,\"2352\":1,\"2355\":4,\"2359\":1,\"2366\":1,\"2439\":1,\"2474\":1}}],[\"hanning\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1}}],[\"hann\",{\"1\":{\"548\":1,\"551\":1,\"558\":1,\"720\":1,\"1250\":1,\"1251\":1,\"1269\":1,\"1270\":1,\"1334\":1,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1419\":1,\"1466\":1,\"1526\":1,\"1553\":1,\"1598\":1,\"1600\":1,\"1607\":1,\"1625\":1,\"1669\":1,\"1680\":1,\"1692\":1,\"1698\":1,\"1776\":1,\"1791\":1,\"1832\":1,\"1835\":1,\"1899\":1,\"1900\":1,\"1923\":1,\"1924\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"2232\":1,\"2238\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2482\":1,\"2490\":1}}],[\"hankaku\",{\"1\":{\"290\":1}}],[\"handling\",{\"1\":{\"1155\":1,\"1157\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1505\":1,\"1506\":1,\"2143\":1}}],[\"handler\",{\"1\":{\"1842\":1,\"2130\":1,\"2136\":1,\"2311\":4}}],[\"handles\",{\"1\":{\"263\":2,\"768\":1,\"2039\":1,\"2044\":6,\"2130\":2,\"2131\":1,\"2133\":1,\"2136\":1,\"2287\":1,\"2355\":1}}],[\"handled\",{\"1\":{\"247\":1,\"2130\":1}}],[\"handle\",{\"1\":{\"71\":1,\"102\":1,\"269\":1,\"278\":1,\"284\":1,\"290\":1,\"756\":1,\"773\":1,\"858\":1,\"978\":1,\"1062\":1,\"1228\":1,\"2044\":5,\"2130\":1,\"2176\":1,\"2355\":1}}],[\"hand\",{\"1\":{\"197\":1}}],[\"han\",{\"1\":{\"5\":1,\"796\":7,\"869\":7,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1768\":2,\"1860\":3,\"1895\":2}}],[\"houlsby\",{\"0\":{\"1657\":2,\"1682\":1,\"1684\":1},\"1\":{\"1657\":2,\"1682\":1,\"1684\":2}}],[\"hour\",{\"1\":{\"196\":1,\"268\":1,\"277\":1}}],[\"hook\",{\"1\":{\"821\":1,\"824\":1,\"2348\":1,\"2355\":1,\"2370\":2,\"2372\":1}}],[\"hooks\",{\"0\":{\"1963\":1},\"1\":{\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1963\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"holds\",{\"1\":{\"717\":1}}],[\"hold\",{\"1\":{\"469\":2,\"697\":1,\"1720\":1,\"1727\":1,\"2018\":6}}],[\"hola\",{\"1\":{\"287\":1}}],[\"hogehoge\",{\"1\":{\"290\":1}}],[\"hot\",{\"1\":{\"289\":2,\"1803\":2,\"2184\":1}}],[\"horizon\",{\"1\":{\"844\":1}}],[\"hori\",{\"1\":{\"156\":1}}],[\"hop\",{\"1\":{\"130\":2,\"243\":1,\"335\":2,\"342\":2,\"361\":2,\"710\":3,\"711\":3,\"720\":1,\"778\":1,\"831\":2,\"976\":1,\"1128\":1,\"1210\":3,\"1250\":2,\"1251\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1385\":3,\"1389\":1,\"1390\":1,\"1392\":3,\"1396\":1,\"1397\":3,\"1401\":1,\"1402\":1,\"1408\":1,\"1409\":1,\"1413\":3,\"1420\":1,\"1422\":3,\"1466\":1,\"1467\":1,\"1511\":1,\"1524\":4,\"1525\":4,\"1526\":1,\"1534\":4,\"1547\":1,\"1549\":1,\"1551\":1,\"1552\":2,\"1553\":1,\"1559\":3,\"1598\":1,\"1600\":1,\"1607\":3,\"1625\":1,\"1660\":1,\"1669\":1,\"1680\":3,\"1692\":3,\"1698\":3,\"1720\":1,\"1721\":5,\"1978\":1,\"1980\":1,\"1982\":1,\"2232\":1,\"2238\":1,\"2363\":1,\"2404\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1}}],[\"hope\",{\"1\":{\"3\":1}}],[\"hosted\",{\"1\":{\"2044\":1}}],[\"hostedtoolcache\",{\"1\":{\"1701\":1}}],[\"hosts\",{\"1\":{\"120\":1}}],[\"host>\",{\"1\":{\"60\":3}}],[\"host2\",{\"1\":{\"58\":1,\"61\":1,\"64\":1}}],[\"host1\",{\"1\":{\"58\":3,\"61\":5,\"64\":1}}],[\"hostxn\",{\"1\":{\"54\":2}}],[\"host\",{\"0\":{\"58\":1},\"1\":{\"54\":9,\"62\":1,\"64\":1,\"66\":1,\"113\":3,\"374\":2}}],[\"home\",{\"1\":{\"50\":1,\"1717\":1,\"2311\":1}}],[\"homepage\",{\"0\":{\"0\":1,\"1\":1},\"1\":{\"0\":2,\"1\":2,\"2\":1,\"3\":1}}],[\"how\",{\"0\":{\"48\":1,\"49\":1,\"117\":1,\"150\":1,\"152\":1,\"156\":1,\"197\":1,\"201\":1,\"206\":1,\"212\":1,\"218\":1,\"236\":1,\"243\":1,\"247\":1,\"255\":1,\"267\":1,\"276\":1,\"286\":1},\"1\":{\"107\":1,\"124\":1,\"139\":1,\"161\":1,\"162\":2,\"178\":1,\"182\":1,\"190\":1,\"191\":1,\"193\":1,\"194\":1,\"198\":1,\"199\":1,\"204\":1,\"210\":1,\"216\":1,\"227\":1,\"234\":1,\"240\":2,\"243\":1,\"247\":1,\"253\":1,\"265\":1,\"274\":1,\"284\":15,\"286\":2,\"290\":15,\"516\":1,\"520\":1,\"521\":1,\"523\":1,\"524\":1,\"525\":1,\"535\":1,\"537\":1,\"846\":2,\"927\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1599\":2,\"1748\":1,\"1770\":1,\"1771\":2,\"1992\":1,\"1993\":1,\"1995\":1,\"2000\":1,\"2001\":1,\"2220\":2,\"2235\":3,\"2236\":3,\"2239\":2,\"2240\":2,\"2245\":1,\"2355\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":1,\"2432\":3,\"2447\":2}}],[\"however\",{\"1\":{\"26\":1,\"27\":1,\"50\":1,\"70\":1,\"145\":1,\"217\":1,\"232\":1,\"242\":1,\"258\":1,\"285\":1,\"1334\":1,\"2220\":1,\"2355\":1}}],[\"w=true\",{\"1\":{\"1750\":1,\"2223\":1}}],[\"wn\",{\"0\":{\"1554\":1},\"1\":{\"1554\":1}}],[\"wnconv2d\",{\"0\":{\"1472\":1},\"1\":{\"1472\":1}}],[\"wnconv1d\",{\"0\":{\"1471\":1},\"1\":{\"1471\":1}}],[\"wnormalization\",{\"1\":{\"1217\":1}}],[\"wnonlinear\",{\"1\":{\"1217\":1}}],[\"wnet\",{\"1\":{\"1217\":1}}],[\"wmpdr\",{\"1\":{\"1126\":2,\"1354\":1}}],[\"wtype\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1127\":1,\"1539\":1,\"1766\":1}}],[\"wpd\",{\"0\":{\"1309\":1,\"1310\":1,\"1311\":1,\"1351\":1},\"1\":{\"1126\":2,\"1309\":3,\"1310\":3,\"1311\":3,\"1351\":2,\"1354\":3,\"1356\":1}}],[\"wprojs\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1127\":1,\"1217\":1,\"1539\":1,\"1766\":1}}],[\"wpe\",{\"0\":{\"1127\":2,\"1314\":1,\"1317\":1,\"1325\":1,\"1352\":1,\"1376\":2,\"1377\":2,\"1853\":2},\"1\":{\"223\":2,\"720\":3,\"738\":2,\"815\":2,\"1127\":4,\"1217\":5,\"1314\":1,\"1317\":1,\"1325\":1,\"1352\":1,\"1376\":3,\"1377\":3,\"1539\":2,\"1766\":2,\"1853\":4}}],[\"wdropout\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1217\":1,\"1539\":1,\"1766\":1}}],[\"w2v=false\",{\"1\":{\"745\":1}}],[\"w2v\",{\"0\":{\"891\":1},\"1\":{\"232\":1,\"258\":1,\"745\":4,\"891\":1,\"2216\":1}}],[\"wkvlinearattention\",{\"0\":{\"654\":1},\"1\":{\"654\":4}}],[\"wkv\",{\"0\":{\"668\":1},\"1\":{\"142\":1,\"642\":1,\"643\":1,\"649\":3,\"668\":2}}],[\"wshare\",{\"1\":{\"731\":1,\"732\":1,\"766\":1,\"767\":1,\"1756\":2,\"1757\":2,\"1789\":2,\"1790\":2}}],[\"wspecifier\",{\"0\":{\"1911\":1},\"1\":{\"548\":1,\"551\":1,\"561\":1,\"606\":1,\"1773\":1,\"1781\":1,\"1826\":1,\"1829\":1,\"1883\":2,\"1911\":3,\"1912\":1}}],[\"wsl\",{\"1\":{\"160\":1}}],[\"ws\",{\"1\":{\"114\":1,\"286\":2,\"1126\":1,\"1293\":3,\"1770\":2,\"1771\":2,\"1861\":2,\"1990\":1,\"1993\":1,\"2407\":2}}],[\"wsj\",{\"1\":{\"24\":1,\"38\":1,\"108\":1,\"110\":1,\"126\":1,\"290\":1}}],[\"wsj1\",{\"1\":{\"24\":1,\"110\":1}}],[\"wsj0\",{\"1\":{\"24\":1,\"110\":1,\"223\":1,\"224\":2}}],[\"wraps\",{\"1\":{\"927\":1}}],[\"wrap=none\",{\"1\":{\"912\":1}}],[\"wrap\",{\"1\":{\"858\":1,\"912\":3,\"943\":1}}],[\"wrapped\",{\"1\":{\"2014\":1,\"2015\":1,\"2018\":1,\"2304\":1}}],[\"wrappers\",{\"0\":{\"1042\":1,\"1132\":1,\"1167\":1,\"1204\":1,\"1209\":1,\"1228\":1},\"1\":{\"223\":1,\"1042\":1,\"1132\":1,\"1155\":2,\"1157\":3,\"1158\":1,\"1167\":1,\"1204\":1,\"1209\":2,\"1228\":2}}],[\"wrapper\",{\"0\":{\"1042\":1},\"1\":{\"119\":1,\"197\":1,\"223\":1,\"263\":1,\"287\":13,\"290\":1,\"699\":1,\"738\":1,\"759\":1,\"794\":2,\"821\":1,\"830\":1,\"922\":1,\"936\":1,\"937\":1,\"1042\":2,\"1155\":1,\"1157\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1493\":1,\"1494\":1,\"1731\":1,\"1761\":1,\"1763\":1,\"1778\":1,\"1797\":1,\"1819\":1,\"1823\":1,\"1834\":1,\"1848\":1,\"2280\":1,\"2286\":1,\"2422\":1}}],[\"wrapping\",{\"1\":{\"68\":1,\"2134\":1,\"2304\":1}}],[\"wrt\",{\"1\":{\"703\":1,\"755\":1,\"785\":1,\"927\":1}}],[\"wrong\",{\"1\":{\"265\":2,\"269\":3,\"274\":2,\"278\":3}}],[\"written\",{\"1\":{\"32\":1,\"47\":1,\"110\":1,\"290\":1,\"2354\":3}}],[\"writing\",{\"1\":{\"26\":1,\"141\":1}}],[\"writer\",{\"0\":{\"985\":1,\"1883\":1,\"1889\":1},\"1\":{\"985\":2,\"986\":2,\"998\":1,\"999\":3,\"1005\":1,\"1006\":3,\"1009\":1,\"1010\":5,\"1013\":1,\"1014\":3,\"1017\":1,\"1018\":3,\"1718\":1,\"1774\":1,\"1781\":2,\"1827\":1,\"1830\":1,\"1883\":3,\"1889\":1,\"1890\":1,\"2347\":1,\"2359\":1,\"2367\":1,\"2369\":2,\"2371\":1}}],[\"writers\",{\"0\":{\"1718\":1,\"1773\":1,\"1781\":1,\"1826\":1,\"1829\":1,\"1883\":1,\"1889\":1,\"1911\":1},\"1\":{\"269\":1,\"278\":1,\"1718\":1,\"1773\":1,\"1781\":1,\"1826\":1,\"1829\":1,\"1883\":1,\"1889\":1,\"1911\":1}}],[\"writes\",{\"1\":{\"803\":1}}],[\"write\",{\"0\":{\"483\":1},\"1\":{\"1\":1,\"31\":1,\"150\":2,\"197\":2,\"222\":1,\"224\":2,\"243\":2,\"266\":1,\"267\":2,\"275\":1,\"286\":3,\"481\":2,\"535\":2,\"548\":2,\"551\":2,\"561\":2,\"986\":1,\"1773\":1,\"1781\":1,\"1826\":1,\"1829\":1,\"1883\":5,\"1889\":1,\"1951\":1}}],[\"ww\",{\"1\":{\"67\":2,\"1290\":2}}],[\"www\",{\"1\":{\"67\":1,\"650\":1,\"1130\":1,\"1131\":1,\"1172\":1,\"1308\":1,\"2168\":2,\"2176\":1,\"2208\":1}}],[\"wlayers\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1127\":1,\"1217\":1,\"1539\":1,\"1766\":1}}],[\"wlan\",{\"1\":{\"67\":1}}],[\"wl\",{\"1\":{\"67\":2}}],[\"wo\",{\"0\":{\"533\":1,\"589\":1},\"1\":{\"533\":1,\"589\":1}}],[\"wook\",{\"1\":{\"202\":1}}],[\"wonder\",{\"1\":{\"124\":1}}],[\"worth\",{\"1\":{\"819\":1}}],[\"wordtokenizer\",{\"0\":{\"2292\":1},\"1\":{\"2292\":1}}],[\"wordlm\",{\"1\":{\"1871\":2,\"1921\":2}}],[\"word\",{\"0\":{\"2292\":1},\"1\":{\"200\":1,\"205\":1,\"211\":1,\"242\":1,\"246\":1,\"259\":2,\"267\":1,\"276\":1,\"287\":3,\"301\":4,\"315\":4,\"389\":5,\"396\":4,\"421\":5,\"429\":5,\"442\":4,\"463\":8,\"469\":4,\"481\":1,\"498\":4,\"505\":7,\"625\":1,\"760\":1,\"1871\":1,\"1921\":1,\"1945\":1,\"2280\":1,\"2286\":1,\"2292\":1}}],[\"words\",{\"1\":{\"96\":1,\"252\":1,\"262\":4,\"572\":1,\"2325\":1}}],[\"worrying\",{\"1\":{\"1462\":1}}],[\"worry\",{\"1\":{\"91\":1}}],[\"world\",{\"0\":{\"59\":1,\"1507\":1,\"2387\":1},\"1\":{\"54\":3,\"58\":2,\"59\":3,\"61\":3,\"80\":1,\"287\":3,\"288\":2,\"290\":8,\"377\":2,\"449\":2,\"1507\":1,\"2132\":2,\"2134\":2,\"2141\":2,\"2166\":1,\"2290\":2,\"2340\":2,\"2387\":1,\"2404\":1}}],[\"workaround\",{\"1\":{\"2276\":1,\"2277\":1}}],[\"worker\",{\"0\":{\"1651\":1,\"2166\":1},\"1\":{\"1651\":3,\"2130\":2,\"2131\":1,\"2132\":2,\"2133\":1,\"2136\":1,\"2137\":1,\"2149\":1,\"2166\":2,\"2249\":1}}],[\"workers\",{\"1\":{\"243\":1,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"368\":2,\"377\":2,\"385\":2,\"389\":2,\"396\":2,\"404\":2,\"406\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":2,\"457\":2,\"463\":2,\"469\":2,\"475\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":2,\"505\":2,\"1477\":1,\"1478\":2,\"1643\":1,\"1645\":3,\"1646\":1,\"1650\":1,\"2130\":5,\"2131\":2,\"2132\":1,\"2133\":2,\"2134\":3,\"2136\":3,\"2137\":1,\"2149\":3,\"2249\":1}}],[\"workshop\",{\"1\":{\"1717\":1}}],[\"workspace\",{\"0\":{\"908\":1},\"1\":{\"276\":1,\"703\":2,\"717\":2,\"755\":2,\"785\":4,\"908\":1}}],[\"works\",{\"0\":{\"202\":1,\"207\":1,\"256\":1},\"1\":{\"108\":1,\"199\":1,\"204\":1,\"223\":2,\"253\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":2,\"2355\":1}}],[\"work\",{\"0\":{\"244\":1},\"1\":{\"22\":1,\"23\":1,\"107\":1,\"108\":2,\"200\":1,\"240\":1,\"242\":1,\"249\":1,\"821\":1,\"1677\":1,\"2311\":1}}],[\"working\",{\"1\":{\"19\":1,\"235\":1,\"703\":5,\"717\":3,\"755\":1,\"785\":2}}],[\"would\",{\"1\":{\"39\":1,\"71\":1,\"101\":1,\"197\":1,\"200\":1,\"205\":1,\"217\":1,\"225\":2,\"242\":1,\"1011\":1,\"1558\":1,\"2184\":1}}],[\"w\",{\"1\":{\"12\":1,\"44\":1,\"144\":1,\"175\":1,\"243\":1,\"271\":3,\"280\":3,\"287\":9,\"625\":1,\"627\":1,\"675\":1,\"701\":4,\"735\":4,\"746\":1,\"756\":3,\"773\":3,\"824\":3,\"866\":3,\"867\":3,\"924\":3,\"961\":1,\"982\":2,\"1029\":2,\"1064\":2,\"1180\":2,\"1181\":2,\"1210\":1,\"1235\":2,\"1264\":1,\"1273\":2,\"1274\":2,\"1279\":1,\"1280\":3,\"1281\":3,\"1282\":3,\"1283\":1,\"1290\":2,\"1300\":1,\"1301\":6,\"1302\":1,\"1306\":5,\"1332\":1,\"1334\":1,\"1371\":5,\"1372\":6,\"1374\":2,\"1375\":3,\"1616\":2,\"1625\":1,\"1712\":1,\"1720\":1,\"1730\":1,\"1750\":1,\"1993\":4,\"2223\":1,\"2245\":4,\"2249\":1,\"2299\":1,\"2431\":4,\"2432\":1,\"2434\":2,\"2435\":2}}],[\"wunits\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1127\":1,\"1217\":1,\"1539\":1,\"1766\":1}}],[\"wu\",{\"1\":{\"9\":1,\"13\":2}}],[\"wai4\",{\"1\":{\"287\":2}}],[\"wait\",{\"1\":{\"167\":1,\"247\":1,\"1677\":1}}],[\"watch\",{\"1\":{\"213\":1,\"269\":1,\"278\":1}}],[\"watanabe18\",{\"1\":{\"156\":1}}],[\"watanabe\",{\"1\":{\"5\":1,\"6\":1,\"7\":1,\"8\":1,\"9\":2,\"10\":2,\"11\":2,\"12\":1,\"13\":1,\"14\":1,\"15\":1,\"16\":1,\"156\":1,\"202\":1,\"207\":1,\"1210\":1,\"1264\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1334\":1,\"1729\":1,\"1730\":1,\"1880\":1}}],[\"wavs\",{\"1\":{\"1028\":1,\"1250\":1,\"1669\":1}}],[\"wavscp\",{\"1\":{\"126\":1,\"267\":2,\"276\":2}}],[\"wavscp=dump\",{\"1\":{\"126\":1}}],[\"wavlm\",{\"1\":{\"276\":16,\"281\":1,\"2043\":2}}],[\"wavdir\",{\"1\":{\"267\":2,\"276\":2}}],[\"wav2vec2encoderlayerstablelayernorm\",{\"1\":{\"1684\":1}}],[\"wav2vec2\",{\"0\":{\"745\":1,\"891\":1},\"1\":{\"128\":1,\"281\":1,\"699\":1,\"745\":5,\"747\":1,\"891\":1,\"1748\":1}}],[\"wav2vec\",{\"1\":{\"128\":1,\"2216\":1,\"2462\":1}}],[\"wavefrom\",{\"1\":{\"1545\":1}}],[\"waveforms\",{\"1\":{\"702\":1,\"1053\":3,\"1155\":2,\"1157\":2,\"1655\":1,\"2136\":1}}],[\"waveform\",{\"1\":{\"217\":1,\"256\":1,\"286\":3,\"536\":1,\"759\":1,\"1155\":2,\"1157\":2,\"1222\":1,\"1381\":2,\"1389\":3,\"1395\":6,\"1401\":3,\"1408\":3,\"1419\":2,\"1466\":3,\"1521\":3,\"1526\":3,\"1552\":2,\"1553\":3,\"1558\":1,\"1585\":2,\"1598\":3,\"1599\":2,\"1600\":3,\"1607\":3,\"1625\":3,\"1626\":2,\"1655\":4,\"1672\":2,\"1673\":2,\"1674\":2,\"1676\":2,\"1677\":1,\"1678\":1,\"1679\":2,\"1680\":3,\"1686\":3,\"1687\":2,\"1689\":2,\"1690\":2,\"1692\":3,\"1693\":1,\"1694\":3,\"1696\":1,\"1697\":2,\"1698\":2,\"1702\":1,\"1794\":1,\"1854\":4,\"2040\":2,\"2045\":2,\"2101\":1,\"2192\":2,\"2228\":3,\"2229\":3,\"2408\":3,\"2422\":2,\"2446\":3,\"2482\":1,\"2490\":2}}],[\"wavenet\",{\"0\":{\"1578\":1,\"1579\":1,\"1628\":3,\"1733\":1,\"1803\":1,\"1849\":1,\"1854\":2,\"1873\":1,\"1877\":1,\"1896\":1},\"1\":{\"523\":2,\"536\":16,\"575\":1,\"1578\":1,\"1579\":1,\"1611\":3,\"1612\":6,\"1613\":6,\"1628\":5,\"1733\":1,\"1750\":1,\"1758\":1,\"1803\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"1849\":1,\"1854\":5,\"1873\":1,\"1877\":1,\"1896\":1,\"2431\":1}}],[\"wavegan\",{\"0\":{\"1580\":1,\"1582\":1,\"1609\":2,\"1610\":2,\"1617\":1,\"1624\":1,\"2422\":1},\"1\":{\"286\":5,\"290\":3,\"536\":9,\"1580\":1,\"1582\":1,\"1609\":3,\"1610\":3,\"1617\":1,\"1624\":1,\"2422\":2}}],[\"wavegan`\",{\"1\":{\"267\":1,\"276\":1,\"286\":1}}],[\"wave\",{\"1\":{\"106\":2,\"196\":1,\"205\":1,\"213\":1,\"242\":1,\"254\":1,\"268\":1,\"277\":1,\"1526\":1,\"1553\":1,\"1559\":1,\"1598\":1,\"1600\":1,\"1625\":1}}],[\"wav\",{\"0\":{\"68\":1,\"75\":1,\"523\":1,\"527\":1,\"536\":1,\"558\":1,\"575\":1,\"592\":1,\"1859\":1,\"1898\":1},\"1\":{\"68\":17,\"69\":7,\"70\":1,\"71\":8,\"73\":5,\"74\":3,\"75\":3,\"76\":10,\"79\":2,\"80\":1,\"126\":2,\"196\":8,\"201\":1,\"204\":1,\"205\":3,\"210\":1,\"211\":5,\"213\":4,\"216\":1,\"217\":6,\"218\":2,\"222\":1,\"223\":12,\"224\":2,\"227\":1,\"228\":1,\"234\":1,\"235\":3,\"240\":1,\"242\":3,\"253\":1,\"254\":3,\"265\":1,\"266\":5,\"267\":9,\"268\":8,\"274\":1,\"275\":5,\"276\":9,\"277\":8,\"284\":1,\"285\":5,\"286\":40,\"290\":18,\"335\":2,\"342\":2,\"361\":2,\"516\":1,\"523\":3,\"527\":12,\"536\":5,\"548\":1,\"551\":1,\"558\":2,\"575\":2,\"592\":3,\"746\":1,\"986\":3,\"993\":1,\"994\":11,\"1007\":1,\"1008\":15,\"1009\":2,\"1010\":5,\"1012\":1,\"1023\":6,\"1027\":14,\"1222\":1,\"1389\":6,\"1395\":8,\"1401\":6,\"1408\":6,\"1466\":6,\"1521\":6,\"1526\":3,\"1552\":1,\"1553\":3,\"1559\":1,\"1582\":2,\"1585\":2,\"1598\":3,\"1599\":1,\"1600\":3,\"1610\":4,\"1624\":1,\"1625\":3,\"1626\":1,\"1678\":2,\"1824\":1,\"1826\":1,\"1829\":1,\"1859\":2,\"1883\":1,\"1898\":1,\"2157\":4,\"2228\":3,\"2229\":6,\"2343\":1,\"2345\":1,\"2350\":2,\"2352\":1,\"2408\":3,\"2422\":1,\"2446\":3}}],[\"warsitz\",{\"1\":{\"1318\":1}}],[\"warmupsteplr\",{\"0\":{\"2021\":1},\"1\":{\"2021\":3}}],[\"warmupreducelronplateau\",{\"0\":{\"2020\":1},\"1\":{\"2020\":3}}],[\"warmup\",{\"0\":{\"2014\":1,\"2015\":1,\"2017\":1,\"2019\":1,\"2020\":1,\"2021\":1},\"1\":{\"243\":1,\"625\":3,\"2014\":5,\"2015\":10,\"2016\":1,\"2017\":4,\"2018\":6,\"2019\":5,\"2020\":7,\"2021\":7,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2040\":1,\"2043\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2065\":1,\"2066\":1,\"2432\":2}}],[\"warmuplr\",{\"0\":{\"2019\":1},\"1\":{\"243\":1,\"2016\":1,\"2017\":1,\"2019\":3,\"2020\":2,\"2021\":2}}],[\"warm\",{\"1\":{\"139\":1,\"2015\":3,\"2040\":1,\"2043\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2066\":1}}],[\"warning\",{\"1\":{\"267\":4,\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"377\":1,\"385\":1,\"389\":1,\"396\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":1,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"511\":1,\"831\":1,\"833\":1,\"1677\":1,\"1833\":1,\"2145\":1,\"2309\":1,\"2355\":2,\"2372\":2}}],[\"warn\",{\"1\":{\"66\":3}}],[\"warping\",{\"1\":{\"267\":1,\"276\":1,\"286\":1,\"1670\":1,\"1699\":1}}],[\"warp\",{\"0\":{\"946\":1,\"1670\":1,\"1699\":2,\"1930\":1},\"1\":{\"29\":2,\"42\":1,\"138\":1,\"139\":1,\"153\":1,\"163\":2,\"243\":3,\"704\":3,\"705\":3,\"803\":1,\"804\":3,\"833\":4,\"922\":2,\"931\":1,\"932\":3,\"933\":1,\"934\":3,\"936\":2,\"937\":2,\"946\":1,\"1670\":2,\"1699\":3}}],[\"ways\",{\"1\":{\"60\":1,\"84\":1,\"102\":1,\"119\":1,\"275\":1,\"756\":1,\"773\":1}}],[\"way\",{\"1\":{\"47\":1,\"86\":1,\"108\":1,\"262\":1,\"276\":1,\"1938\":1}}],[\"waspaa2021\",{\"1\":{\"179\":1}}],[\"waspaa\",{\"1\":{\"179\":1}}],[\"was\",{\"1\":{\"24\":1,\"43\":1,\"110\":1,\"247\":1,\"261\":1,\"267\":1,\"821\":1,\"1334\":1,\"1717\":1,\"2044\":1}}],[\"wan2\",{\"1\":{\"287\":1}}],[\"wandb\",{\"0\":{\"2400\":1},\"1\":{\"92\":5,\"377\":12,\"449\":12,\"2348\":2,\"2359\":1,\"2367\":1,\"2370\":4,\"2372\":2,\"2400\":1}}],[\"wan\",{\"1\":{\"67\":1}}],[\"wanna\",{\"1\":{\"32\":1}}],[\"wants\",{\"1\":{\"828\":1,\"829\":1,\"830\":1}}],[\"wanted\",{\"1\":{\"290\":1}}],[\"want\",{\"1\":{\"22\":1,\"23\":1,\"39\":1,\"41\":2,\"68\":1,\"69\":1,\"141\":1,\"160\":1,\"162\":1,\"173\":1,\"174\":1,\"200\":1,\"206\":2,\"212\":1,\"217\":1,\"218\":2,\"223\":1,\"225\":1,\"254\":1,\"255\":2,\"263\":1,\"267\":5,\"269\":1,\"276\":6,\"278\":1,\"286\":12,\"290\":4,\"536\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"744\":1,\"828\":1,\"829\":1,\"830\":1,\"859\":1,\"1086\":2,\"1207\":2,\"1655\":1}}],[\"wang\",{\"1\":{\"11\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1327\":1,\"1330\":1}}],[\"wangyou\",{\"1\":{\"8\":1,\"11\":2,\"1228\":1}}],[\"whose\",{\"1\":{\"1308\":1,\"1784\":1,\"2355\":1}}],[\"whole\",{\"1\":{\"91\":2,\"747\":1,\"1000\":1,\"1011\":1,\"1053\":1,\"1145\":1,\"1155\":1,\"1157\":1,\"1264\":1,\"1265\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1,\"1386\":1,\"2249\":1,\"2253\":1}}],[\"wh\",{\"1\":{\"1290\":2}}],[\"what\",{\"1\":{\"819\":1,\"2355\":1}}],[\"whatever\",{\"1\":{\"128\":1,\"2355\":1}}],[\"why\",{\"0\":{\"70\":1},\"1\":{\"106\":1,\"262\":4,\"284\":4,\"290\":4,\"1833\":1,\"1962\":1}}],[\"whether\",{\"1\":{\"43\":8,\"104\":1,\"130\":2,\"141\":4,\"142\":1,\"147\":1,\"164\":1,\"269\":1,\"278\":1,\"616\":1,\"619\":1,\"620\":4,\"621\":1,\"622\":1,\"623\":1,\"625\":4,\"627\":2,\"635\":1,\"639\":3,\"643\":1,\"652\":1,\"661\":2,\"663\":1,\"665\":1,\"692\":5,\"706\":1,\"709\":7,\"710\":5,\"711\":5,\"738\":2,\"740\":2,\"745\":1,\"747\":1,\"748\":2,\"756\":1,\"760\":2,\"771\":1,\"773\":1,\"774\":7,\"780\":7,\"804\":1,\"846\":4,\"847\":1,\"849\":2,\"866\":1,\"867\":1,\"932\":1,\"934\":1,\"1029\":3,\"1061\":1,\"1062\":2,\"1064\":1,\"1107\":5,\"1117\":1,\"1118\":4,\"1124\":2,\"1125\":3,\"1130\":2,\"1131\":1,\"1133\":2,\"1134\":1,\"1136\":2,\"1137\":1,\"1139\":1,\"1141\":2,\"1155\":6,\"1157\":6,\"1162\":1,\"1176\":2,\"1202\":2,\"1208\":1,\"1210\":1,\"1232\":2,\"1235\":2,\"1252\":2,\"1255\":1,\"1257\":1,\"1259\":3,\"1261\":3,\"1267\":2,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":3,\"1279\":1,\"1280\":2,\"1281\":2,\"1282\":1,\"1283\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1318\":1,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1327\":2,\"1330\":2,\"1334\":1,\"1356\":1,\"1368\":1,\"1385\":2,\"1389\":1,\"1391\":1,\"1392\":1,\"1395\":1,\"1400\":1,\"1401\":1,\"1402\":1,\"1403\":1,\"1408\":1,\"1410\":1,\"1419\":4,\"1441\":1,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1458\":2,\"1460\":2,\"1466\":1,\"1467\":1,\"1468\":1,\"1469\":1,\"1513\":3,\"1519\":3,\"1521\":1,\"1526\":8,\"1529\":3,\"1534\":1,\"1535\":3,\"1546\":4,\"1548\":5,\"1549\":2,\"1551\":3,\"1552\":10,\"1553\":5,\"1577\":1,\"1581\":1,\"1584\":1,\"1585\":1,\"1586\":1,\"1587\":3,\"1588\":1,\"1591\":1,\"1592\":3,\"1594\":1,\"1595\":1,\"1596\":4,\"1597\":3,\"1598\":6,\"1599\":18,\"1600\":7,\"1603\":1,\"1604\":1,\"1605\":2,\"1606\":2,\"1607\":4,\"1609\":1,\"1610\":3,\"1611\":2,\"1612\":4,\"1613\":4,\"1614\":2,\"1615\":1,\"1616\":1,\"1618\":1,\"1619\":2,\"1620\":1,\"1621\":1,\"1622\":3,\"1625\":6,\"1626\":8,\"1627\":2,\"1628\":6,\"1645\":2,\"1719\":1,\"1721\":1,\"1725\":1,\"1735\":2,\"1736\":3,\"1749\":1,\"1750\":4,\"1751\":2,\"1759\":2,\"1764\":2,\"1770\":1,\"1771\":2,\"1779\":2,\"1784\":1,\"1785\":1,\"1808\":1,\"1810\":1,\"1815\":1,\"1817\":1,\"1839\":2,\"1843\":1,\"1871\":1,\"1897\":1,\"1921\":1,\"1936\":2,\"1971\":2,\"1976\":1,\"1992\":3,\"1993\":5,\"1995\":3,\"2000\":1,\"2001\":1,\"2007\":1,\"2129\":2,\"2134\":1,\"2136\":1,\"2167\":1,\"2176\":2,\"2191\":5,\"2220\":2,\"2222\":2,\"2223\":3,\"2224\":2,\"2226\":3,\"2235\":3,\"2236\":4,\"2237\":2,\"2239\":13,\"2240\":9,\"2241\":2,\"2245\":11,\"2298\":1,\"2327\":1,\"2353\":2,\"2364\":1,\"2403\":2,\"2411\":14,\"2412\":16,\"2413\":2,\"2423\":16,\"2424\":2,\"2431\":10,\"2432\":12,\"2445\":2,\"2447\":15,\"2448\":2,\"2470\":2,\"2471\":1,\"2473\":1}}],[\"when\",{\"1\":{\"26\":2,\"34\":1,\"39\":3,\"48\":1,\"66\":1,\"70\":1,\"71\":1,\"97\":1,\"110\":1,\"118\":1,\"124\":1,\"139\":2,\"159\":1,\"163\":1,\"173\":1,\"223\":2,\"249\":1,\"269\":2,\"276\":1,\"278\":2,\"284\":1,\"285\":1,\"290\":2,\"521\":1,\"625\":1,\"653\":1,\"703\":1,\"755\":1,\"760\":1,\"777\":1,\"785\":1,\"786\":1,\"800\":1,\"833\":1,\"881\":1,\"884\":1,\"922\":1,\"936\":1,\"937\":1,\"960\":1,\"961\":5,\"993\":1,\"1009\":1,\"1086\":2,\"1124\":1,\"1147\":1,\"1153\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1207\":2,\"1210\":1,\"1246\":3,\"1247\":1,\"1259\":1,\"1261\":1,\"1268\":1,\"1269\":2,\"1270\":3,\"1271\":2,\"1280\":4,\"1327\":2,\"1330\":1,\"1334\":1,\"1354\":2,\"1545\":1,\"1552\":2,\"1626\":2,\"1645\":1,\"1650\":1,\"1655\":1,\"1710\":1,\"1768\":1,\"1794\":2,\"1804\":1,\"1824\":1,\"1842\":1,\"1940\":1,\"1942\":1,\"2000\":1,\"2131\":1,\"2134\":1,\"2184\":1,\"2188\":1,\"2239\":1,\"2249\":3,\"2253\":1,\"2333\":1,\"2355\":4}}],[\"whereas\",{\"1\":{\"2355\":1}}],[\"where2\",{\"1\":{\"69\":1}}],[\"where\",{\"1\":{\"3\":1,\"24\":2,\"44\":1,\"60\":1,\"69\":1,\"79\":1,\"97\":1,\"98\":1,\"100\":1,\"144\":1,\"147\":1,\"154\":1,\"201\":1,\"212\":1,\"240\":1,\"242\":1,\"247\":1,\"262\":1,\"269\":1,\"276\":2,\"278\":1,\"286\":2,\"616\":1,\"629\":1,\"650\":1,\"652\":1,\"691\":1,\"787\":2,\"824\":1,\"852\":1,\"856\":1,\"922\":1,\"925\":1,\"936\":1,\"937\":1,\"984\":1,\"986\":3,\"1051\":1,\"1119\":2,\"1246\":1,\"1296\":1,\"1308\":1,\"1316\":1,\"1320\":1,\"1350\":1,\"1514\":1,\"1582\":1,\"1738\":2,\"1739\":2,\"1740\":2,\"1741\":2,\"1742\":2,\"1743\":2,\"1744\":2,\"1745\":2,\"1746\":2,\"1849\":1,\"1883\":1,\"1920\":1,\"2000\":4,\"2001\":1,\"2049\":1,\"2130\":2,\"2133\":2,\"2136\":3,\"2145\":1,\"2146\":1,\"2147\":1,\"2155\":1,\"2162\":1,\"2311\":1}}],[\"while\",{\"1\":{\"45\":1,\"98\":1,\"102\":1,\"128\":1,\"138\":1,\"145\":1,\"175\":1,\"267\":1,\"269\":1,\"276\":1,\"278\":1,\"286\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"699\":1,\"704\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"739\":1,\"742\":1,\"744\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"828\":1,\"829\":1,\"830\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"859\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1011\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1155\":2,\"1157\":2,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1250\":1,\"1251\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1683\":1,\"1919\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2044\":1,\"2125\":1,\"2130\":1,\"2136\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2240\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"whitespaces\",{\"1\":{\"290\":1}}],[\"whitespace\",{\"1\":{\"224\":1,\"290\":3}}],[\"white\",{\"1\":{\"32\":1,\"67\":2}}],[\"which\",{\"0\":{\"61\":1},\"1\":{\"22\":1,\"68\":1,\"69\":1,\"80\":1,\"82\":3,\"98\":1,\"101\":1,\"110\":1,\"111\":1,\"128\":1,\"135\":1,\"154\":1,\"167\":1,\"175\":2,\"196\":1,\"197\":3,\"200\":1,\"205\":1,\"206\":1,\"211\":1,\"212\":2,\"218\":1,\"223\":1,\"224\":2,\"228\":1,\"242\":1,\"243\":1,\"255\":1,\"262\":1,\"263\":2,\"266\":1,\"267\":3,\"268\":1,\"269\":1,\"273\":1,\"275\":1,\"276\":4,\"277\":1,\"278\":1,\"286\":5,\"290\":5,\"536\":1,\"704\":1,\"705\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"733\":1,\"734\":1,\"744\":1,\"755\":1,\"785\":1,\"803\":1,\"819\":1,\"821\":2,\"828\":1,\"829\":1,\"830\":1,\"859\":1,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"912\":1,\"922\":1,\"927\":1,\"936\":1,\"937\":1,\"978\":1,\"980\":2,\"1000\":1,\"1086\":1,\"1207\":1,\"1224\":1,\"1225\":1,\"1273\":1,\"1274\":1,\"1292\":1,\"1301\":1,\"1306\":1,\"1371\":1,\"1372\":1,\"1390\":1,\"1397\":1,\"1402\":1,\"1409\":1,\"1467\":1,\"1533\":1,\"1546\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1606\":1,\"1612\":1,\"1622\":1,\"1626\":1,\"1655\":1,\"1668\":1,\"1697\":1,\"1726\":1,\"1727\":1,\"1729\":1,\"1730\":1,\"1737\":1,\"1750\":1,\"1758\":2,\"1770\":1,\"1794\":1,\"1795\":1,\"1810\":2,\"1811\":2,\"1992\":1,\"1993\":1,\"1995\":1,\"2039\":1,\"2130\":1,\"2220\":1,\"2223\":1,\"2227\":1,\"2231\":2,\"2240\":1,\"2245\":1,\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2325\":1,\"2327\":1,\"2344\":1,\"2355\":4,\"2359\":1,\"2411\":1,\"2431\":1,\"2432\":1,\"2480\":1}}],[\"whispermlp\",{\"0\":{\"2123\":1}}],[\"whisperasrmodel\",{\"0\":{\"2066\":1},\"1\":{\"2066\":1}}],[\"whisperfrontend\",{\"0\":{\"864\":1},\"1\":{\"864\":1}}],[\"whisper\",{\"0\":{\"126\":1,\"511\":1,\"741\":1,\"790\":1,\"791\":1,\"864\":1,\"2066\":1,\"2093\":1,\"2094\":1,\"2283\":1,\"2284\":1},\"1\":{\"6\":2,\"126\":7,\"200\":11,\"240\":1,\"243\":2,\"244\":1,\"267\":2,\"276\":2,\"286\":2,\"481\":2,\"511\":8,\"741\":1,\"790\":4,\"791\":4,\"792\":2,\"864\":4,\"865\":1,\"1725\":1,\"2066\":4,\"2283\":1,\"2284\":1,\"2293\":2,\"2336\":2,\"2337\":2,\"2356\":2}}],[\"wiki\",{\"1\":{\"1672\":1,\"1673\":1,\"1687\":1,\"1689\":1,\"1690\":1}}],[\"wikipedia\",{\"1\":{\"1672\":1,\"1673\":1,\"1687\":1,\"1689\":1,\"1690\":1}}],[\"wiener\",{\"1\":{\"1323\":1,\"1327\":3,\"1330\":4,\"1334\":1}}],[\"wiesner\",{\"1\":{\"156\":1}}],[\"wiley\",{\"1\":{\"1319\":1}}],[\"will\",{\"1\":{\"19\":1,\"22\":1,\"23\":1,\"24\":1,\"25\":1,\"27\":2,\"31\":1,\"34\":1,\"39\":1,\"43\":2,\"45\":2,\"46\":1,\"62\":1,\"74\":1,\"90\":1,\"91\":1,\"94\":1,\"101\":1,\"126\":1,\"128\":1,\"133\":1,\"145\":2,\"148\":1,\"167\":1,\"168\":2,\"173\":1,\"175\":1,\"205\":1,\"211\":7,\"213\":1,\"223\":3,\"224\":2,\"232\":1,\"236\":1,\"242\":4,\"243\":7,\"247\":2,\"259\":6,\"260\":1,\"263\":1,\"266\":6,\"267\":1,\"268\":1,\"269\":1,\"275\":9,\"276\":4,\"277\":1,\"278\":1,\"285\":6,\"286\":1,\"290\":1,\"692\":2,\"699\":2,\"703\":2,\"704\":1,\"709\":3,\"710\":2,\"711\":2,\"755\":5,\"756\":2,\"768\":1,\"773\":2,\"774\":3,\"780\":3,\"785\":6,\"786\":3,\"800\":3,\"804\":1,\"817\":1,\"846\":1,\"849\":2,\"866\":2,\"867\":2,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"911\":3,\"921\":2,\"922\":3,\"932\":1,\"934\":1,\"935\":2,\"936\":3,\"937\":3,\"961\":1,\"974\":2,\"984\":1,\"994\":1,\"1061\":3,\"1062\":2,\"1063\":1,\"1080\":1,\"1107\":2,\"1126\":1,\"1155\":3,\"1157\":3,\"1202\":2,\"1204\":1,\"1209\":4,\"1228\":4,\"1259\":4,\"1261\":4,\"1273\":1,\"1274\":1,\"1278\":2,\"1296\":1,\"1306\":1,\"1308\":1,\"1316\":1,\"1371\":1,\"1409\":1,\"1454\":1,\"1456\":1,\"1484\":1,\"1485\":1,\"1513\":1,\"1526\":2,\"1548\":1,\"1551\":1,\"1552\":4,\"1553\":2,\"1592\":1,\"1593\":1,\"1595\":1,\"1596\":2,\"1597\":3,\"1598\":2,\"1599\":4,\"1600\":2,\"1604\":4,\"1605\":1,\"1606\":2,\"1609\":2,\"1610\":1,\"1618\":1,\"1619\":1,\"1625\":2,\"1626\":4,\"1628\":1,\"1655\":1,\"1719\":3,\"1721\":3,\"1725\":3,\"1735\":2,\"1750\":2,\"1751\":2,\"1759\":2,\"1862\":3,\"1901\":1,\"1903\":1,\"1992\":5,\"1993\":3,\"1995\":5,\"2039\":1,\"2129\":2,\"2130\":1,\"2134\":1,\"2183\":1,\"2191\":1,\"2216\":1,\"2220\":3,\"2224\":2,\"2235\":3,\"2236\":3,\"2239\":4,\"2240\":3,\"2245\":3,\"2344\":1,\"2345\":1,\"2354\":4,\"2355\":11,\"2365\":2,\"2411\":4,\"2412\":4,\"2423\":4,\"2431\":3,\"2432\":3,\"2447\":4}}],[\"william\",{\"1\":{\"5\":1,\"6\":3,\"7\":1,\"15\":1,\"244\":1}}],[\"witaj\",{\"1\":{\"287\":1}}],[\"without\",{\"0\":{\"125\":1,\"1701\":1,\"1746\":1,\"2302\":1},\"1\":{\"19\":1,\"24\":1,\"45\":1,\"70\":1,\"71\":1,\"96\":1,\"141\":1,\"145\":1,\"162\":1,\"197\":1,\"242\":1,\"268\":1,\"277\":1,\"286\":1,\"290\":1,\"481\":1,\"616\":1,\"675\":1,\"716\":1,\"717\":2,\"770\":1,\"804\":1,\"932\":1,\"934\":1,\"959\":1,\"1050\":1,\"1116\":1,\"1161\":1,\"1189\":1,\"1218\":1,\"1221\":1,\"1229\":1,\"1244\":1,\"1279\":1,\"1280\":3,\"1281\":1,\"1283\":3,\"1395\":2,\"1462\":1,\"1602\":2,\"1698\":1,\"1701\":1,\"1742\":2,\"1746\":1,\"2130\":1,\"2131\":1,\"2133\":1,\"2143\":1,\"2184\":1,\"2249\":1,\"2253\":1,\"2302\":1,\"2355\":1}}],[\"within\",{\"1\":{\"3\":1,\"26\":1,\"43\":3,\"101\":1,\"138\":1,\"141\":1,\"150\":1,\"173\":1,\"240\":1,\"290\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"759\":2,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"831\":2,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1645\":1,\"1650\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1717\":1,\"1919\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1960\":2,\"1961\":2,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2000\":4,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2131\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2312\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"with\",{\"0\":{\"56\":2,\"58\":1,\"62\":1,\"63\":1,\"64\":1,\"66\":1,\"73\":1,\"154\":1,\"195\":1,\"203\":1,\"226\":1,\"229\":1,\"230\":1,\"291\":1,\"1311\":1,\"1319\":1,\"1322\":1,\"2296\":1},\"1\":{\"3\":5,\"7\":1,\"8\":1,\"19\":1,\"22\":2,\"23\":1,\"24\":2,\"25\":1,\"26\":1,\"27\":1,\"28\":1,\"38\":4,\"39\":1,\"42\":1,\"43\":2,\"44\":2,\"46\":2,\"50\":3,\"51\":1,\"52\":1,\"54\":7,\"56\":2,\"68\":2,\"69\":1,\"70\":2,\"71\":7,\"73\":2,\"81\":1,\"82\":1,\"84\":1,\"88\":2,\"91\":1,\"95\":1,\"97\":1,\"101\":2,\"102\":1,\"106\":2,\"129\":1,\"136\":1,\"138\":1,\"139\":2,\"140\":1,\"144\":1,\"147\":1,\"148\":1,\"152\":1,\"160\":1,\"161\":1,\"162\":1,\"165\":1,\"167\":1,\"173\":1,\"175\":1,\"178\":2,\"179\":1,\"181\":1,\"184\":2,\"185\":2,\"186\":1,\"187\":1,\"188\":1,\"191\":1,\"193\":3,\"194\":2,\"197\":1,\"200\":3,\"201\":1,\"202\":1,\"205\":1,\"211\":1,\"213\":3,\"218\":1,\"222\":2,\"223\":6,\"224\":4,\"226\":1,\"235\":1,\"242\":3,\"243\":4,\"246\":2,\"247\":1,\"248\":1,\"254\":3,\"259\":1,\"260\":1,\"262\":4,\"265\":2,\"266\":3,\"267\":16,\"269\":4,\"273\":1,\"274\":2,\"275\":3,\"276\":15,\"278\":4,\"284\":7,\"285\":2,\"286\":34,\"287\":1,\"289\":1,\"290\":23,\"295\":2,\"374\":1,\"415\":2,\"481\":1,\"514\":2,\"527\":3,\"561\":1,\"583\":1,\"589\":1,\"606\":1,\"616\":2,\"633\":1,\"634\":1,\"644\":1,\"649\":1,\"661\":1,\"689\":1,\"692\":1,\"699\":2,\"703\":1,\"711\":1,\"724\":2,\"725\":2,\"726\":1,\"727\":1,\"728\":2,\"729\":1,\"744\":2,\"755\":2,\"756\":2,\"759\":2,\"760\":2,\"768\":7,\"773\":2,\"784\":1,\"785\":2,\"786\":2,\"790\":1,\"793\":2,\"800\":2,\"809\":1,\"820\":1,\"824\":1,\"828\":4,\"829\":4,\"830\":3,\"831\":5,\"833\":1,\"846\":2,\"850\":1,\"854\":1,\"859\":1,\"866\":1,\"867\":3,\"878\":1,\"879\":1,\"881\":2,\"882\":1,\"883\":1,\"884\":2,\"912\":1,\"918\":1,\"921\":1,\"922\":1,\"927\":4,\"935\":1,\"936\":1,\"937\":1,\"939\":3,\"941\":1,\"960\":1,\"978\":1,\"982\":1,\"986\":1,\"994\":1,\"1002\":1,\"1008\":1,\"1029\":2,\"1051\":1,\"1053\":1,\"1061\":2,\"1062\":2,\"1064\":1,\"1110\":1,\"1119\":1,\"1126\":1,\"1132\":1,\"1133\":1,\"1137\":1,\"1144\":1,\"1151\":1,\"1155\":1,\"1157\":1,\"1164\":2,\"1167\":1,\"1170\":1,\"1176\":1,\"1180\":1,\"1181\":1,\"1204\":1,\"1209\":1,\"1211\":1,\"1224\":2,\"1225\":2,\"1228\":1,\"1235\":2,\"1245\":1,\"1247\":1,\"1252\":1,\"1269\":5,\"1270\":7,\"1271\":5,\"1273\":1,\"1274\":2,\"1279\":2,\"1280\":4,\"1281\":2,\"1282\":1,\"1283\":4,\"1284\":1,\"1290\":3,\"1303\":1,\"1304\":1,\"1306\":3,\"1310\":1,\"1311\":2,\"1314\":2,\"1315\":2,\"1319\":2,\"1322\":2,\"1325\":2,\"1327\":2,\"1329\":1,\"1330\":3,\"1332\":1,\"1334\":8,\"1347\":1,\"1350\":2,\"1351\":1,\"1356\":1,\"1371\":3,\"1376\":1,\"1377\":1,\"1386\":1,\"1395\":1,\"1397\":2,\"1400\":3,\"1441\":3,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1458\":1,\"1460\":1,\"1469\":1,\"1478\":1,\"1484\":2,\"1511\":1,\"1521\":1,\"1546\":1,\"1553\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1585\":1,\"1590\":1,\"1608\":2,\"1611\":1,\"1612\":1,\"1616\":1,\"1622\":1,\"1625\":1,\"1626\":1,\"1628\":1,\"1645\":1,\"1664\":1,\"1665\":2,\"1668\":2,\"1680\":1,\"1691\":1,\"1692\":1,\"1695\":1,\"1697\":1,\"1698\":2,\"1705\":1,\"1709\":1,\"1712\":1,\"1719\":2,\"1720\":3,\"1721\":2,\"1724\":1,\"1725\":3,\"1731\":1,\"1733\":2,\"1750\":1,\"1756\":1,\"1757\":1,\"1759\":1,\"1760\":2,\"1770\":1,\"1774\":1,\"1782\":1,\"1784\":1,\"1785\":2,\"1787\":1,\"1789\":1,\"1790\":1,\"1796\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1803\":2,\"1806\":1,\"1816\":1,\"1817\":2,\"1822\":1,\"1827\":1,\"1830\":1,\"1849\":3,\"1854\":6,\"1862\":1,\"1872\":1,\"1873\":2,\"1877\":2,\"1883\":4,\"1888\":1,\"1896\":1,\"1902\":3,\"1904\":3,\"1907\":1,\"1915\":1,\"1916\":1,\"1936\":1,\"1944\":1,\"1945\":1,\"1946\":1,\"1947\":1,\"1960\":2,\"1961\":1,\"1963\":1,\"1966\":2,\"1992\":2,\"1993\":1,\"1994\":2,\"1995\":1,\"2000\":4,\"2001\":3,\"2005\":1,\"2006\":1,\"2015\":1,\"2018\":1,\"2039\":4,\"2040\":1,\"2043\":1,\"2045\":1,\"2049\":2,\"2054\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2130\":2,\"2131\":4,\"2133\":1,\"2134\":4,\"2136\":2,\"2137\":2,\"2139\":1,\"2140\":1,\"2143\":2,\"2145\":1,\"2147\":1,\"2148\":1,\"2151\":1,\"2155\":1,\"2162\":2,\"2167\":1,\"2176\":3,\"2216\":1,\"2220\":1,\"2223\":1,\"2236\":2,\"2239\":1,\"2240\":1,\"2245\":1,\"2249\":1,\"2253\":1,\"2280\":1,\"2287\":1,\"2296\":1,\"2298\":1,\"2307\":1,\"2310\":1,\"2311\":1,\"2327\":2,\"2354\":1,\"2355\":5,\"2359\":1,\"2404\":1,\"2411\":3,\"2412\":2,\"2421\":1,\"2422\":2,\"2423\":1,\"2427\":1,\"2431\":1,\"2432\":2,\"2438\":1,\"2439\":1,\"2440\":1,\"2447\":3,\"2507\":1}}],[\"wish\",{\"1\":{\"263\":1,\"286\":1}}],[\"wise\",{\"0\":{\"818\":1},\"1\":{\"141\":6,\"224\":1,\"623\":1,\"649\":2,\"654\":4,\"661\":3,\"691\":1,\"692\":1,\"709\":1,\"710\":1,\"711\":1,\"735\":1,\"771\":1,\"774\":3,\"780\":1,\"849\":1,\"971\":1,\"1055\":1,\"1057\":1,\"1072\":1,\"1074\":1,\"1107\":1,\"1145\":2,\"1265\":3,\"1271\":1,\"1278\":1,\"1519\":3,\"1535\":3,\"1552\":2,\"1626\":2,\"1992\":1,\"1995\":1,\"1997\":1,\"2129\":1,\"2191\":1,\"2432\":2,\"2439\":1}}],[\"wider\",{\"1\":{\"1654\":1}}],[\"widely\",{\"1\":{\"223\":1}}],[\"widim\",{\"1\":{\"1127\":1}}],[\"width=none\",{\"1\":{\"2480\":1}}],[\"width=0\",{\"1\":{\"1634\":1,\"1635\":1,\"1636\":1}}],[\"widths\",{\"1\":{\"1634\":1,\"1635\":1,\"1636\":1}}],[\"widthofthe\",{\"1\":{\"1558\":1}}],[\"width\",{\"1\":{\"243\":2,\"833\":3,\"1064\":1,\"1290\":1,\"1375\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1664\":1,\"1665\":4,\"1691\":3}}],[\"winodw\",{\"1\":{\"691\":1}}],[\"win\",{\"1\":{\"243\":1,\"267\":16,\"286\":5,\"516\":2,\"536\":2,\"537\":2,\"548\":2,\"551\":2,\"558\":2,\"606\":2,\"720\":1,\"778\":1,\"796\":1,\"831\":3,\"869\":1,\"976\":1,\"1162\":2,\"1164\":1,\"1250\":2,\"1251\":1,\"1385\":1,\"1392\":2,\"1396\":1,\"1397\":2,\"1401\":1,\"1402\":1,\"1408\":1,\"1409\":1,\"1422\":2,\"1466\":1,\"1467\":1,\"1525\":2,\"1526\":1,\"1547\":1,\"1552\":2,\"1553\":1,\"1566\":3,\"1598\":1,\"1600\":1,\"1607\":2,\"1625\":1,\"1660\":1,\"1669\":1,\"1680\":2,\"1692\":2,\"1698\":2,\"1711\":4,\"1776\":1,\"1791\":1,\"1832\":1,\"1835\":1,\"1899\":1,\"1900\":1,\"1923\":1,\"1924\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"2232\":1,\"2238\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2482\":2,\"2490\":2}}],[\"windowed\",{\"1\":{\"1375\":1,\"1526\":1,\"1599\":1,\"1600\":1}}],[\"windowattention\",{\"0\":{\"1290\":1},\"1\":{\"1290\":2}}],[\"window=none\",{\"1\":{\"1750\":2}}],[\"window=3\",{\"1\":{\"1708\":1,\"1709\":1,\"1710\":1,\"1768\":1,\"2224\":1}}],[\"window=1\",{\"1\":{\"1708\":1,\"1709\":1,\"1710\":1,\"1768\":1,\"2224\":1}}],[\"window=2\",{\"1\":{\"1703\":1,\"1857\":1}}],[\"window=\",{\"1\":{\"1250\":1,\"1251\":1,\"1269\":1,\"1270\":1,\"1334\":1,\"1776\":1,\"1791\":1,\"1832\":1,\"1835\":1,\"1899\":1,\"1900\":1,\"1923\":1,\"1924\":1}}],[\"windowing\",{\"0\":{\"831\":1},\"1\":{\"768\":3,\"831\":1,\"1668\":3,\"1680\":1,\"1692\":1,\"1698\":1,\"1730\":2}}],[\"windows\",{\"1\":{\"161\":1,\"774\":2,\"1290\":3,\"1374\":3,\"1375\":3,\"1392\":1,\"1397\":1,\"1422\":1,\"1918\":1}}],[\"windows10\",{\"1\":{\"160\":1}}],[\"window\",{\"0\":{\"1374\":1,\"1375\":1},\"1\":{\"43\":2,\"148\":1,\"243\":1,\"267\":8,\"295\":4,\"321\":2,\"406\":4,\"415\":4,\"484\":4,\"490\":4,\"516\":1,\"537\":1,\"548\":1,\"551\":1,\"558\":1,\"639\":6,\"720\":1,\"768\":2,\"774\":1,\"778\":1,\"831\":5,\"833\":1,\"1029\":8,\"1064\":3,\"1162\":1,\"1164\":4,\"1210\":4,\"1235\":9,\"1250\":1,\"1251\":1,\"1262\":3,\"1269\":3,\"1270\":3,\"1271\":3,\"1280\":3,\"1281\":3,\"1282\":3,\"1290\":4,\"1314\":2,\"1334\":3,\"1374\":6,\"1375\":6,\"1389\":1,\"1392\":1,\"1396\":1,\"1397\":1,\"1401\":1,\"1408\":1,\"1413\":4,\"1419\":4,\"1420\":1,\"1422\":1,\"1466\":1,\"1484\":1,\"1525\":1,\"1526\":1,\"1552\":2,\"1553\":1,\"1582\":3,\"1598\":1,\"1600\":1,\"1607\":5,\"1608\":1,\"1610\":3,\"1618\":3,\"1625\":1,\"1631\":2,\"1668\":5,\"1669\":1,\"1670\":2,\"1680\":3,\"1692\":3,\"1698\":3,\"1699\":2,\"1708\":4,\"1709\":4,\"1710\":6,\"1711\":1,\"1750\":4,\"1768\":6,\"1846\":1,\"1875\":1,\"1895\":1,\"1960\":1,\"1961\":1,\"1976\":2,\"1978\":1,\"1980\":1,\"1982\":1,\"1993\":6,\"2224\":4,\"2232\":1,\"2238\":1,\"2245\":6,\"2350\":2,\"2378\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2427\":3,\"2431\":6,\"2435\":1,\"2438\":1,\"2440\":1,\"2441\":3,\"2482\":4,\"2490\":4}}],[\"wireless\",{\"1\":{\"67\":3}}],[\"wip\",{\"1\":{\"32\":1}}],[\"wespeaker\",{\"1\":{\"2198\":1}}],[\"weng\",{\"1\":{\"1061\":1,\"1062\":1}}],[\"wenet\",{\"1\":{\"146\":1,\"2198\":1}}],[\"weeks\",{\"1\":{\"286\":1}}],[\"well\",{\"1\":{\"242\":1,\"262\":1,\"284\":1,\"290\":1,\"703\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"744\":1,\"828\":1,\"829\":1,\"830\":1,\"859\":1,\"1334\":1,\"2044\":1}}],[\"weaknesses\",{\"1\":{\"262\":1}}],[\"weakness\",{\"1\":{\"262\":1}}],[\"weak\",{\"1\":{\"202\":1}}],[\"weakly\",{\"0\":{\"6\":1,\"240\":1},\"1\":{\"184\":1,\"185\":2,\"186\":1}}],[\"wer\",{\"1\":{\"139\":2,\"223\":1,\"246\":1,\"267\":1,\"276\":1,\"625\":3,\"627\":7,\"736\":1,\"740\":7,\"777\":1,\"1640\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2221\":1}}],[\"wereld\",{\"1\":{\"287\":1}}],[\"were\",{\"1\":{\"24\":1,\"145\":2,\"261\":1,\"756\":1,\"773\":1,\"866\":1,\"867\":1}}],[\"weighting\",{\"1\":{\"1314\":2,\"1315\":1}}],[\"weight=weight\",{\"1\":{\"2327\":2}}],[\"weight=20\",{\"1\":{\"1839\":1}}],[\"weight=0\",{\"1\":{\"1210\":1}}],[\"weight=1\",{\"1\":{\"1132\":1,\"1167\":1,\"1209\":1,\"1228\":1}}],[\"weight=10\",{\"1\":{\"290\":1}}],[\"weightedaverage\",{\"0\":{\"2373\":1},\"1\":{\"2373\":1}}],[\"weighted\",{\"0\":{\"1701\":1},\"1\":{\"128\":1,\"201\":2,\"642\":1,\"644\":1,\"649\":1,\"654\":1,\"699\":3,\"738\":2,\"750\":2,\"780\":1,\"784\":1,\"1209\":1,\"1309\":1,\"1311\":1,\"1314\":1,\"1330\":2,\"1526\":1,\"1598\":1,\"1599\":3,\"1600\":1,\"1627\":3,\"1701\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1719\":3,\"1725\":3,\"1764\":3,\"1768\":1,\"1794\":1,\"1801\":1,\"1806\":3,\"1839\":3,\"1991\":1,\"1994\":1,\"2130\":1,\"2226\":3,\"2235\":3,\"2236\":3,\"2237\":3,\"2239\":3,\"2240\":3,\"2241\":3,\"2245\":3,\"2411\":3,\"2412\":3,\"2413\":3,\"2423\":3,\"2424\":3,\"2431\":3,\"2432\":3,\"2447\":3,\"2448\":3}}],[\"weights=none\",{\"1\":{\"1209\":1}}],[\"weights\",{\"0\":{\"92\":1,\"1487\":1,\"2106\":1},\"1\":{\"92\":1,\"625\":1,\"633\":3,\"674\":2,\"699\":2,\"749\":1,\"760\":4,\"787\":10,\"795\":1,\"805\":1,\"809\":1,\"851\":1,\"864\":1,\"911\":7,\"927\":3,\"1051\":1,\"1155\":3,\"1157\":4,\"1209\":3,\"1487\":1,\"1558\":1,\"1638\":3,\"1640\":1,\"1701\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1708\":2,\"1709\":2,\"1710\":1,\"1711\":2,\"1712\":2,\"1719\":3,\"1721\":3,\"1725\":3,\"1726\":1,\"1727\":1,\"1748\":1,\"1750\":4,\"1768\":1,\"1770\":1,\"1771\":1,\"1801\":1,\"1861\":3,\"1862\":3,\"1945\":1,\"1963\":1,\"1993\":1,\"2130\":2,\"2136\":5,\"2137\":1,\"2219\":2,\"2223\":1,\"2224\":1,\"2245\":1,\"2312\":1,\"2431\":1}}],[\"weight\",{\"0\":{\"1331\":1},\"1\":{\"44\":10,\"78\":2,\"84\":1,\"139\":1,\"144\":6,\"175\":10,\"243\":3,\"267\":4,\"276\":4,\"286\":6,\"290\":2,\"301\":6,\"315\":4,\"321\":2,\"389\":2,\"396\":6,\"421\":6,\"429\":4,\"442\":6,\"463\":12,\"469\":4,\"498\":4,\"505\":4,\"616\":3,\"625\":10,\"637\":3,\"696\":3,\"697\":3,\"700\":1,\"701\":3,\"726\":1,\"727\":1,\"736\":3,\"737\":3,\"742\":1,\"746\":1,\"747\":1,\"769\":1,\"777\":3,\"793\":1,\"859\":1,\"911\":1,\"922\":1,\"947\":1,\"948\":1,\"949\":1,\"958\":2,\"962\":3,\"974\":2,\"1043\":1,\"1155\":1,\"1157\":1,\"1204\":3,\"1209\":2,\"1210\":2,\"1228\":2,\"1290\":1,\"1301\":1,\"1327\":3,\"1330\":3,\"1331\":4,\"1372\":1,\"1389\":4,\"1390\":1,\"1391\":1,\"1392\":1,\"1395\":2,\"1396\":2,\"1397\":1,\"1399\":1,\"1401\":4,\"1402\":2,\"1403\":1,\"1405\":1,\"1408\":5,\"1409\":3,\"1420\":1,\"1434\":1,\"1436\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":3,\"1467\":1,\"1468\":1,\"1469\":1,\"1477\":1,\"1513\":7,\"1521\":2,\"1526\":5,\"1548\":7,\"1549\":1,\"1551\":7,\"1552\":10,\"1553\":7,\"1555\":1,\"1585\":2,\"1592\":7,\"1593\":1,\"1594\":1,\"1595\":2,\"1596\":5,\"1597\":7,\"1598\":5,\"1599\":3,\"1600\":5,\"1605\":7,\"1606\":7,\"1609\":6,\"1610\":7,\"1611\":3,\"1612\":3,\"1613\":3,\"1618\":5,\"1619\":7,\"1625\":8,\"1626\":11,\"1628\":7,\"1638\":6,\"1640\":3,\"1655\":2,\"1702\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":2,\"1716\":2,\"1719\":1,\"1721\":1,\"1725\":1,\"1726\":2,\"1727\":2,\"1748\":2,\"1750\":1,\"1768\":1,\"1839\":2,\"1862\":1,\"1942\":1,\"1959\":1,\"1962\":1,\"1963\":9,\"1964\":1,\"1965\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":2,\"1993\":1,\"1996\":1,\"1997\":3,\"2127\":3,\"2130\":4,\"2136\":4,\"2137\":2,\"2202\":1,\"2213\":1,\"2214\":1,\"2219\":1,\"2221\":3,\"2223\":1,\"2228\":1,\"2229\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":5,\"2318\":1,\"2320\":1,\"2325\":3,\"2327\":3,\"2367\":1,\"2373\":2,\"2398\":1,\"2407\":2,\"2408\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":5,\"2432\":5,\"2446\":1,\"2447\":1,\"2458\":1,\"2460\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1}}],[\"weon\",{\"1\":{\"8\":1,\"202\":1,\"256\":1}}],[\"webrtcvad\",{\"1\":{\"2065\":1}}],[\"webrtcvadmodel\",{\"0\":{\"2065\":1},\"1\":{\"2065\":1}}],[\"webrtc\",{\"0\":{\"2065\":1},\"1\":{\"2065\":4}}],[\"webmushra\",{\"1\":{\"267\":1,\"276\":1,\"286\":1}}],[\"web\",{\"0\":{\"1129\":1},\"1\":{\"110\":1,\"245\":1,\"248\":2,\"267\":1,\"276\":1,\"286\":1,\"356\":1,\"1125\":1,\"1129\":1,\"2044\":1,\"2176\":1}}],[\"webpage\",{\"1\":{\"3\":1,\"33\":1}}],[\"website\",{\"1\":{\"2\":1,\"70\":1}}],[\"we\",{\"1\":{\"3\":1,\"31\":1,\"36\":1,\"38\":1,\"39\":2,\"40\":1,\"44\":1,\"47\":2,\"48\":1,\"49\":1,\"50\":1,\"51\":1,\"52\":1,\"57\":1,\"62\":1,\"69\":1,\"70\":1,\"71\":1,\"78\":1,\"79\":2,\"82\":1,\"84\":1,\"85\":1,\"86\":1,\"95\":1,\"96\":2,\"104\":1,\"106\":4,\"111\":1,\"120\":1,\"138\":1,\"139\":4,\"141\":2,\"142\":1,\"143\":1,\"144\":2,\"146\":2,\"150\":1,\"160\":2,\"161\":1,\"162\":2,\"167\":1,\"173\":1,\"195\":1,\"196\":2,\"197\":4,\"200\":2,\"201\":1,\"206\":1,\"211\":3,\"212\":3,\"213\":2,\"217\":4,\"218\":3,\"219\":1,\"220\":1,\"223\":7,\"224\":1,\"228\":1,\"232\":1,\"242\":2,\"243\":27,\"254\":1,\"255\":1,\"258\":1,\"261\":1,\"262\":4,\"267\":7,\"268\":3,\"269\":6,\"276\":7,\"277\":3,\"278\":6,\"285\":3,\"286\":15,\"289\":1,\"290\":8,\"536\":1,\"696\":1,\"699\":3,\"922\":1,\"927\":1,\"1086\":2,\"1207\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":3,\"1484\":2,\"1493\":1,\"1494\":1,\"1546\":1,\"1622\":1,\"1626\":1,\"1647\":1,\"1655\":1,\"1784\":1,\"1806\":1,\"1808\":1,\"1883\":1,\"1938\":1,\"1943\":1,\"2001\":1,\"2133\":2,\"2184\":1,\"2198\":1,\"2216\":1,\"2355\":1,\"2412\":1}}],[\"kr\",{\"1\":{\"2168\":1}}],[\"krause\",{\"1\":{\"202\":1}}],[\"koreascience\",{\"1\":{\"2168\":1}}],[\"koreancleaner\",{\"0\":{\"2282\":1},\"1\":{\"2282\":1}}],[\"korean\",{\"0\":{\"2282\":1},\"1\":{\"287\":2,\"290\":1,\"481\":3,\"2282\":1}}],[\"k=none\",{\"1\":{\"1301\":1,\"1306\":1,\"1371\":1,\"1372\":1}}],[\"ks=4\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1}}],[\"ks\",{\"1\":{\"1182\":1,\"1183\":1,\"1184\":1,\"1269\":1,\"1270\":2,\"1271\":1}}],[\"ksz\",{\"1\":{\"1145\":1,\"1264\":4,\"1265\":1,\"1334\":4}}],[\"ksz=3\",{\"1\":{\"1265\":1}}],[\"ksz=\",{\"1\":{\"1108\":1,\"1145\":1}}],[\"k$\",{\"1\":{\"1119\":1}}],[\"kpad\",{\"1\":{\"972\":1,\"1075\":1}}],[\"kvcache\",{\"0\":{\"2051\":1}}],[\"kv\",{\"1\":{\"787\":1,\"1794\":2,\"1966\":1}}],[\"kv=false\",{\"1\":{\"787\":1,\"1794\":2}}],[\"kdim=none\",{\"1\":{\"787\":1}}],[\"kürzinger\",{\"1\":{\"768\":1}}],[\"kyubyong\",{\"1\":{\"287\":3}}],[\"kyunghyun\",{\"1\":{\"202\":1}}],[\"khz\",{\"1\":{\"267\":4,\"286\":14}}],[\"khudanpur\",{\"1\":{\"14\":1}}],[\"km500\",{\"1\":{\"259\":1}}],[\"km\",{\"1\":{\"259\":2,\"2138\":2}}],[\"kmeansmodel\",{\"0\":{\"2138\":1},\"1\":{\"2138\":1}}],[\"kmeans\",{\"0\":{\"1489\":1},\"1\":{\"259\":1,\"276\":9,\"1130\":1,\"1389\":2,\"1391\":2,\"1396\":2,\"1400\":4,\"1401\":2,\"1403\":2,\"1406\":2,\"1408\":2,\"1410\":2,\"1441\":6,\"1466\":2,\"1468\":2,\"1469\":6,\"1489\":1}}],[\"kwds\",{\"1\":{\"1824\":1}}],[\"kwargs2args\",{\"0\":{\"2486\":1,\"2494\":2},\"1\":{\"2486\":1,\"2494\":2}}],[\"kwargs\",{\"0\":{\"2477\":1,\"2487\":2},\"1\":{\"614\":1,\"625\":4,\"654\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"724\":1,\"725\":1,\"726\":1,\"728\":1,\"729\":1,\"736\":2,\"737\":2,\"744\":2,\"748\":1,\"756\":3,\"769\":1,\"773\":3,\"777\":1,\"784\":3,\"788\":1,\"789\":1,\"793\":1,\"795\":1,\"819\":3,\"821\":1,\"822\":1,\"827\":2,\"828\":2,\"829\":4,\"830\":3,\"835\":1,\"858\":1,\"859\":1,\"861\":1,\"866\":1,\"867\":1,\"912\":3,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":3,\"956\":1,\"958\":2,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"974\":3,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1051\":4,\"1053\":2,\"1119\":1,\"1155\":4,\"1156\":2,\"1157\":3,\"1158\":3,\"1163\":1,\"1211\":1,\"1218\":1,\"1221\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1253\":2,\"1254\":2,\"1274\":1,\"1294\":1,\"1333\":1,\"1359\":1,\"1381\":4,\"1387\":1,\"1389\":4,\"1395\":4,\"1401\":4,\"1406\":1,\"1408\":4,\"1422\":2,\"1424\":2,\"1426\":2,\"1428\":2,\"1430\":2,\"1439\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1466\":4,\"1471\":1,\"1472\":1,\"1483\":1,\"1485\":1,\"1492\":1,\"1508\":2,\"1521\":3,\"1554\":1,\"1576\":2,\"1578\":1,\"1580\":1,\"1585\":3,\"1586\":1,\"1588\":2,\"1598\":2,\"1600\":2,\"1601\":1,\"1602\":1,\"1603\":2,\"1640\":4,\"1641\":4,\"1652\":1,\"1702\":2,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1720\":1,\"1763\":1,\"1778\":2,\"1797\":1,\"1806\":1,\"1819\":1,\"1823\":1,\"1824\":1,\"1834\":2,\"1838\":1,\"1846\":2,\"1938\":1,\"1940\":2,\"1942\":2,\"1959\":3,\"1965\":3,\"1967\":1,\"1969\":1,\"1971\":3,\"1972\":1,\"1975\":3,\"1992\":1,\"1996\":3,\"1997\":3,\"1999\":1,\"2000\":1,\"2001\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2127\":3,\"2140\":2,\"2167\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2184\":2,\"2187\":1,\"2192\":1,\"2198\":1,\"2203\":1,\"2209\":1,\"2215\":1,\"2216\":3,\"2221\":3,\"2222\":3,\"2228\":3,\"2229\":3,\"2239\":1,\"2249\":1,\"2253\":1,\"2286\":1,\"2288\":1,\"2293\":1,\"2305\":1,\"2325\":1,\"2327\":1,\"2338\":1,\"2344\":1,\"2345\":1,\"2366\":1,\"2401\":1,\"2403\":3,\"2407\":1,\"2408\":3,\"2420\":1,\"2443\":1,\"2445\":3,\"2446\":3,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2462\":2,\"2474\":1,\"2477\":1,\"2486\":1,\"2487\":2,\"2488\":1,\"2494\":1,\"2507\":1}}],[\"kwanghee\",{\"1\":{\"5\":1,\"6\":2}}],[\"kwon\",{\"1\":{\"256\":1}}],[\"k\",{\"0\":{\"1920\":1},\"1\":{\"205\":2,\"232\":1,\"258\":1,\"259\":1,\"271\":2,\"280\":2,\"287\":11,\"616\":6,\"644\":11,\"705\":1,\"784\":4,\"804\":1,\"821\":1,\"824\":1,\"932\":1,\"934\":1,\"971\":3,\"972\":1,\"973\":2,\"975\":3,\"978\":3,\"981\":2,\"982\":2,\"984\":2,\"1063\":2,\"1072\":3,\"1074\":3,\"1075\":1,\"1130\":1,\"1131\":1,\"1148\":3,\"1172\":1,\"1179\":3,\"1198\":1,\"1272\":2,\"1273\":2,\"1274\":2,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1296\":2,\"1301\":1,\"1306\":1,\"1309\":1,\"1311\":1,\"1371\":1,\"1372\":1,\"1400\":3,\"1509\":2,\"1511\":2,\"1516\":2,\"1553\":2,\"1701\":1,\"1713\":4,\"1714\":4,\"1715\":4,\"1716\":4,\"1794\":5,\"1920\":6,\"2000\":2,\"2001\":3,\"2138\":4,\"2151\":2,\"2176\":4,\"2198\":1,\"2202\":1,\"2213\":2,\"2214\":2,\"2299\":1,\"2310\":2,\"2325\":1,\"2327\":1,\"2421\":1}}],[\"kuang\",{\"1\":{\"139\":1}}],[\"kumar\",{\"1\":{\"12\":1}}],[\"knowledge\",{\"1\":{\"286\":2}}],[\"known\",{\"1\":{\"167\":1,\"286\":1,\"821\":2,\"831\":1,\"2474\":1}}],[\"know\",{\"1\":{\"104\":1,\"819\":1,\"2355\":1}}],[\"k80s\",{\"1\":{\"66\":1}}],[\"kinoshita\",{\"1\":{\"1309\":1,\"1311\":1}}],[\"kinds\",{\"1\":{\"54\":1}}],[\"kising\",{\"1\":{\"188\":2}}],[\"kim\",{\"1\":{\"45\":2,\"145\":1,\"202\":1,\"256\":1,\"1269\":2,\"1270\":2,\"1271\":2}}],[\"kiyono\",{\"1\":{\"10\":1}}],[\"kldivloss\",{\"1\":{\"1987\":1}}],[\"kldivergencelosswithoutflow\",{\"0\":{\"1602\":1},\"1\":{\"1602\":1}}],[\"kldivergenceloss\",{\"0\":{\"1601\":1},\"1\":{\"1601\":1}}],[\"kl\",{\"1\":{\"44\":6,\"1553\":3,\"1601\":3,\"1602\":2,\"1625\":3,\"1936\":3}}],[\"kenlm\",{\"1\":{\"2462\":1}}],[\"keops\",{\"0\":{\"871\":1}}],[\"keops=false\",{\"1\":{\"824\":1}}],[\"kept\",{\"1\":{\"760\":1,\"1008\":1,\"1155\":3,\"1157\":3,\"1247\":1,\"1316\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"1862\":1}}],[\"kernerl\",{\"1\":{\"709\":1,\"774\":1,\"780\":1,\"1107\":1,\"1747\":1,\"2191\":1}}],[\"kernel=\",{\"1\":{\"1110\":1,\"1211\":1,\"1238\":1}}],[\"kernels=\",{\"1\":{\"2213\":1,\"2214\":1}}],[\"kernels=2\",{\"1\":{\"2202\":1}}],[\"kernels\",{\"0\":{\"1566\":1},\"1\":{\"704\":1,\"716\":1,\"755\":2,\"780\":1,\"785\":2,\"1513\":1,\"1548\":1,\"1552\":2,\"1553\":1,\"1566\":1,\"1746\":2}}],[\"kernel\",{\"0\":{\"668\":1,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"919\":1},\"1\":{\"43\":8,\"141\":14,\"142\":1,\"243\":2,\"619\":5,\"620\":4,\"622\":4,\"623\":4,\"637\":3,\"649\":1,\"665\":2,\"668\":2,\"691\":3,\"700\":1,\"703\":1,\"704\":2,\"709\":5,\"710\":4,\"711\":3,\"712\":1,\"713\":1,\"715\":1,\"731\":1,\"732\":1,\"733\":2,\"734\":2,\"735\":3,\"755\":1,\"766\":1,\"767\":1,\"768\":3,\"774\":5,\"780\":9,\"781\":2,\"783\":2,\"803\":1,\"804\":1,\"817\":4,\"819\":2,\"821\":5,\"823\":1,\"824\":3,\"825\":1,\"846\":4,\"849\":3,\"878\":4,\"879\":4,\"881\":4,\"882\":4,\"883\":4,\"884\":4,\"919\":1,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"973\":1,\"980\":3,\"981\":1,\"982\":1,\"1080\":3,\"1082\":1,\"1107\":5,\"1110\":2,\"1112\":1,\"1113\":1,\"1114\":1,\"1118\":5,\"1120\":1,\"1122\":1,\"1124\":9,\"1125\":9,\"1142\":1,\"1145\":1,\"1147\":6,\"1148\":1,\"1159\":1,\"1180\":3,\"1181\":3,\"1200\":1,\"1252\":3,\"1264\":2,\"1265\":1,\"1267\":3,\"1268\":3,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1278\":3,\"1286\":1,\"1300\":1,\"1302\":1,\"1331\":1,\"1334\":2,\"1368\":2,\"1369\":1,\"1370\":1,\"1383\":1,\"1389\":4,\"1390\":1,\"1391\":3,\"1392\":3,\"1396\":3,\"1398\":2,\"1401\":6,\"1402\":3,\"1403\":3,\"1404\":2,\"1408\":5,\"1409\":3,\"1410\":2,\"1420\":1,\"1433\":1,\"1435\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":10,\"1452\":10,\"1454\":10,\"1456\":10,\"1458\":3,\"1460\":3,\"1466\":5,\"1467\":2,\"1468\":3,\"1482\":1,\"1484\":2,\"1486\":1,\"1509\":1,\"1511\":1,\"1513\":9,\"1517\":1,\"1519\":6,\"1520\":3,\"1524\":3,\"1525\":3,\"1526\":10,\"1530\":1,\"1535\":6,\"1536\":6,\"1543\":1,\"1546\":6,\"1548\":10,\"1549\":3,\"1551\":9,\"1552\":25,\"1553\":11,\"1554\":1,\"1556\":3,\"1562\":1,\"1563\":1,\"1581\":3,\"1582\":3,\"1583\":3,\"1592\":9,\"1593\":1,\"1594\":2,\"1595\":3,\"1596\":3,\"1597\":3,\"1598\":15,\"1599\":34,\"1600\":15,\"1604\":7,\"1605\":6,\"1606\":5,\"1609\":2,\"1610\":3,\"1611\":3,\"1612\":3,\"1613\":3,\"1614\":3,\"1615\":3,\"1616\":3,\"1618\":1,\"1619\":3,\"1620\":3,\"1621\":3,\"1622\":6,\"1624\":3,\"1625\":11,\"1626\":24,\"1628\":3,\"1668\":5,\"1733\":1,\"1736\":4,\"1737\":3,\"1746\":1,\"1747\":2,\"1748\":2,\"1752\":1,\"1753\":2,\"1756\":7,\"1757\":7,\"1784\":1,\"1789\":7,\"1790\":7,\"1795\":3,\"1854\":2,\"1994\":2,\"2126\":2,\"2129\":3,\"2179\":1,\"2185\":1,\"2191\":5,\"2203\":1,\"2209\":1,\"2236\":3,\"2239\":9,\"2240\":7,\"2245\":3,\"2411\":14,\"2412\":26,\"2423\":25,\"2425\":3,\"2429\":3,\"2431\":3,\"2432\":6,\"2433\":3,\"2447\":23,\"2458\":1,\"2460\":1,\"2465\":1}}],[\"keithito\",{\"1\":{\"288\":1}}],[\"keeps\",{\"1\":{\"2139\":1,\"2144\":1}}],[\"keepdim=false\",{\"1\":{\"1298\":1}}],[\"keeping\",{\"1\":{\"706\":1,\"1250\":1,\"1251\":1,\"2130\":1,\"2136\":1}}],[\"keep\",{\"1\":{\"71\":1,\"79\":1,\"108\":1,\"119\":1,\"138\":1,\"243\":1,\"925\":1,\"978\":1,\"982\":1,\"1153\":1,\"1156\":1,\"1273\":1,\"1833\":1,\"2139\":1,\"2144\":1,\"2339\":2,\"2348\":1,\"2370\":2,\"2372\":1,\"2443\":1,\"2449\":1}}],[\"kevin\",{\"1\":{\"10\":1}}],[\"keyerror\",{\"1\":{\"2134\":1}}],[\"keyword\",{\"1\":{\"1655\":1,\"2355\":2}}],[\"key4\",{\"1\":{\"992\":1,\"994\":1,\"997\":1,\"999\":1,\"1004\":1,\"1006\":1,\"1008\":1,\"1016\":1,\"1018\":1}}],[\"key3\",{\"1\":{\"992\":1,\"994\":1,\"997\":1,\"999\":1,\"1004\":1,\"1006\":1,\"1008\":1,\"1016\":1,\"1018\":1,\"1027\":2}}],[\"key2\",{\"1\":{\"268\":1,\"277\":1,\"992\":1,\"994\":1,\"997\":1,\"999\":1,\"1004\":1,\"1006\":1,\"1008\":2,\"1012\":1,\"1014\":1,\"1016\":1,\"1018\":1,\"1020\":1,\"1023\":2,\"1025\":1,\"1027\":2,\"2359\":7}}],[\"key1\",{\"1\":{\"268\":1,\"277\":1,\"992\":2,\"994\":2,\"997\":2,\"999\":1,\"1004\":2,\"1006\":1,\"1008\":4,\"1012\":2,\"1014\":1,\"1016\":2,\"1018\":1,\"1020\":2,\"1023\":2,\"1025\":2,\"1027\":2,\"2359\":2}}],[\"key>\",{\"1\":{\"88\":2,\"2314\":2}}],[\"key=value\",{\"1\":{\"47\":1}}],[\"keys=true\",{\"1\":{\"2480\":1}}],[\"keys2\",{\"1\":{\"2359\":2}}],[\"keys1\",{\"1\":{\"2359\":1}}],[\"keys>\",{\"1\":{\"88\":1,\"2314\":1}}],[\"keys\",{\"0\":{\"925\":1},\"1\":{\"46\":3,\"81\":3,\"290\":1,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"368\":2,\"385\":2,\"389\":2,\"396\":2,\"406\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":2,\"463\":2,\"469\":2,\"475\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":2,\"505\":2,\"787\":1,\"847\":1,\"925\":2,\"992\":2,\"994\":2,\"997\":2,\"1000\":2,\"1002\":2,\"1004\":2,\"1008\":2,\"1012\":2,\"1016\":2,\"1022\":3,\"1268\":1,\"1719\":5,\"1720\":2,\"1725\":6,\"1749\":1,\"1806\":2,\"1815\":1,\"1824\":3,\"1883\":1,\"1916\":1,\"2039\":2,\"2049\":1,\"2134\":1,\"2139\":1,\"2144\":1,\"2145\":4,\"2146\":4,\"2147\":4,\"2249\":4,\"2253\":1,\"2329\":2,\"2330\":2,\"2331\":2,\"2342\":1,\"2355\":2,\"2359\":2,\"2377\":3}}],[\"key\",{\"0\":{\"2\":1},\"1\":{\"50\":1,\"52\":1,\"79\":2,\"81\":1,\"142\":1,\"225\":3,\"290\":2,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"356\":4,\"361\":2,\"368\":2,\"385\":2,\"389\":2,\"396\":2,\"404\":2,\"406\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"457\":2,\"463\":2,\"469\":2,\"475\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":2,\"505\":2,\"594\":2,\"598\":2,\"633\":4,\"634\":4,\"642\":1,\"644\":14,\"649\":4,\"654\":6,\"784\":7,\"787\":4,\"847\":1,\"912\":1,\"925\":1,\"992\":1,\"994\":1,\"997\":1,\"999\":1,\"1004\":1,\"1006\":1,\"1008\":1,\"1009\":2,\"1010\":1,\"1016\":1,\"1018\":1,\"1029\":1,\"1064\":1,\"1070\":1,\"1071\":1,\"1129\":1,\"1235\":1,\"1262\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1290\":1,\"1396\":1,\"1526\":7,\"1553\":7,\"1643\":1,\"1646\":1,\"1655\":1,\"1675\":2,\"1683\":2,\"1688\":2,\"1719\":3,\"1721\":3,\"1725\":3,\"1749\":1,\"1756\":3,\"1757\":3,\"1774\":1,\"1785\":3,\"1789\":3,\"1790\":3,\"1794\":8,\"1815\":1,\"1817\":3,\"1824\":2,\"1827\":1,\"1830\":1,\"1843\":2,\"1862\":3,\"1950\":1,\"1984\":3,\"2006\":2,\"2130\":1,\"2131\":2,\"2143\":2,\"2145\":3,\"2146\":3,\"2147\":3,\"2235\":9,\"2236\":9,\"2239\":9,\"2240\":10,\"2245\":9,\"2249\":1,\"2344\":1,\"2351\":1,\"2355\":5,\"2359\":9,\"2366\":1,\"2367\":1,\"2377\":2,\"2400\":1}}],[\"kaiser\",{\"1\":{\"1608\":1,\"1631\":2,\"1833\":1}}],[\"kazuhikoarase\",{\"1\":{\"288\":1}}],[\"kazuya\",{\"1\":{\"9\":1}}],[\"ka3\",{\"1\":{\"287\":1}}],[\"kanda\",{\"1\":{\"1717\":1}}],[\"kana\",{\"0\":{\"2297\":1},\"1\":{\"287\":2,\"481\":1,\"2297\":1}}],[\"kan\",{\"1\":{\"267\":1,\"276\":1,\"286\":2,\"290\":2,\"1608\":1,\"2045\":2}}],[\"kano\",{\"1\":{\"15\":1}}],[\"kashiwagi\",{\"1\":{\"202\":1}}],[\"kaldiwriter\",{\"0\":{\"1781\":1},\"1\":{\"1781\":1}}],[\"kaldireader\",{\"0\":{\"1780\":1},\"1\":{\"1780\":1}}],[\"kaldi\",{\"0\":{\"161\":1,\"196\":1,\"2390\":1,\"2392\":1},\"1\":{\"26\":1,\"28\":1,\"37\":2,\"38\":2,\"71\":1,\"79\":1,\"80\":1,\"106\":6,\"109\":2,\"161\":14,\"162\":2,\"165\":1,\"194\":1,\"196\":2,\"197\":3,\"200\":2,\"205\":3,\"211\":1,\"213\":2,\"217\":4,\"218\":1,\"224\":2,\"235\":2,\"242\":2,\"254\":3,\"259\":1,\"260\":1,\"266\":1,\"267\":1,\"268\":2,\"275\":1,\"276\":1,\"277\":2,\"285\":5,\"286\":3,\"289\":1,\"290\":3,\"295\":2,\"415\":2,\"570\":1,\"985\":1,\"1780\":2,\"1781\":2,\"1833\":2,\"1881\":3,\"1882\":1,\"1883\":6,\"2157\":3,\"2390\":1,\"2392\":1}}],[\"kamo\",{\"1\":{\"11\":1,\"1656\":1,\"2016\":2}}],[\"karthik\",{\"1\":{\"12\":1}}],[\"karita\",{\"1\":{\"10\":1,\"156\":1}}],[\"karen\",{\"1\":{\"6\":1}}],[\"katsuki\",{\"1\":{\"9\":1}}],[\"k2\",{\"0\":{\"505\":1},\"1\":{\"1\":1,\"137\":1,\"139\":5,\"153\":1,\"505\":3,\"615\":1,\"625\":6,\"644\":1,\"669\":1,\"670\":1}}],[\"ugly\",{\"1\":{\"2480\":2}}],[\"uid\",{\"1\":{\"2345\":1,\"2352\":1}}],[\"uint8\",{\"1\":{\"70\":1,\"692\":2,\"790\":1,\"850\":1,\"1901\":1,\"1902\":4,\"1903\":1,\"1904\":4,\"1992\":1}}],[\"uk\",{\"1\":{\"1717\":1}}],[\"ukrainian\",{\"1\":{\"481\":1}}],[\"ulaw\",{\"1\":{\"1678\":1}}],[\"uv\",{\"1\":{\"1545\":2}}],[\"umbach\",{\"1\":{\"1318\":1}}],[\"uhifigangenerator\",{\"0\":{\"1548\":1},\"1\":{\"1548\":1}}],[\"uhifigan\",{\"0\":{\"1545\":1,\"1548\":2},\"1\":{\"1545\":1,\"1548\":3}}],[\"uhlenbeck\",{\"1\":{\"1224\":1,\"1225\":1}}],[\"uhh\",{\"1\":{\"1211\":1}}],[\"u+1\",{\"1\":{\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1}}],[\"u=none\",{\"1\":{\"823\":1}}],[\"uasrsmoothnesspenalty\",{\"0\":{\"2473\":1},\"1\":{\"2473\":1}}],[\"uasrpseudolabelloss\",{\"0\":{\"2472\":1},\"1\":{\"2472\":1}}],[\"uasrphonemediversityloss\",{\"0\":{\"2471\":1},\"1\":{\"2471\":1}}],[\"uasrprefixscorer\",{\"0\":{\"1848\":1},\"1\":{\"1848\":1}}],[\"uasrgradientpenalty\",{\"0\":{\"2470\":1},\"1\":{\"2470\":1}}],[\"uasrdiscriminatorloss\",{\"0\":{\"2469\":1},\"1\":{\"2469\":1}}],[\"uasrtraineroptions\",{\"0\":{\"2372\":1},\"1\":{\"2371\":2,\"2372\":1}}],[\"uasrtrainer\",{\"0\":{\"2371\":1},\"1\":{\"2273\":1,\"2371\":1,\"2372\":1}}],[\"uasrtask\",{\"0\":{\"2273\":1},\"1\":{\"2273\":1}}],[\"uasr\",{\"0\":{\"496\":1,\"498\":1,\"505\":1,\"1848\":1,\"2273\":1,\"2371\":1,\"2372\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1,\"2554\":1},\"1\":{\"496\":6,\"498\":8,\"505\":6,\"1000\":1,\"1011\":1,\"1848\":1,\"2273\":1,\"2371\":3,\"2372\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":2,\"2460\":2,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":2,\"2470\":2,\"2471\":2,\"2472\":2,\"2473\":2}}],[\"uasr1\",{\"1\":{\"292\":1}}],[\"ua2\",{\"1\":{\"287\":1}}],[\"uan2\",{\"1\":{\"287\":1}}],[\"u3\",{\"1\":{\"287\":1}}],[\"utf\",{\"1\":{\"290\":1,\"2249\":1}}],[\"utmos\",{\"1\":{\"285\":2,\"1598\":1,\"1625\":1}}],[\"utokyo\",{\"1\":{\"285\":1}}],[\"utagoe\",{\"1\":{\"267\":2,\"268\":2,\"272\":1}}],[\"utt=true\",{\"1\":{\"2354\":1}}],[\"uttb\",{\"1\":{\"988\":2,\"990\":2}}],[\"utta\",{\"1\":{\"988\":2,\"990\":2,\"2007\":1}}],[\"utts\",{\"1\":{\"625\":2}}],[\"utt\",{\"0\":{\"663\":1,\"1869\":1},\"1\":{\"242\":1,\"286\":1,\"295\":4,\"377\":4,\"415\":4,\"449\":4,\"515\":1,\"653\":1,\"663\":1,\"736\":1,\"737\":1,\"954\":1,\"974\":1,\"1155\":1,\"1157\":1,\"1158\":1,\"1395\":1,\"1521\":1,\"1585\":1,\"1640\":1,\"1641\":1,\"1702\":1,\"1842\":1,\"1869\":1,\"1959\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2174\":1,\"2184\":1,\"2221\":1,\"2228\":1,\"2229\":1,\"2343\":2,\"2352\":2,\"2354\":6,\"2408\":1,\"2446\":1}}],[\"utt1\",{\"1\":{\"242\":2,\"2354\":3}}],[\"utt2dataset\",{\"1\":{\"2000\":2}}],[\"utt2rir\",{\"1\":{\"1813\":1}}],[\"utt2ratio=none\",{\"1\":{\"1802\":1,\"1833\":1,\"1852\":1}}],[\"utt2noise=none\",{\"1\":{\"1802\":1}}],[\"utt2num\",{\"1\":{\"201\":1,\"242\":1,\"535\":2}}],[\"utt2lid\",{\"1\":{\"267\":2,\"276\":2,\"286\":2}}],[\"utt2lang\",{\"1\":{\"235\":3,\"266\":1,\"267\":1,\"268\":3,\"275\":1,\"276\":1,\"277\":3,\"285\":1,\"286\":1}}],[\"utt2sid\",{\"1\":{\"267\":2,\"276\":2,\"286\":2}}],[\"utt2spk=none\",{\"1\":{\"1728\":1}}],[\"utt2spk\",{\"1\":{\"196\":8,\"201\":1,\"213\":8,\"224\":1,\"266\":1,\"267\":1,\"268\":8,\"275\":1,\"276\":1,\"277\":8,\"285\":1,\"286\":3,\"2341\":1,\"2345\":1}}],[\"utt2\",{\"1\":{\"242\":2,\"2354\":1}}],[\"utt2category\",{\"1\":{\"223\":1,\"2002\":1,\"2006\":1,\"2007\":1}}],[\"uttid\",{\"1\":{\"213\":2,\"1881\":1,\"1883\":2,\"2343\":2}}],[\"uttidd\",{\"1\":{\"196\":2,\"213\":2,\"268\":3,\"277\":3}}],[\"uttidc\",{\"1\":{\"196\":2,\"213\":2,\"268\":3,\"277\":3}}],[\"uttidb\",{\"1\":{\"196\":8,\"200\":1,\"213\":8,\"268\":10,\"277\":10,\"986\":1,\"1883\":3}}],[\"uttida\",{\"1\":{\"196\":8,\"200\":1,\"213\":8,\"268\":10,\"277\":10,\"986\":2,\"1883\":3}}],[\"utterancecmvn\",{\"0\":{\"1850\":1},\"1\":{\"1850\":1}}],[\"utterancemvn\",{\"0\":{\"1671\":1},\"1\":{\"1671\":1}}],[\"utterance\",{\"0\":{\"1671\":1,\"1700\":2},\"1\":{\"175\":1,\"196\":7,\"200\":1,\"213\":3,\"223\":1,\"235\":4,\"242\":3,\"243\":1,\"268\":8,\"277\":8,\"286\":2,\"290\":1,\"1011\":1,\"1671\":1,\"1700\":3,\"1720\":1,\"1721\":1,\"1726\":1,\"1727\":1,\"1850\":1,\"1869\":1,\"2000\":4,\"2001\":2,\"2183\":2,\"2184\":1,\"2190\":2,\"2208\":2,\"2354\":6}}],[\"utterances\",{\"1\":{\"133\":1,\"196\":2,\"197\":1,\"205\":2,\"211\":1,\"213\":1,\"217\":1,\"223\":1,\"242\":3,\"254\":1,\"266\":1,\"268\":2,\"275\":1,\"277\":2,\"285\":1,\"286\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"2000\":5,\"2001\":7,\"2354\":6,\"2364\":1}}],[\"uttmvn\",{\"1\":{\"128\":1}}],[\"utilization\",{\"1\":{\"2147\":1}}],[\"utilize\",{\"1\":{\"276\":1}}],[\"utilizing\",{\"1\":{\"200\":2}}],[\"utilities\",{\"0\":{\"182\":1},\"1\":{\"109\":4,\"2130\":1}}],[\"utility\",{\"1\":{\"37\":1,\"47\":1,\"68\":1}}],[\"util\",{\"1\":{\"173\":1,\"2215\":1,\"2216\":2}}],[\"utils\",{\"0\":{\"653\":1,\"663\":1,\"665\":1,\"667\":1,\"669\":1,\"670\":1,\"703\":2,\"704\":2,\"716\":2,\"717\":2,\"755\":2,\"764\":2,\"773\":2,\"785\":2,\"801\":1,\"802\":2,\"803\":2,\"868\":1,\"878\":2,\"879\":2,\"880\":1,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"886\":1,\"888\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":1,\"903\":1,\"908\":1,\"909\":1,\"912\":1,\"913\":1,\"914\":1,\"916\":1,\"918\":2,\"919\":2,\"920\":1,\"923\":1,\"925\":1,\"931\":2,\"933\":2,\"940\":1,\"942\":1,\"943\":1,\"946\":1,\"1055\":1,\"1057\":1,\"1068\":1,\"1076\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1110\":1,\"1114\":1,\"1144\":1,\"1151\":1,\"1177\":1,\"1187\":1,\"1196\":1,\"1200\":1,\"1213\":1,\"1219\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1294\":1,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1307\":1,\"1312\":1,\"1324\":1,\"1331\":1,\"1337\":1,\"1338\":1,\"1339\":1,\"1342\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1348\":1,\"1349\":1,\"1355\":1,\"1357\":1,\"1359\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1367\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1559\":1,\"1632\":1,\"1633\":1,\"1675\":1,\"1688\":1,\"1695\":1,\"1718\":1,\"1761\":1,\"1763\":1,\"1772\":1,\"1773\":1,\"1778\":1,\"1780\":1,\"1781\":1,\"1797\":1,\"1819\":1,\"1823\":1,\"1824\":2,\"1825\":1,\"1826\":1,\"1828\":1,\"1829\":1,\"1834\":1,\"1859\":2,\"1868\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1876\":1,\"1881\":1,\"1883\":1,\"1886\":1,\"1887\":2,\"1888\":1,\"1889\":1,\"1892\":1,\"1894\":1,\"1897\":1,\"1898\":2,\"1901\":1,\"1903\":1,\"1905\":1,\"1908\":1,\"1910\":1,\"1911\":1,\"1915\":1,\"1916\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1927\":1,\"1928\":1,\"1931\":1,\"1932\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"2037\":1,\"2039\":1,\"2042\":1,\"2050\":1,\"2059\":1,\"2060\":1,\"2061\":2,\"2070\":1,\"2071\":1,\"2072\":1,\"2073\":1,\"2074\":1,\"2076\":1,\"2077\":1,\"2078\":1,\"2079\":1,\"2080\":1,\"2088\":2,\"2089\":1,\"2090\":1,\"2095\":2,\"2096\":2,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":2,\"2102\":2,\"2104\":1,\"2106\":1,\"2113\":1,\"2114\":1,\"2115\":2,\"2116\":2,\"2117\":1,\"2120\":1,\"2121\":1,\"2151\":1,\"2154\":1,\"2155\":1,\"2163\":1,\"2164\":1,\"2220\":1,\"2304\":1,\"2307\":1,\"2308\":1,\"2309\":1,\"2310\":1,\"2311\":1,\"2312\":1,\"2313\":1,\"2314\":1,\"2316\":1,\"2317\":1,\"2318\":1,\"2319\":1,\"2320\":1,\"2321\":1,\"2322\":1,\"2323\":1,\"2340\":1,\"2380\":1,\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2388\":1,\"2389\":1,\"2395\":1,\"2407\":1,\"2422\":1,\"2474\":1,\"2475\":1,\"2476\":1,\"2477\":1,\"2478\":1,\"2480\":1,\"2481\":1,\"2482\":1,\"2483\":1,\"2484\":1,\"2486\":1,\"2487\":1,\"2489\":1,\"2490\":1,\"2491\":1,\"2492\":1,\"2494\":1,\"2495\":1,\"2496\":1,\"2497\":1,\"2498\":1,\"2499\":1,\"2501\":1,\"2503\":1,\"2504\":1,\"2506\":1,\"2507\":1,\"2512\":1,\"2513\":1,\"2550\":1,\"2555\":1},\"1\":{\"3\":1,\"32\":3,\"37\":4,\"40\":2,\"48\":1,\"82\":2,\"102\":1,\"106\":1,\"109\":1,\"117\":2,\"126\":1,\"133\":1,\"136\":1,\"196\":6,\"197\":1,\"212\":1,\"213\":6,\"223\":5,\"242\":1,\"243\":1,\"267\":5,\"268\":6,\"276\":5,\"277\":6,\"286\":15,\"290\":5,\"516\":1,\"520\":1,\"521\":1,\"523\":1,\"524\":1,\"525\":1,\"653\":1,\"663\":1,\"665\":1,\"667\":1,\"669\":2,\"670\":2,\"703\":2,\"704\":2,\"716\":2,\"717\":2,\"755\":2,\"764\":2,\"773\":2,\"785\":2,\"801\":1,\"802\":2,\"803\":2,\"868\":1,\"878\":2,\"879\":2,\"880\":1,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"886\":1,\"888\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":1,\"903\":1,\"908\":1,\"909\":1,\"912\":1,\"913\":1,\"914\":1,\"916\":1,\"918\":2,\"919\":2,\"920\":1,\"923\":1,\"925\":1,\"931\":2,\"933\":2,\"940\":1,\"942\":1,\"943\":1,\"946\":1,\"1055\":1,\"1057\":1,\"1068\":1,\"1076\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":2,\"1105\":1,\"1110\":1,\"1114\":1,\"1144\":1,\"1151\":1,\"1177\":1,\"1187\":1,\"1196\":1,\"1200\":1,\"1213\":1,\"1219\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1294\":1,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1307\":1,\"1312\":1,\"1324\":1,\"1331\":1,\"1337\":1,\"1338\":1,\"1339\":1,\"1342\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1348\":1,\"1349\":1,\"1355\":1,\"1357\":1,\"1359\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1367\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1559\":1,\"1632\":1,\"1633\":1,\"1675\":1,\"1688\":1,\"1695\":1,\"1718\":1,\"1761\":1,\"1763\":1,\"1772\":1,\"1773\":1,\"1778\":1,\"1780\":1,\"1781\":1,\"1797\":1,\"1819\":1,\"1823\":1,\"1824\":2,\"1825\":1,\"1826\":1,\"1828\":1,\"1829\":1,\"1834\":1,\"1859\":2,\"1868\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1876\":1,\"1881\":1,\"1883\":1,\"1886\":1,\"1887\":2,\"1888\":1,\"1889\":1,\"1892\":1,\"1894\":1,\"1897\":1,\"1898\":2,\"1901\":1,\"1903\":1,\"1905\":1,\"1908\":1,\"1910\":1,\"1911\":1,\"1915\":1,\"1916\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1927\":1,\"1928\":1,\"1931\":1,\"1932\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"2039\":1,\"2061\":2,\"2088\":2,\"2095\":2,\"2096\":2,\"2101\":2,\"2102\":2,\"2115\":2,\"2116\":2,\"2151\":1,\"2154\":1,\"2155\":1,\"2163\":1,\"2164\":1,\"2220\":1,\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2304\":1,\"2307\":1,\"2308\":1,\"2309\":1,\"2310\":1,\"2311\":1,\"2312\":1,\"2313\":1,\"2314\":1,\"2316\":1,\"2317\":1,\"2318\":1,\"2319\":1,\"2320\":1,\"2321\":1,\"2322\":1,\"2323\":1,\"2340\":1,\"2355\":1,\"2380\":1,\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2388\":1,\"2389\":1,\"2395\":1,\"2407\":1,\"2422\":1,\"2474\":1,\"2475\":1,\"2476\":1,\"2477\":1,\"2478\":1,\"2480\":1,\"2481\":1,\"2482\":1,\"2483\":1,\"2484\":1,\"2486\":1,\"2487\":1,\"2489\":1,\"2490\":1,\"2491\":1,\"2492\":1,\"2494\":1,\"2495\":1,\"2496\":1,\"2497\":1,\"2498\":1,\"2499\":1,\"2501\":1,\"2503\":1,\"2504\":1,\"2506\":1,\"2507\":1}}],[\"url\",{\"1\":{\"92\":1,\"249\":3,\"290\":3,\"518\":1,\"738\":1,\"745\":3,\"746\":2,\"747\":3,\"790\":1,\"791\":1,\"864\":1,\"889\":1,\"890\":1,\"891\":1}}],[\"upon\",{\"1\":{\"2276\":1,\"2277\":1}}],[\"up=1\",{\"1\":{\"1369\":1}}],[\"up=false\",{\"1\":{\"1110\":1,\"1238\":1}}],[\"upfirdn2d\",{\"0\":{\"1369\":2,\"1370\":2},\"1\":{\"1369\":2,\"1370\":2}}],[\"upenn\",{\"1\":{\"1002\":1}}],[\"upsampling\",{\"0\":{\"1849\":1},\"1\":{\"1110\":1,\"1264\":1,\"1334\":1,\"1368\":1,\"1371\":3,\"1372\":2,\"1454\":1,\"1456\":1,\"1513\":1,\"1548\":2,\"1551\":1,\"1552\":2,\"1582\":2,\"1590\":1,\"1592\":1,\"1599\":1,\"1605\":1,\"1610\":3,\"1619\":4,\"1624\":2,\"1626\":2,\"1645\":2,\"1849\":7,\"1854\":3,\"1977\":2,\"2000\":6,\"2001\":5,\"2008\":3}}],[\"upsamplenetwork\",{\"0\":{\"1624\":1},\"1\":{\"1624\":2}}],[\"upsampled\",{\"1\":{\"1582\":1,\"1620\":1,\"1621\":1,\"1624\":1,\"1977\":1}}],[\"upsamples\",{\"1\":{\"1371\":1}}],[\"upsampleconv\",{\"0\":{\"1286\":1},\"1\":{\"1286\":1}}],[\"upsample=false\",{\"1\":{\"1108\":1}}],[\"upsample\",{\"0\":{\"860\":1,\"945\":1,\"1284\":1,\"1345\":1,\"1371\":1,\"1372\":1,\"1575\":1,\"1580\":1,\"1582\":1,\"1617\":1,\"1624\":1},\"1\":{\"860\":1,\"945\":1,\"1155\":1,\"1157\":1,\"1284\":1,\"1345\":1,\"1371\":2,\"1372\":2,\"1398\":2,\"1404\":3,\"1408\":3,\"1410\":3,\"1513\":5,\"1526\":2,\"1548\":5,\"1551\":5,\"1552\":5,\"1553\":2,\"1575\":1,\"1580\":1,\"1582\":4,\"1590\":1,\"1592\":6,\"1598\":2,\"1599\":6,\"1600\":2,\"1605\":4,\"1610\":7,\"1617\":1,\"1619\":12,\"1620\":8,\"1621\":8,\"1624\":3,\"1625\":2,\"1626\":5,\"1977\":1,\"2000\":1,\"2001\":1}}],[\"upstream\",{\"1\":{\"128\":6,\"750\":1}}],[\"uppool\",{\"0\":{\"859\":1},\"1\":{\"859\":1}}],[\"upper=1\",{\"1\":{\"1833\":1,\"1852\":1}}],[\"upper=\",{\"1\":{\"1802\":1}}],[\"upper=0\",{\"1\":{\"1717\":1}}],[\"upper\",{\"1\":{\"536\":1,\"709\":1,\"774\":1,\"780\":1,\"1676\":1,\"1785\":1,\"1817\":1,\"2191\":1}}],[\"upto\",{\"1\":{\"699\":1}}],[\"uploads\",{\"1\":{\"211\":1,\"235\":1,\"242\":1,\"254\":1}}],[\"uploaded\",{\"1\":{\"127\":2}}],[\"uploading\",{\"1\":{\"118\":2,\"200\":1,\"205\":1,\"217\":1,\"228\":1,\"285\":1}}],[\"upload\",{\"1\":{\"118\":3,\"127\":1,\"199\":1,\"200\":3,\"204\":1,\"205\":3,\"210\":1,\"211\":4,\"216\":1,\"217\":3,\"222\":2,\"223\":4,\"227\":1,\"228\":1,\"234\":1,\"235\":1,\"240\":1,\"242\":2,\"246\":1,\"253\":1,\"254\":1,\"284\":2,\"285\":3}}],[\"updates\",{\"1\":{\"703\":1,\"745\":1,\"746\":1,\"747\":2,\"755\":3,\"785\":1,\"846\":2,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"2044\":1,\"2355\":2,\"2462\":1}}],[\"updated\",{\"1\":{\"266\":1,\"269\":1,\"275\":1,\"278\":1,\"755\":1,\"785\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"2044\":6,\"2355\":1}}],[\"update\",{\"1\":{\"102\":1,\"259\":2,\"290\":2,\"698\":1,\"755\":1,\"785\":1,\"958\":1,\"960\":4,\"1050\":2,\"1116\":2,\"1161\":2,\"1189\":2,\"1218\":2,\"1221\":2,\"1229\":3,\"1244\":2,\"1400\":1,\"1834\":2,\"2044\":1,\"2176\":2,\"2355\":1}}],[\"updating\",{\"1\":{\"102\":1,\"2355\":1}}],[\"up\",{\"0\":{\"888\":1,\"1110\":1,\"1301\":1,\"1306\":1,\"1331\":1,\"1344\":1,\"1345\":1,\"1371\":1,\"1372\":1},\"1\":{\"84\":1,\"91\":1,\"110\":1,\"139\":1,\"262\":1,\"756\":1,\"773\":1,\"888\":1,\"1110\":1,\"1155\":1,\"1157\":1,\"1246\":1,\"1270\":1,\"1301\":1,\"1306\":1,\"1331\":1,\"1344\":1,\"1345\":1,\"1370\":2,\"1371\":1,\"1372\":1,\"1692\":1,\"1698\":2,\"2040\":1,\"2043\":1,\"2045\":2,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2132\":1,\"2357\":1,\"2367\":1}}],[\"u\",{\"1\":{\"31\":1,\"45\":1,\"70\":1,\"126\":2,\"145\":1,\"243\":2,\"269\":1,\"278\":1,\"286\":1,\"287\":1,\"616\":2,\"627\":1,\"630\":2,\"632\":2,\"634\":2,\"641\":1,\"643\":2,\"649\":2,\"651\":1,\"654\":6,\"667\":3,\"696\":2,\"697\":2,\"703\":15,\"716\":2,\"717\":5,\"740\":1,\"755\":5,\"785\":3,\"804\":3,\"819\":5,\"822\":3,\"823\":2,\"824\":1,\"847\":1,\"878\":4,\"879\":4,\"881\":6,\"882\":4,\"883\":4,\"884\":6,\"919\":7,\"922\":3,\"932\":3,\"934\":3,\"936\":3,\"937\":3,\"1054\":1,\"1309\":1,\"1321\":1,\"1323\":1,\"1327\":1,\"1330\":1,\"1368\":1,\"1509\":1,\"1511\":1,\"1516\":1,\"1545\":1,\"1553\":1,\"1678\":1,\"1749\":12,\"1779\":2,\"1815\":1,\"1847\":5,\"1882\":2,\"1888\":1,\"2218\":2}}],[\"u18\",{\"1\":{\"29\":1}}],[\"ubuntu18\",{\"1\":{\"160\":1}}],[\"ubuntu\",{\"0\":{\"29\":1},\"1\":{\"26\":3,\"27\":2,\"159\":3,\"160\":1,\"161\":2}}],[\"ueda\",{\"1\":{\"12\":1}}],[\"unk\",{\"1\":{\"2291\":1,\"2336\":1,\"2337\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1}}],[\"unknown\",{\"1\":{\"81\":2,\"286\":1,\"1155\":1,\"1157\":1,\"1252\":1,\"1350\":1}}],[\"unfortunately\",{\"1\":{\"2280\":1}}],[\"unfolded\",{\"1\":{\"1334\":1}}],[\"unfold\",{\"1\":{\"1334\":1}}],[\"unfolding\",{\"1\":{\"1269\":2,\"1270\":2,\"1271\":2,\"1334\":1}}],[\"unchanged\",{\"1\":{\"2162\":2}}],[\"uncased\",{\"1\":{\"2137\":1}}],[\"unconstrained\",{\"0\":{\"1636\":1},\"1\":{\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1636\":1}}],[\"uncompressed\",{\"1\":{\"71\":1,\"243\":1}}],[\"uncommenting\",{\"1\":{\"267\":1}}],[\"uncomment\",{\"1\":{\"3\":1}}],[\"unregistered\",{\"1\":{\"2134\":4,\"2149\":1,\"2166\":1}}],[\"unrolling\",{\"1\":{\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"744\":1,\"784\":1,\"828\":1,\"829\":1,\"830\":1}}],[\"unbatchfy\",{\"1\":{\"1719\":1}}],[\"unormalized\",{\"1\":{\"1477\":1}}],[\"unet\",{\"0\":{\"1341\":1,\"1368\":1},\"1\":{\"1341\":1,\"1368\":1,\"1548\":1}}],[\"unmasked\",{\"1\":{\"846\":1,\"1638\":1,\"2218\":4,\"2219\":1}}],[\"unless\",{\"1\":{\"817\":1,\"819\":1,\"2220\":1,\"2355\":2}}],[\"unlike\",{\"1\":{\"79\":1,\"96\":1,\"197\":1,\"262\":1,\"1883\":1}}],[\"unpad2d\",{\"0\":{\"1506\":1},\"1\":{\"1506\":1}}],[\"unpad1d\",{\"0\":{\"1505\":1},\"1\":{\"1505\":1}}],[\"unpadded\",{\"1\":{\"770\":1,\"959\":1,\"2184\":1}}],[\"unpaired\",{\"1\":{\"1000\":1}}],[\"unpack\",{\"0\":{\"1955\":1},\"1\":{\"286\":1,\"1955\":1,\"1956\":1}}],[\"untie\",{\"1\":{\"674\":2}}],[\"until\",{\"1\":{\"138\":1,\"696\":1,\"697\":1,\"1677\":1}}],[\"un1\",{\"1\":{\"287\":1}}],[\"unvoiced\",{\"1\":{\"267\":2,\"276\":2,\"1545\":1,\"2298\":3}}],[\"unused\",{\"1\":{\"243\":1,\"377\":2,\"449\":2,\"738\":1,\"1062\":1,\"1211\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1334\":2,\"1748\":2,\"2219\":1,\"2348\":1,\"2370\":2,\"2372\":1}}],[\"unavailable\",{\"1\":{\"242\":1}}],[\"unnormalized\",{\"1\":{\"1634\":3,\"1635\":3,\"1636\":3}}],[\"unno\",{\"1\":{\"156\":1}}],[\"unnecessary\",{\"1\":{\"81\":1,\"1310\":1}}],[\"undocumented\",{\"1\":{\"152\":1,\"403\":14}}],[\"undefined\",{\"1\":{\"126\":1,\"243\":1}}],[\"undernormalization\",{\"1\":{\"882\":1,\"883\":1,\"884\":1,\"922\":1}}],[\"understand\",{\"1\":{\"162\":1,\"197\":1,\"212\":1,\"218\":1,\"267\":1,\"276\":1,\"286\":1,\"1962\":1}}],[\"understanding\",{\"0\":{\"12\":1,\"107\":1,\"180\":1,\"186\":1,\"250\":1},\"1\":{\"11\":1,\"12\":1,\"175\":1,\"180\":1,\"190\":1,\"202\":1}}],[\"under\",{\"1\":{\"24\":1,\"38\":1,\"48\":2,\"138\":3,\"161\":2,\"162\":2,\"224\":6,\"225\":2,\"232\":1,\"243\":1,\"258\":1,\"260\":1,\"269\":2,\"278\":2,\"290\":1,\"786\":1,\"866\":1,\"921\":1,\"1450\":1,\"1452\":1,\"1717\":1,\"2216\":1}}],[\"underlying\",{\"1\":{\"0\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1}}],[\"unsigned\",{\"1\":{\"1678\":1}}],[\"unsplit\",{\"1\":{\"1026\":1}}],[\"unsortedbatchsampler\",{\"0\":{\"2006\":1},\"1\":{\"2006\":1}}],[\"unsorted\",{\"0\":{\"2006\":1},\"1\":{\"95\":1,\"96\":1,\"243\":1,\"449\":2,\"2006\":1,\"2007\":2}}],[\"unsupervised\",{\"0\":{\"14\":1,\"292\":1},\"1\":{\"14\":1,\"1000\":1,\"2425\":1,\"2429\":1,\"2430\":1,\"2462\":2}}],[\"unsupported\",{\"1\":{\"3\":1}}],[\"univnet\",{\"1\":{\"1534\":1}}],[\"universal\",{\"1\":{\"202\":1,\"1061\":1,\"1062\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1}}],[\"universlu\",{\"1\":{\"202\":1}}],[\"unidirectional\",{\"1\":{\"1124\":1,\"1125\":1,\"1176\":1}}],[\"unifying\",{\"1\":{\"1155\":1,\"1157\":1}}],[\"uniform\",{\"0\":{\"1504\":1,\"1907\":1},\"1\":{\"846\":2,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1504\":1,\"1526\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1907\":2,\"1994\":1,\"2001\":1,\"2220\":2,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2432\":1,\"2447\":1}}],[\"unified\",{\"1\":{\"9\":1,\"82\":1,\"102\":1,\"167\":1,\"245\":1,\"1309\":1,\"1311\":1,\"2044\":1}}],[\"union\",{\"1\":{\"701\":1,\"709\":1,\"735\":1,\"774\":1,\"780\":1,\"978\":1,\"1053\":1,\"1062\":1,\"1107\":2,\"1117\":1,\"1118\":2,\"1125\":1,\"1130\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1162\":1,\"1209\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":1,\"1280\":1,\"1283\":1,\"1334\":1,\"1548\":1,\"1584\":2,\"1587\":2,\"1591\":1,\"2039\":1,\"2101\":1,\"2191\":1,\"2426\":1}}],[\"unicode=none\",{\"1\":{\"2480\":1}}],[\"unicode\",{\"1\":{\"290\":1}}],[\"unigram50000\",{\"1\":{\"243\":3}}],[\"unique\",{\"1\":{\"43\":1,\"78\":1,\"141\":1,\"211\":1,\"242\":2}}],[\"unitysynthesizer\",{\"0\":{\"1995\":1},\"1\":{\"1995\":1}}],[\"unity\",{\"0\":{\"1995\":1},\"1\":{\"1995\":2}}],[\"unitarily\",{\"1\":{\"924\":1}}],[\"units`\",{\"1\":{\"1995\":1}}],[\"units=128\",{\"1\":{\"1985\":1}}],[\"units=192\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1}}],[\"units=256\",{\"1\":{\"1750\":1,\"1811\":1,\"2223\":1}}],[\"units\",{\"0\":{\"203\":1,\"291\":1},\"1\":{\"52\":1,\"243\":3,\"627\":1,\"692\":2,\"700\":1,\"706\":1,\"709\":3,\"710\":3,\"711\":3,\"713\":1,\"731\":1,\"732\":1,\"733\":2,\"734\":2,\"748\":2,\"766\":1,\"767\":1,\"771\":2,\"774\":3,\"775\":1,\"780\":6,\"781\":1,\"847\":1,\"848\":1,\"849\":3,\"850\":1,\"1107\":3,\"1118\":3,\"1180\":1,\"1181\":1,\"1199\":1,\"1213\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1278\":3,\"1519\":3,\"1535\":3,\"1536\":3,\"1546\":3,\"1598\":1,\"1599\":5,\"1600\":1,\"1622\":3,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1750\":3,\"1768\":2,\"1809\":3,\"1810\":1,\"1812\":2,\"1814\":2,\"1815\":2,\"1816\":2,\"1895\":2,\"1992\":4,\"1993\":4,\"1994\":3,\"1995\":3,\"2126\":1,\"2129\":3,\"2191\":3,\"2223\":3,\"2235\":2,\"2236\":2,\"2239\":2,\"2240\":2,\"2245\":8,\"2411\":5,\"2412\":5,\"2423\":5,\"2425\":4,\"2429\":3,\"2431\":8,\"2432\":8,\"2447\":2}}],[\"unit\",{\"0\":{\"412\":1},\"1\":{\"37\":2,\"73\":1,\"162\":1,\"175\":1,\"222\":1,\"225\":2,\"622\":1,\"712\":1,\"715\":1,\"783\":1,\"979\":1,\"1117\":2,\"1130\":2,\"1131\":2,\"1136\":2,\"1141\":2,\"1232\":2,\"1261\":2,\"1456\":1,\"1945\":1,\"1947\":3,\"1975\":3,\"1992\":5,\"1995\":4,\"2355\":1,\"2445\":1}}],[\"usually\",{\"1\":{\"205\":1,\"217\":1,\"219\":1,\"269\":1,\"273\":1,\"276\":1,\"278\":1,\"635\":2,\"650\":2,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"2184\":1}}],[\"uslu14\",{\"1\":{\"201\":1}}],[\"us\",{\"1\":{\"78\":1,\"286\":1,\"287\":1,\"290\":1,\"481\":1}}],[\"usages\",{\"0\":{\"170\":1},\"1\":{\"691\":1}}],[\"usage\",{\"0\":{\"36\":1,\"69\":1,\"84\":1,\"128\":1,\"134\":1,\"139\":1,\"167\":1},\"1\":{\"107\":1,\"119\":1,\"134\":1,\"135\":1,\"138\":1,\"173\":1,\"220\":3,\"223\":1,\"290\":1,\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"374\":1,\"377\":1,\"385\":1,\"389\":1,\"396\":1,\"402\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":1,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"511\":1,\"513\":1,\"514\":1,\"515\":1,\"516\":1,\"517\":1,\"518\":1,\"519\":1,\"520\":1,\"521\":1,\"522\":1,\"523\":1,\"524\":1,\"525\":1,\"526\":1,\"527\":1,\"528\":1,\"529\":1,\"531\":1,\"533\":1,\"535\":1,\"536\":1,\"537\":1,\"538\":1,\"541\":1,\"543\":1,\"545\":1,\"548\":1,\"551\":1,\"554\":1,\"556\":1,\"558\":1,\"561\":1,\"564\":1,\"567\":1,\"570\":1,\"572\":1,\"575\":1,\"578\":1,\"581\":1,\"583\":1,\"586\":1,\"589\":1,\"592\":1,\"594\":1,\"596\":1,\"598\":1,\"600\":1,\"603\":1,\"606\":1,\"609\":1,\"611\":1,\"696\":1,\"697\":1,\"756\":2,\"773\":2,\"777\":1,\"1011\":1,\"1118\":1,\"1156\":1,\"1794\":1,\"2130\":1,\"2136\":1,\"2184\":1,\"2367\":1,\"2480\":1}}],[\"usage2rst\",{\"1\":{\"3\":2}}],[\"usr\",{\"1\":{\"31\":1,\"40\":1,\"50\":1,\"117\":1,\"126\":1,\"161\":1,\"163\":1,\"243\":1}}],[\"usebias\",{\"1\":{\"731\":1,\"732\":1,\"766\":1,\"767\":1}}],[\"useful\",{\"1\":{\"141\":1,\"733\":1,\"828\":1,\"993\":1,\"1050\":1,\"1086\":2,\"1116\":1,\"1155\":1,\"1157\":1,\"1161\":1,\"1189\":1,\"1207\":2,\"1218\":1,\"1221\":1,\"1224\":1,\"1225\":1,\"1229\":1,\"1244\":1,\"1245\":2,\"1655\":1,\"1794\":1,\"1883\":1,\"1918\":1,\"1919\":1,\"2000\":1,\"2162\":1}}],[\"usesseparator\",{\"0\":{\"1283\":1},\"1\":{\"1283\":1}}],[\"uses2separator\",{\"0\":{\"1280\":1},\"1\":{\"1280\":1}}],[\"uses2\",{\"0\":{\"1029\":1,\"1071\":1,\"1235\":1,\"1280\":1,\"1281\":2,\"1282\":2},\"1\":{\"1029\":1,\"1071\":1,\"1235\":1,\"1280\":2,\"1281\":4,\"1282\":4}}],[\"uses\",{\"0\":{\"1070\":1,\"1073\":1,\"1279\":2,\"1283\":1},\"1\":{\"39\":1,\"41\":1,\"161\":1,\"197\":1,\"240\":1,\"267\":1,\"702\":1,\"1070\":1,\"1073\":1,\"1279\":4,\"1283\":2,\"1402\":1,\"1454\":1,\"1456\":1,\"1467\":1,\"1469\":1,\"1533\":1,\"1594\":1,\"1595\":1,\"1672\":2,\"1716\":1,\"1720\":1,\"1721\":1,\"1725\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1,\"1806\":1,\"1833\":1,\"1862\":1,\"1938\":1,\"2065\":1,\"2130\":2,\"2137\":1,\"2138\":1,\"2147\":1,\"2155\":1,\"2280\":1,\"2447\":1}}],[\"users\",{\"1\":{\"46\":1,\"107\":1,\"139\":1,\"223\":1,\"224\":1,\"232\":1,\"245\":1,\"259\":1,\"267\":1,\"269\":1,\"276\":1,\"278\":1}}],[\"user\",{\"1\":{\"22\":1,\"25\":2,\"36\":1,\"43\":1,\"49\":1,\"107\":1,\"138\":2,\"147\":1,\"150\":1,\"246\":1,\"1533\":1,\"2039\":1,\"2044\":1,\"2049\":1}}],[\"use\",{\"0\":{\"41\":1,\"49\":1,\"72\":1,\"74\":1,\"115\":1,\"124\":1,\"247\":1},\"1\":{\"18\":1,\"25\":1,\"27\":1,\"31\":2,\"39\":2,\"41\":6,\"43\":12,\"44\":5,\"52\":3,\"60\":1,\"69\":1,\"70\":1,\"71\":2,\"84\":1,\"88\":1,\"92\":2,\"95\":1,\"96\":1,\"101\":1,\"102\":1,\"103\":1,\"110\":1,\"121\":1,\"128\":1,\"130\":2,\"131\":2,\"132\":1,\"136\":1,\"139\":3,\"141\":4,\"153\":1,\"159\":1,\"161\":5,\"162\":1,\"168\":3,\"173\":1,\"178\":1,\"193\":1,\"195\":1,\"196\":1,\"197\":1,\"200\":5,\"205\":1,\"206\":1,\"211\":3,\"212\":1,\"213\":1,\"217\":2,\"218\":1,\"223\":5,\"224\":2,\"225\":1,\"242\":3,\"243\":12,\"247\":1,\"248\":1,\"249\":1,\"255\":1,\"259\":1,\"260\":1,\"261\":1,\"262\":1,\"263\":1,\"266\":2,\"267\":21,\"268\":1,\"269\":3,\"275\":2,\"276\":16,\"277\":1,\"278\":3,\"284\":1,\"285\":7,\"286\":41,\"287\":5,\"289\":1,\"290\":25,\"356\":6,\"377\":10,\"406\":4,\"449\":12,\"475\":4,\"484\":4,\"490\":4,\"505\":2,\"513\":1,\"514\":1,\"536\":3,\"614\":1,\"616\":1,\"617\":1,\"618\":1,\"619\":1,\"620\":4,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":4,\"634\":1,\"635\":3,\"636\":1,\"641\":1,\"643\":1,\"644\":4,\"651\":1,\"652\":4,\"661\":2,\"692\":5,\"696\":1,\"697\":1,\"699\":4,\"700\":4,\"709\":8,\"710\":4,\"711\":3,\"713\":1,\"715\":1,\"720\":3,\"731\":1,\"732\":1,\"733\":5,\"734\":4,\"738\":8,\"745\":1,\"747\":1,\"748\":4,\"759\":1,\"760\":2,\"766\":1,\"767\":1,\"768\":3,\"771\":1,\"774\":7,\"775\":2,\"780\":11,\"781\":2,\"783\":2,\"790\":1,\"791\":1,\"792\":1,\"798\":3,\"815\":3,\"817\":1,\"818\":1,\"820\":1,\"821\":1,\"828\":1,\"829\":1,\"830\":1,\"847\":3,\"848\":2,\"849\":1,\"850\":3,\"862\":3,\"960\":1,\"978\":1,\"1002\":1,\"1029\":4,\"1064\":3,\"1107\":7,\"1110\":1,\"1118\":11,\"1124\":2,\"1125\":1,\"1126\":3,\"1127\":2,\"1128\":1,\"1139\":1,\"1141\":1,\"1145\":2,\"1147\":2,\"1176\":1,\"1185\":1,\"1217\":6,\"1235\":4,\"1246\":2,\"1251\":1,\"1253\":1,\"1264\":2,\"1265\":2,\"1267\":1,\"1268\":4,\"1269\":5,\"1270\":6,\"1271\":5,\"1273\":1,\"1274\":1,\"1278\":4,\"1279\":3,\"1280\":8,\"1281\":4,\"1282\":3,\"1283\":5,\"1334\":2,\"1353\":1,\"1385\":1,\"1389\":5,\"1390\":2,\"1391\":3,\"1396\":4,\"1400\":2,\"1401\":5,\"1402\":3,\"1403\":3,\"1408\":8,\"1409\":4,\"1410\":3,\"1419\":6,\"1420\":2,\"1441\":2,\"1450\":2,\"1452\":2,\"1454\":3,\"1456\":3,\"1458\":2,\"1460\":2,\"1466\":4,\"1467\":1,\"1468\":3,\"1469\":1,\"1489\":1,\"1509\":1,\"1511\":1,\"1513\":6,\"1515\":1,\"1516\":1,\"1519\":14,\"1526\":24,\"1529\":9,\"1530\":1,\"1534\":1,\"1535\":6,\"1536\":4,\"1539\":3,\"1541\":1,\"1543\":1,\"1546\":10,\"1548\":10,\"1549\":4,\"1551\":6,\"1552\":30,\"1553\":16,\"1592\":6,\"1593\":2,\"1594\":1,\"1595\":3,\"1596\":8,\"1597\":6,\"1598\":16,\"1599\":31,\"1600\":22,\"1605\":5,\"1606\":3,\"1607\":7,\"1609\":3,\"1610\":5,\"1611\":3,\"1612\":6,\"1613\":6,\"1614\":3,\"1618\":2,\"1619\":3,\"1620\":1,\"1621\":1,\"1622\":7,\"1625\":13,\"1626\":22,\"1627\":4,\"1628\":12,\"1645\":3,\"1662\":2,\"1704\":1,\"1707\":2,\"1712\":1,\"1713\":1,\"1714\":1,\"1716\":1,\"1735\":1,\"1736\":2,\"1748\":2,\"1749\":3,\"1750\":7,\"1751\":1,\"1756\":6,\"1757\":6,\"1758\":4,\"1759\":1,\"1764\":4,\"1766\":3,\"1789\":6,\"1790\":6,\"1794\":6,\"1801\":2,\"1806\":1,\"1810\":3,\"1815\":3,\"1833\":1,\"1839\":4,\"1843\":2,\"1883\":1,\"1936\":2,\"1943\":1,\"1949\":1,\"1955\":1,\"1957\":1,\"1976\":2,\"1991\":2,\"1992\":6,\"1993\":13,\"1994\":4,\"1995\":6,\"2000\":1,\"2001\":2,\"2043\":1,\"2044\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2126\":1,\"2127\":1,\"2129\":1,\"2130\":3,\"2136\":2,\"2145\":1,\"2155\":1,\"2167\":1,\"2176\":1,\"2187\":1,\"2191\":6,\"2192\":1,\"2203\":1,\"2216\":1,\"2217\":2,\"2223\":5,\"2224\":4,\"2226\":4,\"2231\":4,\"2235\":10,\"2236\":10,\"2237\":4,\"2239\":22,\"2240\":15,\"2241\":4,\"2245\":26,\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2280\":1,\"2336\":2,\"2337\":2,\"2346\":1,\"2348\":5,\"2353\":1,\"2355\":6,\"2364\":1,\"2368\":1,\"2369\":1,\"2370\":10,\"2372\":5,\"2384\":1,\"2385\":1,\"2404\":3,\"2409\":1,\"2411\":25,\"2412\":26,\"2413\":4,\"2423\":25,\"2424\":4,\"2431\":24,\"2432\":22,\"2434\":1,\"2447\":22,\"2448\":4,\"2462\":3,\"2480\":1}}],[\"used\",{\"1\":{\"3\":1,\"31\":1,\"39\":1,\"40\":1,\"43\":1,\"44\":1,\"45\":1,\"67\":4,\"68\":1,\"70\":1,\"78\":2,\"81\":1,\"82\":2,\"101\":2,\"110\":3,\"126\":2,\"128\":1,\"133\":1,\"138\":2,\"139\":1,\"141\":1,\"145\":1,\"159\":1,\"161\":1,\"162\":1,\"163\":1,\"166\":1,\"175\":2,\"194\":1,\"197\":1,\"203\":1,\"218\":1,\"223\":3,\"224\":1,\"228\":1,\"233\":1,\"235\":1,\"242\":5,\"243\":2,\"252\":1,\"259\":1,\"261\":1,\"262\":1,\"267\":2,\"276\":3,\"286\":1,\"290\":1,\"535\":1,\"620\":2,\"625\":1,\"636\":2,\"640\":1,\"666\":1,\"691\":1,\"692\":1,\"699\":1,\"700\":2,\"701\":2,\"702\":1,\"703\":2,\"705\":1,\"709\":2,\"710\":5,\"711\":5,\"717\":3,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"733\":2,\"734\":2,\"737\":1,\"745\":1,\"746\":1,\"747\":2,\"748\":2,\"749\":1,\"755\":1,\"756\":4,\"760\":1,\"770\":2,\"771\":1,\"773\":4,\"774\":2,\"780\":5,\"785\":2,\"787\":1,\"792\":1,\"819\":1,\"828\":1,\"829\":3,\"830\":2,\"831\":1,\"846\":3,\"849\":1,\"850\":2,\"859\":1,\"866\":1,\"867\":1,\"922\":1,\"929\":1,\"939\":1,\"954\":1,\"959\":2,\"962\":1,\"974\":3,\"980\":1,\"1029\":1,\"1061\":1,\"1062\":2,\"1107\":1,\"1112\":1,\"1113\":1,\"1118\":1,\"1126\":2,\"1131\":1,\"1136\":1,\"1141\":1,\"1145\":2,\"1147\":1,\"1155\":6,\"1156\":2,\"1157\":6,\"1162\":2,\"1217\":1,\"1222\":1,\"1223\":1,\"1232\":1,\"1235\":1,\"1240\":1,\"1245\":1,\"1246\":1,\"1252\":1,\"1261\":1,\"1265\":1,\"1267\":1,\"1268\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1273\":1,\"1274\":1,\"1278\":1,\"1279\":3,\"1280\":10,\"1281\":4,\"1282\":1,\"1283\":5,\"1305\":1,\"1316\":1,\"1320\":1,\"1327\":1,\"1330\":1,\"1354\":3,\"1368\":3,\"1385\":2,\"1400\":1,\"1420\":1,\"1441\":2,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1469\":1,\"1478\":1,\"1484\":1,\"1524\":1,\"1526\":2,\"1529\":1,\"1545\":1,\"1549\":3,\"1551\":1,\"1552\":5,\"1553\":2,\"1597\":1,\"1598\":2,\"1600\":2,\"1604\":2,\"1606\":2,\"1612\":1,\"1619\":1,\"1625\":2,\"1626\":2,\"1640\":1,\"1641\":1,\"1676\":1,\"1678\":2,\"1679\":1,\"1680\":2,\"1686\":1,\"1692\":2,\"1694\":1,\"1697\":1,\"1698\":3,\"1725\":1,\"1726\":1,\"1727\":1,\"1735\":2,\"1748\":1,\"1751\":3,\"1756\":2,\"1757\":2,\"1759\":2,\"1779\":2,\"1788\":1,\"1789\":2,\"1790\":2,\"1794\":2,\"1824\":1,\"1881\":1,\"1936\":1,\"1951\":1,\"1959\":1,\"1975\":1,\"1997\":1,\"2007\":5,\"2127\":1,\"2129\":1,\"2130\":1,\"2131\":2,\"2136\":1,\"2143\":1,\"2184\":2,\"2188\":1,\"2220\":2,\"2221\":1,\"2246\":2,\"2248\":2,\"2249\":5,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":3,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2344\":1,\"2354\":4,\"2355\":3,\"2364\":1,\"2367\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2447\":1}}],[\"using\",{\"0\":{\"24\":1,\"25\":1,\"32\":1,\"62\":1,\"63\":1,\"64\":1,\"68\":1,\"76\":1,\"88\":1,\"125\":1,\"126\":1,\"165\":1},\"1\":{\"1\":1,\"2\":1,\"6\":1,\"22\":2,\"24\":1,\"25\":2,\"26\":3,\"32\":1,\"33\":1,\"39\":1,\"41\":1,\"46\":2,\"69\":2,\"79\":1,\"80\":1,\"91\":2,\"93\":1,\"96\":1,\"102\":3,\"110\":1,\"120\":1,\"121\":1,\"125\":1,\"126\":1,\"127\":1,\"128\":2,\"139\":1,\"144\":1,\"159\":1,\"162\":1,\"163\":1,\"168\":1,\"175\":1,\"190\":2,\"196\":1,\"197\":2,\"200\":1,\"201\":4,\"206\":2,\"207\":1,\"212\":2,\"218\":2,\"228\":1,\"235\":4,\"236\":1,\"242\":1,\"243\":3,\"244\":1,\"246\":2,\"255\":2,\"261\":1,\"262\":5,\"267\":4,\"268\":1,\"269\":1,\"274\":1,\"275\":1,\"276\":3,\"277\":1,\"278\":1,\"284\":1,\"285\":4,\"286\":5,\"290\":2,\"404\":1,\"527\":2,\"558\":1,\"575\":1,\"592\":1,\"625\":1,\"691\":1,\"705\":1,\"706\":1,\"738\":1,\"768\":3,\"778\":1,\"804\":1,\"823\":1,\"833\":1,\"864\":1,\"911\":3,\"932\":1,\"934\":1,\"1000\":1,\"1130\":1,\"1209\":1,\"1246\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1301\":1,\"1372\":1,\"1477\":1,\"1533\":1,\"1536\":1,\"1668\":1,\"1670\":1,\"1691\":1,\"1699\":1,\"1713\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1833\":1,\"1934\":1,\"1948\":1,\"1965\":1,\"1997\":1,\"2000\":1,\"2001\":2,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2021\":1,\"2040\":1,\"2045\":3,\"2049\":1,\"2065\":1,\"2131\":1,\"2133\":1,\"2136\":2,\"2137\":2,\"2139\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2249\":1,\"2287\":1,\"2312\":1,\"2354\":1,\"2355\":2,\"2359\":1,\"2380\":2,\"2490\":1}}],[\"m=3\",{\"1\":{\"1880\":1}}],[\"mravanelli\",{\"1\":{\"1668\":1}}],[\"mrd\",{\"1\":{\"1420\":2}}],[\"mwf\",{\"0\":{\"1323\":1,\"1327\":1,\"1330\":1},\"1\":{\"1126\":1,\"1323\":2,\"1327\":2,\"1330\":3}}],[\"mmap\",{\"1\":{\"1000\":1}}],[\"mms\",{\"1\":{\"236\":1}}],[\"mha\",{\"1\":{\"784\":1}}],[\"mb\",{\"1\":{\"703\":1,\"919\":4}}],[\"mbg\",{\"1\":{\"696\":3}}],[\"mʲ\",{\"1\":{\"287\":1}}],[\"mvdr\",{\"0\":{\"1321\":1,\"1322\":1},\"1\":{\"720\":1,\"1126\":2,\"1217\":1,\"1321\":2,\"1322\":2,\"1327\":1,\"1354\":2}}],[\"mv\",{\"1\":{\"286\":2}}],[\"mvn\",{\"0\":{\"1656\":1,\"1671\":1,\"1700\":2},\"1\":{\"243\":2,\"1656\":1,\"1671\":1,\"1700\":2}}],[\"mfd\",{\"1\":{\"1511\":1,\"1549\":1}}],[\"mfmcwf\",{\"1\":{\"1334\":6}}],[\"mfaconformerencoder\",{\"0\":{\"2191\":1},\"1\":{\"2191\":1}}],[\"mfa\",{\"1\":{\"284\":1,\"285\":8,\"2191\":2}}],[\"mfcc\",{\"1\":{\"217\":1,\"285\":1,\"524\":1,\"2187\":1,\"2198\":1}}],[\"mcwf\",{\"1\":{\"1334\":2}}],[\"mcd\",{\"1\":{\"267\":4,\"276\":4,\"285\":5,\"286\":6}}],[\"mcleavey\",{\"1\":{\"202\":1}}],[\"mlr\",{\"1\":{\"1126\":1}}],[\"mlabels\",{\"1\":{\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2}}],[\"mlmdecoder\",{\"0\":{\"775\":1},\"1\":{\"775\":1,\"777\":1}}],[\"mlm\",{\"0\":{\"775\":1},\"1\":{\"775\":1,\"2219\":1}}],[\"ml\",{\"1\":{\"260\":1}}],[\"mlp\",{\"0\":{\"1205\":1,\"1568\":1},\"1\":{\"44\":4,\"713\":1,\"781\":1,\"1029\":3,\"1064\":3,\"1205\":1,\"1235\":3,\"1262\":3,\"1280\":3,\"1281\":3,\"1282\":3,\"1568\":1,\"1784\":1,\"1993\":1,\"2245\":1,\"2431\":1}}],[\"mkdir\",{\"1\":{\"243\":1}}],[\"mkl\",{\"1\":{\"161\":4}}],[\"mdcdconfig\",{\"0\":{\"1532\":1},\"1\":{\"1532\":1}}],[\"mdc\",{\"0\":{\"1530\":1},\"1\":{\"1530\":1}}],[\"mdpi\",{\"1\":{\"650\":1}}],[\"md\",{\"1\":{\"201\":1,\"225\":1,\"260\":1,\"290\":1,\"1053\":1,\"2221\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1}}],[\"mttask\",{\"0\":{\"2261\":1},\"1\":{\"2261\":1}}],[\"mt\",{\"0\":{\"396\":1,\"586\":1,\"1760\":1,\"1957\":1,\"1959\":1,\"1960\":1,\"1961\":1,\"2261\":1,\"2534\":1},\"1\":{\"261\":1,\"262\":2,\"396\":6,\"515\":1,\"586\":1,\"737\":1,\"1760\":2,\"1892\":1,\"1957\":1,\"1959\":2,\"1960\":1,\"1961\":1,\"2221\":3,\"2261\":1}}],[\"mt1\",{\"1\":{\"195\":1,\"197\":1,\"238\":1}}],[\"mtlalpha\",{\"1\":{\"175\":4,\"2221\":2}}],[\"mtl\",{\"1\":{\"46\":1}}],[\"msd\",{\"1\":{\"1420\":2,\"1549\":1}}],[\"msstft\",{\"0\":{\"1392\":1,\"1415\":1,\"1422\":1,\"1426\":1,\"1482\":1},\"1\":{\"1392\":1,\"1397\":2,\"1408\":1,\"1409\":2,\"1415\":1,\"1422\":1,\"1426\":1,\"1482\":1}}],[\"msmpmb\",{\"0\":{\"1413\":1,\"1417\":1,\"1420\":1,\"1471\":1,\"1472\":1},\"1\":{\"1389\":1,\"1390\":1,\"1413\":1,\"1417\":1,\"1420\":1,\"1471\":1,\"1472\":1}}],[\"msa\",{\"1\":{\"1262\":1,\"1290\":1}}],[\"msfblock\",{\"0\":{\"1196\":1},\"1\":{\"1196\":1}}],[\"mse\",{\"1\":{\"1157\":1,\"1175\":1,\"1217\":1,\"1277\":1,\"1389\":2,\"1396\":2,\"1401\":2,\"1408\":2,\"1466\":2,\"1526\":2,\"1553\":2,\"1584\":2,\"1591\":2,\"1598\":2,\"1600\":2,\"1607\":2,\"1625\":2,\"2239\":1,\"2240\":1,\"2241\":1}}],[\"ms\",{\"1\":{\"136\":1,\"639\":1,\"1422\":1,\"2065\":5,\"2435\":1}}],[\"mps\",{\"1\":{\"2249\":1}}],[\"mp\",{\"1\":{\"2176\":2}}],[\"mpd\",{\"1\":{\"1420\":1,\"1549\":1}}],[\"mpdr\",{\"1\":{\"1126\":1}}],[\"mp4\",{\"1\":{\"74\":2}}],[\"mp3\",{\"1\":{\"70\":1,\"71\":4,\"1678\":1,\"2054\":1}}],[\"mpirun\",{\"1\":{\"64\":1}}],[\"mpi\",{\"1\":{\"59\":1,\"64\":1,\"377\":1,\"449\":1}}],[\"myespnetmodel\",{\"1\":{\"960\":1}}],[\"my\",{\"1\":{\"67\":1,\"242\":1,\"267\":1,\"276\":1,\"284\":3,\"286\":1,\"290\":4}}],[\"m\",{\"1\":{\"56\":2,\"57\":1,\"58\":2,\"61\":3,\"62\":1,\"63\":1,\"64\":1,\"79\":1,\"80\":1,\"81\":3,\"82\":1,\"84\":4,\"85\":2,\"86\":2,\"87\":1,\"88\":6,\"89\":1,\"90\":1,\"91\":1,\"92\":1,\"93\":2,\"94\":4,\"96\":1,\"97\":1,\"98\":2,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":1,\"242\":1,\"287\":9,\"290\":6,\"691\":1,\"783\":1,\"947\":1,\"949\":2,\"971\":3,\"972\":2,\"973\":2,\"975\":3,\"978\":6,\"981\":2,\"982\":3,\"984\":2,\"1029\":1,\"1072\":3,\"1074\":3,\"1075\":2,\"1148\":3,\"1179\":3,\"1198\":1,\"1235\":1,\"1269\":2,\"1270\":2,\"1271\":1,\"1272\":2,\"1273\":3,\"1274\":5,\"1296\":2,\"1321\":1,\"1322\":1,\"1487\":1,\"1601\":2,\"1602\":4,\"1874\":1,\"1879\":1,\"1880\":1,\"1896\":2,\"1931\":2,\"2000\":3,\"2151\":2,\"2198\":2,\"2218\":2,\"2242\":1,\"2243\":1,\"2310\":2,\"2435\":3}}],[\"million\",{\"1\":{\"2151\":1,\"2310\":1}}],[\"millions\",{\"1\":{\"2151\":1,\"2310\":1}}],[\"millisecond\",{\"1\":{\"1162\":2}}],[\"milliseconds\",{\"1\":{\"134\":1,\"135\":1,\"148\":1,\"2065\":2}}],[\"mics\",{\"1\":{\"1164\":2,\"1279\":1,\"1281\":1,\"1282\":1,\"1334\":1}}],[\"mic\",{\"1\":{\"1059\":1,\"1137\":1,\"1163\":2,\"1164\":2,\"1264\":2,\"1333\":2,\"1334\":2}}],[\"microphones\",{\"1\":{\"1264\":2,\"1269\":2,\"1270\":2,\"1271\":2,\"1334\":5}}],[\"microphone\",{\"1\":{\"247\":1,\"527\":1,\"1117\":1,\"1125\":2,\"1280\":1,\"1283\":1,\"1717\":1}}],[\"microsoft\",{\"1\":{\"71\":1}}],[\"mimicing\",{\"1\":{\"716\":1}}],[\"mid\",{\"1\":{\"991\":1,\"992\":5}}],[\"midreader\",{\"0\":{\"991\":1},\"1\":{\"991\":1}}],[\"miditoolkit\",{\"1\":{\"269\":1,\"278\":1}}],[\"midi\",{\"1\":{\"265\":1,\"269\":2,\"274\":1,\"278\":2,\"995\":1,\"1017\":1,\"1521\":15,\"1526\":1,\"1546\":3,\"1552\":2,\"2228\":15,\"2229\":15,\"2232\":4,\"2235\":6,\"2236\":6,\"2238\":6,\"2239\":3,\"2240\":3,\"2244\":1,\"2245\":1,\"2363\":1,\"2434\":2,\"2435\":5}}],[\"middle\",{\"0\":{\"174\":1},\"1\":{\"90\":1,\"91\":1,\"290\":1,\"722\":1,\"1645\":1,\"1650\":1}}],[\"mitigate\",{\"1\":{\"173\":1}}],[\"misaligned\",{\"1\":{\"269\":1,\"278\":1}}],[\"mismatch\",{\"1\":{\"243\":1,\"290\":1,\"2308\":1,\"2314\":1}}],[\"misc\",{\"1\":{\"202\":1}}],[\"miss\",{\"1\":{\"2476\":1}}],[\"missed\",{\"1\":{\"153\":1}}],[\"missing\",{\"1\":{\"138\":1,\"269\":2,\"278\":2,\"1484\":1,\"2134\":1}}],[\"mish\",{\"0\":{\"635\":1,\"2420\":1},\"1\":{\"141\":2,\"635\":3,\"664\":2,\"2420\":2}}],[\"min=0\",{\"1\":{\"821\":1,\"1224\":1}}],[\"minus\",{\"1\":{\"625\":1,\"803\":1,\"804\":3,\"931\":1,\"932\":3,\"933\":1,\"934\":3}}],[\"minutes\",{\"1\":{\"113\":1,\"290\":1}}],[\"minlen\",{\"1\":{\"389\":2,\"1719\":1,\"1720\":1,\"1725\":1}}],[\"minlenratio=0\",{\"1\":{\"1750\":1,\"2224\":1}}],[\"minlenratio<0\",{\"1\":{\"1725\":1}}],[\"minlenratio\",{\"1\":{\"175\":4,\"301\":2,\"315\":2,\"396\":2,\"406\":4,\"421\":2,\"429\":2,\"442\":2,\"463\":4,\"469\":2,\"484\":2,\"490\":2,\"505\":2,\"1720\":2,\"1721\":2,\"1725\":2,\"1726\":1,\"1727\":1,\"1750\":2,\"1806\":1,\"1862\":2,\"1976\":1,\"1993\":2,\"2224\":2,\"2245\":2,\"2431\":2,\"2432\":2}}],[\"minmax\",{\"1\":{\"267\":1}}],[\"minor\",{\"1\":{\"145\":1,\"223\":1,\"722\":1}}],[\"min\",{\"1\":{\"84\":2,\"141\":1,\"175\":1,\"211\":1,\"217\":1,\"266\":1,\"275\":1,\"285\":1,\"295\":2,\"415\":2,\"514\":2,\"537\":1,\"606\":2,\"664\":2,\"674\":4,\"778\":1,\"819\":1,\"821\":2,\"846\":4,\"941\":2,\"1224\":4,\"1225\":4,\"1316\":4,\"1533\":3,\"1547\":1,\"1558\":2,\"1634\":3,\"1635\":3,\"1636\":3,\"1676\":2,\"1720\":1,\"1721\":1,\"1725\":2,\"1806\":1,\"1862\":1,\"1949\":1,\"1999\":1,\"2000\":2,\"2001\":2,\"2002\":1,\"2003\":1,\"2004\":1,\"2007\":2,\"2008\":1,\"2014\":3,\"2015\":2,\"2019\":2,\"2020\":4,\"2021\":2,\"2065\":2,\"2220\":4,\"2336\":1,\"2359\":1,\"2442\":2,\"2462\":1,\"2482\":1,\"2495\":1}}],[\"mindcf\",{\"1\":{\"254\":1}}],[\"mind\",{\"1\":{\"79\":1,\"108\":1,\"119\":1,\"138\":1}}],[\"miniomnie2emodel\",{\"0\":{\"2054\":1},\"1\":{\"2054\":1}}],[\"minimal\",{\"1\":{\"2280\":1}}],[\"minimization\",{\"1\":{\"1309\":1,\"1311\":1}}],[\"minimum\",{\"1\":{\"141\":1,\"147\":1,\"175\":2,\"254\":1,\"516\":1,\"537\":1,\"661\":1,\"664\":1,\"846\":3,\"1132\":1,\"1167\":1,\"1204\":1,\"1209\":1,\"1228\":1,\"1316\":2,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1419\":1,\"1533\":1,\"1558\":1,\"1607\":1,\"1667\":1,\"1750\":3,\"1993\":1,\"2000\":1,\"2001\":1,\"2065\":1,\"2147\":1,\"2220\":3,\"2224\":3,\"2245\":1,\"2431\":1,\"2432\":1,\"2442\":1,\"2482\":1,\"2495\":1}}],[\"mini\",{\"0\":{\"94\":1,\"95\":1,\"123\":1,\"2022\":1,\"2023\":1,\"2024\":1,\"2025\":1,\"2036\":1,\"2037\":1,\"2038\":1,\"2041\":1,\"2042\":1,\"2046\":1,\"2047\":1,\"2048\":1,\"2050\":1,\"2051\":1,\"2052\":1,\"2053\":1,\"2054\":1,\"2057\":1,\"2058\":1,\"2059\":1,\"2060\":1,\"2061\":1,\"2062\":1,\"2063\":1,\"2064\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2070\":1,\"2071\":1,\"2072\":1,\"2073\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2078\":1,\"2079\":1,\"2080\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2085\":1,\"2086\":1,\"2087\":1,\"2088\":1,\"2089\":1,\"2090\":1,\"2091\":1,\"2092\":1,\"2093\":1,\"2094\":1,\"2095\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2119\":1,\"2120\":1,\"2121\":1,\"2122\":1,\"2123\":1},\"1\":{\"78\":2,\"79\":3,\"95\":1,\"96\":1,\"97\":1,\"98\":2,\"99\":3,\"100\":2,\"101\":3,\"102\":3,\"196\":2,\"197\":1,\"290\":1,\"1245\":1,\"1647\":1,\"1760\":1,\"1845\":1,\"2001\":1,\"2007\":2,\"2044\":1,\"2054\":2,\"2061\":1,\"2088\":1,\"2095\":1,\"2096\":1,\"2102\":1,\"2115\":1,\"2116\":1,\"2249\":4,\"2253\":4}}],[\"minibatch=none\",{\"1\":{\"1254\":2}}],[\"minibatches\",{\"1\":{\"1155\":1,\"1157\":1}}],[\"minibatch\",{\"0\":{\"48\":1},\"1\":{\"48\":5,\"284\":1,\"290\":1,\"703\":3,\"716\":3,\"755\":2,\"785\":2,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"908\":1}}],[\"miniconda\",{\"1\":{\"31\":1,\"162\":2}}],[\"miniforge\",{\"1\":{\"1\":1,\"162\":4}}],[\"mixture\",{\"1\":{\"982\":2,\"1159\":1,\"1204\":1,\"1264\":1,\"1269\":3,\"1270\":3,\"1271\":3,\"1273\":2,\"1274\":2,\"1334\":7,\"2341\":1}}],[\"mixupaugment\",{\"0\":{\"1667\":1},\"1\":{\"1667\":1}}],[\"mixup\",{\"0\":{\"1667\":1},\"1\":{\"958\":1,\"1667\":4}}],[\"mixitsolver\",{\"0\":{\"1204\":1},\"1\":{\"1204\":1}}],[\"mixit\",{\"0\":{\"1204\":1},\"1\":{\"224\":2,\"1204\":2}}],[\"mixing\",{\"1\":{\"43\":1,\"592\":1,\"630\":1,\"649\":1,\"2341\":1,\"2350\":2}}],[\"mixed\",{\"0\":{\"103\":1},\"1\":{\"43\":1,\"223\":1,\"243\":5,\"748\":1,\"1113\":2,\"1217\":1,\"1223\":1,\"1251\":2,\"1667\":1,\"2353\":1,\"2364\":1}}],[\"mix\",{\"0\":{\"592\":1},\"1\":{\"32\":1,\"162\":1,\"276\":8,\"475\":2,\"592\":1,\"1155\":8,\"1157\":8,\"1158\":8,\"1174\":1,\"1253\":1,\"1270\":3,\"1271\":3,\"1291\":1,\"1892\":1,\"2341\":1,\"2346\":1,\"2350\":1,\"2353\":2,\"2364\":2,\"2368\":1}}],[\"might\",{\"1\":{\"31\":1,\"37\":1,\"47\":1,\"60\":1,\"82\":1,\"96\":2,\"108\":1,\"162\":1,\"210\":1,\"212\":1,\"213\":1,\"218\":1,\"242\":2,\"243\":1,\"248\":1,\"265\":1,\"267\":1,\"269\":1,\"274\":1,\"276\":1,\"278\":1,\"286\":1,\"699\":1,\"1063\":1,\"1484\":1,\"1881\":1,\"2130\":2,\"2355\":2,\"2380\":1}}],[\"mutablemapping\",{\"1\":{\"2481\":1}}],[\"mutlitokenizercommonpreprocessor\",{\"0\":{\"2356\":1},\"1\":{\"2356\":1}}],[\"mutually\",{\"1\":{\"1655\":1}}],[\"mun\",{\"1\":{\"2203\":1}}],[\"mundo\",{\"1\":{\"287\":1}}],[\"mu=256\",{\"1\":{\"1873\":1,\"1877\":1}}],[\"mulenc\",{\"1\":{\"1892\":1}}],[\"mul\",{\"1\":{\"1268\":1,\"1274\":1}}],[\"muladdadaptlayer\",{\"0\":{\"1207\":1},\"1\":{\"1207\":2}}],[\"mulcatblock\",{\"0\":{\"1208\":1},\"1\":{\"1208\":3}}],[\"mulcat\",{\"1\":{\"1133\":2,\"1208\":1,\"1252\":1}}],[\"mults\",{\"1\":{\"1385\":2,\"1401\":1,\"1402\":1,\"1466\":1,\"1467\":1}}],[\"mult=\",{\"1\":{\"1211\":1}}],[\"mult\",{\"0\":{\"870\":1,\"871\":1,\"872\":1},\"1\":{\"674\":2,\"746\":1,\"747\":1,\"846\":2,\"2014\":2}}],[\"multimodal\",{\"0\":{\"2130\":1,\"2133\":1,\"2136\":1,\"2137\":1,\"2138\":1,\"2139\":1,\"2144\":1},\"1\":{\"2130\":3,\"2133\":1,\"2136\":1,\"2137\":1,\"2138\":1,\"2139\":1,\"2140\":2,\"2143\":2,\"2144\":1,\"2148\":1,\"2287\":1}}],[\"multimask\",{\"0\":{\"978\":1},\"1\":{\"978\":1,\"980\":1}}],[\"multitask\",{\"0\":{\"1942\":1},\"1\":{\"1942\":1}}],[\"multifrequencydiscriminator\",{\"0\":{\"1534\":1},\"1\":{\"1534\":1}}],[\"multinomial\",{\"0\":{\"1497\":1,\"2107\":1},\"1\":{\"1497\":1}}],[\"multidiscriminator\",{\"0\":{\"1415\":1},\"1\":{\"1415\":1,\"1422\":1}}],[\"multibanddiscriminator\",{\"0\":{\"1413\":1},\"1\":{\"1413\":1}}],[\"multiblankrnntnumba\",{\"0\":{\"866\":1},\"1\":{\"866\":2}}],[\"multiblankrnntlossnumba\",{\"0\":{\"786\":1},\"1\":{\"786\":2}}],[\"multiblank\",{\"0\":{\"882\":1,\"883\":1,\"884\":1,\"921\":1,\"922\":1},\"1\":{\"786\":1,\"882\":1,\"883\":1,\"884\":1,\"921\":1,\"922\":1}}],[\"multiblankgpurnnt\",{\"0\":{\"785\":1},\"1\":{\"785\":1}}],[\"multiblocks\",{\"0\":{\"636\":1},\"1\":{\"636\":6,\"655\":2}}],[\"multiresl1specloss\",{\"0\":{\"1210\":1},\"1\":{\"1210\":1,\"1334\":1}}],[\"multiref\",{\"0\":{\"554\":1},\"1\":{\"554\":1}}],[\"multisequential\",{\"0\":{\"1796\":1},\"1\":{\"1796\":2,\"1863\":1,\"1917\":1}}],[\"multiscale\",{\"0\":{\"1569\":1},\"1\":{\"1530\":1,\"1569\":1}}],[\"multiscalestftdiscriminator\",{\"0\":{\"1422\":1},\"1\":{\"1422\":1}}],[\"multiscalemultiperiodmultibanddiscriminator\",{\"0\":{\"1420\":1},\"1\":{\"1420\":1}}],[\"multiscalemelspectrogramloss\",{\"0\":{\"1419\":1},\"1\":{\"1419\":1}}],[\"multiscalediscriminator\",{\"0\":{\"1417\":1},\"1\":{\"1417\":1}}],[\"multisoundscpreader\",{\"0\":{\"993\":1},\"1\":{\"993\":1}}],[\"multispkr\",{\"0\":{\"849\":1},\"1\":{\"849\":1}}],[\"multicut\",{\"1\":{\"2139\":1,\"2150\":3}}],[\"multicgmlp\",{\"1\":{\"780\":9}}],[\"multiconvolutionalspatialgatingunit\",{\"0\":{\"783\":1},\"1\":{\"783\":1}}],[\"multiconvolutionalgatingmlp\",{\"0\":{\"781\":1},\"1\":{\"781\":1}}],[\"multiconv\",{\"0\":{\"781\":1,\"783\":1},\"1\":{\"781\":1,\"783\":1}}],[\"multiconvconformerencoder\",{\"0\":{\"780\":1},\"1\":{\"780\":1}}],[\"multiconvformer\",{\"0\":{\"780\":1},\"1\":{\"780\":2}}],[\"multichannel\",{\"1\":{\"768\":1,\"1126\":1,\"1321\":1,\"1322\":1,\"1327\":2,\"1330\":2}}],[\"multilabelauprc\",{\"1\":{\"960\":2}}],[\"multilabelauprccallback\",{\"0\":{\"960\":1},\"1\":{\"960\":1}}],[\"multilabel\",{\"1\":{\"960\":1}}],[\"multilayeredconv1d\",{\"0\":{\"1795\":1},\"1\":{\"1735\":1,\"1737\":1,\"1751\":1,\"1759\":1,\"1795\":2}}],[\"multilayerpitsolver\",{\"0\":{\"1209\":1},\"1\":{\"1209\":1}}],[\"multilayer\",{\"0\":{\"1209\":1},\"1\":{\"128\":2,\"223\":1,\"738\":2,\"815\":1,\"1209\":1,\"1539\":1}}],[\"multilingual\",{\"1\":{\"268\":3,\"277\":3}}],[\"multiheadattention\",{\"0\":{\"787\":1,\"1984\":1},\"1\":{\"787\":1,\"911\":1,\"1984\":1}}],[\"multiheadedattention\",{\"0\":{\"784\":1,\"1794\":1,\"2421\":1},\"1\":{\"644\":1,\"784\":2,\"1735\":1,\"1751\":2,\"1759\":1,\"1785\":1,\"1794\":2,\"1817\":1,\"1847\":1,\"1891\":1,\"2421\":2}}],[\"multiheaddamped\",{\"1\":{\"637\":1}}],[\"multiheaddampedema\",{\"0\":{\"637\":1},\"1\":{\"637\":10}}],[\"multihead\",{\"0\":{\"1984\":1},\"1\":{\"53\":1,\"1599\":1,\"1984\":2,\"1993\":1,\"2245\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":1}}],[\"multi\",{\"0\":{\"44\":1,\"76\":1,\"93\":1,\"121\":1,\"144\":1,\"198\":1,\"412\":1,\"636\":1,\"637\":1,\"703\":1,\"704\":1,\"716\":1,\"717\":1,\"755\":1,\"764\":1,\"773\":1,\"785\":1,\"786\":2,\"800\":2,\"801\":1,\"802\":1,\"803\":1,\"866\":2,\"867\":2,\"868\":1,\"873\":2,\"874\":2,\"875\":2,\"876\":2,\"878\":1,\"879\":1,\"880\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"886\":1,\"888\":1,\"896\":1,\"897\":1,\"899\":1,\"908\":1,\"909\":1,\"916\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":2,\"922\":1,\"923\":1,\"931\":1,\"933\":1,\"935\":2,\"936\":1,\"937\":1,\"940\":1,\"946\":1,\"978\":1,\"993\":1,\"1026\":1,\"1737\":1,\"1795\":1,\"2337\":1,\"2393\":1},\"1\":{\"41\":1,\"43\":2,\"44\":1,\"53\":1,\"54\":8,\"55\":1,\"56\":1,\"76\":4,\"94\":1,\"104\":1,\"106\":1,\"123\":1,\"141\":3,\"144\":1,\"173\":1,\"198\":2,\"199\":1,\"200\":9,\"201\":2,\"211\":8,\"213\":2,\"260\":1,\"261\":2,\"262\":7,\"265\":2,\"267\":7,\"274\":2,\"275\":3,\"276\":22,\"284\":4,\"286\":18,\"289\":2,\"301\":2,\"449\":2,\"592\":1,\"636\":1,\"637\":2,\"692\":1,\"696\":9,\"703\":1,\"704\":1,\"709\":1,\"710\":1,\"711\":1,\"716\":1,\"717\":1,\"748\":1,\"755\":1,\"764\":1,\"773\":1,\"774\":1,\"780\":1,\"783\":1,\"784\":1,\"785\":4,\"786\":4,\"787\":1,\"800\":2,\"801\":1,\"802\":1,\"803\":1,\"849\":1,\"866\":5,\"867\":2,\"868\":1,\"873\":2,\"874\":2,\"875\":2,\"876\":2,\"878\":1,\"879\":1,\"880\":1,\"881\":1,\"882\":3,\"883\":3,\"884\":3,\"886\":1,\"888\":1,\"896\":1,\"897\":1,\"899\":1,\"907\":1,\"908\":1,\"909\":1,\"916\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":5,\"922\":4,\"923\":1,\"931\":1,\"933\":1,\"935\":2,\"936\":1,\"937\":1,\"940\":1,\"946\":1,\"958\":1,\"962\":2,\"978\":1,\"993\":1,\"1007\":1,\"1008\":3,\"1009\":6,\"1010\":1,\"1026\":1,\"1027\":1,\"1064\":1,\"1066\":1,\"1078\":1,\"1107\":1,\"1130\":1,\"1153\":1,\"1155\":1,\"1157\":1,\"1202\":1,\"1204\":1,\"1209\":2,\"1210\":2,\"1217\":1,\"1228\":1,\"1262\":1,\"1264\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":1,\"1290\":2,\"1323\":1,\"1327\":1,\"1330\":2,\"1334\":6,\"1354\":1,\"1390\":2,\"1397\":1,\"1402\":3,\"1409\":4,\"1413\":1,\"1419\":1,\"1422\":1,\"1467\":3,\"1511\":1,\"1515\":1,\"1516\":1,\"1526\":3,\"1534\":3,\"1536\":1,\"1549\":5,\"1553\":4,\"1593\":1,\"1594\":3,\"1595\":7,\"1598\":2,\"1600\":3,\"1606\":2,\"1625\":2,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1667\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1713\":5,\"1714\":4,\"1715\":5,\"1716\":6,\"1734\":1,\"1737\":1,\"1771\":2,\"1785\":1,\"1794\":1,\"1795\":3,\"1796\":2,\"1817\":1,\"1860\":1,\"1895\":1,\"1992\":1,\"1995\":1,\"2000\":2,\"2045\":1,\"2129\":1,\"2130\":7,\"2131\":1,\"2136\":2,\"2139\":2,\"2143\":2,\"2150\":1,\"2159\":1,\"2176\":2,\"2184\":1,\"2191\":2,\"2203\":1,\"2232\":1,\"2238\":1,\"2249\":1,\"2258\":2,\"2280\":1,\"2331\":1,\"2337\":1,\"2342\":1,\"2344\":2,\"2355\":1,\"2390\":1,\"2393\":2,\"2397\":2,\"2399\":1,\"2421\":2,\"2430\":1,\"2432\":1,\"2435\":1,\"2443\":1}}],[\"multiprocessiterator\",{\"0\":{\"1797\":1},\"1\":{\"1797\":2}}],[\"multiprocessing\",{\"0\":{\"58\":1,\"62\":1,\"64\":1},\"1\":{\"54\":4,\"56\":3,\"57\":1,\"58\":3,\"61\":3,\"62\":1,\"63\":1,\"64\":1,\"263\":1,\"374\":2,\"377\":2,\"449\":2,\"2130\":6,\"2133\":1,\"2136\":1,\"2137\":1,\"2276\":1,\"2277\":1,\"2340\":2,\"2384\":1,\"2385\":1}}],[\"multiplication\",{\"1\":{\"818\":1,\"1051\":1}}],[\"multiplier\",{\"1\":{\"689\":1,\"2000\":1,\"2001\":1}}],[\"multipliers\",{\"1\":{\"689\":1}}],[\"multiplied\",{\"1\":{\"94\":1,\"768\":1,\"2220\":1}}],[\"multiply\",{\"1\":{\"335\":2,\"1555\":1}}],[\"multipleiterfactory\",{\"0\":{\"1648\":1},\"1\":{\"1648\":1}}],[\"multiplers\",{\"1\":{\"1385\":1}}],[\"multiples\",{\"1\":{\"1029\":2,\"1235\":2,\"1919\":1}}],[\"multiple\",{\"0\":{\"173\":1,\"1648\":1,\"2078\":1},\"1\":{\"24\":2,\"50\":1,\"67\":2,\"69\":1,\"120\":1,\"165\":2,\"173\":7,\"174\":1,\"196\":1,\"197\":3,\"200\":4,\"224\":1,\"232\":1,\"235\":1,\"240\":2,\"242\":2,\"258\":1,\"267\":1,\"268\":1,\"277\":1,\"520\":1,\"536\":1,\"538\":1,\"554\":1,\"696\":1,\"978\":2,\"993\":1,\"1008\":1,\"1009\":1,\"1139\":1,\"1209\":1,\"1252\":1,\"1270\":1,\"1279\":2,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1306\":1,\"1371\":1,\"1420\":1,\"1648\":1,\"1717\":1,\"1729\":1,\"1730\":1,\"2131\":1,\"2132\":1,\"2134\":1,\"2143\":1,\"2176\":1,\"2216\":2,\"2249\":1,\"2344\":1,\"2355\":17,\"2366\":1,\"2369\":1,\"2474\":1}}],[\"multipurpose\",{\"1\":{\"10\":1}}],[\"mu\",{\"0\":{\"1873\":1,\"1877\":1},\"1\":{\"1126\":1,\"1327\":1,\"1330\":1,\"1678\":1,\"1873\":4,\"1877\":4}}],[\"much\",{\"1\":{\"106\":2,\"224\":1,\"696\":1,\"821\":2,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1}}],[\"musescore\",{\"1\":{\"269\":1,\"278\":1}}],[\"mustard\",{\"1\":{\"201\":2}}],[\"must\",{\"1\":{\"22\":1,\"67\":1,\"78\":1,\"82\":2,\"98\":1,\"161\":1,\"162\":1,\"168\":1,\"185\":2,\"197\":3,\"242\":3,\"261\":2,\"262\":1,\"286\":1,\"699\":1,\"706\":1,\"755\":1,\"756\":3,\"773\":3,\"820\":1,\"821\":1,\"824\":2,\"828\":1,\"829\":1,\"866\":1,\"867\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"927\":1,\"960\":1,\"994\":1,\"1124\":1,\"1139\":1,\"1224\":1,\"1225\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1292\":1,\"1334\":1,\"1350\":2,\"1454\":1,\"1456\":1,\"1962\":1,\"2000\":1,\"2001\":1,\"2134\":2,\"2215\":1,\"2309\":1,\"2325\":1,\"2327\":5,\"2355\":2}}],[\"musical\",{\"1\":{\"2240\":2}}],[\"music21\",{\"1\":{\"269\":1,\"278\":1}}],[\"musicxml\",{\"1\":{\"265\":1,\"268\":1,\"269\":2,\"274\":1,\"277\":1,\"278\":2,\"1018\":4}}],[\"music\",{\"1\":{\"13\":1,\"211\":1,\"268\":2,\"277\":2,\"2235\":1,\"2236\":1}}],[\"muskits\",{\"1\":{\"13\":1}}],[\"muhammad\",{\"1\":{\"6\":1}}],[\"mobile\",{\"1\":{\"1125\":1}}],[\"moment\",{\"1\":{\"927\":1}}],[\"momentum=0\",{\"1\":{\"1078\":1}}],[\"momentum\",{\"1\":{\"141\":2,\"1065\":1,\"1962\":1}}],[\"moving\",{\"1\":{\"633\":1,\"637\":1,\"1400\":2,\"1441\":2,\"1469\":2,\"1481\":1}}],[\"moves\",{\"1\":{\"1387\":2}}],[\"move\",{\"1\":{\"38\":2,\"200\":1,\"201\":1,\"205\":1,\"206\":1,\"212\":1,\"218\":1,\"236\":1,\"255\":1,\"267\":1,\"276\":1,\"286\":2}}],[\"mol\",{\"1\":{\"536\":13}}],[\"mos\",{\"1\":{\"267\":1,\"276\":2,\"285\":7,\"286\":4,\"1128\":1,\"1598\":6,\"1625\":6}}],[\"mostly\",{\"1\":{\"242\":1,\"243\":1}}],[\"most\",{\"1\":{\"39\":1,\"138\":1,\"228\":1,\"262\":2,\"290\":1,\"1246\":1,\"1350\":1}}],[\"mozillanzg\",{\"1\":{\"287\":2}}],[\"mozilla\",{\"1\":{\"248\":1}}],[\"mohamed\",{\"1\":{\"207\":1}}],[\"monocut\",{\"1\":{\"2139\":1,\"2150\":3}}],[\"mono\",{\"0\":{\"592\":1},\"1\":{\"592\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1}}],[\"monotonic\",{\"0\":{\"1630\":1},\"1\":{\"262\":1,\"1552\":1,\"1625\":1,\"1626\":2,\"2420\":1}}],[\"mondo\",{\"1\":{\"287\":1}}],[\"monde\",{\"1\":{\"287\":1}}],[\"monitor\",{\"1\":{\"286\":1,\"2355\":6}}],[\"monitored\",{\"1\":{\"39\":1,\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1521\":1,\"1526\":1,\"1553\":1,\"1585\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"2228\":1,\"2229\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2327\":1,\"2408\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2446\":1,\"2447\":1}}],[\"montreal\",{\"1\":{\"285\":1}}],[\"month\",{\"1\":{\"202\":1}}],[\"monaural\",{\"1\":{\"70\":1,\"76\":2,\"1009\":1,\"1061\":1,\"1062\":1,\"1185\":1,\"1264\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":3}}],[\"modalities\",{\"1\":{\"2130\":12}}],[\"modality\",{\"1\":{\"674\":4,\"675\":2,\"746\":1,\"2130\":6,\"2143\":2}}],[\"modrelu\",{\"0\":{\"1411\":1},\"1\":{\"1411\":1}}],[\"modulate\",{\"1\":{\"1784\":1}}],[\"modulated\",{\"1\":{\"1631\":1}}],[\"modulation\",{\"1\":{\"1319\":1}}],[\"moduledict\",{\"1\":{\"2216\":1}}],[\"modulenotfounderror\",{\"0\":{\"153\":1}}],[\"module\",{\"0\":{\"153\":1,\"665\":1,\"1059\":1,\"1398\":1,\"1404\":1,\"1406\":1,\"1433\":1,\"1435\":1,\"1485\":1,\"1486\":1,\"1487\":1,\"1675\":1,\"1684\":1,\"1685\":1,\"1695\":1},\"1\":{\"42\":1,\"43\":8,\"50\":2,\"140\":2,\"141\":27,\"142\":10,\"143\":2,\"147\":2,\"148\":1,\"163\":1,\"225\":1,\"266\":1,\"275\":1,\"284\":2,\"285\":1,\"290\":6,\"614\":4,\"615\":2,\"616\":4,\"617\":13,\"618\":20,\"619\":5,\"620\":4,\"621\":2,\"622\":7,\"623\":3,\"624\":22,\"625\":7,\"626\":4,\"627\":2,\"629\":3,\"630\":3,\"632\":2,\"633\":15,\"634\":8,\"635\":3,\"636\":9,\"637\":5,\"638\":9,\"639\":8,\"640\":2,\"641\":1,\"642\":6,\"643\":3,\"644\":1,\"645\":4,\"646\":3,\"647\":4,\"648\":2,\"649\":3,\"650\":3,\"651\":1,\"652\":3,\"653\":1,\"661\":4,\"662\":1,\"664\":1,\"665\":6,\"666\":5,\"669\":1,\"675\":3,\"676\":3,\"677\":1,\"678\":3,\"679\":1,\"680\":3,\"681\":1,\"682\":3,\"683\":1,\"684\":3,\"685\":1,\"686\":3,\"687\":1,\"689\":2,\"690\":1,\"692\":3,\"693\":3,\"694\":1,\"696\":4,\"697\":4,\"699\":2,\"700\":3,\"701\":6,\"702\":2,\"706\":4,\"709\":11,\"710\":5,\"711\":3,\"712\":4,\"713\":3,\"714\":1,\"715\":3,\"718\":2,\"719\":1,\"720\":2,\"721\":1,\"722\":1,\"723\":1,\"724\":2,\"725\":2,\"726\":2,\"727\":2,\"728\":2,\"729\":2,\"730\":2,\"731\":2,\"732\":2,\"733\":4,\"734\":4,\"735\":11,\"736\":3,\"737\":2,\"739\":1,\"740\":2,\"741\":3,\"742\":1,\"744\":2,\"745\":3,\"746\":3,\"747\":3,\"748\":3,\"749\":4,\"750\":1,\"751\":2,\"752\":2,\"753\":1,\"754\":3,\"757\":2,\"758\":1,\"759\":2,\"761\":1,\"762\":1,\"765\":1,\"766\":2,\"767\":2,\"768\":3,\"769\":1,\"770\":1,\"771\":3,\"772\":1,\"774\":11,\"775\":2,\"776\":1,\"777\":3,\"778\":2,\"779\":1,\"780\":9,\"781\":3,\"782\":1,\"783\":3,\"784\":2,\"786\":3,\"787\":3,\"788\":3,\"789\":1,\"790\":2,\"791\":2,\"792\":1,\"793\":4,\"794\":2,\"795\":1,\"796\":2,\"797\":1,\"798\":2,\"799\":1,\"800\":3,\"805\":3,\"806\":1,\"807\":3,\"808\":1,\"809\":3,\"810\":1,\"811\":3,\"812\":1,\"813\":3,\"814\":1,\"815\":2,\"816\":1,\"817\":2,\"820\":3,\"821\":1,\"822\":1,\"823\":2,\"824\":2,\"825\":3,\"826\":1,\"828\":2,\"829\":3,\"830\":4,\"831\":1,\"832\":3,\"833\":2,\"834\":1,\"835\":3,\"836\":1,\"837\":4,\"838\":1,\"839\":3,\"840\":1,\"841\":3,\"842\":3,\"843\":1,\"844\":3,\"845\":1,\"846\":3,\"847\":3,\"848\":2,\"849\":3,\"850\":2,\"851\":3,\"852\":5,\"853\":1,\"854\":2,\"855\":1,\"856\":5,\"857\":1,\"858\":2,\"859\":2,\"860\":3,\"861\":1,\"862\":2,\"863\":1,\"864\":2,\"865\":1,\"911\":1,\"912\":1,\"927\":4,\"947\":2,\"948\":2,\"949\":2,\"950\":3,\"951\":1,\"952\":3,\"953\":1,\"954\":2,\"955\":2,\"956\":3,\"957\":1,\"958\":2,\"959\":1,\"960\":1,\"961\":5,\"963\":3,\"964\":1,\"965\":3,\"966\":1,\"967\":3,\"968\":1,\"969\":3,\"970\":1,\"971\":3,\"972\":3,\"973\":3,\"974\":3,\"975\":3,\"976\":3,\"977\":2,\"978\":4,\"979\":2,\"980\":1,\"981\":3,\"982\":2,\"1029\":2,\"1030\":3,\"1031\":1,\"1032\":3,\"1033\":1,\"1034\":3,\"1035\":1,\"1036\":3,\"1037\":1,\"1038\":3,\"1039\":1,\"1040\":3,\"1041\":1,\"1042\":3,\"1043\":1,\"1044\":3,\"1045\":1,\"1046\":3,\"1047\":1,\"1048\":3,\"1049\":1,\"1051\":8,\"1052\":1,\"1054\":3,\"1055\":3,\"1056\":1,\"1057\":3,\"1058\":1,\"1059\":4,\"1060\":1,\"1061\":1,\"1063\":3,\"1064\":5,\"1065\":2,\"1066\":2,\"1067\":1,\"1068\":3,\"1069\":1,\"1070\":2,\"1071\":2,\"1072\":3,\"1073\":2,\"1074\":3,\"1075\":3,\"1076\":3,\"1077\":1,\"1078\":4,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1083\":1,\"1084\":3,\"1085\":1,\"1086\":3,\"1087\":3,\"1088\":1,\"1089\":3,\"1090\":1,\"1091\":3,\"1092\":1,\"1093\":3,\"1094\":1,\"1095\":3,\"1096\":1,\"1097\":3,\"1098\":1,\"1099\":3,\"1100\":1,\"1101\":3,\"1102\":1,\"1103\":3,\"1104\":1,\"1105\":3,\"1106\":1,\"1107\":3,\"1108\":3,\"1109\":1,\"1110\":3,\"1111\":1,\"1112\":2,\"1113\":2,\"1114\":3,\"1115\":1,\"1119\":3,\"1120\":3,\"1121\":1,\"1122\":3,\"1123\":1,\"1124\":1,\"1126\":3,\"1127\":3,\"1132\":2,\"1133\":7,\"1134\":3,\"1135\":1,\"1137\":3,\"1138\":1,\"1139\":3,\"1140\":1,\"1142\":3,\"1143\":1,\"1144\":3,\"1145\":3,\"1146\":1,\"1147\":1,\"1148\":3,\"1149\":3,\"1150\":1,\"1151\":3,\"1152\":1,\"1153\":4,\"1154\":1,\"1155\":2,\"1156\":2,\"1157\":3,\"1158\":2,\"1159\":3,\"1160\":1,\"1163\":2,\"1164\":3,\"1165\":3,\"1166\":1,\"1167\":2,\"1168\":3,\"1169\":1,\"1170\":2,\"1171\":2,\"1172\":2,\"1173\":2,\"1174\":2,\"1175\":2,\"1176\":1,\"1177\":3,\"1178\":1,\"1179\":3,\"1180\":1,\"1181\":1,\"1182\":3,\"1183\":3,\"1184\":3,\"1185\":4,\"1186\":1,\"1187\":3,\"1188\":1,\"1190\":3,\"1191\":1,\"1192\":3,\"1193\":1,\"1194\":3,\"1195\":1,\"1196\":3,\"1197\":1,\"1198\":3,\"1199\":3,\"1200\":3,\"1201\":1,\"1202\":4,\"1203\":1,\"1205\":3,\"1206\":1,\"1207\":3,\"1208\":3,\"1210\":2,\"1211\":3,\"1212\":1,\"1213\":3,\"1214\":1,\"1215\":3,\"1216\":1,\"1217\":2,\"1219\":3,\"1220\":1,\"1222\":2,\"1223\":2,\"1226\":4,\"1227\":1,\"1230\":3,\"1231\":1,\"1233\":3,\"1234\":1,\"1235\":2,\"1236\":3,\"1237\":1,\"1238\":3,\"1239\":1,\"1240\":3,\"1241\":1,\"1242\":3,\"1243\":1,\"1246\":2,\"1247\":2,\"1248\":2,\"1249\":1,\"1250\":2,\"1251\":2,\"1252\":3,\"1253\":2,\"1254\":1,\"1255\":3,\"1256\":1,\"1257\":4,\"1258\":1,\"1259\":3,\"1260\":1,\"1261\":2,\"1262\":6,\"1263\":1,\"1264\":3,\"1265\":3,\"1266\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1272\":3,\"1273\":2,\"1274\":1,\"1275\":2,\"1276\":2,\"1277\":2,\"1279\":3,\"1281\":3,\"1282\":3,\"1284\":3,\"1285\":1,\"1286\":3,\"1287\":1,\"1288\":3,\"1289\":1,\"1290\":4,\"1331\":1,\"1333\":2,\"1334\":2,\"1381\":3,\"1383\":4,\"1384\":1,\"1385\":1,\"1386\":3,\"1387\":2,\"1388\":1,\"1390\":3,\"1391\":2,\"1392\":3,\"1393\":1,\"1395\":1,\"1397\":3,\"1398\":4,\"1399\":1,\"1400\":3,\"1402\":7,\"1403\":2,\"1404\":4,\"1405\":1,\"1406\":4,\"1407\":1,\"1409\":8,\"1410\":2,\"1411\":4,\"1412\":1,\"1413\":1,\"1414\":1,\"1415\":1,\"1416\":1,\"1417\":3,\"1418\":1,\"1419\":3,\"1420\":1,\"1421\":1,\"1423\":1,\"1424\":3,\"1425\":1,\"1426\":3,\"1427\":1,\"1428\":3,\"1429\":1,\"1430\":3,\"1431\":1,\"1433\":4,\"1434\":1,\"1435\":4,\"1436\":1,\"1437\":3,\"1438\":1,\"1439\":3,\"1440\":1,\"1441\":3,\"1442\":3,\"1443\":1,\"1444\":3,\"1445\":1,\"1446\":3,\"1447\":1,\"1448\":3,\"1449\":1,\"1450\":3,\"1451\":1,\"1452\":3,\"1453\":1,\"1454\":3,\"1455\":1,\"1456\":3,\"1457\":1,\"1458\":3,\"1459\":1,\"1460\":3,\"1461\":1,\"1462\":3,\"1463\":1,\"1464\":3,\"1465\":1,\"1467\":7,\"1468\":2,\"1469\":3,\"1470\":1,\"1476\":3,\"1485\":6,\"1486\":1,\"1487\":1,\"1508\":2,\"1509\":4,\"1510\":1,\"1511\":3,\"1512\":1,\"1513\":6,\"1514\":1,\"1515\":4,\"1516\":4,\"1517\":3,\"1518\":1,\"1519\":3,\"1520\":3,\"1521\":1,\"1522\":2,\"1523\":1,\"1524\":2,\"1525\":2,\"1526\":2,\"1527\":3,\"1528\":1,\"1529\":3,\"1530\":3,\"1531\":1,\"1533\":3,\"1534\":4,\"1535\":3,\"1536\":3,\"1537\":3,\"1538\":1,\"1539\":2,\"1540\":1,\"1541\":3,\"1542\":1,\"1543\":3,\"1544\":1,\"1545\":3,\"1546\":5,\"1547\":3,\"1548\":6,\"1549\":2,\"1550\":1,\"1551\":5,\"1552\":3,\"1553\":3,\"1554\":3,\"1555\":1,\"1556\":3,\"1576\":2,\"1577\":1,\"1578\":2,\"1579\":1,\"1580\":2,\"1581\":3,\"1582\":3,\"1583\":3,\"1584\":3,\"1585\":1,\"1586\":3,\"1587\":3,\"1588\":4,\"1589\":2,\"1590\":3,\"1591\":3,\"1592\":6,\"1593\":4,\"1594\":6,\"1595\":7,\"1596\":6,\"1597\":8,\"1598\":3,\"1599\":8,\"1600\":2,\"1601\":3,\"1602\":3,\"1603\":4,\"1604\":5,\"1605\":8,\"1606\":9,\"1607\":1,\"1608\":4,\"1609\":5,\"1610\":5,\"1611\":4,\"1612\":4,\"1613\":2,\"1614\":4,\"1615\":5,\"1616\":4,\"1617\":3,\"1618\":5,\"1619\":6,\"1620\":3,\"1621\":3,\"1622\":5,\"1623\":3,\"1624\":3,\"1625\":3,\"1626\":4,\"1627\":2,\"1628\":4,\"1638\":4,\"1639\":1,\"1640\":2,\"1641\":2,\"1652\":3,\"1653\":1,\"1656\":3,\"1657\":3,\"1658\":1,\"1660\":4,\"1661\":1,\"1662\":4,\"1663\":1,\"1664\":4,\"1665\":4,\"1667\":4,\"1668\":2,\"1669\":4,\"1670\":4,\"1671\":3,\"1675\":1,\"1681\":2,\"1682\":1,\"1683\":3,\"1684\":5,\"1685\":5,\"1688\":1,\"1695\":9,\"1702\":2,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1725\":1,\"1726\":2,\"1727\":2,\"1731\":2,\"1733\":1,\"1735\":6,\"1736\":4,\"1737\":2,\"1738\":2,\"1739\":2,\"1740\":2,\"1741\":2,\"1742\":3,\"1743\":2,\"1744\":2,\"1745\":2,\"1746\":1,\"1747\":2,\"1748\":1,\"1749\":2,\"1750\":6,\"1751\":8,\"1752\":3,\"1753\":1,\"1754\":3,\"1756\":1,\"1757\":1,\"1758\":4,\"1759\":6,\"1764\":3,\"1766\":1,\"1768\":2,\"1770\":4,\"1771\":2,\"1779\":2,\"1782\":2,\"1783\":1,\"1784\":1,\"1786\":1,\"1788\":4,\"1789\":1,\"1790\":1,\"1794\":1,\"1795\":3,\"1801\":1,\"1803\":1,\"1808\":1,\"1809\":1,\"1810\":4,\"1811\":3,\"1812\":2,\"1814\":2,\"1815\":2,\"1816\":2,\"1818\":2,\"1820\":1,\"1837\":1,\"1838\":3,\"1839\":3,\"1847\":4,\"1849\":1,\"1851\":3,\"1854\":1,\"1855\":3,\"1856\":4,\"1860\":3,\"1861\":1,\"1863\":1,\"1864\":2,\"1866\":2,\"1872\":1,\"1876\":2,\"1878\":3,\"1885\":1,\"1891\":1,\"1894\":2,\"1895\":2,\"1896\":2,\"1917\":2,\"1931\":4,\"1938\":3,\"1939\":1,\"1940\":2,\"1941\":1,\"1942\":2,\"1943\":1,\"1944\":2,\"1945\":2,\"1946\":1,\"1947\":2,\"1958\":1,\"1959\":2,\"1965\":2,\"1967\":3,\"1968\":1,\"1969\":3,\"1970\":1,\"1971\":3,\"1972\":2,\"1973\":1,\"1974\":4,\"1975\":2,\"1976\":1,\"1977\":3,\"1978\":2,\"1979\":1,\"1980\":2,\"1981\":1,\"1982\":2,\"1983\":1,\"1984\":2,\"1985\":3,\"1986\":1,\"1987\":3,\"1988\":2,\"1989\":1,\"1990\":2,\"1991\":2,\"1992\":2,\"1993\":2,\"1994\":4,\"1995\":2,\"1996\":2,\"1997\":2,\"2026\":3,\"2027\":1,\"2028\":3,\"2029\":1,\"2030\":3,\"2031\":1,\"2032\":3,\"2033\":1,\"2034\":3,\"2035\":1,\"2124\":3,\"2125\":1,\"2126\":4,\"2127\":3,\"2128\":1,\"2129\":3,\"2130\":2,\"2131\":1,\"2138\":1,\"2142\":1,\"2154\":1,\"2167\":2,\"2168\":3,\"2169\":1,\"2170\":3,\"2171\":1,\"2172\":3,\"2173\":1,\"2174\":3,\"2175\":1,\"2176\":2,\"2177\":3,\"2178\":1,\"2179\":3,\"2180\":1,\"2181\":3,\"2182\":1,\"2183\":2,\"2184\":2,\"2185\":3,\"2186\":1,\"2187\":2,\"2188\":2,\"2189\":1,\"2190\":2,\"2191\":10,\"2192\":2,\"2193\":1,\"2194\":2,\"2195\":1,\"2196\":3,\"2197\":1,\"2198\":4,\"2199\":1,\"2200\":3,\"2201\":1,\"2202\":3,\"2203\":2,\"2204\":1,\"2205\":2,\"2206\":1,\"2207\":2,\"2208\":2,\"2209\":2,\"2210\":1,\"2211\":2,\"2212\":1,\"2213\":3,\"2214\":3,\"2215\":3,\"2216\":2,\"2217\":1,\"2218\":1,\"2220\":2,\"2221\":3,\"2222\":3,\"2223\":6,\"2226\":3,\"2227\":4,\"2228\":1,\"2229\":1,\"2231\":4,\"2232\":3,\"2235\":2,\"2236\":2,\"2237\":3,\"2238\":3,\"2239\":3,\"2240\":5,\"2241\":3,\"2245\":3,\"2249\":1,\"2276\":1,\"2277\":1,\"2280\":1,\"2286\":2,\"2304\":5,\"2305\":3,\"2306\":1,\"2307\":1,\"2311\":2,\"2312\":3,\"2314\":1,\"2316\":1,\"2325\":4,\"2326\":1,\"2327\":4,\"2333\":1,\"2347\":2,\"2354\":1,\"2365\":2,\"2369\":4,\"2371\":2,\"2401\":3,\"2402\":1,\"2403\":3,\"2405\":3,\"2406\":1,\"2407\":4,\"2408\":1,\"2409\":2,\"2410\":1,\"2411\":6,\"2412\":6,\"2413\":3,\"2414\":2,\"2415\":1,\"2416\":2,\"2417\":1,\"2418\":2,\"2419\":1,\"2420\":3,\"2421\":2,\"2422\":2,\"2423\":6,\"2424\":3,\"2425\":4,\"2426\":1,\"2427\":1,\"2428\":1,\"2429\":3,\"2430\":4,\"2431\":3,\"2432\":7,\"2433\":4,\"2434\":2,\"2435\":1,\"2443\":3,\"2444\":1,\"2445\":3,\"2446\":1,\"2447\":6,\"2448\":3,\"2449\":2,\"2450\":1,\"2451\":3,\"2452\":1,\"2453\":3,\"2454\":1,\"2455\":3,\"2456\":3,\"2457\":1,\"2458\":2,\"2459\":1,\"2460\":2,\"2461\":1,\"2462\":2,\"2463\":2,\"2464\":2,\"2465\":3,\"2466\":1,\"2467\":3,\"2468\":1,\"2469\":2,\"2470\":2,\"2471\":2,\"2472\":2,\"2473\":2,\"2482\":2}}],[\"modules\",{\"0\":{\"619\":1,\"622\":1,\"623\":1,\"630\":1,\"636\":1,\"637\":1,\"638\":1,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"649\":1,\"654\":1,\"668\":1,\"1400\":1,\"1439\":1,\"1469\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1501\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1537\":1,\"1554\":1,\"1574\":1},\"1\":{\"37\":1,\"50\":3,\"51\":2,\"52\":2,\"78\":1,\"107\":2,\"138\":3,\"140\":1,\"142\":1,\"150\":1,\"156\":1,\"301\":2,\"321\":2,\"389\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"498\":2,\"617\":5,\"618\":7,\"619\":1,\"622\":3,\"623\":1,\"624\":8,\"630\":1,\"633\":4,\"634\":1,\"636\":4,\"637\":1,\"638\":5,\"642\":2,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"649\":1,\"654\":1,\"668\":1,\"699\":1,\"733\":1,\"927\":2,\"950\":1,\"978\":1,\"1036\":1,\"1042\":1,\"1051\":1,\"1064\":2,\"1078\":1,\"1108\":1,\"1145\":1,\"1153\":1,\"1168\":1,\"1174\":1,\"1202\":1,\"1205\":1,\"1236\":1,\"1252\":1,\"1262\":3,\"1264\":1,\"1265\":1,\"1276\":1,\"1280\":1,\"1283\":1,\"1290\":1,\"1324\":1,\"1385\":1,\"1400\":1,\"1439\":1,\"1469\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1501\":1,\"1502\":1,\"1504\":1,\"1507\":1,\"1537\":1,\"1554\":1,\"1574\":1,\"1605\":1,\"1606\":1,\"1618\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1675\":2,\"1683\":3,\"1702\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"1860\":1,\"1862\":1,\"1872\":1,\"1967\":1,\"1969\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"2044\":1,\"2198\":1,\"2216\":1,\"2232\":1,\"2238\":1,\"2286\":1,\"2432\":2,\"2456\":1}}],[\"mode=\",{\"1\":{\"821\":1,\"1029\":1,\"1235\":1,\"1279\":1,\"1281\":1,\"1282\":1,\"1318\":1,\"1328\":1,\"1824\":2,\"1835\":1,\"1853\":1,\"1854\":1,\"1900\":1,\"1924\":1,\"1948\":2,\"2020\":2,\"2249\":1}}],[\"mode=false\",{\"1\":{\"710\":1,\"711\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1735\":1,\"1768\":1,\"1860\":1,\"1895\":1}}],[\"modern\",{\"1\":{\"162\":1}}],[\"mode\",{\"0\":{\"56\":1,\"58\":1,\"383\":1,\"455\":1,\"483\":1,\"2395\":1},\"1\":{\"56\":3,\"58\":1,\"84\":2,\"96\":3,\"97\":2,\"98\":3,\"99\":2,\"100\":1,\"104\":1,\"126\":1,\"137\":1,\"162\":1,\"175\":5,\"223\":1,\"243\":2,\"356\":1,\"527\":1,\"702\":2,\"710\":1,\"711\":1,\"712\":1,\"756\":1,\"773\":1,\"774\":2,\"796\":1,\"819\":1,\"821\":4,\"833\":2,\"837\":1,\"846\":3,\"866\":1,\"867\":1,\"869\":1,\"939\":2,\"1029\":2,\"1118\":2,\"1125\":2,\"1133\":2,\"1202\":1,\"1235\":2,\"1259\":1,\"1261\":1,\"1279\":2,\"1280\":11,\"1281\":2,\"1282\":2,\"1283\":4,\"1318\":1,\"1328\":1,\"1373\":1,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1442\":1,\"1444\":1,\"1450\":3,\"1452\":3,\"1454\":3,\"1456\":3,\"1458\":3,\"1460\":3,\"1466\":1,\"1468\":1,\"1493\":1,\"1494\":1,\"1582\":3,\"1617\":3,\"1619\":3,\"1620\":3,\"1621\":3,\"1624\":3,\"1670\":3,\"1699\":3,\"1702\":2,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1768\":2,\"1824\":1,\"1854\":1,\"1860\":2,\"1892\":3,\"1895\":2,\"1951\":1,\"1965\":1,\"2006\":1,\"2007\":4,\"2176\":1,\"2239\":1,\"2247\":9,\"2249\":11,\"2253\":1,\"2353\":1,\"2355\":4,\"2359\":5,\"2364\":1,\"2384\":1,\"2385\":1,\"2395\":1}}],[\"model2\",{\"1\":{\"2369\":1}}],[\"model1\",{\"1\":{\"2369\":1}}],[\"modeldownloader\",{\"1\":{\"286\":2}}],[\"modelhere\",{\"1\":{\"267\":1}}],[\"modeling=true\",{\"1\":{\"1029\":1,\"1235\":1}}],[\"modeling\",{\"0\":{\"237\":1},\"1\":{\"287\":1,\"1029\":2,\"1061\":1,\"1062\":2,\"1185\":1,\"1235\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1279\":2,\"1280\":4,\"1281\":2,\"1282\":2,\"1283\":3,\"1708\":1,\"1709\":1,\"2142\":1,\"2240\":1,\"2298\":1,\"2425\":1,\"2429\":1,\"2430\":1}}],[\"model\",{\"0\":{\"88\":1,\"125\":1,\"127\":1,\"225\":1,\"251\":1,\"281\":1,\"297\":1,\"304\":1,\"312\":1,\"318\":1,\"324\":1,\"330\":1,\"334\":1,\"338\":1,\"346\":1,\"353\":1,\"365\":1,\"371\":1,\"380\":1,\"388\":1,\"392\":1,\"399\":1,\"409\":1,\"417\":1,\"424\":1,\"432\":1,\"439\":1,\"445\":1,\"452\":1,\"460\":1,\"466\":1,\"472\":1,\"478\":1,\"487\":1,\"493\":1,\"501\":1,\"508\":1,\"526\":1,\"625\":1,\"736\":1,\"737\":1,\"776\":1,\"777\":1,\"794\":1,\"795\":1,\"828\":1,\"904\":1,\"905\":1,\"954\":1,\"958\":1,\"962\":1,\"974\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1360\":1,\"1395\":1,\"1521\":1,\"1585\":1,\"1640\":1,\"1641\":1,\"1702\":1,\"1913\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1959\":1,\"1965\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2036\":1,\"2038\":1,\"2044\":1,\"2046\":1,\"2047\":1,\"2048\":1,\"2051\":1,\"2052\":1,\"2053\":1,\"2058\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2075\":1,\"2105\":1,\"2123\":1,\"2127\":1,\"2130\":1,\"2131\":1,\"2133\":1,\"2136\":1,\"2137\":1,\"2138\":1,\"2140\":1,\"2142\":1,\"2143\":1,\"2148\":1,\"2151\":1,\"2154\":2,\"2163\":1,\"2184\":1,\"2216\":1,\"2221\":1,\"2228\":1,\"2229\":1,\"2308\":1,\"2310\":1,\"2314\":2,\"2316\":2,\"2322\":1,\"2325\":1,\"2327\":1,\"2355\":1,\"2408\":1,\"2446\":1,\"2462\":1},\"1\":{\"25\":1,\"42\":2,\"43\":2,\"46\":3,\"50\":8,\"51\":1,\"52\":4,\"68\":1,\"70\":1,\"78\":2,\"79\":4,\"82\":1,\"87\":1,\"88\":7,\"91\":1,\"102\":1,\"107\":2,\"118\":1,\"124\":1,\"125\":3,\"126\":2,\"127\":2,\"128\":2,\"132\":1,\"136\":4,\"139\":7,\"144\":2,\"147\":2,\"175\":8,\"180\":1,\"184\":2,\"185\":3,\"186\":1,\"187\":1,\"188\":1,\"190\":1,\"193\":1,\"200\":9,\"205\":10,\"210\":1,\"211\":6,\"217\":8,\"218\":6,\"220\":2,\"222\":9,\"223\":16,\"225\":11,\"228\":9,\"233\":1,\"235\":5,\"236\":1,\"240\":3,\"242\":8,\"243\":20,\"247\":6,\"254\":2,\"261\":4,\"262\":11,\"263\":3,\"265\":3,\"266\":4,\"267\":37,\"274\":5,\"275\":7,\"276\":32,\"281\":2,\"284\":13,\"285\":13,\"286\":55,\"289\":3,\"290\":41,\"295\":2,\"301\":6,\"309\":4,\"315\":2,\"321\":6,\"327\":2,\"331\":2,\"335\":4,\"342\":4,\"349\":4,\"356\":4,\"361\":4,\"368\":4,\"372\":2,\"377\":4,\"385\":2,\"389\":2,\"396\":4,\"403\":28,\"404\":2,\"406\":2,\"415\":2,\"421\":6,\"429\":6,\"436\":6,\"442\":6,\"449\":6,\"457\":4,\"463\":4,\"469\":2,\"475\":2,\"484\":4,\"490\":4,\"496\":2,\"498\":6,\"505\":4,\"511\":2,\"513\":5,\"514\":5,\"523\":1,\"526\":3,\"527\":12,\"536\":8,\"575\":2,\"625\":1,\"675\":2,\"699\":1,\"701\":1,\"724\":5,\"725\":5,\"726\":2,\"727\":1,\"728\":5,\"729\":4,\"735\":1,\"736\":2,\"737\":2,\"738\":1,\"744\":2,\"745\":2,\"746\":2,\"747\":3,\"759\":1,\"760\":9,\"761\":1,\"762\":1,\"768\":1,\"776\":2,\"777\":2,\"784\":3,\"785\":1,\"787\":1,\"790\":2,\"791\":2,\"794\":1,\"795\":1,\"809\":1,\"817\":2,\"819\":1,\"821\":3,\"824\":2,\"827\":1,\"828\":6,\"829\":15,\"830\":5,\"846\":3,\"859\":5,\"864\":2,\"882\":2,\"883\":2,\"884\":2,\"889\":1,\"890\":1,\"891\":1,\"904\":2,\"905\":2,\"911\":1,\"912\":1,\"922\":2,\"927\":1,\"930\":1,\"954\":3,\"958\":4,\"960\":2,\"961\":1,\"962\":1,\"974\":2,\"1053\":8,\"1061\":1,\"1062\":3,\"1107\":2,\"1117\":1,\"1118\":2,\"1128\":2,\"1131\":2,\"1136\":2,\"1141\":2,\"1145\":2,\"1155\":9,\"1156\":4,\"1157\":10,\"1158\":2,\"1162\":3,\"1211\":1,\"1217\":2,\"1232\":2,\"1245\":2,\"1252\":3,\"1261\":2,\"1264\":1,\"1265\":1,\"1267\":2,\"1268\":1,\"1269\":4,\"1270\":4,\"1271\":5,\"1278\":2,\"1279\":1,\"1280\":2,\"1283\":2,\"1334\":5,\"1360\":2,\"1381\":1,\"1389\":2,\"1395\":2,\"1396\":3,\"1401\":2,\"1408\":2,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":2,\"1478\":1,\"1502\":1,\"1508\":1,\"1521\":2,\"1526\":7,\"1539\":2,\"1552\":5,\"1553\":2,\"1576\":1,\"1585\":2,\"1598\":1,\"1600\":7,\"1625\":1,\"1638\":1,\"1640\":2,\"1641\":2,\"1681\":4,\"1682\":1,\"1683\":4,\"1688\":1,\"1702\":3,\"1747\":1,\"1749\":1,\"1756\":4,\"1757\":4,\"1784\":2,\"1785\":1,\"1786\":2,\"1789\":4,\"1790\":4,\"1794\":2,\"1798\":4,\"1799\":4,\"1800\":4,\"1808\":2,\"1815\":1,\"1817\":1,\"1818\":2,\"1820\":2,\"1837\":2,\"1847\":1,\"1863\":1,\"1872\":7,\"1894\":2,\"1913\":2,\"1917\":1,\"1938\":4,\"1940\":2,\"1942\":2,\"1945\":2,\"1949\":7,\"1950\":2,\"1951\":1,\"1956\":3,\"1957\":3,\"1959\":2,\"1963\":2,\"1964\":1,\"1965\":3,\"1966\":1,\"1975\":2,\"1984\":1,\"1993\":1,\"1996\":2,\"1997\":2,\"2007\":1,\"2016\":4,\"2019\":1,\"2040\":2,\"2043\":3,\"2044\":18,\"2045\":8,\"2049\":7,\"2054\":1,\"2055\":3,\"2056\":3,\"2065\":1,\"2066\":3,\"2127\":2,\"2128\":1,\"2130\":3,\"2131\":10,\"2132\":1,\"2133\":9,\"2136\":10,\"2137\":3,\"2138\":4,\"2140\":6,\"2141\":1,\"2142\":5,\"2143\":3,\"2148\":5,\"2151\":1,\"2154\":3,\"2163\":1,\"2184\":5,\"2187\":2,\"2192\":2,\"2203\":2,\"2215\":1,\"2216\":2,\"2221\":2,\"2228\":2,\"2229\":2,\"2235\":2,\"2236\":2,\"2239\":2,\"2240\":1,\"2245\":1,\"2246\":7,\"2247\":3,\"2248\":7,\"2249\":14,\"2250\":7,\"2251\":7,\"2252\":7,\"2253\":8,\"2254\":8,\"2255\":8,\"2256\":8,\"2257\":7,\"2259\":7,\"2260\":7,\"2261\":7,\"2262\":3,\"2263\":8,\"2264\":7,\"2265\":7,\"2266\":7,\"2267\":7,\"2268\":8,\"2269\":7,\"2270\":8,\"2271\":8,\"2272\":7,\"2273\":8,\"2278\":1,\"2279\":1,\"2280\":1,\"2283\":1,\"2284\":1,\"2287\":1,\"2288\":1,\"2305\":6,\"2307\":3,\"2308\":1,\"2310\":1,\"2312\":2,\"2314\":5,\"2315\":14,\"2316\":3,\"2322\":1,\"2325\":3,\"2327\":2,\"2338\":4,\"2339\":2,\"2347\":2,\"2348\":2,\"2354\":1,\"2355\":4,\"2365\":2,\"2369\":8,\"2370\":4,\"2371\":2,\"2372\":2,\"2377\":1,\"2408\":2,\"2411\":1,\"2412\":1,\"2422\":1,\"2423\":2,\"2427\":1,\"2431\":1,\"2432\":1,\"2446\":2,\"2447\":1,\"2462\":2}}],[\"models\",{\"0\":{\"16\":1,\"214\":1,\"219\":1,\"262\":1,\"272\":1,\"282\":1,\"289\":1,\"1053\":1,\"1949\":2},\"1\":{\"6\":2,\"8\":1,\"42\":2,\"43\":1,\"52\":2,\"125\":2,\"126\":1,\"127\":1,\"128\":1,\"138\":5,\"146\":1,\"156\":1,\"178\":3,\"179\":1,\"181\":1,\"182\":1,\"190\":2,\"191\":1,\"193\":2,\"200\":2,\"205\":2,\"208\":1,\"210\":1,\"211\":2,\"216\":1,\"217\":1,\"219\":2,\"220\":4,\"223\":5,\"225\":3,\"227\":1,\"228\":1,\"242\":2,\"243\":1,\"247\":1,\"260\":1,\"261\":4,\"262\":6,\"265\":1,\"266\":1,\"272\":2,\"273\":2,\"274\":1,\"275\":1,\"276\":1,\"282\":2,\"284\":2,\"285\":1,\"286\":7,\"289\":3,\"290\":4,\"527\":4,\"536\":8,\"541\":1,\"627\":1,\"702\":1,\"738\":1,\"740\":1,\"747\":1,\"759\":1,\"817\":1,\"828\":1,\"829\":1,\"846\":2,\"912\":1,\"1053\":6,\"1153\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1502\":1,\"1513\":1,\"1548\":1,\"1551\":1,\"1577\":1,\"1592\":1,\"1710\":1,\"1725\":1,\"1748\":1,\"1760\":1,\"1805\":1,\"1822\":2,\"1843\":1,\"1938\":1,\"1949\":3,\"1957\":1,\"2044\":3,\"2049\":1,\"2130\":1,\"2131\":1,\"2136\":3,\"2198\":1,\"2216\":1,\"2325\":1,\"2327\":1,\"2339\":2,\"2344\":1,\"2348\":1,\"2357\":2,\"2370\":2,\"2372\":1,\"2435\":1}}],[\"mods\",{\"1\":{\"50\":4,\"51\":3}}],[\"modifying\",{\"1\":{\"197\":1,\"260\":1,\"263\":1,\"290\":1,\"1361\":1,\"1698\":1}}],[\"modify\",{\"1\":{\"47\":1,\"82\":1,\"110\":1,\"168\":1,\"206\":2,\"212\":2,\"218\":2,\"243\":1,\"255\":2,\"267\":7,\"276\":5,\"286\":2,\"536\":2}}],[\"modifies\",{\"1\":{\"1271\":1}}],[\"modified\",{\"1\":{\"45\":3,\"47\":3,\"82\":3,\"139\":1,\"141\":1,\"143\":2,\"145\":2,\"243\":1,\"616\":3,\"696\":5,\"697\":5,\"1057\":1,\"1644\":1,\"1855\":1,\"2427\":1}}],[\"modifications\",{\"1\":{\"26\":1,\"128\":1,\"200\":1,\"223\":1,\"269\":1,\"278\":1,\"696\":1,\"697\":1}}],[\"mod\",{\"1\":{\"43\":3,\"141\":14,\"617\":2,\"618\":2,\"624\":4,\"661\":4,\"1863\":2,\"1864\":2}}],[\"mounted\",{\"1\":{\"18\":1}}],[\"moto\",{\"1\":{\"10\":1,\"11\":1}}],[\"more\",{\"1\":{\"3\":1,\"41\":1,\"55\":1,\"66\":1,\"70\":1,\"78\":1,\"94\":1,\"101\":1,\"119\":1,\"123\":1,\"129\":1,\"132\":1,\"139\":1,\"141\":1,\"150\":1,\"165\":1,\"173\":1,\"175\":2,\"195\":1,\"205\":1,\"208\":1,\"211\":1,\"223\":4,\"232\":1,\"240\":1,\"242\":1,\"243\":2,\"258\":1,\"260\":1,\"262\":7,\"263\":1,\"269\":1,\"275\":1,\"278\":1,\"285\":2,\"286\":2,\"290\":2,\"514\":1,\"705\":1,\"709\":1,\"756\":2,\"773\":2,\"774\":2,\"780\":1,\"787\":1,\"804\":2,\"846\":2,\"852\":1,\"856\":1,\"927\":1,\"1026\":1,\"1053\":1,\"1247\":1,\"1301\":1,\"1310\":1,\"1327\":2,\"1330\":2,\"1372\":1,\"1678\":3,\"1720\":1,\"1721\":1,\"1860\":1,\"2000\":1,\"2001\":1,\"2151\":1,\"2191\":1,\"2220\":2,\"2307\":1,\"2309\":1,\"2310\":1,\"2355\":2}}],[\"mediator\",{\"1\":{\"1938\":1,\"2325\":1}}],[\"median\",{\"1\":{\"1326\":1}}],[\"medium\",{\"1\":{\"126\":1}}],[\"mertfrontend\",{\"0\":{\"2234\":1}}],[\"mert\",{\"1\":{\"267\":1}}],[\"merges\",{\"1\":{\"1031\":1,\"1112\":1,\"1250\":1}}],[\"merge\",{\"0\":{\"1343\":1},\"1\":{\"243\":1,\"700\":1,\"701\":4,\"733\":1,\"734\":1,\"735\":3,\"780\":2,\"781\":1,\"783\":1,\"1031\":3,\"1112\":3,\"1141\":1,\"1250\":3,\"1343\":1,\"1719\":2,\"1725\":4,\"1806\":1}}],[\"merged\",{\"1\":{\"34\":1,\"200\":1,\"260\":1}}],[\"mel2wav\",{\"1\":{\"1605\":1,\"1606\":1}}],[\"melscale\",{\"0\":{\"1533\":1,\"1666\":1},\"1\":{\"1533\":2,\"1666\":1}}],[\"mels=40\",{\"1\":{\"1320\":1}}],[\"mels=120\",{\"1\":{\"1128\":1}}],[\"melspectrogramloss\",{\"0\":{\"1607\":1},\"1\":{\"1607\":1}}],[\"melspectrogramtorch\",{\"0\":{\"778\":1},\"1\":{\"778\":1}}],[\"melspec\",{\"0\":{\"778\":1},\"1\":{\"778\":1,\"1128\":1}}],[\"mels\",{\"1\":{\"516\":2,\"548\":2,\"558\":2,\"692\":1,\"720\":1,\"760\":1,\"775\":1,\"778\":1,\"790\":1,\"820\":1,\"850\":1,\"1320\":1,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1419\":2,\"1466\":1,\"1526\":1,\"1533\":4,\"1553\":1,\"1558\":3,\"1598\":1,\"1600\":1,\"1607\":2,\"1625\":1,\"1662\":2,\"1791\":1,\"1836\":1,\"1846\":1,\"1900\":1,\"1925\":1,\"1980\":1,\"2416\":1,\"2422\":1,\"2427\":2,\"2428\":1,\"2482\":2,\"2495\":3}}],[\"melganmultiscalediscriminator\",{\"0\":{\"1606\":1},\"1\":{\"1606\":2}}],[\"melgangenerator\",{\"0\":{\"1605\":1},\"1\":{\"1605\":2}}],[\"melgandiscriminator\",{\"0\":{\"1604\":1},\"1\":{\"1604\":2}}],[\"melgan\",{\"0\":{\"1604\":2,\"1605\":2,\"1606\":2,\"1608\":1,\"1615\":1,\"1618\":2,\"1619\":2,\"1620\":1,\"1621\":1,\"1631\":1},\"1\":{\"286\":4,\"290\":5,\"1604\":3,\"1605\":4,\"1606\":4,\"1608\":1,\"1615\":2,\"1618\":3,\"1619\":3,\"1620\":1,\"1621\":1,\"1631\":1}}],[\"melody\",{\"1\":{\"269\":1,\"278\":1,\"1526\":6,\"1552\":4,\"1553\":6,\"2235\":10,\"2236\":10,\"2239\":9,\"2240\":9,\"2245\":9}}],[\"mel\",{\"0\":{\"1320\":1,\"1662\":1,\"1980\":1,\"2416\":1},\"1\":{\"267\":2,\"273\":1,\"276\":2,\"285\":1,\"286\":3,\"516\":1,\"536\":2,\"720\":1,\"768\":1,\"778\":2,\"792\":2,\"865\":1,\"1320\":4,\"1389\":3,\"1396\":3,\"1401\":3,\"1408\":3,\"1419\":5,\"1466\":3,\"1511\":1,\"1519\":1,\"1526\":10,\"1529\":2,\"1533\":3,\"1534\":3,\"1547\":1,\"1549\":1,\"1553\":6,\"1558\":3,\"1598\":6,\"1600\":9,\"1607\":7,\"1625\":6,\"1662\":3,\"1666\":4,\"1668\":1,\"1750\":1,\"1758\":1,\"1810\":2,\"1811\":1,\"1812\":1,\"1980\":2,\"1993\":1,\"2130\":1,\"2133\":1,\"2187\":1,\"2198\":1,\"2239\":2,\"2240\":4,\"2241\":2,\"2416\":2,\"2425\":1,\"2428\":1,\"2429\":1,\"2431\":2,\"2432\":1,\"2482\":1,\"2495\":3}}],[\"metavar=none\",{\"1\":{\"2478\":1}}],[\"meta\",{\"1\":{\"1381\":2,\"1389\":2,\"1395\":2,\"1401\":2,\"1408\":2,\"1466\":2,\"1953\":1,\"2049\":2}}],[\"metallic\",{\"1\":{\"284\":1,\"290\":2}}],[\"metadata\",{\"0\":{\"717\":1},\"1\":{\"235\":2,\"717\":2,\"2136\":1,\"2160\":1}}],[\"metrices\",{\"1\":{\"1246\":1}}],[\"metric\",{\"1\":{\"227\":1,\"247\":1,\"253\":1,\"254\":1,\"541\":1,\"1246\":1,\"2355\":4}}],[\"metrics=none\",{\"1\":{\"2020\":1}}],[\"metrics\",{\"0\":{\"1477\":1},\"1\":{\"211\":1,\"212\":1,\"235\":1,\"245\":1,\"246\":3,\"247\":1,\"267\":3,\"276\":3,\"285\":2,\"286\":5,\"958\":2,\"1246\":1,\"1382\":1,\"1477\":3,\"2355\":2}}],[\"method=2\",{\"1\":{\"1781\":1}}],[\"method=\",{\"1\":{\"1076\":1,\"1522\":1,\"1559\":1}}],[\"methods\",{\"1\":{\"60\":1,\"81\":1,\"150\":1,\"217\":1,\"232\":1,\"258\":1,\"285\":1,\"829\":2,\"1224\":1,\"1225\":1,\"2130\":3,\"2131\":3,\"2369\":1}}],[\"method\",{\"0\":{\"60\":1},\"1\":{\"45\":1,\"60\":3,\"62\":1,\"63\":1,\"64\":1,\"82\":1,\"126\":1,\"145\":1,\"150\":2,\"262\":1,\"267\":3,\"269\":1,\"278\":1,\"286\":3,\"377\":2,\"449\":2,\"548\":2,\"551\":2,\"561\":2,\"616\":2,\"696\":1,\"697\":1,\"700\":1,\"701\":4,\"715\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"737\":1,\"744\":1,\"749\":1,\"752\":1,\"768\":1,\"770\":1,\"783\":1,\"784\":1,\"785\":1,\"786\":1,\"828\":2,\"829\":2,\"830\":2,\"833\":1,\"842\":1,\"844\":1,\"866\":1,\"921\":1,\"922\":1,\"929\":1,\"931\":1,\"933\":1,\"936\":1,\"937\":1,\"960\":2,\"1064\":1,\"1078\":1,\"1153\":1,\"1156\":2,\"1202\":1,\"1246\":1,\"1262\":1,\"1267\":1,\"1290\":1,\"1311\":1,\"1318\":2,\"1322\":1,\"1327\":1,\"1328\":3,\"1330\":1,\"1392\":1,\"1441\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1552\":3,\"1553\":1,\"1559\":2,\"1631\":1,\"1640\":1,\"1641\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1674\":1,\"1748\":1,\"1793\":1,\"1806\":2,\"1883\":2,\"1920\":2,\"1951\":1,\"1959\":1,\"1965\":1,\"1975\":1,\"1997\":1,\"2043\":1,\"2044\":10,\"2055\":1,\"2056\":1,\"2066\":1,\"2127\":1,\"2130\":6,\"2131\":2,\"2134\":3,\"2143\":1,\"2145\":5,\"2221\":1,\"2232\":1,\"2238\":1,\"2249\":2,\"2304\":2,\"2312\":2,\"2327\":1,\"2338\":1,\"2340\":2,\"2354\":1,\"2355\":4,\"2357\":1,\"2369\":1,\"2480\":1}}],[\"measures\",{\"1\":{\"877\":1,\"944\":1}}],[\"measure=\",{\"1\":{\"821\":1}}],[\"measure\",{\"1\":{\"205\":1,\"254\":1,\"819\":1,\"821\":3,\"924\":1,\"928\":1,\"938\":1,\"944\":1,\"2367\":2}}],[\"meanpooling\",{\"0\":{\"2190\":1},\"1\":{\"2190\":1}}],[\"meanpoolconv\",{\"0\":{\"1200\":1},\"1\":{\"1200\":1}}],[\"mean=0\",{\"1\":{\"1487\":1}}],[\"mean=true\",{\"1\":{\"1246\":1,\"1247\":1}}],[\"meaningful\",{\"1\":{\"290\":1}}],[\"meaningless\",{\"1\":{\"284\":1,\"290\":1}}],[\"meaning\",{\"1\":{\"141\":1,\"142\":1,\"1558\":1,\"2355\":1}}],[\"means=true\",{\"1\":{\"1728\":1,\"1850\":1}}],[\"means\",{\"1\":{\"81\":1,\"128\":1,\"138\":1,\"168\":1,\"200\":1,\"205\":3,\"211\":2,\"232\":1,\"242\":1,\"258\":1,\"259\":1,\"661\":1,\"669\":1,\"745\":1,\"821\":2,\"1130\":1,\"1400\":3,\"1450\":1,\"1452\":1,\"1610\":1,\"1628\":1,\"1656\":2,\"1671\":1,\"1683\":3,\"1700\":2,\"1730\":1,\"1782\":1,\"1817\":1,\"2138\":4}}],[\"mean\",{\"0\":{\"1567\":1,\"2190\":1},\"1\":{\"48\":4,\"141\":2,\"211\":1,\"217\":1,\"243\":2,\"266\":1,\"267\":1,\"275\":1,\"276\":1,\"285\":2,\"286\":1,\"629\":3,\"664\":3,\"699\":1,\"770\":1,\"786\":5,\"800\":5,\"921\":5,\"935\":5,\"959\":1,\"1050\":1,\"1116\":1,\"1161\":1,\"1189\":1,\"1210\":1,\"1218\":1,\"1221\":1,\"1224\":1,\"1225\":1,\"1229\":1,\"1244\":1,\"1246\":2,\"1247\":2,\"1325\":1,\"1326\":2,\"1552\":5,\"1553\":1,\"1567\":1,\"1601\":1,\"1602\":2,\"1611\":1,\"1612\":3,\"1613\":3,\"1622\":1,\"1625\":1,\"1626\":5,\"1656\":2,\"1700\":1,\"1754\":2,\"1839\":1,\"1991\":1,\"2183\":1,\"2190\":2,\"2237\":1,\"2435\":2,\"2436\":2,\"2463\":2,\"2464\":2}}],[\"megadecoder\",{\"0\":{\"634\":1},\"1\":{\"634\":3}}],[\"mega\",{\"0\":{\"633\":2,\"634\":1,\"637\":1,\"638\":1,\"646\":1,\"647\":1},\"1\":{\"142\":21,\"631\":1,\"633\":8,\"634\":4,\"637\":1,\"638\":1,\"646\":1,\"647\":1}}],[\"mentioned\",{\"1\":{\"127\":1,\"225\":1,\"290\":1}}],[\"menus\",{\"1\":{\"3\":1}}],[\"messages\",{\"1\":{\"2039\":1,\"2049\":4}}],[\"message\",{\"1\":{\"117\":1,\"134\":1,\"653\":3,\"1842\":3,\"2039\":10,\"2049\":2,\"2359\":1,\"2367\":1}}],[\"member\",{\"1\":{\"2325\":1,\"2327\":1}}],[\"members\",{\"1\":{\"3\":1}}],[\"members2rst\",{\"1\":{\"3\":1}}],[\"memlstm\",{\"0\":{\"1202\":1},\"1\":{\"1202\":2,\"1259\":2,\"1261\":2}}],[\"mem\",{\"1\":{\"168\":2,\"1029\":2,\"1202\":3,\"1259\":3,\"1261\":3,\"1279\":2,\"1281\":1}}],[\"mem=0\",{\"1\":{\"168\":1}}],[\"mem=\",{\"1\":{\"168\":1}}],[\"memory=none\",{\"1\":{\"733\":1,\"784\":2,\"1751\":1}}],[\"memory\",{\"1\":{\"95\":1,\"102\":1,\"106\":1,\"128\":1,\"173\":1,\"195\":1,\"243\":1,\"692\":10,\"703\":6,\"705\":1,\"717\":3,\"733\":1,\"755\":2,\"760\":1,\"775\":1,\"777\":1,\"785\":4,\"790\":4,\"820\":1,\"850\":8,\"1000\":1,\"1008\":1,\"1029\":2,\"1064\":1,\"1156\":1,\"1235\":1,\"1259\":1,\"1261\":1,\"1279\":10,\"1280\":12,\"1281\":9,\"1282\":1,\"1283\":11,\"1643\":1,\"1645\":3,\"1646\":1,\"1650\":1,\"1751\":15,\"1794\":1,\"1992\":3,\"2000\":1,\"2001\":1,\"2130\":1,\"2136\":1,\"2151\":1,\"2310\":1,\"2354\":1}}],[\"mechanism\",{\"0\":{\"76\":1},\"1\":{\"1519\":1,\"1536\":1,\"1705\":1,\"1706\":1}}],[\"meet\",{\"1\":{\"56\":1,\"163\":1,\"195\":1,\"265\":1,\"269\":1,\"274\":1,\"278\":1}}],[\"magnification\",{\"1\":{\"2014\":1}}],[\"magnitude\",{\"1\":{\"1210\":1,\"1301\":1,\"1306\":1,\"1371\":1,\"1372\":1,\"1392\":1,\"2307\":1}}],[\"magic\",{\"1\":{\"1502\":1}}],[\"mag\",{\"1\":{\"1210\":1}}],[\"maailma\",{\"1\":{\"287\":1}}],[\"mae\",{\"1\":{\"2239\":1,\"2240\":1,\"2241\":1}}],[\"maekaku\",{\"1\":{\"207\":1}}],[\"maes\",{\"1\":{\"45\":2,\"139\":1,\"145\":2,\"616\":4,\"625\":1,\"627\":1,\"628\":1,\"696\":6,\"697\":6,\"743\":1,\"1762\":1}}],[\"matmul\",{\"0\":{\"1342\":1},\"1\":{\"1342\":1}}],[\"mat^h\",{\"1\":{\"1319\":1}}],[\"matters\",{\"1\":{\"1155\":1,\"1157\":1}}],[\"matthew\",{\"1\":{\"156\":1}}],[\"mat=none\",{\"1\":{\"1126\":1}}],[\"matrices\",{\"1\":{\"944\":1,\"1683\":1,\"1883\":1}}],[\"matrix\",{\"0\":{\"1317\":1,\"1326\":1,\"1329\":1,\"1558\":1},\"1\":{\"703\":1,\"709\":1,\"717\":2,\"755\":3,\"774\":1,\"780\":1,\"785\":3,\"804\":3,\"823\":1,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"924\":1,\"928\":1,\"932\":3,\"934\":3,\"1126\":5,\"1293\":1,\"1308\":3,\"1309\":2,\"1310\":2,\"1311\":3,\"1314\":1,\"1315\":2,\"1317\":6,\"1318\":2,\"1319\":2,\"1321\":2,\"1322\":3,\"1323\":2,\"1326\":2,\"1327\":2,\"1328\":2,\"1329\":2,\"1330\":2,\"1334\":1,\"1351\":3,\"1352\":2,\"1361\":2,\"1533\":1,\"1558\":5,\"1577\":1,\"1589\":1,\"1599\":1,\"1637\":2,\"1785\":1,\"1817\":1,\"1881\":2,\"1882\":1,\"1883\":2,\"2191\":1,\"2443\":1}}],[\"mat\",{\"1\":{\"548\":1,\"551\":1,\"558\":1,\"561\":2,\"567\":1,\"575\":1,\"1126\":1,\"1319\":4,\"1361\":2,\"1728\":1,\"1881\":2,\"1882\":1,\"1883\":2}}],[\"mat|hdf5|sound\",{\"1\":{\"521\":1,\"524\":1,\"525\":1}}],[\"matplotlib\",{\"1\":{\"377\":2,\"449\":2,\"2348\":1,\"2359\":2,\"2370\":2,\"2372\":1}}],[\"matchs\",{\"1\":{\"1675\":1}}],[\"matching\",{\"1\":{\"1587\":3,\"2216\":1}}],[\"match\",{\"1\":{\"50\":1,\"51\":1,\"162\":1,\"225\":1,\"249\":1,\"267\":1,\"276\":2,\"286\":1,\"699\":1,\"1292\":1,\"1389\":3,\"1396\":3,\"1401\":3,\"1408\":3,\"1454\":1,\"1456\":1,\"1466\":3,\"1526\":9,\"1529\":1,\"1553\":6,\"1598\":6,\"1600\":9,\"1625\":6}}],[\"maps\",{\"1\":{\"1165\":1,\"1515\":1,\"1516\":1,\"1534\":1}}],[\"map\",{\"0\":{\"2106\":1},\"1\":{\"74\":1,\"211\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"744\":1,\"817\":1,\"821\":1,\"828\":1,\"829\":1,\"830\":1,\"859\":1,\"958\":2,\"960\":7,\"961\":1,\"2168\":1,\"2314\":1}}],[\"mapping\",{\"1\":{\"66\":1,\"196\":2,\"213\":2,\"235\":2,\"268\":3,\"277\":3,\"828\":1,\"829\":1,\"830\":1,\"912\":1,\"942\":1,\"987\":1,\"989\":1,\"991\":1,\"993\":1,\"996\":1,\"1000\":1,\"1001\":1,\"1003\":1,\"1007\":1,\"1011\":1,\"1015\":1,\"1125\":3,\"1267\":1,\"2000\":3,\"2001\":2,\"2132\":2,\"2145\":1,\"2146\":1,\"2147\":1,\"2156\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2334\":1,\"2354\":2}}],[\"maximize\",{\"1\":{\"2147\":1}}],[\"maximizing\",{\"1\":{\"1131\":1,\"1172\":1}}],[\"maximum\",{\"0\":{\"920\":1},\"1\":{\"45\":6,\"48\":3,\"71\":2,\"98\":1,\"139\":1,\"141\":2,\"142\":2,\"145\":3,\"175\":3,\"515\":2,\"516\":1,\"616\":3,\"625\":1,\"627\":1,\"633\":2,\"634\":2,\"637\":1,\"645\":1,\"646\":1,\"647\":2,\"661\":1,\"664\":1,\"696\":4,\"697\":4,\"699\":1,\"703\":2,\"755\":2,\"785\":2,\"802\":1,\"817\":1,\"821\":1,\"824\":1,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"919\":2,\"920\":1,\"1061\":2,\"1062\":1,\"1419\":1,\"1526\":1,\"1533\":1,\"1552\":1,\"1553\":1,\"1556\":1,\"1558\":1,\"1596\":1,\"1597\":1,\"1604\":1,\"1606\":1,\"1607\":1,\"1625\":1,\"1626\":1,\"1665\":1,\"1716\":1,\"1719\":2,\"1720\":1,\"1721\":1,\"1725\":3,\"1750\":1,\"1784\":1,\"1786\":1,\"1806\":3,\"1808\":1,\"1818\":1,\"1820\":1,\"1837\":1,\"1862\":1,\"1868\":1,\"1870\":1,\"1993\":1,\"2000\":2,\"2001\":2,\"2019\":1,\"2020\":1,\"2021\":1,\"2039\":3,\"2049\":1,\"2065\":1,\"2134\":1,\"2136\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2224\":1,\"2245\":1,\"2428\":1,\"2431\":1,\"2432\":1,\"2442\":1,\"2482\":1,\"2495\":1}}],[\"maxpool=true\",{\"1\":{\"1068\":1,\"1233\":1}}],[\"maxpool2d\",{\"1\":{\"665\":1}}],[\"max=0\",{\"1\":{\"821\":1,\"1224\":1}}],[\"max=none\",{\"1\":{\"817\":2}}],[\"maxu\",{\"1\":{\"703\":2,\"716\":2,\"755\":2,\"785\":2,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"908\":1,\"919\":2}}],[\"maxt\",{\"1\":{\"703\":2,\"755\":2,\"785\":2,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"908\":1,\"919\":2}}],[\"maxchars\",{\"1\":{\"515\":1}}],[\"maxframes\",{\"1\":{\"515\":1}}],[\"maxlen=none\",{\"1\":{\"1903\":1}}],[\"maxlenratio=30\",{\"1\":{\"2224\":1}}],[\"maxlenratio=10\",{\"1\":{\"1750\":1}}],[\"maxlenratio=0\",{\"1\":{\"1720\":1,\"1721\":1,\"1725\":1,\"1806\":1,\"1862\":1}}],[\"maxlenratio<0\",{\"1\":{\"1725\":1,\"1806\":1}}],[\"maxlenratio\",{\"1\":{\"175\":6,\"301\":2,\"315\":2,\"396\":2,\"406\":4,\"421\":2,\"429\":2,\"442\":2,\"463\":4,\"469\":2,\"484\":2,\"490\":2,\"505\":2,\"1719\":2,\"1720\":4,\"1721\":2,\"1725\":4,\"1726\":1,\"1727\":1,\"1806\":2,\"1862\":2,\"1976\":1,\"1993\":2,\"2245\":2,\"2431\":2,\"2432\":2}}],[\"maxlen\",{\"1\":{\"48\":2,\"389\":2,\"692\":13,\"760\":3,\"775\":4,\"790\":8,\"820\":4,\"850\":11,\"1719\":2,\"1720\":2,\"1725\":2,\"1751\":11,\"1806\":2,\"1992\":4}}],[\"max\",{\"0\":{\"933\":1},\"1\":{\"45\":3,\"48\":2,\"91\":1,\"98\":1,\"132\":1,\"141\":2,\"142\":2,\"145\":2,\"168\":2,\"211\":1,\"217\":1,\"243\":3,\"266\":1,\"275\":1,\"285\":1,\"295\":2,\"301\":4,\"374\":2,\"377\":4,\"415\":2,\"421\":4,\"449\":6,\"463\":2,\"514\":2,\"541\":2,\"616\":4,\"633\":2,\"634\":2,\"645\":2,\"646\":2,\"647\":6,\"661\":2,\"664\":2,\"670\":1,\"674\":2,\"692\":2,\"696\":4,\"697\":4,\"699\":7,\"709\":1,\"710\":1,\"711\":1,\"716\":1,\"733\":3,\"734\":3,\"746\":1,\"778\":1,\"780\":1,\"787\":1,\"790\":1,\"795\":2,\"817\":1,\"819\":1,\"821\":2,\"850\":1,\"851\":1,\"933\":2,\"941\":2,\"970\":1,\"974\":1,\"978\":4,\"1062\":1,\"1130\":3,\"1163\":1,\"1164\":1,\"1224\":3,\"1225\":3,\"1333\":1,\"1389\":1,\"1390\":1,\"1392\":1,\"1401\":3,\"1402\":3,\"1408\":3,\"1409\":3,\"1420\":1,\"1466\":2,\"1467\":2,\"1526\":5,\"1533\":2,\"1549\":3,\"1552\":2,\"1553\":5,\"1558\":2,\"1574\":1,\"1593\":1,\"1594\":2,\"1595\":3,\"1596\":2,\"1597\":3,\"1598\":3,\"1600\":3,\"1604\":2,\"1606\":2,\"1618\":1,\"1625\":5,\"1626\":2,\"1643\":1,\"1646\":1,\"1665\":2,\"1676\":2,\"1704\":2,\"1705\":1,\"1706\":1,\"1707\":2,\"1708\":2,\"1710\":3,\"1711\":3,\"1712\":3,\"1713\":2,\"1714\":2,\"1715\":3,\"1716\":3,\"1720\":1,\"1721\":1,\"1725\":2,\"1748\":2,\"1768\":4,\"1770\":2,\"1771\":2,\"1784\":2,\"1786\":2,\"1801\":1,\"1806\":3,\"1808\":2,\"1818\":2,\"1820\":2,\"1837\":2,\"1854\":1,\"1862\":1,\"1868\":3,\"1870\":3,\"1941\":2,\"1943\":2,\"1949\":1,\"1977\":1,\"1992\":1,\"2000\":2,\"2001\":2,\"2008\":1,\"2014\":4,\"2015\":2,\"2018\":5,\"2065\":2,\"2124\":1,\"2128\":1,\"2136\":2,\"2146\":1,\"2155\":1,\"2191\":1,\"2249\":1,\"2253\":1,\"2258\":4,\"2339\":2,\"2342\":2,\"2348\":1,\"2354\":2,\"2370\":2,\"2372\":3,\"2390\":1,\"2428\":2,\"2434\":1,\"2435\":1,\"2438\":2,\"2439\":2,\"2440\":2,\"2442\":2,\"2458\":1,\"2462\":2,\"2482\":1,\"2495\":1}}],[\"macro\",{\"1\":{\"235\":1}}],[\"macosx\",{\"1\":{\"161\":1}}],[\"macos12\",{\"1\":{\"160\":1}}],[\"macaron\",{\"1\":{\"43\":2,\"243\":1,\"618\":3,\"624\":3,\"709\":3,\"710\":1,\"733\":1,\"734\":1,\"735\":2,\"774\":3,\"780\":3,\"1107\":3,\"1519\":3,\"1526\":1,\"1535\":3,\"1536\":3,\"1546\":3,\"1552\":3,\"1553\":1,\"1598\":1,\"1599\":3,\"1600\":1,\"1622\":3,\"1625\":1,\"1626\":3,\"1994\":1,\"2126\":1,\"2191\":3,\"2239\":2,\"2240\":1,\"2411\":3,\"2412\":3,\"2423\":3,\"2447\":3}}],[\"machines\",{\"1\":{\"165\":2,\"166\":1}}],[\"machine\",{\"0\":{\"152\":1,\"238\":1},\"1\":{\"26\":1,\"67\":2,\"110\":1,\"152\":1,\"165\":1,\"166\":1,\"202\":2,\"522\":1,\"586\":1}}],[\"made\",{\"1\":{\"43\":1,\"106\":1,\"139\":2,\"223\":1,\"269\":1,\"278\":1,\"2355\":1}}],[\"major\",{\"1\":{\"38\":1,\"1011\":1}}],[\"majority\",{\"1\":{\"36\":1,\"821\":1}}],[\"maybe\",{\"1\":{\"66\":1,\"2304\":1}}],[\"may\",{\"1\":{\"36\":1,\"70\":1,\"71\":1,\"96\":1,\"124\":1,\"132\":1,\"138\":4,\"152\":1,\"174\":1,\"217\":1,\"224\":1,\"225\":1,\"243\":2,\"262\":1,\"263\":1,\"269\":4,\"278\":4,\"286\":1,\"290\":1,\"722\":1,\"724\":2,\"725\":2,\"726\":2,\"727\":2,\"728\":2,\"744\":2,\"777\":1,\"828\":2,\"829\":2,\"830\":2,\"859\":2,\"1031\":1,\"1035\":1,\"1086\":2,\"1112\":1,\"1113\":1,\"1156\":1,\"1207\":2,\"1228\":1,\"1246\":1,\"1250\":1,\"1251\":1,\"1350\":1,\"1726\":1,\"1727\":1,\"1759\":1,\"1940\":1,\"1942\":1,\"2130\":2,\"2162\":1}}],[\"make\",{\"0\":{\"197\":1,\"524\":1,\"525\":1,\"669\":1,\"670\":1,\"1340\":1,\"1341\":1,\"1901\":1,\"1903\":1},\"1\":{\"82\":1,\"95\":1,\"104\":1,\"128\":1,\"132\":1,\"139\":1,\"152\":1,\"161\":3,\"162\":4,\"194\":1,\"200\":2,\"223\":1,\"224\":1,\"225\":2,\"242\":2,\"243\":2,\"267\":1,\"269\":3,\"276\":1,\"278\":3,\"284\":1,\"286\":5,\"290\":2,\"524\":3,\"525\":3,\"535\":1,\"536\":1,\"606\":1,\"616\":1,\"669\":1,\"670\":1,\"738\":1,\"821\":1,\"1051\":1,\"1271\":1,\"1340\":1,\"1341\":1,\"1397\":1,\"1444\":1,\"1448\":1,\"1484\":1,\"1656\":1,\"1679\":1,\"1754\":1,\"1901\":2,\"1902\":5,\"1903\":2,\"1904\":5,\"1935\":1,\"2355\":1,\"2480\":1}}],[\"makes\",{\"1\":{\"48\":2,\"262\":2,\"2325\":1,\"2327\":1}}],[\"makefile\",{\"1\":{\"33\":1,\"162\":1}}],[\"making\",{\"1\":{\"26\":1,\"1269\":1,\"1270\":1,\"1271\":2}}],[\"manifests\",{\"1\":{\"2139\":1,\"2157\":1}}],[\"manifest\",{\"1\":{\"2139\":3,\"2157\":1}}],[\"manifold\",{\"1\":{\"1131\":1,\"1172\":1}}],[\"manner\",{\"1\":{\"1513\":1,\"1548\":1,\"1551\":1,\"1592\":1,\"1605\":1,\"1606\":1,\"1750\":3,\"2224\":1,\"2225\":1}}],[\"manages\",{\"1\":{\"2044\":1,\"2134\":1}}],[\"management\",{\"1\":{\"2044\":1}}],[\"manage\",{\"1\":{\"1031\":1,\"1035\":1,\"1112\":1,\"1113\":1,\"1250\":1,\"1251\":1,\"2045\":1,\"2054\":1,\"2334\":1,\"2344\":1}}],[\"mandarin\",{\"1\":{\"269\":2,\"278\":2}}],[\"mandatory\",{\"1\":{\"43\":1,\"50\":1,\"81\":1,\"96\":1,\"106\":2,\"140\":1,\"141\":2,\"150\":1}}],[\"manual\",{\"1\":{\"153\":1,\"2355\":4}}],[\"manually\",{\"1\":{\"1\":1,\"39\":1,\"41\":1,\"55\":1,\"123\":1,\"162\":1,\"173\":1,\"243\":1,\"267\":1,\"269\":1,\"276\":1,\"278\":1,\"286\":1,\"536\":1}}],[\"many\",{\"1\":{\"32\":1,\"78\":2,\"82\":1,\"139\":1,\"756\":2,\"773\":2,\"866\":2,\"867\":2,\"1396\":1,\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2355\":1}}],[\"maintaining\",{\"1\":{\"2039\":1}}],[\"mainly\",{\"1\":{\"276\":1,\"2184\":1}}],[\"main\",{\"0\":{\"106\":1,\"661\":1,\"1948\":1,\"1949\":1,\"1950\":1,\"1951\":1,\"1952\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"2153\":1,\"2533\":1},\"1\":{\"22\":1,\"38\":3,\"39\":8,\"44\":3,\"49\":1,\"78\":2,\"141\":4,\"142\":1,\"143\":1,\"147\":2,\"150\":3,\"195\":1,\"201\":1,\"263\":1,\"285\":1,\"625\":1,\"626\":3,\"637\":1,\"655\":3,\"656\":3,\"657\":3,\"659\":3,\"661\":3,\"689\":1,\"750\":1,\"1086\":3,\"1153\":2,\"1155\":1,\"1157\":1,\"1207\":3,\"1316\":1,\"1320\":1,\"1731\":2,\"1822\":2,\"1948\":1,\"1949\":1,\"1950\":1,\"1951\":1,\"1952\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"1962\":1,\"1966\":2,\"2018\":1,\"2130\":1,\"2153\":2,\"2240\":1,\"2249\":2,\"2338\":1,\"2369\":1,\"2462\":1}}],[\"maiti\",{\"1\":{\"7\":1,\"10\":1}}],[\"martix\",{\"1\":{\"1881\":1,\"1883\":1}}],[\"margins\",{\"1\":{\"2167\":1,\"2176\":1}}],[\"margin\",{\"1\":{\"1730\":2,\"2167\":8,\"2176\":11}}],[\"margin=0\",{\"1\":{\"1730\":1}}],[\"marginal\",{\"1\":{\"1224\":3,\"1225\":3,\"1245\":2}}],[\"marginalize\",{\"1\":{\"262\":1}}],[\"maruyama\",{\"1\":{\"1245\":1,\"1253\":1}}],[\"marked\",{\"1\":{\"2147\":1}}],[\"marker\",{\"1\":{\"134\":4,\"135\":2,\"136\":2,\"543\":2,\"2477\":1}}],[\"markers\",{\"1\":{\"133\":1,\"2143\":1}}],[\"mark\",{\"1\":{\"269\":2,\"278\":2}}],[\"marks\",{\"1\":{\"265\":1,\"269\":1,\"274\":1,\"278\":1}}],[\"marking\",{\"1\":{\"134\":2}}],[\"markdown\",{\"1\":{\"2\":2,\"3\":2}}],[\"marc\",{\"1\":{\"15\":1}}],[\"maskalongaxisvariablemaxwidth\",{\"0\":{\"1665\":1},\"1\":{\"1665\":1}}],[\"maskalongaxis\",{\"0\":{\"1664\":1},\"1\":{\"1664\":1}}],[\"masknet\",{\"1\":{\"1267\":1,\"1273\":1,\"1274\":1}}],[\"maskestimator\",{\"0\":{\"1199\":1},\"1\":{\"1199\":1}}],[\"masked\",{\"1\":{\"674\":2,\"777\":1,\"846\":4,\"978\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":2,\"1125\":1,\"1130\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1268\":1,\"1278\":1,\"1280\":1,\"1283\":1,\"1638\":3,\"1640\":1,\"1782\":1,\"1905\":2,\"2218\":4,\"2219\":1,\"2220\":2}}],[\"maskdecoder\",{\"0\":{\"1198\":1},\"1\":{\"1198\":2}}],[\"maskparallelscorerinterface\",{\"0\":{\"1793\":1},\"1\":{\"692\":1,\"1793\":1}}],[\"mask=false\",{\"1\":{\"675\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1175\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1}}],[\"mask=none\",{\"1\":{\"675\":2,\"733\":1,\"781\":1,\"784\":2,\"1185\":1,\"1290\":1,\"1469\":1,\"1751\":1,\"1966\":1}}],[\"masks=none\",{\"1\":{\"1577\":1,\"1590\":2,\"1753\":2}}],[\"masks\",{\"1\":{\"633\":1,\"699\":1,\"709\":1,\"734\":1,\"846\":2,\"978\":1,\"980\":1,\"1053\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":7,\"1125\":1,\"1126\":7,\"1127\":2,\"1130\":1,\"1136\":1,\"1141\":1,\"1155\":1,\"1157\":1,\"1162\":1,\"1199\":2,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1268\":1,\"1278\":1,\"1280\":1,\"1283\":1,\"1354\":5,\"1577\":1,\"1590\":2,\"1753\":4,\"1770\":1,\"1771\":2,\"1806\":2,\"1901\":1,\"1902\":1,\"1903\":1,\"1904\":1,\"1934\":1,\"2143\":2,\"2220\":3,\"2428\":2,\"2433\":3}}],[\"masking=false\",{\"1\":{\"1764\":1,\"1839\":1,\"2226\":1,\"2237\":1}}],[\"masking=true\",{\"1\":{\"1764\":1,\"1839\":1,\"2226\":1,\"2237\":1,\"2245\":1,\"2431\":1,\"2432\":1}}],[\"masking\",{\"0\":{\"941\":1,\"2220\":1},\"1\":{\"225\":1,\"674\":2,\"738\":1,\"846\":4,\"941\":2,\"1118\":2,\"1125\":4,\"1267\":3,\"1526\":2,\"1598\":2,\"1599\":6,\"1600\":2,\"1627\":6,\"1764\":4,\"1839\":4,\"1991\":2,\"1994\":2,\"2133\":1,\"2215\":1,\"2220\":7,\"2226\":4,\"2235\":5,\"2236\":5,\"2237\":4,\"2239\":6,\"2240\":6,\"2241\":6,\"2245\":5,\"2411\":6,\"2412\":6,\"2413\":6,\"2423\":6,\"2424\":6,\"2431\":5,\"2432\":6,\"2447\":6,\"2448\":6}}],[\"mask\",{\"0\":{\"669\":1,\"670\":1,\"969\":1,\"978\":1,\"1199\":1,\"1574\":1,\"1664\":1,\"1665\":1,\"1691\":2,\"1884\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1907\":2,\"1926\":2,\"1929\":1,\"2068\":1,\"2220\":1},\"1\":{\"223\":1,\"243\":6,\"301\":2,\"421\":2,\"617\":11,\"618\":11,\"619\":3,\"620\":18,\"621\":5,\"622\":3,\"623\":3,\"624\":11,\"633\":11,\"636\":9,\"637\":3,\"644\":12,\"669\":6,\"670\":3,\"674\":28,\"675\":6,\"692\":12,\"699\":5,\"701\":4,\"710\":3,\"711\":3,\"713\":1,\"720\":1,\"730\":1,\"735\":4,\"738\":4,\"745\":1,\"746\":1,\"747\":10,\"748\":2,\"749\":2,\"771\":1,\"776\":3,\"777\":2,\"784\":5,\"787\":7,\"790\":3,\"795\":4,\"815\":1,\"818\":1,\"820\":1,\"828\":1,\"830\":1,\"833\":7,\"846\":30,\"849\":1,\"850\":9,\"851\":2,\"969\":1,\"974\":1,\"978\":8,\"980\":1,\"1053\":3,\"1062\":3,\"1107\":4,\"1117\":4,\"1118\":10,\"1119\":1,\"1125\":3,\"1126\":5,\"1127\":4,\"1130\":4,\"1131\":1,\"1136\":4,\"1141\":4,\"1155\":4,\"1157\":7,\"1162\":3,\"1170\":3,\"1171\":3,\"1172\":3,\"1173\":3,\"1174\":3,\"1175\":3,\"1198\":1,\"1199\":2,\"1217\":9,\"1232\":4,\"1252\":3,\"1261\":4,\"1262\":1,\"1267\":6,\"1268\":3,\"1273\":6,\"1274\":6,\"1278\":4,\"1280\":3,\"1283\":3,\"1290\":2,\"1326\":2,\"1354\":3,\"1400\":1,\"1519\":1,\"1520\":3,\"1524\":3,\"1525\":3,\"1535\":3,\"1536\":1,\"1537\":1,\"1539\":1,\"1546\":1,\"1552\":2,\"1554\":1,\"1556\":6,\"1574\":1,\"1577\":1,\"1581\":3,\"1583\":3,\"1586\":1,\"1590\":2,\"1601\":3,\"1603\":3,\"1611\":1,\"1612\":1,\"1613\":1,\"1616\":3,\"1622\":1,\"1626\":2,\"1628\":3,\"1664\":3,\"1665\":4,\"1691\":6,\"1735\":9,\"1736\":12,\"1738\":4,\"1739\":4,\"1740\":4,\"1741\":4,\"1742\":4,\"1743\":4,\"1744\":4,\"1745\":4,\"1746\":4,\"1749\":5,\"1751\":12,\"1756\":5,\"1757\":5,\"1759\":4,\"1766\":1,\"1785\":3,\"1789\":5,\"1790\":5,\"1793\":1,\"1794\":7,\"1806\":6,\"1817\":3,\"1847\":5,\"1851\":12,\"1901\":3,\"1902\":5,\"1903\":3,\"1904\":5,\"1905\":2,\"1906\":1,\"1907\":4,\"1926\":5,\"1965\":2,\"1966\":1,\"1977\":1,\"1984\":3,\"1992\":3,\"2124\":1,\"2128\":1,\"2129\":1,\"2130\":2,\"2133\":2,\"2136\":2,\"2137\":2,\"2143\":3,\"2217\":1,\"2218\":4,\"2219\":2,\"2220\":37,\"2235\":1,\"2236\":1,\"2245\":1,\"2428\":1,\"2431\":1,\"2451\":1,\"2458\":1,\"2460\":2,\"2463\":2,\"2464\":2,\"2473\":3}}],[\"maskctcmodel\",{\"0\":{\"777\":1},\"1\":{\"776\":1,\"777\":1}}],[\"maskctcinference\",{\"0\":{\"776\":1},\"1\":{\"776\":1}}],[\"maskctc\",{\"0\":{\"309\":1,\"776\":1,\"777\":1,\"1907\":1},\"1\":{\"137\":1,\"309\":5,\"776\":1,\"777\":1,\"1907\":1}}],[\"master\",{\"0\":{\"2382\":1,\"2383\":1},\"1\":{\"26\":1,\"34\":1,\"58\":4,\"60\":4,\"61\":6,\"195\":2,\"213\":1,\"242\":1,\"260\":1,\"268\":1,\"277\":1,\"374\":4,\"377\":4,\"449\":4,\"536\":1,\"669\":1,\"670\":1,\"747\":1,\"1053\":2,\"1308\":1,\"1332\":1,\"1513\":1,\"1548\":1,\"1551\":1,\"1592\":1,\"1605\":1,\"1606\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1,\"2151\":1,\"2198\":1,\"2286\":1,\"2310\":1,\"2340\":4,\"2382\":1,\"2383\":1}}],[\"masuyama\",{\"1\":{\"11\":1}}],[\"masao\",{\"1\":{\"5\":1,\"16\":1}}],[\"vpsde\",{\"1\":{\"2423\":1,\"2428\":2}}],[\"vq\",{\"0\":{\"1400\":1,\"1432\":1,\"1439\":1,\"1441\":1,\"1469\":1,\"1473\":1,\"1474\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1495\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1504\":1},\"1\":{\"1400\":1,\"1432\":1,\"1439\":1,\"1441\":1,\"1469\":1,\"1473\":1,\"1474\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1495\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1504\":1}}],[\"vbar^h\",{\"1\":{\"1311\":1}}],[\"vbar\",{\"1\":{\"1311\":2}}],[\"vboxnet\",{\"1\":{\"67\":2}}],[\"vram\",{\"1\":{\"1264\":1,\"1334\":1}}],[\"v=none\",{\"1\":{\"926\":1}}],[\"vdim=none\",{\"1\":{\"787\":1}}],[\"vs\",{\"1\":{\"768\":1,\"820\":2,\"828\":2,\"2130\":1,\"2241\":2}}],[\"vscode\",{\"1\":{\"18\":1,\"19\":1}}],[\"vjp\",{\"1\":{\"756\":2,\"773\":2,\"866\":1,\"867\":1}}],[\"v+1\",{\"1\":{\"703\":4,\"705\":1,\"716\":1,\"717\":1,\"755\":4,\"785\":1,\"804\":3,\"878\":2,\"879\":2,\"881\":3,\"882\":1,\"883\":1,\"884\":1,\"919\":2,\"932\":3,\"934\":3,\"936\":2,\"937\":2}}],[\"vʲ\",{\"1\":{\"287\":1}}],[\"vctk\",{\"1\":{\"187\":2,\"286\":3,\"289\":1}}],[\"v13\",{\"1\":{\"1002\":1}}],[\"v1\",{\"1\":{\"175\":1,\"242\":1,\"243\":4,\"267\":1,\"276\":1,\"286\":6,\"290\":3,\"523\":1,\"527\":13,\"536\":30}}],[\"v\",{\"1\":{\"142\":2,\"162\":2,\"168\":2,\"269\":3,\"278\":3,\"287\":2,\"633\":4,\"634\":2,\"644\":2,\"785\":1,\"882\":1,\"883\":1,\"884\":2,\"919\":3,\"922\":2,\"924\":5,\"926\":2,\"1130\":1,\"1545\":1,\"1713\":4,\"1714\":4,\"1715\":4,\"1716\":4,\"2235\":1,\"2236\":1,\"2241\":2,\"2359\":2,\"2398\":1,\"2421\":1}}],[\"vowels\",{\"1\":{\"2281\":1,\"2298\":3}}],[\"vowels=false\",{\"1\":{\"2277\":1}}],[\"vol\",{\"1\":{\"1309\":1,\"1311\":1}}],[\"volumeperturbation\",{\"0\":{\"1852\":1},\"1\":{\"1852\":1}}],[\"volume=\",{\"1\":{\"207\":1}}],[\"volume\",{\"1\":{\"202\":1,\"1852\":1,\"2336\":1,\"2337\":1,\"2346\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2368\":1}}],[\"vocabs\",{\"1\":{\"1535\":2,\"1546\":2,\"1552\":2,\"1622\":2,\"1626\":2}}],[\"vocabrary\",{\"1\":{\"1526\":1,\"1553\":1,\"1598\":1,\"1600\":1,\"1625\":1}}],[\"vocab\",{\"1\":{\"625\":2,\"628\":1,\"634\":2,\"641\":2,\"643\":2,\"651\":2,\"692\":3,\"703\":1,\"731\":1,\"732\":1,\"736\":1,\"737\":2,\"755\":1,\"760\":4,\"766\":1,\"767\":1,\"770\":1,\"775\":1,\"777\":1,\"785\":1,\"790\":2,\"796\":1,\"797\":1,\"820\":4,\"847\":2,\"848\":1,\"850\":2,\"878\":1,\"879\":1,\"881\":1,\"884\":1,\"958\":2,\"959\":1,\"960\":1,\"962\":3,\"1535\":1,\"1640\":1,\"1641\":1,\"1719\":5,\"1720\":1,\"1721\":2,\"1723\":1,\"1724\":1,\"1725\":3,\"1726\":1,\"1727\":1,\"1787\":4,\"1798\":1,\"1799\":1,\"1800\":1,\"1806\":2,\"1822\":1,\"1862\":2,\"1940\":1,\"1942\":1,\"1944\":3,\"1945\":2,\"1946\":1,\"1947\":3,\"1959\":2,\"1965\":1,\"1975\":3,\"1987\":1,\"1992\":2,\"1995\":2,\"1996\":1,\"1997\":1,\"2061\":2,\"2127\":1,\"2137\":2,\"2143\":2,\"2219\":1,\"2221\":2,\"2462\":1}}],[\"vocabulary\",{\"0\":{\"372\":1,\"483\":1,\"511\":1},\"1\":{\"200\":6,\"243\":2,\"259\":1,\"372\":2,\"481\":4,\"511\":2,\"625\":2,\"634\":1,\"641\":1,\"643\":1,\"703\":2,\"716\":1,\"717\":1,\"755\":2,\"760\":1,\"785\":2,\"804\":2,\"878\":3,\"879\":3,\"881\":3,\"882\":3,\"883\":3,\"884\":3,\"919\":3,\"922\":2,\"932\":2,\"934\":2,\"936\":1,\"937\":1,\"1535\":1,\"1546\":1,\"1552\":1,\"1622\":1,\"1626\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"1760\":1,\"1787\":1,\"1821\":1,\"1862\":1,\"2130\":9,\"2136\":4,\"2137\":3,\"2278\":1,\"2283\":1,\"2291\":1}}],[\"vocoders\",{\"1\":{\"286\":2,\"290\":2}}],[\"vocoder\",{\"0\":{\"413\":1,\"480\":1,\"489\":1,\"495\":1,\"1514\":1,\"1517\":1,\"1524\":1,\"1525\":1,\"1527\":1,\"1533\":1,\"1534\":1,\"1547\":1,\"1549\":1,\"1551\":1,\"1558\":1,\"2422\":1},\"1\":{\"265\":1,\"266\":2,\"267\":28,\"274\":1,\"275\":2,\"276\":22,\"284\":3,\"286\":10,\"290\":26,\"406\":6,\"475\":4,\"484\":6,\"490\":6,\"523\":1,\"536\":4,\"575\":1,\"1514\":1,\"1517\":1,\"1524\":1,\"1525\":1,\"1526\":10,\"1527\":1,\"1533\":1,\"1534\":1,\"1547\":1,\"1549\":1,\"1551\":1,\"1552\":3,\"1553\":5,\"1558\":1,\"1598\":2,\"1600\":10,\"1625\":2,\"1698\":1,\"1971\":2,\"1976\":2,\"1979\":1,\"1981\":1,\"1983\":1,\"2222\":2,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2263\":3,\"2268\":3,\"2270\":3,\"2271\":3,\"2403\":2,\"2404\":1,\"2411\":1,\"2412\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2422\":3,\"2423\":1,\"2431\":1,\"2432\":1,\"2445\":2,\"2447\":1}}],[\"voxceleb2\",{\"1\":{\"255\":1}}],[\"voxceleb\",{\"1\":{\"255\":3,\"289\":1}}],[\"voxceleb1\",{\"1\":{\"201\":1,\"255\":1}}],[\"voxlingua107\",{\"1\":{\"234\":1,\"236\":3,\"2000\":1}}],[\"voxforge\",{\"1\":{\"201\":1}}],[\"vorbis\",{\"1\":{\"71\":1,\"1678\":1}}],[\"voicemos\",{\"1\":{\"285\":1}}],[\"voiced\",{\"1\":{\"267\":2,\"276\":2,\"1545\":4}}],[\"voice\",{\"0\":{\"13\":1,\"188\":1,\"264\":1},\"1\":{\"13\":1,\"200\":1,\"247\":3,\"269\":1,\"278\":1,\"1252\":1,\"1521\":1,\"1750\":1,\"1768\":1,\"1994\":2,\"2044\":1,\"2223\":1,\"2224\":1,\"2227\":1,\"2228\":1,\"2229\":1,\"2231\":2,\"2235\":2,\"2236\":2,\"2239\":1,\"2240\":2,\"2245\":3,\"2255\":1,\"2363\":1,\"2378\":1}}],[\"vmnet\",{\"1\":{\"67\":2}}],[\"vgg2l\",{\"0\":{\"1851\":2,\"1893\":1},\"1\":{\"1851\":6,\"1863\":1,\"1866\":1,\"1893\":1}}],[\"vggrnnencoder\",{\"0\":{\"862\":1},\"1\":{\"862\":2}}],[\"vgg\",{\"0\":{\"862\":1},\"1\":{\"53\":1,\"141\":2,\"621\":3,\"665\":3,\"862\":1,\"1851\":5,\"1893\":1}}],[\"v0\",{\"1\":{\"48\":2}}],[\"van\",{\"1\":{\"1319\":1}}],[\"vandermonde\",{\"1\":{\"637\":2}}],[\"vaswani\",{\"1\":{\"1149\":1}}],[\"vaccines\",{\"1\":{\"242\":1}}],[\"vadscpwriter\",{\"0\":{\"1013\":1},\"1\":{\"1013\":1,\"1014\":1}}],[\"vadscpreader\",{\"0\":{\"1011\":1},\"1\":{\"1011\":1,\"1012\":1}}],[\"vad\",{\"0\":{\"1011\":1,\"1013\":1,\"2034\":2,\"2065\":2},\"1\":{\"201\":1,\"217\":1,\"285\":1,\"1011\":3,\"1013\":2,\"1014\":1,\"2034\":2,\"2044\":1,\"2065\":7,\"2101\":1}}],[\"vars=false\",{\"1\":{\"1728\":1,\"1850\":1}}],[\"vars\",{\"1\":{\"1656\":2,\"1671\":1,\"1700\":2,\"1846\":1,\"1977\":2}}],[\"var=\",{\"1\":{\"1331\":1}}],[\"varoable\",{\"1\":{\"881\":1,\"884\":1}}],[\"var\",{\"1\":{\"874\":1,\"875\":1,\"876\":1,\"1598\":2,\"1656\":1,\"2334\":7}}],[\"vary\",{\"1\":{\"162\":1}}],[\"varied\",{\"1\":{\"1647\":1}}],[\"variacne\",{\"1\":{\"2433\":1}}],[\"variation\",{\"1\":{\"2000\":1,\"2001\":1,\"2176\":1}}],[\"variational\",{\"1\":{\"1546\":1,\"1553\":1,\"1611\":1,\"1612\":1,\"1616\":1,\"1622\":1,\"1625\":1,\"1626\":1}}],[\"variably\",{\"0\":{\"1474\":1},\"1\":{\"1474\":1}}],[\"variables\",{\"1\":{\"24\":1,\"37\":1,\"59\":1,\"109\":1,\"166\":2,\"255\":1,\"286\":1,\"755\":1,\"785\":1}}],[\"variable\",{\"0\":{\"2399\":1},\"1\":{\"22\":1,\"81\":2,\"82\":1,\"95\":2,\"96\":1,\"126\":1,\"162\":1,\"206\":1,\"212\":1,\"218\":1,\"223\":2,\"243\":1,\"267\":1,\"276\":1,\"286\":1,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"368\":2,\"385\":2,\"389\":2,\"396\":2,\"406\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":2,\"463\":2,\"469\":2,\"475\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":2,\"505\":2,\"703\":6,\"755\":1,\"785\":1,\"878\":4,\"879\":4,\"881\":2,\"882\":4,\"883\":4,\"884\":2,\"993\":1,\"1155\":1,\"1157\":1,\"1665\":1,\"1803\":2,\"2155\":1,\"2249\":2,\"2334\":1,\"2399\":1}}],[\"variancepredictor\",{\"0\":{\"2433\":1},\"1\":{\"2433\":1}}],[\"varianceloss\",{\"0\":{\"1627\":1},\"1\":{\"1627\":1}}],[\"variancenorm2d\",{\"0\":{\"1288\":1},\"1\":{\"1288\":1}}],[\"variance=false\",{\"1\":{\"1210\":1}}],[\"variance\",{\"0\":{\"1373\":1,\"2433\":1},\"1\":{\"211\":1,\"217\":1,\"243\":2,\"266\":1,\"275\":1,\"285\":1,\"1155\":6,\"1157\":8,\"1210\":2,\"1224\":1,\"1225\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1319\":1,\"1321\":1,\"1322\":1,\"1334\":2,\"1373\":1,\"1598\":1,\"1627\":1,\"1656\":1,\"1700\":1,\"1784\":1,\"2433\":2}}],[\"variant\",{\"1\":{\"141\":2,\"652\":1,\"664\":1,\"733\":1,\"1737\":1,\"2167\":1,\"2176\":1}}],[\"variants\",{\"1\":{\"53\":1}}],[\"various\",{\"0\":{\"122\":1},\"1\":{\"44\":1,\"45\":1,\"70\":1,\"128\":1,\"140\":1,\"145\":1,\"178\":1,\"179\":1,\"181\":1,\"240\":1,\"243\":1,\"245\":1,\"246\":1,\"290\":2,\"1155\":1,\"1157\":1,\"2045\":1,\"2286\":2}}],[\"val\",{\"1\":{\"141\":2,\"633\":2,\"637\":2,\"638\":2,\"646\":2,\"647\":2,\"664\":4,\"961\":2,\"1128\":1,\"1308\":2,\"1480\":1,\"2013\":1,\"2348\":1,\"2355\":12,\"2359\":3,\"2370\":2,\"2372\":1}}],[\"validated\",{\"1\":{\"911\":1,\"2156\":1}}],[\"validate\",{\"0\":{\"671\":1,\"672\":1,\"673\":1,\"2165\":1},\"1\":{\"150\":1,\"162\":1,\"196\":4,\"213\":4,\"268\":4,\"277\":4,\"284\":1,\"290\":6,\"671\":2,\"672\":2,\"673\":2,\"710\":1,\"711\":1,\"1063\":2,\"2156\":1,\"2347\":2,\"2355\":2,\"2365\":1,\"2369\":1,\"2371\":2}}],[\"validation\",{\"0\":{\"671\":1,\"672\":1,\"673\":1},\"1\":{\"39\":5,\"46\":1,\"78\":1,\"91\":2,\"94\":2,\"138\":1,\"139\":3,\"150\":1,\"175\":1,\"197\":6,\"200\":1,\"205\":1,\"211\":3,\"217\":2,\"218\":4,\"242\":1,\"254\":1,\"266\":3,\"267\":4,\"275\":3,\"276\":4,\"285\":2,\"286\":6,\"625\":4,\"671\":1,\"672\":1,\"673\":1,\"819\":1,\"960\":1,\"961\":6,\"2134\":1,\"2355\":14,\"2365\":1,\"2462\":1}}],[\"valid2\",{\"1\":{\"97\":1,\"98\":2,\"99\":1,\"100\":1}}],[\"valid\",{\"0\":{\"1936\":1,\"2071\":1},\"1\":{\"79\":2,\"94\":1,\"96\":4,\"97\":5,\"98\":12,\"99\":7,\"100\":7,\"101\":4,\"136\":4,\"175\":1,\"195\":2,\"197\":1,\"201\":2,\"205\":2,\"243\":8,\"267\":4,\"276\":4,\"377\":2,\"449\":7,\"671\":1,\"702\":2,\"846\":1,\"1678\":2,\"1936\":5,\"1937\":1,\"1949\":1,\"1951\":1,\"2065\":3,\"2139\":3,\"2144\":3,\"2183\":1,\"2190\":1,\"2208\":1,\"2239\":1,\"2281\":1,\"2338\":2,\"2369\":1}}],[\"valued=false\",{\"1\":{\"1084\":1,\"1149\":1,\"1165\":1}}],[\"valued\",{\"1\":{\"1051\":2,\"1084\":2,\"1314\":1,\"1352\":1,\"1376\":1,\"1377\":1}}],[\"valueerror\",{\"1\":{\"269\":1,\"278\":1,\"2134\":1,\"2145\":1,\"2155\":1}}],[\"value=\",{\"1\":{\"82\":1}}],[\"value=0\",{\"1\":{\"82\":1,\"1529\":1,\"1788\":1}}],[\"values\",{\"1\":{\"41\":1,\"44\":1,\"45\":1,\"78\":1,\"79\":1,\"104\":1,\"128\":1,\"141\":1,\"144\":1,\"145\":1,\"175\":1,\"211\":1,\"217\":1,\"266\":1,\"267\":1,\"275\":1,\"276\":1,\"285\":1,\"286\":3,\"538\":1,\"635\":1,\"664\":1,\"710\":1,\"711\":1,\"768\":1,\"787\":1,\"821\":1,\"831\":2,\"846\":1,\"1514\":1,\"1654\":1,\"1655\":1,\"1666\":1,\"1668\":2,\"1678\":2,\"1679\":1,\"1719\":5,\"1720\":2,\"1725\":6,\"1778\":1,\"1806\":2,\"1824\":3,\"1946\":1,\"2000\":1,\"2001\":1,\"2101\":1,\"2130\":1,\"2136\":2,\"2139\":1,\"2144\":1,\"2176\":1,\"2325\":1,\"2359\":3,\"2374\":1,\"2487\":1}}],[\"value\",{\"0\":{\"86\":1,\"2398\":1},\"1\":{\"40\":1,\"45\":3,\"46\":1,\"67\":1,\"78\":3,\"80\":1,\"81\":1,\"82\":5,\"86\":2,\"117\":1,\"128\":1,\"141\":23,\"142\":1,\"145\":3,\"175\":3,\"290\":1,\"538\":1,\"615\":1,\"616\":2,\"625\":2,\"629\":2,\"633\":2,\"634\":1,\"635\":1,\"637\":1,\"638\":1,\"640\":2,\"642\":1,\"644\":11,\"646\":1,\"647\":1,\"648\":1,\"649\":4,\"650\":4,\"654\":6,\"661\":3,\"664\":9,\"666\":2,\"669\":1,\"692\":2,\"696\":1,\"697\":1,\"703\":2,\"738\":1,\"755\":2,\"756\":1,\"764\":1,\"773\":1,\"784\":11,\"785\":2,\"786\":3,\"787\":1,\"790\":1,\"800\":2,\"801\":2,\"802\":1,\"850\":1,\"866\":2,\"867\":1,\"881\":2,\"884\":2,\"910\":1,\"911\":1,\"921\":1,\"922\":2,\"936\":2,\"937\":2,\"961\":2,\"962\":1,\"994\":1,\"1000\":1,\"1029\":1,\"1064\":1,\"1235\":1,\"1246\":3,\"1247\":1,\"1262\":1,\"1269\":1,\"1270\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1290\":1,\"1316\":1,\"1327\":1,\"1330\":1,\"1356\":3,\"1400\":1,\"1419\":2,\"1469\":1,\"1493\":1,\"1494\":1,\"1526\":9,\"1529\":2,\"1553\":9,\"1581\":1,\"1584\":2,\"1587\":1,\"1589\":1,\"1591\":1,\"1599\":2,\"1607\":2,\"1616\":1,\"1625\":2,\"1627\":3,\"1655\":2,\"1683\":1,\"1725\":2,\"1753\":1,\"1754\":3,\"1756\":3,\"1757\":3,\"1758\":1,\"1764\":2,\"1770\":1,\"1771\":1,\"1782\":1,\"1785\":3,\"1788\":3,\"1789\":3,\"1790\":3,\"1794\":12,\"1806\":1,\"1817\":3,\"1839\":3,\"1859\":2,\"1898\":2,\"1905\":1,\"1908\":3,\"1920\":2,\"1928\":1,\"1944\":1,\"1947\":1,\"1952\":1,\"1962\":2,\"1984\":3,\"1991\":3,\"1992\":1,\"2016\":1,\"2143\":2,\"2155\":3,\"2187\":1,\"2192\":1,\"2203\":1,\"2226\":3,\"2231\":1,\"2235\":11,\"2236\":11,\"2237\":2,\"2239\":13,\"2240\":13,\"2241\":4,\"2245\":11,\"2309\":2,\"2327\":1,\"2332\":2,\"2335\":2,\"2336\":1,\"2350\":2,\"2355\":4,\"2359\":2,\"2373\":2,\"2376\":2,\"2398\":1,\"2407\":1,\"2411\":4,\"2412\":5,\"2413\":4,\"2423\":4,\"2424\":4,\"2427\":3,\"2428\":1,\"2431\":2,\"2432\":5,\"2447\":4,\"2448\":4,\"2484\":1,\"2491\":1,\"2492\":1,\"2496\":1,\"2497\":1,\"2498\":1,\"2499\":1,\"2501\":1,\"2503\":1,\"2504\":1}}],[\"vaibhav\",{\"1\":{\"5\":1}}],[\"v70\",{\"1\":{\"1126\":1}}],[\"v7\",{\"1\":{\"22\":1,\"106\":2}}],[\"vec\",{\"1\":{\"1308\":4}}],[\"vectorized\",{\"1\":{\"1722\":1,\"1730\":1}}],[\"vectorizing\",{\"1\":{\"1270\":1}}],[\"vectorquantization\",{\"0\":{\"1469\":1},\"1\":{\"1469\":1}}],[\"vector\",{\"0\":{\"1291\":1,\"1318\":1,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1330\":1},\"1\":{\"252\":1,\"276\":1,\"286\":2,\"289\":4,\"620\":1,\"644\":1,\"649\":2,\"654\":4,\"755\":4,\"784\":1,\"785\":4,\"820\":1,\"878\":2,\"879\":2,\"881\":3,\"882\":3,\"883\":3,\"884\":4,\"922\":3,\"936\":3,\"937\":3,\"1117\":1,\"1127\":1,\"1130\":1,\"1131\":1,\"1170\":1,\"1199\":1,\"1291\":2,\"1293\":2,\"1309\":4,\"1310\":4,\"1311\":4,\"1314\":1,\"1315\":2,\"1317\":3,\"1318\":5,\"1319\":5,\"1321\":5,\"1322\":5,\"1323\":5,\"1327\":5,\"1328\":3,\"1330\":5,\"1332\":3,\"1400\":2,\"1439\":1,\"1441\":4,\"1469\":2,\"1735\":8,\"1736\":1,\"1794\":1,\"1803\":2,\"1817\":1,\"2209\":2,\"2412\":1,\"2423\":1,\"2447\":1}}],[\"vectors>\",{\"1\":{\"1489\":1}}],[\"vectors\",{\"0\":{\"1498\":1,\"1499\":1},\"1\":{\"130\":1,\"710\":1,\"711\":1,\"1131\":1,\"1172\":1,\"1332\":1,\"1498\":1,\"1499\":1,\"1683\":3,\"2133\":1,\"2208\":1,\"2209\":1}}],[\"ve\",{\"1\":{\"286\":1}}],[\"verify\",{\"0\":{\"1937\":1},\"1\":{\"1937\":2,\"2132\":2}}],[\"verification\",{\"0\":{\"208\":1},\"1\":{\"92\":1,\"252\":1,\"2176\":1,\"2183\":1,\"2184\":1,\"2187\":1,\"2191\":1,\"2203\":1}}],[\"very\",{\"1\":{\"232\":1,\"246\":1,\"258\":1,\"262\":1,\"1833\":1,\"2133\":1}}],[\"versa\",{\"1\":{\"217\":2,\"284\":1,\"285\":4}}],[\"version=none\",{\"1\":{\"2480\":1}}],[\"version=11\",{\"1\":{\"162\":1}}],[\"version=1\",{\"1\":{\"162\":2}}],[\"version|default=none\",{\"1\":{\"162\":1}}],[\"versions\",{\"1\":{\"138\":1,\"249\":1}}],[\"version\",{\"0\":{\"2317\":2},\"1\":{\"22\":2,\"26\":3,\"104\":1,\"138\":4,\"139\":1,\"145\":2,\"146\":1,\"162\":5,\"179\":1,\"190\":2,\"260\":1,\"527\":2,\"616\":1,\"821\":1,\"823\":1,\"887\":1,\"1002\":1,\"1711\":1,\"1712\":1,\"1785\":1,\"1786\":1,\"2016\":1,\"2317\":2}}],[\"verbose=false\",{\"1\":{\"817\":1,\"821\":1,\"824\":1,\"2020\":1}}],[\"verbose\",{\"1\":{\"39\":3,\"84\":1,\"96\":1,\"521\":1,\"538\":2,\"548\":2,\"551\":2,\"561\":2,\"567\":2,\"606\":2}}],[\"venv\",{\"1\":{\"1\":1,\"154\":1,\"162\":2,\"2311\":1}}],[\"viterbi\",{\"0\":{\"1637\":1},\"1\":{\"706\":1,\"1637\":1}}],[\"vitsgenerator\",{\"0\":{\"1626\":1},\"1\":{\"1626\":1}}],[\"vits\",{\"0\":{\"1519\":1,\"1520\":1,\"1529\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1546\":1,\"1552\":1,\"1553\":3,\"1574\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1616\":1,\"1622\":1,\"1623\":1,\"1625\":3,\"1626\":1,\"1630\":1,\"1634\":1,\"1635\":1,\"1636\":1},\"1\":{\"187\":1,\"267\":2,\"284\":1,\"285\":1,\"286\":20,\"287\":3,\"289\":1,\"290\":1,\"481\":1,\"1519\":1,\"1520\":1,\"1529\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1546\":1,\"1552\":2,\"1553\":7,\"1574\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1611\":2,\"1612\":1,\"1613\":1,\"1616\":1,\"1622\":2,\"1623\":1,\"1625\":8,\"1626\":4,\"1634\":1,\"1635\":1,\"1636\":1,\"2045\":2}}],[\"vincent\",{\"1\":{\"691\":1}}],[\"vietnamese\",{\"1\":{\"481\":1}}],[\"view\",{\"1\":{\"247\":1,\"992\":1,\"994\":1,\"997\":1,\"1000\":1,\"1002\":1,\"1004\":1,\"1008\":1,\"1012\":1,\"1016\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2348\":1,\"2370\":2,\"2372\":1}}],[\"viewer\",{\"1\":{\"114\":1}}],[\"viewpoint\",{\"1\":{\"96\":2}}],[\"világ\",{\"1\":{\"287\":1}}],[\"vim\",{\"1\":{\"206\":2,\"212\":2,\"218\":2,\"236\":3,\"255\":2,\"267\":2,\"276\":2,\"286\":3}}],[\"vi\",{\"1\":{\"197\":1}}],[\"via\",{\"1\":{\"135\":1,\"162\":1,\"200\":2,\"202\":1,\"205\":2,\"211\":4,\"212\":1,\"217\":4,\"218\":2,\"235\":1,\"242\":2,\"254\":1,\"262\":1,\"266\":6,\"267\":2,\"269\":1,\"270\":1,\"271\":1,\"275\":6,\"276\":2,\"278\":1,\"279\":1,\"280\":1,\"285\":4,\"286\":4,\"287\":1,\"288\":1,\"704\":1,\"803\":1,\"1066\":1,\"1224\":1,\"1225\":1,\"1245\":1,\"1698\":1,\"2276\":1,\"2277\":1}}],[\"video\",{\"0\":{\"74\":1},\"1\":{\"675\":4,\"746\":1}}],[\"virbr\",{\"1\":{\"67\":2}}],[\"virtual\",{\"1\":{\"1\":1,\"67\":3}}],[\"vivos\",{\"1\":{\"50\":2}}],[\"vision\",{\"1\":{\"2130\":1}}],[\"visibility\",{\"1\":{\"2044\":3}}],[\"visible\",{\"1\":{\"41\":2,\"93\":2,\"195\":1}}],[\"visit\",{\"1\":{\"247\":1}}],[\"visingergenerator\",{\"0\":{\"1552\":1},\"1\":{\"1552\":1}}],[\"visinger2vocodergenerator\",{\"0\":{\"1551\":1},\"1\":{\"1551\":1}}],[\"visinger2discriminator\",{\"0\":{\"1549\":1},\"1\":{\"1549\":1}}],[\"visinger2\",{\"0\":{\"1514\":2,\"1517\":2,\"1524\":2,\"1525\":2,\"1527\":2,\"1533\":2,\"1534\":2,\"1547\":2,\"1549\":2,\"1551\":2,\"1557\":1,\"1558\":2,\"1560\":1,\"1561\":1,\"1562\":1,\"1564\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1575\":1},\"1\":{\"267\":3,\"1514\":2,\"1517\":2,\"1524\":2,\"1525\":2,\"1527\":2,\"1533\":2,\"1534\":2,\"1539\":1,\"1547\":2,\"1549\":3,\"1551\":2,\"1552\":1,\"1557\":1,\"1558\":2,\"1560\":1,\"1561\":1,\"1562\":1,\"1564\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1575\":1}}],[\"visinger\",{\"1\":{\"188\":1,\"265\":2,\"267\":9,\"272\":2,\"1519\":2,\"1535\":1,\"1546\":1,\"1552\":2,\"1553\":1}}],[\"visualize\",{\"1\":{\"768\":1}}],[\"visualizes\",{\"1\":{\"235\":1}}],[\"visualizations\",{\"1\":{\"2354\":1}}],[\"visualization\",{\"1\":{\"233\":1,\"234\":1,\"235\":1}}],[\"visualstudio\",{\"1\":{\"18\":1}}],[\"visual\",{\"1\":{\"18\":2}}],[\"vuv\",{\"1\":{\"267\":3,\"276\":3,\"1526\":1,\"2239\":3,\"2240\":3,\"2241\":3}}],[\"vu\",{\"1\":{\"12\":1}}],[\"vuepress\",{\"1\":{\"2\":1,\"3\":5}}],[\"v202\",{\"1\":{\"202\":1}}],[\"v2\",{\"0\":{\"894\":1,\"1310\":1},\"1\":{\"10\":1,\"22\":1,\"175\":1,\"185\":2,\"201\":1,\"243\":8,\"260\":1,\"261\":2,\"527\":3,\"536\":7,\"894\":1,\"1119\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1310\":2,\"1346\":1}}],[\"v3\",{\"1\":{\"6\":3,\"243\":1,\"536\":9,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"2055\":2,\"2056\":2}}],[\"eq\",{\"1\":{\"1731\":2,\"1880\":1,\"2435\":3}}],[\"equation\",{\"1\":{\"1307\":1,\"1357\":1,\"2436\":1,\"2438\":1,\"2440\":1}}],[\"equals\",{\"1\":{\"2019\":1,\"2020\":1,\"2021\":1,\"2249\":1,\"2253\":1,\"2384\":1,\"2385\":1}}],[\"equalization\",{\"0\":{\"1687\":1},\"1\":{\"1655\":1,\"1687\":2}}],[\"equal\",{\"1\":{\"78\":1,\"98\":1,\"99\":1,\"100\":1,\"254\":2,\"616\":1,\"1020\":1,\"1025\":1,\"1350\":1,\"1450\":1,\"1452\":1,\"1618\":1,\"1645\":1,\"1680\":1,\"1692\":1,\"1698\":1,\"1920\":1,\"2136\":1}}],[\"equiped\",{\"1\":{\"633\":1}}],[\"equivalently\",{\"1\":{\"756\":1,\"773\":1}}],[\"equivalent\",{\"1\":{\"3\":1,\"60\":1,\"85\":2,\"167\":1,\"243\":1,\"705\":2,\"756\":1,\"773\":1,\"866\":1,\"867\":1,\"924\":1,\"980\":1,\"1316\":1}}],[\"eunits=512\",{\"1\":{\"1758\":1,\"2231\":1}}],[\"eunits\",{\"1\":{\"1526\":1,\"1598\":1,\"1599\":2,\"1600\":1,\"1709\":3,\"1758\":3,\"2231\":3,\"2235\":2,\"2236\":2,\"2239\":2,\"2240\":2,\"2245\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":2,\"2432\":2,\"2447\":2}}],[\"euclidean\",{\"1\":{\"1400\":1,\"1469\":1}}],[\"euclideancodebook\",{\"0\":{\"1400\":1},\"1\":{\"1400\":1}}],[\"euler\",{\"1\":{\"1245\":1,\"1253\":1}}],[\"eulermaruyamapredictor\",{\"0\":{\"1161\":1},\"1\":{\"1161\":1}}],[\"euro\",{\"1\":{\"14\":1}}],[\"eer\",{\"0\":{\"2475\":1,\"2476\":1,\"2506\":1},\"1\":{\"2365\":1,\"2475\":1,\"2476\":1,\"2506\":1}}],[\"eers\",{\"1\":{\"254\":1}}],[\"eend\",{\"1\":{\"974\":6,\"978\":1}}],[\"ey\",{\"1\":{\"797\":1}}],[\"eye\",{\"1\":{\"114\":1}}],[\"eː\",{\"1\":{\"287\":1}}],[\"eigenvectors\",{\"1\":{\"1308\":2}}],[\"eigenvalues\",{\"1\":{\"1308\":2}}],[\"eigenvalue\",{\"0\":{\"1308\":1},\"1\":{\"1308\":2,\"1318\":3,\"1328\":1}}],[\"einsum\",{\"0\":{\"1307\":1},\"1\":{\"1307\":1}}],[\"ei2\",{\"1\":{\"287\":1}}],[\"either\",{\"1\":{\"1\":1,\"43\":1,\"139\":1,\"141\":2,\"196\":1,\"213\":1,\"232\":1,\"259\":1,\"262\":1,\"267\":1,\"268\":1,\"269\":1,\"277\":1,\"278\":1,\"755\":1,\"756\":1,\"768\":1,\"773\":1,\"785\":1,\"1559\":1,\"1655\":1,\"1758\":2,\"1863\":1,\"1867\":1,\"1891\":1,\"1913\":1,\"1937\":1,\"2149\":1,\"2198\":1,\"2231\":2}}],[\"erbs\",{\"1\":{\"1316\":2}}],[\"erbs=64\",{\"1\":{\"1316\":1}}],[\"erb\",{\"0\":{\"1316\":1},\"1\":{\"1316\":4}}],[\"er3\",{\"1\":{\"287\":2}}],[\"er1\",{\"1\":{\"271\":2,\"280\":2,\"287\":4}}],[\"errorcalculatortransducer\",{\"0\":{\"740\":1},\"1\":{\"627\":1,\"740\":2}}],[\"errorcalculator\",{\"0\":{\"627\":1,\"1760\":1},\"1\":{\"627\":1,\"1760\":2}}],[\"error\",{\"0\":{\"564\":1,\"627\":1,\"740\":1},\"1\":{\"66\":1,\"81\":1,\"126\":2,\"163\":1,\"195\":1,\"197\":1,\"200\":1,\"205\":1,\"242\":1,\"243\":2,\"246\":1,\"254\":1,\"267\":5,\"269\":2,\"276\":5,\"278\":2,\"285\":1,\"286\":2,\"290\":1,\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"377\":1,\"385\":1,\"389\":1,\"396\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":1,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"511\":1,\"564\":2,\"625\":2,\"627\":1,\"653\":1,\"663\":1,\"699\":1,\"740\":1,\"974\":1,\"1485\":1,\"1754\":1,\"1839\":1,\"1842\":2,\"1991\":1,\"2237\":1,\"2439\":1}}],[\"errors\",{\"1\":{\"3\":1,\"56\":1,\"175\":2,\"213\":1,\"269\":4,\"278\":4,\"284\":1,\"290\":1}}],[\"eg\",{\"1\":{\"267\":1,\"269\":3,\"276\":1,\"278\":3}}],[\"egs2\",{\"1\":{\"23\":2,\"69\":1,\"71\":1,\"107\":5,\"108\":5,\"109\":1,\"110\":1,\"136\":2,\"161\":1,\"195\":6,\"196\":1,\"197\":4,\"201\":1,\"206\":2,\"212\":3,\"213\":1,\"218\":2,\"220\":1,\"223\":4,\"224\":6,\"227\":1,\"228\":1,\"236\":1,\"242\":1,\"243\":7,\"255\":2,\"267\":3,\"268\":1,\"269\":2,\"276\":3,\"277\":1,\"278\":2,\"285\":1,\"286\":12,\"289\":4,\"290\":2,\"1155\":1,\"1157\":1,\"2344\":1}}],[\"egs\",{\"1\":{\"22\":2,\"23\":1,\"24\":5,\"25\":1,\"37\":1,\"38\":2,\"50\":1,\"107\":1,\"161\":1,\"197\":1,\"268\":2,\"272\":1,\"277\":2,\"282\":1,\"290\":1}}],[\"e12\",{\"1\":{\"243\":1}}],[\"ebidirectional\",{\"1\":{\"2235\":2,\"2236\":2}}],[\"ebf\",{\"1\":{\"243\":1,\"2056\":2}}],[\"ebranchformerencoderlayer\",{\"0\":{\"735\":1},\"1\":{\"735\":1}}],[\"ebranchformerencoder\",{\"0\":{\"734\":1},\"1\":{\"734\":1}}],[\"ebranchformerctcencoder\",{\"0\":{\"733\":1},\"1\":{\"733\":1}}],[\"ebranchformer\",{\"0\":{\"624\":2,\"659\":1},\"1\":{\"141\":1,\"243\":6,\"624\":2,\"659\":2}}],[\"eot\",{\"1\":{\"2143\":1}}],[\"eos>\",{\"1\":{\"736\":2,\"1942\":1,\"2221\":2}}],[\"eos\",{\"0\":{\"1858\":2},\"1\":{\"242\":5,\"625\":1,\"736\":1,\"795\":1,\"1719\":2,\"1721\":2,\"1725\":2,\"1729\":1,\"1730\":2,\"1731\":2,\"1806\":1,\"1807\":3,\"1822\":1,\"1848\":1,\"1858\":4,\"1862\":2,\"1907\":2,\"1942\":1,\"1966\":1,\"1996\":1,\"1997\":1,\"2143\":1,\"2221\":1,\"2462\":1}}],[\"eog\",{\"1\":{\"114\":3}}],[\"eccv\",{\"1\":{\"2176\":2}}],[\"ecva\",{\"1\":{\"2176\":1}}],[\"econv\",{\"1\":{\"1758\":6,\"2231\":6,\"2245\":6,\"2431\":6}}],[\"ecapatdnnencoder\",{\"0\":{\"2187\":1},\"1\":{\"2187\":1}}],[\"ecapablock\",{\"0\":{\"2185\":1},\"1\":{\"2185\":1,\"2187\":1}}],[\"ecapa\",{\"0\":{\"2185\":1,\"2187\":1,\"2200\":1},\"1\":{\"236\":1,\"2183\":1,\"2185\":1,\"2187\":4,\"2200\":1}}],[\"echo\",{\"1\":{\"40\":2,\"117\":2,\"167\":5,\"197\":1,\"536\":3}}],[\"efficiently\",{\"1\":{\"1729\":1,\"1730\":1,\"1770\":1,\"2130\":1}}],[\"efficient\",{\"1\":{\"207\":1,\"701\":1,\"705\":1,\"735\":1,\"804\":1,\"932\":1,\"934\":1,\"1029\":1,\"1061\":1,\"1062\":1,\"1176\":1,\"1235\":1,\"1279\":1,\"1280\":2,\"1281\":2,\"1282\":1,\"1283\":1,\"1301\":1,\"1310\":1,\"1372\":1,\"2130\":1,\"2131\":1,\"2143\":1}}],[\"effectiveness\",{\"1\":{\"175\":1,\"207\":1}}],[\"effective\",{\"1\":{\"102\":1,\"1061\":2,\"1063\":2,\"1674\":1,\"2136\":1}}],[\"effect\",{\"1\":{\"85\":1,\"269\":1,\"278\":1,\"1655\":4,\"1679\":1,\"1712\":1}}],[\"effects\",{\"1\":{\"6\":1,\"1655\":8,\"1678\":1,\"2336\":1,\"2337\":1,\"2346\":1,\"2356\":1,\"2362\":1,\"2368\":1}}],[\"el\",{\"1\":{\"2187\":1}}],[\"eladhoffer\",{\"1\":{\"1855\":1}}],[\"elayers=1\",{\"1\":{\"1758\":1,\"2231\":1}}],[\"elayers\",{\"1\":{\"1526\":1,\"1598\":1,\"1599\":2,\"1600\":1,\"1758\":1,\"1814\":2,\"1816\":2,\"2231\":1,\"2235\":2,\"2236\":2,\"2239\":2,\"2240\":2,\"2245\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":2,\"2432\":2,\"2447\":2}}],[\"elapsed\",{\"1\":{\"39\":1}}],[\"elimination\",{\"1\":{\"1246\":1}}],[\"elu\",{\"1\":{\"1108\":1,\"1145\":2,\"1168\":1,\"1264\":2,\"1265\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":2,\"1396\":1,\"1401\":1,\"1403\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":2,\"1458\":1,\"1460\":1,\"1466\":1,\"1468\":1,\"2311\":4}}],[\"elemwise=false\",{\"1\":{\"757\":1}}],[\"elementwiseaffineflow\",{\"0\":{\"1586\":1},\"1\":{\"1586\":2}}],[\"elementwise\",{\"1\":{\"722\":1,\"854\":1,\"1586\":1,\"1661\":1}}],[\"element\",{\"1\":{\"79\":1,\"912\":1,\"1354\":2,\"1655\":1}}],[\"elements\",{\"0\":{\"2004\":1},\"1\":{\"43\":2,\"100\":1,\"141\":1,\"647\":2,\"787\":1,\"1655\":1,\"1927\":1,\"2004\":1,\"2220\":2}}],[\"else\",{\"1\":{\"81\":1,\"242\":1,\"266\":1,\"275\":1,\"804\":1,\"819\":1,\"932\":1,\"934\":1,\"974\":1,\"1118\":1,\"1356\":1,\"1628\":2,\"2015\":2,\"2020\":2,\"2021\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2327\":1,\"2355\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"eta\",{\"1\":{\"2307\":2}}],[\"eth\",{\"1\":{\"67\":2}}],[\"eth0\",{\"1\":{\"67\":1}}],[\"ethernets\",{\"1\":{\"67\":1}}],[\"ethernet\",{\"0\":{\"66\":1},\"1\":{\"67\":3}}],[\"et\",{\"1\":{\"45\":4,\"46\":1,\"139\":2,\"145\":4,\"146\":1,\"711\":1,\"768\":1,\"833\":1,\"995\":1,\"1066\":1,\"1117\":1,\"1125\":1,\"1126\":1,\"1130\":1,\"1131\":2,\"1149\":1,\"1170\":1,\"1172\":2,\"1176\":1,\"1185\":1,\"1252\":1,\"1321\":1,\"1322\":1,\"1327\":1,\"1330\":2,\"1668\":1,\"1720\":1,\"1721\":1,\"1729\":1,\"1730\":2,\"1880\":1,\"2191\":1,\"2192\":1,\"2198\":1,\"2203\":1,\"2209\":1}}],[\"etype\",{\"1\":{\"43\":5}}],[\"etc\",{\"0\":{\"152\":1,\"153\":1},\"1\":{\"26\":1,\"31\":1,\"70\":3,\"78\":1,\"79\":1,\"108\":1,\"109\":1,\"124\":1,\"126\":1,\"152\":1,\"161\":1,\"162\":1,\"163\":1,\"168\":1,\"174\":1,\"197\":1,\"200\":1,\"205\":1,\"290\":1,\"699\":1,\"1117\":1,\"1130\":1,\"1131\":1,\"1232\":1,\"1824\":1,\"2130\":2,\"2133\":2,\"2215\":1,\"2262\":1}}],[\"e2e\",{\"0\":{\"1130\":1,\"1760\":1,\"1764\":1,\"1770\":1,\"1771\":1,\"1839\":1,\"1880\":1,\"1893\":1,\"2028\":1,\"2054\":1},\"1\":{\"42\":2,\"46\":1,\"223\":1,\"245\":1,\"247\":3,\"526\":2,\"527\":1,\"1130\":1,\"1760\":1,\"1764\":1,\"1770\":1,\"1771\":1,\"1839\":1,\"1880\":1,\"1893\":1,\"2028\":1,\"2044\":6,\"2054\":2,\"2198\":1}}],[\"evd\",{\"1\":{\"1318\":2,\"1328\":2}}],[\"eval1\",{\"1\":{\"218\":3,\"267\":8,\"276\":8,\"286\":34}}],[\"eval92\",{\"1\":{\"126\":1}}],[\"evaluator\",{\"0\":{\"1761\":1},\"1\":{\"1761\":2}}],[\"evaluated\",{\"1\":{\"223\":1}}],[\"evaluate\",{\"1\":{\"124\":1,\"126\":2,\"197\":1,\"246\":1,\"247\":1,\"267\":6,\"276\":6,\"286\":21,\"564\":1}}],[\"evaluations\",{\"1\":{\"266\":1,\"275\":1}}],[\"evaluation\",{\"0\":{\"124\":1,\"125\":1,\"126\":1},\"1\":{\"15\":1,\"107\":1,\"111\":1,\"118\":2,\"196\":1,\"200\":2,\"205\":2,\"210\":1,\"211\":2,\"212\":1,\"213\":1,\"217\":2,\"218\":3,\"232\":1,\"235\":2,\"242\":1,\"245\":1,\"246\":1,\"247\":3,\"254\":3,\"258\":1,\"265\":2,\"266\":4,\"267\":8,\"268\":1,\"274\":2,\"275\":4,\"276\":8,\"277\":1,\"284\":1,\"285\":2,\"286\":10,\"1209\":1,\"1485\":1,\"1645\":1,\"1650\":1,\"1812\":1,\"2364\":1}}],[\"eval\",{\"0\":{\"520\":1,\"564\":1},\"1\":{\"118\":1,\"211\":4,\"213\":1,\"223\":3,\"267\":5,\"268\":1,\"276\":6,\"277\":1,\"284\":1,\"285\":2,\"449\":2,\"520\":3,\"564\":1,\"1133\":1,\"1247\":1,\"1734\":1,\"2338\":1,\"2347\":1,\"2355\":1,\"2359\":2,\"2364\":2,\"2369\":1,\"2371\":1}}],[\"even\",{\"1\":{\"223\":1,\"242\":1,\"821\":1,\"1008\":1,\"1124\":2,\"1125\":1,\"1484\":1,\"1812\":1}}],[\"events\",{\"1\":{\"39\":1}}],[\"everyday\",{\"1\":{\"1717\":1}}],[\"everything\",{\"1\":{\"232\":1,\"258\":1}}],[\"every\",{\"1\":{\"87\":1,\"91\":1,\"102\":1,\"142\":2,\"643\":3,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"693\":1,\"713\":1,\"718\":1,\"720\":1,\"722\":1,\"738\":1,\"741\":1,\"745\":1,\"752\":1,\"757\":1,\"778\":1,\"781\":1,\"788\":1,\"791\":1,\"796\":1,\"798\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"820\":2,\"821\":1,\"825\":1,\"828\":2,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"842\":1,\"844\":1,\"852\":1,\"854\":1,\"856\":1,\"860\":1,\"862\":1,\"864\":1,\"950\":1,\"952\":1,\"956\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1066\":1,\"1068\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1114\":1,\"1120\":1,\"1122\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1145\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1159\":1,\"1165\":1,\"1168\":1,\"1177\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1200\":1,\"1202\":1,\"1205\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1219\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1248\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1262\":1,\"1265\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1383\":1,\"1387\":1,\"1392\":1,\"1398\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1509\":1,\"1511\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1549\":1,\"1554\":1,\"1638\":1,\"1652\":1,\"1657\":1,\"1662\":1,\"1797\":1,\"1819\":1,\"1823\":1,\"1834\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1945\":1,\"1957\":1,\"1967\":1,\"1969\":1,\"1972\":1,\"1975\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2185\":1,\"2188\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2203\":1,\"2205\":1,\"2209\":1,\"2211\":1,\"2216\":1,\"2249\":1,\"2253\":1,\"2305\":1,\"2325\":1,\"2354\":2,\"2355\":3,\"2401\":1,\"2405\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2434\":1,\"2443\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2465\":1,\"2467\":1}}],[\"est\",{\"1\":{\"1118\":1,\"1142\":1,\"1204\":1,\"1246\":2,\"1247\":2,\"1273\":1,\"1274\":1,\"1334\":1}}],[\"estimation\",{\"1\":{\"980\":1,\"1107\":1,\"1117\":1,\"1130\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1232\":1,\"1261\":1,\"1267\":1,\"1268\":2,\"1278\":1,\"1319\":1,\"2404\":1}}],[\"estimating\",{\"1\":{\"225\":1}}],[\"estimates\",{\"1\":{\"980\":1}}],[\"estimate\",{\"0\":{\"2076\":1},\"1\":{\"267\":1,\"276\":1,\"286\":1,\"1118\":1,\"1210\":2,\"1334\":5,\"1612\":1,\"1613\":1}}],[\"estimated\",{\"1\":{\"39\":1,\"113\":1,\"978\":1,\"1053\":1,\"1107\":1,\"1118\":2,\"1125\":1,\"1130\":1,\"1136\":1,\"1141\":1,\"1162\":1,\"1232\":1,\"1246\":1,\"1247\":1,\"1267\":1,\"1278\":1}}],[\"estimator\",{\"0\":{\"1199\":1},\"1\":{\"223\":1,\"1199\":2}}],[\"esc50\",{\"1\":{\"201\":1}}],[\"espeak\",{\"1\":{\"285\":1,\"287\":39,\"481\":14}}],[\"especially\",{\"1\":{\"70\":1,\"71\":1,\"267\":1,\"269\":1,\"276\":1,\"278\":1,\"286\":1,\"290\":1,\"2130\":1}}],[\"espnetuasrmodel\",{\"0\":{\"2462\":1},\"1\":{\"2273\":2,\"2462\":1}}],[\"espnettts2model\",{\"0\":{\"2446\":1},\"1\":{\"2270\":2,\"2446\":1}}],[\"espnetttsmodel\",{\"0\":{\"2045\":1,\"2408\":1},\"1\":{\"2045\":1,\"2271\":2,\"2408\":2,\"2446\":1}}],[\"espnetqwen2audiomodel\",{\"0\":{\"1965\":1},\"1\":{\"1965\":1,\"2262\":1}}],[\"espnetlanguagemodel\",{\"0\":{\"1940\":1},\"1\":{\"1940\":1,\"2260\":1}}],[\"espnetlidmodel\",{\"0\":{\"1702\":1},\"1\":{\"1702\":1,\"2259\":1}}],[\"espnetganttsmodel\",{\"0\":{\"1585\":1},\"1\":{\"1585\":2,\"2256\":2}}],[\"espnetgansvsmodel\",{\"0\":{\"1521\":1},\"1\":{\"1521\":2,\"2255\":2}}],[\"espnetgancodecmodel\",{\"0\":{\"1395\":1},\"1\":{\"1395\":2,\"2254\":2}}],[\"espnetextractionmodel\",{\"0\":{\"1158\":1},\"1\":{\"1158\":1,\"2272\":1}}],[\"espnetenhs2tmodel\",{\"0\":{\"1156\":1},\"1\":{\"228\":2,\"1156\":1,\"2252\":1}}],[\"espnetenhancementmodel\",{\"0\":{\"1157\":1},\"1\":{\"223\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"2253\":1}}],[\"espnetclassificationmodel\",{\"0\":{\"958\":1},\"1\":{\"958\":1,\"2250\":1}}],[\"espnetasvspoofmodel\",{\"0\":{\"954\":1},\"1\":{\"954\":1,\"2248\":1}}],[\"espnetasrmodel\",{\"0\":{\"736\":1,\"2043\":1},\"1\":{\"138\":1,\"736\":2,\"777\":1,\"1156\":1,\"2043\":1,\"2127\":1,\"2246\":1}}],[\"espnetasrtransducermodel\",{\"0\":{\"625\":1},\"1\":{\"138\":1,\"625\":2,\"2247\":1}}],[\"espnetmultitaskdataset\",{\"0\":{\"2344\":1},\"1\":{\"2344\":1,\"2345\":1}}],[\"espnetmultitasklanguagemodel\",{\"0\":{\"1942\":1},\"1\":{\"1942\":1,\"2260\":1}}],[\"espnetmtmodel\",{\"0\":{\"1959\":1},\"1\":{\"737\":1,\"1959\":1,\"2261\":1}}],[\"espnetmodel\",{\"1\":{\"78\":1,\"2325\":1,\"2327\":1}}],[\"espnetdiscretesvsmodel\",{\"0\":{\"2228\":1},\"1\":{\"2228\":1}}],[\"espnetdiscreteasrmodel\",{\"0\":{\"737\":1},\"1\":{\"737\":1}}],[\"espnetdiffusionmodel\",{\"0\":{\"1155\":1},\"1\":{\"1155\":1}}],[\"espnetdiarizationmodel\",{\"0\":{\"974\":1},\"1\":{\"974\":1,\"1156\":1,\"2251\":1}}],[\"espnetdataset\",{\"0\":{\"2342\":1},\"1\":{\"82\":1,\"2342\":1,\"2343\":1,\"2345\":1,\"2377\":2}}],[\"espnetspeechlmdataset\",{\"0\":{\"2345\":1},\"1\":{\"2344\":2,\"2345\":1}}],[\"espnetspeakermodel\",{\"0\":{\"2184\":1},\"1\":{\"2184\":1,\"2269\":1}}],[\"espnetsvsmodel\",{\"0\":{\"2229\":1},\"1\":{\"2228\":2,\"2229\":2,\"2268\":2}}],[\"espnetsslmodel\",{\"0\":{\"2216\":1},\"1\":{\"2216\":1,\"2266\":1}}],[\"espnetsslfrontend\",{\"0\":{\"738\":1},\"1\":{\"738\":1}}],[\"espnetslumodel\",{\"0\":{\"2127\":1},\"1\":{\"2127\":1,\"2265\":1}}],[\"espnetsdsmodelinterface\",{\"0\":{\"2044\":1},\"1\":{\"2044\":1}}],[\"espnets2tmodel\",{\"0\":{\"1997\":1},\"1\":{\"1997\":1}}],[\"espnets2tctcmodel\",{\"0\":{\"1996\":1},\"1\":{\"1996\":1}}],[\"espnets2stmodel\",{\"0\":{\"1975\":1},\"1\":{\"1975\":1,\"2263\":2}}],[\"espnetstmodel\",{\"0\":{\"2221\":1},\"1\":{\"1156\":1,\"2221\":1,\"2267\":1}}],[\"espnets\",{\"1\":{\"699\":1}}],[\"espnet1\",{\"0\":{\"42\":1,\"106\":1,\"171\":1,\"193\":1},\"1\":{\"36\":1,\"55\":1,\"78\":1,\"94\":2,\"98\":1,\"99\":1,\"100\":1,\"107\":3,\"123\":1,\"133\":1,\"134\":3,\"135\":2,\"141\":1,\"161\":1,\"171\":1,\"173\":2,\"175\":4,\"197\":2,\"284\":1,\"285\":1,\"290\":2}}],[\"espnet2asrtransducermodel\",{\"1\":{\"625\":1}}],[\"espnet2\",{\"0\":{\"71\":1,\"105\":1,\"107\":1,\"172\":1,\"273\":1,\"614\":1,\"615\":1,\"616\":1,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"629\":1,\"630\":1,\"631\":1,\"632\":1,\"633\":1,\"634\":1,\"635\":1,\"636\":1,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"651\":1,\"652\":1,\"653\":1,\"654\":1,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"663\":1,\"664\":1,\"665\":1,\"666\":1,\"667\":1,\"668\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":1,\"673\":1,\"674\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"688\":1,\"689\":1,\"691\":1,\"692\":1,\"693\":1,\"695\":1,\"696\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"703\":1,\"704\":1,\"706\":1,\"707\":1,\"708\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"720\":1,\"722\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"738\":1,\"740\":1,\"741\":1,\"743\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"750\":1,\"751\":1,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"757\":1,\"759\":1,\"760\":1,\"761\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":1,\"767\":1,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"773\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"788\":1,\"790\":1,\"791\":1,\"793\":1,\"794\":1,\"795\":1,\"796\":1,\"798\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"817\":1,\"820\":1,\"821\":1,\"823\":1,\"824\":1,\"825\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":1,\"832\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"862\":1,\"864\":1,\"866\":1,\"867\":1,\"868\":1,\"869\":1,\"870\":1,\"871\":1,\"872\":1,\"873\":1,\"874\":1,\"875\":1,\"876\":1,\"877\":1,\"878\":1,\"879\":1,\"880\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":1,\"890\":1,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":1,\"900\":1,\"901\":1,\"902\":1,\"903\":1,\"904\":1,\"905\":1,\"906\":1,\"907\":1,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":1,\"914\":1,\"915\":1,\"916\":1,\"917\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"928\":1,\"929\":1,\"930\":1,\"931\":1,\"933\":1,\"935\":1,\"936\":1,\"937\":1,\"938\":1,\"939\":1,\"940\":1,\"941\":1,\"942\":1,\"943\":1,\"944\":1,\"945\":1,\"946\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":1,\"959\":1,\"960\":1,\"962\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"978\":1,\"979\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"984\":1,\"985\":1,\"987\":1,\"989\":1,\"991\":1,\"993\":1,\"995\":1,\"996\":1,\"998\":1,\"1000\":1,\"1001\":1,\"1003\":1,\"1005\":1,\"1007\":1,\"1009\":1,\"1011\":1,\"1013\":1,\"1015\":1,\"1017\":1,\"1019\":1,\"1021\":1,\"1022\":1,\"1024\":1,\"1026\":1,\"1028\":1,\"1029\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1050\":1,\"1051\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1107\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1118\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1129\":1,\"1130\":1,\"1131\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1137\":1,\"1139\":1,\"1141\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1147\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1159\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1176\":1,\"1177\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1189\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":1,\"1204\":1,\"1205\":1,\"1207\":1,\"1208\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1219\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1228\":1,\"1229\":1,\"1230\":1,\"1232\":1,\"1233\":1,\"1235\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1244\":1,\"1245\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1267\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1291\":1,\"1292\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1297\":1,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1307\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1312\":1,\"1313\":1,\"1314\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1330\":1,\"1331\":1,\"1332\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":1,\"1337\":1,\"1338\":1,\"1339\":1,\"1340\":1,\"1341\":1,\"1342\":1,\"1343\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1351\":1,\"1352\":1,\"1353\":1,\"1354\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1,\"1359\":1,\"1360\":1,\"1361\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1380\":1,\"1381\":1,\"1382\":1,\"1383\":1,\"1385\":1,\"1386\":1,\"1387\":1,\"1389\":1,\"1390\":1,\"1391\":1,\"1392\":1,\"1394\":1,\"1395\":1,\"1396\":1,\"1397\":1,\"1398\":1,\"1400\":1,\"1401\":1,\"1402\":1,\"1403\":1,\"1404\":1,\"1406\":1,\"1408\":1,\"1409\":1,\"1410\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1432\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1466\":1,\"1467\":1,\"1468\":1,\"1469\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1486\":1,\"1487\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1505\":1,\"1506\":1,\"1507\":1,\"1508\":1,\"1509\":1,\"1511\":1,\"1513\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1519\":1,\"1520\":1,\"1521\":1,\"1522\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1529\":1,\"1530\":1,\"1532\":1,\"1533\":1,\"1534\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1548\":1,\"1549\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1556\":1,\"1557\":1,\"1558\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1562\":1,\"1563\":1,\"1564\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1574\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1625\":1,\"1626\":1,\"1627\":1,\"1628\":1,\"1629\":1,\"1630\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1634\":1,\"1635\":1,\"1636\":1,\"1637\":1,\"1638\":1,\"1640\":1,\"1641\":1,\"1642\":1,\"1643\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1657\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1672\":1,\"1673\":1,\"1674\":1,\"1675\":1,\"1676\":1,\"1677\":1,\"1679\":1,\"1680\":1,\"1681\":1,\"1682\":1,\"1683\":1,\"1684\":1,\"1685\":1,\"1686\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1694\":1,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1699\":1,\"1700\":1,\"1701\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1718\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1728\":1,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1733\":1,\"1734\":1,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1761\":1,\"1762\":1,\"1763\":1,\"1764\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1770\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1775\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1779\":1,\"1780\":1,\"1781\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1791\":1,\"1792\":1,\"1793\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1802\":1,\"1803\":1,\"1804\":1,\"1806\":1,\"1807\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1813\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":1,\"1821\":1,\"1823\":1,\"1824\":1,\"1825\":1,\"1826\":1,\"1828\":1,\"1829\":1,\"1831\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":1,\"1876\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1881\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1889\":1,\"1891\":1,\"1892\":1,\"1893\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1898\":1,\"1899\":1,\"1900\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1907\":1,\"1908\":1,\"1910\":1,\"1911\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1922\":1,\"1923\":1,\"1924\":1,\"1925\":1,\"1926\":1,\"1927\":1,\"1928\":1,\"1929\":1,\"1930\":1,\"1931\":1,\"1932\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"1937\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1948\":1,\"1949\":1,\"1950\":1,\"1951\":1,\"1952\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"1957\":1,\"1959\":1,\"1960\":1,\"1961\":1,\"1962\":1,\"1963\":1,\"1964\":1,\"1965\":1,\"1966\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1992\":1,\"1993\":1,\"1994\":1,\"1995\":1,\"1996\":1,\"1997\":1,\"1998\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":1,\"2024\":1,\"2025\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2037\":1,\"2038\":1,\"2039\":1,\"2040\":1,\"2041\":1,\"2042\":1,\"2043\":1,\"2044\":1,\"2045\":1,\"2046\":1,\"2047\":1,\"2048\":1,\"2049\":1,\"2050\":1,\"2051\":1,\"2052\":1,\"2053\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2057\":1,\"2058\":1,\"2059\":1,\"2060\":1,\"2061\":1,\"2062\":1,\"2063\":1,\"2064\":1,\"2065\":1,\"2066\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2070\":1,\"2071\":1,\"2072\":1,\"2073\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2078\":1,\"2079\":1,\"2080\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2085\":1,\"2086\":1,\"2087\":1,\"2088\":1,\"2089\":1,\"2090\":1,\"2091\":1,\"2092\":1,\"2093\":1,\"2094\":1,\"2095\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2119\":1,\"2120\":1,\"2121\":1,\"2122\":1,\"2123\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2128\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2132\":1,\"2133\":1,\"2134\":1,\"2135\":1,\"2136\":1,\"2137\":1,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1,\"2143\":1,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2148\":1,\"2149\":1,\"2150\":1,\"2151\":1,\"2152\":1,\"2153\":1,\"2154\":1,\"2155\":1,\"2156\":1,\"2157\":1,\"2158\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2162\":1,\"2163\":1,\"2164\":1,\"2165\":1,\"2166\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2218\":1,\"2219\":1,\"2220\":1,\"2221\":1,\"2222\":1,\"2223\":1,\"2226\":1,\"2227\":1,\"2228\":1,\"2229\":1,\"2230\":1,\"2231\":1,\"2232\":1,\"2233\":1,\"2234\":1,\"2235\":1,\"2236\":1,\"2237\":1,\"2238\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2242\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2274\":1,\"2275\":1,\"2276\":1,\"2277\":1,\"2278\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2282\":1,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2287\":1,\"2288\":1,\"2289\":1,\"2291\":1,\"2292\":1,\"2293\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":1,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2307\":1,\"2308\":1,\"2309\":1,\"2310\":1,\"2311\":1,\"2312\":1,\"2313\":1,\"2314\":1,\"2316\":1,\"2317\":1,\"2318\":1,\"2319\":1,\"2320\":1,\"2321\":1,\"2322\":1,\"2323\":1,\"2324\":1,\"2325\":1,\"2327\":1,\"2328\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":1,\"2335\":1,\"2336\":1,\"2337\":1,\"2338\":1,\"2339\":1,\"2340\":1,\"2341\":1,\"2342\":1,\"2344\":1,\"2345\":1,\"2346\":1,\"2347\":1,\"2348\":1,\"2349\":1,\"2350\":1,\"2351\":1,\"2353\":1,\"2354\":1,\"2355\":1,\"2356\":1,\"2357\":1,\"2358\":1,\"2359\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2364\":1,\"2365\":1,\"2366\":1,\"2367\":1,\"2368\":1,\"2369\":1,\"2370\":1,\"2371\":1,\"2372\":1,\"2373\":1,\"2374\":1,\"2375\":1,\"2376\":1,\"2378\":1,\"2379\":1,\"2380\":1,\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2388\":1,\"2389\":1,\"2390\":1,\"2391\":1,\"2392\":1,\"2393\":1,\"2394\":1,\"2395\":1,\"2396\":1,\"2397\":1,\"2398\":1,\"2399\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2404\":1,\"2407\":1,\"2408\":1,\"2409\":1,\"2411\":1,\"2412\":1,\"2413\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2421\":1,\"2422\":1,\"2423\":1,\"2424\":1,\"2425\":1,\"2426\":1,\"2427\":1,\"2428\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":1,\"2433\":1,\"2434\":1,\"2436\":1,\"2437\":1,\"2438\":1,\"2439\":1,\"2440\":1,\"2441\":1,\"2442\":1,\"2443\":1,\"2445\":1,\"2446\":1,\"2447\":1,\"2448\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1,\"2474\":1,\"2475\":1,\"2476\":1,\"2477\":1,\"2478\":1,\"2480\":1,\"2481\":1,\"2482\":1,\"2483\":1,\"2484\":1,\"2486\":1,\"2487\":1,\"2489\":1,\"2490\":1,\"2491\":1,\"2492\":1,\"2494\":1,\"2495\":1,\"2496\":1,\"2497\":1,\"2498\":1,\"2499\":1,\"2501\":1,\"2503\":1,\"2504\":1,\"2506\":1,\"2507\":1,\"2509\":1,\"2515\":1},\"1\":{\"3\":2,\"9\":1,\"36\":2,\"51\":2,\"54\":1,\"55\":2,\"56\":2,\"57\":1,\"58\":2,\"61\":3,\"62\":1,\"63\":1,\"64\":1,\"71\":1,\"78\":3,\"79\":2,\"80\":1,\"82\":3,\"84\":6,\"85\":2,\"86\":2,\"87\":1,\"88\":6,\"89\":1,\"90\":1,\"91\":1,\"92\":1,\"93\":2,\"94\":3,\"101\":1,\"106\":1,\"107\":4,\"109\":2,\"117\":1,\"119\":1,\"123\":2,\"126\":1,\"133\":2,\"134\":3,\"135\":2,\"136\":1,\"137\":1,\"138\":2,\"141\":1,\"144\":1,\"150\":1,\"161\":1,\"172\":1,\"173\":3,\"175\":4,\"190\":1,\"191\":1,\"192\":1,\"197\":1,\"198\":1,\"203\":1,\"209\":1,\"215\":1,\"220\":1,\"221\":1,\"222\":1,\"223\":10,\"225\":12,\"226\":1,\"228\":4,\"229\":1,\"230\":1,\"231\":1,\"232\":1,\"233\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":1,\"250\":1,\"251\":1,\"252\":1,\"257\":1,\"258\":1,\"260\":1,\"263\":6,\"264\":1,\"273\":1,\"274\":1,\"283\":1,\"284\":1,\"290\":21,\"291\":1,\"292\":1,\"404\":1,\"614\":1,\"615\":1,\"616\":1,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"629\":1,\"630\":1,\"631\":1,\"632\":1,\"633\":1,\"634\":1,\"635\":1,\"636\":1,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"651\":1,\"652\":1,\"653\":1,\"654\":1,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":2,\"662\":1,\"663\":1,\"664\":1,\"665\":1,\"666\":1,\"667\":1,\"668\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":1,\"673\":1,\"674\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"688\":1,\"689\":1,\"691\":1,\"692\":2,\"693\":1,\"696\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"703\":1,\"704\":1,\"706\":1,\"709\":1,\"710\":2,\"711\":2,\"712\":1,\"713\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"720\":1,\"722\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":2,\"732\":2,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"738\":1,\"740\":1,\"741\":1,\"743\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"750\":1,\"751\":1,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"757\":1,\"759\":1,\"760\":1,\"761\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":2,\"767\":2,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"773\":1,\"774\":1,\"775\":2,\"776\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"788\":1,\"790\":1,\"791\":1,\"793\":1,\"794\":2,\"795\":1,\"796\":1,\"798\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"817\":1,\"820\":1,\"821\":1,\"823\":1,\"824\":1,\"825\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":1,\"832\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":1,\"847\":1,\"848\":2,\"849\":2,\"850\":2,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"862\":1,\"864\":1,\"866\":1,\"867\":1,\"868\":1,\"869\":1,\"873\":1,\"874\":1,\"875\":1,\"876\":1,\"877\":1,\"878\":1,\"879\":1,\"880\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":1,\"890\":1,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":1,\"900\":1,\"901\":1,\"902\":1,\"903\":1,\"904\":1,\"905\":1,\"906\":1,\"907\":2,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":1,\"914\":1,\"915\":1,\"916\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"928\":1,\"929\":1,\"930\":1,\"931\":1,\"933\":1,\"935\":1,\"936\":1,\"937\":1,\"938\":1,\"939\":1,\"940\":1,\"941\":1,\"942\":1,\"943\":1,\"944\":1,\"945\":1,\"946\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":1,\"959\":1,\"960\":1,\"962\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":2,\"975\":1,\"976\":1,\"977\":1,\"978\":1,\"979\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"984\":1,\"985\":1,\"987\":1,\"989\":1,\"991\":1,\"993\":1,\"995\":1,\"996\":1,\"998\":1,\"1000\":1,\"1001\":1,\"1003\":1,\"1005\":1,\"1007\":1,\"1009\":1,\"1011\":1,\"1013\":1,\"1015\":1,\"1017\":1,\"1019\":1,\"1021\":1,\"1022\":1,\"1024\":1,\"1026\":1,\"1028\":1,\"1029\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1050\":1,\"1051\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":2,\"1105\":1,\"1107\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1118\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1129\":1,\"1130\":1,\"1131\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1137\":1,\"1139\":1,\"1141\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1147\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1155\":4,\"1156\":2,\"1157\":4,\"1158\":2,\"1159\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1176\":1,\"1177\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1189\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":1,\"1204\":1,\"1205\":1,\"1207\":1,\"1208\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1219\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1228\":1,\"1229\":1,\"1230\":1,\"1232\":1,\"1233\":1,\"1235\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1244\":1,\"1245\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1267\":1,\"1268\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":2,\"1275\":1,\"1276\":1,\"1277\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1291\":1,\"1292\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1297\":1,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1307\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1312\":1,\"1313\":1,\"1314\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1330\":1,\"1331\":1,\"1332\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":1,\"1337\":1,\"1338\":1,\"1339\":1,\"1340\":1,\"1341\":1,\"1342\":1,\"1343\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1351\":1,\"1352\":1,\"1353\":1,\"1354\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1,\"1359\":1,\"1360\":1,\"1361\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1381\":1,\"1382\":1,\"1383\":1,\"1385\":1,\"1386\":1,\"1387\":1,\"1389\":1,\"1390\":1,\"1391\":1,\"1392\":1,\"1394\":1,\"1395\":1,\"1396\":1,\"1397\":1,\"1398\":1,\"1400\":1,\"1401\":1,\"1402\":1,\"1403\":1,\"1404\":1,\"1406\":1,\"1408\":1,\"1409\":1,\"1410\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1432\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1466\":1,\"1467\":1,\"1468\":1,\"1469\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1486\":1,\"1487\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1505\":1,\"1506\":1,\"1507\":1,\"1508\":1,\"1509\":1,\"1511\":1,\"1513\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1519\":1,\"1520\":1,\"1521\":1,\"1522\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1529\":1,\"1530\":1,\"1532\":1,\"1533\":1,\"1534\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1548\":1,\"1549\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1556\":1,\"1557\":1,\"1558\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1562\":1,\"1563\":1,\"1564\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1574\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1625\":1,\"1626\":1,\"1627\":1,\"1628\":1,\"1629\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1634\":1,\"1635\":1,\"1636\":1,\"1637\":1,\"1638\":1,\"1640\":1,\"1641\":1,\"1642\":1,\"1643\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1657\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1672\":1,\"1673\":1,\"1674\":1,\"1675\":1,\"1676\":1,\"1677\":1,\"1679\":1,\"1680\":1,\"1681\":1,\"1682\":1,\"1683\":1,\"1684\":1,\"1685\":1,\"1686\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1694\":1,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1699\":1,\"1700\":1,\"1701\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1718\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1728\":1,\"1729\":1,\"1730\":1,\"1731\":2,\"1732\":1,\"1733\":1,\"1734\":1,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1761\":1,\"1762\":1,\"1763\":1,\"1764\":1,\"1766\":1,\"1768\":1,\"1770\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1775\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1779\":1,\"1780\":1,\"1781\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1791\":1,\"1793\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1802\":1,\"1803\":1,\"1804\":1,\"1805\":1,\"1806\":1,\"1807\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1813\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":1,\"1821\":1,\"1822\":6,\"1823\":1,\"1824\":1,\"1825\":1,\"1826\":1,\"1828\":1,\"1829\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":1,\"1876\":2,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1881\":1,\"1883\":1,\"1885\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1889\":1,\"1891\":1,\"1892\":1,\"1893\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1898\":1,\"1899\":1,\"1900\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1907\":1,\"1908\":1,\"1910\":1,\"1911\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1923\":1,\"1924\":1,\"1925\":1,\"1926\":1,\"1927\":1,\"1928\":1,\"1931\":1,\"1932\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"1937\":1,\"1938\":2,\"1940\":1,\"1942\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1948\":1,\"1949\":1,\"1950\":1,\"1951\":1,\"1952\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"1957\":2,\"1959\":1,\"1960\":2,\"1961\":2,\"1962\":1,\"1963\":1,\"1964\":1,\"1965\":1,\"1966\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1992\":2,\"1993\":1,\"1994\":1,\"1995\":2,\"1996\":1,\"1997\":1,\"1998\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2039\":1,\"2040\":1,\"2043\":1,\"2044\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2061\":1,\"2065\":1,\"2066\":1,\"2088\":1,\"2095\":1,\"2096\":1,\"2101\":1,\"2102\":1,\"2115\":1,\"2116\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2128\":1,\"2129\":2,\"2130\":1,\"2131\":1,\"2132\":1,\"2133\":1,\"2134\":1,\"2136\":1,\"2137\":1,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1,\"2143\":1,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2148\":1,\"2149\":1,\"2150\":1,\"2151\":1,\"2152\":1,\"2153\":1,\"2154\":1,\"2155\":1,\"2156\":1,\"2157\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2162\":1,\"2163\":1,\"2164\":1,\"2166\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":2,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2218\":1,\"2219\":1,\"2220\":1,\"2221\":1,\"2222\":1,\"2223\":1,\"2226\":1,\"2227\":1,\"2228\":1,\"2229\":1,\"2231\":1,\"2232\":1,\"2233\":1,\"2235\":1,\"2236\":1,\"2237\":1,\"2238\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2242\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2246\":4,\"2247\":2,\"2248\":4,\"2249\":4,\"2250\":4,\"2251\":4,\"2252\":4,\"2253\":4,\"2254\":4,\"2255\":4,\"2256\":4,\"2257\":4,\"2258\":1,\"2259\":4,\"2260\":4,\"2261\":4,\"2262\":2,\"2263\":4,\"2264\":4,\"2265\":4,\"2266\":4,\"2267\":4,\"2268\":4,\"2269\":4,\"2270\":4,\"2271\":4,\"2272\":4,\"2273\":4,\"2274\":1,\"2275\":1,\"2276\":1,\"2277\":1,\"2278\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2282\":1,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2287\":1,\"2288\":1,\"2289\":1,\"2291\":1,\"2292\":1,\"2293\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":1,\"2299\":1,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2307\":1,\"2308\":1,\"2309\":1,\"2310\":1,\"2311\":1,\"2312\":1,\"2313\":1,\"2314\":1,\"2316\":1,\"2317\":1,\"2318\":1,\"2319\":1,\"2320\":1,\"2321\":1,\"2322\":1,\"2323\":1,\"2324\":1,\"2325\":2,\"2327\":2,\"2328\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":1,\"2335\":1,\"2336\":1,\"2337\":1,\"2338\":1,\"2339\":1,\"2340\":1,\"2341\":1,\"2342\":1,\"2344\":1,\"2345\":1,\"2346\":1,\"2347\":1,\"2348\":1,\"2349\":1,\"2350\":1,\"2351\":1,\"2353\":1,\"2354\":1,\"2355\":1,\"2356\":1,\"2357\":1,\"2358\":1,\"2359\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2364\":1,\"2365\":1,\"2366\":1,\"2367\":1,\"2368\":1,\"2369\":1,\"2370\":1,\"2371\":1,\"2372\":1,\"2373\":1,\"2374\":1,\"2375\":1,\"2376\":1,\"2377\":3,\"2378\":1,\"2379\":1,\"2380\":1,\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2388\":1,\"2389\":1,\"2390\":1,\"2391\":1,\"2392\":1,\"2393\":1,\"2394\":1,\"2395\":1,\"2396\":1,\"2397\":1,\"2398\":1,\"2399\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2404\":1,\"2407\":1,\"2408\":1,\"2409\":1,\"2411\":1,\"2412\":1,\"2413\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2421\":1,\"2422\":1,\"2423\":1,\"2424\":1,\"2425\":1,\"2426\":1,\"2427\":1,\"2428\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":1,\"2433\":1,\"2434\":1,\"2436\":1,\"2437\":1,\"2438\":1,\"2439\":1,\"2440\":1,\"2441\":1,\"2442\":1,\"2443\":1,\"2445\":1,\"2446\":1,\"2447\":1,\"2448\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1,\"2474\":1,\"2475\":1,\"2476\":1,\"2477\":1,\"2478\":1,\"2480\":1,\"2481\":1,\"2482\":1,\"2483\":1,\"2484\":1,\"2486\":1,\"2487\":1,\"2489\":1,\"2490\":1,\"2491\":1,\"2492\":1,\"2494\":1,\"2495\":1,\"2496\":1,\"2497\":1,\"2498\":1,\"2499\":1,\"2501\":1,\"2503\":1,\"2504\":1,\"2506\":1,\"2507\":1}}],[\"espnet\",{\"0\":{\"0\":1,\"5\":1,\"23\":1,\"30\":1,\"152\":1,\"153\":1,\"155\":1,\"156\":1,\"162\":1,\"176\":1,\"183\":1,\"195\":1,\"245\":1,\"625\":1,\"736\":1,\"737\":1,\"738\":1,\"750\":1,\"794\":1,\"954\":1,\"958\":1,\"962\":1,\"974\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1395\":1,\"1521\":1,\"1585\":1,\"1640\":1,\"1641\":1,\"1702\":1,\"1940\":1,\"1942\":1,\"1959\":1,\"1965\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2043\":1,\"2044\":1,\"2045\":1,\"2127\":1,\"2184\":1,\"2216\":1,\"2221\":1,\"2228\":1,\"2229\":1,\"2325\":1,\"2327\":1,\"2355\":1,\"2408\":1,\"2446\":1,\"2462\":1},\"1\":{\"0\":1,\"1\":4,\"3\":1,\"5\":2,\"8\":1,\"9\":1,\"10\":2,\"11\":2,\"12\":2,\"14\":1,\"15\":1,\"16\":1,\"19\":1,\"22\":1,\"23\":1,\"26\":7,\"28\":1,\"32\":5,\"34\":2,\"37\":2,\"41\":1,\"42\":2,\"43\":2,\"46\":4,\"48\":2,\"49\":1,\"50\":3,\"69\":1,\"71\":1,\"78\":1,\"94\":2,\"96\":2,\"97\":1,\"98\":2,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":2,\"107\":1,\"110\":1,\"117\":1,\"125\":3,\"127\":1,\"128\":1,\"129\":1,\"136\":2,\"153\":1,\"154\":2,\"156\":1,\"161\":1,\"162\":14,\"175\":1,\"179\":1,\"182\":1,\"184\":2,\"185\":2,\"186\":1,\"187\":1,\"188\":1,\"190\":8,\"191\":1,\"194\":1,\"195\":7,\"200\":2,\"205\":2,\"208\":1,\"213\":2,\"217\":1,\"220\":3,\"223\":3,\"225\":2,\"228\":2,\"240\":1,\"242\":4,\"243\":7,\"245\":3,\"254\":1,\"259\":3,\"260\":3,\"263\":3,\"268\":2,\"277\":2,\"284\":1,\"285\":2,\"286\":3,\"287\":2,\"289\":2,\"290\":3,\"536\":3,\"625\":1,\"696\":2,\"697\":2,\"699\":1,\"709\":2,\"715\":1,\"736\":1,\"737\":1,\"738\":2,\"749\":1,\"750\":1,\"768\":1,\"774\":2,\"780\":2,\"783\":1,\"784\":1,\"794\":1,\"954\":1,\"958\":1,\"960\":1,\"962\":1,\"974\":1,\"1002\":1,\"1011\":1,\"1155\":1,\"1156\":1,\"1157\":2,\"1158\":1,\"1210\":1,\"1264\":1,\"1334\":1,\"1395\":2,\"1521\":2,\"1585\":2,\"1640\":1,\"1641\":1,\"1668\":1,\"1702\":2,\"1785\":2,\"1786\":2,\"1817\":2,\"1818\":2,\"1940\":1,\"1942\":1,\"1957\":2,\"1959\":1,\"1965\":2,\"1975\":2,\"1996\":1,\"1997\":1,\"2043\":4,\"2044\":1,\"2045\":6,\"2056\":2,\"2127\":1,\"2132\":1,\"2136\":3,\"2141\":1,\"2184\":1,\"2191\":2,\"2216\":1,\"2221\":1,\"2228\":2,\"2229\":2,\"2235\":1,\"2236\":1,\"2246\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2311\":2,\"2312\":1,\"2325\":2,\"2327\":1,\"2342\":1,\"2344\":2,\"2345\":1,\"2351\":1,\"2355\":1,\"2408\":2,\"2446\":2,\"2462\":1}}],[\"es\",{\"1\":{\"44\":1,\"625\":1,\"1627\":2,\"2413\":2,\"2424\":2,\"2448\":2}}],[\"eprenet\",{\"1\":{\"2235\":8,\"2236\":8,\"2432\":8}}],[\"eprojs\",{\"1\":{\"706\":3,\"869\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1768\":2,\"1814\":1,\"1895\":2}}],[\"eprint=\",{\"1\":{\"202\":1}}],[\"ep\",{\"1\":{\"50\":1}}],[\"eps=1\",{\"1\":{\"1248\":1}}],[\"eps=1e\",{\"1\":{\"722\":1,\"887\":1,\"1029\":1,\"1046\":1,\"1048\":1,\"1054\":1,\"1070\":1,\"1071\":1,\"1073\":1,\"1078\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1210\":2,\"1235\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1279\":1,\"1281\":1,\"1282\":1,\"1293\":1,\"1308\":1,\"1334\":1,\"1354\":1,\"1479\":1,\"1490\":1,\"1527\":1,\"1791\":1,\"1836\":1,\"1900\":1,\"1925\":1,\"2020\":1,\"2436\":1,\"2437\":1}}],[\"eps=none\",{\"1\":{\"1247\":1}}],[\"epsilon\",{\"1\":{\"141\":7,\"661\":1,\"1029\":1,\"1070\":1,\"1071\":1,\"1073\":1,\"1210\":1,\"1235\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1382\":1,\"1400\":3,\"1469\":3,\"1491\":1,\"1583\":1,\"1603\":1}}],[\"eps\",{\"1\":{\"39\":1,\"84\":2,\"141\":7,\"243\":1,\"615\":2,\"640\":2,\"648\":2,\"661\":2,\"666\":2,\"854\":1,\"1029\":1,\"1065\":1,\"1070\":1,\"1071\":1,\"1073\":1,\"1126\":2,\"1127\":2,\"1210\":1,\"1217\":2,\"1235\":1,\"1247\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1279\":1,\"1280\":2,\"1281\":1,\"1282\":1,\"1283\":2,\"1293\":1,\"1309\":4,\"1310\":4,\"1311\":4,\"1317\":2,\"1318\":4,\"1319\":4,\"1321\":4,\"1322\":4,\"1323\":4,\"1326\":2,\"1327\":4,\"1329\":2,\"1330\":4,\"1334\":3,\"1354\":1,\"1361\":2,\"1377\":2,\"1583\":2,\"1603\":2,\"1656\":2,\"1671\":1,\"1700\":2}}],[\"epoch=none\",{\"1\":{\"2014\":1,\"2020\":1}}],[\"epochs`\",{\"1\":{\"2355\":2}}],[\"epochs=10\",{\"1\":{\"47\":1}}],[\"epochs10\",{\"1\":{\"47\":1}}],[\"epochs\",{\"1\":{\"39\":1,\"1645\":1,\"1647\":1,\"1650\":1,\"2000\":1,\"2001\":1,\"2015\":1,\"2020\":1,\"2134\":2,\"2249\":2,\"2253\":2,\"2355\":1,\"2359\":3,\"2367\":1}}],[\"epoch\",{\"0\":{\"91\":1},\"1\":{\"39\":3,\"87\":2,\"91\":16,\"104\":1,\"218\":1,\"243\":1,\"267\":1,\"276\":1,\"286\":1,\"541\":2,\"958\":1,\"960\":1,\"961\":4,\"1642\":1,\"1643\":1,\"1644\":2,\"1645\":6,\"1646\":1,\"1647\":4,\"1648\":1,\"1650\":4,\"1949\":1,\"1999\":1,\"2000\":3,\"2001\":3,\"2008\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":3,\"2015\":4,\"2016\":1,\"2017\":1,\"2018\":2,\"2019\":1,\"2021\":3,\"2249\":14,\"2253\":14,\"2258\":2,\"2338\":2,\"2339\":2,\"2347\":4,\"2348\":1,\"2355\":5,\"2359\":19,\"2365\":1,\"2367\":2,\"2369\":4,\"2370\":2,\"2371\":4,\"2372\":1,\"2462\":1}}],[\"emulating\",{\"1\":{\"818\":1}}],[\"emulate\",{\"1\":{\"175\":1,\"1678\":1}}],[\"emebedding\",{\"1\":{\"741\":1}}],[\"embs\",{\"1\":{\"2430\":2}}],[\"emb\",{\"1\":{\"641\":1,\"651\":2,\"701\":3,\"709\":1,\"733\":1,\"734\":1,\"735\":3,\"748\":1,\"780\":1,\"847\":1,\"947\":4,\"949\":4,\"1117\":2,\"1130\":2,\"1131\":2,\"1182\":3,\"1183\":3,\"1184\":3,\"1268\":6,\"1269\":6,\"1270\":8,\"1271\":6,\"1274\":2,\"1400\":1,\"1785\":2,\"1815\":1,\"1817\":2,\"1934\":1,\"2191\":1,\"2458\":1}}],[\"embds\",{\"1\":{\"2354\":3}}],[\"embd=true\",{\"1\":{\"1702\":1,\"2354\":1}}],[\"embd\",{\"1\":{\"377\":6,\"449\":2,\"1702\":4,\"2174\":1,\"2184\":3,\"2354\":12}}],[\"embeds=none\",{\"1\":{\"1745\":1}}],[\"embeds\",{\"1\":{\"733\":1,\"1745\":1}}],[\"embedded\",{\"1\":{\"710\":3,\"711\":3,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"771\":1,\"846\":1,\"849\":1,\"2129\":1}}],[\"embedding=false\",{\"1\":{\"790\":1,\"1120\":1,\"1122\":1}}],[\"embedding\",{\"0\":{\"1748\":1,\"1784\":1,\"1786\":1,\"1808\":1,\"1818\":1,\"1820\":1,\"1837\":1,\"1957\":1,\"1960\":2,\"1961\":1},\"1\":{\"8\":1,\"50\":1,\"51\":3,\"142\":2,\"216\":1,\"217\":1,\"227\":1,\"234\":1,\"235\":2,\"252\":1,\"253\":1,\"254\":1,\"265\":3,\"266\":3,\"267\":3,\"274\":3,\"275\":3,\"276\":3,\"284\":4,\"285\":5,\"286\":11,\"289\":5,\"377\":1,\"449\":1,\"457\":1,\"617\":4,\"618\":4,\"620\":7,\"624\":4,\"634\":2,\"636\":2,\"641\":3,\"643\":2,\"644\":4,\"645\":1,\"647\":1,\"651\":2,\"674\":2,\"692\":1,\"710\":1,\"711\":1,\"731\":1,\"732\":1,\"766\":1,\"767\":1,\"775\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":2,\"911\":1,\"927\":1,\"947\":2,\"949\":2,\"1029\":1,\"1061\":1,\"1064\":1,\"1086\":1,\"1119\":2,\"1131\":4,\"1132\":2,\"1149\":1,\"1155\":2,\"1158\":2,\"1172\":1,\"1177\":1,\"1207\":1,\"1211\":1,\"1235\":1,\"1262\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1274\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1521\":3,\"1526\":1,\"1546\":1,\"1552\":3,\"1553\":1,\"1577\":1,\"1585\":2,\"1598\":1,\"1599\":10,\"1625\":2,\"1626\":5,\"1702\":1,\"1736\":3,\"1748\":3,\"1750\":1,\"1784\":2,\"1785\":1,\"1786\":3,\"1788\":1,\"1808\":2,\"1815\":2,\"1817\":1,\"1818\":2,\"1820\":2,\"1837\":2,\"1863\":1,\"1866\":1,\"1957\":2,\"1958\":1,\"1960\":6,\"1961\":5,\"1992\":5,\"1993\":6,\"1995\":5,\"2129\":1,\"2148\":1,\"2167\":2,\"2176\":2,\"2184\":4,\"2187\":1,\"2192\":1,\"2203\":1,\"2207\":2,\"2209\":1,\"2218\":1,\"2220\":1,\"2223\":1,\"2228\":3,\"2229\":3,\"2235\":6,\"2236\":6,\"2239\":5,\"2240\":5,\"2245\":6,\"2354\":4,\"2368\":1,\"2408\":3,\"2411\":5,\"2412\":9,\"2423\":9,\"2425\":1,\"2429\":1,\"2430\":2,\"2431\":6,\"2432\":7,\"2446\":3,\"2447\":9}}],[\"embeddings\",{\"0\":{\"8\":1},\"1\":{\"190\":1,\"233\":1,\"235\":1,\"254\":2,\"284\":2,\"285\":3,\"286\":5,\"289\":1,\"647\":4,\"674\":2,\"699\":2,\"760\":2,\"846\":2,\"1131\":1,\"1172\":1,\"1177\":1,\"1268\":2,\"1526\":1,\"1552\":2,\"1553\":1,\"1599\":1,\"1702\":1,\"1745\":1,\"1752\":1,\"1788\":1,\"1957\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"2045\":1,\"2130\":2,\"2167\":1,\"2176\":1,\"2183\":2,\"2184\":2,\"2187\":1,\"2190\":2,\"2192\":1,\"2198\":1,\"2203\":1,\"2207\":1,\"2208\":3,\"2209\":2,\"2235\":2,\"2236\":2,\"2239\":1,\"2240\":1,\"2245\":2,\"2354\":6,\"2411\":2,\"2412\":2,\"2423\":2,\"2429\":2,\"2430\":3,\"2431\":2,\"2432\":2,\"2447\":1}}],[\"embed=false\",{\"1\":{\"285\":1}}],[\"embed\",{\"0\":{\"449\":1},\"1\":{\"50\":1,\"51\":3,\"88\":4,\"142\":5,\"197\":1,\"285\":4,\"286\":16,\"290\":4,\"449\":1,\"634\":4,\"641\":6,\"643\":4,\"644\":2,\"651\":8,\"674\":10,\"692\":2,\"710\":3,\"711\":3,\"731\":1,\"732\":1,\"737\":2,\"746\":2,\"766\":1,\"767\":1,\"771\":1,\"775\":2,\"787\":1,\"790\":1,\"820\":2,\"846\":3,\"847\":5,\"848\":1,\"849\":1,\"850\":2,\"911\":1,\"1119\":1,\"1120\":2,\"1122\":2,\"1149\":1,\"1400\":4,\"1469\":1,\"1526\":4,\"1552\":4,\"1553\":3,\"1598\":7,\"1599\":14,\"1600\":6,\"1625\":3,\"1626\":4,\"1736\":4,\"1748\":2,\"1749\":1,\"1758\":3,\"1815\":4,\"1947\":1,\"1959\":2,\"1960\":2,\"1961\":2,\"1992\":6,\"1993\":8,\"1994\":2,\"1995\":6,\"2129\":1,\"2218\":2,\"2220\":2,\"2227\":2,\"2231\":3,\"2235\":10,\"2236\":10,\"2239\":6,\"2240\":6,\"2245\":8,\"2315\":1,\"2354\":1,\"2365\":1,\"2411\":6,\"2412\":14,\"2423\":14,\"2430\":3,\"2431\":8,\"2432\":8,\"2447\":14}}],[\"emma\",{\"1\":{\"202\":1}}],[\"emotion\",{\"1\":{\"201\":1}}],[\"ema\",{\"0\":{\"637\":1,\"1394\":1,\"1481\":1},\"1\":{\"142\":4,\"633\":5,\"634\":5,\"637\":10,\"912\":1,\"1382\":1,\"1389\":1,\"1391\":1,\"1394\":2,\"1396\":2,\"1400\":4,\"1401\":1,\"1403\":1,\"1406\":1,\"1408\":2,\"1410\":1,\"1441\":2,\"1466\":2,\"1468\":1,\"1469\":2,\"1481\":1}}],[\"emits\",{\"1\":{\"716\":1}}],[\"emitted\",{\"1\":{\"139\":1}}],[\"emiru\",{\"1\":{\"202\":1}}],[\"emission\",{\"1\":{\"45\":1,\"703\":1,\"755\":1,\"785\":1,\"786\":1,\"800\":1,\"867\":1,\"881\":1,\"884\":1,\"922\":1,\"936\":1,\"937\":1}}],[\"emphasis\",{\"1\":{\"1686\":1,\"1694\":1}}],[\"emphasizes\",{\"1\":{\"2001\":1}}],[\"emphasized\",{\"1\":{\"1686\":1,\"1694\":1,\"2183\":1,\"2187\":1}}],[\"emphasize\",{\"1\":{\"1686\":1,\"1694\":1}}],[\"employs\",{\"1\":{\"285\":1,\"2240\":1}}],[\"employing\",{\"1\":{\"200\":1}}],[\"employ\",{\"1\":{\"130\":1,\"132\":1}}],[\"empty\",{\"1\":{\"22\":1,\"27\":1,\"211\":1,\"266\":1,\"275\":1,\"1218\":1,\"1221\":1,\"1420\":1,\"1799\":1,\"2155\":1,\"2380\":1}}],[\"em\",{\"1\":{\"67\":3}}],[\"edropout\",{\"1\":{\"2235\":2,\"2236\":2}}],[\"edu\",{\"1\":{\"1002\":1,\"1125\":1}}],[\"eda\",{\"1\":{\"974\":3,\"978\":1}}],[\"editor\",{\"1\":{\"202\":1}}],[\"edit\",{\"1\":{\"19\":1,\"47\":1,\"236\":1,\"286\":2}}],[\"edge\",{\"1\":{\"9\":1}}],[\"early\",{\"1\":{\"2348\":1,\"2359\":1,\"2370\":2,\"2372\":1}}],[\"earlier\",{\"1\":{\"1711\":1}}],[\"easier\",{\"1\":{\"243\":1,\"821\":1}}],[\"easily\",{\"1\":{\"39\":1,\"175\":1,\"194\":1,\"223\":1,\"224\":1,\"232\":1,\"243\":1,\"258\":1,\"290\":1}}],[\"ease\",{\"1\":{\"43\":1}}],[\"easy\",{\"1\":{\"5\":1,\"262\":1,\"2167\":3,\"2176\":3}}],[\"eachlayer\",{\"1\":{\"1594\":1}}],[\"each\",{\"0\":{\"58\":1,\"62\":1,\"64\":1,\"91\":1,\"109\":1},\"1\":{\"3\":1,\"37\":1,\"43\":2,\"44\":2,\"45\":5,\"50\":2,\"54\":3,\"58\":1,\"59\":1,\"78\":3,\"79\":1,\"80\":1,\"81\":2,\"82\":2,\"91\":2,\"94\":2,\"96\":1,\"99\":1,\"100\":2,\"101\":1,\"102\":2,\"104\":1,\"109\":1,\"127\":1,\"139\":1,\"140\":2,\"141\":6,\"144\":1,\"145\":3,\"168\":1,\"175\":4,\"194\":1,\"196\":5,\"197\":4,\"201\":3,\"211\":4,\"212\":2,\"213\":3,\"218\":2,\"223\":3,\"224\":2,\"235\":1,\"242\":1,\"254\":1,\"259\":1,\"267\":2,\"268\":5,\"276\":2,\"277\":5,\"286\":3,\"616\":2,\"625\":1,\"627\":1,\"636\":3,\"661\":1,\"696\":4,\"697\":2,\"702\":2,\"704\":1,\"706\":2,\"756\":3,\"768\":2,\"773\":3,\"777\":2,\"786\":2,\"787\":1,\"800\":2,\"820\":3,\"828\":4,\"830\":1,\"846\":4,\"847\":2,\"866\":3,\"867\":5,\"921\":2,\"935\":2,\"960\":1,\"980\":1,\"982\":1,\"994\":2,\"1000\":1,\"1029\":1,\"1053\":1,\"1061\":1,\"1118\":2,\"1124\":2,\"1125\":2,\"1133\":1,\"1137\":1,\"1139\":1,\"1141\":1,\"1155\":3,\"1156\":2,\"1157\":3,\"1163\":1,\"1164\":1,\"1176\":4,\"1181\":1,\"1209\":2,\"1210\":1,\"1235\":2,\"1267\":1,\"1268\":1,\"1270\":1,\"1273\":1,\"1274\":1,\"1279\":4,\"1280\":5,\"1281\":4,\"1282\":2,\"1283\":4,\"1306\":1,\"1329\":1,\"1333\":1,\"1334\":2,\"1356\":2,\"1371\":1,\"1390\":2,\"1392\":3,\"1397\":5,\"1402\":2,\"1409\":2,\"1422\":3,\"1441\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1467\":2,\"1514\":3,\"1515\":1,\"1516\":1,\"1521\":3,\"1526\":2,\"1534\":2,\"1552\":4,\"1553\":2,\"1558\":1,\"1587\":1,\"1589\":2,\"1593\":2,\"1594\":1,\"1595\":2,\"1596\":1,\"1597\":1,\"1604\":1,\"1606\":2,\"1609\":1,\"1627\":1,\"1645\":3,\"1647\":1,\"1650\":1,\"1651\":1,\"1655\":3,\"1702\":1,\"1713\":1,\"1715\":1,\"1716\":3,\"1719\":2,\"1721\":1,\"1725\":2,\"1749\":2,\"1750\":2,\"1751\":1,\"1752\":1,\"1758\":2,\"1764\":2,\"1788\":2,\"1796\":1,\"1806\":4,\"1815\":2,\"1839\":1,\"1843\":2,\"1862\":1,\"1878\":1,\"1917\":1,\"1919\":2,\"1940\":2,\"1942\":2,\"1943\":1,\"1949\":1,\"1991\":1,\"1992\":2,\"1993\":2,\"1995\":2,\"1997\":2,\"2000\":8,\"2001\":1,\"2007\":1,\"2049\":2,\"2130\":5,\"2131\":1,\"2133\":1,\"2134\":1,\"2136\":7,\"2143\":1,\"2145\":2,\"2146\":2,\"2147\":3,\"2155\":1,\"2162\":1,\"2168\":1,\"2183\":1,\"2184\":1,\"2190\":1,\"2198\":1,\"2208\":1,\"2215\":2,\"2219\":2,\"2220\":1,\"2223\":1,\"2226\":2,\"2227\":1,\"2228\":3,\"2229\":3,\"2231\":2,\"2235\":2,\"2236\":2,\"2237\":1,\"2239\":2,\"2240\":2,\"2241\":2,\"2245\":2,\"2249\":3,\"2253\":3,\"2325\":1,\"2327\":1,\"2344\":1,\"2353\":2,\"2354\":5,\"2355\":1,\"2364\":2,\"2407\":1,\"2411\":2,\"2412\":2,\"2413\":2,\"2423\":2,\"2424\":2,\"2428\":1,\"2429\":1,\"2430\":1,\"2431\":2,\"2432\":3,\"2447\":1,\"2448\":2}}],[\"ez\",{\"0\":{\"5\":1,\"183\":1},\"1\":{\"5\":1,\"184\":2,\"185\":2,\"186\":1,\"187\":1,\"188\":1}}],[\"e\",{\"0\":{\"733\":1,\"734\":1,\"735\":1},\"1\":{\"3\":1,\"6\":1,\"22\":2,\"24\":1,\"26\":1,\"31\":2,\"40\":1,\"41\":2,\"43\":5,\"46\":1,\"47\":7,\"48\":1,\"49\":1,\"50\":2,\"52\":1,\"59\":1,\"60\":1,\"67\":3,\"70\":1,\"71\":1,\"79\":2,\"80\":1,\"81\":1,\"82\":2,\"85\":1,\"86\":3,\"97\":2,\"98\":1,\"99\":1,\"100\":1,\"102\":1,\"106\":1,\"108\":2,\"117\":1,\"118\":1,\"119\":2,\"126\":3,\"127\":1,\"128\":1,\"134\":3,\"138\":1,\"141\":3,\"142\":1,\"147\":1,\"153\":1,\"162\":9,\"163\":5,\"167\":1,\"168\":2,\"175\":1,\"195\":3,\"197\":5,\"201\":2,\"220\":1,\"223\":1,\"224\":2,\"225\":2,\"233\":1,\"235\":1,\"240\":2,\"242\":6,\"243\":6,\"246\":2,\"252\":1,\"254\":1,\"263\":1,\"267\":4,\"269\":2,\"271\":2,\"273\":1,\"276\":4,\"278\":2,\"280\":2,\"286\":4,\"287\":29,\"288\":2,\"290\":2,\"515\":1,\"516\":1,\"517\":1,\"518\":1,\"520\":2,\"521\":1,\"522\":1,\"524\":1,\"525\":1,\"527\":1,\"535\":3,\"537\":1,\"624\":6,\"649\":1,\"652\":2,\"659\":3,\"691\":2,\"692\":2,\"709\":2,\"710\":2,\"711\":2,\"724\":2,\"725\":2,\"726\":1,\"727\":1,\"728\":2,\"729\":1,\"733\":3,\"734\":2,\"735\":2,\"744\":1,\"756\":1,\"773\":1,\"774\":2,\"780\":2,\"786\":1,\"820\":1,\"821\":2,\"822\":1,\"828\":2,\"829\":3,\"830\":2,\"831\":1,\"846\":1,\"849\":2,\"859\":2,\"866\":2,\"867\":1,\"912\":2,\"921\":1,\"922\":1,\"924\":1,\"939\":1,\"943\":1,\"978\":2,\"1008\":1,\"1053\":2,\"1062\":1,\"1107\":3,\"1117\":2,\"1118\":3,\"1125\":1,\"1130\":2,\"1131\":1,\"1132\":1,\"1136\":1,\"1141\":1,\"1145\":1,\"1155\":1,\"1157\":1,\"1162\":1,\"1176\":1,\"1224\":1,\"1225\":1,\"1232\":1,\"1252\":2,\"1261\":1,\"1264\":1,\"1265\":1,\"1267\":1,\"1268\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1278\":3,\"1279\":1,\"1280\":2,\"1281\":1,\"1283\":2,\"1308\":6,\"1318\":1,\"1334\":2,\"1354\":2,\"1533\":1,\"1610\":1,\"1626\":1,\"1627\":2,\"1628\":1,\"1655\":1,\"1681\":1,\"1683\":1,\"1697\":1,\"1698\":1,\"1716\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"1735\":2,\"1751\":2,\"1759\":2,\"1833\":1,\"1854\":1,\"1856\":1,\"1862\":1,\"1876\":1,\"1878\":4,\"1883\":3,\"1949\":1,\"1958\":1,\"1960\":1,\"1961\":1,\"1992\":2,\"1995\":2,\"2000\":3,\"2001\":3,\"2006\":1,\"2007\":1,\"2039\":1,\"2044\":1,\"2049\":1,\"2129\":2,\"2130\":7,\"2133\":1,\"2136\":1,\"2137\":1,\"2184\":2,\"2188\":1,\"2240\":1,\"2249\":1,\"2253\":1,\"2311\":5,\"2345\":1,\"2354\":1,\"2355\":2,\"2359\":4,\"2413\":2,\"2424\":2,\"2443\":1,\"2448\":2,\"2479\":2}}],[\"ensuring\",{\"1\":{\"2130\":1,\"2480\":1}}],[\"ensures\",{\"1\":{\"760\":1,\"1645\":1,\"2040\":1,\"2162\":1}}],[\"ensure\",{\"1\":{\"200\":2,\"224\":1,\"242\":1,\"249\":1,\"276\":1,\"972\":1,\"1075\":1,\"1478\":1,\"1484\":1,\"1485\":1,\"2000\":1,\"2001\":1,\"2131\":1,\"2184\":1}}],[\"enrollment\",{\"1\":{\"1086\":1,\"1155\":2,\"1158\":2,\"1207\":1,\"1268\":1}}],[\"enroll\",{\"1\":{\"1086\":2,\"1155\":2,\"1158\":4,\"1207\":2,\"1268\":5,\"1274\":6,\"2368\":1}}],[\"enrolldim\",{\"1\":{\"1086\":1,\"1207\":1,\"1340\":1}}],[\"enrique\",{\"1\":{\"10\":1,\"156\":1}}],[\"enforce\",{\"1\":{\"2355\":1}}],[\"enforced\",{\"1\":{\"756\":1,\"773\":1}}],[\"enforcing\",{\"1\":{\"1155\":1,\"1157\":1}}],[\"enffiles\",{\"1\":{\"520\":1}}],[\"enumeration\",{\"1\":{\"801\":1}}],[\"enumerate\",{\"1\":{\"102\":1}}],[\"enum\",{\"1\":{\"755\":1,\"764\":1,\"785\":1,\"801\":1,\"802\":1}}],[\"entire\",{\"1\":{\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"919\":1,\"939\":1,\"1035\":1,\"1113\":1,\"1251\":1,\"1334\":1,\"1720\":1,\"1721\":1}}],[\"entity\",{\"1\":{\"377\":2,\"449\":2}}],[\"entries\",{\"1\":{\"696\":1,\"2132\":3,\"2141\":1,\"2216\":1}}],[\"entry\",{\"1\":{\"109\":1,\"223\":1,\"224\":1,\"696\":1,\"1155\":1,\"1157\":1,\"2141\":1,\"2153\":1}}],[\"entropy\",{\"0\":{\"2073\":1},\"1\":{\"44\":1,\"144\":1,\"1171\":1,\"1839\":1,\"1991\":1,\"2167\":1,\"2176\":1,\"2207\":1,\"2219\":1}}],[\"energy\",{\"0\":{\"2409\":2},\"1\":{\"242\":2,\"286\":1,\"1521\":17,\"1585\":14,\"1598\":10,\"1599\":32,\"1600\":7,\"1627\":3,\"2187\":1,\"2198\":1,\"2228\":17,\"2229\":17,\"2408\":17,\"2409\":4,\"2411\":1,\"2412\":34,\"2413\":3,\"2423\":33,\"2424\":3,\"2446\":17,\"2447\":33,\"2448\":3}}],[\"engelhardt\",{\"1\":{\"202\":1}}],[\"english\",{\"1\":{\"200\":2,\"261\":1,\"285\":1,\"287\":1,\"481\":1}}],[\"engine\",{\"1\":{\"166\":1,\"2040\":1}}],[\"enh+asr\",{\"1\":{\"1156\":1}}],[\"enh2\",{\"1\":{\"520\":1}}],[\"enhancing\",{\"1\":{\"228\":1}}],[\"enhances\",{\"1\":{\"269\":1,\"278\":1}}],[\"enhanced\",{\"1\":{\"223\":2,\"520\":1,\"1126\":2,\"1127\":1,\"1217\":1,\"1253\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1,\"1351\":1,\"1376\":1,\"1377\":1}}],[\"enhance\",{\"1\":{\"198\":1,\"228\":1,\"1032\":1,\"1155\":2,\"1157\":2,\"1158\":1,\"1253\":2}}],[\"enhancementtask\",{\"0\":{\"2253\":1},\"1\":{\"223\":4,\"2253\":1}}],[\"enhancement\",{\"0\":{\"11\":1,\"179\":1,\"222\":1,\"226\":1,\"229\":1,\"230\":1},\"1\":{\"11\":2,\"162\":2,\"179\":1,\"190\":1,\"222\":4,\"223\":11,\"226\":2,\"228\":1,\"1036\":1,\"1042\":1,\"1053\":1,\"1061\":2,\"1062\":2,\"1112\":1,\"1113\":1,\"1125\":1,\"1155\":2,\"1156\":1,\"1157\":3,\"1174\":1,\"1210\":1,\"1250\":1,\"1251\":1,\"1264\":1,\"1276\":1,\"1279\":2,\"1280\":3,\"1281\":3,\"1282\":3,\"1283\":2,\"1334\":2,\"1679\":6,\"2240\":1,\"2346\":1}}],[\"enhs2ttask\",{\"0\":{\"2252\":1},\"1\":{\"228\":1,\"2252\":1}}],[\"enhpreprocessor\",{\"0\":{\"2346\":1},\"1\":{\"223\":1,\"2346\":1,\"2368\":1}}],[\"enh1\",{\"1\":{\"195\":1,\"197\":1,\"223\":2,\"224\":6,\"227\":5,\"520\":1,\"1155\":1,\"1157\":1}}],[\"enh\",{\"0\":{\"223\":1,\"228\":1,\"341\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"1029\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":2,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1050\":1,\"1051\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1107\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1118\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1129\":1,\"1130\":1,\"1131\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1137\":1,\"1139\":1,\"1141\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1147\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1155\":2,\"1156\":2,\"1157\":1,\"1158\":1,\"1159\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1176\":1,\"1177\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1189\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":1,\"1204\":1,\"1205\":1,\"1207\":1,\"1208\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1219\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1228\":1,\"1229\":1,\"1230\":1,\"1232\":1,\"1233\":1,\"1235\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1244\":1,\"1245\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1267\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1291\":1,\"1292\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1297\":1,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1307\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1312\":1,\"1313\":1,\"1314\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1330\":1,\"1331\":1,\"1332\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":1,\"1337\":1,\"1338\":1,\"1339\":1,\"1340\":1,\"1341\":1,\"1342\":1,\"1343\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1351\":1,\"1352\":1,\"1353\":1,\"1354\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1,\"1359\":1,\"1360\":1,\"1361\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"2252\":1,\"2253\":1,\"2272\":1,\"2522\":1},\"1\":{\"162\":1,\"197\":1,\"222\":1,\"223\":29,\"224\":1,\"225\":12,\"227\":14,\"228\":12,\"229\":1,\"230\":1,\"301\":2,\"335\":2,\"342\":3,\"349\":3,\"356\":1,\"361\":1,\"402\":2,\"403\":8,\"463\":2,\"794\":1,\"954\":1,\"974\":1,\"980\":1,\"1029\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":2,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1050\":1,\"1051\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":2,\"1105\":1,\"1107\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1118\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1129\":1,\"1130\":1,\"1131\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1137\":1,\"1139\":1,\"1141\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1147\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1155\":5,\"1156\":6,\"1157\":4,\"1158\":1,\"1159\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1176\":1,\"1177\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1189\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":1,\"1204\":1,\"1205\":1,\"1207\":1,\"1208\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1219\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1228\":1,\"1229\":1,\"1230\":1,\"1232\":1,\"1233\":1,\"1235\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1244\":1,\"1245\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1267\":1,\"1268\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":2,\"1275\":1,\"1276\":1,\"1277\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1291\":1,\"1292\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1297\":1,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1307\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1312\":1,\"1313\":1,\"1314\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1330\":1,\"1331\":1,\"1332\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1336\":1,\"1337\":1,\"1338\":1,\"1339\":1,\"1340\":1,\"1341\":1,\"1342\":1,\"1343\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1351\":1,\"1352\":1,\"1353\":1,\"1354\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1,\"1359\":1,\"1360\":1,\"1361\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"2252\":1,\"2253\":1,\"2272\":1,\"2346\":1}}],[\"en\",{\"0\":{\"2276\":1},\"1\":{\"67\":1,\"126\":3,\"136\":2,\"218\":2,\"242\":1,\"259\":1,\"286\":13,\"287\":2,\"481\":3,\"515\":1,\"535\":1,\"536\":15,\"1672\":1,\"1673\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"2043\":2,\"2276\":4,\"2283\":1,\"2284\":1}}],[\"enough\",{\"1\":{\"67\":1,\"106\":1,\"128\":1,\"173\":1,\"196\":1,\"213\":1,\"268\":1,\"277\":1}}],[\"enc=none\",{\"1\":{\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1}}],[\"encdec\",{\"1\":{\"1389\":17,\"1391\":17,\"1396\":17,\"1401\":17,\"1403\":17,\"1466\":17,\"1468\":17}}],[\"encs\",{\"1\":{\"796\":2,\"869\":1}}],[\"encapsulation\",{\"1\":{\"655\":1}}],[\"encapsulating\",{\"1\":{\"141\":1}}],[\"enc\",{\"0\":{\"1891\":1},\"1\":{\"43\":3,\"44\":1,\"50\":8,\"51\":1,\"52\":1,\"89\":1,\"130\":1,\"141\":3,\"243\":1,\"261\":2,\"290\":4,\"616\":8,\"617\":6,\"618\":6,\"620\":10,\"624\":6,\"625\":1,\"626\":2,\"632\":4,\"636\":4,\"644\":6,\"645\":1,\"661\":4,\"692\":2,\"696\":18,\"697\":15,\"700\":1,\"709\":2,\"710\":4,\"711\":4,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"760\":1,\"765\":1,\"766\":1,\"767\":1,\"774\":2,\"775\":1,\"776\":1,\"780\":2,\"795\":1,\"848\":1,\"849\":2,\"850\":1,\"947\":3,\"949\":3,\"963\":1,\"979\":2,\"1107\":4,\"1124\":14,\"1125\":14,\"1141\":1,\"1159\":2,\"1162\":2,\"1164\":1,\"1252\":2,\"1278\":2,\"1280\":2,\"1283\":2,\"1519\":2,\"1526\":7,\"1598\":7,\"1599\":14,\"1600\":7,\"1638\":1,\"1704\":7,\"1705\":7,\"1706\":7,\"1707\":7,\"1708\":6,\"1709\":4,\"1710\":7,\"1711\":7,\"1712\":7,\"1713\":6,\"1714\":6,\"1715\":6,\"1716\":6,\"1726\":2,\"1727\":2,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1768\":7,\"1779\":3,\"1801\":6,\"1851\":2,\"1863\":3,\"1866\":2,\"1891\":4,\"1914\":3,\"1936\":2,\"1947\":1,\"1957\":2,\"1960\":2,\"1961\":2,\"1992\":6,\"1993\":7,\"1994\":3,\"1995\":6,\"2126\":1,\"2129\":2,\"2191\":2,\"2221\":1,\"2239\":12,\"2240\":12,\"2245\":1,\"2411\":14,\"2412\":14,\"2423\":14,\"2431\":1,\"2432\":12,\"2447\":14}}],[\"encouraged\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1}}],[\"encourage\",{\"1\":{\"262\":1}}],[\"encourages\",{\"1\":{\"127\":1,\"262\":1}}],[\"encountered\",{\"1\":{\"132\":1}}],[\"encounter\",{\"1\":{\"42\":1,\"138\":1,\"210\":1,\"213\":1,\"269\":2,\"278\":2}}],[\"encode\",{\"0\":{\"514\":1,\"1877\":1},\"1\":{\"368\":2,\"514\":2,\"614\":1,\"617\":2,\"618\":2,\"620\":2,\"621\":1,\"624\":2,\"625\":1,\"626\":2,\"634\":2,\"641\":2,\"643\":2,\"651\":1,\"737\":1,\"792\":1,\"847\":2,\"865\":1,\"954\":1,\"958\":2,\"974\":1,\"1156\":2,\"1381\":1,\"1389\":1,\"1391\":1,\"1395\":2,\"1400\":1,\"1401\":1,\"1403\":1,\"1406\":1,\"1408\":1,\"1410\":1,\"1439\":1,\"1441\":3,\"1466\":1,\"1468\":1,\"1469\":1,\"1640\":1,\"1641\":1,\"1702\":1,\"1749\":1,\"1815\":2,\"1877\":1,\"1959\":1,\"1975\":1,\"1996\":2,\"1997\":1,\"2127\":1,\"2130\":6,\"2133\":2,\"2136\":2,\"2184\":1,\"2216\":1,\"2217\":1,\"2221\":1,\"2288\":1,\"2293\":1,\"2356\":1,\"2462\":1}}],[\"encoded\",{\"1\":{\"315\":2,\"469\":2,\"514\":1,\"675\":2,\"692\":4,\"701\":1,\"735\":1,\"759\":1,\"760\":1,\"775\":1,\"790\":2,\"797\":1,\"820\":1,\"850\":5,\"980\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1125\":1,\"1130\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1268\":2,\"1278\":1,\"1381\":1,\"1546\":3,\"1611\":1,\"1622\":1,\"1719\":5,\"1720\":5,\"1721\":3,\"1724\":1,\"1725\":6,\"1731\":3,\"1735\":2,\"1751\":6,\"1759\":1,\"1784\":1,\"1786\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1806\":3,\"1808\":1,\"1818\":1,\"1820\":1,\"1822\":1,\"1837\":1,\"1848\":2,\"1862\":1,\"1966\":1,\"1992\":1,\"2130\":3,\"2136\":3,\"2184\":1,\"2215\":1,\"2219\":1,\"2220\":1,\"2428\":1}}],[\"encodecfrontend\",{\"0\":{\"2230\":1}}],[\"encodecdiscriminator\",{\"0\":{\"1397\":1},\"1\":{\"1397\":1}}],[\"encodec\",{\"0\":{\"1396\":3,\"1397\":2},\"1\":{\"219\":1,\"1396\":4,\"1397\":4,\"1957\":1}}],[\"encoderlayer\",{\"0\":{\"1759\":1},\"1\":{\"1735\":1,\"1759\":2,\"1864\":1,\"1867\":1}}],[\"encoders\",{\"0\":{\"1814\":1,\"1816\":1,\"1878\":1,\"1918\":1},\"1\":{\"223\":1,\"225\":1,\"262\":1,\"1119\":2,\"1368\":3,\"1683\":1,\"1814\":1,\"1816\":1,\"1878\":1,\"1918\":1,\"2133\":1}}],[\"encoder\",{\"0\":{\"141\":1,\"150\":1,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"626\":3,\"636\":1,\"644\":1,\"645\":1,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"671\":1,\"672\":1,\"673\":1,\"674\":2,\"675\":2,\"678\":2,\"693\":2,\"698\":2,\"699\":2,\"700\":2,\"701\":2,\"709\":2,\"710\":2,\"711\":2,\"733\":2,\"734\":2,\"735\":2,\"745\":2,\"746\":2,\"747\":2,\"748\":2,\"754\":2,\"756\":2,\"761\":2,\"771\":2,\"774\":2,\"780\":2,\"787\":2,\"791\":2,\"798\":2,\"805\":2,\"807\":2,\"825\":2,\"839\":2,\"841\":2,\"846\":2,\"849\":2,\"851\":2,\"862\":2,\"885\":2,\"889\":2,\"890\":2,\"891\":2,\"893\":2,\"894\":2,\"900\":2,\"901\":2,\"902\":2,\"910\":2,\"911\":2,\"915\":2,\"927\":2,\"941\":2,\"1034\":2,\"1113\":2,\"1159\":1,\"1223\":2,\"1251\":2,\"1341\":1,\"1387\":1,\"1398\":1,\"1424\":1,\"1442\":1,\"1444\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1462\":1,\"1464\":1,\"1476\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1493\":1,\"1494\":1,\"1500\":1,\"1503\":1,\"1546\":1,\"1611\":1,\"1622\":1,\"1735\":1,\"1758\":2,\"1759\":1,\"1878\":1,\"1879\":2,\"1936\":1,\"2187\":2,\"2188\":2,\"2191\":2,\"2192\":2,\"2196\":2,\"2198\":2,\"2202\":2,\"2203\":2,\"2209\":2,\"2213\":2,\"2214\":2,\"2227\":2,\"2231\":2,\"2243\":2,\"2421\":1,\"2425\":1,\"2429\":1,\"2430\":1},\"1\":{\"43\":4,\"44\":2,\"50\":4,\"51\":3,\"52\":4,\"53\":1,\"88\":2,\"89\":2,\"128\":2,\"130\":4,\"140\":1,\"141\":10,\"142\":1,\"143\":1,\"147\":2,\"150\":2,\"223\":6,\"225\":9,\"240\":1,\"243\":2,\"260\":2,\"261\":1,\"262\":11,\"616\":4,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":9,\"626\":16,\"632\":5,\"636\":5,\"639\":3,\"644\":1,\"645\":1,\"655\":4,\"656\":2,\"657\":2,\"658\":1,\"659\":2,\"660\":2,\"661\":4,\"662\":1,\"667\":3,\"671\":6,\"672\":1,\"673\":5,\"674\":14,\"675\":4,\"678\":2,\"686\":1,\"692\":3,\"693\":2,\"696\":7,\"697\":6,\"698\":2,\"699\":4,\"700\":3,\"701\":3,\"706\":3,\"709\":8,\"710\":3,\"711\":4,\"731\":1,\"732\":1,\"733\":5,\"734\":3,\"735\":3,\"736\":3,\"737\":5,\"738\":2,\"745\":4,\"746\":9,\"747\":4,\"748\":5,\"754\":2,\"756\":2,\"760\":7,\"761\":2,\"766\":1,\"767\":1,\"768\":3,\"770\":1,\"771\":3,\"774\":8,\"775\":1,\"777\":11,\"780\":8,\"787\":3,\"790\":2,\"791\":3,\"796\":1,\"797\":1,\"798\":2,\"805\":2,\"807\":2,\"820\":4,\"825\":2,\"829\":1,\"831\":1,\"839\":2,\"841\":2,\"846\":36,\"848\":1,\"849\":5,\"850\":2,\"851\":4,\"862\":2,\"864\":1,\"885\":2,\"889\":2,\"890\":2,\"891\":2,\"893\":2,\"894\":2,\"900\":2,\"901\":2,\"902\":2,\"910\":2,\"911\":2,\"915\":2,\"927\":2,\"941\":2,\"947\":2,\"949\":2,\"954\":3,\"955\":1,\"958\":1,\"959\":1,\"974\":3,\"977\":1,\"979\":2,\"1034\":2,\"1053\":2,\"1107\":3,\"1113\":4,\"1118\":1,\"1124\":7,\"1125\":7,\"1155\":4,\"1156\":11,\"1157\":4,\"1158\":3,\"1159\":1,\"1162\":1,\"1223\":3,\"1251\":3,\"1252\":2,\"1280\":1,\"1283\":1,\"1341\":2,\"1368\":1,\"1387\":1,\"1391\":2,\"1398\":1,\"1403\":2,\"1410\":2,\"1424\":1,\"1442\":1,\"1444\":1,\"1450\":1,\"1452\":1,\"1454\":4,\"1456\":4,\"1458\":1,\"1462\":1,\"1464\":1,\"1468\":2,\"1476\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1493\":1,\"1494\":1,\"1500\":1,\"1503\":1,\"1519\":1,\"1526\":3,\"1535\":1,\"1536\":1,\"1546\":5,\"1552\":68,\"1553\":21,\"1598\":3,\"1599\":19,\"1600\":3,\"1601\":3,\"1602\":4,\"1611\":3,\"1622\":5,\"1625\":21,\"1626\":69,\"1640\":3,\"1641\":3,\"1683\":1,\"1702\":2,\"1704\":4,\"1705\":4,\"1706\":4,\"1707\":4,\"1708\":4,\"1709\":4,\"1710\":4,\"1711\":4,\"1712\":4,\"1713\":4,\"1714\":4,\"1715\":4,\"1716\":4,\"1719\":1,\"1720\":1,\"1721\":4,\"1723\":1,\"1724\":1,\"1725\":1,\"1731\":2,\"1735\":2,\"1736\":1,\"1750\":2,\"1752\":1,\"1758\":10,\"1759\":2,\"1768\":4,\"1779\":5,\"1787\":2,\"1801\":3,\"1805\":1,\"1806\":1,\"1814\":1,\"1816\":1,\"1822\":1,\"1851\":1,\"1863\":2,\"1864\":1,\"1865\":1,\"1867\":2,\"1878\":4,\"1879\":3,\"1891\":1,\"1895\":1,\"1913\":1,\"1934\":1,\"1936\":5,\"1937\":1,\"1944\":2,\"1945\":1,\"1946\":1,\"1947\":2,\"1959\":5,\"1974\":3,\"1975\":3,\"1977\":5,\"1992\":2,\"1995\":2,\"1996\":3,\"1997\":4,\"2127\":4,\"2129\":1,\"2133\":9,\"2184\":4,\"2187\":4,\"2188\":3,\"2191\":9,\"2192\":4,\"2196\":2,\"2198\":4,\"2202\":2,\"2203\":4,\"2209\":3,\"2213\":2,\"2214\":2,\"2215\":7,\"2216\":2,\"2218\":2,\"2219\":8,\"2220\":2,\"2221\":7,\"2223\":1,\"2224\":1,\"2227\":6,\"2231\":10,\"2235\":4,\"2236\":4,\"2239\":31,\"2240\":16,\"2243\":3,\"2245\":5,\"2355\":2,\"2411\":17,\"2412\":19,\"2421\":1,\"2423\":19,\"2425\":10,\"2429\":10,\"2430\":1,\"2431\":5,\"2432\":18,\"2447\":19}}],[\"encoding=none\",{\"1\":{\"2480\":1}}],[\"encoding=\",{\"1\":{\"2249\":1}}],[\"encodings\",{\"1\":{\"1784\":2,\"1808\":1,\"1818\":1,\"1837\":1}}],[\"encoding\",{\"0\":{\"645\":1,\"662\":1},\"1\":{\"43\":2,\"130\":1,\"141\":2,\"260\":1,\"262\":4,\"514\":1,\"644\":2,\"645\":4,\"661\":2,\"662\":4,\"699\":1,\"709\":4,\"710\":2,\"711\":2,\"774\":4,\"780\":4,\"849\":1,\"1107\":2,\"1278\":2,\"1389\":2,\"1391\":1,\"1395\":2,\"1401\":2,\"1403\":1,\"1406\":1,\"1408\":2,\"1410\":1,\"1466\":2,\"1468\":1,\"1519\":1,\"1535\":4,\"1536\":4,\"1546\":4,\"1552\":4,\"1553\":1,\"1599\":9,\"1622\":4,\"1625\":1,\"1626\":4,\"1677\":1,\"1678\":2,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1749\":3,\"1784\":3,\"1785\":3,\"1786\":2,\"1808\":2,\"1817\":3,\"1818\":2,\"1820\":2,\"1837\":2,\"1851\":1,\"1863\":3,\"1866\":1,\"1877\":1,\"1891\":3,\"1957\":1,\"1960\":1,\"1961\":1,\"2129\":1,\"2130\":4,\"2133\":3,\"2136\":5,\"2191\":4,\"2239\":8,\"2240\":7,\"2411\":9,\"2412\":9,\"2423\":9,\"2432\":7,\"2447\":9}}],[\"enables\",{\"1\":{\"107\":1,\"286\":1,\"2131\":1}}],[\"enabled\",{\"1\":{\"44\":1,\"51\":1,\"144\":1,\"377\":2,\"449\":2,\"846\":3,\"2220\":2,\"2355\":1}}],[\"enable\",{\"0\":{\"57\":1},\"1\":{\"41\":1,\"78\":1,\"92\":1,\"101\":1,\"118\":1,\"131\":1,\"139\":2,\"146\":1,\"200\":1,\"286\":1,\"290\":1,\"929\":1,\"1645\":1,\"1650\":1,\"2000\":1,\"2176\":1}}],[\"envfile\",{\"1\":{\"374\":2}}],[\"envs\",{\"1\":{\"31\":3}}],[\"env\",{\"1\":{\"22\":2,\"24\":1,\"31\":2,\"40\":1,\"67\":1,\"117\":1,\"126\":1,\"162\":2,\"163\":1,\"243\":1,\"2340\":2}}],[\"environments\",{\"1\":{\"160\":1,\"1327\":1,\"1330\":1,\"1717\":1}}],[\"environment\",{\"1\":{\"1\":2,\"18\":1,\"22\":1,\"24\":1,\"26\":1,\"31\":1,\"37\":1,\"59\":1,\"67\":3,\"109\":1,\"110\":2,\"121\":1,\"152\":1,\"159\":1,\"162\":5,\"163\":1,\"165\":1,\"249\":1}}],[\"endless\",{\"1\":{\"2134\":2}}],[\"end=none\",{\"1\":{\"2480\":1}}],[\"end=np\",{\"1\":{\"1880\":1}}],[\"end=false\",{\"1\":{\"1093\":1,\"1233\":1}}],[\"ended\",{\"1\":{\"1719\":4,\"1720\":1,\"1721\":1,\"1725\":4,\"1806\":3,\"1880\":2}}],[\"endb2\",{\"1\":{\"268\":1,\"277\":1}}],[\"endb1\",{\"1\":{\"268\":1,\"277\":1}}],[\"enda2\",{\"1\":{\"268\":1,\"277\":1}}],[\"enda1\",{\"1\":{\"268\":1,\"277\":1}}],[\"endtime2\",{\"1\":{\"242\":2}}],[\"endtime1\",{\"1\":{\"242\":2}}],[\"endine\",{\"1\":{\"166\":1}}],[\"ending\",{\"1\":{\"3\":1}}],[\"endian\",{\"1\":{\"70\":2}}],[\"end\",{\"0\":{\"1880\":1,\"2022\":2,\"2023\":2,\"2024\":2,\"2025\":2,\"2028\":2,\"2036\":2,\"2037\":2,\"2038\":2,\"2041\":2,\"2042\":2,\"2046\":2,\"2047\":2,\"2048\":2,\"2050\":2,\"2051\":2,\"2052\":2,\"2053\":2,\"2054\":2,\"2057\":2,\"2058\":2,\"2059\":2,\"2060\":2,\"2061\":2,\"2062\":2,\"2063\":2,\"2064\":2,\"2067\":2,\"2068\":2,\"2069\":2,\"2070\":2,\"2071\":2,\"2072\":2,\"2073\":2,\"2074\":2,\"2075\":2,\"2076\":2,\"2077\":2,\"2078\":2,\"2079\":2,\"2080\":2,\"2081\":2,\"2082\":2,\"2083\":2,\"2084\":2,\"2085\":2,\"2086\":2,\"2087\":2,\"2088\":2,\"2089\":2,\"2090\":2,\"2091\":2,\"2092\":2,\"2093\":2,\"2094\":2,\"2095\":2,\"2096\":2,\"2097\":2,\"2098\":2,\"2099\":2,\"2100\":2,\"2102\":2,\"2103\":2,\"2104\":2,\"2105\":2,\"2106\":2,\"2107\":2,\"2108\":2,\"2109\":2,\"2110\":2,\"2111\":2,\"2112\":2,\"2113\":2,\"2114\":2,\"2115\":2,\"2116\":2,\"2117\":2,\"2118\":2,\"2119\":2,\"2120\":2,\"2121\":2,\"2122\":2,\"2123\":2},\"1\":{\"9\":2,\"11\":2,\"13\":2,\"87\":1,\"134\":3,\"135\":2,\"136\":1,\"156\":2,\"196\":3,\"207\":2,\"211\":2,\"228\":2,\"232\":2,\"245\":2,\"246\":2,\"258\":2,\"260\":1,\"262\":7,\"266\":2,\"268\":4,\"269\":1,\"275\":2,\"277\":4,\"278\":1,\"284\":1,\"286\":3,\"287\":4,\"290\":2,\"543\":1,\"691\":2,\"706\":1,\"768\":4,\"846\":1,\"852\":2,\"960\":2,\"961\":2,\"1000\":3,\"1002\":3,\"1025\":3,\"1028\":1,\"1063\":1,\"1126\":2,\"1130\":3,\"1185\":2,\"1224\":1,\"1225\":1,\"1245\":1,\"1316\":1,\"1320\":1,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1419\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1466\":1,\"1484\":1,\"1526\":2,\"1546\":2,\"1553\":4,\"1598\":4,\"1600\":2,\"1611\":2,\"1612\":2,\"1616\":2,\"1622\":2,\"1625\":4,\"1626\":2,\"1719\":1,\"1720\":1,\"1721\":2,\"1725\":2,\"1729\":2,\"1730\":3,\"1731\":1,\"1768\":2,\"1806\":1,\"1862\":2,\"1880\":5,\"2028\":2,\"2044\":4,\"2054\":4,\"2061\":3,\"2088\":2,\"2095\":2,\"2096\":2,\"2102\":2,\"2115\":2,\"2116\":2,\"2130\":1,\"2136\":2,\"2155\":1,\"2162\":1,\"2223\":2,\"2227\":2,\"2231\":4,\"2245\":6,\"2333\":1,\"2355\":2,\"2367\":1,\"2411\":2,\"2412\":2,\"2425\":2,\"2429\":2,\"2430\":2,\"2431\":2,\"2433\":2,\"2434\":1}}],[\"ends\",{\"1\":{\"8\":1,\"960\":1,\"961\":2,\"2333\":1}}],[\"exmpale\",{\"1\":{\"276\":4}}],[\"exists\",{\"0\":{\"1675\":1},\"1\":{\"1675\":1}}],[\"existence\",{\"1\":{\"286\":1}}],[\"exist\",{\"1\":{\"138\":1,\"162\":1,\"1927\":1,\"2000\":1,\"2134\":1}}],[\"existing\",{\"1\":{\"32\":1,\"60\":1,\"150\":1,\"168\":1,\"190\":1,\"243\":1,\"1279\":1,\"2354\":1}}],[\"exit\",{\"1\":{\"126\":1,\"134\":1,\"243\":1}}],[\"exact\",{\"1\":{\"259\":2}}],[\"exactly\",{\"1\":{\"71\":1,\"107\":1,\"211\":1,\"213\":1}}],[\"examples\",{\"0\":{\"55\":1,\"986\":1,\"988\":1,\"990\":1,\"992\":1,\"994\":1,\"997\":1,\"999\":1,\"1002\":1,\"1004\":1,\"1006\":1,\"1008\":1,\"1010\":1,\"1012\":1,\"1014\":1,\"1016\":1,\"1018\":1,\"1020\":1,\"1023\":1,\"1025\":1,\"1027\":1,\"1644\":1,\"1647\":1,\"1774\":1,\"1805\":1,\"1822\":1,\"1827\":1,\"1830\":1,\"1846\":1,\"1856\":1,\"1882\":1,\"1890\":1,\"1902\":1,\"1904\":1,\"1906\":1,\"1909\":1,\"1912\":1,\"1933\":1,\"1956\":1,\"2290\":1,\"2299\":1,\"2305\":1,\"2315\":1,\"2343\":1,\"2352\":1,\"2377\":1,\"2479\":1,\"2485\":1,\"2488\":1,\"2493\":1,\"2500\":1,\"2502\":1,\"2505\":1},\"1\":{\"28\":1,\"60\":1,\"107\":1,\"150\":1,\"180\":1,\"190\":4,\"269\":1,\"278\":1,\"1000\":1,\"1945\":1,\"2124\":1,\"2128\":1,\"2132\":2,\"2151\":1,\"2310\":1,\"2344\":1,\"2355\":2,\"2462\":1}}],[\"example\",{\"0\":{\"38\":1,\"136\":1},\"1\":{\"22\":1,\"24\":1,\"26\":2,\"31\":1,\"38\":2,\"50\":2,\"51\":3,\"67\":1,\"69\":1,\"70\":1,\"79\":1,\"84\":1,\"98\":1,\"101\":1,\"107\":1,\"110\":1,\"111\":1,\"126\":1,\"128\":2,\"130\":1,\"135\":1,\"141\":1,\"142\":2,\"162\":3,\"168\":2,\"174\":1,\"175\":1,\"190\":1,\"195\":1,\"211\":3,\"212\":1,\"223\":1,\"234\":1,\"236\":1,\"242\":3,\"243\":2,\"259\":1,\"261\":2,\"262\":5,\"267\":2,\"270\":1,\"271\":1,\"272\":1,\"276\":1,\"279\":1,\"280\":1,\"282\":1,\"286\":7,\"287\":1,\"288\":1,\"289\":2,\"290\":8,\"523\":1,\"526\":1,\"527\":7,\"536\":8,\"724\":2,\"725\":2,\"726\":1,\"727\":1,\"728\":2,\"729\":1,\"744\":2,\"784\":1,\"786\":1,\"800\":1,\"828\":2,\"829\":3,\"830\":2,\"831\":1,\"859\":1,\"867\":1,\"921\":1,\"935\":1,\"960\":1,\"1604\":1,\"1609\":1,\"1655\":1,\"1731\":1,\"1901\":1,\"1903\":1,\"2130\":2,\"2131\":1,\"2134\":3,\"2249\":1,\"2262\":1,\"2287\":1,\"2325\":1,\"2327\":1,\"2334\":1,\"2345\":1,\"2355\":3,\"2359\":2,\"2480\":1}}],[\"exhausted\",{\"1\":{\"66\":1,\"2134\":1}}],[\"exceed\",{\"1\":{\"2146\":1}}],[\"exceeding\",{\"1\":{\"2145\":1}}],[\"exceeds\",{\"1\":{\"2039\":1,\"2134\":1}}],[\"exception\",{\"1\":{\"653\":1,\"760\":1,\"1842\":1}}],[\"except\",{\"1\":{\"45\":1,\"102\":1,\"145\":1,\"162\":1,\"224\":1,\"242\":1,\"747\":1,\"980\":1,\"1292\":1,\"1599\":2,\"1645\":1,\"1650\":1,\"1962\":1,\"2017\":1,\"2019\":1,\"2239\":2,\"2240\":2,\"2304\":1,\"2411\":2,\"2412\":2,\"2423\":2,\"2432\":2,\"2447\":2}}],[\"excitation\",{\"1\":{\"1548\":1}}],[\"excitation=none\",{\"1\":{\"1548\":2}}],[\"exctraction\",{\"1\":{\"276\":1}}],[\"exclusive\",{\"1\":{\"1655\":1}}],[\"excluding\",{\"1\":{\"88\":2,\"290\":2,\"2130\":3}}],[\"exclude\",{\"1\":{\"290\":1,\"572\":1,\"787\":1,\"2249\":1}}],[\"exclude=$\",{\"1\":{\"32\":1}}],[\"excluded\",{\"1\":{\"3\":1,\"1643\":1,\"1646\":1}}],[\"excludes\",{\"1\":{\"3\":1}}],[\"excl\",{\"1\":{\"142\":1}}],[\"extact\",{\"1\":{\"2434\":1}}],[\"exteded\",{\"1\":{\"1731\":1}}],[\"extended\",{\"1\":{\"616\":1,\"628\":1,\"743\":1,\"1711\":1,\"1712\":1,\"1720\":2,\"1721\":2,\"1729\":1,\"1730\":1,\"1762\":1,\"1920\":1}}],[\"extendedhypothesis\",{\"0\":{\"628\":1,\"743\":1,\"1762\":1},\"1\":{\"616\":3,\"628\":1,\"696\":6,\"697\":6,\"743\":1,\"847\":1,\"1749\":1,\"1762\":1,\"1815\":1,\"1843\":1,\"1920\":2,\"1927\":3}}],[\"extend\",{\"0\":{\"2077\":1},\"1\":{\"32\":2,\"645\":1,\"1002\":1,\"1720\":2,\"1721\":2,\"1726\":1,\"1727\":1,\"1730\":3,\"1731\":4,\"1784\":1,\"1808\":1,\"1818\":1,\"1837\":1}}],[\"extending\",{\"1\":{\"9\":1,\"756\":1,\"773\":1}}],[\"extensions\",{\"1\":{\"159\":1,\"2184\":1}}],[\"extension\",{\"0\":{\"1763\":1},\"1\":{\"18\":1,\"19\":1,\"284\":1,\"289\":1,\"518\":1,\"1155\":1,\"1157\":1,\"1731\":2,\"1763\":3,\"2184\":1}}],[\"ext\",{\"0\":{\"1630\":1},\"1\":{\"518\":2}}],[\"extreme\",{\"1\":{\"927\":1}}],[\"extremely\",{\"1\":{\"242\":1}}],[\"extra\",{\"0\":{\"1484\":1},\"1\":{\"162\":1,\"164\":1,\"223\":2,\"242\":1,\"515\":2,\"733\":1,\"1064\":3,\"1078\":3,\"1153\":3,\"1202\":3,\"1262\":3,\"1290\":3,\"1484\":2,\"1493\":1,\"1494\":1,\"1656\":3,\"1660\":3,\"1662\":3,\"1664\":3,\"1665\":3,\"1669\":3,\"1670\":3,\"1671\":3,\"2221\":3,\"2232\":3,\"2235\":1,\"2236\":1,\"2238\":3}}],[\"extras\",{\"1\":{\"161\":2}}],[\"extractfile\",{\"1\":{\"1948\":1}}],[\"extracts\",{\"1\":{\"254\":1,\"276\":1,\"1720\":1,\"1721\":1,\"2133\":1,\"2187\":1,\"2192\":1,\"2198\":1,\"2203\":1,\"2209\":1,\"2354\":1}}],[\"extracting\",{\"1\":{\"197\":1,\"266\":1,\"275\":1}}],[\"extraction\",{\"1\":{\"38\":1,\"106\":1,\"197\":1,\"234\":1,\"235\":2,\"253\":1,\"254\":1,\"267\":2,\"274\":1,\"275\":3,\"276\":1,\"285\":1,\"286\":4,\"377\":1,\"449\":1,\"457\":1,\"846\":1,\"1155\":1,\"1158\":1,\"1702\":1,\"2130\":1,\"2133\":2,\"2184\":3,\"2220\":1,\"2368\":1}}],[\"extractor\",{\"0\":{\"1040\":2,\"1268\":2},\"1\":{\"161\":1,\"639\":3,\"691\":1,\"702\":1,\"846\":8,\"1040\":2,\"1158\":1,\"1268\":3,\"2184\":1,\"2404\":1,\"2409\":1}}],[\"extracted\",{\"1\":{\"106\":1,\"205\":1,\"233\":1,\"254\":2,\"273\":1,\"276\":3,\"286\":1,\"746\":1,\"1086\":1,\"1207\":1,\"1637\":1,\"1806\":1,\"2354\":1}}],[\"extract\",{\"0\":{\"73\":1,\"74\":1,\"449\":1,\"496\":1,\"898\":1,\"1560\":1,\"1561\":1,\"1972\":2,\"1978\":1,\"1980\":1,\"1982\":1,\"2232\":2,\"2233\":2,\"2238\":2,\"2244\":2,\"2401\":2,\"2404\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2434\":1,\"2436\":1,\"2437\":1,\"2438\":1,\"2439\":1,\"2440\":1,\"2443\":2,\"2449\":1},\"1\":{\"73\":1,\"190\":1,\"217\":1,\"235\":1,\"243\":2,\"267\":20,\"268\":1,\"275\":1,\"276\":12,\"277\":1,\"284\":1,\"285\":4,\"286\":8,\"377\":2,\"449\":1,\"496\":1,\"614\":1,\"625\":3,\"641\":1,\"651\":1,\"675\":1,\"699\":2,\"736\":1,\"737\":1,\"777\":1,\"847\":1,\"898\":1,\"1155\":1,\"1157\":2,\"1158\":1,\"1521\":8,\"1560\":1,\"1561\":1,\"1585\":3,\"1637\":1,\"1702\":5,\"1749\":1,\"1806\":1,\"1815\":1,\"1843\":1,\"1921\":1,\"1948\":1,\"1959\":1,\"1972\":2,\"1975\":2,\"1978\":1,\"1980\":1,\"1982\":1,\"1993\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2133\":1,\"2160\":1,\"2184\":4,\"2216\":1,\"2221\":1,\"2228\":8,\"2229\":8,\"2232\":2,\"2233\":2,\"2238\":2,\"2239\":1,\"2240\":1,\"2244\":2,\"2245\":1,\"2298\":1,\"2354\":5,\"2365\":1,\"2401\":2,\"2404\":1,\"2408\":3,\"2409\":1,\"2411\":1,\"2412\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2434\":1,\"2436\":1,\"2437\":1,\"2438\":1,\"2439\":1,\"2440\":1,\"2443\":2,\"2446\":3,\"2449\":1}}],[\"exe\",{\"1\":{\"31\":1}}],[\"executated\",{\"1\":{\"267\":1,\"276\":1}}],[\"executable\",{\"1\":{\"3\":1,\"37\":2}}],[\"executing\",{\"1\":{\"110\":1,\"1951\":1}}],[\"execution\",{\"0\":{\"38\":1}}],[\"executed\",{\"1\":{\"22\":1,\"24\":1,\"195\":1}}],[\"executes\",{\"1\":{\"3\":1,\"2131\":1}}],[\"execute\",{\"0\":{\"22\":1},\"1\":{\"1\":1,\"22\":2,\"28\":2,\"38\":2,\"41\":1,\"127\":1,\"243\":2,\"285\":1,\"1726\":1,\"1727\":1}}],[\"expressivetacotron\",{\"1\":{\"1977\":2}}],[\"expire\",{\"1\":{\"1400\":1}}],[\"expired\",{\"1\":{\"71\":1}}],[\"expiration\",{\"1\":{\"1400\":1,\"1441\":1,\"1469\":1}}],[\"exponent\",{\"1\":{\"1250\":1,\"1251\":1}}],[\"exponentially\",{\"1\":{\"2018\":1}}],[\"exponentialdecaywarmup\",{\"0\":{\"2015\":1},\"1\":{\"2015\":1}}],[\"exponential\",{\"0\":{\"897\":1,\"2015\":1},\"1\":{\"637\":1,\"764\":1,\"897\":1,\"1400\":2,\"1441\":2,\"1456\":1,\"1469\":2,\"2015\":2,\"2018\":2}}],[\"export\",{\"0\":{\"372\":1,\"511\":1},\"1\":{\"22\":1,\"24\":7,\"60\":1,\"92\":3,\"224\":1,\"372\":2,\"511\":2}}],[\"exporting\",{\"0\":{\"16\":1}}],[\"exploding\",{\"1\":{\"1224\":1}}],[\"exploration\",{\"1\":{\"190\":1,\"207\":1}}],[\"explained\",{\"1\":{\"206\":1,\"212\":1,\"218\":1,\"255\":1,\"267\":1,\"276\":1,\"286\":1}}],[\"explanations\",{\"1\":{\"786\":1,\"866\":1,\"921\":1,\"922\":1}}],[\"explanation\",{\"1\":{\"146\":1,\"191\":1,\"192\":1,\"193\":3,\"1000\":1}}],[\"explicit\",{\"1\":{\"268\":1,\"277\":1,\"287\":2,\"481\":1,\"2277\":1,\"2480\":2}}],[\"explicitely\",{\"1\":{\"101\":1}}],[\"explicitly\",{\"1\":{\"79\":1}}],[\"experience\",{\"1\":{\"248\":1}}],[\"experimenting\",{\"1\":{\"262\":1}}],[\"experiment\",{\"0\":{\"124\":1},\"1\":{\"38\":1,\"41\":1,\"107\":3,\"120\":1,\"218\":1,\"246\":1,\"262\":1,\"267\":1,\"276\":1,\"286\":1,\"2355\":1}}],[\"experiments\",{\"1\":{\"24\":1,\"38\":1,\"39\":1,\"111\":1,\"139\":1,\"160\":1,\"223\":2}}],[\"experimental\",{\"1\":{\"24\":1,\"818\":1,\"819\":1}}],[\"expects\",{\"1\":{\"1462\":1}}],[\"expect\",{\"1\":{\"49\":1,\"50\":1,\"51\":1,\"197\":1}}],[\"expected\",{\"1\":{\"46\":1,\"52\":1,\"81\":1,\"145\":1,\"235\":1,\"242\":2,\"615\":1,\"616\":1,\"640\":1,\"648\":1,\"798\":1,\"821\":1,\"862\":1,\"1011\":1,\"1119\":1,\"2130\":2,\"2188\":1}}],[\"expense\",{\"1\":{\"45\":1,\"145\":1,\"1327\":1,\"1330\":1}}],[\"expands\",{\"1\":{\"1788\":1}}],[\"expand\",{\"0\":{\"1559\":2,\"2244\":1},\"1\":{\"1356\":1,\"1529\":2,\"1552\":5,\"1553\":2,\"1559\":4,\"1625\":1,\"1626\":2,\"1794\":4,\"2239\":1,\"2244\":2}}],[\"expand=2\",{\"1\":{\"744\":1}}],[\"expand=none\",{\"1\":{\"726\":1}}],[\"expand=1\",{\"1\":{\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"860\":1,\"892\":1,\"945\":1}}],[\"expandedtokenembedding\",{\"0\":{\"741\":1},\"1\":{\"741\":1}}],[\"expanded\",{\"1\":{\"45\":1,\"145\":1,\"616\":1,\"632\":2,\"696\":1,\"697\":1,\"1552\":2,\"1590\":2,\"1601\":2,\"1602\":2,\"1626\":2,\"1779\":2}}],[\"expansions\",{\"0\":{\"1920\":1},\"1\":{\"45\":1,\"139\":1,\"145\":1,\"616\":3,\"625\":1,\"627\":1,\"696\":1,\"697\":1,\"1920\":3}}],[\"expansion\",{\"1\":{\"45\":6,\"139\":1,\"145\":5,\"616\":9,\"693\":1,\"696\":7,\"697\":7,\"1552\":1,\"1626\":1,\"1920\":2,\"2177\":1,\"2181\":1}}],[\"expname\",{\"1\":{\"39\":1}}],[\"expdir\",{\"1\":{\"39\":1,\"223\":3,\"267\":2,\"286\":5}}],[\"exp\",{\"0\":{\"917\":1,\"918\":1,\"931\":1},\"1\":{\"39\":1,\"45\":2,\"50\":1,\"113\":1,\"114\":2,\"115\":1,\"124\":8,\"136\":2,\"145\":1,\"212\":2,\"218\":1,\"223\":3,\"243\":6,\"267\":11,\"276\":8,\"286\":44,\"516\":1,\"523\":1,\"524\":1,\"525\":1,\"526\":2,\"537\":1,\"616\":2,\"696\":2,\"697\":2,\"706\":1,\"823\":1,\"824\":1,\"918\":1,\"931\":2,\"2015\":1}}],[\"rhythm\",{\"1\":{\"2240\":1}}],[\"rho\",{\"1\":{\"86\":1}}],[\"rho=0\",{\"1\":{\"86\":1}}],[\"rl\",{\"1\":{\"2239\":3}}],[\"rq\",{\"1\":{\"2216\":1}}],[\"r+\",{\"1\":{\"1824\":1}}],[\"rng=<module\",{\"1\":{\"1701\":1}}],[\"rnns\",{\"1\":{\"1855\":1}}],[\"rnnseparator\",{\"0\":{\"1232\":1},\"1\":{\"1232\":1}}],[\"rnnp\",{\"0\":{\"1816\":1},\"1\":{\"1816\":3}}],[\"rnnattractor\",{\"0\":{\"979\":1},\"1\":{\"979\":1}}],[\"rnnencoder\",{\"0\":{\"798\":1},\"1\":{\"798\":2}}],[\"rnntnumba\",{\"0\":{\"867\":1},\"1\":{\"866\":1,\"867\":2}}],[\"rnntlossnumba\",{\"0\":{\"800\":1},\"1\":{\"800\":2}}],[\"rnntstatus\",{\"0\":{\"801\":1},\"1\":{\"703\":1,\"755\":1,\"785\":1,\"801\":1,\"908\":1}}],[\"rnnt\",{\"0\":{\"703\":2,\"704\":1,\"716\":2,\"717\":2,\"755\":2,\"764\":1,\"773\":2,\"785\":2,\"786\":2,\"800\":2,\"801\":1,\"802\":1,\"803\":1,\"866\":2,\"867\":2,\"868\":2,\"873\":2,\"874\":2,\"875\":2,\"876\":2,\"878\":2,\"879\":2,\"880\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"886\":2,\"888\":2,\"896\":1,\"897\":2,\"899\":2,\"908\":2,\"909\":2,\"916\":2,\"918\":2,\"919\":2,\"920\":2,\"921\":3,\"922\":3,\"923\":2,\"931\":1,\"933\":1,\"935\":3,\"936\":3,\"937\":3,\"940\":1,\"946\":1},\"1\":{\"703\":4,\"704\":1,\"716\":3,\"717\":3,\"755\":5,\"764\":1,\"773\":2,\"785\":4,\"786\":3,\"800\":3,\"801\":3,\"802\":1,\"803\":1,\"866\":2,\"867\":2,\"868\":2,\"873\":2,\"874\":2,\"875\":2,\"876\":2,\"878\":5,\"879\":5,\"880\":2,\"881\":5,\"882\":5,\"883\":5,\"884\":5,\"886\":2,\"888\":2,\"896\":1,\"897\":2,\"899\":2,\"908\":2,\"909\":2,\"916\":2,\"918\":2,\"919\":3,\"920\":2,\"921\":3,\"922\":4,\"923\":2,\"931\":1,\"933\":1,\"935\":3,\"936\":4,\"937\":4,\"940\":1,\"946\":1}}],[\"rnndecoder\",{\"0\":{\"641\":1,\"796\":1,\"1815\":1},\"1\":{\"641\":2,\"796\":1,\"1815\":1}}],[\"rnnlm\",{\"1\":{\"53\":1,\"526\":2,\"527\":1,\"631\":1,\"1945\":1}}],[\"rnn\",{\"0\":{\"641\":1,\"796\":1,\"798\":1,\"862\":1,\"869\":1,\"979\":1,\"1232\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1768\":1,\"1801\":1,\"1814\":2,\"1815\":1,\"1816\":1,\"1860\":1,\"1861\":1,\"1878\":1,\"1895\":1,\"1918\":2,\"1945\":1,\"2235\":2,\"2236\":2,\"2237\":2},\"1\":{\"43\":4,\"46\":1,\"52\":4,\"138\":1,\"139\":1,\"142\":10,\"223\":1,\"225\":1,\"265\":2,\"267\":8,\"272\":1,\"274\":1,\"276\":4,\"282\":1,\"527\":4,\"631\":1,\"641\":7,\"724\":2,\"725\":2,\"726\":1,\"727\":1,\"728\":2,\"729\":1,\"744\":2,\"784\":1,\"796\":2,\"797\":1,\"798\":2,\"828\":2,\"829\":2,\"830\":2,\"847\":6,\"859\":1,\"862\":2,\"869\":1,\"921\":1,\"935\":1,\"979\":1,\"1029\":5,\"1061\":4,\"1062\":3,\"1117\":4,\"1118\":5,\"1130\":4,\"1131\":4,\"1133\":3,\"1134\":6,\"1136\":6,\"1137\":6,\"1139\":6,\"1141\":6,\"1185\":5,\"1208\":1,\"1232\":6,\"1252\":2,\"1257\":5,\"1259\":1,\"1279\":5,\"1280\":4,\"1281\":5,\"1283\":4,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":2,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1768\":1,\"1801\":1,\"1814\":8,\"1815\":5,\"1816\":5,\"1822\":2,\"1860\":1,\"1861\":1,\"1878\":1,\"1892\":4,\"1895\":1,\"1918\":2,\"1945\":2,\"1994\":2,\"2235\":3,\"2236\":3,\"2237\":2}}],[\"rwd\",{\"1\":{\"1618\":1}}],[\"rwkvdecoder\",{\"0\":{\"643\":1},\"1\":{\"643\":3}}],[\"rwkv\",{\"0\":{\"630\":1,\"642\":2,\"643\":1,\"649\":1,\"654\":1,\"668\":1},\"1\":{\"142\":11,\"630\":1,\"642\":6,\"643\":3,\"649\":1,\"654\":1,\"668\":1}}],[\"r1\",{\"1\":{\"1327\":1,\"1350\":1}}],[\"r^\",{\"1\":{\"1311\":1}}],[\"rfft\",{\"1\":{\"2438\":1}}],[\"rf^\",{\"1\":{\"1309\":2,\"1311\":1}}],[\"rf\",{\"1\":{\"1309\":2,\"1310\":2}}],[\"rcublock\",{\"0\":{\"1230\":1},\"1\":{\"1230\":1}}],[\"rtype\",{\"1\":{\"1760\":2,\"1782\":1,\"1860\":1,\"1878\":1,\"1893\":1,\"1907\":2}}],[\"rttmreader\",{\"0\":{\"1001\":1},\"1\":{\"1001\":1,\"1002\":1}}],[\"rttm\",{\"0\":{\"1001\":1,\"1021\":2},\"1\":{\"1001\":2,\"1002\":4,\"1021\":3}}],[\"rtf^h\",{\"1\":{\"1322\":1}}],[\"rtf\",{\"0\":{\"543\":1,\"1311\":1,\"1319\":1,\"1322\":1,\"1328\":1,\"1329\":1},\"1\":{\"133\":2,\"134\":2,\"136\":4,\"137\":1,\"543\":2,\"1126\":4,\"1217\":1,\"1311\":2,\"1319\":8,\"1322\":4,\"1328\":13,\"1329\":3}}],[\"rspecifier\",{\"1\":{\"548\":1,\"551\":1,\"558\":1,\"561\":1,\"567\":1,\"575\":1,\"606\":1,\"1772\":1,\"1780\":1,\"1825\":1,\"1828\":1,\"1881\":3}}],[\"rst\",{\"1\":{\"3\":5}}],[\"rʲ\",{\"1\":{\"287\":1}}],[\"rvq\",{\"1\":{\"275\":3,\"276\":26,\"1441\":1}}],[\"r9y9\",{\"1\":{\"271\":1,\"280\":1,\"287\":5,\"2298\":1}}],[\"rirs\",{\"1\":{\"2350\":1}}],[\"rir\",{\"1\":{\"1813\":1,\"2336\":2,\"2337\":2,\"2346\":2,\"2350\":2,\"2353\":6,\"2356\":2,\"2360\":2,\"2361\":2,\"2362\":2,\"2364\":6,\"2368\":2}}],[\"rirconvolve\",{\"0\":{\"1813\":1},\"1\":{\"1813\":1}}],[\"right=1\",{\"1\":{\"1635\":1}}],[\"right\",{\"1\":{\"704\":1,\"927\":1,\"994\":2,\"1268\":1,\"1368\":1,\"1387\":1,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1446\":1,\"1448\":1,\"1450\":4,\"1452\":4,\"1466\":1,\"1468\":1,\"1493\":1,\"1494\":1,\"2155\":1}}],[\"risk\",{\"0\":{\"695\":1,\"917\":1},\"1\":{\"262\":1,\"706\":2}}],[\"rita\",{\"1\":{\"15\":1}}],[\"rm\",{\"1\":{\"162\":1,\"535\":1}}],[\"rmsnorm\",{\"0\":{\"640\":1,\"2058\":1},\"1\":{\"640\":3,\"661\":1,\"666\":1}}],[\"rmse\",{\"1\":{\"267\":3,\"276\":3,\"285\":1,\"286\":4}}],[\"rms\",{\"1\":{\"141\":6,\"640\":2,\"666\":1}}],[\"rkwv\",{\"1\":{\"142\":1}}],[\"r\",{\"0\":{\"802\":1},\"1\":{\"136\":1,\"243\":1,\"287\":2,\"527\":1,\"704\":1,\"705\":2,\"756\":3,\"773\":3,\"802\":1,\"803\":2,\"804\":2,\"824\":2,\"866\":3,\"867\":3,\"877\":1,\"938\":2,\"961\":1,\"982\":2,\"1061\":1,\"1062\":1,\"1131\":1,\"1172\":1,\"1198\":1,\"1273\":2,\"1274\":2,\"1311\":1,\"1318\":1,\"1327\":1,\"1330\":1,\"1948\":2}}],[\"radio\",{\"1\":{\"2044\":6}}],[\"radford\",{\"1\":{\"202\":1}}],[\"radford23a\",{\"1\":{\"202\":1}}],[\"ravanelli\",{\"1\":{\"1668\":1}}],[\"rapid\",{\"1\":{\"262\":1}}],[\"rarely\",{\"1\":{\"223\":1}}],[\"rather\",{\"1\":{\"223\":1,\"696\":1,\"1224\":1,\"1225\":1,\"1719\":1,\"1721\":1,\"1725\":1}}],[\"rational\",{\"0\":{\"1634\":1,\"1635\":1,\"1636\":1},\"1\":{\"1634\":1,\"1635\":1,\"1636\":1}}],[\"ratios\",{\"1\":{\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1450\":3,\"1452\":3,\"1454\":6,\"1456\":6,\"1466\":1,\"1468\":1}}],[\"ratio=2\",{\"1\":{\"1235\":1}}],[\"ratio=4\",{\"1\":{\"1029\":1,\"1064\":1,\"1262\":1,\"1281\":1,\"1282\":1}}],[\"ratio\",{\"1\":{\"175\":1,\"243\":1,\"830\":1,\"833\":1,\"1029\":6,\"1064\":2,\"1117\":1,\"1130\":1,\"1131\":1,\"1134\":1,\"1136\":1,\"1137\":1,\"1139\":1,\"1141\":1,\"1185\":1,\"1202\":1,\"1232\":1,\"1235\":5,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":2,\"1279\":1,\"1280\":6,\"1281\":5,\"1282\":5,\"1283\":1,\"1290\":2,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1446\":1,\"1448\":1,\"1450\":3,\"1452\":5,\"1456\":1,\"1460\":1,\"1466\":1,\"1468\":1,\"1526\":1,\"1552\":1,\"1600\":1,\"1608\":4,\"1626\":1,\"1631\":3,\"1643\":1,\"1646\":1,\"1665\":2,\"1719\":4,\"1720\":2,\"1721\":5,\"1725\":6,\"1726\":3,\"1727\":3,\"1750\":2,\"1806\":3,\"1862\":5,\"1993\":2,\"2018\":9,\"2224\":2,\"2245\":2,\"2431\":2,\"2432\":2}}],[\"rate=0\",{\"1\":{\"1750\":2,\"1752\":1,\"1758\":1,\"1759\":1,\"1766\":1,\"1784\":1,\"1796\":1,\"1810\":1,\"1811\":1,\"1855\":1,\"1917\":1,\"2223\":2,\"2227\":1,\"2231\":1,\"2421\":1}}],[\"rate=none\",{\"1\":{\"821\":1}}],[\"rate=1\",{\"1\":{\"819\":1,\"823\":1,\"824\":1}}],[\"rates=false\",{\"1\":{\"2390\":1,\"2393\":1,\"2397\":1,\"2399\":1}}],[\"rates\",{\"1\":{\"200\":1,\"205\":1,\"242\":1,\"254\":1,\"286\":1,\"449\":2,\"821\":1,\"994\":1,\"1155\":1,\"1157\":1,\"1389\":1,\"1390\":1,\"1398\":1,\"1404\":1,\"1408\":1,\"1410\":1,\"1420\":3,\"2258\":2,\"2331\":1,\"2342\":1}}],[\"rate\",{\"1\":{\"43\":18,\"44\":6,\"71\":1,\"119\":1,\"128\":1,\"135\":1,\"141\":23,\"142\":16,\"144\":3,\"211\":1,\"243\":9,\"246\":2,\"247\":3,\"267\":14,\"276\":4,\"286\":2,\"516\":1,\"523\":1,\"617\":3,\"618\":3,\"620\":3,\"622\":3,\"624\":3,\"625\":6,\"633\":9,\"634\":15,\"636\":2,\"638\":3,\"639\":3,\"641\":6,\"642\":6,\"643\":9,\"644\":3,\"645\":3,\"651\":3,\"661\":5,\"674\":4,\"691\":1,\"692\":6,\"699\":3,\"700\":5,\"701\":6,\"706\":3,\"709\":11,\"710\":9,\"711\":9,\"713\":1,\"715\":1,\"720\":2,\"731\":4,\"732\":4,\"733\":4,\"734\":4,\"735\":2,\"738\":2,\"747\":5,\"748\":13,\"749\":1,\"765\":1,\"766\":4,\"767\":4,\"768\":1,\"771\":3,\"774\":9,\"775\":4,\"780\":11,\"781\":1,\"783\":1,\"784\":2,\"790\":1,\"791\":1,\"793\":1,\"815\":2,\"820\":1,\"823\":2,\"824\":2,\"831\":1,\"847\":2,\"848\":5,\"849\":9,\"850\":4,\"994\":1,\"1008\":2,\"1061\":2,\"1062\":1,\"1063\":1,\"1064\":3,\"1107\":9,\"1112\":1,\"1113\":1,\"1126\":1,\"1127\":1,\"1133\":1,\"1155\":1,\"1157\":1,\"1162\":1,\"1208\":1,\"1217\":2,\"1222\":1,\"1223\":1,\"1250\":2,\"1251\":2,\"1262\":3,\"1278\":9,\"1389\":3,\"1390\":2,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1406\":2,\"1408\":1,\"1410\":1,\"1413\":3,\"1417\":2,\"1419\":1,\"1420\":4,\"1441\":8,\"1466\":1,\"1468\":1,\"1511\":1,\"1517\":1,\"1519\":9,\"1520\":3,\"1524\":6,\"1525\":3,\"1526\":11,\"1533\":5,\"1534\":1,\"1535\":9,\"1536\":9,\"1539\":3,\"1545\":4,\"1546\":9,\"1547\":1,\"1549\":1,\"1552\":16,\"1553\":8,\"1554\":1,\"1556\":3,\"1558\":3,\"1560\":1,\"1561\":1,\"1565\":1,\"1570\":1,\"1583\":3,\"1598\":10,\"1599\":29,\"1600\":11,\"1607\":1,\"1610\":3,\"1611\":3,\"1612\":3,\"1613\":3,\"1616\":3,\"1622\":9,\"1625\":9,\"1626\":18,\"1628\":3,\"1654\":1,\"1662\":1,\"1666\":1,\"1668\":1,\"1672\":3,\"1673\":3,\"1674\":4,\"1676\":3,\"1677\":1,\"1678\":2,\"1679\":3,\"1680\":3,\"1683\":2,\"1685\":1,\"1686\":3,\"1687\":3,\"1689\":3,\"1690\":3,\"1692\":3,\"1693\":1,\"1694\":3,\"1696\":1,\"1697\":3,\"1698\":3,\"1735\":3,\"1736\":3,\"1737\":3,\"1738\":3,\"1739\":3,\"1740\":3,\"1741\":3,\"1742\":3,\"1743\":3,\"1744\":3,\"1745\":3,\"1746\":3,\"1749\":3,\"1750\":4,\"1751\":3,\"1753\":2,\"1756\":3,\"1757\":3,\"1758\":1,\"1759\":4,\"1766\":1,\"1782\":1,\"1784\":2,\"1785\":3,\"1786\":3,\"1789\":3,\"1790\":3,\"1794\":3,\"1795\":3,\"1796\":1,\"1808\":3,\"1809\":3,\"1810\":2,\"1814\":1,\"1815\":6,\"1816\":1,\"1817\":3,\"1818\":3,\"1820\":3,\"1824\":1,\"1837\":3,\"1847\":3,\"1856\":1,\"1863\":6,\"1914\":6,\"1917\":1,\"1936\":1,\"1945\":1,\"1947\":3,\"1957\":3,\"1960\":3,\"1961\":3,\"1984\":1,\"1992\":9,\"1993\":6,\"1994\":2,\"1995\":9,\"2014\":5,\"2015\":3,\"2016\":1,\"2017\":1,\"2018\":8,\"2019\":1,\"2021\":1,\"2034\":1,\"2040\":1,\"2044\":1,\"2045\":1,\"2054\":1,\"2065\":4,\"2126\":3,\"2129\":9,\"2133\":2,\"2136\":2,\"2191\":11,\"2221\":1,\"2223\":4,\"2227\":1,\"2231\":1,\"2235\":12,\"2236\":15,\"2239\":27,\"2240\":24,\"2245\":6,\"2346\":1,\"2350\":1,\"2353\":3,\"2355\":4,\"2357\":2,\"2364\":3,\"2368\":1,\"2407\":1,\"2411\":24,\"2412\":32,\"2423\":29,\"2428\":3,\"2430\":3,\"2431\":6,\"2432\":31,\"2433\":3,\"2435\":3,\"2447\":32,\"2463\":1,\"2464\":1,\"2472\":1}}],[\"ram\",{\"1\":{\"168\":1}}],[\"rawnet3projector\",{\"0\":{\"2194\":1},\"1\":{\"2194\":1}}],[\"rawnet3encoder\",{\"0\":{\"2192\":1},\"1\":{\"2192\":1}}],[\"rawnet3\",{\"0\":{\"2192\":1,\"2194\":1},\"1\":{\"2192\":2,\"2194\":1}}],[\"rawnet2\",{\"1\":{\"2168\":1}}],[\"rawnet\",{\"0\":{\"2168\":1,\"2179\":1},\"1\":{\"2168\":1,\"2179\":1,\"2192\":1}}],[\"rawsampler\",{\"0\":{\"1649\":1},\"1\":{\"1649\":1}}],[\"rawwav\",{\"1\":{\"968\":1,\"1039\":1,\"1053\":1}}],[\"raw\",{\"1\":{\"106\":1,\"126\":2,\"136\":3,\"201\":2,\"204\":1,\"205\":2,\"211\":1,\"218\":3,\"235\":1,\"243\":4,\"256\":1,\"266\":1,\"267\":11,\"269\":6,\"275\":1,\"276\":11,\"278\":6,\"286\":30,\"290\":1,\"603\":1,\"699\":2,\"702\":1,\"768\":2,\"787\":1,\"831\":2,\"1053\":1,\"1155\":2,\"1158\":2,\"1526\":1,\"1553\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1702\":1,\"1971\":2,\"2043\":2,\"2065\":1,\"2130\":2,\"2131\":3,\"2136\":1,\"2143\":2,\"2192\":2,\"2222\":2,\"2403\":2,\"2435\":2,\"2445\":2}}],[\"raise\",{\"1\":{\"2130\":1}}],[\"raised\",{\"1\":{\"653\":1,\"1842\":1}}],[\"raises\",{\"1\":{\"81\":1,\"760\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2065\":1,\"2134\":2,\"2145\":1,\"2155\":1}}],[\"rainy\",{\"1\":{\"80\":1}}],[\"ran\",{\"1\":{\"108\":1}}],[\"ranges\",{\"1\":{\"1509\":1,\"1511\":1,\"1553\":1,\"1977\":1,\"2130\":2,\"2136\":1}}],[\"range\",{\"1\":{\"91\":1,\"139\":1,\"141\":2,\"242\":1,\"243\":2,\"286\":1,\"536\":2,\"632\":3,\"664\":2,\"833\":3,\"1328\":1,\"1389\":2,\"1396\":2,\"1401\":2,\"1408\":2,\"1419\":4,\"1466\":2,\"1556\":2,\"1655\":1,\"1664\":1,\"1665\":1,\"1679\":1,\"1691\":3,\"1717\":1,\"1846\":1,\"1873\":2,\"1877\":2,\"2101\":1,\"2130\":2,\"2134\":2,\"2136\":1,\"2137\":1,\"2249\":1,\"2253\":1,\"2336\":1,\"2337\":1,\"2346\":1,\"2350\":1,\"2353\":4,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2364\":4,\"2368\":1,\"2434\":1,\"2435\":2}}],[\"randn\",{\"1\":{\"1846\":1,\"2305\":1,\"2378\":1}}],[\"randint\",{\"1\":{\"1824\":1}}],[\"rand\",{\"0\":{\"987\":1,\"989\":1,\"2394\":1},\"1\":{\"60\":1,\"62\":1,\"63\":1,\"64\":1,\"987\":1,\"989\":1,\"2350\":1,\"2394\":1}}],[\"randomsegmenter\",{\"0\":{\"2464\":1},\"1\":{\"2464\":1}}],[\"randomtextreader\",{\"0\":{\"1000\":1},\"1\":{\"1000\":1}}],[\"randomly\",{\"1\":{\"699\":1,\"927\":1,\"939\":4,\"1000\":1,\"1400\":1,\"1441\":1,\"1469\":1,\"1655\":1,\"1691\":1,\"1717\":2,\"1855\":1}}],[\"randomness\",{\"1\":{\"104\":1}}],[\"random\",{\"0\":{\"1632\":2,\"1633\":1,\"2321\":2,\"2464\":1},\"1\":{\"60\":1,\"91\":1,\"102\":1,\"104\":2,\"895\":3,\"941\":1,\"1000\":1,\"1050\":1,\"1116\":1,\"1161\":1,\"1189\":1,\"1218\":1,\"1221\":1,\"1229\":1,\"1244\":1,\"1526\":1,\"1599\":1,\"1600\":1,\"1618\":1,\"1632\":3,\"1633\":1,\"1645\":2,\"1650\":1,\"1651\":1,\"1674\":1,\"1680\":1,\"1701\":2,\"1734\":1,\"1824\":1,\"1846\":1,\"1883\":1,\"1907\":1,\"1919\":2,\"2000\":1,\"2001\":1,\"2134\":1,\"2249\":1,\"2253\":1,\"2321\":2,\"2378\":1,\"2464\":1}}],[\"ranks\",{\"1\":{\"2162\":3}}],[\"rank1\",{\"0\":{\"1327\":1},\"1\":{\"1327\":1}}],[\"rank=1\",{\"1\":{\"821\":1,\"895\":1,\"924\":1,\"928\":1}}],[\"rank\",{\"0\":{\"59\":1,\"928\":1,\"929\":1,\"1496\":1,\"2381\":1,\"2384\":1,\"2386\":1},\"1\":{\"58\":2,\"59\":3,\"61\":3,\"377\":4,\"449\":4,\"821\":3,\"824\":5,\"924\":1,\"928\":2,\"929\":2,\"938\":1,\"1327\":7,\"1330\":6,\"1350\":1,\"1496\":1,\"1681\":1,\"1683\":3,\"1685\":1,\"2132\":3,\"2134\":3,\"2141\":3,\"2162\":1,\"2166\":1,\"2340\":4,\"2381\":1,\"2384\":5,\"2385\":3,\"2386\":1}}],[\"raj\",{\"1\":{\"15\":1}}],[\"rubric\",{\"1\":{\"1655\":1,\"2151\":1,\"2310\":1}}],[\"russian\",{\"1\":{\"287\":1,\"481\":1}}],[\"ruled\",{\"1\":{\"1521\":9,\"2228\":10,\"2229\":10}}],[\"rule\",{\"1\":{\"268\":1,\"277\":1}}],[\"rules\",{\"0\":{\"67\":1},\"1\":{\"1051\":1}}],[\"ruchira\",{\"1\":{\"15\":1}}],[\"run=\",{\"1\":{\"168\":1}}],[\"runner\",{\"1\":{\"2311\":1}}],[\"runners\",{\"1\":{\"162\":1}}],[\"running\",{\"1\":{\"1\":1,\"3\":1,\"24\":1,\"66\":1,\"163\":1,\"196\":1,\"201\":2,\"249\":1,\"268\":1,\"277\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1065\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1078\":1,\"1079\":2,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1387\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1719\":8,\"1725\":8,\"1806\":8,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1951\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2043\":1,\"2045\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"runs\",{\"1\":{\"136\":1,\"137\":1,\"165\":1,\"2130\":2}}],[\"runtime\",{\"1\":{\"26\":2,\"28\":1}}],[\"run\",{\"0\":{\"40\":1,\"116\":1,\"167\":1,\"195\":1,\"201\":1,\"206\":1,\"212\":1,\"218\":1,\"236\":1,\"243\":1,\"255\":1,\"267\":1,\"276\":1,\"286\":1},\"1\":{\"1\":2,\"3\":1,\"22\":4,\"23\":2,\"24\":5,\"25\":1,\"26\":2,\"33\":1,\"37\":1,\"38\":2,\"39\":1,\"40\":3,\"41\":7,\"47\":6,\"69\":1,\"71\":2,\"76\":1,\"91\":1,\"107\":2,\"108\":2,\"109\":2,\"110\":2,\"111\":3,\"117\":3,\"118\":6,\"119\":5,\"120\":2,\"121\":2,\"124\":3,\"125\":1,\"126\":1,\"127\":2,\"128\":1,\"131\":2,\"133\":2,\"136\":1,\"139\":1,\"162\":2,\"166\":2,\"167\":1,\"168\":1,\"173\":1,\"174\":2,\"175\":1,\"194\":1,\"195\":1,\"196\":1,\"197\":5,\"199\":1,\"201\":8,\"204\":1,\"206\":4,\"210\":1,\"211\":1,\"212\":7,\"213\":1,\"216\":1,\"218\":7,\"222\":1,\"223\":1,\"224\":10,\"227\":3,\"232\":1,\"234\":1,\"236\":2,\"240\":1,\"243\":7,\"247\":3,\"253\":1,\"254\":1,\"255\":4,\"259\":3,\"263\":2,\"265\":1,\"267\":39,\"268\":1,\"272\":1,\"274\":1,\"276\":31,\"277\":1,\"282\":1,\"284\":1,\"285\":2,\"286\":43,\"290\":3,\"442\":2,\"449\":2,\"516\":2,\"520\":2,\"521\":2,\"523\":2,\"524\":2,\"525\":2,\"535\":2,\"537\":1,\"1389\":3,\"1400\":1,\"1401\":3,\"1408\":3,\"1413\":1,\"1420\":4,\"1466\":3,\"1526\":1,\"1552\":1,\"1553\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1625\":1,\"1626\":1,\"2049\":1,\"2054\":1,\"2136\":1,\"2249\":1,\"2253\":1,\"2338\":1,\"2348\":1,\"2355\":1,\"2369\":1,\"2370\":2,\"2372\":1}}],[\"role\",{\"1\":{\"2039\":2,\"2049\":1,\"2143\":1}}],[\"rolled\",{\"1\":{\"1919\":2}}],[\"rolling\",{\"1\":{\"699\":1}}],[\"roll\",{\"0\":{\"1919\":1},\"1\":{\"699\":6,\"1919\":7}}],[\"routines\",{\"1\":{\"2312\":1}}],[\"route\",{\"1\":{\"66\":1}}],[\"round\",{\"0\":{\"2009\":1},\"1\":{\"2009\":1}}],[\"roformer\",{\"1\":{\"1320\":3}}],[\"rong\",{\"1\":{\"1316\":1}}],[\"rope\",{\"0\":{\"2067\":1,\"2069\":1}}],[\"rop\",{\"1\":{\"704\":1}}],[\"rows\",{\"1\":{\"705\":1,\"803\":1,\"804\":2,\"931\":1,\"932\":2,\"933\":1,\"934\":2,\"939\":2}}],[\"row\",{\"1\":{\"211\":3,\"939\":2}}],[\"rotaryrelativepositionbias\",{\"0\":{\"647\":1},\"1\":{\"647\":3}}],[\"rotary\",{\"1\":{\"142\":1,\"647\":5}}],[\"rooted\",{\"1\":{\"267\":1,\"276\":1}}],[\"root=<cuda\",{\"1\":{\"163\":1}}],[\"root=$\",{\"1\":{\"162\":1}}],[\"root=\",{\"1\":{\"161\":1}}],[\"root>\",{\"1\":{\"31\":4,\"33\":1,\"154\":1,\"161\":4,\"162\":16,\"163\":6,\"164\":1}}],[\"root\",{\"1\":{\"22\":2,\"25\":3,\"162\":3,\"163\":1,\"285\":1,\"286\":1}}],[\"robust\",{\"1\":{\"11\":1,\"202\":1,\"290\":1,\"1752\":1,\"1788\":1,\"1795\":1,\"2208\":1,\"2209\":1,\"2411\":1}}],[\"robin\",{\"1\":{\"11\":1}}],[\"roshan\",{\"1\":{\"6\":1,\"15\":1,\"202\":1,\"244\":1}}],[\"ryuichi\",{\"1\":{\"9\":2}}],[\"reordering\",{\"1\":{\"2346\":1,\"2368\":1}}],[\"reopen\",{\"1\":{\"19\":1}}],[\"reinforcement\",{\"1\":{\"2239\":1}}],[\"reim\",{\"0\":{\"1366\":1},\"1\":{\"1366\":1}}],[\"reject\",{\"1\":{\"1673\":1}}],[\"redundant\",{\"1\":{\"269\":1,\"278\":1}}],[\"reduction=8\",{\"1\":{\"2213\":1,\"2214\":1}}],[\"reduction=4\",{\"1\":{\"2202\":1}}],[\"reduction=\",{\"1\":{\"786\":1,\"800\":1,\"921\":1,\"935\":1,\"1210\":1,\"1326\":1,\"1754\":1}}],[\"reduction\",{\"1\":{\"290\":1,\"704\":3,\"705\":2,\"786\":3,\"800\":3,\"802\":1,\"803\":1,\"804\":2,\"866\":1,\"867\":1,\"921\":3,\"931\":2,\"932\":1,\"933\":2,\"934\":1,\"935\":3,\"1210\":1,\"1321\":1,\"1322\":1,\"1326\":1,\"1327\":3,\"1330\":4,\"1526\":1,\"1598\":1,\"1599\":3,\"1600\":1,\"1750\":3,\"1754\":2,\"1758\":2,\"1993\":3,\"2196\":1,\"2223\":3,\"2231\":2,\"2235\":3,\"2236\":3,\"2239\":3,\"2240\":3,\"2245\":3,\"2404\":1,\"2409\":1,\"2411\":3,\"2412\":3,\"2423\":3,\"2427\":3,\"2431\":3,\"2432\":3,\"2447\":3,\"2469\":1,\"2470\":1,\"2472\":1,\"2473\":1}}],[\"reduce=true\",{\"1\":{\"1638\":1}}],[\"reducehelper\",{\"0\":{\"803\":1},\"1\":{\"803\":1}}],[\"reduced\",{\"1\":{\"704\":1,\"878\":1,\"879\":1,\"882\":1,\"883\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1}}],[\"reducelronplateau\",{\"0\":{\"2020\":1},\"1\":{\"84\":3,\"2020\":2,\"2355\":1}}],[\"reduce\",{\"0\":{\"528\":1,\"704\":1,\"764\":1,\"802\":1,\"803\":1,\"931\":2,\"933\":2,\"1475\":1},\"1\":{\"71\":1,\"128\":3,\"290\":1,\"528\":1,\"704\":1,\"705\":1,\"706\":3,\"764\":1,\"802\":1,\"803\":1,\"804\":2,\"819\":1,\"931\":2,\"932\":2,\"933\":2,\"934\":2,\"1264\":2,\"1325\":1,\"1332\":1,\"1334\":2,\"1475\":1,\"1489\":1,\"1794\":1,\"1881\":1,\"2136\":1}}],[\"reduces\",{\"1\":{\"48\":1,\"803\":1,\"1084\":1}}],[\"reducing\",{\"1\":{\"7\":1,\"104\":1,\"2130\":1}}],[\"revisiting\",{\"1\":{\"927\":1}}],[\"review\",{\"1\":{\"269\":1,\"278\":1}}],[\"reverse=false\",{\"1\":{\"1728\":1,\"1808\":1}}],[\"reverse\",{\"0\":{\"1355\":1,\"1375\":1,\"1696\":1},\"1\":{\"1245\":4,\"1253\":3,\"1355\":1,\"1375\":1,\"1454\":1,\"1456\":1,\"1696\":1,\"1808\":2}}],[\"reversediffusionpredictor\",{\"0\":{\"1244\":1},\"1\":{\"1244\":1}}],[\"reversibleinstancenorm1doutput\",{\"0\":{\"813\":1},\"1\":{\"813\":1}}],[\"reversibleinstancenorm1dinput\",{\"0\":{\"811\":1},\"1\":{\"811\":1}}],[\"reverb\",{\"1\":{\"1155\":1,\"1157\":1,\"2350\":1}}],[\"reverberant\",{\"1\":{\"1066\":1,\"2346\":1,\"2368\":1}}],[\"reverberation\",{\"1\":{\"223\":1}}],[\"revert\",{\"1\":{\"141\":1,\"286\":1,\"635\":1,\"664\":1,\"1155\":1,\"1157\":1,\"1719\":1}}],[\"rebuilt\",{\"1\":{\"269\":1,\"278\":1}}],[\"rebuild\",{\"1\":{\"19\":1,\"1484\":1}}],[\"rename\",{\"0\":{\"1916\":1},\"1\":{\"1916\":1}}],[\"renamed\",{\"1\":{\"46\":1,\"286\":1}}],[\"renduchintala\",{\"1\":{\"156\":1}}],[\"reworked\",{\"1\":{\"133\":1,\"145\":1}}],[\"rearrange\",{\"1\":{\"1124\":2,\"1125\":3,\"1176\":2}}],[\"rearrange=false\",{\"1\":{\"1124\":1,\"1176\":1}}],[\"reassign\",{\"1\":{\"269\":2,\"278\":2}}],[\"reasonable\",{\"1\":{\"262\":1,\"286\":2}}],[\"reason\",{\"1\":{\"66\":3,\"174\":1,\"1156\":1,\"1647\":1}}],[\"really\",{\"1\":{\"2380\":1}}],[\"real+imag\",{\"1\":{\"1080\":2,\"1082\":2}}],[\"real=false\",{\"1\":{\"895\":1}}],[\"realtime\",{\"1\":{\"178\":3,\"181\":2,\"190\":1,\"193\":1,\"536\":1}}],[\"real\",{\"0\":{\"133\":1},\"1\":{\"133\":2,\"134\":1,\"223\":1,\"245\":1,\"543\":1,\"823\":1,\"824\":2,\"895\":1,\"949\":1,\"1031\":1,\"1035\":1,\"1051\":2,\"1112\":1,\"1113\":1,\"1118\":3,\"1124\":2,\"1125\":1,\"1250\":1,\"1251\":1,\"1280\":1,\"1283\":1,\"1308\":2,\"1348\":1,\"1361\":1,\"1515\":2,\"1584\":1,\"1933\":2,\"2044\":1,\"2384\":1,\"2385\":1,\"2404\":1,\"2469\":1,\"2470\":3}}],[\"readable\",{\"0\":{\"2151\":1,\"2310\":1},\"1\":{\"2143\":1,\"2151\":8,\"2310\":8,\"2480\":1}}],[\"reading\",{\"1\":{\"1883\":1}}],[\"ready\",{\"1\":{\"286\":1,\"2130\":2,\"2131\":3,\"2143\":3}}],[\"readmes\",{\"1\":{\"1053\":1}}],[\"readme\",{\"1\":{\"198\":1,\"245\":1,\"285\":1,\"290\":1}}],[\"readers\",{\"0\":{\"1772\":1,\"1780\":1,\"1825\":1,\"1828\":1,\"1881\":1},\"1\":{\"269\":1,\"278\":1,\"1772\":1,\"1780\":1,\"1825\":1,\"1828\":1,\"1881\":1}}],[\"reader\",{\"0\":{\"1881\":1},\"1\":{\"146\":1,\"991\":1,\"992\":2,\"993\":1,\"994\":2,\"996\":1,\"997\":2,\"1000\":3,\"1001\":1,\"1002\":2,\"1003\":1,\"1004\":2,\"1007\":1,\"1008\":4,\"1011\":1,\"1012\":2,\"1015\":1,\"1016\":2,\"1772\":2,\"1780\":2,\"1825\":2,\"1828\":2,\"1881\":1,\"1882\":2,\"2139\":2,\"2144\":1}}],[\"read\",{\"0\":{\"930\":1,\"1000\":1,\"1019\":1,\"1022\":2,\"1024\":2,\"1026\":2,\"1028\":1},\"1\":{\"127\":1,\"153\":1,\"195\":2,\"704\":1,\"930\":2,\"1000\":1,\"1019\":2,\"1021\":1,\"1022\":3,\"1023\":1,\"1024\":3,\"1026\":3,\"1027\":1,\"1028\":1,\"1881\":1,\"1882\":2}}],[\"retain\",{\"1\":{\"2039\":1}}],[\"retained\",{\"1\":{\"760\":1,\"2000\":1,\"2001\":1}}],[\"ret1\",{\"1\":{\"959\":1}}],[\"ret\",{\"1\":{\"675\":1,\"1361\":1,\"1672\":1,\"1673\":1,\"1674\":1,\"1676\":1,\"1678\":1,\"1679\":1,\"1680\":1,\"1686\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1692\":1,\"1694\":1,\"1697\":1,\"1698\":1}}],[\"retrieved\",{\"1\":{\"756\":1,\"773\":1}}],[\"retrieve\",{\"1\":{\"756\":1,\"773\":1,\"866\":1,\"867\":1,\"1726\":1,\"1727\":1}}],[\"retrieval\",{\"1\":{\"262\":1}}],[\"retrying\",{\"1\":{\"66\":1}}],[\"retval\",{\"1\":{\"81\":5,\"2247\":2}}],[\"returning\",{\"1\":{\"2043\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2139\":1,\"2355\":1}}],[\"returns\",{\"1\":{\"100\":1,\"614\":6,\"615\":1,\"616\":8,\"617\":2,\"618\":2,\"619\":1,\"620\":4,\"621\":1,\"622\":1,\"623\":1,\"624\":2,\"625\":3,\"626\":2,\"627\":3,\"630\":1,\"632\":1,\"633\":2,\"634\":8,\"636\":2,\"637\":5,\"638\":1,\"639\":4,\"640\":1,\"641\":7,\"642\":1,\"643\":7,\"644\":6,\"645\":1,\"646\":1,\"647\":3,\"648\":1,\"649\":2,\"651\":6,\"654\":2,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"663\":1,\"664\":1,\"665\":1,\"666\":1,\"667\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":1,\"673\":1,\"675\":2,\"691\":1,\"692\":4,\"696\":8,\"697\":7,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"703\":2,\"706\":4,\"709\":1,\"710\":3,\"711\":3,\"712\":1,\"715\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"733\":1,\"734\":1,\"735\":1,\"740\":3,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":2,\"750\":1,\"755\":1,\"759\":1,\"760\":3,\"768\":1,\"770\":2,\"771\":1,\"774\":1,\"775\":1,\"780\":1,\"783\":1,\"784\":4,\"785\":1,\"790\":3,\"797\":2,\"819\":2,\"820\":3,\"822\":1,\"823\":1,\"824\":2,\"828\":1,\"829\":1,\"830\":1,\"831\":1,\"846\":1,\"847\":7,\"849\":1,\"850\":3,\"859\":1,\"902\":1,\"919\":1,\"930\":1,\"939\":1,\"958\":3,\"959\":2,\"962\":1,\"971\":1,\"972\":1,\"973\":1,\"975\":1,\"976\":1,\"978\":1,\"979\":1,\"980\":1,\"981\":1,\"982\":1,\"994\":1,\"1029\":2,\"1031\":1,\"1035\":1,\"1050\":1,\"1051\":1,\"1053\":1,\"1054\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1066\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1107\":1,\"1112\":1,\"1113\":2,\"1116\":1,\"1117\":1,\"1118\":2,\"1119\":1,\"1124\":1,\"1125\":1,\"1126\":3,\"1127\":2,\"1130\":1,\"1131\":1,\"1132\":1,\"1133\":3,\"1136\":1,\"1141\":1,\"1147\":1,\"1148\":1,\"1156\":1,\"1157\":1,\"1161\":1,\"1162\":1,\"1167\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1175\":1,\"1176\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1189\":1,\"1198\":1,\"1199\":1,\"1204\":1,\"1208\":1,\"1209\":1,\"1210\":1,\"1217\":1,\"1218\":1,\"1221\":1,\"1224\":1,\"1225\":1,\"1228\":1,\"1229\":1,\"1232\":1,\"1235\":2,\"1244\":1,\"1245\":3,\"1246\":1,\"1247\":1,\"1250\":1,\"1251\":3,\"1252\":1,\"1253\":1,\"1261\":1,\"1264\":1,\"1267\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1275\":1,\"1277\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1293\":1,\"1301\":1,\"1306\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1314\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1330\":1,\"1332\":1,\"1334\":3,\"1350\":1,\"1351\":1,\"1354\":1,\"1356\":1,\"1361\":1,\"1368\":1,\"1371\":1,\"1372\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1385\":1,\"1386\":1,\"1389\":4,\"1390\":1,\"1391\":3,\"1395\":6,\"1397\":1,\"1400\":1,\"1401\":4,\"1402\":1,\"1403\":3,\"1406\":2,\"1408\":4,\"1409\":1,\"1410\":3,\"1419\":1,\"1441\":2,\"1466\":4,\"1467\":1,\"1468\":3,\"1513\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1519\":1,\"1520\":1,\"1521\":3,\"1524\":1,\"1525\":1,\"1526\":2,\"1529\":3,\"1533\":1,\"1534\":1,\"1535\":1,\"1536\":1,\"1546\":1,\"1548\":2,\"1551\":1,\"1552\":2,\"1553\":2,\"1556\":3,\"1558\":1,\"1559\":1,\"1577\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":2,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":2,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":2,\"1599\":2,\"1600\":2,\"1601\":1,\"1603\":1,\"1604\":1,\"1605\":2,\"1606\":1,\"1607\":1,\"1608\":2,\"1609\":1,\"1610\":2,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":2,\"1620\":1,\"1621\":1,\"1622\":1,\"1624\":1,\"1625\":2,\"1626\":2,\"1627\":1,\"1628\":1,\"1629\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1637\":1,\"1654\":1,\"1660\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":2,\"1672\":1,\"1673\":1,\"1674\":1,\"1676\":1,\"1678\":1,\"1679\":1,\"1680\":1,\"1686\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1692\":1,\"1694\":1,\"1697\":1,\"1698\":1,\"1702\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1719\":7,\"1720\":3,\"1721\":2,\"1723\":1,\"1724\":2,\"1725\":10,\"1726\":1,\"1727\":1,\"1729\":1,\"1731\":6,\"1733\":1,\"1735\":2,\"1736\":3,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":6,\"1750\":3,\"1751\":1,\"1753\":2,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":2,\"1759\":1,\"1760\":2,\"1764\":1,\"1768\":2,\"1770\":1,\"1771\":1,\"1779\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":2,\"1786\":1,\"1787\":2,\"1788\":1,\"1789\":1,\"1790\":1,\"1794\":3,\"1795\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":2,\"1803\":1,\"1805\":1,\"1806\":6,\"1808\":1,\"1810\":1,\"1812\":1,\"1814\":1,\"1815\":7,\"1816\":1,\"1817\":2,\"1818\":1,\"1820\":1,\"1822\":4,\"1837\":1,\"1839\":1,\"1843\":5,\"1847\":1,\"1848\":2,\"1849\":1,\"1851\":2,\"1854\":2,\"1856\":1,\"1858\":2,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1870\":1,\"1871\":1,\"1873\":1,\"1876\":1,\"1877\":1,\"1880\":1,\"1888\":1,\"1891\":1,\"1892\":1,\"1893\":1,\"1894\":1,\"1895\":1,\"1897\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1908\":1,\"1910\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1917\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1927\":1,\"1928\":1,\"1931\":1,\"1932\":1,\"1936\":1,\"1937\":1,\"1944\":2,\"1945\":1,\"1946\":1,\"1947\":2,\"1950\":1,\"1960\":1,\"1961\":1,\"1965\":1,\"1966\":3,\"1974\":1,\"1977\":1,\"1984\":1,\"1990\":1,\"1991\":1,\"1992\":3,\"1993\":2,\"1995\":1,\"1997\":1,\"2039\":2,\"2040\":1,\"2043\":1,\"2044\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2065\":2,\"2066\":1,\"2101\":1,\"2129\":1,\"2130\":11,\"2131\":2,\"2132\":1,\"2133\":5,\"2134\":1,\"2136\":10,\"2137\":7,\"2139\":2,\"2140\":1,\"2142\":2,\"2143\":2,\"2145\":1,\"2146\":1,\"2147\":1,\"2148\":1,\"2149\":1,\"2150\":1,\"2151\":1,\"2155\":1,\"2156\":1,\"2162\":3,\"2167\":1,\"2176\":1,\"2183\":1,\"2184\":1,\"2187\":1,\"2190\":1,\"2191\":1,\"2207\":1,\"2208\":1,\"2218\":1,\"2220\":1,\"2223\":1,\"2224\":1,\"2226\":1,\"2227\":1,\"2228\":3,\"2229\":3,\"2231\":2,\"2232\":2,\"2235\":2,\"2236\":2,\"2237\":1,\"2238\":1,\"2239\":2,\"2240\":2,\"2241\":1,\"2245\":2,\"2247\":5,\"2298\":1,\"2310\":1,\"2311\":2,\"2327\":1,\"2355\":3,\"2359\":2,\"2367\":1,\"2407\":1,\"2408\":3,\"2411\":2,\"2412\":2,\"2413\":1,\"2420\":1,\"2422\":1,\"2423\":2,\"2424\":1,\"2425\":1,\"2426\":1,\"2427\":2,\"2428\":4,\"2429\":1,\"2430\":1,\"2431\":2,\"2432\":2,\"2433\":1,\"2435\":3,\"2436\":1,\"2438\":1,\"2439\":1,\"2440\":1,\"2441\":1,\"2442\":1,\"2446\":3,\"2447\":2,\"2448\":1,\"2490\":1,\"2495\":1}}],[\"return\",{\"1\":{\"78\":2,\"81\":2,\"82\":6,\"197\":1,\"614\":2,\"616\":8,\"617\":2,\"618\":2,\"619\":1,\"620\":4,\"621\":1,\"622\":1,\"623\":1,\"624\":2,\"625\":3,\"626\":2,\"627\":1,\"630\":1,\"632\":1,\"633\":2,\"634\":5,\"636\":2,\"637\":3,\"638\":1,\"639\":3,\"640\":1,\"641\":5,\"642\":1,\"643\":4,\"644\":3,\"645\":1,\"646\":1,\"647\":2,\"649\":2,\"651\":3,\"654\":2,\"664\":1,\"665\":1,\"667\":1,\"669\":1,\"671\":1,\"672\":1,\"673\":1,\"691\":2,\"692\":14,\"696\":8,\"697\":7,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"706\":4,\"709\":4,\"712\":1,\"715\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"733\":1,\"734\":2,\"735\":1,\"740\":1,\"744\":1,\"749\":1,\"756\":1,\"759\":1,\"760\":3,\"765\":1,\"768\":1,\"769\":1,\"770\":2,\"773\":1,\"774\":4,\"775\":1,\"777\":1,\"779\":1,\"780\":1,\"783\":1,\"784\":3,\"787\":4,\"790\":3,\"797\":1,\"820\":3,\"828\":3,\"829\":3,\"830\":3,\"831\":2,\"847\":5,\"850\":7,\"859\":1,\"866\":1,\"867\":1,\"924\":1,\"928\":1,\"930\":1,\"939\":1,\"943\":1,\"958\":3,\"959\":2,\"971\":1,\"973\":1,\"975\":1,\"976\":1,\"978\":1,\"979\":1,\"980\":1,\"982\":1,\"994\":1,\"1026\":1,\"1028\":1,\"1029\":2,\"1031\":1,\"1035\":1,\"1050\":1,\"1053\":1,\"1054\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1066\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1107\":1,\"1112\":1,\"1113\":2,\"1116\":1,\"1117\":1,\"1118\":2,\"1124\":1,\"1125\":1,\"1126\":3,\"1127\":2,\"1130\":1,\"1131\":1,\"1132\":1,\"1136\":1,\"1141\":1,\"1147\":1,\"1148\":1,\"1156\":2,\"1157\":1,\"1161\":1,\"1162\":1,\"1167\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1175\":1,\"1176\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1189\":1,\"1198\":1,\"1199\":1,\"1204\":1,\"1208\":1,\"1209\":1,\"1210\":1,\"1217\":1,\"1218\":1,\"1221\":1,\"1222\":1,\"1228\":1,\"1229\":1,\"1232\":1,\"1235\":2,\"1244\":1,\"1246\":1,\"1247\":1,\"1250\":1,\"1251\":2,\"1252\":1,\"1253\":1,\"1261\":1,\"1264\":1,\"1267\":1,\"1268\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1273\":1,\"1274\":1,\"1275\":1,\"1277\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1293\":1,\"1308\":1,\"1309\":2,\"1310\":2,\"1311\":2,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":2,\"1319\":2,\"1320\":1,\"1321\":2,\"1322\":2,\"1323\":2,\"1326\":1,\"1327\":2,\"1328\":1,\"1330\":2,\"1332\":1,\"1334\":4,\"1351\":1,\"1354\":1,\"1356\":1,\"1361\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1381\":4,\"1385\":1,\"1386\":1,\"1389\":5,\"1390\":1,\"1391\":3,\"1395\":9,\"1397\":1,\"1400\":1,\"1401\":5,\"1402\":1,\"1403\":3,\"1406\":2,\"1408\":5,\"1409\":1,\"1410\":3,\"1419\":1,\"1441\":3,\"1466\":5,\"1467\":1,\"1468\":3,\"1485\":2,\"1508\":1,\"1513\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1519\":1,\"1520\":1,\"1521\":6,\"1524\":1,\"1525\":1,\"1526\":4,\"1529\":3,\"1533\":1,\"1534\":1,\"1535\":1,\"1536\":1,\"1546\":1,\"1548\":2,\"1551\":1,\"1552\":2,\"1553\":4,\"1556\":3,\"1558\":1,\"1559\":1,\"1576\":1,\"1577\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":4,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":2,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":4,\"1599\":2,\"1600\":4,\"1601\":1,\"1603\":1,\"1604\":1,\"1605\":2,\"1606\":1,\"1607\":1,\"1608\":2,\"1609\":1,\"1610\":3,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":2,\"1620\":1,\"1621\":1,\"1622\":1,\"1624\":1,\"1625\":4,\"1626\":2,\"1627\":1,\"1628\":2,\"1629\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1637\":1,\"1654\":1,\"1660\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":2,\"1672\":1,\"1673\":1,\"1674\":1,\"1676\":1,\"1678\":1,\"1679\":1,\"1680\":1,\"1686\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":1,\"1692\":1,\"1694\":1,\"1697\":1,\"1698\":1,\"1702\":2,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1719\":10,\"1720\":3,\"1721\":5,\"1723\":1,\"1724\":1,\"1725\":13,\"1730\":2,\"1731\":3,\"1733\":1,\"1735\":2,\"1736\":3,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":6,\"1750\":3,\"1751\":1,\"1753\":2,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":2,\"1759\":2,\"1764\":1,\"1768\":2,\"1770\":1,\"1771\":1,\"1772\":1,\"1779\":1,\"1780\":1,\"1783\":1,\"1784\":1,\"1785\":2,\"1786\":1,\"1787\":2,\"1788\":1,\"1789\":1,\"1790\":1,\"1794\":3,\"1795\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":2,\"1803\":1,\"1805\":1,\"1806\":6,\"1808\":1,\"1810\":1,\"1812\":1,\"1814\":1,\"1815\":5,\"1816\":1,\"1817\":2,\"1818\":1,\"1820\":1,\"1822\":3,\"1824\":3,\"1825\":1,\"1828\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1843\":5,\"1847\":1,\"1849\":1,\"1851\":2,\"1854\":2,\"1856\":1,\"1858\":2,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1868\":1,\"1870\":1,\"1871\":1,\"1873\":1,\"1877\":1,\"1878\":1,\"1881\":4,\"1886\":1,\"1888\":1,\"1891\":1,\"1892\":1,\"1893\":1,\"1894\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1907\":2,\"1908\":1,\"1910\":1,\"1914\":1,\"1915\":1,\"1917\":1,\"1919\":1,\"1920\":2,\"1921\":1,\"1926\":1,\"1927\":1,\"1928\":1,\"1931\":1,\"1932\":1,\"1936\":2,\"1937\":1,\"1940\":1,\"1942\":1,\"1944\":2,\"1945\":1,\"1947\":2,\"1950\":2,\"1955\":1,\"1958\":1,\"1960\":2,\"1961\":2,\"1965\":1,\"1966\":2,\"1971\":4,\"1975\":1,\"1976\":1,\"1977\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1984\":1,\"1990\":1,\"1991\":1,\"1992\":5,\"1993\":1,\"1995\":2,\"1997\":1,\"2039\":1,\"2040\":1,\"2043\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2065\":1,\"2066\":1,\"2101\":1,\"2130\":2,\"2131\":3,\"2132\":2,\"2133\":1,\"2136\":3,\"2137\":1,\"2139\":3,\"2141\":2,\"2144\":3,\"2151\":1,\"2155\":1,\"2167\":1,\"2176\":1,\"2183\":1,\"2187\":1,\"2190\":1,\"2191\":1,\"2207\":1,\"2208\":1,\"2218\":1,\"2220\":1,\"2221\":1,\"2222\":4,\"2223\":1,\"2224\":1,\"2226\":1,\"2227\":1,\"2228\":6,\"2229\":6,\"2231\":2,\"2232\":2,\"2235\":2,\"2236\":2,\"2237\":1,\"2238\":1,\"2239\":2,\"2240\":2,\"2241\":1,\"2245\":2,\"2246\":1,\"2247\":3,\"2248\":1,\"2249\":3,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2298\":1,\"2310\":1,\"2311\":2,\"2325\":1,\"2327\":10,\"2355\":4,\"2359\":1,\"2403\":4,\"2407\":1,\"2408\":6,\"2411\":2,\"2412\":2,\"2413\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2420\":1,\"2422\":1,\"2423\":2,\"2424\":1,\"2425\":1,\"2426\":1,\"2427\":2,\"2428\":4,\"2429\":1,\"2430\":1,\"2431\":2,\"2432\":2,\"2433\":1,\"2435\":3,\"2436\":1,\"2438\":1,\"2440\":1,\"2441\":1,\"2442\":1,\"2445\":4,\"2446\":6,\"2447\":2,\"2448\":1,\"2480\":1}}],[\"returned\",{\"1\":{\"66\":1,\"756\":2,\"773\":2,\"866\":2,\"867\":2,\"878\":1,\"879\":1,\"882\":1,\"883\":1,\"961\":1,\"974\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1202\":1,\"1204\":1,\"1209\":1,\"1228\":1,\"1259\":1,\"1261\":1,\"1354\":2,\"1485\":1,\"2043\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2309\":1,\"2355\":1}}],[\"refines\",{\"1\":{\"1810\":1}}],[\"refineblock\",{\"0\":{\"1233\":1},\"1\":{\"1233\":1}}],[\"refs\",{\"1\":{\"578\":2,\"583\":3,\"586\":3,\"589\":3}}],[\"ref2\",{\"1\":{\"520\":1,\"1155\":2,\"1158\":2}}],[\"ref1\",{\"1\":{\"520\":1,\"1155\":2,\"1158\":2}}],[\"reffiles\",{\"1\":{\"520\":1}}],[\"reflectionpad1d\",{\"1\":{\"1604\":1,\"1605\":1,\"1606\":1,\"1615\":1,\"1618\":1}}],[\"reflection\",{\"1\":{\"1493\":1,\"1494\":1}}],[\"reflect\",{\"1\":{\"286\":2,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1442\":1,\"1444\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":1,\"1468\":1,\"1493\":1,\"1494\":1,\"1835\":1,\"1900\":1,\"1924\":1}}],[\"reformats\",{\"1\":{\"211\":1,\"217\":1,\"266\":1,\"275\":1,\"285\":1}}],[\"ref\",{\"1\":{\"69\":3,\"223\":8,\"224\":3,\"342\":2,\"349\":2,\"356\":4,\"361\":2,\"581\":1,\"596\":2,\"720\":1,\"736\":1,\"738\":1,\"794\":7,\"815\":1,\"950\":1,\"1029\":2,\"1036\":1,\"1042\":1,\"1062\":2,\"1066\":2,\"1070\":2,\"1071\":2,\"1073\":2,\"1117\":1,\"1125\":2,\"1126\":1,\"1132\":2,\"1155\":1,\"1157\":4,\"1158\":3,\"1164\":2,\"1167\":2,\"1170\":2,\"1171\":2,\"1172\":2,\"1173\":2,\"1174\":1,\"1175\":2,\"1204\":2,\"1209\":2,\"1217\":1,\"1228\":3,\"1235\":2,\"1246\":2,\"1247\":2,\"1248\":1,\"1253\":1,\"1269\":1,\"1275\":2,\"1277\":2,\"1279\":2,\"1280\":3,\"1281\":2,\"1282\":2,\"1283\":3,\"1328\":1,\"1329\":1,\"1348\":1,\"1539\":1,\"1547\":1,\"1766\":1,\"2016\":1,\"2341\":3,\"2346\":7,\"2368\":7,\"2428\":3,\"2430\":5}}],[\"refused\",{\"1\":{\"66\":2}}],[\"referece\",{\"1\":{\"2425\":1,\"2429\":1}}],[\"referenceencoder\",{\"0\":{\"2425\":1},\"1\":{\"2425\":1}}],[\"references\",{\"1\":{\"224\":3,\"652\":1,\"705\":1,\"804\":1,\"932\":1,\"934\":1,\"1061\":1,\"1130\":1,\"1131\":1,\"1172\":1}}],[\"reference\",{\"1\":{\"223\":3,\"224\":3,\"225\":2,\"235\":1,\"285\":1,\"520\":1,\"615\":1,\"617\":1,\"624\":1,\"629\":1,\"635\":1,\"640\":1,\"644\":1,\"648\":1,\"650\":1,\"669\":1,\"670\":1,\"696\":1,\"697\":1,\"833\":1,\"1029\":1,\"1054\":1,\"1062\":2,\"1066\":1,\"1070\":1,\"1071\":1,\"1073\":1,\"1117\":2,\"1124\":1,\"1125\":2,\"1164\":1,\"1170\":1,\"1176\":1,\"1180\":1,\"1181\":1,\"1185\":1,\"1209\":2,\"1210\":1,\"1235\":1,\"1246\":2,\"1247\":1,\"1252\":1,\"1264\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1279\":2,\"1280\":3,\"1281\":2,\"1282\":2,\"1283\":3,\"1308\":1,\"1309\":4,\"1310\":3,\"1311\":3,\"1316\":1,\"1318\":3,\"1319\":3,\"1320\":1,\"1321\":3,\"1322\":3,\"1323\":2,\"1327\":3,\"1328\":4,\"1330\":3,\"1334\":3,\"1385\":1,\"1411\":1,\"1705\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1760\":2,\"1768\":1,\"1901\":1,\"1902\":2,\"1903\":1,\"1904\":2,\"2000\":1,\"2001\":1,\"2018\":1,\"2167\":1,\"2168\":1,\"2176\":1,\"2183\":1,\"2208\":1,\"2308\":1,\"2425\":9,\"2428\":1,\"2429\":5,\"2430\":2}}],[\"refers\",{\"1\":{\"1053\":2,\"2262\":1}}],[\"referred\",{\"1\":{\"80\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"1526\":1,\"1553\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"2325\":1,\"2327\":1}}],[\"refer\",{\"1\":{\"3\":1,\"46\":1,\"55\":1,\"123\":1,\"129\":1,\"141\":1,\"146\":1,\"150\":2,\"156\":1,\"173\":1,\"200\":1,\"201\":1,\"208\":1,\"219\":1,\"243\":1,\"267\":1,\"276\":1,\"286\":1,\"703\":1,\"747\":1,\"755\":1,\"785\":1,\"786\":2,\"800\":1,\"846\":1,\"866\":2,\"867\":1,\"881\":1,\"884\":1,\"921\":1,\"922\":2,\"936\":1,\"937\":1,\"974\":1,\"1031\":1,\"1035\":1,\"1112\":1,\"1113\":1,\"1250\":1,\"1251\":1,\"1967\":1}}],[\"reused\",{\"1\":{\"223\":1}}],[\"reuse\",{\"1\":{\"60\":1,\"71\":1,\"243\":1}}],[\"regulates\",{\"1\":{\"1529\":1}}],[\"regulator\",{\"0\":{\"1529\":1,\"1590\":1,\"1788\":1},\"1\":{\"1529\":4,\"1590\":1,\"1788\":5}}],[\"regularizing\",{\"1\":{\"1855\":1}}],[\"regularized\",{\"1\":{\"1361\":1,\"2420\":1}}],[\"regularization\",{\"1\":{\"46\":1,\"139\":2,\"703\":2,\"755\":2,\"785\":2,\"786\":2,\"800\":2,\"867\":2,\"881\":2,\"884\":2,\"922\":2,\"936\":2,\"937\":2,\"1361\":2}}],[\"regular\",{\"1\":{\"46\":1,\"139\":2,\"1084\":1,\"1131\":1,\"1172\":1,\"1180\":1,\"1181\":1}}],[\"reg\",{\"0\":{\"1361\":1},\"1\":{\"1361\":3}}],[\"regressive\",{\"1\":{\"262\":1,\"1750\":1,\"1811\":1,\"2225\":1,\"2411\":2}}],[\"registry\",{\"1\":{\"903\":1,\"912\":3,\"2132\":1}}],[\"registering\",{\"1\":{\"793\":1}}],[\"registered\",{\"1\":{\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"912\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1876\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2132\":4,\"2134\":4,\"2149\":1,\"2166\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"register\",{\"1\":{\"254\":1,\"793\":2,\"1548\":2,\"2359\":1,\"2367\":1}}],[\"region\",{\"1\":{\"141\":2,\"664\":2,\"2143\":1}}],[\"regard\",{\"1\":{\"197\":1}}],[\"regarding\",{\"1\":{\"70\":1}}],[\"regardless\",{\"1\":{\"55\":1,\"68\":1,\"94\":1,\"123\":1,\"173\":1,\"2139\":1}}],[\"reloaded\",{\"1\":{\"2355\":2}}],[\"reloading\",{\"1\":{\"2044\":4}}],[\"reload\",{\"1\":{\"699\":1,\"739\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"759\":1,\"760\":1,\"761\":1,\"762\":1,\"816\":1,\"846\":1,\"1540\":1,\"1944\":1,\"2355\":2}}],[\"relpositionalencoding\",{\"0\":{\"645\":1,\"1818\":1},\"1\":{\"645\":1,\"662\":1,\"1808\":1,\"1818\":1,\"1891\":1}}],[\"relpositionmultiheadedattention\",{\"0\":{\"644\":1,\"1817\":1},\"1\":{\"644\":2,\"1735\":1,\"1759\":1,\"1785\":1,\"1817\":2,\"1891\":1}}],[\"reliance\",{\"1\":{\"262\":1}}],[\"relies\",{\"1\":{\"262\":1}}],[\"relevant\",{\"1\":{\"246\":1,\"247\":1,\"819\":1}}],[\"relevance\",{\"1\":{\"246\":2,\"247\":2}}],[\"release\",{\"1\":{\"107\":1}}],[\"released\",{\"1\":{\"106\":1}}],[\"relativepositionalencoding\",{\"1\":{\"645\":1}}],[\"relativepositionbias\",{\"0\":{\"646\":1},\"1\":{\"142\":1,\"633\":1,\"634\":1,\"646\":3}}],[\"relatively\",{\"1\":{\"262\":2}}],[\"relative\",{\"1\":{\"142\":1,\"150\":1,\"633\":1,\"634\":1,\"644\":1,\"645\":1,\"646\":3,\"647\":3,\"709\":2,\"774\":2,\"780\":2,\"787\":2,\"831\":1,\"851\":1,\"1290\":2,\"1328\":1,\"1329\":1,\"1546\":1,\"1599\":2,\"1622\":1,\"1626\":1,\"1731\":1,\"1785\":2,\"1786\":1,\"1817\":2,\"1818\":1,\"1822\":1,\"1966\":1,\"2191\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2447\":2}}],[\"relationship\",{\"0\":{\"123\":1}}],[\"relation\",{\"0\":{\"94\":1}}],[\"related\",{\"0\":{\"202\":1,\"207\":1,\"244\":1,\"256\":1,\"297\":1,\"298\":1,\"299\":1,\"303\":1,\"304\":1,\"305\":1,\"306\":1,\"307\":1,\"308\":1,\"311\":1,\"312\":1,\"313\":1,\"314\":1,\"317\":1,\"318\":1,\"319\":1,\"320\":1,\"323\":1,\"324\":1,\"325\":1,\"326\":1,\"329\":1,\"330\":1,\"333\":1,\"334\":1,\"337\":1,\"338\":1,\"339\":1,\"340\":1,\"341\":1,\"344\":1,\"345\":1,\"346\":1,\"347\":1,\"348\":1,\"351\":1,\"352\":1,\"353\":1,\"354\":1,\"355\":1,\"358\":1,\"359\":1,\"360\":1,\"363\":1,\"364\":1,\"365\":1,\"366\":1,\"367\":1,\"370\":1,\"371\":1,\"379\":1,\"380\":1,\"381\":1,\"382\":1,\"383\":1,\"384\":1,\"387\":1,\"388\":1,\"391\":1,\"392\":1,\"393\":1,\"394\":1,\"395\":1,\"398\":1,\"399\":1,\"400\":1,\"401\":1,\"408\":1,\"409\":1,\"410\":1,\"411\":1,\"412\":1,\"413\":1,\"414\":1,\"417\":1,\"418\":1,\"419\":1,\"423\":1,\"424\":1,\"425\":1,\"426\":1,\"427\":1,\"428\":1,\"431\":1,\"432\":1,\"433\":1,\"434\":1,\"435\":1,\"438\":1,\"439\":1,\"440\":1,\"441\":1,\"444\":1,\"445\":1,\"446\":1,\"447\":1,\"448\":1,\"451\":1,\"452\":1,\"453\":1,\"454\":1,\"455\":1,\"456\":1,\"459\":1,\"460\":1,\"465\":1,\"466\":1,\"467\":1,\"468\":1,\"471\":1,\"472\":1,\"473\":1,\"474\":1,\"477\":1,\"478\":1,\"479\":1,\"480\":1,\"483\":1,\"486\":1,\"487\":1,\"488\":1,\"489\":1,\"492\":1,\"493\":1,\"494\":1,\"495\":1,\"500\":1,\"501\":1,\"502\":1,\"503\":1,\"504\":1,\"507\":1,\"508\":1,\"509\":1,\"510\":1},\"1\":{\"42\":1,\"79\":1,\"111\":1,\"138\":1,\"143\":1,\"150\":1,\"161\":1,\"199\":1,\"204\":1,\"222\":1,\"223\":11,\"224\":1,\"225\":1,\"228\":2,\"240\":1,\"253\":1,\"268\":1,\"277\":1,\"285\":1,\"785\":2,\"1053\":2,\"1992\":1,\"1993\":1,\"1995\":1,\"2176\":1,\"2184\":1,\"2235\":1,\"2236\":1}}],[\"rel\",{\"1\":{\"84\":1,\"142\":2,\"243\":1,\"633\":2,\"634\":2,\"644\":2,\"700\":3,\"709\":4,\"710\":1,\"733\":3,\"734\":3,\"774\":2,\"780\":4,\"787\":1,\"851\":1,\"1107\":2,\"1519\":2,\"1526\":3,\"1535\":2,\"1536\":4,\"1546\":2,\"1552\":2,\"1553\":2,\"1598\":3,\"1599\":4,\"1600\":3,\"1622\":2,\"1625\":2,\"1626\":2,\"1785\":2,\"1817\":2,\"1994\":3,\"2020\":1,\"2126\":3,\"2191\":4,\"2239\":5,\"2240\":3,\"2411\":4,\"2412\":4,\"2423\":4,\"2447\":4}}],[\"relu\",{\"1\":{\"43\":4,\"141\":2,\"619\":1,\"620\":3,\"633\":1,\"637\":1,\"638\":1,\"674\":2,\"693\":2,\"805\":1,\"807\":2,\"851\":1,\"978\":1,\"1029\":1,\"1070\":1,\"1071\":1,\"1107\":2,\"1117\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1130\":1,\"1131\":1,\"1136\":2,\"1139\":1,\"1141\":4,\"1145\":1,\"1185\":1,\"1232\":1,\"1235\":1,\"1261\":2,\"1264\":1,\"1265\":1,\"1267\":2,\"1268\":3,\"1269\":1,\"1270\":1,\"1271\":1,\"1273\":1,\"1274\":1,\"1278\":2,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1334\":1,\"1736\":3,\"1749\":1,\"1863\":2}}],[\"rely\",{\"1\":{\"40\":1,\"138\":1,\"139\":1,\"1502\":1}}],[\"re\",{\"1\":{\"36\":1,\"261\":1,\"262\":2,\"759\":1,\"819\":1,\"1064\":1,\"1078\":1,\"1153\":1,\"1202\":1,\"1262\":1,\"1290\":1,\"1366\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"2232\":1,\"2238\":1}}],[\"remarks\",{\"1\":{\"927\":1}}],[\"remark\",{\"1\":{\"768\":1}}],[\"remaining\",{\"1\":{\"1597\":1,\"2134\":1,\"2147\":1}}],[\"remain\",{\"1\":{\"27\":1}}],[\"remote\",{\"1\":{\"246\":2}}],[\"removal\",{\"1\":{\"199\":1,\"200\":1,\"204\":1,\"205\":1,\"216\":1,\"217\":1,\"227\":1,\"284\":1,\"285\":1}}],[\"removing\",{\"1\":{\"197\":1}}],[\"removes\",{\"1\":{\"2039\":1,\"2044\":3,\"2136\":1}}],[\"removed\",{\"1\":{\"74\":1,\"211\":1,\"242\":1,\"252\":1,\"266\":1,\"275\":1,\"1259\":1,\"1261\":1,\"1484\":3,\"1872\":1,\"1936\":1}}],[\"remove\",{\"0\":{\"529\":1,\"1380\":1,\"1570\":1,\"2496\":1,\"2497\":1},\"1\":{\"32\":1,\"41\":1,\"205\":1,\"211\":1,\"217\":1,\"222\":1,\"223\":1,\"228\":1,\"240\":1,\"242\":2,\"266\":1,\"267\":2,\"269\":2,\"275\":1,\"276\":2,\"278\":2,\"285\":1,\"286\":2,\"290\":1,\"481\":2,\"529\":1,\"1399\":1,\"1405\":1,\"1434\":1,\"1436\":1,\"1505\":1,\"1506\":1,\"1513\":2,\"1548\":2,\"1551\":2,\"1555\":1,\"1570\":1,\"1592\":2,\"1597\":4,\"1605\":2,\"1606\":2,\"1609\":2,\"1610\":2,\"1619\":2,\"1628\":2,\"1808\":1,\"1927\":1,\"2275\":1,\"2285\":1,\"2292\":1,\"2293\":1,\"2463\":1,\"2464\":1,\"2496\":1,\"2497\":1}}],[\"remember\",{\"1\":{\"23\":1,\"24\":1,\"127\":1,\"224\":2,\"225\":1}}],[\"request\",{\"1\":{\"168\":1}}],[\"requested\",{\"1\":{\"22\":1,\"2134\":1,\"2311\":1}}],[\"requiring\",{\"1\":{\"106\":1,\"756\":1,\"773\":1,\"866\":1,\"867\":1}}],[\"require\",{\"1\":{\"96\":1,\"527\":1,\"1224\":1,\"1225\":1,\"1526\":2,\"1553\":2,\"1598\":2,\"1600\":2,\"1625\":2,\"1971\":2,\"1976\":1,\"2133\":1,\"2222\":2,\"2403\":2,\"2411\":1,\"2445\":2}}],[\"requires\",{\"1\":{\"71\":1,\"81\":1,\"97\":1,\"98\":1,\"99\":1,\"106\":1,\"175\":1,\"285\":1,\"286\":1,\"1720\":1,\"1721\":1,\"2355\":1}}],[\"requirement\",{\"1\":{\"31\":1,\"95\":1,\"2249\":1}}],[\"requirements\",{\"0\":{\"159\":1,\"160\":1,\"248\":1},\"1\":{\"31\":1,\"161\":1,\"194\":1,\"197\":1,\"249\":1,\"269\":1,\"278\":1,\"1264\":1,\"1334\":1,\"2246\":2,\"2248\":2,\"2249\":3,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2}}],[\"required=false\",{\"1\":{\"2478\":1}}],[\"required\",{\"1\":{\"22\":1,\"43\":1,\"44\":1,\"74\":1,\"75\":1,\"78\":1,\"81\":3,\"97\":1,\"159\":1,\"162\":1,\"173\":1,\"197\":1,\"223\":2,\"232\":1,\"235\":1,\"243\":1,\"248\":1,\"258\":1,\"260\":1,\"262\":2,\"285\":2,\"699\":1,\"703\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"755\":1,\"760\":2,\"785\":2,\"797\":1,\"820\":1,\"829\":3,\"830\":1,\"859\":1,\"1008\":1,\"1162\":1,\"1484\":1,\"1526\":2,\"1553\":2,\"1598\":2,\"1600\":2,\"1625\":2,\"1723\":1,\"1724\":1,\"1805\":1,\"1822\":1,\"1883\":1,\"1962\":1,\"1965\":1,\"1971\":2,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"2006\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2065\":1,\"2132\":2,\"2134\":1,\"2184\":1,\"2215\":1,\"2216\":1,\"2222\":2,\"2246\":4,\"2247\":4,\"2248\":4,\"2249\":5,\"2250\":4,\"2251\":4,\"2252\":4,\"2253\":4,\"2254\":4,\"2255\":4,\"2256\":4,\"2257\":4,\"2259\":4,\"2260\":4,\"2261\":4,\"2262\":2,\"2263\":4,\"2264\":4,\"2265\":4,\"2266\":4,\"2267\":4,\"2268\":4,\"2269\":4,\"2270\":4,\"2271\":4,\"2272\":4,\"2273\":4,\"2355\":2,\"2403\":2,\"2415\":1,\"2417\":1,\"2419\":1,\"2445\":2}}],[\"res2net\",{\"1\":{\"2187\":1,\"2192\":1,\"2203\":1}}],[\"resynthesized\",{\"1\":{\"1391\":3,\"1403\":3,\"1406\":1,\"1410\":3,\"1468\":3}}],[\"resswinblock\",{\"0\":{\"1235\":1},\"1\":{\"1235\":2,\"1282\":3}}],[\"resblocks\",{\"1\":{\"1552\":2,\"1626\":2}}],[\"resblock2\",{\"0\":{\"1435\":1},\"1\":{\"1435\":1}}],[\"resblock1\",{\"0\":{\"1433\":1},\"1\":{\"1433\":1}}],[\"resblock\",{\"0\":{\"2196\":1},\"1\":{\"1211\":1,\"1242\":1,\"1398\":3,\"1404\":3,\"1408\":3,\"1410\":3,\"1513\":4,\"1526\":2,\"1548\":4,\"1551\":4,\"1552\":4,\"1553\":2,\"1592\":4,\"1598\":2,\"1599\":4,\"1600\":2,\"1625\":2,\"1626\":4,\"2196\":1,\"2203\":1}}],[\"resampling\",{\"1\":{\"1674\":1,\"1833\":1,\"2065\":1,\"2134\":1}}],[\"resampled\",{\"1\":{\"2000\":3,\"2065\":1}}],[\"resampler\",{\"1\":{\"1539\":1}}],[\"resample=none\",{\"1\":{\"1236\":1}}],[\"resample=1\",{\"1\":{\"1103\":1}}],[\"resample\",{\"0\":{\"1571\":1},\"1\":{\"1110\":1,\"1539\":1,\"1571\":1,\"1833\":1}}],[\"resamp\",{\"1\":{\"1211\":1}}],[\"resize\",{\"1\":{\"828\":1,\"2443\":1}}],[\"residual=false\",{\"1\":{\"1758\":1,\"2231\":1}}],[\"residual=none\",{\"1\":{\"820\":1,\"828\":1,\"830\":1}}],[\"residualaffinecouplinglayer\",{\"0\":{\"1613\":1},\"1\":{\"1613\":2}}],[\"residualaffinecouplingblock\",{\"0\":{\"1612\":1},\"1\":{\"1612\":2}}],[\"residualvectorquantizer\",{\"0\":{\"1441\":1},\"1\":{\"1441\":1}}],[\"residualvectorquantization\",{\"0\":{\"1439\":1},\"1\":{\"1439\":1}}],[\"residualblock\",{\"0\":{\"1236\":1,\"1614\":1,\"2426\":1},\"1\":{\"1236\":1,\"1614\":2,\"2426\":1}}],[\"residualstack\",{\"0\":{\"1615\":1},\"1\":{\"1615\":2}}],[\"residuals\",{\"1\":{\"828\":1,\"830\":1}}],[\"residual\",{\"0\":{\"689\":1,\"718\":1,\"751\":1,\"757\":1,\"809\":2,\"1432\":1,\"1441\":1,\"1578\":1,\"1579\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1},\"1\":{\"276\":1,\"689\":3,\"718\":3,\"751\":2,\"757\":2,\"796\":1,\"809\":5,\"820\":3,\"828\":4,\"830\":3,\"939\":2,\"1153\":1,\"1198\":1,\"1235\":1,\"1386\":1,\"1389\":2,\"1391\":2,\"1396\":2,\"1401\":2,\"1403\":2,\"1432\":1,\"1439\":1,\"1441\":4,\"1450\":8,\"1452\":8,\"1454\":8,\"1456\":8,\"1458\":2,\"1460\":2,\"1466\":2,\"1468\":2,\"1513\":3,\"1548\":3,\"1551\":3,\"1578\":1,\"1579\":1,\"1592\":3,\"1596\":1,\"1599\":3,\"1605\":2,\"1610\":4,\"1612\":3,\"1613\":2,\"1614\":2,\"1615\":2,\"1628\":9,\"1748\":1,\"1758\":1,\"1759\":1,\"1854\":1,\"1993\":1,\"2168\":1,\"2198\":1,\"2231\":1,\"2240\":1,\"2245\":1,\"2426\":1,\"2431\":1,\"2460\":1}}],[\"reshapemodule\",{\"0\":{\"1437\":1},\"1\":{\"1437\":1}}],[\"reshapes\",{\"1\":{\"1165\":1}}],[\"reshape\",{\"1\":{\"749\":1}}],[\"reshaped\",{\"1\":{\"703\":1,\"755\":1,\"785\":2}}],[\"resnetencoder\",{\"0\":{\"2198\":1},\"1\":{\"2198\":1}}],[\"resnetblockddpmpp\",{\"0\":{\"1242\":1},\"1\":{\"1242\":1}}],[\"resnetblockddpm\",{\"0\":{\"1240\":1},\"1\":{\"1240\":1}}],[\"resnetblockbigganpp\",{\"0\":{\"1238\":1},\"1\":{\"1238\":1}}],[\"resnet=none\",{\"1\":{\"839\":1}}],[\"resnets\",{\"1\":{\"828\":1}}],[\"resnet\",{\"0\":{\"807\":1,\"2177\":1,\"2181\":1,\"2198\":1},\"1\":{\"674\":4,\"807\":1,\"1240\":1,\"2177\":1,\"2181\":1,\"2198\":6}}],[\"resch\",{\"1\":{\"1854\":1}}],[\"resch=512\",{\"1\":{\"1854\":1}}],[\"rescore\",{\"0\":{\"1378\":1,\"1379\":1,\"1380\":1}}],[\"rescoring\",{\"1\":{\"505\":2}}],[\"rescale=true\",{\"1\":{\"1211\":1,\"1238\":1}}],[\"rescale=false\",{\"1\":{\"1057\":1,\"1242\":1}}],[\"rescale\",{\"1\":{\"142\":2,\"643\":3,\"787\":1,\"851\":1}}],[\"res\",{\"0\":{\"1620\":1,\"1621\":1},\"1\":{\"267\":4,\"276\":4,\"1148\":1,\"1211\":1,\"1452\":1,\"1456\":1,\"1620\":1,\"1621\":1,\"1674\":2,\"1833\":1}}],[\"respect\",{\"1\":{\"881\":1,\"884\":1}}],[\"respective\",{\"1\":{\"44\":1,\"144\":1,\"259\":1}}],[\"respectively\",{\"0\":{\"61\":1},\"1\":{\"22\":1,\"43\":1,\"44\":1,\"47\":1,\"48\":2,\"59\":1,\"81\":1,\"96\":1,\"144\":1,\"224\":1,\"225\":3,\"1515\":2,\"1604\":1,\"2151\":1,\"2310\":1}}],[\"responses\",{\"1\":{\"246\":3,\"247\":2,\"2044\":1}}],[\"response\",{\"0\":{\"1557\":1},\"1\":{\"246\":1,\"247\":5,\"1309\":1,\"1311\":1,\"1321\":1,\"1322\":1,\"1557\":1,\"1631\":1,\"2039\":2,\"2044\":1,\"2049\":4,\"2054\":2}}],[\"resolved\",{\"1\":{\"716\":1}}],[\"resolve\",{\"0\":{\"2395\":1},\"1\":{\"269\":1,\"278\":1,\"2395\":1}}],[\"resolutions=\",{\"1\":{\"1211\":1}}],[\"resolution=\",{\"1\":{\"1029\":1,\"1235\":1,\"1281\":1,\"1282\":1}}],[\"resolution\",{\"1\":{\"242\":1,\"243\":1,\"1029\":1,\"1064\":3,\"1210\":1,\"1235\":1,\"1262\":2,\"1280\":2,\"1281\":1,\"1282\":1,\"1716\":1,\"2361\":1,\"2443\":1}}],[\"resource\",{\"1\":{\"168\":1,\"2000\":2,\"2001\":3}}],[\"resources\",{\"1\":{\"66\":2,\"2044\":4}}],[\"restart\",{\"0\":{\"2014\":1},\"1\":{\"2014\":2}}],[\"restore\",{\"1\":{\"1375\":1,\"2134\":1}}],[\"rest\",{\"1\":{\"197\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"829\":2,\"830\":1,\"859\":1,\"991\":1,\"1343\":1,\"2249\":1,\"2253\":1}}],[\"restriction\",{\"1\":{\"2309\":1}}],[\"restricts\",{\"1\":{\"2249\":1,\"2253\":1}}],[\"restricted\",{\"1\":{\"139\":1}}],[\"restrict\",{\"1\":{\"91\":1,\"1645\":1,\"1650\":1}}],[\"reserve\",{\"1\":{\"1259\":1,\"1261\":1}}],[\"reserved\",{\"1\":{\"625\":1,\"1132\":1,\"1167\":1,\"2369\":1}}],[\"resencoder\",{\"0\":{\"805\":1},\"1\":{\"805\":1}}],[\"reset\",{\"0\":{\"1918\":1,\"2117\":1},\"1\":{\"150\":1,\"616\":2,\"617\":2,\"618\":2,\"620\":2,\"624\":2,\"626\":2,\"630\":2,\"633\":2,\"636\":2,\"637\":2,\"638\":2,\"639\":2,\"645\":1,\"646\":2,\"647\":2,\"649\":2,\"697\":1,\"787\":1,\"971\":1,\"975\":1,\"1072\":1,\"1074\":1,\"1079\":2,\"1179\":1,\"1513\":2,\"1548\":2,\"1551\":2,\"1578\":2,\"1580\":2,\"1592\":2,\"1605\":2,\"1606\":2,\"1618\":2,\"1619\":2,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1720\":2,\"1726\":2,\"1727\":2,\"1768\":2,\"1770\":3,\"1771\":5,\"1784\":1,\"1801\":2,\"1808\":1,\"1818\":1,\"1820\":2,\"1837\":1,\"1918\":1,\"2367\":1}}],[\"researchers\",{\"1\":{\"246\":1}}],[\"research\",{\"1\":{\"9\":1,\"16\":1,\"202\":1,\"536\":1}}],[\"resumption\",{\"1\":{\"2134\":1}}],[\"resuming\",{\"1\":{\"1645\":1,\"1650\":1,\"2249\":1,\"2253\":1}}],[\"resumed\",{\"1\":{\"87\":1,\"91\":2}}],[\"resume\",{\"0\":{\"87\":1,\"2079\":1},\"1\":{\"87\":1,\"377\":2,\"2338\":1,\"2339\":2,\"2348\":1,\"2354\":2,\"2369\":1,\"2370\":2,\"2372\":1}}],[\"resulotion\",{\"1\":{\"1262\":1}}],[\"resulted\",{\"1\":{\"1814\":1,\"1816\":1,\"2359\":1}}],[\"resulting\",{\"1\":{\"702\":1,\"1350\":1,\"1748\":1,\"2411\":1}}],[\"result2json\",{\"0\":{\"594\":1},\"1\":{\"594\":1}}],[\"result\",{\"0\":{\"534\":1},\"1\":{\"90\":1,\"91\":1,\"287\":13,\"335\":2,\"594\":1,\"750\":1,\"755\":1,\"803\":2,\"973\":1,\"1558\":1,\"1926\":1}}],[\"results\",{\"0\":{\"261\":1},\"1\":{\"24\":1,\"50\":3,\"127\":1,\"128\":1,\"136\":1,\"199\":1,\"200\":1,\"201\":2,\"204\":1,\"205\":1,\"216\":1,\"217\":1,\"218\":1,\"223\":3,\"227\":1,\"228\":1,\"234\":1,\"235\":1,\"240\":1,\"242\":1,\"247\":2,\"253\":1,\"254\":1,\"260\":1,\"261\":1,\"262\":1,\"267\":5,\"276\":5,\"284\":1,\"285\":1,\"286\":6,\"290\":1,\"526\":1,\"564\":2,\"777\":1,\"1156\":1,\"1720\":1,\"1721\":1,\"1725\":1,\"1806\":1,\"1862\":1,\"1940\":1,\"1942\":1,\"2131\":1,\"2354\":1}}],[\"rectangular\",{\"1\":{\"1316\":1}}],[\"recursive\",{\"0\":{\"1952\":1,\"2318\":2,\"2319\":2,\"2320\":2},\"1\":{\"943\":1,\"1948\":1,\"1952\":1,\"2318\":2,\"2319\":2,\"2320\":2}}],[\"recursive=false\",{\"1\":{\"943\":1}}],[\"recursive=true\",{\"1\":{\"942\":1}}],[\"recursively\",{\"1\":{\"704\":1,\"2164\":1,\"2309\":1,\"2323\":1,\"2489\":1}}],[\"recurrent\",{\"1\":{\"724\":1,\"725\":1,\"726\":1,\"728\":1,\"729\":1,\"744\":1,\"784\":1,\"798\":1,\"819\":1,\"824\":1,\"828\":1,\"829\":1,\"830\":1,\"859\":1,\"862\":1,\"1124\":1,\"1125\":1,\"1176\":1,\"1712\":1,\"1856\":1}}],[\"recurrently\",{\"1\":{\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"744\":1,\"784\":1,\"828\":1,\"829\":2,\"830\":1}}],[\"rec\",{\"1\":{\"527\":1}}],[\"receptive=\",{\"1\":{\"2213\":1,\"2214\":1}}],[\"receptive\",{\"1\":{\"1610\":2,\"1628\":2}}],[\"receptance\",{\"1\":{\"642\":1}}],[\"recent\",{\"1\":{\"67\":3,\"290\":1}}],[\"receives\",{\"1\":{\"820\":1,\"828\":1,\"830\":1,\"1804\":1}}],[\"received\",{\"1\":{\"82\":1}}],[\"receive\",{\"1\":{\"27\":1}}],[\"reconscruct\",{\"0\":{\"2115\":1},\"1\":{\"2115\":1}}],[\"reconstructed\",{\"1\":{\"2136\":1,\"2490\":1}}],[\"reconstruct\",{\"0\":{\"2116\":1},\"1\":{\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"2116\":1}}],[\"reconstruction\",{\"1\":{\"1350\":1,\"1608\":1,\"2136\":1}}],[\"reconstructs\",{\"1\":{\"1350\":1,\"2116\":1}}],[\"reconfigure\",{\"1\":{\"1250\":1,\"1251\":1}}],[\"recover\",{\"1\":{\"1124\":1}}],[\"recommmend\",{\"1\":{\"1833\":1}}],[\"recommendations\",{\"1\":{\"821\":2}}],[\"recommended\",{\"1\":{\"67\":1,\"101\":1,\"128\":1,\"224\":1,\"225\":1,\"786\":1,\"817\":1,\"821\":1,\"866\":1,\"921\":1,\"1124\":1,\"1748\":1,\"2000\":1,\"2001\":1,\"2355\":1}}],[\"recommend\",{\"1\":{\"47\":1,\"60\":1,\"62\":1,\"70\":1,\"162\":1,\"196\":1,\"212\":1,\"218\":1,\"267\":2,\"268\":1,\"269\":1,\"276\":2,\"277\":1,\"278\":1,\"286\":2,\"290\":1,\"536\":1,\"2355\":1}}],[\"recompute\",{\"1\":{\"1727\":3}}],[\"recombined\",{\"1\":{\"616\":1,\"1915\":1}}],[\"recombine\",{\"0\":{\"1915\":1},\"1\":{\"616\":2,\"1915\":2}}],[\"recordingset\",{\"1\":{\"2159\":1}}],[\"recordings\",{\"1\":{\"235\":1,\"246\":1,\"2139\":1,\"2150\":1}}],[\"recording\",{\"0\":{\"2150\":1,\"2160\":1},\"1\":{\"196\":1,\"268\":1,\"277\":1,\"1002\":1,\"2150\":6,\"2156\":7,\"2159\":1,\"2160\":4}}],[\"record\",{\"1\":{\"73\":3,\"290\":1,\"527\":1}}],[\"recoding\",{\"0\":{\"73\":1}}],[\"recog\",{\"0\":{\"527\":1},\"1\":{\"41\":1,\"46\":1,\"175\":1,\"527\":7}}],[\"recognize\",{\"1\":{\"1720\":2}}],[\"recognized\",{\"1\":{\"581\":1}}],[\"recognizes\",{\"1\":{\"3\":1}}],[\"recognition\",{\"0\":{\"178\":1,\"184\":1,\"198\":1,\"203\":1,\"226\":1,\"292\":1},\"1\":{\"11\":1,\"38\":1,\"41\":1,\"162\":1,\"173\":1,\"175\":1,\"190\":1,\"193\":2,\"202\":1,\"228\":1,\"247\":1,\"256\":1,\"768\":2,\"833\":1,\"849\":1,\"1126\":1,\"1327\":1,\"1330\":1,\"1710\":1,\"1717\":1,\"1729\":1,\"1730\":2,\"1880\":1,\"2167\":1,\"2176\":1,\"2184\":1,\"2192\":1,\"2198\":1,\"2208\":1,\"2209\":1,\"2365\":1}}],[\"recipe\",{\"0\":{\"109\":1,\"194\":1,\"197\":1,\"200\":1,\"205\":1,\"211\":1,\"217\":1,\"224\":1,\"235\":1,\"242\":1,\"254\":1,\"266\":1,\"273\":1,\"275\":1,\"285\":1},\"1\":{\"23\":1,\"24\":1,\"37\":3,\"38\":1,\"39\":2,\"53\":1,\"68\":1,\"70\":3,\"107\":1,\"110\":1,\"111\":1,\"124\":2,\"191\":1,\"193\":3,\"194\":3,\"196\":2,\"197\":8,\"198\":1,\"199\":1,\"200\":2,\"201\":1,\"203\":1,\"204\":1,\"205\":1,\"206\":2,\"208\":1,\"209\":1,\"210\":1,\"211\":2,\"212\":4,\"213\":1,\"215\":1,\"216\":1,\"217\":1,\"218\":3,\"219\":1,\"221\":1,\"222\":3,\"224\":7,\"226\":1,\"227\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":5,\"233\":1,\"234\":1,\"235\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":3,\"242\":1,\"243\":2,\"245\":1,\"250\":1,\"251\":1,\"252\":1,\"253\":1,\"254\":1,\"255\":2,\"257\":1,\"258\":3,\"259\":2,\"263\":1,\"264\":1,\"265\":1,\"266\":1,\"267\":4,\"268\":3,\"269\":1,\"273\":1,\"274\":2,\"275\":1,\"276\":4,\"277\":3,\"278\":1,\"283\":1,\"284\":2,\"285\":1,\"286\":4,\"290\":7,\"291\":1,\"292\":1,\"515\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"recipes\",{\"0\":{\"23\":1,\"107\":1,\"232\":1,\"258\":1},\"1\":{\"8\":1,\"23\":1,\"24\":1,\"39\":1,\"70\":2,\"97\":1,\"106\":1,\"107\":6,\"108\":1,\"156\":1,\"159\":1,\"162\":1,\"163\":1,\"165\":1,\"191\":1,\"192\":1,\"193\":1,\"194\":1,\"197\":3,\"200\":1,\"201\":2,\"208\":1,\"223\":1,\"224\":3,\"226\":1,\"232\":1,\"243\":1,\"258\":1,\"290\":1}}],[\"replicated\",{\"1\":{\"1788\":1}}],[\"replacing\",{\"1\":{\"846\":1,\"2220\":1}}],[\"replacement\",{\"0\":{\"1701\":1},\"1\":{\"286\":1,\"1701\":1}}],[\"replaces\",{\"1\":{\"200\":2,\"1737\":1}}],[\"replace\",{\"0\":{\"1695\":1},\"1\":{\"3\":1,\"24\":1,\"84\":1,\"119\":1,\"128\":1,\"269\":1,\"273\":1,\"278\":1,\"286\":1,\"295\":2,\"415\":2,\"796\":1,\"833\":1,\"1002\":1,\"1327\":1,\"1330\":1,\"1400\":2,\"1441\":1,\"1469\":1,\"1664\":1,\"1665\":1,\"1691\":1,\"1695\":2,\"1795\":1,\"1907\":1,\"1916\":1}}],[\"repititons\",{\"1\":{\"1618\":1}}],[\"rep\",{\"1\":{\"1264\":2,\"1334\":1}}],[\"repetitions\",{\"1\":{\"1264\":1,\"1334\":1}}],[\"repetition\",{\"1\":{\"315\":2,\"469\":2,\"1720\":1}}],[\"repeat=3\",{\"1\":{\"1854\":1}}],[\"repeat=1\",{\"1\":{\"820\":1,\"828\":1,\"1858\":1}}],[\"repeating\",{\"1\":{\"1788\":1}}],[\"repeats=4\",{\"1\":{\"1264\":1,\"1334\":1}}],[\"repeats\",{\"1\":{\"982\":1,\"1264\":1,\"1273\":1,\"1274\":1,\"1334\":1,\"1618\":3}}],[\"repeatable\",{\"1\":{\"47\":1}}],[\"repeat\",{\"0\":{\"1796\":1,\"1917\":2},\"1\":{\"43\":3,\"98\":1,\"820\":1,\"828\":1,\"982\":1,\"1273\":1,\"1274\":1,\"1552\":2,\"1553\":1,\"1559\":1,\"1611\":1,\"1749\":2,\"1796\":2,\"1854\":2,\"1863\":2,\"1906\":1,\"1917\":4}}],[\"repeated\",{\"0\":{\"1380\":1},\"1\":{\"43\":1,\"79\":1,\"820\":1,\"828\":1,\"1749\":1,\"1863\":1,\"1917\":1}}],[\"repo\",{\"1\":{\"127\":1,\"138\":1,\"211\":1,\"267\":1,\"276\":1,\"286\":2,\"2422\":1}}],[\"repo>\",{\"1\":{\"127\":2}}],[\"reported\",{\"0\":{\"2398\":1},\"1\":{\"2398\":1}}],[\"reportedvalue\",{\"0\":{\"2358\":1},\"1\":{\"2332\":1,\"2358\":1,\"2373\":1,\"2374\":1,\"2398\":1}}],[\"reporter\",{\"0\":{\"1819\":1,\"2332\":1,\"2358\":1,\"2359\":2,\"2367\":1,\"2373\":1,\"2374\":1,\"2398\":1,\"2400\":1},\"1\":{\"87\":1,\"1819\":2,\"1949\":4,\"2332\":1,\"2338\":4,\"2347\":2,\"2354\":1,\"2358\":1,\"2359\":10,\"2365\":2,\"2367\":3,\"2369\":5,\"2371\":2,\"2373\":1,\"2374\":1,\"2398\":1,\"2400\":1}}],[\"reports\",{\"1\":{\"133\":1}}],[\"report\",{\"1\":{\"106\":1,\"625\":6,\"627\":4,\"736\":2,\"737\":1,\"740\":4,\"777\":2,\"1640\":2,\"1760\":3,\"1797\":1,\"1819\":2,\"1823\":1,\"1959\":1,\"1975\":3,\"1996\":2,\"1997\":2,\"2127\":2,\"2221\":3}}],[\"reporting\",{\"1\":{\"56\":1,\"139\":2,\"261\":1,\"625\":1}}],[\"repository\",{\"1\":{\"1\":2,\"3\":1,\"26\":1,\"127\":1,\"1211\":1}}],[\"repr\",{\"1\":{\"1064\":1,\"1078\":1,\"1153\":1,\"1202\":1,\"1262\":1,\"1290\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"2232\":1,\"2238\":1}}],[\"represented\",{\"1\":{\"755\":1,\"824\":1,\"881\":1,\"884\":1}}],[\"representing\",{\"1\":{\"702\":1,\"755\":2,\"756\":1,\"773\":1,\"785\":2,\"866\":1,\"867\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"1050\":2,\"1116\":2,\"1161\":2,\"1189\":2,\"1218\":2,\"1221\":2,\"1229\":2,\"1244\":2,\"1245\":1}}],[\"represents\",{\"1\":{\"276\":1,\"703\":2,\"755\":3,\"764\":1,\"785\":3,\"802\":1,\"804\":2,\"878\":5,\"879\":5,\"881\":5,\"882\":5,\"883\":5,\"884\":5,\"919\":4,\"932\":2,\"934\":2,\"2130\":1,\"2136\":1,\"2353\":1,\"2364\":1}}],[\"represent\",{\"1\":{\"82\":1,\"252\":1,\"1002\":1,\"1934\":1,\"2136\":1}}],[\"representations\",{\"0\":{\"128\":1},\"1\":{\"106\":1,\"128\":1,\"262\":3,\"699\":1,\"846\":1,\"1155\":2,\"1157\":2,\"2130\":4,\"2133\":1,\"2218\":1,\"2220\":2}}],[\"representation\",{\"0\":{\"252\":1},\"1\":{\"44\":1,\"128\":1,\"190\":1,\"252\":1,\"699\":3,\"815\":1,\"824\":1,\"864\":1,\"1064\":1,\"1078\":1,\"1153\":1,\"1202\":1,\"1262\":1,\"1290\":1,\"1334\":1,\"1350\":1,\"1395\":1,\"1441\":2,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1513\":1,\"1525\":1,\"1546\":3,\"1548\":1,\"1551\":1,\"1552\":2,\"1592\":1,\"1599\":1,\"1601\":1,\"1611\":1,\"1622\":1,\"1626\":2,\"1645\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"2101\":1,\"2133\":1,\"2136\":1,\"2143\":1,\"2187\":1,\"2188\":1,\"2192\":1,\"2203\":1,\"2209\":1,\"2218\":1,\"2232\":1,\"2238\":1}}],[\"reproduces\",{\"1\":{\"2249\":1,\"2253\":1}}],[\"reproduce\",{\"1\":{\"107\":2}}],[\"reproducability\",{\"1\":{\"26\":1}}],[\"reproducibility\",{\"0\":{\"104\":1},\"1\":{\"232\":1,\"235\":1,\"258\":1,\"833\":1,\"1645\":1,\"1650\":1,\"2000\":1,\"2001\":1,\"2134\":1}}],[\"reproducible\",{\"1\":{\"8\":1,\"9\":1,\"104\":1}}],[\"reproducing\",{\"1\":{\"6\":1,\"244\":1}}],[\"reproduction\",{\"1\":{\"7\":1,\"43\":1}}],[\"cwskattention\",{\"0\":{\"2213\":1},\"1\":{\"2213\":1}}],[\"cwd\",{\"1\":{\"168\":2}}],[\"cx\",{\"1\":{\"1712\":1}}],[\"c=2\",{\"1\":{\"2479\":1}}],[\"c=4\",{\"1\":{\"2479\":1}}],[\"c=none\",{\"1\":{\"1548\":2}}],[\"c=1\",{\"1\":{\"823\":1,\"824\":1}}],[\"cnt\",{\"1\":{\"1521\":6,\"2228\":6,\"2229\":6}}],[\"cnnfrontend\",{\"0\":{\"702\":1},\"1\":{\"702\":2}}],[\"cnn\",{\"0\":{\"702\":1,\"712\":1,\"722\":1,\"854\":1,\"887\":1},\"1\":{\"702\":1,\"709\":4,\"710\":2,\"712\":1,\"722\":1,\"774\":4,\"780\":3,\"854\":1,\"887\":1,\"1107\":2,\"1526\":1,\"1598\":1,\"1599\":3,\"1600\":1,\"1863\":1,\"1994\":1,\"2126\":2,\"2191\":4,\"2239\":1,\"2240\":1,\"2411\":3,\"2412\":3,\"2423\":3,\"2447\":3}}],[\"cg\",{\"1\":{\"1246\":2}}],[\"cgmlp\",{\"0\":{\"713\":1,\"715\":1,\"781\":1,\"783\":1},\"1\":{\"243\":2,\"700\":4,\"701\":5,\"713\":2,\"715\":1,\"733\":2,\"734\":2,\"735\":2,\"780\":4,\"781\":2,\"783\":1}}],[\"cbn\",{\"1\":{\"1118\":2}}],[\"c3\",{\"1\":{\"994\":1}}],[\"c1\",{\"1\":{\"994\":1,\"1027\":2}}],[\"ckpt\",{\"1\":{\"699\":2,\"2333\":1}}],[\"cfln\",{\"1\":{\"1061\":1,\"1062\":1,\"1072\":1}}],[\"cfg=none\",{\"1\":{\"698\":1,\"839\":1}}],[\"cfg\",{\"1\":{\"675\":2,\"698\":1,\"1974\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1}}],[\"cfsd\",{\"1\":{\"286\":3}}],[\"cjk\",{\"1\":{\"205\":1}}],[\"cse\",{\"1\":{\"1125\":1}}],[\"csv\",{\"1\":{\"987\":1,\"989\":1,\"1019\":1}}],[\"csgu\",{\"1\":{\"715\":1,\"783\":1}}],[\"csmsc\",{\"1\":{\"536\":8}}],[\"cs\",{\"1\":{\"202\":1}}],[\"csj\",{\"1\":{\"197\":1,\"527\":1}}],[\"cp\",{\"1\":{\"197\":1,\"243\":2,\"259\":1}}],[\"cpurnnt\",{\"0\":{\"703\":1,\"716\":1,\"717\":1},\"1\":{\"703\":1,\"716\":1,\"717\":3}}],[\"cpu>\",{\"1\":{\"161\":2}}],[\"cpu\",{\"0\":{\"25\":1,\"703\":2,\"716\":2,\"717\":2,\"773\":2,\"918\":2,\"936\":1},\"1\":{\"25\":1,\"26\":2,\"28\":2,\"29\":1,\"41\":2,\"162\":3,\"173\":1,\"703\":3,\"716\":3,\"717\":4,\"773\":2,\"918\":2,\"936\":3,\"1373\":1,\"1678\":1,\"1926\":2,\"2043\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2130\":8,\"2133\":2,\"2136\":2,\"2138\":2,\"2249\":2,\"2263\":1,\"2268\":1,\"2270\":1,\"2271\":1,\"2314\":1}}],[\"cycleiterator\",{\"0\":{\"2042\":1}}],[\"cycle\",{\"1\":{\"2014\":8,\"2423\":1,\"2428\":3}}],[\"cycles\",{\"1\":{\"262\":1,\"1610\":1,\"1628\":1}}],[\"cygwin\",{\"1\":{\"161\":1}}],[\"cyberciti\",{\"1\":{\"67\":1}}],[\"cta\",{\"1\":{\"705\":1}}],[\"ctareduce\",{\"0\":{\"704\":1},\"1\":{\"704\":1}}],[\"ctm\",{\"1\":{\"578\":3,\"609\":2}}],[\"ctx=none\",{\"1\":{\"1735\":6}}],[\"ctx\",{\"1\":{\"130\":1,\"654\":2,\"710\":2,\"711\":2,\"756\":15,\"773\":15,\"866\":5,\"867\":5,\"1735\":8}}],[\"ctcprefixscoreth\",{\"0\":{\"1730\":1},\"1\":{\"1730\":1}}],[\"ctcprefixscore\",{\"0\":{\"1729\":1},\"1\":{\"1729\":2,\"1730\":1,\"1731\":1,\"1848\":1}}],[\"ctcprefixscorer\",{\"0\":{\"1731\":1},\"1\":{\"1719\":1,\"1721\":1,\"1725\":1,\"1731\":1,\"1805\":1,\"1848\":1,\"1862\":1}}],[\"ctc=none\",{\"1\":{\"1720\":1}}],[\"ctc=59\",{\"1\":{\"113\":1}}],[\"ctc=65\",{\"1\":{\"113\":1}}],[\"ctc\",{\"0\":{\"175\":2,\"299\":1,\"415\":1,\"419\":1,\"429\":1,\"695\":1,\"706\":2,\"733\":1,\"917\":1,\"1729\":1,\"1730\":1,\"1731\":1,\"1769\":1,\"1934\":1,\"1988\":1,\"1996\":1,\"2055\":1,\"2264\":1},\"1\":{\"29\":2,\"39\":2,\"44\":7,\"46\":1,\"138\":1,\"144\":6,\"175\":24,\"242\":10,\"243\":3,\"260\":1,\"261\":4,\"262\":13,\"267\":2,\"276\":2,\"286\":3,\"301\":2,\"315\":2,\"396\":2,\"415\":2,\"421\":2,\"429\":1,\"442\":2,\"463\":6,\"469\":2,\"505\":4,\"541\":1,\"625\":6,\"706\":7,\"709\":7,\"733\":6,\"734\":5,\"736\":4,\"737\":3,\"774\":6,\"776\":3,\"777\":5,\"780\":2,\"795\":4,\"1726\":7,\"1727\":7,\"1729\":5,\"1730\":10,\"1731\":6,\"1760\":2,\"1805\":1,\"1880\":1,\"1934\":3,\"1975\":4,\"1988\":2,\"1996\":11,\"1997\":11,\"2055\":4,\"2127\":4,\"2221\":6,\"2264\":1,\"2360\":3,\"2361\":2}}],[\"c\",{\"1\":{\"75\":2,\"162\":3,\"163\":4,\"164\":1,\"185\":2,\"243\":1,\"261\":2,\"276\":3,\"286\":1,\"527\":1,\"641\":1,\"768\":6,\"797\":2,\"817\":1,\"821\":5,\"823\":2,\"824\":8,\"831\":3,\"832\":1,\"847\":1,\"984\":2,\"992\":1,\"997\":1,\"999\":1,\"1008\":1,\"1016\":1,\"1018\":1,\"1029\":2,\"1054\":3,\"1061\":1,\"1062\":2,\"1064\":1,\"1066\":1,\"1070\":2,\"1071\":2,\"1073\":2,\"1080\":2,\"1125\":1,\"1126\":19,\"1127\":6,\"1147\":2,\"1170\":2,\"1171\":2,\"1173\":2,\"1175\":2,\"1176\":2,\"1180\":2,\"1181\":2,\"1182\":2,\"1183\":2,\"1184\":2,\"1199\":3,\"1202\":1,\"1210\":1,\"1235\":4,\"1250\":1,\"1251\":1,\"1259\":1,\"1261\":1,\"1264\":2,\"1273\":3,\"1275\":2,\"1277\":2,\"1280\":2,\"1283\":2,\"1290\":1,\"1293\":3,\"1296\":2,\"1298\":1,\"1301\":4,\"1306\":5,\"1308\":6,\"1309\":6,\"1310\":6,\"1311\":6,\"1314\":5,\"1315\":5,\"1317\":6,\"1318\":6,\"1319\":4,\"1321\":6,\"1322\":8,\"1323\":6,\"1325\":1,\"1326\":4,\"1327\":6,\"1328\":6,\"1330\":6,\"1332\":1,\"1334\":6,\"1337\":1,\"1338\":1,\"1339\":1,\"1351\":2,\"1352\":1,\"1354\":3,\"1361\":4,\"1363\":1,\"1364\":1,\"1365\":1,\"1371\":5,\"1372\":4,\"1374\":2,\"1375\":2,\"1376\":2,\"1377\":2,\"1513\":2,\"1515\":2,\"1516\":3,\"1526\":1,\"1548\":2,\"1551\":2,\"1553\":2,\"1556\":10,\"1582\":4,\"1592\":4,\"1596\":1,\"1600\":1,\"1605\":4,\"1610\":6,\"1615\":2,\"1617\":2,\"1619\":4,\"1620\":2,\"1621\":2,\"1624\":4,\"1628\":2,\"1632\":2,\"1633\":2,\"1668\":2,\"1750\":1,\"1815\":1,\"1849\":2,\"2202\":5,\"2213\":5,\"2214\":4,\"2224\":1,\"2435\":1,\"2476\":2,\"2479\":5,\"2486\":1}}],[\"c2\",{\"1\":{\"62\":1,\"994\":1}}],[\"ce\",{\"1\":{\"1782\":1}}],[\"cell\",{\"1\":{\"1029\":2,\"1202\":2,\"1259\":2,\"1261\":2,\"1279\":2,\"1281\":2,\"1855\":2,\"1856\":5}}],[\"cells\",{\"1\":{\"142\":1}}],[\"cepstral\",{\"1\":{\"267\":2,\"276\":2,\"285\":1,\"286\":1}}],[\"cer\",{\"1\":{\"139\":2,\"175\":1,\"223\":1,\"267\":2,\"276\":2,\"286\":3,\"541\":2,\"625\":3,\"627\":7,\"736\":1,\"740\":7,\"777\":1,\"1640\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2221\":1}}],[\"certify\",{\"0\":{\"873\":1},\"1\":{\"873\":1}}],[\"certificate\",{\"1\":{\"92\":1}}],[\"certain\",{\"1\":{\"26\":1,\"36\":1,\"248\":1,\"830\":1,\"2480\":1}}],[\"centroids\",{\"1\":{\"1400\":1}}],[\"centered\",{\"1\":{\"2379\":1}}],[\"centered=true\",{\"1\":{\"1211\":1}}],[\"centers\",{\"1\":{\"2176\":1}}],[\"center=true\",{\"1\":{\"1776\":1,\"1835\":1,\"1899\":1,\"1924\":1}}],[\"center\",{\"1\":{\"720\":1,\"976\":1,\"1250\":1,\"1251\":1,\"1419\":3,\"1607\":3,\"1655\":1,\"1660\":1,\"1669\":1,\"1672\":3,\"1673\":3,\"1687\":3,\"1978\":1,\"1980\":1,\"1982\":1,\"2176\":4,\"2232\":1,\"2238\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1}}],[\"centos\",{\"1\":{\"159\":2,\"161\":1}}],[\"centos7\",{\"1\":{\"67\":1,\"160\":1}}],[\"census\",{\"1\":{\"38\":1}}],[\"cmnd\",{\"1\":{\"2436\":1}}],[\"cmndfs\",{\"1\":{\"2435\":3}}],[\"cmndf\",{\"1\":{\"2435\":1}}],[\"cmvn\",{\"0\":{\"1728\":2,\"1850\":1},\"1\":{\"526\":3,\"527\":4,\"1728\":3,\"1846\":2,\"1850\":2}}],[\"cmvnark\",{\"1\":{\"519\":1}}],[\"cmake\",{\"1\":{\"159\":1,\"163\":1}}],[\"cmake3\",{\"1\":{\"159\":1}}],[\"cmu\",{\"0\":{\"190\":1,\"191\":1,\"192\":1},\"1\":{\"38\":1}}],[\"cmd=utils\",{\"1\":{\"69\":1}}],[\"cmd\",{\"1\":{\"24\":2,\"37\":1,\"69\":3,\"109\":1,\"110\":1,\"166\":2,\"167\":2,\"206\":2,\"212\":2,\"218\":2,\"236\":1,\"255\":2,\"267\":2,\"276\":2,\"286\":2,\"374\":2,\"516\":1,\"520\":1,\"521\":1,\"523\":1,\"524\":1,\"525\":1,\"535\":1,\"537\":2,\"2249\":1}}],[\"cumulativemeannormalizeddifferencefunctiontorch\",{\"0\":{\"2437\":1},\"1\":{\"2437\":1}}],[\"cumulativemeannormalizeddifferencefunction\",{\"0\":{\"2436\":1},\"1\":{\"2436\":1}}],[\"cumulative\",{\"1\":{\"2435\":2,\"2436\":2}}],[\"cumulate\",{\"1\":{\"1750\":3,\"1993\":3,\"2223\":3,\"2245\":3,\"2431\":3}}],[\"cutset\",{\"1\":{\"2159\":1}}],[\"cuts\",{\"1\":{\"2139\":1}}],[\"cut\",{\"0\":{\"2150\":1},\"1\":{\"1608\":1,\"1631\":1,\"1717\":1,\"2150\":5,\"2159\":1}}],[\"cutoff\",{\"1\":{\"481\":2,\"1526\":1,\"1600\":1,\"1608\":3,\"1631\":2,\"1655\":2,\"1689\":3,\"1690\":3}}],[\"cur\",{\"1\":{\"1735\":2}}],[\"curves\",{\"1\":{\"218\":1,\"267\":1,\"276\":1,\"286\":1}}],[\"curl\",{\"1\":{\"92\":2}}],[\"currently\",{\"1\":{\"26\":1,\"43\":1,\"49\":1,\"137\":1,\"138\":1,\"143\":1,\"173\":1,\"223\":1,\"254\":1,\"286\":1,\"717\":1,\"756\":1,\"773\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":3,\"1269\":2,\"1270\":2,\"1271\":2,\"1334\":2,\"1469\":1,\"1655\":1,\"1668\":1,\"1684\":1,\"2216\":1}}],[\"current\",{\"1\":{\"19\":1,\"110\":1,\"148\":1,\"219\":1,\"225\":1,\"614\":1,\"617\":2,\"618\":2,\"620\":2,\"624\":2,\"626\":2,\"634\":1,\"636\":2,\"639\":3,\"641\":1,\"643\":1,\"644\":4,\"645\":2,\"651\":1,\"716\":1,\"768\":1,\"1050\":2,\"1116\":2,\"1161\":2,\"1189\":2,\"1204\":1,\"1209\":1,\"1218\":2,\"1221\":2,\"1228\":1,\"1229\":2,\"1244\":2,\"1400\":1,\"1441\":1,\"1469\":1,\"1720\":1,\"1721\":1,\"1735\":2,\"1808\":1,\"2044\":1,\"2134\":1,\"2249\":1}}],[\"customdecoder\",{\"0\":{\"1749\":1},\"1\":{\"1749\":2}}],[\"customed\",{\"1\":{\"269\":2,\"278\":2}}],[\"customizing\",{\"1\":{\"2131\":1}}],[\"customized\",{\"1\":{\"1064\":1,\"1078\":1,\"1153\":1,\"1202\":1,\"1262\":1,\"1290\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"2232\":1,\"2238\":1}}],[\"customize\",{\"0\":{\"82\":1},\"1\":{\"82\":2,\"2249\":1}}],[\"customizable\",{\"1\":{\"43\":1,\"232\":1,\"258\":1}}],[\"custom\",{\"0\":{\"150\":1,\"163\":1,\"1749\":1,\"1872\":1},\"1\":{\"43\":11,\"52\":2,\"141\":1,\"184\":1,\"243\":1,\"756\":1,\"773\":1,\"821\":1,\"1736\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1749\":2,\"1847\":1,\"1851\":1,\"1863\":1,\"1872\":1,\"1965\":1,\"2148\":1,\"2312\":2,\"2354\":1,\"2355\":1,\"2365\":1,\"2480\":1}}],[\"cudnn7\",{\"1\":{\"29\":4}}],[\"cudnn\",{\"0\":{\"383\":1,\"455\":1,\"2317\":1},\"1\":{\"22\":1,\"104\":3,\"377\":6,\"449\":6,\"2317\":1}}],[\"cudatk\",{\"1\":{\"161\":1}}],[\"cuda=no\",{\"1\":{\"161\":2}}],[\"cuda=9\",{\"1\":{\"22\":1}}],[\"cuda10\",{\"1\":{\"29\":2}}],[\"cuda\",{\"0\":{\"704\":1,\"755\":1,\"764\":1,\"785\":1,\"802\":1,\"803\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"919\":1,\"931\":1,\"933\":1},\"1\":{\"22\":3,\"26\":3,\"41\":2,\"66\":2,\"87\":1,\"93\":2,\"104\":1,\"161\":3,\"162\":4,\"163\":3,\"195\":2,\"668\":1,\"704\":2,\"705\":2,\"716\":1,\"755\":3,\"764\":1,\"785\":3,\"802\":1,\"803\":2,\"804\":2,\"833\":1,\"852\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"919\":1,\"922\":1,\"931\":1,\"932\":2,\"933\":1,\"934\":2,\"937\":1,\"1926\":1,\"2043\":3,\"2045\":2,\"2049\":2,\"2054\":3,\"2055\":3,\"2056\":3,\"2066\":3,\"2130\":1,\"2131\":1,\"2133\":1,\"2162\":1,\"2249\":2,\"2309\":1}}],[\"cdim\",{\"1\":{\"1814\":3,\"1816\":3}}],[\"cdist\",{\"0\":{\"1479\":1},\"1\":{\"1479\":1}}],[\"cd\",{\"1\":{\"22\":1,\"23\":1,\"24\":3,\"25\":1,\"31\":1,\"33\":1,\"38\":1,\"69\":1,\"71\":1,\"107\":1,\"108\":4,\"153\":1,\"161\":5,\"162\":16,\"163\":4,\"164\":1,\"195\":1,\"196\":1,\"197\":1,\"206\":1,\"212\":2,\"218\":1,\"236\":1,\"243\":3,\"255\":1,\"267\":2,\"268\":1,\"276\":2,\"277\":1,\"286\":2}}],[\"cropped\",{\"1\":{\"1556\":3}}],[\"crop\",{\"1\":{\"1556\":2,\"2350\":2,\"2434\":1}}],[\"crossentropy\",{\"1\":{\"2448\":1}}],[\"cross\",{\"0\":{\"2073\":1},\"1\":{\"15\":1,\"44\":1,\"144\":1,\"733\":2,\"1171\":1,\"1326\":1,\"1794\":3,\"1839\":1,\"1991\":1,\"2167\":1,\"2176\":1,\"2207\":1,\"2219\":1}}],[\"crelu\",{\"1\":{\"1217\":1}}],[\"creating\",{\"0\":{\"224\":1,\"225\":1},\"1\":{\"195\":1,\"196\":1,\"197\":1,\"201\":1,\"213\":1,\"222\":2,\"268\":1,\"277\":1,\"521\":1,\"2134\":1}}],[\"creates\",{\"1\":{\"48\":2,\"96\":1,\"97\":1,\"98\":1,\"200\":1,\"205\":1,\"211\":1,\"217\":1,\"242\":1,\"266\":1,\"275\":1,\"285\":1,\"1643\":1,\"1645\":1,\"1646\":1,\"1650\":1,\"2130\":1,\"2136\":1,\"2143\":1,\"2148\":1}}],[\"created\",{\"1\":{\"24\":1,\"81\":1,\"223\":2,\"224\":1,\"228\":1,\"232\":1,\"243\":3,\"258\":1,\"263\":1,\"266\":1,\"275\":1,\"285\":1,\"824\":1,\"986\":1,\"1883\":1,\"2141\":1}}],[\"create\",{\"0\":{\"1558\":1,\"1675\":1,\"1681\":2,\"1682\":2,\"1683\":2,\"1684\":2,\"1685\":2,\"1688\":1,\"1695\":1,\"1871\":1,\"2150\":1},\"1\":{\"1\":1,\"3\":1,\"31\":1,\"73\":1,\"74\":1,\"75\":1,\"76\":1,\"106\":1,\"127\":1,\"162\":2,\"195\":1,\"196\":2,\"197\":3,\"201\":3,\"213\":1,\"222\":4,\"224\":3,\"225\":4,\"232\":1,\"243\":4,\"254\":1,\"259\":1,\"267\":3,\"268\":2,\"276\":3,\"277\":2,\"286\":3,\"377\":2,\"449\":2,\"614\":2,\"616\":1,\"620\":4,\"634\":2,\"641\":2,\"643\":2,\"651\":2,\"669\":1,\"670\":1,\"726\":1,\"828\":1,\"829\":1,\"830\":1,\"847\":2,\"859\":1,\"938\":1,\"974\":1,\"985\":1,\"1118\":2,\"1174\":1,\"1245\":2,\"1331\":1,\"1558\":3,\"1675\":1,\"1681\":3,\"1682\":2,\"1683\":3,\"1684\":3,\"1685\":3,\"1688\":1,\"1695\":1,\"1722\":1,\"1736\":4,\"1749\":2,\"1807\":1,\"1815\":2,\"1824\":2,\"1843\":2,\"1851\":2,\"1864\":1,\"1865\":1,\"1867\":1,\"1871\":2,\"1883\":1,\"1926\":1,\"2130\":2,\"2133\":2,\"2136\":2,\"2137\":1,\"2140\":1,\"2143\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2148\":1,\"2150\":2,\"2157\":1,\"2287\":2,\"2348\":1,\"2370\":2,\"2372\":1}}],[\"crpblock\",{\"0\":{\"1068\":1},\"1\":{\"1068\":1}}],[\"criteria\",{\"1\":{\"1066\":1}}],[\"criterion=kldivloss\",{\"1\":{\"1782\":1}}],[\"criterions\",{\"0\":{\"1036\":1,\"1066\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1210\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1275\":1,\"1276\":1,\"1277\":1},\"1\":{\"223\":1,\"225\":2,\"1036\":1,\"1066\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1210\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1334\":1,\"1949\":1}}],[\"criterion\",{\"1\":{\"42\":1,\"78\":1,\"175\":1,\"225\":1,\"243\":1,\"794\":1,\"1132\":1,\"1155\":1,\"1157\":1,\"1167\":1,\"1204\":2,\"1209\":2,\"1228\":2,\"1638\":1,\"1782\":1,\"1949\":2,\"1987\":1,\"2339\":2,\"2348\":3,\"2370\":6,\"2372\":3}}],[\"critical\",{\"1\":{\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"377\":1,\"385\":1,\"389\":1,\"396\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":1,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"498\":1,\"505\":1,\"511\":1,\"1654\":1}}],[\"crnseparator\",{\"0\":{\"1125\":1},\"1\":{\"1125\":1}}],[\"crn\",{\"0\":{\"1124\":2,\"1125\":1,\"1147\":1,\"1176\":1,\"1180\":1,\"1181\":1},\"1\":{\"223\":2,\"1118\":1,\"1124\":4,\"1125\":3,\"1147\":1,\"1176\":1,\"1180\":1,\"1181\":1}}],[\"chnattnstatpooling\",{\"0\":{\"2183\":1},\"1\":{\"2183\":1}}],[\"chnage\",{\"1\":{\"120\":2}}],[\"chn\",{\"0\":{\"2183\":1},\"1\":{\"2183\":1}}],[\"chnn\",{\"1\":{\"1609\":1}}],[\"ch=none\",{\"1\":{\"1238\":1,\"1240\":1,\"1242\":1}}],[\"ch\",{\"1\":{\"271\":1,\"280\":1,\"287\":4,\"1009\":1,\"1029\":4,\"1110\":2,\"1155\":2,\"1157\":3,\"1163\":1,\"1164\":2,\"1211\":1,\"1235\":4,\"1238\":1,\"1240\":1,\"1242\":1,\"1279\":4,\"1280\":4,\"1281\":4,\"1282\":4,\"1283\":4,\"1333\":1,\"1716\":1,\"2299\":1}}],[\"child\",{\"1\":{\"1695\":1}}],[\"chinese\",{\"1\":{\"267\":1}}],[\"chime\",{\"1\":{\"24\":1,\"38\":1,\"1717\":3}}],[\"chime5\",{\"1\":{\"24\":2}}],[\"chime3\",{\"1\":{\"22\":1,\"24\":2}}],[\"chime4\",{\"1\":{\"22\":2,\"24\":8,\"224\":1,\"227\":1}}],[\"chrome\",{\"1\":{\"248\":1}}],[\"christine\",{\"1\":{\"202\":1}}],[\"christoph\",{\"1\":{\"11\":1}}],[\"chung\",{\"1\":{\"256\":1}}],[\"chunkiterfacotry\",{\"1\":{\"1644\":1}}],[\"chunkiterfactory\",{\"0\":{\"1646\":1},\"1\":{\"1644\":1,\"1646\":1,\"1647\":1}}],[\"chunked\",{\"0\":{\"2073\":1},\"1\":{\"1035\":1,\"1113\":1,\"1251\":1}}],[\"chunks=3\",{\"1\":{\"1334\":1}}],[\"chunks\",{\"1\":{\"147\":4,\"626\":1,\"661\":3,\"669\":3,\"774\":3,\"1031\":3,\"1035\":1,\"1112\":3,\"1113\":1,\"1164\":2,\"1250\":3,\"1251\":1,\"1280\":1,\"1283\":1,\"1334\":4,\"1643\":2,\"1644\":1,\"1646\":2,\"1720\":1,\"1721\":1}}],[\"chunk\",{\"0\":{\"669\":1,\"1643\":1,\"1646\":1},\"1\":{\"142\":3,\"146\":3,\"147\":12,\"148\":5,\"150\":1,\"315\":2,\"442\":2,\"469\":2,\"616\":2,\"617\":7,\"618\":7,\"620\":7,\"624\":7,\"626\":3,\"633\":3,\"634\":3,\"636\":6,\"639\":5,\"644\":10,\"645\":2,\"661\":9,\"669\":6,\"703\":1,\"755\":1,\"785\":2,\"822\":1,\"974\":3,\"1117\":1,\"1130\":1,\"1131\":1,\"1136\":1,\"1139\":1,\"1140\":2,\"1141\":2,\"1155\":3,\"1156\":3,\"1157\":3,\"1158\":3,\"1185\":2,\"1232\":1,\"1280\":2,\"1283\":2,\"1334\":1,\"1643\":4,\"1644\":3,\"1646\":4,\"1647\":2,\"2249\":2}}],[\"chuang\",{\"1\":{\"14\":1}}],[\"ch1\",{\"1\":{\"76\":1,\"1010\":1}}],[\"ch0\",{\"1\":{\"76\":1,\"1010\":1}}],[\"cholesky\",{\"1\":{\"1308\":2}}],[\"chose\",{\"0\":{\"984\":1},\"1\":{\"984\":1}}],[\"chosen\",{\"1\":{\"276\":1,\"846\":1,\"1907\":1}}],[\"chomp\",{\"1\":{\"972\":1,\"1075\":1}}],[\"chomp1d\",{\"0\":{\"972\":1,\"1075\":1},\"1\":{\"972\":1,\"1075\":1}}],[\"chong\",{\"1\":{\"269\":1,\"278\":1}}],[\"cho\",{\"1\":{\"202\":1}}],[\"choose\",{\"0\":{\"1296\":1,\"2072\":1},\"1\":{\"71\":1,\"211\":1,\"247\":5,\"262\":4,\"267\":1,\"774\":1,\"819\":1,\"846\":2,\"980\":1,\"1267\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1296\":1,\"1334\":1,\"1559\":1,\"2220\":2,\"2355\":1}}],[\"choice\",{\"1\":{\"262\":1,\"290\":2,\"768\":4,\"1397\":2,\"2133\":2,\"2136\":4}}],[\"choices=none\",{\"1\":{\"2478\":1}}],[\"choices\",{\"0\":{\"2334\":1},\"1\":{\"145\":1,\"225\":4,\"290\":1,\"2246\":11,\"2247\":6,\"2248\":8,\"2249\":2,\"2250\":9,\"2251\":9,\"2252\":26,\"2253\":8,\"2254\":3,\"2255\":12,\"2256\":9,\"2257\":8,\"2259\":14,\"2260\":4,\"2261\":9,\"2262\":3,\"2263\":16,\"2264\":9,\"2265\":12,\"2266\":8,\"2267\":15,\"2268\":12,\"2269\":10,\"2270\":8,\"2271\":9,\"2272\":6,\"2273\":7,\"2334\":5}}],[\"choi\",{\"1\":{\"5\":1,\"6\":2,\"1269\":2,\"1270\":2,\"1271\":2}}],[\"checks\",{\"1\":{\"269\":1,\"278\":1,\"918\":1}}],[\"checkout\",{\"1\":{\"260\":1}}],[\"checkpointing\",{\"1\":{\"1029\":1,\"1064\":1,\"1235\":1,\"1280\":1,\"1281\":1,\"1282\":1}}],[\"checkpoint=false\",{\"1\":{\"1029\":1,\"1064\":1,\"1235\":1,\"1281\":1,\"1282\":1}}],[\"checkpoints\",{\"0\":{\"541\":1},\"1\":{\"541\":1}}],[\"checkpoint\",{\"0\":{\"2071\":1,\"2077\":1,\"2104\":1},\"1\":{\"87\":2,\"91\":1,\"102\":1,\"218\":1,\"243\":2,\"267\":3,\"276\":3,\"286\":5,\"290\":2,\"377\":2,\"475\":2,\"523\":1,\"692\":1,\"699\":6,\"734\":1,\"748\":1,\"848\":1,\"1029\":1,\"1064\":1,\"1235\":1,\"1280\":2,\"1281\":1,\"1282\":1,\"2354\":3,\"2369\":1}}],[\"checking\",{\"1\":{\"36\":1,\"81\":2,\"196\":1,\"268\":1,\"277\":1,\"2474\":1}}],[\"check\",{\"0\":{\"32\":1,\"164\":1,\"663\":1,\"874\":1,\"875\":1,\"876\":1,\"983\":1,\"1295\":1,\"1675\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"2071\":1},\"1\":{\"31\":1,\"32\":2,\"67\":1,\"71\":1,\"81\":1,\"132\":1,\"154\":2,\"160\":1,\"162\":2,\"164\":3,\"171\":1,\"172\":1,\"173\":1,\"196\":1,\"197\":1,\"201\":2,\"213\":1,\"223\":2,\"224\":3,\"225\":2,\"243\":1,\"267\":7,\"268\":1,\"269\":1,\"276\":4,\"277\":1,\"278\":1,\"286\":2,\"289\":1,\"290\":3,\"663\":2,\"847\":1,\"874\":1,\"875\":1,\"876\":1,\"983\":1,\"1295\":1,\"1396\":1,\"1675\":2,\"1749\":2,\"1815\":1,\"1868\":2,\"1869\":2,\"1870\":2,\"1897\":1,\"1898\":1,\"1936\":1,\"2246\":2,\"2248\":2,\"2249\":5,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2334\":1,\"2359\":1}}],[\"chennels\",{\"1\":{\"1615\":1}}],[\"chenda\",{\"1\":{\"11\":2}}],[\"chen2023reducing\",{\"1\":{\"7\":1}}],[\"chen\",{\"1\":{\"5\":1,\"6\":3,\"7\":1,\"11\":1,\"15\":1,\"156\":1,\"244\":1,\"1061\":1,\"1062\":1,\"1117\":1,\"1185\":1}}],[\"chattts\",{\"1\":{\"2040\":2}}],[\"chatttsmodel\",{\"0\":{\"2040\":1},\"1\":{\"2040\":2}}],[\"chatts\",{\"1\":{\"2040\":1}}],[\"chat\",{\"0\":{\"2039\":2,\"2040\":1},\"1\":{\"2039\":19,\"2040\":1,\"2049\":4,\"2143\":1}}],[\"chatbot\",{\"1\":{\"247\":1}}],[\"chainable\",{\"1\":{\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2021\":1}}],[\"chainer|pytorch\",{\"1\":{\"527\":1}}],[\"chainer\",{\"0\":{\"53\":1,\"1761\":1,\"1763\":1,\"1778\":1,\"1797\":1,\"1819\":1,\"1823\":1,\"1834\":1},\"1\":{\"1\":1,\"32\":1,\"38\":3,\"53\":1,\"106\":4,\"527\":1,\"1761\":1,\"1763\":1,\"1778\":1,\"1797\":1,\"1819\":2,\"1823\":1,\"1834\":1}}],[\"chapter\",{\"1\":{\"1319\":1}}],[\"challenge\",{\"1\":{\"285\":1,\"1210\":1,\"1264\":1,\"1334\":1}}],[\"chars\",{\"1\":{\"2281\":1}}],[\"chartokenizer\",{\"0\":{\"2275\":1},\"1\":{\"2275\":1}}],[\"character\",{\"1\":{\"267\":1,\"276\":1,\"286\":1,\"290\":1,\"515\":1,\"625\":1,\"627\":7,\"706\":4,\"740\":7,\"1526\":1,\"1552\":2,\"1553\":1,\"1758\":2,\"1992\":1,\"1993\":1,\"1995\":1,\"2231\":2,\"2235\":2,\"2236\":2,\"2239\":1,\"2240\":1,\"2245\":1,\"2411\":1,\"2431\":1,\"2432\":2}}],[\"characters\",{\"1\":{\"106\":1,\"205\":1,\"290\":2,\"1750\":1,\"1758\":1,\"1993\":2,\"2224\":1,\"2231\":1,\"2239\":2,\"2240\":2,\"2245\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":3,\"2432\":2,\"2447\":2}}],[\"char\",{\"0\":{\"2275\":1},\"1\":{\"200\":2,\"205\":2,\"242\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"389\":1,\"396\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"442\":1,\"463\":2,\"469\":1,\"481\":1,\"498\":1,\"526\":1,\"536\":10,\"603\":1,\"627\":11,\"740\":11,\"1760\":2,\"1788\":2,\"1798\":1,\"1799\":1,\"1800\":1,\"2275\":1}}],[\"chan\",{\"1\":{\"1120\":2,\"1122\":2,\"1265\":4,\"1368\":4,\"1385\":2,\"1401\":1,\"1402\":1,\"1466\":1,\"1467\":1}}],[\"chans=384\",{\"1\":{\"1752\":1}}],[\"chans=32\",{\"1\":{\"1145\":1,\"1264\":1,\"1334\":1}}],[\"chans=512\",{\"1\":{\"1750\":1,\"1758\":1,\"1810\":1,\"2223\":1,\"2231\":1}}],[\"chans=100\",{\"1\":{\"1716\":1}}],[\"chans\",{\"1\":{\"796\":2,\"869\":2,\"1145\":1,\"1264\":3,\"1334\":3,\"1526\":2,\"1598\":4,\"1599\":8,\"1600\":5,\"1706\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1715\":2,\"1716\":3,\"1737\":6,\"1750\":1,\"1753\":1,\"1758\":1,\"1768\":2,\"1795\":6,\"1895\":2,\"1993\":4,\"1994\":1,\"2223\":1,\"2231\":1,\"2235\":4,\"2236\":6,\"2239\":4,\"2240\":4,\"2245\":8,\"2411\":6,\"2412\":10,\"2423\":10,\"2425\":2,\"2429\":2,\"2431\":8,\"2432\":6,\"2433\":2,\"2447\":8}}],[\"channles\",{\"1\":{\"1620\":2,\"1621\":2}}],[\"channnels\",{\"1\":{\"71\":1}}],[\"channeltac\",{\"0\":{\"1073\":1},\"1\":{\"1073\":2}}],[\"channelfreqwiselayernorm\",{\"0\":{\"1072\":1},\"1\":{\"1072\":1}}],[\"channelattentiontac\",{\"0\":{\"1071\":1},\"1\":{\"1071\":1}}],[\"channelattention\",{\"0\":{\"1070\":1},\"1\":{\"1070\":2}}],[\"channel=128\",{\"1\":{\"1893\":1,\"2202\":1,\"2213\":1,\"2214\":1}}],[\"channel=16\",{\"1\":{\"1061\":1}}],[\"channel=3\",{\"1\":{\"1893\":1}}],[\"channel=0\",{\"1\":{\"1734\":1}}],[\"channel=\",{\"1\":{\"1269\":1,\"1734\":1}}],[\"channel=4\",{\"1\":{\"1184\":1,\"1271\":1}}],[\"channel=none\",{\"1\":{\"1029\":1,\"1070\":1,\"1071\":1,\"1073\":1,\"1235\":1,\"1273\":1,\"1274\":1,\"1279\":1,\"1281\":1,\"1282\":1}}],[\"channelwiselayernorm\",{\"0\":{\"971\":1,\"1074\":1},\"1\":{\"971\":1,\"1074\":1}}],[\"channel>th\",{\"1\":{\"74\":2}}],[\"channel>\",{\"1\":{\"74\":1}}],[\"channel>=c<in\",{\"1\":{\"74\":1}}],[\"channel\",{\"0\":{\"1734\":1},\"1\":{\"69\":2,\"74\":3,\"223\":1,\"342\":2,\"349\":2,\"356\":2,\"361\":2,\"592\":1,\"630\":1,\"649\":2,\"654\":4,\"674\":12,\"720\":1,\"738\":1,\"747\":4,\"759\":1,\"768\":1,\"787\":1,\"815\":1,\"846\":18,\"862\":1,\"971\":3,\"975\":2,\"980\":1,\"984\":2,\"1008\":1,\"1009\":2,\"1029\":7,\"1031\":1,\"1055\":1,\"1057\":1,\"1061\":1,\"1062\":3,\"1066\":1,\"1070\":3,\"1071\":3,\"1072\":3,\"1073\":3,\"1074\":3,\"1112\":2,\"1113\":2,\"1125\":2,\"1126\":2,\"1127\":1,\"1130\":1,\"1155\":1,\"1157\":1,\"1179\":2,\"1210\":1,\"1217\":9,\"1235\":7,\"1250\":1,\"1264\":2,\"1267\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":2,\"1273\":1,\"1274\":1,\"1279\":6,\"1280\":9,\"1281\":5,\"1282\":5,\"1283\":9,\"1296\":2,\"1323\":1,\"1326\":1,\"1327\":1,\"1328\":2,\"1329\":1,\"1330\":2,\"1334\":7,\"1385\":3,\"1386\":6,\"1389\":1,\"1390\":1,\"1404\":1,\"1408\":1,\"1410\":1,\"1413\":1,\"1420\":1,\"1539\":1,\"1655\":1,\"1678\":1,\"1734\":2,\"1766\":1,\"1893\":4,\"2136\":1,\"2139\":4,\"2150\":2,\"2159\":1,\"2183\":2,\"2187\":1,\"2196\":1,\"2220\":17,\"2336\":1,\"2346\":2,\"2368\":2,\"2378\":1}}],[\"channelselector\",{\"0\":{\"1734\":1},\"1\":{\"1734\":1}}],[\"channels=\",{\"1\":{\"1534\":1}}],[\"channels=512\",{\"1\":{\"1514\":1,\"1548\":1}}],[\"channels=none\",{\"1\":{\"1483\":1}}],[\"channels=384\",{\"1\":{\"1264\":1,\"1334\":1}}],[\"channels=2\",{\"1\":{\"1124\":1}}],[\"channels=80\",{\"1\":{\"1548\":1}}],[\"channels=8\",{\"1\":{\"1124\":1,\"1147\":1}}],[\"channels=128\",{\"1\":{\"1063\":1,\"1198\":1}}],[\"channels=1\",{\"1\":{\"817\":1,\"821\":2,\"1264\":1,\"1334\":1,\"1548\":1}}],[\"channels=0\",{\"1\":{\"69\":1,\"1520\":1,\"1554\":1,\"1556\":1}}],[\"channels\",{\"0\":{\"76\":1},\"1\":{\"43\":4,\"69\":3,\"70\":1,\"74\":1,\"76\":2,\"141\":2,\"619\":3,\"620\":2,\"622\":1,\"623\":1,\"712\":4,\"768\":14,\"817\":1,\"821\":2,\"824\":1,\"831\":3,\"832\":1,\"973\":2,\"978\":1,\"981\":2,\"982\":2,\"1055\":1,\"1057\":1,\"1062\":2,\"1064\":1,\"1080\":4,\"1082\":4,\"1108\":2,\"1124\":10,\"1125\":8,\"1145\":8,\"1147\":8,\"1148\":3,\"1151\":1,\"1155\":3,\"1157\":2,\"1158\":3,\"1162\":1,\"1163\":1,\"1164\":1,\"1168\":3,\"1180\":6,\"1181\":6,\"1182\":1,\"1183\":1,\"1184\":1,\"1262\":1,\"1264\":6,\"1265\":2,\"1268\":1,\"1269\":2,\"1270\":2,\"1271\":3,\"1272\":3,\"1273\":4,\"1274\":4,\"1279\":1,\"1280\":3,\"1283\":3,\"1284\":1,\"1290\":1,\"1333\":1,\"1334\":7,\"1368\":1,\"1385\":4,\"1387\":1,\"1389\":5,\"1390\":4,\"1391\":1,\"1392\":6,\"1396\":4,\"1397\":6,\"1401\":11,\"1402\":10,\"1403\":1,\"1408\":10,\"1409\":10,\"1420\":4,\"1422\":9,\"1433\":1,\"1435\":1,\"1442\":2,\"1444\":2,\"1446\":2,\"1448\":2,\"1450\":3,\"1452\":3,\"1454\":3,\"1456\":2,\"1466\":7,\"1467\":6,\"1468\":1,\"1511\":1,\"1513\":15,\"1514\":7,\"1517\":3,\"1519\":6,\"1520\":10,\"1524\":5,\"1525\":4,\"1526\":13,\"1527\":1,\"1530\":2,\"1534\":2,\"1535\":3,\"1536\":8,\"1537\":2,\"1548\":13,\"1549\":9,\"1551\":15,\"1552\":15,\"1553\":16,\"1554\":1,\"1555\":1,\"1556\":6,\"1579\":2,\"1581\":9,\"1582\":3,\"1583\":6,\"1586\":5,\"1588\":2,\"1592\":18,\"1593\":4,\"1594\":4,\"1595\":8,\"1596\":13,\"1597\":12,\"1598\":14,\"1599\":15,\"1600\":13,\"1603\":2,\"1604\":12,\"1605\":12,\"1606\":12,\"1609\":9,\"1610\":20,\"1611\":17,\"1612\":12,\"1613\":12,\"1614\":5,\"1615\":4,\"1616\":8,\"1618\":3,\"1619\":17,\"1620\":8,\"1621\":8,\"1625\":14,\"1626\":14,\"1628\":26,\"1654\":3,\"1666\":3,\"1668\":7,\"1669\":2,\"1706\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1715\":1,\"1716\":1,\"1733\":4,\"1736\":2,\"1737\":2,\"1747\":5,\"1750\":1,\"1753\":1,\"1768\":1,\"1795\":2,\"1810\":1,\"1854\":2,\"1895\":1,\"1993\":2,\"2133\":3,\"2136\":3,\"2139\":2,\"2198\":3,\"2200\":1,\"2223\":1,\"2235\":2,\"2236\":3,\"2239\":3,\"2240\":2,\"2245\":4,\"2411\":3,\"2412\":5,\"2423\":6,\"2425\":1,\"2426\":3,\"2427\":3,\"2428\":3,\"2429\":1,\"2431\":4,\"2432\":3,\"2433\":1,\"2447\":4,\"2458\":1}}],[\"chang2023exploration\",{\"1\":{\"207\":1}}],[\"changing\",{\"0\":{\"47\":1},\"1\":{\"219\":1,\"223\":1,\"272\":1,\"282\":1,\"289\":1,\"821\":1,\"2309\":1}}],[\"changes\",{\"0\":{\"106\":1},\"1\":{\"269\":1,\"278\":1,\"286\":1,\"290\":1,\"1697\":2,\"1698\":1}}],[\"changeable\",{\"1\":{\"84\":1}}],[\"changed\",{\"1\":{\"55\":1,\"82\":1,\"94\":2,\"123\":1,\"128\":3,\"167\":1,\"173\":1,\"175\":1,\"242\":1,\"1833\":1,\"2016\":1}}],[\"change\",{\"0\":{\"40\":1,\"83\":1,\"86\":1,\"90\":1,\"91\":1,\"95\":1,\"108\":1,\"110\":1,\"119\":1,\"120\":1,\"545\":1,\"1952\":1},\"1\":{\"40\":1,\"47\":2,\"69\":1,\"71\":1,\"86\":1,\"94\":2,\"95\":1,\"110\":3,\"117\":1,\"118\":1,\"119\":3,\"128\":2,\"168\":2,\"195\":1,\"200\":5,\"205\":5,\"206\":1,\"211\":3,\"212\":1,\"217\":7,\"218\":1,\"242\":5,\"254\":2,\"255\":1,\"266\":6,\"267\":1,\"270\":1,\"271\":1,\"275\":6,\"276\":1,\"279\":1,\"280\":1,\"284\":1,\"285\":6,\"286\":1,\"287\":1,\"288\":1,\"290\":3,\"511\":2,\"545\":2,\"696\":1,\"699\":1,\"777\":1,\"817\":1,\"1156\":1,\"1678\":2,\"1932\":1,\"1940\":1,\"1942\":1,\"1952\":1,\"2016\":1,\"2044\":1,\"2164\":1,\"2283\":1,\"2284\":1,\"2307\":1,\"2309\":1,\"2323\":1,\"2337\":1}}],[\"chang\",{\"1\":{\"6\":2,\"7\":1,\"11\":2,\"12\":1,\"13\":1,\"207\":1,\"244\":1,\"1210\":1,\"1264\":1,\"1334\":1}}],[\"capture\",{\"0\":{\"2070\":1}}],[\"capable\",{\"1\":{\"1155\":1,\"1157\":1}}],[\"capabilities\",{\"1\":{\"146\":1,\"2140\":1}}],[\"capability\",{\"1\":{\"68\":1}}],[\"capacity\",{\"1\":{\"102\":1,\"262\":1,\"2147\":1}}],[\"caching\",{\"1\":{\"1732\":1}}],[\"cacheitem\",{\"0\":{\"1732\":1},\"1\":{\"1732\":1}}],[\"cache=none\",{\"1\":{\"701\":1,\"735\":1,\"1735\":3,\"1751\":2,\"1759\":1}}],[\"cached\",{\"1\":{\"692\":2,\"790\":1,\"850\":1,\"1726\":2,\"1727\":2,\"1751\":1,\"1847\":1,\"1992\":1}}],[\"cache\",{\"0\":{\"1953\":1,\"2068\":1,\"2069\":1},\"1\":{\"150\":2,\"243\":1,\"449\":6,\"616\":2,\"617\":3,\"618\":3,\"619\":5,\"620\":3,\"622\":3,\"623\":3,\"624\":3,\"626\":2,\"636\":3,\"639\":2,\"692\":8,\"701\":2,\"735\":2,\"790\":5,\"847\":4,\"850\":4,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1526\":3,\"1553\":3,\"1577\":3,\"1598\":3,\"1600\":3,\"1625\":3,\"1643\":1,\"1646\":1,\"1726\":1,\"1727\":4,\"1735\":4,\"1749\":4,\"1751\":1,\"1759\":2,\"1815\":4,\"1843\":4,\"1847\":2,\"1953\":1,\"1955\":1,\"1992\":4,\"2258\":4,\"2342\":2,\"2390\":1}}],[\"caclualte\",{\"1\":{\"1521\":1,\"2228\":3,\"2229\":3,\"2408\":3,\"2446\":3}}],[\"cast\",{\"1\":{\"748\":1}}],[\"cascade\",{\"1\":{\"262\":1}}],[\"cascaded\",{\"1\":{\"245\":1,\"246\":1,\"247\":3,\"262\":1,\"2044\":8}}],[\"case4\",{\"0\":{\"76\":1}}],[\"case3\",{\"0\":{\"75\":1}}],[\"case2\",{\"0\":{\"74\":1}}],[\"case1\",{\"0\":{\"73\":1}}],[\"case\",{\"0\":{\"66\":1,\"72\":1,\"532\":1},\"1\":{\"26\":1,\"62\":1,\"68\":1,\"70\":1,\"91\":1,\"101\":1,\"102\":1,\"119\":1,\"168\":1,\"195\":1,\"197\":1,\"200\":1,\"211\":1,\"213\":1,\"223\":1,\"265\":2,\"267\":1,\"269\":2,\"274\":2,\"278\":2,\"285\":1,\"286\":8,\"535\":1,\"536\":1,\"768\":1,\"943\":1,\"1008\":1,\"1228\":1,\"1246\":2,\"1493\":1,\"1494\":1,\"1860\":1,\"1883\":1,\"2130\":1,\"2311\":1,\"2355\":6,\"2384\":1,\"2385\":1}}],[\"cases\",{\"1\":{\"26\":1,\"78\":1,\"82\":1,\"128\":1,\"242\":1,\"269\":1,\"278\":1,\"535\":1,\"692\":1,\"775\":1,\"790\":1,\"792\":1,\"820\":1,\"821\":1,\"850\":1,\"1246\":1,\"2130\":1,\"2188\":1,\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1}}],[\"care\",{\"1\":{\"232\":1,\"258\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"carefully\",{\"1\":{\"197\":1,\"290\":1}}],[\"careful\",{\"1\":{\"173\":1,\"266\":1,\"267\":1,\"275\":1,\"285\":2,\"286\":1,\"1209\":1,\"1228\":1}}],[\"calcualte\",{\"1\":{\"1768\":1}}],[\"calcualate\",{\"1\":{\"1584\":1,\"1591\":1}}],[\"calcuated\",{\"1\":{\"254\":1,\"1587\":2}}],[\"calculating\",{\"1\":{\"1210\":1,\"2044\":1,\"2354\":1}}],[\"calculation\",{\"1\":{\"136\":1,\"137\":1,\"200\":2,\"205\":2,\"211\":1,\"217\":1,\"234\":1,\"235\":1,\"242\":2,\"253\":2,\"254\":3,\"266\":1,\"267\":1,\"275\":1,\"276\":1,\"285\":1,\"286\":1,\"717\":1,\"1155\":2,\"1157\":2,\"1301\":1,\"1368\":1,\"1372\":1,\"1587\":1,\"1599\":2,\"1627\":2,\"1753\":1,\"1754\":1,\"1764\":2,\"1839\":2,\"1938\":1,\"2215\":1,\"2226\":2,\"2235\":2,\"2236\":2,\"2237\":2,\"2239\":2,\"2240\":2,\"2241\":2,\"2245\":2,\"2327\":2,\"2365\":1,\"2411\":2,\"2412\":2,\"2413\":2,\"2423\":2,\"2424\":2,\"2427\":1,\"2431\":2,\"2432\":3,\"2447\":2,\"2448\":2}}],[\"calculator\",{\"0\":{\"627\":1,\"740\":1,\"2407\":1},\"1\":{\"627\":1,\"740\":1,\"2407\":2,\"2435\":1}}],[\"calculated\",{\"1\":{\"831\":1,\"1155\":1,\"1157\":1,\"1311\":1,\"1319\":1,\"1322\":1,\"1533\":1,\"1584\":2,\"1753\":2,\"1754\":1,\"2435\":3}}],[\"calculates\",{\"1\":{\"133\":1,\"200\":1,\"205\":1,\"211\":1,\"217\":1,\"242\":1,\"254\":2,\"266\":1,\"275\":1,\"285\":1,\"1314\":1,\"1315\":1,\"1325\":1,\"1770\":1,\"2435\":1}}],[\"calculate\",{\"0\":{\"543\":1,\"1950\":2},\"1\":{\"133\":2,\"134\":2,\"136\":2,\"212\":1,\"223\":1,\"242\":1,\"267\":1,\"276\":1,\"286\":3,\"290\":1,\"543\":2,\"627\":5,\"700\":1,\"706\":1,\"709\":1,\"733\":1,\"734\":1,\"740\":5,\"774\":1,\"780\":1,\"958\":1,\"1262\":1,\"1317\":1,\"1328\":1,\"1329\":1,\"1385\":1,\"1390\":1,\"1395\":1,\"1397\":1,\"1402\":1,\"1409\":1,\"1419\":1,\"1467\":1,\"1513\":1,\"1521\":1,\"1546\":1,\"1548\":1,\"1551\":1,\"1552\":1,\"1577\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1585\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1599\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1606\":1,\"1607\":1,\"1609\":1,\"1610\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1622\":1,\"1624\":1,\"1626\":1,\"1627\":1,\"1628\":1,\"1705\":1,\"1706\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1726\":1,\"1727\":1,\"1733\":1,\"1735\":1,\"1737\":1,\"1750\":3,\"1753\":1,\"1754\":1,\"1758\":1,\"1760\":5,\"1764\":1,\"1766\":1,\"1770\":1,\"1771\":1,\"1788\":1,\"1795\":1,\"1803\":1,\"1810\":1,\"1812\":1,\"1816\":1,\"1839\":1,\"1849\":1,\"1854\":1,\"1856\":1,\"1928\":1,\"1943\":1,\"1950\":2,\"1971\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"1997\":1,\"2130\":1,\"2133\":1,\"2136\":1,\"2187\":1,\"2191\":1,\"2222\":1,\"2223\":1,\"2226\":2,\"2227\":1,\"2231\":1,\"2235\":2,\"2236\":2,\"2237\":1,\"2239\":1,\"2240\":2,\"2241\":1,\"2245\":1,\"2355\":4,\"2403\":1,\"2411\":1,\"2412\":1,\"2413\":1,\"2420\":1,\"2423\":1,\"2424\":1,\"2425\":1,\"2426\":1,\"2427\":2,\"2428\":4,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":2,\"2433\":1,\"2445\":1,\"2447\":1,\"2448\":1}}],[\"calc\",{\"0\":{\"385\":1},\"1\":{\"228\":1,\"385\":2,\"736\":1,\"737\":1,\"777\":1,\"954\":1,\"974\":2,\"1155\":1,\"1156\":2,\"1157\":1,\"1158\":1,\"1640\":1,\"1641\":1,\"1959\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2221\":1,\"2462\":1}}],[\"callback\",{\"1\":{\"960\":2,\"2333\":1,\"2355\":1}}],[\"callbacks\",{\"0\":{\"960\":1,\"2333\":1},\"1\":{\"960\":1,\"2333\":2}}],[\"callable>\",{\"1\":{\"2258\":2}}],[\"callable\",{\"1\":{\"794\":1,\"929\":2,\"1051\":1,\"1648\":1,\"1917\":1,\"2131\":1,\"2134\":1,\"2142\":2,\"2246\":3,\"2247\":4,\"2248\":3,\"2249\":3,\"2250\":3,\"2251\":3,\"2252\":3,\"2253\":3,\"2254\":3,\"2255\":3,\"2256\":3,\"2257\":3,\"2258\":2,\"2259\":3,\"2260\":3,\"2261\":3,\"2262\":2,\"2263\":3,\"2264\":3,\"2265\":1,\"2266\":3,\"2267\":3,\"2268\":3,\"2269\":3,\"2270\":3,\"2271\":3,\"2272\":3,\"2273\":3,\"2342\":1,\"2351\":1,\"2366\":1}}],[\"calls\",{\"1\":{\"200\":1,\"205\":1,\"211\":1,\"217\":1,\"242\":1,\"254\":1,\"266\":1,\"275\":1,\"285\":1,\"804\":2,\"932\":2,\"934\":2,\"960\":1,\"2131\":1,\"2355\":2}}],[\"calling\",{\"1\":{\"150\":1,\"824\":1,\"2276\":1,\"2277\":1}}],[\"called\",{\"1\":{\"138\":1,\"144\":1,\"164\":1,\"200\":1,\"223\":2,\"242\":1,\"259\":3,\"699\":1,\"704\":1,\"777\":1,\"821\":2,\"824\":1,\"929\":1,\"960\":1,\"961\":5,\"1051\":1,\"1156\":1,\"1941\":1,\"1943\":1,\"2130\":2,\"2333\":1,\"2355\":1}}],[\"call\",{\"1\":{\"66\":1,\"111\":1,\"136\":1,\"676\":1,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"689\":1,\"690\":1,\"693\":1,\"694\":1,\"704\":1,\"713\":1,\"714\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"738\":1,\"739\":1,\"741\":1,\"742\":1,\"752\":1,\"753\":1,\"757\":1,\"758\":1,\"777\":1,\"778\":1,\"779\":1,\"781\":1,\"782\":1,\"788\":1,\"789\":1,\"791\":1,\"792\":1,\"796\":1,\"797\":1,\"798\":1,\"799\":1,\"805\":1,\"806\":1,\"807\":1,\"808\":1,\"809\":1,\"810\":1,\"811\":1,\"812\":1,\"813\":1,\"814\":1,\"815\":1,\"816\":1,\"821\":1,\"822\":1,\"825\":1,\"826\":1,\"833\":1,\"834\":1,\"835\":1,\"836\":1,\"837\":1,\"838\":1,\"839\":1,\"840\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"852\":1,\"853\":1,\"854\":1,\"855\":1,\"856\":1,\"857\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":1,\"931\":1,\"933\":1,\"950\":1,\"951\":1,\"952\":1,\"953\":1,\"956\":1,\"957\":1,\"963\":1,\"964\":1,\"965\":1,\"966\":1,\"967\":1,\"968\":1,\"969\":1,\"970\":1,\"1030\":1,\"1031\":1,\"1032\":1,\"1033\":1,\"1034\":1,\"1035\":1,\"1036\":1,\"1037\":1,\"1038\":1,\"1039\":1,\"1040\":1,\"1041\":1,\"1042\":1,\"1043\":1,\"1044\":1,\"1045\":1,\"1046\":1,\"1047\":1,\"1048\":1,\"1049\":1,\"1051\":1,\"1052\":1,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1059\":1,\"1060\":1,\"1066\":1,\"1067\":1,\"1068\":1,\"1069\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1083\":1,\"1084\":1,\"1085\":1,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":1,\"1094\":1,\"1095\":1,\"1096\":1,\"1097\":1,\"1098\":1,\"1099\":1,\"1100\":1,\"1101\":1,\"1102\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1106\":1,\"1108\":1,\"1109\":1,\"1110\":1,\"1111\":1,\"1114\":1,\"1115\":1,\"1120\":1,\"1121\":1,\"1122\":1,\"1123\":1,\"1134\":1,\"1135\":1,\"1137\":1,\"1138\":1,\"1139\":1,\"1140\":1,\"1142\":1,\"1143\":1,\"1145\":1,\"1146\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1152\":1,\"1153\":1,\"1154\":1,\"1156\":1,\"1159\":1,\"1160\":1,\"1165\":1,\"1166\":1,\"1168\":1,\"1169\":1,\"1177\":1,\"1178\":1,\"1185\":1,\"1186\":1,\"1187\":1,\"1188\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1193\":1,\"1194\":1,\"1195\":1,\"1196\":1,\"1197\":1,\"1200\":1,\"1201\":1,\"1202\":1,\"1203\":1,\"1205\":1,\"1206\":1,\"1211\":1,\"1212\":1,\"1213\":1,\"1214\":1,\"1215\":1,\"1216\":1,\"1219\":1,\"1220\":1,\"1226\":1,\"1227\":1,\"1230\":1,\"1231\":1,\"1233\":1,\"1234\":1,\"1236\":1,\"1237\":1,\"1238\":1,\"1239\":1,\"1240\":1,\"1241\":1,\"1242\":1,\"1243\":1,\"1248\":1,\"1249\":1,\"1253\":1,\"1254\":1,\"1255\":1,\"1256\":1,\"1257\":1,\"1258\":1,\"1259\":1,\"1260\":1,\"1262\":1,\"1263\":1,\"1265\":1,\"1266\":1,\"1284\":1,\"1285\":1,\"1286\":1,\"1287\":1,\"1288\":1,\"1289\":1,\"1383\":1,\"1384\":1,\"1387\":1,\"1388\":1,\"1392\":1,\"1393\":1,\"1398\":1,\"1399\":1,\"1404\":1,\"1405\":1,\"1406\":1,\"1407\":1,\"1411\":1,\"1412\":1,\"1413\":1,\"1414\":1,\"1415\":1,\"1416\":1,\"1417\":1,\"1418\":1,\"1420\":1,\"1421\":1,\"1422\":1,\"1423\":1,\"1424\":1,\"1425\":1,\"1426\":1,\"1427\":1,\"1428\":1,\"1429\":1,\"1430\":1,\"1431\":1,\"1433\":1,\"1434\":1,\"1435\":1,\"1436\":1,\"1437\":1,\"1438\":1,\"1439\":1,\"1440\":1,\"1442\":1,\"1443\":1,\"1444\":1,\"1445\":1,\"1446\":1,\"1447\":1,\"1448\":1,\"1449\":1,\"1450\":1,\"1451\":1,\"1452\":1,\"1453\":1,\"1454\":1,\"1455\":1,\"1456\":1,\"1457\":1,\"1458\":1,\"1459\":1,\"1460\":1,\"1461\":1,\"1462\":1,\"1463\":1,\"1464\":1,\"1465\":1,\"1469\":1,\"1470\":1,\"1502\":2,\"1509\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1517\":1,\"1518\":1,\"1522\":1,\"1523\":1,\"1527\":1,\"1528\":1,\"1530\":1,\"1531\":1,\"1537\":1,\"1538\":1,\"1539\":1,\"1540\":1,\"1541\":1,\"1542\":1,\"1543\":1,\"1544\":1,\"1549\":1,\"1550\":1,\"1554\":1,\"1555\":1,\"1638\":1,\"1639\":1,\"1652\":1,\"1653\":1,\"1657\":1,\"1658\":1,\"1662\":1,\"1663\":1,\"1938\":1,\"1939\":1,\"1940\":2,\"1941\":1,\"1942\":2,\"1943\":1,\"1945\":1,\"1946\":1,\"1957\":1,\"1958\":1,\"1967\":1,\"1968\":1,\"1969\":1,\"1970\":1,\"1972\":1,\"1973\":1,\"1975\":1,\"1976\":1,\"1978\":1,\"1979\":1,\"1980\":1,\"1981\":1,\"1982\":1,\"1983\":1,\"1985\":1,\"1986\":1,\"1988\":1,\"1989\":1,\"2026\":1,\"2027\":1,\"2028\":1,\"2029\":1,\"2030\":1,\"2031\":1,\"2032\":1,\"2033\":1,\"2034\":1,\"2035\":1,\"2124\":1,\"2125\":1,\"2133\":1,\"2168\":1,\"2169\":1,\"2170\":1,\"2171\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2175\":1,\"2177\":1,\"2178\":1,\"2179\":1,\"2180\":1,\"2181\":1,\"2182\":1,\"2185\":1,\"2186\":1,\"2188\":1,\"2189\":1,\"2192\":1,\"2193\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2197\":1,\"2198\":1,\"2199\":1,\"2200\":1,\"2201\":1,\"2203\":1,\"2204\":1,\"2205\":1,\"2206\":1,\"2209\":1,\"2210\":1,\"2211\":1,\"2212\":1,\"2216\":1,\"2217\":1,\"2305\":1,\"2306\":1,\"2325\":1,\"2326\":1,\"2355\":1,\"2401\":1,\"2402\":1,\"2405\":1,\"2406\":1,\"2409\":1,\"2410\":1,\"2414\":1,\"2415\":1,\"2416\":1,\"2417\":1,\"2418\":1,\"2419\":1,\"2434\":1,\"2435\":1,\"2443\":1,\"2444\":1,\"2449\":1,\"2450\":1,\"2451\":1,\"2452\":1,\"2453\":1,\"2454\":1,\"2456\":1,\"2457\":1,\"2458\":1,\"2459\":1,\"2460\":1,\"2461\":1,\"2465\":1,\"2466\":1,\"2467\":1,\"2468\":1}}],[\"catch\",{\"1\":{\"1842\":1,\"2380\":1}}],[\"catpow\",{\"1\":{\"1645\":2}}],[\"cat\",{\"0\":{\"1294\":1,\"1297\":1},\"1\":{\"1076\":1,\"1294\":1,\"1297\":1,\"2235\":1,\"2236\":1}}],[\"catalog\",{\"1\":{\"1002\":1}}],[\"categorical\",{\"1\":{\"1644\":1}}],[\"categories\",{\"1\":{\"1155\":4,\"1157\":5,\"1491\":1,\"1645\":2,\"2000\":3,\"2001\":4,\"2341\":1,\"2346\":1,\"2368\":1}}],[\"categorized\",{\"1\":{\"269\":1,\"278\":1}}],[\"categorypowersampler\",{\"0\":{\"2001\":1},\"1\":{\"2001\":1}}],[\"categorydatasetpowersampler\",{\"0\":{\"2000\":1},\"1\":{\"2000\":1}}],[\"category2utt\",{\"1\":{\"1999\":1,\"2000\":2,\"2001\":2,\"2008\":1}}],[\"categorybalancedsampler\",{\"0\":{\"1999\":1},\"1\":{\"1999\":1}}],[\"categoryiterfactory\",{\"0\":{\"1645\":1},\"1\":{\"1645\":1}}],[\"categorychunkiterfactory\",{\"0\":{\"1643\":1},\"1\":{\"1643\":1}}],[\"category\",{\"0\":{\"1643\":1,\"1645\":1,\"1649\":1,\"1651\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2008\":1,\"2009\":1},\"1\":{\"101\":3,\"223\":2,\"242\":1,\"1155\":3,\"1157\":5,\"1643\":1,\"1644\":1,\"1645\":5,\"1649\":1,\"1651\":1,\"1999\":1,\"2000\":15,\"2001\":8,\"2008\":2,\"2009\":1,\"2249\":2}}],[\"catbel\",{\"1\":{\"95\":1,\"101\":2,\"1645\":2,\"2007\":2}}],[\"ca\",{\"1\":{\"92\":2}}],[\"cauchymultiplysymmetric\",{\"0\":{\"708\":1}}],[\"cauchymultiply\",{\"0\":{\"707\":1}}],[\"cauchy\",{\"0\":{\"707\":1,\"708\":1,\"870\":2,\"871\":2,\"872\":2}}],[\"caution\",{\"1\":{\"74\":1,\"262\":4,\"1697\":1,\"1698\":1}}],[\"causalselfattention\",{\"0\":{\"2038\":1}}],[\"causalconv1d\",{\"0\":{\"1733\":1},\"1\":{\"1733\":2,\"1865\":2}}],[\"causally\",{\"1\":{\"1444\":1,\"1448\":1}}],[\"causal=true\",{\"1\":{\"1080\":1}}],[\"causal=false\",{\"1\":{\"825\":1,\"973\":1,\"981\":1,\"982\":1,\"1061\":1,\"1082\":1,\"1148\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1794\":1,\"2465\":1}}],[\"causal\",{\"1\":{\"43\":5,\"147\":1,\"619\":3,\"620\":3,\"622\":3,\"623\":3,\"658\":1,\"760\":3,\"787\":1,\"980\":2,\"982\":3,\"1061\":2,\"1062\":3,\"1080\":2,\"1202\":1,\"1255\":1,\"1259\":1,\"1261\":3,\"1267\":2,\"1268\":2,\"1273\":3,\"1274\":3,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1424\":1,\"1428\":1,\"1430\":1,\"1442\":2,\"1444\":3,\"1446\":2,\"1448\":3,\"1450\":4,\"1452\":4,\"1454\":3,\"1456\":3,\"1458\":3,\"1460\":3,\"1466\":1,\"1468\":1,\"1485\":4,\"1548\":3,\"1733\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1,\"1794\":2,\"1854\":1,\"1865\":2,\"2458\":1}}],[\"caused\",{\"1\":{\"290\":2}}],[\"cause\",{\"1\":{\"3\":1}}],[\"canonical=none\",{\"1\":{\"2480\":1}}],[\"cannot\",{\"1\":{\"173\":1,\"284\":1,\"290\":2,\"653\":1,\"760\":1,\"1155\":1,\"1157\":1,\"1842\":1,\"2136\":1}}],[\"candidates\",{\"1\":{\"45\":1,\"145\":1,\"616\":6,\"696\":1,\"697\":1,\"1726\":1,\"1727\":1,\"1920\":4}}],[\"can\",{\"1\":{\"1\":1,\"18\":1,\"19\":3,\"22\":3,\"24\":1,\"25\":1,\"26\":1,\"32\":3,\"33\":1,\"37\":1,\"38\":1,\"39\":2,\"40\":1,\"43\":1,\"44\":4,\"47\":3,\"48\":1,\"49\":1,\"50\":3,\"51\":1,\"56\":1,\"59\":1,\"60\":1,\"62\":1,\"67\":1,\"69\":2,\"70\":1,\"71\":4,\"73\":1,\"76\":1,\"79\":2,\"80\":1,\"81\":2,\"82\":4,\"84\":2,\"85\":1,\"87\":1,\"91\":3,\"94\":1,\"96\":1,\"97\":1,\"98\":3,\"104\":1,\"106\":1,\"107\":1,\"108\":2,\"110\":2,\"117\":2,\"118\":2,\"124\":1,\"125\":1,\"126\":1,\"127\":1,\"128\":3,\"130\":1,\"133\":2,\"136\":1,\"139\":2,\"141\":2,\"143\":2,\"144\":1,\"147\":2,\"148\":1,\"150\":2,\"159\":1,\"160\":1,\"161\":2,\"162\":4,\"163\":1,\"164\":1,\"165\":1,\"167\":1,\"168\":4,\"173\":1,\"174\":1,\"175\":8,\"195\":1,\"196\":2,\"197\":3,\"200\":4,\"201\":1,\"205\":3,\"211\":6,\"213\":2,\"217\":5,\"218\":1,\"219\":2,\"220\":1,\"223\":6,\"224\":5,\"225\":3,\"233\":1,\"240\":2,\"242\":6,\"243\":5,\"247\":1,\"254\":1,\"260\":1,\"262\":9,\"263\":2,\"266\":8,\"267\":18,\"268\":2,\"269\":6,\"270\":2,\"271\":2,\"272\":2,\"275\":9,\"276\":13,\"277\":2,\"278\":6,\"279\":2,\"280\":2,\"282\":2,\"285\":10,\"286\":26,\"287\":2,\"288\":2,\"289\":4,\"290\":12,\"536\":4,\"617\":2,\"618\":2,\"620\":2,\"624\":2,\"626\":2,\"636\":2,\"645\":2,\"661\":1,\"669\":1,\"691\":1,\"699\":1,\"709\":1,\"718\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"744\":1,\"756\":4,\"760\":1,\"768\":1,\"773\":4,\"774\":1,\"780\":1,\"809\":1,\"817\":1,\"821\":2,\"828\":1,\"829\":1,\"830\":1,\"852\":1,\"856\":1,\"859\":1,\"866\":2,\"867\":2,\"929\":1,\"1062\":1,\"1145\":1,\"1155\":1,\"1157\":1,\"1246\":2,\"1264\":1,\"1265\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1301\":1,\"1328\":1,\"1334\":1,\"1354\":1,\"1372\":1,\"1478\":1,\"1484\":1,\"1502\":1,\"1533\":1,\"1647\":2,\"1655\":4,\"1678\":1,\"1683\":1,\"1735\":2,\"1751\":3,\"1759\":2,\"1785\":1,\"1786\":1,\"1794\":1,\"1817\":1,\"1818\":1,\"1860\":1,\"1881\":1,\"1883\":1,\"1962\":1,\"2001\":1,\"2130\":2,\"2131\":1,\"2147\":1,\"2155\":1,\"2184\":1,\"2191\":1,\"2216\":1,\"2246\":1,\"2248\":1,\"2249\":2,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":2,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2276\":1,\"2277\":1,\"2286\":1,\"2304\":1,\"2312\":1,\"2344\":1,\"2345\":1,\"2355\":7,\"2480\":1}}],[\"cln\",{\"1\":{\"971\":2,\"980\":1,\"982\":1,\"1029\":1,\"1061\":1,\"1062\":1,\"1074\":2,\"1202\":3,\"1255\":3,\"1259\":2,\"1267\":1,\"1268\":1,\"1273\":1,\"1274\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1}}],[\"clamp\",{\"1\":{\"703\":5,\"755\":5,\"773\":1,\"785\":5,\"786\":5,\"800\":5,\"866\":1,\"867\":1,\"881\":5,\"884\":5,\"921\":1,\"922\":5,\"935\":1,\"936\":5,\"937\":5,\"1246\":5,\"1247\":6}}],[\"clarity\",{\"1\":{\"246\":1}}],[\"class=<class\",{\"1\":{\"692\":1,\"710\":1,\"711\":1,\"731\":1,\"732\":1,\"766\":1,\"767\":1,\"775\":1,\"848\":1,\"849\":1,\"850\":1,\"1957\":1,\"1960\":1,\"1961\":1,\"1992\":1,\"1995\":1,\"2129\":1}}],[\"classchoices\",{\"0\":{\"2334\":1},\"1\":{\"225\":1,\"2246\":12,\"2247\":7,\"2248\":9,\"2249\":3,\"2250\":10,\"2251\":10,\"2252\":27,\"2253\":9,\"2254\":4,\"2255\":13,\"2256\":10,\"2257\":9,\"2259\":15,\"2260\":5,\"2261\":10,\"2262\":4,\"2263\":17,\"2264\":10,\"2265\":13,\"2266\":9,\"2267\":16,\"2268\":13,\"2269\":11,\"2270\":9,\"2271\":10,\"2272\":7,\"2273\":8,\"2334\":2}}],[\"classify\",{\"1\":{\"770\":1,\"959\":1}}],[\"classifier\",{\"1\":{\"233\":1,\"252\":1}}],[\"classification\",{\"0\":{\"209\":1},\"1\":{\"101\":1,\"175\":1,\"201\":1,\"202\":1,\"211\":8,\"212\":1,\"213\":2,\"233\":1,\"235\":1,\"252\":1,\"331\":4,\"403\":4,\"691\":1,\"958\":3,\"962\":2,\"1545\":1,\"1667\":1,\"1805\":1,\"2006\":1,\"2167\":1,\"2176\":1,\"2207\":1,\"2262\":1,\"2365\":1}}],[\"classic\",{\"1\":{\"295\":1,\"415\":1}}],[\"classically\",{\"1\":{\"67\":1}}],[\"classmethod\",{\"1\":{\"78\":6,\"81\":2,\"82\":3,\"675\":1,\"794\":1,\"1654\":1,\"1666\":1,\"1844\":1,\"2246\":6,\"2247\":6,\"2248\":6,\"2249\":29,\"2250\":6,\"2251\":6,\"2252\":6,\"2253\":7,\"2254\":7,\"2255\":7,\"2256\":7,\"2257\":6,\"2259\":6,\"2260\":6,\"2261\":6,\"2262\":6,\"2263\":7,\"2264\":6,\"2265\":5,\"2266\":6,\"2267\":6,\"2268\":7,\"2269\":6,\"2270\":7,\"2271\":7,\"2272\":6,\"2273\":7,\"2282\":1,\"2325\":1,\"2327\":1,\"2338\":5,\"2347\":4,\"2354\":1,\"2365\":2,\"2369\":8,\"2371\":4}}],[\"class\",{\"0\":{\"77\":1,\"78\":1,\"903\":1,\"1891\":1,\"2148\":1,\"2334\":1},\"1\":{\"3\":1,\"78\":4,\"79\":1,\"81\":2,\"82\":4,\"84\":2,\"150\":3,\"211\":6,\"213\":5,\"614\":1,\"615\":1,\"616\":1,\"617\":4,\"618\":4,\"619\":1,\"620\":1,\"621\":1,\"622\":4,\"623\":1,\"624\":4,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"629\":1,\"630\":1,\"631\":1,\"632\":1,\"633\":1,\"634\":1,\"635\":1,\"636\":4,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":4,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"651\":1,\"652\":1,\"653\":1,\"654\":1,\"666\":1,\"674\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":2,\"689\":1,\"691\":1,\"692\":3,\"693\":1,\"696\":2,\"697\":2,\"698\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"703\":2,\"706\":1,\"709\":1,\"710\":2,\"711\":2,\"712\":1,\"713\":1,\"715\":1,\"716\":2,\"717\":1,\"718\":1,\"720\":1,\"722\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"738\":1,\"740\":1,\"741\":1,\"743\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"750\":1,\"751\":1,\"752\":1,\"754\":1,\"755\":2,\"756\":1,\"757\":1,\"759\":1,\"760\":1,\"761\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":1,\"767\":1,\"768\":1,\"770\":1,\"771\":1,\"772\":1,\"773\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"784\":1,\"785\":2,\"786\":1,\"787\":1,\"788\":1,\"790\":1,\"791\":1,\"793\":1,\"794\":1,\"795\":1,\"796\":1,\"798\":2,\"800\":1,\"801\":1,\"802\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"817\":1,\"819\":1,\"820\":1,\"821\":1,\"823\":1,\"824\":1,\"825\":1,\"827\":1,\"828\":2,\"829\":2,\"830\":2,\"831\":1,\"832\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":2,\"850\":1,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"862\":2,\"864\":1,\"866\":4,\"867\":1,\"903\":1,\"912\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":2,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":2,\"959\":1,\"960\":2,\"962\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"978\":1,\"979\":1,\"980\":1,\"981\":1,\"982\":1,\"985\":2,\"987\":1,\"989\":1,\"991\":2,\"993\":2,\"995\":1,\"996\":2,\"998\":2,\"1000\":2,\"1001\":2,\"1003\":2,\"1005\":2,\"1007\":2,\"1009\":2,\"1011\":2,\"1013\":2,\"1015\":2,\"1017\":2,\"1029\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":2,\"1038\":1,\"1040\":1,\"1042\":2,\"1044\":1,\"1046\":1,\"1048\":1,\"1050\":1,\"1051\":2,\"1053\":2,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1061\":2,\"1062\":2,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1107\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1116\":2,\"1117\":1,\"1118\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1129\":1,\"1130\":1,\"1131\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1137\":1,\"1139\":1,\"1141\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1147\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1159\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":2,\"1175\":1,\"1176\":1,\"1177\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1189\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":1,\"1204\":1,\"1205\":1,\"1207\":1,\"1208\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1219\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1224\":1,\"1225\":2,\"1226\":1,\"1228\":1,\"1229\":2,\"1230\":1,\"1232\":1,\"1233\":1,\"1235\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1244\":1,\"1245\":2,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1267\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1275\":1,\"1276\":2,\"1277\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1333\":1,\"1334\":1,\"1381\":2,\"1382\":1,\"1383\":1,\"1385\":1,\"1387\":1,\"1389\":1,\"1390\":1,\"1391\":1,\"1392\":1,\"1394\":1,\"1395\":1,\"1396\":1,\"1397\":1,\"1398\":1,\"1400\":1,\"1401\":1,\"1402\":1,\"1403\":1,\"1404\":1,\"1406\":1,\"1408\":1,\"1409\":1,\"1410\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1432\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1466\":1,\"1467\":1,\"1468\":1,\"1469\":1,\"1508\":2,\"1509\":1,\"1511\":1,\"1513\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1519\":1,\"1520\":1,\"1521\":1,\"1522\":1,\"1524\":1,\"1525\":1,\"1526\":2,\"1527\":1,\"1529\":1,\"1530\":1,\"1532\":1,\"1533\":2,\"1534\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1548\":1,\"1549\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1556\":1,\"1576\":2,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1600\":2,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1625\":1,\"1626\":1,\"1627\":1,\"1628\":1,\"1638\":1,\"1640\":1,\"1641\":1,\"1642\":1,\"1643\":1,\"1644\":2,\"1645\":3,\"1646\":1,\"1648\":1,\"1649\":1,\"1650\":2,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":2,\"1657\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1702\":1,\"1703\":3,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":3,\"1718\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1728\":3,\"1729\":1,\"1730\":1,\"1731\":2,\"1732\":1,\"1733\":1,\"1734\":2,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":2,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1761\":1,\"1762\":1,\"1763\":1,\"1764\":1,\"1766\":2,\"1768\":1,\"1770\":1,\"1771\":1,\"1772\":2,\"1773\":2,\"1775\":1,\"1776\":3,\"1777\":1,\"1778\":1,\"1779\":1,\"1780\":2,\"1781\":2,\"1782\":4,\"1783\":1,\"1784\":2,\"1785\":1,\"1786\":2,\"1787\":2,\"1788\":1,\"1789\":1,\"1790\":1,\"1791\":3,\"1793\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1802\":2,\"1803\":2,\"1804\":1,\"1806\":1,\"1807\":1,\"1808\":3,\"1809\":1,\"1810\":1,\"1811\":1,\"1813\":3,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":2,\"1821\":1,\"1823\":1,\"1824\":1,\"1825\":2,\"1826\":2,\"1828\":2,\"1829\":2,\"1832\":2,\"1833\":3,\"1834\":1,\"1835\":3,\"1836\":3,\"1837\":1,\"1838\":1,\"1839\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1847\":1,\"1848\":2,\"1849\":2,\"1850\":3,\"1851\":2,\"1852\":3,\"1853\":2,\"1854\":2,\"1855\":1,\"1864\":1,\"1866\":3,\"1876\":4,\"1891\":6,\"1938\":4,\"1940\":1,\"1942\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1948\":1,\"1957\":2,\"1959\":1,\"1960\":2,\"1961\":2,\"1962\":1,\"1964\":1,\"1965\":1,\"1966\":1,\"1967\":2,\"1969\":2,\"1971\":2,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1992\":2,\"1993\":1,\"1994\":1,\"1995\":2,\"1996\":1,\"1997\":1,\"1998\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":3,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":1,\"2015\":1,\"2016\":2,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2039\":1,\"2040\":2,\"2043\":1,\"2044\":2,\"2045\":3,\"2049\":2,\"2054\":2,\"2055\":1,\"2056\":1,\"2061\":1,\"2065\":2,\"2066\":1,\"2124\":1,\"2126\":1,\"2127\":2,\"2128\":1,\"2129\":2,\"2130\":3,\"2131\":2,\"2132\":1,\"2133\":2,\"2134\":2,\"2136\":2,\"2137\":2,\"2138\":2,\"2139\":1,\"2141\":1,\"2142\":2,\"2143\":1,\"2144\":1,\"2148\":3,\"2167\":2,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":5,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":2,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":2,\"2194\":1,\"2196\":1,\"2198\":2,\"2200\":1,\"2202\":1,\"2203\":2,\"2205\":1,\"2207\":2,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":3,\"2216\":1,\"2218\":1,\"2219\":1,\"2220\":1,\"2221\":1,\"2222\":2,\"2223\":2,\"2226\":1,\"2227\":1,\"2228\":1,\"2229\":1,\"2231\":1,\"2232\":1,\"2235\":1,\"2236\":1,\"2237\":1,\"2238\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2245\":1,\"2246\":14,\"2247\":7,\"2248\":11,\"2249\":7,\"2250\":12,\"2251\":12,\"2252\":29,\"2253\":12,\"2254\":6,\"2255\":15,\"2256\":12,\"2257\":11,\"2258\":1,\"2259\":17,\"2260\":7,\"2261\":12,\"2262\":4,\"2263\":19,\"2264\":12,\"2265\":15,\"2266\":11,\"2267\":18,\"2268\":15,\"2269\":13,\"2270\":11,\"2271\":12,\"2272\":9,\"2273\":10,\"2274\":1,\"2275\":1,\"2276\":2,\"2277\":2,\"2278\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2282\":1,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2287\":1,\"2288\":1,\"2289\":1,\"2291\":1,\"2292\":1,\"2304\":2,\"2305\":1,\"2324\":1,\"2325\":7,\"2327\":5,\"2328\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":9,\"2335\":2,\"2336\":1,\"2337\":1,\"2338\":1,\"2339\":1,\"2340\":1,\"2341\":1,\"2342\":2,\"2344\":1,\"2345\":1,\"2346\":1,\"2347\":1,\"2348\":1,\"2349\":1,\"2350\":2,\"2351\":2,\"2353\":1,\"2354\":1,\"2355\":1,\"2356\":1,\"2357\":1,\"2358\":1,\"2359\":2,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2364\":1,\"2365\":1,\"2366\":1,\"2367\":2,\"2368\":1,\"2369\":3,\"2370\":1,\"2371\":1,\"2372\":1,\"2373\":1,\"2401\":1,\"2403\":2,\"2404\":1,\"2407\":1,\"2408\":1,\"2409\":1,\"2411\":1,\"2412\":1,\"2413\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2421\":1,\"2422\":2,\"2423\":1,\"2424\":1,\"2425\":1,\"2426\":1,\"2427\":1,\"2428\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":1,\"2433\":1,\"2434\":1,\"2443\":1,\"2445\":2,\"2446\":1,\"2447\":1,\"2448\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":2,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1,\"2474\":3,\"2477\":1,\"2478\":2,\"2480\":1,\"2481\":1,\"2482\":1}}],[\"classes=1000\",{\"1\":{\"807\":1}}],[\"classes\",{\"1\":{\"3\":1,\"101\":1,\"150\":1,\"259\":1,\"770\":2,\"846\":3,\"958\":2,\"959\":2,\"960\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1667\":2,\"2167\":1,\"2176\":1,\"2207\":1,\"2218\":3,\"2219\":2,\"2334\":1}}],[\"clip\",{\"1\":{\"2348\":2,\"2353\":1,\"2370\":4,\"2372\":2}}],[\"clipped\",{\"1\":{\"1676\":3}}],[\"clipping\",{\"0\":{\"1676\":1},\"1\":{\"1676\":2}}],[\"client\",{\"1\":{\"2054\":1}}],[\"cli\",{\"0\":{\"1718\":1,\"1772\":1,\"1773\":1,\"1780\":1,\"1781\":1,\"1825\":1,\"1826\":1,\"1828\":1,\"1829\":1,\"1859\":1,\"1881\":1,\"1883\":1,\"1887\":1,\"1889\":1,\"1898\":1,\"1911\":1,\"2037\":1},\"1\":{\"1718\":1,\"1772\":1,\"1773\":1,\"1780\":1,\"1781\":1,\"1825\":1,\"1826\":1,\"1828\":1,\"1829\":1,\"1859\":1,\"1881\":1,\"1883\":1,\"1887\":1,\"1889\":1,\"1898\":1,\"1911\":1}}],[\"climate\",{\"1\":{\"242\":2}}],[\"click\",{\"1\":{\"161\":1,\"247\":1}}],[\"cl\",{\"1\":{\"202\":1}}],[\"clusters\",{\"1\":{\"1489\":1}}],[\"clustering\",{\"1\":{\"1130\":3,\"1131\":3,\"1172\":3,\"2138\":1}}],[\"cluster\",{\"1\":{\"168\":1,\"236\":1,\"1400\":1,\"1441\":1,\"1469\":1,\"2138\":1}}],[\"closure\",{\"1\":{\"2355\":1}}],[\"close\",{\"1\":{\"254\":1,\"986\":1,\"999\":1,\"1006\":1,\"1010\":1,\"1014\":1,\"1018\":1,\"1029\":1,\"1235\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1718\":2,\"1770\":1,\"1771\":2,\"1824\":2,\"1948\":1,\"2367\":1}}],[\"closed\",{\"1\":{\"233\":1,\"252\":1,\"2365\":1}}],[\"cloud\",{\"0\":{\"152\":1}}],[\"cloning\",{\"1\":{\"1\":1}}],[\"clone\",{\"1\":{\"1\":1,\"161\":2,\"162\":2,\"260\":2}}],[\"clstask\",{\"0\":{\"2250\":1},\"1\":{\"2250\":1}}],[\"clstm\",{\"1\":{\"1118\":2}}],[\"cls1\",{\"1\":{\"209\":1,\"212\":3,\"213\":1}}],[\"cls\",{\"0\":{\"331\":1,\"956\":1,\"958\":1,\"959\":1,\"960\":1,\"962\":1,\"2250\":1,\"2519\":1},\"1\":{\"78\":6,\"81\":2,\"82\":3,\"210\":3,\"211\":5,\"212\":3,\"331\":2,\"402\":1,\"403\":2,\"702\":1,\"956\":1,\"958\":1,\"959\":1,\"960\":1,\"962\":1,\"1051\":2,\"1226\":1,\"2246\":2,\"2247\":6,\"2248\":2,\"2249\":5,\"2250\":3,\"2251\":2,\"2252\":2,\"2253\":3,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2325\":1,\"2327\":1,\"2369\":2}}],[\"cleaning\",{\"1\":{\"290\":1}}],[\"cleaned\",{\"1\":{\"266\":1,\"275\":1,\"285\":1}}],[\"cleaner\",{\"0\":{\"270\":1,\"279\":1,\"288\":1,\"2282\":1,\"2289\":1},\"1\":{\"126\":4,\"243\":2,\"265\":1,\"266\":3,\"270\":2,\"274\":1,\"275\":3,\"279\":2,\"284\":2,\"285\":4,\"288\":2,\"290\":2,\"481\":2,\"2282\":1,\"2289\":3,\"2290\":2,\"2336\":1,\"2337\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1}}],[\"cleaner=whisper\",{\"1\":{\"126\":2}}],[\"clean\",{\"0\":{\"515\":1},\"1\":{\"31\":2,\"136\":3,\"161\":1,\"201\":2,\"224\":1,\"290\":1,\"515\":2,\"536\":1}}],[\"cochlear\",{\"1\":{\"1327\":1,\"1330\":1}}],[\"coherence\",{\"1\":{\"1170\":1}}],[\"cohort\",{\"1\":{\"254\":1,\"449\":4}}],[\"coverage\",{\"1\":{\"1705\":1,\"1706\":2}}],[\"covered\",{\"1\":{\"150\":1}}],[\"covolution\",{\"1\":{\"1552\":1,\"1626\":1}}],[\"covariances\",{\"0\":{\"1315\":1},\"1\":{\"1315\":1}}],[\"covariance\",{\"1\":{\"1126\":5,\"1309\":1,\"1310\":1,\"1311\":3,\"1315\":1,\"1318\":2,\"1319\":1,\"1321\":2,\"1322\":3,\"1323\":2,\"1327\":2,\"1328\":2,\"1330\":2}}],[\"co\",{\"1\":{\"1053\":1}}],[\"coeifficient\",{\"1\":{\"691\":1}}],[\"coeff\",{\"1\":{\"1686\":3,\"1694\":3}}],[\"coefficient\",{\"1\":{\"637\":4,\"1526\":4,\"1553\":8,\"1598\":5,\"1600\":4,\"1608\":1,\"1625\":5,\"1631\":1,\"1686\":1,\"1694\":1,\"1770\":1,\"1771\":2,\"2239\":4,\"2240\":4}}],[\"coefficients\",{\"1\":{\"637\":4,\"1246\":3,\"1353\":1,\"1419\":1}}],[\"coef\",{\"1\":{\"691\":2}}],[\"cos\",{\"1\":{\"647\":1,\"1164\":1,\"1545\":1}}],[\"cosineannealingwarmuprestarts\",{\"0\":{\"2014\":1},\"1\":{\"2014\":1}}],[\"cosine\",{\"0\":{\"2014\":1},\"1\":{\"286\":1,\"674\":2,\"1164\":1,\"1489\":1,\"1631\":1,\"2014\":2}}],[\"costs\",{\"0\":{\"880\":1},\"1\":{\"703\":2,\"755\":7,\"785\":7,\"880\":1,\"922\":3,\"936\":3,\"937\":3}}],[\"cost\",{\"1\":{\"128\":1,\"254\":1,\"262\":1,\"703\":2,\"755\":2,\"785\":2,\"1881\":1}}],[\"coding\",{\"1\":{\"225\":1}}],[\"codebooks\",{\"1\":{\"1400\":2,\"1441\":3,\"1469\":2}}],[\"codebook\",{\"1\":{\"1391\":1,\"1400\":6,\"1403\":1,\"1441\":2,\"1469\":8,\"2239\":1}}],[\"codebases\",{\"1\":{\"269\":1,\"278\":1}}],[\"codebase\",{\"1\":{\"232\":2,\"258\":1,\"259\":1}}],[\"coded\",{\"1\":{\"267\":1,\"286\":1}}],[\"codecembedding\",{\"0\":{\"1957\":1},\"1\":{\"1957\":1}}],[\"codec1\",{\"1\":{\"218\":2,\"220\":1}}],[\"codecs\",{\"0\":{\"1677\":1},\"1\":{\"71\":3,\"1381\":2,\"1391\":3,\"1395\":1,\"1403\":3,\"1406\":2,\"1410\":3,\"1468\":3,\"1677\":2,\"2136\":1}}],[\"codec\",{\"0\":{\"74\":1,\"215\":1,\"368\":1,\"1381\":2,\"1382\":1,\"1383\":1,\"1385\":1,\"1386\":1,\"1387\":1,\"1389\":1,\"1390\":1,\"1391\":1,\"1392\":1,\"1394\":1,\"1395\":1,\"1396\":1,\"1397\":1,\"1398\":1,\"1400\":1,\"1401\":1,\"1402\":1,\"1403\":1,\"1404\":1,\"1406\":1,\"1408\":1,\"1409\":1,\"1410\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1432\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1466\":1,\"1467\":1,\"1468\":1,\"1469\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1486\":1,\"1487\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1505\":1,\"1506\":1,\"1507\":1,\"2254\":1,\"2524\":1},\"1\":{\"69\":1,\"70\":1,\"71\":2,\"215\":1,\"216\":4,\"217\":8,\"218\":2,\"219\":2,\"220\":6,\"227\":4,\"273\":1,\"281\":1,\"368\":2,\"402\":1,\"403\":2,\"1381\":4,\"1382\":1,\"1383\":1,\"1385\":1,\"1386\":1,\"1387\":1,\"1389\":4,\"1390\":1,\"1391\":3,\"1392\":1,\"1394\":1,\"1395\":10,\"1396\":1,\"1397\":1,\"1398\":1,\"1400\":1,\"1401\":5,\"1402\":1,\"1403\":4,\"1404\":1,\"1406\":3,\"1408\":4,\"1409\":1,\"1410\":3,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1432\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1466\":4,\"1467\":1,\"1468\":3,\"1469\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1486\":1,\"1487\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1505\":1,\"1506\":1,\"1507\":1,\"1678\":1,\"1957\":3,\"2136\":18,\"2239\":1,\"2254\":2}}],[\"codes\",{\"1\":{\"3\":1,\"243\":1,\"1389\":2,\"1391\":1,\"1395\":3,\"1400\":2,\"1401\":2,\"1403\":1,\"1406\":1,\"1408\":2,\"1410\":1,\"1432\":2,\"1441\":3,\"1466\":2,\"1468\":1,\"1469\":1,\"2130\":3,\"2136\":3}}],[\"code\",{\"0\":{\"263\":1},\"1\":{\"3\":1,\"18\":3,\"19\":1,\"26\":3,\"162\":1,\"260\":2,\"263\":2,\"269\":2,\"270\":1,\"271\":1,\"278\":2,\"279\":1,\"280\":1,\"287\":1,\"288\":1,\"536\":1,\"699\":1,\"750\":1,\"1224\":1,\"1225\":1,\"1245\":1,\"1270\":1,\"1389\":4,\"1391\":1,\"1396\":1,\"1400\":3,\"1401\":4,\"1403\":1,\"1406\":2,\"1408\":4,\"1410\":1,\"1441\":3,\"1466\":4,\"1468\":1,\"1469\":3,\"1855\":1,\"1957\":1,\"1977\":1,\"2345\":1,\"2427\":1,\"2462\":1}}],[\"coarse\",{\"1\":{\"91\":1}}],[\"cooldown=0\",{\"1\":{\"2020\":1}}],[\"cooldown\",{\"1\":{\"84\":1,\"2020\":1}}],[\"cols\",{\"1\":{\"803\":1,\"804\":1,\"931\":1,\"932\":1,\"933\":1,\"934\":1}}],[\"colab\",{\"1\":{\"162\":1,\"290\":1,\"536\":2}}],[\"collating\",{\"1\":{\"2142\":1}}],[\"collation\",{\"1\":{\"2142\":1}}],[\"collate\",{\"0\":{\"2335\":1,\"2350\":1,\"2376\":2},\"1\":{\"78\":1,\"82\":12,\"1643\":1,\"1644\":1,\"1645\":3,\"1646\":1,\"1647\":1,\"1650\":1,\"2131\":5,\"2134\":3,\"2143\":1,\"2246\":5,\"2247\":3,\"2248\":5,\"2249\":6,\"2250\":5,\"2251\":5,\"2252\":5,\"2253\":5,\"2254\":5,\"2255\":5,\"2256\":5,\"2257\":5,\"2258\":2,\"2259\":5,\"2260\":5,\"2261\":5,\"2262\":2,\"2263\":5,\"2264\":5,\"2266\":5,\"2267\":5,\"2268\":5,\"2269\":5,\"2270\":5,\"2271\":5,\"2272\":5,\"2273\":5,\"2335\":2,\"2350\":2,\"2376\":2,\"2377\":1}}],[\"collaborative\",{\"1\":{\"1515\":1,\"1516\":1}}],[\"collecting\",{\"1\":{\"1132\":1,\"1167\":1,\"1204\":1,\"1209\":1,\"1228\":1,\"1824\":1}}],[\"collection\",{\"1\":{\"199\":2,\"200\":2,\"204\":2,\"205\":2,\"210\":1,\"211\":1,\"216\":1,\"217\":1,\"227\":1,\"234\":1,\"235\":1,\"240\":2,\"242\":2,\"243\":1,\"245\":1,\"246\":1,\"253\":1,\"254\":1,\"265\":1,\"266\":1,\"274\":1,\"275\":1,\"284\":1,\"285\":1,\"625\":1,\"1648\":1,\"1949\":1,\"2130\":3,\"2131\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2289\":1,\"2335\":1,\"2336\":2,\"2337\":2,\"2342\":1,\"2344\":1,\"2350\":1,\"2351\":1,\"2355\":2,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2366\":1,\"2376\":2}}],[\"collected\",{\"1\":{\"266\":1,\"267\":2,\"275\":1,\"286\":3,\"1951\":1,\"2462\":1}}],[\"collects\",{\"1\":{\"200\":2,\"205\":2,\"211\":1,\"217\":1,\"235\":1,\"242\":2,\"254\":1,\"266\":1,\"275\":1,\"285\":1}}],[\"collect\",{\"0\":{\"1951\":2,\"2149\":1},\"1\":{\"128\":1,\"222\":1,\"223\":3,\"228\":2,\"243\":2,\"625\":4,\"736\":1,\"737\":1,\"777\":1,\"954\":1,\"958\":1,\"974\":1,\"1155\":3,\"1156\":1,\"1157\":4,\"1158\":2,\"1395\":1,\"1521\":1,\"1585\":1,\"1640\":1,\"1641\":1,\"1702\":2,\"1940\":1,\"1942\":1,\"1951\":3,\"1959\":2,\"1965\":2,\"1975\":2,\"1996\":2,\"1997\":2,\"2044\":1,\"2127\":2,\"2130\":1,\"2149\":2,\"2166\":1,\"2184\":1,\"2216\":2,\"2221\":2,\"2228\":1,\"2229\":1,\"2325\":1,\"2327\":1,\"2408\":1,\"2446\":1,\"2462\":1}}],[\"column\",{\"1\":{\"97\":1,\"168\":1,\"1329\":1,\"1558\":1}}],[\"columns=false\",{\"1\":{\"2397\":1}}],[\"columns=true\",{\"1\":{\"1008\":3,\"1010\":1}}],[\"columns\",{\"0\":{\"1026\":1,\"2393\":1,\"2399\":1},\"1\":{\"76\":2,\"80\":1,\"1007\":1,\"1009\":5,\"1022\":1,\"1026\":2,\"1027\":1,\"2393\":1,\"2399\":1}}],[\"coupling\",{\"0\":{\"1612\":1,\"1613\":1},\"1\":{\"1612\":3,\"1613\":2}}],[\"coupled\",{\"1\":{\"1334\":1}}],[\"course\",{\"0\":{\"189\":1},\"1\":{\"70\":1,\"190\":1,\"267\":1,\"276\":1,\"286\":1}}],[\"count=1\",{\"1\":{\"1477\":1}}],[\"counter\",{\"1\":{\"2354\":1}}],[\"counterpart\",{\"1\":{\"262\":2}}],[\"counteracts\",{\"1\":{\"262\":1}}],[\"countermeasures\",{\"0\":{\"208\":1}}],[\"counting\",{\"1\":{\"99\":1,\"100\":1}}],[\"counts\",{\"1\":{\"48\":1,\"2143\":1}}],[\"count\",{\"0\":{\"2151\":1,\"2310\":1},\"1\":{\"48\":6,\"243\":3,\"290\":1,\"704\":1,\"705\":1,\"1477\":1,\"1497\":1,\"1606\":1,\"1794\":1,\"2137\":1,\"2151\":8,\"2159\":1,\"2310\":8,\"2367\":2}}],[\"could\",{\"1\":{\"3\":1,\"2355\":1}}],[\"copies\",{\"1\":{\"235\":1,\"821\":1,\"824\":2,\"938\":1}}],[\"copied\",{\"1\":{\"26\":1,\"223\":1,\"243\":1,\"290\":1,\"2276\":1,\"2277\":1}}],[\"copy=false\",{\"1\":{\"2164\":1,\"2323\":1}}],[\"copy\",{\"0\":{\"561\":1,\"886\":1,\"2074\":1},\"1\":{\"3\":1,\"242\":1,\"243\":2,\"259\":1,\"269\":1,\"278\":1,\"290\":1,\"561\":2,\"886\":1,\"1224\":1,\"1225\":1,\"1245\":1,\"2130\":7,\"2133\":3,\"2136\":3,\"2137\":2}}],[\"copying\",{\"1\":{\"3\":1,\"195\":1}}],[\"combdblock\",{\"0\":{\"1516\":1},\"1\":{\"1516\":1}}],[\"combd\",{\"0\":{\"1515\":1},\"1\":{\"1509\":10,\"1511\":10,\"1515\":3,\"1516\":2,\"1553\":10}}],[\"combining\",{\"1\":{\"284\":1,\"290\":1,\"756\":1,\"773\":1,\"2000\":1,\"2136\":1}}],[\"combination\",{\"0\":{\"877\":1},\"1\":{\"262\":1,\"718\":1,\"821\":2,\"877\":1,\"1706\":1,\"2020\":1,\"2021\":1,\"2184\":1}}],[\"combine=\",{\"1\":{\"1211\":1}}],[\"combines\",{\"1\":{\"247\":1,\"1420\":1,\"2131\":1,\"2132\":1,\"2176\":1}}],[\"combineddataset\",{\"0\":{\"2132\":1},\"1\":{\"2132\":1}}],[\"combined\",{\"1\":{\"201\":2,\"276\":1,\"756\":1,\"773\":1,\"2001\":1,\"2130\":1,\"2132\":1,\"2136\":3}}],[\"combine\",{\"0\":{\"1076\":1},\"1\":{\"197\":2,\"267\":1,\"275\":1,\"276\":3,\"286\":3,\"289\":1,\"777\":1,\"831\":1,\"1076\":2,\"1156\":1,\"1668\":1,\"1940\":1,\"1942\":1,\"2344\":1}}],[\"comes\",{\"1\":{\"1717\":1}}],[\"come\",{\"1\":{\"104\":1,\"173\":1,\"2136\":1}}],[\"coming\",{\"1\":{\"65\":1,\"276\":1}}],[\"comprises\",{\"1\":{\"2184\":1}}],[\"compress=true\",{\"1\":{\"1774\":1}}],[\"compress=false\",{\"1\":{\"1773\":1,\"1781\":1}}],[\"compressed\",{\"1\":{\"1678\":1}}],[\"compress\",{\"1\":{\"548\":2,\"551\":2,\"561\":2,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1458\":2,\"1460\":2,\"1466\":1,\"1468\":1,\"1883\":3}}],[\"compression\",{\"1\":{\"71\":2,\"548\":2,\"551\":2,\"561\":2,\"691\":1,\"927\":1,\"1661\":2,\"1668\":1,\"1677\":1,\"1678\":1,\"1781\":1,\"1883\":3}}],[\"comprehensive\",{\"1\":{\"198\":1,\"245\":1}}],[\"compensate\",{\"1\":{\"1810\":1}}],[\"comp\",{\"0\":{\"1029\":1,\"1281\":2},\"1\":{\"1029\":1,\"1280\":6,\"1281\":4}}],[\"component\",{\"1\":{\"1051\":1}}],[\"components\",{\"0\":{\"263\":1,\"688\":1,\"730\":1,\"769\":1,\"788\":1,\"811\":1,\"813\":1,\"818\":1,\"835\":1,\"837\":1,\"842\":1,\"844\":1,\"852\":1,\"856\":1,\"906\":1,\"939\":1},\"1\":{\"246\":1,\"247\":1,\"260\":1,\"263\":1,\"688\":1,\"730\":1,\"769\":1,\"788\":1,\"811\":1,\"813\":1,\"835\":1,\"837\":1,\"842\":1,\"844\":1,\"852\":1,\"856\":1,\"906\":1,\"939\":1,\"1546\":1,\"1552\":1,\"1622\":1,\"1662\":1,\"2044\":1,\"2130\":4,\"2136\":1}}],[\"composed\",{\"1\":{\"140\":1,\"143\":1,\"1415\":1}}],[\"complicated\",{\"1\":{\"232\":1,\"258\":1}}],[\"complexrelu\",{\"1\":{\"1411\":1}}],[\"complexstftresidualunit\",{\"0\":{\"1386\":1},\"1\":{\"1386\":1}}],[\"complexstft\",{\"1\":{\"1385\":1,\"1401\":1,\"1402\":2,\"1466\":1,\"1467\":2}}],[\"complexstftdiscriminator\",{\"0\":{\"1385\":1},\"1\":{\"1385\":1}}],[\"complex=false\",{\"1\":{\"1269\":1,\"1270\":1}}],[\"complexlinear\",{\"0\":{\"1084\":1},\"1\":{\"1084\":1}}],[\"complexconvtranspose2d\",{\"0\":{\"1082\":1},\"1\":{\"1082\":2}}],[\"complexconv2d\",{\"0\":{\"1080\":1,\"1383\":1},\"1\":{\"1080\":2,\"1383\":2}}],[\"complexbatchnorm\",{\"0\":{\"1078\":1},\"1\":{\"1078\":1}}],[\"complex64\",{\"1\":{\"1054\":1,\"1126\":10,\"1127\":1,\"1217\":2,\"1293\":3,\"1309\":3,\"1310\":3,\"1311\":4,\"1318\":3,\"1319\":3,\"1321\":3,\"1322\":4,\"1323\":3,\"1326\":2,\"1327\":3,\"1328\":3,\"1330\":3,\"1351\":1,\"1354\":1,\"1361\":2}}],[\"complextensor\",{\"1\":{\"978\":3,\"980\":1,\"1054\":2,\"1062\":3,\"1107\":3,\"1117\":3,\"1118\":6,\"1125\":3,\"1126\":13,\"1127\":5,\"1130\":3,\"1131\":3,\"1136\":3,\"1141\":3,\"1162\":1,\"1199\":1,\"1204\":2,\"1217\":5,\"1232\":3,\"1250\":3,\"1251\":1,\"1252\":1,\"1261\":3,\"1267\":3,\"1268\":3,\"1278\":3,\"1280\":3,\"1283\":3,\"1291\":3,\"1293\":3,\"1294\":1,\"1298\":1,\"1309\":6,\"1310\":6,\"1311\":8,\"1314\":3,\"1315\":2,\"1317\":4,\"1318\":6,\"1319\":6,\"1321\":3,\"1322\":8,\"1323\":3,\"1326\":2,\"1327\":3,\"1328\":3,\"1330\":3,\"1334\":4,\"1337\":2,\"1342\":3,\"1348\":1,\"1351\":4,\"1352\":3,\"1354\":1,\"1355\":1,\"1356\":2,\"1357\":2,\"1359\":1,\"1361\":2,\"1367\":1,\"1376\":2,\"1377\":2,\"1669\":2,\"1766\":3,\"1932\":3,\"1933\":1}}],[\"complexity\",{\"1\":{\"262\":1}}],[\"complexnn\",{\"0\":{\"1078\":1,\"1080\":1,\"1082\":1,\"1215\":1,\"1297\":1},\"1\":{\"223\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1215\":1,\"1297\":1}}],[\"complex\",{\"0\":{\"1294\":1,\"1297\":1,\"1298\":2,\"1307\":1,\"1337\":1,\"1338\":2,\"1339\":2,\"1342\":1,\"1348\":2,\"1355\":1,\"1357\":1,\"1359\":1,\"1363\":2,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1},\"1\":{\"223\":1,\"823\":1,\"846\":2,\"1051\":3,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":3,\"1118\":5,\"1119\":2,\"1120\":1,\"1122\":1,\"1125\":2,\"1145\":2,\"1149\":1,\"1165\":1,\"1251\":1,\"1264\":3,\"1269\":2,\"1270\":2,\"1271\":2,\"1280\":1,\"1283\":1,\"1294\":1,\"1297\":1,\"1298\":2,\"1307\":1,\"1308\":2,\"1314\":1,\"1315\":1,\"1317\":1,\"1318\":1,\"1334\":5,\"1337\":1,\"1338\":2,\"1339\":2,\"1342\":1,\"1348\":2,\"1351\":1,\"1352\":1,\"1355\":1,\"1357\":1,\"1359\":1,\"1363\":2,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1,\"1376\":1,\"1377\":1,\"1385\":1,\"1386\":2,\"1402\":1,\"1413\":1,\"1467\":1,\"1547\":1,\"2220\":2,\"2332\":2,\"2367\":3,\"2373\":6,\"2374\":1,\"2398\":2}}],[\"completely\",{\"1\":{\"106\":1,\"821\":1}}],[\"complete\",{\"1\":{\"37\":1,\"51\":1,\"146\":1,\"160\":1,\"196\":1,\"213\":1,\"267\":6,\"268\":1,\"276\":4,\"277\":1,\"625\":1,\"2039\":1,\"2130\":1,\"2136\":1}}],[\"computing\",{\"1\":{\"777\":1,\"846\":2,\"1156\":1,\"1224\":1,\"1225\":1,\"1245\":1,\"1940\":1,\"1942\":1,\"2184\":1,\"2354\":1}}],[\"computations\",{\"1\":{\"785\":1}}],[\"computationally\",{\"1\":{\"262\":1}}],[\"computational\",{\"1\":{\"262\":2,\"2405\":1}}],[\"computation\",{\"1\":{\"139\":4,\"141\":1,\"142\":1,\"629\":1,\"632\":1,\"633\":1,\"634\":1,\"635\":1,\"639\":1,\"642\":1,\"643\":1,\"644\":6,\"650\":1,\"652\":1,\"661\":1,\"665\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"693\":1,\"713\":1,\"716\":1,\"718\":1,\"720\":1,\"722\":1,\"738\":1,\"741\":1,\"752\":1,\"757\":1,\"778\":1,\"781\":1,\"788\":1,\"791\":1,\"792\":1,\"796\":1,\"798\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"821\":1,\"825\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"842\":1,\"844\":1,\"852\":1,\"854\":1,\"856\":1,\"860\":1,\"862\":1,\"864\":1,\"950\":1,\"952\":1,\"956\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1066\":1,\"1068\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1114\":1,\"1120\":1,\"1122\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1145\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1159\":1,\"1165\":1,\"1168\":1,\"1177\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1200\":1,\"1202\":1,\"1205\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1219\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1246\":1,\"1248\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1262\":1,\"1265\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1310\":1,\"1334\":3,\"1383\":1,\"1387\":1,\"1392\":1,\"1398\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1509\":1,\"1511\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1549\":1,\"1554\":1,\"1638\":1,\"1652\":1,\"1657\":1,\"1662\":1,\"1750\":3,\"1759\":1,\"1779\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1945\":1,\"1957\":1,\"1965\":1,\"1967\":1,\"1969\":1,\"1972\":1,\"1975\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2043\":1,\"2045\":1,\"2049\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2124\":1,\"2130\":1,\"2131\":1,\"2148\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2185\":1,\"2188\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2203\":1,\"2205\":1,\"2209\":1,\"2211\":1,\"2216\":1,\"2224\":1,\"2225\":1,\"2305\":1,\"2325\":1,\"2401\":1,\"2405\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2434\":1,\"2443\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2465\":1,\"2467\":1}}],[\"computemindcf\",{\"0\":{\"2476\":1},\"1\":{\"2476\":1}}],[\"computeerrorrates\",{\"0\":{\"2475\":1},\"1\":{\"2475\":1}}],[\"computes\",{\"1\":{\"235\":1,\"755\":1,\"821\":1,\"822\":1,\"824\":2,\"960\":2,\"1719\":2,\"1725\":3,\"1751\":1,\"2130\":1,\"2131\":1}}],[\"computed\",{\"1\":{\"200\":2,\"205\":2,\"242\":1,\"639\":2,\"706\":1,\"756\":1,\"773\":1,\"866\":1,\"867\":1,\"1308\":1,\"1334\":1,\"1794\":2,\"1997\":1}}],[\"computer\",{\"1\":{\"22\":1,\"958\":1}}],[\"compute\",{\"0\":{\"548\":1,\"551\":1,\"878\":1,\"879\":1,\"880\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"1378\":1},\"1\":{\"7\":1,\"44\":1,\"139\":1,\"175\":1,\"201\":3,\"548\":2,\"551\":2,\"615\":1,\"619\":1,\"622\":1,\"623\":1,\"625\":1,\"627\":2,\"630\":1,\"633\":2,\"637\":5,\"638\":1,\"639\":2,\"640\":1,\"642\":1,\"644\":5,\"645\":1,\"646\":1,\"647\":3,\"648\":1,\"649\":2,\"701\":1,\"703\":5,\"735\":1,\"740\":2,\"749\":1,\"755\":3,\"777\":2,\"784\":2,\"785\":3,\"787\":2,\"847\":1,\"878\":2,\"879\":2,\"880\":1,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"919\":1,\"926\":1,\"960\":1,\"1133\":1,\"1155\":1,\"1156\":3,\"1157\":1,\"1170\":2,\"1171\":2,\"1172\":2,\"1173\":2,\"1174\":1,\"1175\":2,\"1208\":1,\"1209\":1,\"1224\":1,\"1225\":1,\"1245\":1,\"1640\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1719\":2,\"1725\":2,\"1729\":2,\"1730\":2,\"1735\":2,\"1747\":1,\"1749\":1,\"1751\":1,\"1759\":1,\"1768\":1,\"1782\":1,\"1785\":2,\"1786\":1,\"1794\":2,\"1806\":1,\"1815\":1,\"1817\":2,\"1843\":1,\"1847\":1,\"1940\":1,\"1941\":1,\"1942\":1,\"1943\":1,\"1944\":1,\"1947\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2021\":1,\"2131\":1,\"2143\":1,\"2355\":1,\"2436\":1,\"2438\":1,\"2440\":1}}],[\"compiled\",{\"1\":{\"161\":1,\"162\":2,\"285\":1,\"286\":1}}],[\"compile\",{\"1\":{\"106\":1,\"161\":3}}],[\"compatability\",{\"0\":{\"2313\":1},\"1\":{\"2313\":1}}],[\"compatiblity\",{\"1\":{\"706\":1}}],[\"compatible\",{\"1\":{\"48\":1,\"248\":1,\"266\":1,\"275\":1,\"284\":1,\"290\":1,\"831\":1}}],[\"compatibility\",{\"1\":{\"248\":1,\"699\":1,\"756\":1,\"773\":1,\"831\":1,\"1155\":4,\"1157\":4,\"1247\":1,\"1526\":1,\"1553\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1,\"2000\":1,\"2001\":1}}],[\"comparing\",{\"1\":{\"235\":1,\"784\":1}}],[\"compared\",{\"1\":{\"43\":1,\"52\":1,\"262\":1,\"286\":1,\"733\":1,\"1270\":1,\"2411\":1}}],[\"compare\",{\"1\":{\"39\":1}}],[\"com\",{\"1\":{\"18\":1,\"66\":1,\"67\":1,\"69\":1,\"92\":2,\"125\":1,\"161\":1,\"162\":1,\"195\":3,\"201\":1,\"213\":1,\"242\":1,\"260\":1,\"268\":1,\"277\":1,\"287\":1,\"289\":1,\"290\":2,\"518\":1,\"536\":2,\"615\":1,\"644\":1,\"650\":1,\"669\":1,\"670\":1,\"705\":1,\"709\":1,\"747\":1,\"750\":1,\"774\":2,\"780\":1,\"790\":1,\"791\":1,\"804\":1,\"864\":1,\"922\":1,\"932\":1,\"934\":1,\"936\":1,\"937\":1,\"1053\":2,\"1153\":1,\"1211\":2,\"1308\":1,\"1316\":1,\"1320\":1,\"1332\":1,\"1350\":1,\"1385\":2,\"1411\":1,\"1513\":1,\"1548\":1,\"1551\":1,\"1592\":1,\"1605\":1,\"1606\":1,\"1608\":1,\"1668\":1,\"1756\":1,\"1757\":1,\"1785\":1,\"1786\":1,\"1789\":1,\"1790\":1,\"1817\":1,\"1818\":1,\"1833\":1,\"1945\":1,\"1977\":1,\"2018\":1,\"2101\":1,\"2151\":1,\"2191\":1,\"2198\":1,\"2208\":1,\"2280\":1,\"2286\":2,\"2310\":1,\"2427\":1,\"2462\":1,\"2474\":1,\"2489\":1}}],[\"comm\",{\"1\":{\"2348\":1,\"2370\":2,\"2372\":1}}],[\"commonpreprocessor\",{\"0\":{\"2336\":1,\"2337\":1},\"1\":{\"2336\":1,\"2337\":2,\"2346\":1,\"2353\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2364\":1}}],[\"commonvoice\",{\"1\":{\"527\":1}}],[\"commonly\",{\"1\":{\"262\":1}}],[\"commoncollatefn\",{\"0\":{\"2335\":1},\"1\":{\"82\":3,\"2335\":1,\"2350\":1}}],[\"common\",{\"0\":{\"170\":1,\"1760\":1,\"1880\":1,\"1893\":1,\"2376\":1},\"1\":{\"50\":1,\"78\":2,\"82\":1,\"138\":1,\"194\":1,\"197\":5,\"211\":1,\"213\":1,\"222\":1,\"226\":1,\"269\":1,\"278\":1,\"1124\":10,\"1125\":10,\"1760\":1,\"1880\":1,\"1893\":1,\"2043\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2184\":1,\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2325\":1,\"2327\":1,\"2335\":1,\"2350\":1,\"2376\":1,\"2377\":1}}],[\"commitment\",{\"1\":{\"1391\":1,\"1403\":1,\"1410\":1,\"1468\":1,\"1469\":1}}],[\"commit\",{\"1\":{\"26\":2,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1466\":1}}],[\"comma\",{\"1\":{\"50\":1,\"51\":1,\"780\":1}}],[\"commas\",{\"1\":{\"24\":1,\"79\":1}}],[\"commandline\",{\"0\":{\"1887\":1},\"1\":{\"1887\":1}}],[\"commands\",{\"0\":{\"403\":1},\"1\":{\"1\":1,\"76\":1,\"126\":1,\"167\":1,\"201\":1,\"243\":1,\"263\":1}}],[\"command\",{\"0\":{\"117\":1},\"1\":{\"1\":1,\"3\":2,\"22\":1,\"23\":1,\"25\":2,\"39\":1,\"40\":1,\"67\":1,\"71\":1,\"79\":2,\"81\":1,\"84\":3,\"85\":1,\"117\":1,\"119\":1,\"162\":5,\"167\":1,\"168\":2,\"197\":2,\"201\":2,\"212\":1,\"224\":1,\"236\":1,\"243\":1,\"263\":1,\"267\":1,\"276\":4,\"286\":2,\"290\":1,\"1887\":1,\"2249\":1}}],[\"comments\",{\"1\":{\"1993\":1}}],[\"comment\",{\"1\":{\"3\":1,\"866\":1,\"1993\":1}}],[\"core\",{\"0\":{\"1400\":1,\"1439\":1,\"1469\":1,\"1473\":1,\"1474\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1495\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1504\":1},\"1\":{\"138\":1,\"260\":1,\"261\":1,\"262\":1,\"1400\":1,\"1439\":1,\"1469\":1,\"1473\":1,\"1474\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1495\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1504\":1,\"1834\":1,\"2151\":1,\"2184\":1,\"2287\":1,\"2310\":1}}],[\"corrupted\",{\"1\":{\"1680\":1}}],[\"corrupt\",{\"0\":{\"1680\":1},\"1\":{\"1680\":1}}],[\"corruption\",{\"1\":{\"3\":1}}],[\"corr\",{\"1\":{\"285\":1}}],[\"corresonding\",{\"1\":{\"1155\":1,\"1157\":1}}],[\"correspond\",{\"1\":{\"96\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"744\":1,\"784\":1,\"828\":1,\"829\":1,\"830\":1}}],[\"corresponds\",{\"1\":{\"79\":1,\"196\":1,\"268\":1,\"277\":1,\"639\":3,\"978\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"1301\":1,\"1306\":1,\"1327\":1,\"1371\":1,\"1372\":1,\"1514\":1,\"2355\":1,\"2436\":1,\"2438\":1,\"2440\":1}}],[\"corresponding\",{\"1\":{\"43\":1,\"97\":1,\"98\":1,\"142\":1,\"146\":1,\"208\":1,\"224\":1,\"225\":1,\"263\":1,\"267\":1,\"269\":1,\"276\":2,\"278\":1,\"692\":1,\"756\":1,\"773\":1,\"824\":1,\"850\":2,\"866\":1,\"867\":1,\"902\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1155\":1,\"1157\":1,\"1316\":1,\"1320\":1,\"1329\":1,\"1719\":2,\"1720\":1,\"1725\":2,\"1788\":1,\"1806\":1,\"1927\":1,\"2000\":1,\"2054\":1,\"2354\":1}}],[\"correlate\",{\"1\":{\"286\":1}}],[\"correlations\",{\"0\":{\"1314\":1},\"1\":{\"1314\":2,\"1317\":1}}],[\"correlation\",{\"1\":{\"285\":1,\"1314\":3,\"1315\":2,\"1317\":6}}],[\"corrected\",{\"1\":{\"1332\":1}}],[\"corrector=\",{\"1\":{\"1253\":1}}],[\"corrector\",{\"0\":{\"1116\":1},\"1\":{\"1050\":2,\"1116\":3,\"1189\":2,\"1218\":3,\"1253\":7,\"1254\":1}}],[\"correctors\",{\"0\":{\"1050\":1,\"1116\":1,\"1189\":1,\"1218\":1},\"1\":{\"1050\":1,\"1116\":1,\"1189\":1,\"1218\":1}}],[\"correctness\",{\"1\":{\"790\":1}}],[\"correction\",{\"0\":{\"928\":1,\"1332\":1},\"1\":{\"821\":1,\"928\":1,\"1332\":2}}],[\"correction=false\",{\"1\":{\"757\":1}}],[\"corrections\",{\"1\":{\"269\":1,\"278\":1}}],[\"correctly\",{\"1\":{\"121\":1,\"266\":1,\"267\":1,\"275\":1,\"276\":1,\"285\":1,\"286\":1}}],[\"correct\",{\"1\":{\"32\":1,\"235\":1,\"243\":2,\"269\":2,\"278\":2,\"286\":1,\"1640\":1,\"2127\":1,\"2355\":2}}],[\"corpus=\",{\"1\":{\"24\":4,\"197\":1}}],[\"corpus\",{\"0\":{\"195\":1,\"515\":1},\"1\":{\"24\":7,\"37\":1,\"68\":1,\"69\":1,\"70\":2,\"71\":4,\"79\":1,\"91\":3,\"96\":1,\"106\":1,\"108\":1,\"110\":2,\"194\":1,\"195\":1,\"196\":1,\"197\":4,\"213\":1,\"219\":1,\"261\":1,\"268\":1,\"277\":1,\"289\":1,\"515\":2,\"1760\":4,\"2249\":1,\"2253\":1}}],[\"corporas\",{\"1\":{\"24\":1}}],[\"corpora4\",{\"1\":{\"22\":1,\"24\":1}}],[\"corpora\",{\"1\":{\"15\":1,\"37\":1,\"109\":1,\"110\":1,\"197\":2}}],[\"cornell\",{\"1\":{\"5\":1,\"11\":1,\"691\":1,\"1210\":1,\"1264\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1334\":1}}],[\"conj\",{\"0\":{\"1317\":1},\"1\":{\"1317\":2,\"1352\":1}}],[\"conjugate\",{\"1\":{\"824\":1,\"1317\":1}}],[\"concate\",{\"1\":{\"1750\":1,\"1993\":2,\"2223\":1,\"2245\":2,\"2431\":2}}],[\"concate=true\",{\"1\":{\"1750\":1,\"2223\":1}}],[\"concatenating\",{\"1\":{\"2183\":1}}],[\"concatenation\",{\"1\":{\"200\":1,\"1883\":1}}],[\"concatenated\",{\"1\":{\"1008\":2,\"1124\":1,\"1390\":1,\"1402\":1,\"1409\":1,\"1467\":1,\"1595\":1,\"2136\":1}}],[\"concatenate\",{\"1\":{\"201\":1,\"554\":1,\"556\":1,\"1071\":1,\"1073\":1,\"1599\":2,\"1750\":1,\"2223\":1,\"2239\":2,\"2240\":2,\"2376\":1,\"2411\":2,\"2412\":2,\"2423\":2,\"2432\":2,\"2447\":2}}],[\"concatadaptlayer\",{\"0\":{\"1086\":1},\"1\":{\"1086\":2}}],[\"concatjson\",{\"0\":{\"556\":1},\"1\":{\"556\":1}}],[\"concat\",{\"0\":{\"554\":1},\"1\":{\"554\":1,\"674\":2,\"692\":3,\"700\":1,\"701\":1,\"709\":4,\"710\":4,\"711\":4,\"731\":1,\"732\":1,\"766\":1,\"767\":1,\"774\":4,\"775\":1,\"780\":7,\"848\":1,\"849\":4,\"850\":1,\"1007\":1,\"1028\":1,\"1107\":4,\"1278\":4,\"1526\":2,\"1598\":2,\"1599\":4,\"1600\":2,\"1735\":4,\"1751\":4,\"1759\":4,\"1992\":5,\"1993\":2,\"1995\":5,\"2126\":1,\"2129\":4,\"2239\":4,\"2240\":4,\"2245\":2,\"2411\":4,\"2412\":4,\"2423\":3,\"2431\":2,\"2432\":4,\"2447\":4}}],[\"concealment\",{\"1\":{\"285\":1}}],[\"cond\",{\"1\":{\"1211\":1}}],[\"condrefineblock\",{\"0\":{\"1093\":1},\"1\":{\"1093\":1}}],[\"condrcublock\",{\"0\":{\"1091\":1},\"1\":{\"1091\":1}}],[\"condmsfblock\",{\"0\":{\"1089\":1},\"1\":{\"1089\":1}}],[\"condcrpblock\",{\"0\":{\"1087\":1},\"1\":{\"1087\":1}}],[\"conducting\",{\"1\":{\"223\":1}}],[\"conducts\",{\"1\":{\"206\":1,\"212\":1,\"218\":1,\"255\":1,\"266\":1,\"267\":1,\"275\":1,\"276\":1,\"286\":1}}],[\"conduct\",{\"1\":{\"160\":1,\"190\":1}}],[\"conditioned\",{\"1\":{\"2355\":2}}],[\"conditioning\",{\"1\":{\"709\":1,\"733\":2,\"734\":1,\"774\":1,\"780\":1,\"1513\":2,\"1519\":2,\"1520\":1,\"1548\":1,\"1551\":2,\"1552\":1,\"1556\":3,\"1581\":1,\"1583\":1,\"1592\":3,\"1599\":1,\"1610\":2,\"1611\":2,\"1612\":1,\"1613\":1,\"1616\":2,\"1626\":1,\"1628\":5,\"1750\":1,\"1758\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"2426\":1,\"2428\":2,\"2431\":1}}],[\"conditional=false\",{\"1\":{\"1324\":1}}],[\"conditional=true\",{\"1\":{\"1211\":1}}],[\"conditionalvariancenorm2d\",{\"0\":{\"1105\":1},\"1\":{\"1105\":1}}],[\"conditionalresidualblock\",{\"0\":{\"1103\":1},\"1\":{\"1103\":1}}],[\"conditionalnonenorm2d\",{\"0\":{\"1101\":1},\"1\":{\"1101\":1}}],[\"conditionalinstancenorm2dplus\",{\"0\":{\"1099\":1},\"1\":{\"1099\":1,\"1103\":1}}],[\"conditionalinstancenorm2d\",{\"0\":{\"1097\":1},\"1\":{\"1097\":1}}],[\"conditionalbatchnorm2d\",{\"0\":{\"1095\":1},\"1\":{\"1095\":1}}],[\"conditional\",{\"1\":{\"240\":1,\"286\":1,\"389\":1,\"1546\":1,\"1553\":1,\"1610\":2,\"1611\":1,\"1612\":1,\"1616\":1,\"1622\":1,\"1625\":1,\"1626\":1,\"1854\":1,\"2001\":1}}],[\"conditions\",{\"1\":{\"150\":1,\"205\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1}}],[\"condition\",{\"1\":{\"150\":1,\"240\":1,\"449\":2,\"1155\":1,\"1157\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1520\":1,\"1943\":1,\"2426\":2,\"2428\":4}}],[\"conda\",{\"1\":{\"31\":10,\"153\":1,\"159\":2,\"162\":11,\"163\":2}}],[\"consumed\",{\"1\":{\"2338\":1,\"2347\":1,\"2369\":1,\"2371\":1}}],[\"consumes\",{\"1\":{\"173\":1,\"1000\":1}}],[\"consecutive\",{\"1\":{\"262\":1}}],[\"conservative\",{\"1\":{\"70\":1}}],[\"consistency\",{\"1\":{\"1155\":3,\"1157\":4,\"1397\":1}}],[\"consistent\",{\"1\":{\"224\":2,\"235\":1,\"243\":2,\"2139\":1}}],[\"consisting\",{\"1\":{\"262\":1}}],[\"consists\",{\"1\":{\"200\":1,\"205\":1,\"211\":1,\"217\":1,\"235\":1,\"242\":1,\"254\":1,\"266\":1,\"275\":1,\"285\":1,\"768\":1,\"927\":1,\"1390\":1,\"1397\":1,\"1402\":1,\"1409\":1,\"1467\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1606\":1,\"2353\":1,\"2364\":1}}],[\"considerably\",{\"1\":{\"1301\":1,\"1372\":1}}],[\"consider\",{\"1\":{\"290\":2,\"1334\":2}}],[\"considered\",{\"1\":{\"98\":1}}],[\"considering\",{\"1\":{\"95\":1,\"287\":2,\"819\":1,\"1919\":1}}],[\"const\",{\"1\":{\"1672\":2}}],[\"constantbatchsampler\",{\"1\":{\"2377\":2}}],[\"constants\",{\"0\":{\"801\":1,\"896\":1,\"940\":1,\"946\":1},\"1\":{\"801\":1,\"896\":1,\"940\":1,\"946\":1}}],[\"constant\",{\"1\":{\"96\":1,\"97\":1,\"175\":1,\"809\":2,\"1306\":1,\"1354\":1,\"1371\":1,\"1672\":2,\"1683\":1,\"1725\":2,\"1806\":1,\"2006\":1,\"2016\":1,\"2018\":2,\"2377\":1}}],[\"constraining\",{\"1\":{\"1710\":1,\"1768\":1}}],[\"constraint=false\",{\"1\":{\"1750\":1,\"2224\":1}}],[\"constraint=true\",{\"1\":{\"290\":1}}],[\"constraint\",{\"1\":{\"290\":1,\"406\":2,\"484\":2,\"490\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1717\":1,\"1750\":4,\"1768\":2,\"1976\":1,\"1993\":5,\"2224\":4,\"2245\":5,\"2431\":5}}],[\"constraints\",{\"1\":{\"139\":1}}],[\"constrained\",{\"1\":{\"45\":3,\"139\":1,\"696\":1,\"697\":1,\"1319\":1,\"1327\":1,\"1330\":1}}],[\"constructs\",{\"1\":{\"2001\":1,\"2131\":1}}],[\"constructed\",{\"1\":{\"2000\":1,\"2001\":1}}],[\"constructing\",{\"1\":{\"1354\":1}}],[\"construction\",{\"1\":{\"101\":1,\"260\":1,\"290\":1,\"1224\":1,\"1225\":1,\"2131\":1,\"2143\":1}}],[\"constructor\",{\"1\":{\"150\":3,\"819\":1,\"912\":2,\"1051\":1}}],[\"construct\",{\"1\":{\"79\":1,\"615\":1,\"616\":1,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"630\":1,\"632\":1,\"633\":1,\"634\":1,\"636\":1,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"651\":1,\"653\":1,\"740\":1,\"784\":1,\"1051\":1,\"1224\":1,\"1225\":1,\"1245\":1,\"1368\":3,\"1730\":1,\"1735\":1,\"1736\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1749\":1,\"1751\":1,\"1756\":1,\"1757\":1,\"1759\":1,\"1760\":1,\"1782\":1,\"1783\":1,\"1785\":1,\"1789\":1,\"1790\":1,\"1794\":1,\"1808\":1,\"1809\":1,\"1817\":1,\"1818\":1,\"1837\":1,\"1838\":1,\"1842\":1,\"1847\":1,\"1851\":1}}],[\"connected\",{\"1\":{\"1124\":1,\"1125\":1,\"1147\":1,\"1165\":1,\"1331\":1,\"2184\":1,\"2405\":1}}],[\"connect\",{\"1\":{\"66\":2,\"1628\":2}}],[\"connectionist\",{\"1\":{\"1805\":1}}],[\"connection\",{\"1\":{\"66\":2,\"248\":2,\"689\":1,\"718\":1,\"809\":1,\"1086\":2,\"1207\":2,\"1268\":1,\"1273\":1,\"1274\":3,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1458\":2,\"1460\":2,\"1628\":1,\"1854\":1,\"2240\":1}}],[\"connections\",{\"1\":{\"43\":2,\"141\":1,\"620\":1,\"1076\":1,\"1368\":3,\"1736\":1}}],[\"connecting\",{\"1\":{\"39\":1}}],[\"conti\",{\"1\":{\"2130\":2,\"2133\":2,\"2136\":2,\"2137\":2}}],[\"contiguous\",{\"0\":{\"874\":1},\"1\":{\"874\":1,\"941\":1}}],[\"continues\",{\"1\":{\"704\":1}}],[\"continue\",{\"1\":{\"285\":1,\"2065\":2,\"2134\":1}}],[\"continuousaudioio\",{\"0\":{\"2133\":1},\"1\":{\"2133\":2}}],[\"continuous\",{\"1\":{\"273\":1,\"289\":2,\"831\":1,\"1035\":1,\"1113\":1,\"1251\":1,\"1395\":3,\"1934\":1,\"2130\":10,\"2133\":7,\"2136\":1,\"2138\":1,\"2143\":3,\"2404\":1}}],[\"continuing\",{\"1\":{\"127\":1,\"2065\":1}}],[\"contexutal\",{\"1\":{\"1735\":9}}],[\"contexts\",{\"1\":{\"2480\":1}}],[\"contextmanager\",{\"1\":{\"2359\":1}}],[\"context=0\",{\"1\":{\"1853\":1}}],[\"contextualblockencoderlayer\",{\"0\":{\"1735\":1},\"1\":{\"1735\":1}}],[\"contextualblocktransformerencoder\",{\"0\":{\"711\":1},\"1\":{\"711\":1}}],[\"contextualblockconformerencoder\",{\"0\":{\"710\":1},\"1\":{\"710\":1}}],[\"contextual\",{\"0\":{\"710\":1,\"711\":1,\"1735\":1},\"1\":{\"130\":3,\"710\":3,\"711\":4,\"1735\":1}}],[\"context\",{\"1\":{\"43\":2,\"130\":2,\"142\":3,\"147\":2,\"148\":1,\"321\":2,\"617\":4,\"618\":4,\"620\":4,\"624\":4,\"626\":4,\"633\":1,\"634\":1,\"636\":4,\"642\":3,\"643\":3,\"644\":9,\"645\":4,\"649\":3,\"661\":1,\"668\":3,\"669\":1,\"710\":2,\"711\":2,\"756\":7,\"773\":7,\"784\":1,\"796\":1,\"866\":2,\"867\":2,\"1162\":3,\"1164\":10,\"1185\":1,\"1334\":3,\"1582\":3,\"1610\":3,\"1794\":1,\"2298\":1}}],[\"content>\",{\"1\":{\"2144\":2}}],[\"content\",{\"1\":{\"286\":1,\"2039\":2,\"2049\":1,\"2143\":2}}],[\"contents\",{\"0\":{\"199\":1,\"204\":1,\"210\":1,\"216\":1,\"227\":1,\"234\":1,\"241\":1,\"253\":1,\"265\":1,\"274\":1,\"284\":1},\"1\":{\"196\":1,\"197\":1,\"222\":1,\"234\":1,\"240\":1,\"260\":1,\"268\":1,\"274\":1,\"277\":1,\"284\":1,\"2039\":1}}],[\"contentsgenerated\",{\"1\":{\"194\":1}}],[\"contrib\",{\"1\":{\"1350\":1}}],[\"contributing\",{\"1\":{\"225\":1}}],[\"contribution\",{\"1\":{\"44\":1,\"144\":1}}],[\"contribute\",{\"1\":{\"144\":1}}],[\"contrast\",{\"0\":{\"1679\":1},\"1\":{\"1679\":3}}],[\"contract\",{\"0\":{\"1299\":1},\"1\":{\"1299\":1}}],[\"contrary\",{\"1\":{\"79\":1,\"142\":1}}],[\"controllable\",{\"1\":{\"1752\":1,\"1788\":1,\"1795\":1,\"2411\":1}}],[\"controlled\",{\"1\":{\"139\":1,\"141\":1,\"2249\":1,\"2253\":1,\"2307\":1}}],[\"controls\",{\"1\":{\"821\":1,\"829\":1,\"1202\":1,\"1259\":1,\"1261\":1,\"1645\":1,\"1650\":1,\"1679\":1,\"2000\":3,\"2001\":1}}],[\"controlable\",{\"1\":{\"142\":1}}],[\"control\",{\"1\":{\"43\":2,\"45\":2,\"50\":1,\"141\":1,\"145\":2,\"147\":2,\"148\":1,\"287\":2,\"484\":2,\"490\":2,\"821\":1,\"846\":1,\"1526\":1,\"1533\":1,\"1552\":1,\"1553\":1,\"1625\":1,\"1626\":1,\"1768\":2,\"1770\":1,\"1771\":2,\"1788\":1,\"2001\":1,\"2231\":1,\"2240\":1,\"2245\":1,\"2298\":1,\"2307\":1,\"2355\":2,\"2411\":1,\"2412\":1,\"2423\":1,\"2425\":1,\"2429\":1,\"2430\":1,\"2447\":1}}],[\"contain\",{\"1\":{\"201\":1,\"224\":2,\"242\":4,\"777\":1,\"881\":1,\"884\":1,\"1061\":1,\"1063\":1,\"1156\":1,\"1940\":1,\"1942\":1,\"2155\":1,\"2215\":1,\"2480\":1}}],[\"containing\",{\"1\":{\"37\":1,\"142\":1,\"200\":1,\"243\":1,\"692\":1,\"699\":1,\"760\":1,\"775\":1,\"786\":4,\"790\":1,\"800\":4,\"820\":1,\"850\":1,\"867\":4,\"921\":4,\"935\":4,\"993\":1,\"1350\":1,\"1354\":1,\"1515\":1,\"1516\":1,\"1892\":1,\"1901\":2,\"1903\":2,\"2040\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2130\":1,\"2131\":1,\"2134\":1,\"2139\":1,\"2142\":1,\"2183\":1,\"2190\":1,\"2208\":1}}],[\"contains\",{\"1\":{\"26\":1,\"242\":4,\"243\":1,\"267\":1,\"276\":1,\"284\":1,\"286\":1,\"290\":6,\"625\":2,\"755\":2,\"785\":2,\"878\":3,\"879\":3,\"881\":5,\"882\":3,\"883\":3,\"884\":5,\"1155\":1,\"1157\":1,\"1546\":1,\"1622\":1,\"1626\":1,\"1725\":1,\"1949\":1,\"2132\":1,\"2219\":1,\"2355\":2}}],[\"container\",{\"0\":{\"25\":1},\"1\":{\"18\":2,\"19\":6,\"22\":3,\"26\":3,\"27\":1,\"1029\":1,\"1185\":1,\"1235\":1,\"1257\":1}}],[\"containers\",{\"0\":{\"19\":1,\"24\":1},\"1\":{\"18\":3,\"19\":2,\"22\":1,\"25\":1,\"26\":1,\"27\":2}}],[\"convgenerator\",{\"0\":{\"2460\":1},\"1\":{\"2460\":1}}],[\"convglu\",{\"1\":{\"1180\":1}}],[\"convdiscriminator\",{\"0\":{\"2458\":1},\"1\":{\"2458\":1}}],[\"convdecoder\",{\"0\":{\"1112\":1},\"1\":{\"1112\":1}}],[\"convinupsamplenetwork\",{\"0\":{\"1582\":1},\"1\":{\"1582\":2,\"1610\":1}}],[\"convinput\",{\"0\":{\"621\":1,\"665\":1},\"1\":{\"621\":5,\"660\":2,\"665\":1}}],[\"convflow\",{\"0\":{\"1581\":1},\"1\":{\"1581\":2}}],[\"convrelunorm\",{\"0\":{\"1517\":1},\"1\":{\"1517\":1}}],[\"convs=true\",{\"1\":{\"1548\":1}}],[\"convs\",{\"1\":{\"1513\":2,\"1526\":1,\"1548\":1,\"1551\":2,\"1592\":2,\"1596\":1,\"1598\":1,\"1599\":2,\"1600\":1,\"1614\":2,\"2202\":1}}],[\"convlayernorm\",{\"0\":{\"1387\":1},\"1\":{\"1387\":1}}],[\"convlayerblock\",{\"0\":{\"712\":1},\"1\":{\"712\":2}}],[\"conv=true\",{\"1\":{\"1211\":1}}],[\"conv=false\",{\"1\":{\"675\":1,\"1151\":1,\"1284\":1,\"1548\":1}}],[\"convtranspose1d\",{\"1\":{\"1428\":1,\"1446\":1}}],[\"convtranspose2d\",{\"1\":{\"1181\":5,\"1430\":1,\"1448\":1}}],[\"convtransp2d\",{\"0\":{\"1302\":1},\"1\":{\"1302\":1}}],[\"convtasnet\",{\"1\":{\"1053\":1}}],[\"convmeanpool\",{\"0\":{\"1114\":1},\"1\":{\"1114\":1}}],[\"conv1x1\",{\"0\":{\"1303\":1,\"1346\":1},\"1\":{\"978\":1,\"1303\":1,\"1346\":1}}],[\"conv1dsubsampling3\",{\"0\":{\"1740\":1},\"1\":{\"1740\":2}}],[\"conv1dsubsampling2\",{\"0\":{\"1739\":1},\"1\":{\"1739\":2}}],[\"conv1dsubsampling1\",{\"0\":{\"1738\":1},\"1\":{\"1738\":2}}],[\"conv1dlinear\",{\"0\":{\"1737\":1},\"1\":{\"1735\":1,\"1737\":2,\"1751\":1,\"1759\":1}}],[\"conv1d1x1\",{\"0\":{\"1579\":1},\"1\":{\"1579\":1}}],[\"conv1d\",{\"0\":{\"620\":2,\"658\":1,\"1484\":1,\"1578\":1,\"1736\":2,\"1865\":1},\"1\":{\"43\":9,\"141\":4,\"620\":9,\"658\":5,\"709\":3,\"710\":2,\"711\":2,\"774\":3,\"780\":3,\"849\":2,\"1107\":3,\"1252\":1,\"1278\":3,\"1280\":1,\"1283\":1,\"1424\":1,\"1442\":1,\"1444\":1,\"1484\":1,\"1515\":1,\"1516\":1,\"1519\":1,\"1526\":1,\"1535\":1,\"1536\":2,\"1546\":1,\"1552\":3,\"1553\":1,\"1578\":4,\"1579\":3,\"1598\":1,\"1599\":1,\"1600\":1,\"1622\":1,\"1625\":1,\"1626\":3,\"1668\":3,\"1736\":3,\"1737\":2,\"1795\":3,\"1865\":5,\"2129\":2,\"2191\":3,\"2239\":2,\"2240\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2432\":1,\"2447\":1}}],[\"conv3x3\",{\"0\":{\"885\":1,\"1304\":1,\"1347\":1},\"1\":{\"885\":1,\"1304\":1,\"1347\":1}}],[\"convet\",{\"1\":{\"570\":1}}],[\"convencoderlayer\",{\"1\":{\"1736\":1}}],[\"convencoder\",{\"0\":{\"1113\":1},\"1\":{\"1113\":1}}],[\"convenience\",{\"1\":{\"269\":1,\"278\":1}}],[\"convenient\",{\"1\":{\"205\":1,\"2006\":1}}],[\"convention\",{\"1\":{\"224\":1}}],[\"conventional\",{\"1\":{\"203\":1,\"720\":1,\"1782\":1,\"1980\":1,\"1982\":1,\"2416\":1,\"2418\":1}}],[\"convergence\",{\"1\":{\"261\":1,\"262\":3}}],[\"conversation\",{\"1\":{\"2044\":1,\"2049\":1}}],[\"conversion\",{\"1\":{\"182\":1,\"247\":1,\"1533\":1,\"1558\":1,\"1811\":1,\"2482\":1}}],[\"conversely\",{\"1\":{\"76\":1}}],[\"converter\",{\"0\":{\"298\":1,\"307\":1,\"314\":1,\"320\":1,\"326\":1,\"395\":1,\"401\":1,\"414\":1,\"418\":1,\"427\":1,\"435\":1,\"448\":1,\"468\":1,\"474\":1,\"504\":1,\"510\":1,\"1053\":1,\"2278\":1,\"2283\":1,\"2291\":1},\"1\":{\"295\":1,\"415\":1,\"1053\":1,\"2278\":1,\"2283\":1,\"2291\":1}}],[\"converted\",{\"1\":{\"168\":2,\"205\":1,\"266\":1,\"275\":1,\"285\":1,\"759\":1,\"942\":1,\"1932\":1,\"2040\":1,\"2045\":1,\"2101\":1,\"2136\":1}}],[\"converting\",{\"0\":{\"68\":1},\"1\":{\"3\":1,\"70\":1,\"247\":1}}],[\"convert\",{\"0\":{\"75\":1,\"516\":1,\"558\":1},\"1\":{\"3\":1,\"68\":2,\"71\":1,\"182\":1,\"269\":1,\"278\":1,\"286\":2,\"356\":2,\"516\":2,\"536\":1,\"558\":2,\"567\":1,\"578\":1,\"581\":1,\"583\":1,\"586\":1,\"589\":1,\"594\":1,\"598\":1,\"603\":1,\"609\":1,\"611\":1,\"627\":2,\"740\":2,\"942\":1,\"943\":1,\"962\":1,\"1053\":1,\"1063\":2,\"1128\":1,\"1155\":1,\"1157\":1,\"1654\":3,\"1662\":1,\"1666\":3,\"1719\":1,\"1803\":1,\"1807\":1,\"1861\":1,\"1925\":1,\"2124\":1,\"2128\":1,\"2138\":1,\"2143\":2,\"2287\":2,\"2376\":1,\"2407\":1,\"2432\":1,\"2490\":1,\"2495\":1}}],[\"converts\",{\"1\":{\"2\":1,\"1155\":3,\"1157\":3,\"1758\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"2040\":1,\"2045\":1,\"2101\":1,\"2130\":1,\"2131\":1,\"2143\":1,\"2227\":1,\"2231\":1,\"2431\":1,\"2435\":1}}],[\"convoluational\",{\"1\":{\"1748\":1}}],[\"convolutive\",{\"1\":{\"1066\":1}}],[\"convolutionmodule\",{\"0\":{\"1747\":1},\"1\":{\"1747\":3}}],[\"convolutions\",{\"1\":{\"768\":7,\"831\":1,\"1392\":1,\"1397\":1,\"1422\":1,\"1450\":2,\"1452\":2,\"1454\":1,\"1456\":1,\"1458\":3,\"1460\":3,\"1668\":2}}],[\"convolutionalpositionalembedding\",{\"0\":{\"1748\":1},\"1\":{\"1748\":1}}],[\"convolutionalgatingmlp\",{\"0\":{\"713\":1},\"1\":{\"701\":1,\"713\":1,\"735\":1}}],[\"convolutionalspationgatingunit\",{\"1\":{\"622\":1}}],[\"convolutionalspatialgatingunit\",{\"0\":{\"622\":1,\"715\":1},\"1\":{\"141\":6,\"622\":4,\"624\":1,\"715\":1}}],[\"convolutional\",{\"1\":{\"43\":2,\"622\":1,\"691\":1,\"702\":1,\"713\":1,\"715\":1,\"768\":1,\"781\":1,\"783\":1,\"846\":3,\"927\":1,\"982\":3,\"1112\":1,\"1113\":1,\"1124\":1,\"1125\":1,\"1147\":1,\"1273\":3,\"1274\":3,\"1309\":2,\"1311\":2,\"1397\":2,\"1462\":1,\"1519\":2,\"1520\":1,\"1524\":1,\"1525\":1,\"1536\":1,\"1556\":3,\"1581\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1748\":1,\"1753\":3,\"1770\":1,\"1863\":1,\"1864\":1,\"2433\":3,\"2458\":1,\"2460\":1}}],[\"convolution\",{\"0\":{\"619\":1,\"622\":1,\"623\":1,\"1747\":1},\"1\":{\"43\":5,\"141\":6,\"147\":1,\"617\":2,\"618\":2,\"619\":3,\"620\":4,\"621\":1,\"622\":3,\"623\":4,\"624\":1,\"661\":2,\"665\":3,\"709\":2,\"712\":1,\"768\":4,\"774\":2,\"780\":3,\"817\":1,\"821\":2,\"823\":1,\"824\":1,\"846\":3,\"978\":1,\"980\":2,\"1107\":2,\"1118\":1,\"1145\":2,\"1168\":1,\"1265\":2,\"1267\":2,\"1268\":1,\"1301\":1,\"1303\":1,\"1304\":1,\"1331\":1,\"1346\":1,\"1347\":1,\"1372\":1,\"1387\":1,\"1450\":6,\"1452\":6,\"1454\":5,\"1456\":5,\"1458\":3,\"1460\":3,\"1484\":2,\"1513\":1,\"1530\":1,\"1535\":2,\"1536\":2,\"1546\":1,\"1548\":1,\"1551\":1,\"1552\":1,\"1582\":1,\"1592\":1,\"1596\":1,\"1597\":1,\"1599\":1,\"1604\":2,\"1605\":2,\"1606\":2,\"1610\":1,\"1614\":4,\"1615\":4,\"1619\":1,\"1622\":1,\"1626\":2,\"1628\":1,\"1668\":5,\"1706\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1715\":2,\"1716\":2,\"1733\":1,\"1736\":4,\"1747\":2,\"1756\":5,\"1757\":5,\"1768\":2,\"1789\":5,\"1790\":5,\"1854\":1,\"1895\":2,\"2191\":2,\"2198\":1,\"2432\":3}}],[\"convolve\",{\"0\":{\"1562\":1},\"1\":{\"1562\":1,\"1813\":1}}],[\"convolving\",{\"1\":{\"141\":5,\"619\":1,\"620\":1,\"622\":1,\"623\":1,\"1736\":1}}],[\"conv2ds\",{\"1\":{\"1386\":1}}],[\"conv2dsubsamplingwoposenc\",{\"0\":{\"1746\":1},\"1\":{\"1746\":2}}],[\"conv2dsubsampling8\",{\"0\":{\"1745\":1},\"1\":{\"1745\":2}}],[\"conv2dsubsampling6\",{\"0\":{\"1744\":1},\"1\":{\"1744\":2}}],[\"conv2dsubsampling2\",{\"0\":{\"1743\":1},\"1\":{\"1743\":2}}],[\"conv2dsubsampling1\",{\"0\":{\"1742\":1},\"1\":{\"1742\":2}}],[\"conv2dsubsampling\",{\"0\":{\"1741\":1},\"1\":{\"663\":1,\"1741\":2,\"1742\":1,\"1863\":1,\"1866\":1}}],[\"conv2dactnorm\",{\"0\":{\"1108\":1},\"1\":{\"1108\":1}}],[\"conv2d8\",{\"1\":{\"733\":1}}],[\"conv2d\",{\"0\":{\"1110\":1,\"1300\":1,\"1580\":1,\"1757\":1},\"1\":{\"141\":4,\"243\":2,\"665\":1,\"700\":1,\"709\":1,\"710\":1,\"711\":1,\"734\":1,\"771\":1,\"774\":1,\"780\":1,\"849\":1,\"927\":1,\"1108\":1,\"1110\":2,\"1145\":2,\"1180\":5,\"1271\":1,\"1300\":1,\"1301\":1,\"1372\":1,\"1385\":1,\"1392\":3,\"1426\":1,\"1580\":4,\"1742\":1,\"1757\":1}}],[\"conv2d2\",{\"1\":{\"128\":1,\"2191\":1}}],[\"conv\",{\"0\":{\"621\":1,\"1112\":1,\"1113\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1349\":1,\"1372\":1,\"1426\":1,\"1654\":1,\"1661\":1,\"1666\":1,\"1668\":1,\"1737\":1,\"1756\":1,\"1795\":1,\"2458\":1,\"2460\":1,\"2465\":1,\"2467\":1},\"1\":{\"43\":4,\"141\":23,\"223\":3,\"225\":2,\"243\":3,\"617\":2,\"618\":2,\"619\":2,\"621\":3,\"622\":1,\"623\":1,\"624\":4,\"661\":4,\"665\":2,\"674\":8,\"691\":1,\"700\":2,\"702\":1,\"709\":2,\"710\":2,\"711\":2,\"712\":1,\"713\":1,\"715\":1,\"731\":3,\"732\":3,\"733\":3,\"734\":3,\"735\":3,\"766\":3,\"767\":3,\"774\":2,\"780\":6,\"781\":2,\"783\":2,\"796\":2,\"846\":8,\"849\":2,\"869\":2,\"978\":3,\"982\":1,\"1107\":2,\"1112\":1,\"1113\":1,\"1124\":7,\"1125\":7,\"1147\":4,\"1240\":1,\"1242\":1,\"1264\":1,\"1273\":2,\"1274\":2,\"1278\":2,\"1300\":1,\"1301\":1,\"1302\":1,\"1334\":1,\"1349\":1,\"1372\":1,\"1424\":1,\"1426\":2,\"1428\":1,\"1430\":1,\"1452\":2,\"1456\":1,\"1460\":1,\"1484\":1,\"1513\":3,\"1519\":5,\"1526\":1,\"1535\":4,\"1536\":4,\"1546\":6,\"1548\":4,\"1551\":3,\"1552\":6,\"1553\":2,\"1582\":2,\"1583\":1,\"1592\":3,\"1596\":5,\"1597\":4,\"1598\":5,\"1599\":16,\"1600\":5,\"1604\":2,\"1605\":4,\"1606\":2,\"1609\":6,\"1610\":6,\"1611\":1,\"1616\":4,\"1619\":4,\"1620\":1,\"1621\":1,\"1622\":6,\"1625\":3,\"1626\":9,\"1628\":13,\"1654\":1,\"1661\":1,\"1666\":1,\"1668\":1,\"1737\":2,\"1747\":2,\"1748\":2,\"1756\":1,\"1795\":1,\"1863\":3,\"1864\":2,\"1896\":1,\"1963\":1,\"1993\":2,\"2126\":1,\"2129\":2,\"2191\":2,\"2235\":9,\"2236\":9,\"2239\":3,\"2240\":1,\"2245\":17,\"2411\":13,\"2412\":13,\"2423\":13,\"2425\":12,\"2429\":12,\"2431\":17,\"2432\":21,\"2447\":1,\"2458\":5,\"2460\":4,\"2465\":1,\"2467\":1}}],[\"conf=\",{\"1\":{\"2479\":4}}],[\"conf=none\",{\"1\":{\"1720\":1}}],[\"confidently\",{\"1\":{\"1934\":1}}],[\"configs\",{\"1\":{\"267\":1,\"272\":1,\"282\":1,\"286\":3,\"289\":2}}],[\"config=conf\",{\"1\":{\"243\":2}}],[\"configurable\",{\"1\":{\"793\":2,\"2134\":1}}],[\"configurations\",{\"1\":{\"47\":1,\"135\":1,\"141\":3,\"160\":1,\"175\":1,\"267\":1,\"276\":1,\"2045\":1,\"2142\":1}}],[\"configuration\",{\"0\":{\"47\":1,\"83\":1,\"85\":1,\"86\":1,\"110\":1,\"119\":1,\"168\":1,\"297\":1,\"304\":1,\"312\":1,\"318\":1,\"324\":1,\"330\":1,\"334\":1,\"338\":1,\"346\":1,\"353\":1,\"365\":1,\"371\":1,\"380\":1,\"388\":1,\"392\":1,\"399\":1,\"409\":1,\"417\":1,\"424\":1,\"432\":1,\"439\":1,\"445\":1,\"452\":1,\"460\":1,\"466\":1,\"472\":1,\"478\":1,\"487\":1,\"493\":1,\"501\":1,\"508\":1},\"1\":{\"3\":2,\"22\":1,\"37\":1,\"70\":1,\"84\":2,\"85\":3,\"109\":2,\"119\":3,\"128\":2,\"130\":1,\"141\":5,\"142\":1,\"143\":1,\"148\":1,\"152\":1,\"166\":2,\"168\":2,\"200\":3,\"205\":3,\"211\":1,\"217\":3,\"222\":1,\"223\":2,\"224\":4,\"235\":2,\"236\":2,\"242\":3,\"254\":1,\"266\":2,\"267\":1,\"275\":2,\"276\":2,\"285\":2,\"286\":1,\"527\":2,\"536\":1,\"626\":3,\"639\":1,\"655\":3,\"656\":3,\"657\":3,\"658\":3,\"659\":3,\"660\":3,\"662\":3,\"671\":2,\"672\":3,\"673\":4,\"674\":1,\"760\":2,\"768\":1,\"846\":2,\"1163\":1,\"1164\":1,\"1333\":1,\"1681\":1,\"2001\":1,\"2131\":3,\"2142\":1,\"2216\":1,\"2249\":1,\"2334\":1,\"2355\":3}}],[\"configured\",{\"1\":{\"147\":1,\"819\":1}}],[\"configure\",{\"0\":{\"1964\":1},\"1\":{\"86\":1,\"140\":1,\"161\":3,\"225\":1,\"247\":2,\"821\":1,\"1964\":1,\"2355\":2}}],[\"configargparse\",{\"1\":{\"85\":1,\"2474\":1}}],[\"config3\",{\"1\":{\"47\":1}}],[\"config2\",{\"1\":{\"47\":2}}],[\"config\",{\"0\":{\"930\":1,\"2041\":2,\"2074\":1,\"2120\":1,\"2474\":1},\"1\":{\"42\":1,\"43\":1,\"44\":1,\"45\":1,\"47\":7,\"48\":2,\"49\":1,\"84\":5,\"85\":3,\"119\":1,\"140\":1,\"145\":1,\"195\":1,\"200\":5,\"201\":3,\"205\":2,\"211\":1,\"217\":3,\"218\":2,\"219\":2,\"223\":3,\"224\":1,\"235\":1,\"242\":3,\"243\":19,\"254\":1,\"259\":1,\"261\":5,\"262\":4,\"266\":2,\"267\":29,\"272\":2,\"275\":2,\"276\":18,\"282\":2,\"285\":2,\"286\":38,\"289\":2,\"290\":3,\"295\":4,\"301\":8,\"309\":4,\"315\":8,\"321\":8,\"327\":4,\"331\":4,\"335\":4,\"342\":6,\"349\":6,\"356\":2,\"361\":6,\"368\":4,\"377\":4,\"385\":4,\"389\":6,\"396\":8,\"403\":34,\"404\":6,\"406\":6,\"415\":4,\"421\":8,\"429\":8,\"436\":4,\"442\":8,\"449\":4,\"457\":4,\"463\":12,\"469\":8,\"475\":6,\"484\":6,\"490\":6,\"496\":2,\"498\":8,\"505\":10,\"527\":2,\"699\":10,\"760\":2,\"820\":4,\"821\":2,\"828\":4,\"830\":4,\"846\":2,\"912\":3,\"930\":4,\"1209\":1,\"1228\":1,\"1312\":2,\"1324\":2,\"1509\":2,\"1511\":2,\"1521\":1,\"1553\":2,\"1721\":5,\"1965\":1,\"2131\":2,\"2142\":2,\"2228\":1,\"2229\":1,\"2249\":6,\"2263\":1,\"2268\":1,\"2270\":1,\"2271\":1,\"2338\":1,\"2339\":2,\"2355\":5,\"2408\":1,\"2422\":1,\"2446\":1,\"2474\":4}}],[\"confusing\",{\"1\":{\"1881\":1}}],[\"confusion\",{\"1\":{\"84\":1}}],[\"conffile=none\",{\"1\":{\"1845\":1}}],[\"conference\",{\"1\":{\"202\":1,\"207\":1}}],[\"conflr0\",{\"1\":{\"136\":3}}],[\"conformerpostencoder\",{\"0\":{\"2126\":1},\"1\":{\"2126\":1}}],[\"conformer7\",{\"1\":{\"2043\":2}}],[\"conformerseparator\",{\"0\":{\"1107\":1},\"1\":{\"1107\":1}}],[\"conformerencoder\",{\"0\":{\"709\":1},\"1\":{\"709\":1,\"774\":1}}],[\"conformerconvolution\",{\"0\":{\"619\":1},\"1\":{\"141\":3,\"619\":7}}],[\"conformer\",{\"0\":{\"618\":2,\"657\":1,\"709\":1,\"710\":1,\"1107\":1,\"1747\":1,\"1838\":1,\"1864\":1,\"2126\":1,\"2191\":1},\"1\":{\"43\":4,\"52\":2,\"129\":1,\"130\":2,\"136\":3,\"141\":7,\"223\":1,\"286\":13,\"289\":1,\"290\":1,\"618\":8,\"636\":2,\"657\":5,\"661\":1,\"709\":2,\"710\":2,\"774\":1,\"1107\":14,\"1519\":6,\"1526\":8,\"1535\":6,\"1536\":6,\"1546\":7,\"1552\":20,\"1553\":2,\"1598\":8,\"1599\":25,\"1600\":10,\"1622\":7,\"1625\":2,\"1626\":21,\"1747\":2,\"1838\":1,\"1864\":4,\"1994\":8,\"2126\":1,\"2191\":5,\"2239\":12,\"2240\":10,\"2411\":25,\"2412\":25,\"2423\":24,\"2447\":25}}],[\"conf\",{\"1\":{\"37\":1,\"47\":12,\"49\":1,\"84\":2,\"85\":3,\"86\":6,\"109\":1,\"119\":3,\"128\":3,\"130\":1,\"139\":6,\"141\":11,\"142\":4,\"143\":3,\"144\":2,\"147\":2,\"150\":1,\"166\":6,\"168\":2,\"175\":3,\"195\":1,\"201\":3,\"206\":2,\"212\":2,\"218\":3,\"224\":5,\"227\":1,\"236\":1,\"243\":9,\"255\":2,\"259\":1,\"267\":25,\"272\":1,\"276\":14,\"282\":1,\"285\":1,\"286\":36,\"289\":3,\"290\":1,\"301\":4,\"377\":2,\"442\":2,\"449\":2,\"463\":2,\"469\":2,\"521\":1,\"526\":6,\"527\":1,\"561\":2,\"567\":2,\"626\":6,\"639\":2,\"671\":4,\"673\":2,\"720\":1,\"738\":3,\"791\":1,\"796\":1,\"815\":1,\"930\":2,\"1401\":1,\"1403\":1,\"1539\":1,\"1681\":2,\"1964\":2,\"2216\":1,\"2334\":3,\"2356\":1,\"2479\":7}}],[\"cisdrloss\",{\"0\":{\"1066\":1},\"1\":{\"1066\":1}}],[\"ciao\",{\"1\":{\"287\":1}}],[\"circumventing\",{\"1\":{\"263\":1}}],[\"citation\",{\"1\":{\"1126\":1}}],[\"citations\",{\"0\":{\"4\":1},\"1\":{\"156\":1}}],[\"cite\",{\"0\":{\"156\":1},\"1\":{\"156\":1}}],[\"ci\",{\"1\":{\"1\":1,\"2\":1,\"32\":2,\"33\":1,\"34\":1,\"152\":1,\"160\":2,\"224\":1,\"225\":1,\"1066\":1}}],[\"pw\",{\"1\":{\"1519\":4,\"1864\":4,\"1867\":4}}],[\"pwd\",{\"1\":{\"60\":1,\"62\":1,\"63\":1,\"64\":1}}],[\"pqmf\",{\"0\":{\"1608\":2,\"1631\":1},\"1\":{\"1509\":2,\"1511\":2,\"1515\":1,\"1526\":6,\"1553\":2,\"1600\":6,\"1608\":6,\"1618\":3,\"1631\":2}}],[\"pc\",{\"1\":{\"1253\":2,\"1254\":1}}],[\"pcm\",{\"1\":{\"68\":1,\"70\":1,\"71\":3,\"74\":2,\"1678\":6,\"1826\":1,\"1829\":1,\"1883\":1,\"2065\":1,\"2101\":2}}],[\"p1\",{\"1\":{\"916\":1}}],[\"pdf`\",{\"1\":{\"2223\":1}}],[\"pdfs\",{\"1\":{\"2168\":1}}],[\"pdf\",{\"1\":{\"616\":2,\"617\":2,\"624\":2,\"634\":2,\"640\":2,\"643\":2,\"648\":2,\"696\":6,\"697\":6,\"785\":1,\"786\":1,\"866\":3,\"882\":1,\"883\":1,\"884\":1,\"921\":3,\"922\":3,\"974\":6,\"1002\":1,\"1125\":1,\"1126\":1,\"1385\":2,\"1439\":2,\"1530\":2,\"1541\":2,\"1683\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1717\":1,\"1784\":4,\"1994\":2,\"2000\":1,\"2001\":1,\"2016\":2,\"2167\":1,\"2168\":1,\"2176\":3,\"2183\":1,\"2208\":1,\"2223\":3,\"2245\":2,\"2428\":2}}],[\"pdb\",{\"1\":{\"263\":1}}],[\"p808\",{\"1\":{\"285\":1,\"356\":2,\"1128\":1}}],[\"pjs\",{\"1\":{\"269\":1,\"278\":1}}],[\"pkl\",{\"1\":{\"267\":2,\"276\":2,\"286\":4,\"290\":4,\"523\":1}}],[\"ps\",{\"1\":{\"1627\":2,\"2226\":2,\"2241\":2,\"2413\":2,\"2424\":2,\"2448\":2}}],[\"psd\",{\"1\":{\"1054\":2,\"1126\":6,\"1293\":3,\"1309\":2,\"1310\":2,\"1311\":7,\"1318\":7,\"1319\":3,\"1321\":5,\"1322\":7,\"1323\":5,\"1326\":2,\"1327\":9,\"1328\":7,\"1329\":2,\"1330\":9,\"1354\":6,\"1853\":1}}],[\"ps2sttask\",{\"0\":{\"2262\":1},\"1\":{\"2262\":1}}],[\"ps2st\",{\"0\":{\"404\":1,\"1965\":1,\"1966\":1,\"2262\":1,\"2536\":1},\"1\":{\"404\":1,\"1965\":1,\"1966\":1,\"2262\":2}}],[\"pseudomos\",{\"1\":{\"286\":1}}],[\"pseudo\",{\"0\":{\"2472\":1},\"1\":{\"232\":2,\"258\":1,\"259\":5,\"276\":1,\"285\":4,\"286\":1,\"1608\":1,\"2462\":2,\"2472\":3}}],[\"pscore\",{\"1\":{\"212\":1}}],[\"phi\",{\"1\":{\"1309\":4,\"1310\":2}}],[\"phonemize\",{\"1\":{\"2286\":1}}],[\"phonemizer\",{\"0\":{\"2286\":1},\"1\":{\"287\":13,\"290\":2,\"2286\":7}}],[\"phonemization\",{\"1\":{\"2280\":2}}],[\"phonemetokenizer\",{\"0\":{\"2285\":1},\"1\":{\"2285\":1}}],[\"phonemepredictor\",{\"0\":{\"1535\":1},\"1\":{\"1535\":2}}],[\"phonemes\",{\"1\":{\"269\":2,\"278\":2,\"290\":1,\"2240\":1}}],[\"phoneme\",{\"0\":{\"1535\":1,\"2276\":1,\"2277\":1,\"2280\":1,\"2281\":1,\"2285\":1,\"2286\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":1,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2471\":1},\"1\":{\"265\":3,\"269\":8,\"274\":3,\"278\":8,\"287\":3,\"290\":3,\"1535\":3,\"1552\":3,\"1553\":4,\"1788\":2,\"1977\":2,\"2276\":1,\"2277\":1,\"2280\":1,\"2281\":1,\"2285\":1,\"2286\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":3,\"2299\":1,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2428\":1,\"2471\":2}}],[\"phone\",{\"0\":{\"2301\":1,\"2302\":1},\"1\":{\"287\":1,\"481\":2,\"1025\":3,\"1521\":6,\"1546\":4,\"1678\":1,\"2228\":5,\"2229\":5,\"2235\":1,\"2236\":1,\"2286\":1,\"2301\":1,\"2302\":1}}],[\"phonetic\",{\"1\":{\"286\":1}}],[\"phones\",{\"1\":{\"269\":2,\"278\":2,\"1125\":1,\"1521\":3,\"2228\":3,\"2229\":3}}],[\"phb2\",{\"1\":{\"268\":1,\"277\":1}}],[\"phb1\",{\"1\":{\"268\":1,\"277\":1}}],[\"pha1\",{\"1\":{\"268\":2,\"277\":2}}],[\"phases\",{\"1\":{\"2018\":1}}],[\"phase\",{\"0\":{\"1332\":1,\"1680\":1},\"1\":{\"254\":1,\"1332\":4,\"1680\":4,\"1698\":1,\"2130\":2,\"2184\":1}}],[\"phn\",{\"1\":{\"218\":2,\"267\":3,\"276\":3,\"285\":5,\"286\":13,\"290\":1,\"481\":1,\"505\":1,\"536\":4,\"603\":1,\"1521\":24,\"1526\":2,\"1553\":2,\"2228\":26,\"2229\":26,\"2236\":3,\"2239\":3,\"2240\":3,\"2245\":3,\"2363\":1}}],[\"phrase\",{\"1\":{\"200\":2}}],[\"phrases\",{\"1\":{\"200\":1,\"287\":2}}],[\"pt\",{\"1\":{\"536\":2,\"746\":1}}],[\"ptxt\",{\"1\":{\"212\":1}}],[\"pth\",{\"1\":{\"87\":1,\"88\":6,\"218\":5,\"243\":3,\"267\":9,\"276\":5,\"286\":13,\"290\":13,\"1956\":2,\"2315\":5}}],[\"pmlr\",{\"1\":{\"202\":2}}],[\"pp\",{\"1\":{\"1309\":1,\"1311\":1,\"1730\":1}}],[\"pp^\",{\"1\":{\"824\":1}}],[\"ppl\",{\"1\":{\"200\":1,\"205\":1,\"242\":1}}],[\"ppp\",{\"1\":{\"67\":2}}],[\"pbs\",{\"1\":{\"165\":1,\"166\":3,\"167\":1}}],[\"plens\",{\"1\":{\"2226\":1}}],[\"please\",{\"1\":{\"36\":1,\"39\":1,\"41\":2,\"42\":1,\"55\":1,\"56\":1,\"69\":2,\"71\":1,\"106\":1,\"118\":1,\"119\":1,\"123\":1,\"127\":1,\"128\":3,\"129\":1,\"130\":1,\"132\":1,\"138\":1,\"141\":1,\"150\":1,\"153\":2,\"156\":1,\"160\":1,\"165\":1,\"171\":1,\"172\":1,\"173\":3,\"200\":3,\"201\":1,\"211\":1,\"224\":4,\"225\":2,\"243\":3,\"249\":1,\"260\":1,\"266\":1,\"267\":8,\"269\":2,\"275\":1,\"276\":3,\"278\":2,\"285\":2,\"286\":7,\"289\":1,\"290\":6,\"536\":2,\"696\":1,\"697\":1,\"747\":1,\"846\":1}}],[\"pl|queue\",{\"1\":{\"535\":1}}],[\"pl|utils\",{\"1\":{\"516\":1,\"520\":1,\"521\":1,\"523\":1,\"524\":1,\"525\":1}}],[\"plcmos\",{\"1\":{\"285\":2}}],[\"plug\",{\"1\":{\"246\":1}}],[\"plus\",{\"0\":{\"916\":1},\"1\":{\"201\":2,\"265\":1,\"267\":4,\"916\":1,\"924\":1,\"1539\":1,\"1552\":1}}],[\"plain\",{\"1\":{\"1330\":1,\"2144\":4}}],[\"planes\",{\"1\":{\"693\":1,\"885\":2,\"1089\":1,\"1093\":1,\"1196\":1,\"1233\":1,\"1303\":2,\"1304\":2,\"1346\":2,\"1347\":2,\"2177\":2,\"2179\":1,\"2181\":2,\"2185\":1,\"2196\":1}}],[\"planet\",{\"1\":{\"242\":1}}],[\"plays\",{\"1\":{\"247\":1}}],[\"play\",{\"1\":{\"246\":1}}],[\"placeholder\",{\"1\":{\"716\":1,\"831\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"2044\":1,\"2133\":1,\"2136\":2}}],[\"places\",{\"1\":{\"243\":1,\"2143\":1}}],[\"placed\",{\"1\":{\"223\":1,\"2130\":1}}],[\"place>\",{\"1\":{\"161\":1,\"162\":1}}],[\"place\",{\"1\":{\"138\":1,\"243\":1,\"616\":1,\"1931\":1,\"2138\":1}}],[\"platform\",{\"1\":{\"127\":1}}],[\"platforms\",{\"1\":{\"127\":1}}],[\"plotting\",{\"1\":{\"2354\":1}}],[\"plot\",{\"1\":{\"114\":2,\"218\":1,\"243\":1,\"267\":1,\"276\":1,\"286\":5,\"290\":1,\"377\":2,\"768\":1,\"1598\":3,\"1625\":3,\"2338\":1,\"2347\":1,\"2354\":1,\"2359\":2,\"2369\":3,\"2371\":1}}],[\"pl\",{\"0\":{\"167\":1},\"1\":{\"69\":1,\"166\":5,\"167\":5,\"168\":2,\"196\":2,\"213\":2,\"267\":1,\"268\":2,\"276\":1,\"277\":2,\"286\":1,\"290\":2,\"516\":1,\"520\":1,\"521\":1,\"523\":1,\"524\":1,\"525\":1,\"535\":1,\"537\":1,\"960\":1,\"961\":5,\"2333\":1}}],[\"p2\",{\"1\":{\"66\":1,\"916\":1}}],[\"p\",{\"0\":{\"2119\":1},\"1\":{\"31\":1,\"75\":2,\"243\":1,\"267\":2,\"268\":2,\"272\":1,\"285\":2,\"287\":3,\"637\":2,\"730\":1,\"824\":2,\"837\":1,\"924\":3,\"927\":2,\"939\":2,\"982\":2,\"985\":1,\"1210\":1,\"1264\":1,\"1273\":2,\"1274\":2,\"1319\":1,\"1334\":1,\"1509\":1,\"1511\":1,\"1516\":1,\"1553\":1,\"1554\":1,\"1589\":2,\"1601\":6,\"1602\":4,\"1627\":2,\"1637\":3,\"1726\":1,\"1727\":1,\"2000\":10,\"2001\":5,\"2226\":2,\"2241\":2,\"2413\":2,\"2424\":2,\"2448\":2,\"2476\":1}}],[\"pickalable\",{\"1\":{\"2276\":1,\"2277\":1}}],[\"picklable\",{\"1\":{\"2131\":1}}],[\"pi\",{\"1\":{\"1545\":1}}],[\"pixels\",{\"1\":{\"1306\":2,\"1371\":2}}],[\"pin\",{\"1\":{\"639\":1,\"1643\":1,\"1645\":3,\"1646\":1,\"1650\":1}}],[\"pinyin\",{\"1\":{\"269\":1,\"278\":1,\"287\":2,\"536\":2}}],[\"pino\",{\"1\":{\"10\":1}}],[\"pits\",{\"0\":{\"1554\":1,\"1556\":1},\"1\":{\"1554\":1,\"1556\":1}}],[\"pitsolver\",{\"0\":{\"1228\":1},\"1\":{\"1228\":2}}],[\"pitloss\",{\"1\":{\"794\":1}}],[\"pitlosswrapper\",{\"0\":{\"794\":1},\"1\":{\"794\":1}}],[\"pitch=false\",{\"1\":{\"2226\":1}}],[\"pitch\",{\"0\":{\"1519\":1,\"1561\":1,\"1692\":1},\"1\":{\"266\":1,\"267\":5,\"269\":1,\"275\":1,\"276\":3,\"278\":1,\"535\":1,\"1519\":2,\"1521\":17,\"1524\":1,\"1526\":5,\"1546\":1,\"1552\":4,\"1553\":7,\"1561\":1,\"1565\":1,\"1570\":1,\"1585\":14,\"1598\":10,\"1599\":32,\"1600\":7,\"1627\":3,\"1692\":5,\"1697\":1,\"1698\":1,\"1833\":1,\"2226\":4,\"2228\":17,\"2229\":17,\"2235\":7,\"2236\":7,\"2239\":12,\"2240\":10,\"2241\":1,\"2245\":6,\"2408\":17,\"2411\":1,\"2412\":35,\"2413\":3,\"2423\":33,\"2424\":3,\"2446\":17,\"2447\":33,\"2448\":3}}],[\"pit\",{\"0\":{\"736\":1,\"794\":1,\"1209\":1,\"1228\":1},\"1\":{\"223\":2,\"736\":1,\"794\":2,\"974\":2,\"1156\":2,\"1204\":1,\"1209\":4,\"1228\":3}}],[\"piecewiselinearwarmuplr\",{\"0\":{\"2017\":1},\"1\":{\"2017\":2}}],[\"piecewise\",{\"0\":{\"1634\":1,\"2017\":1},\"1\":{\"1634\":1,\"2017\":2}}],[\"pieces\",{\"1\":{\"102\":1}}],[\"piece\",{\"1\":{\"102\":1,\"513\":2,\"514\":2}}],[\"pipefail\",{\"1\":{\"126\":1,\"243\":1}}],[\"pipelines\",{\"1\":{\"246\":1}}],[\"pipeline\",{\"1\":{\"8\":1,\"126\":1,\"233\":1,\"236\":1,\"243\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"829\":2,\"830\":1,\"859\":1,\"2131\":1}}],[\"pip\",{\"1\":{\"31\":3,\"39\":2,\"153\":1,\"162\":7,\"163\":2,\"213\":1,\"286\":1}}],[\"peak\",{\"1\":{\"1672\":2}}],[\"pei2\",{\"1\":{\"287\":1}}],[\"pesq\",{\"0\":{\"360\":1},\"1\":{\"285\":3,\"356\":2}}],[\"penalize\",{\"1\":{\"2176\":1}}],[\"penalty\",{\"0\":{\"2470\":1,\"2473\":1},\"1\":{\"262\":1,\"301\":2,\"315\":2,\"389\":2,\"396\":2,\"406\":4,\"421\":2,\"429\":2,\"442\":2,\"463\":4,\"469\":4,\"505\":2,\"697\":1,\"1432\":2,\"1441\":1,\"2176\":5,\"2470\":3,\"2473\":2}}],[\"peng2023reproducing\",{\"1\":{\"6\":2,\"244\":1}}],[\"peng\",{\"1\":{\"5\":1,\"6\":3,\"7\":1,\"10\":1,\"12\":1,\"202\":1,\"244\":1}}],[\"people\",{\"1\":{\"242\":1}}],[\"pe\",{\"1\":{\"168\":1,\"645\":1,\"1784\":1,\"1808\":1,\"1818\":1,\"1837\":1}}],[\"pem\",{\"1\":{\"92\":1}}],[\"peer\",{\"1\":{\"66\":2}}],[\"perfect\",{\"1\":{\"1608\":1}}],[\"performed\",{\"1\":{\"148\":1,\"200\":2,\"266\":1,\"275\":1,\"285\":1,\"286\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"693\":1,\"704\":1,\"713\":1,\"718\":1,\"720\":1,\"722\":1,\"738\":1,\"741\":1,\"752\":1,\"757\":1,\"764\":1,\"778\":1,\"781\":1,\"788\":1,\"791\":1,\"796\":1,\"798\":1,\"802\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"821\":1,\"825\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"842\":1,\"844\":1,\"852\":1,\"854\":1,\"856\":1,\"860\":1,\"862\":1,\"864\":1,\"950\":1,\"952\":1,\"956\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1066\":1,\"1068\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1114\":1,\"1120\":1,\"1122\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1145\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1159\":1,\"1165\":1,\"1168\":1,\"1177\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1200\":1,\"1202\":1,\"1205\":1,\"1209\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1219\":1,\"1226\":1,\"1228\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1248\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1262\":1,\"1265\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1301\":2,\"1328\":1,\"1334\":1,\"1372\":2,\"1383\":1,\"1387\":1,\"1392\":1,\"1398\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1509\":1,\"1511\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1549\":1,\"1554\":1,\"1638\":1,\"1652\":1,\"1657\":1,\"1662\":1,\"1742\":1,\"1750\":3,\"1938\":1,\"1940\":1,\"1942\":1,\"1945\":1,\"1957\":1,\"1967\":1,\"1969\":1,\"1972\":1,\"1975\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2185\":1,\"2188\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2203\":1,\"2205\":1,\"2209\":1,\"2211\":1,\"2215\":1,\"2216\":1,\"2224\":1,\"2225\":1,\"2305\":1,\"2325\":1,\"2401\":1,\"2405\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2434\":1,\"2443\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2465\":1,\"2467\":1}}],[\"performs\",{\"1\":{\"104\":1,\"235\":1,\"768\":1,\"1645\":1,\"1668\":1,\"1804\":1,\"1821\":1,\"2000\":1,\"2130\":2,\"2338\":1,\"2354\":1,\"2369\":1}}],[\"performance\",{\"1\":{\"45\":3,\"53\":1,\"66\":1,\"128\":1,\"145\":3,\"197\":1,\"224\":1,\"246\":1,\"247\":1,\"249\":1,\"261\":1,\"262\":3,\"821\":1}}],[\"perform\",{\"0\":{\"1351\":1,\"1352\":1},\"1\":{\"38\":1,\"108\":1,\"148\":1,\"197\":1,\"200\":4,\"240\":1,\"242\":1,\"262\":2,\"269\":1,\"278\":1,\"286\":2,\"616\":1,\"637\":1,\"776\":1,\"795\":1,\"931\":1,\"933\":1,\"1351\":2,\"1352\":1,\"1361\":1,\"1389\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1514\":1,\"1526\":1,\"1535\":1,\"1548\":2,\"1553\":1,\"1592\":1,\"1598\":1,\"1600\":1,\"1605\":1,\"1610\":1,\"1619\":1,\"1625\":1,\"1719\":2,\"1720\":1,\"1721\":2,\"1725\":3,\"1726\":1,\"1727\":1,\"1806\":3,\"1862\":2,\"1873\":1,\"1877\":1,\"1908\":1,\"1951\":1,\"2014\":1,\"2015\":1,\"2040\":1,\"2043\":2,\"2044\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":2,\"2056\":2,\"2066\":2,\"2215\":1,\"2236\":1,\"2239\":2,\"2240\":1,\"2245\":1,\"2338\":1,\"2369\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"performing\",{\"1\":{\"38\":1,\"212\":1,\"218\":1,\"267\":2,\"276\":2,\"286\":2,\"290\":1,\"1301\":1,\"1372\":1,\"2130\":1,\"2131\":1}}],[\"personalized\",{\"1\":{\"1128\":1}}],[\"perm=true\",{\"1\":{\"1209\":1,\"1228\":1}}],[\"permutate\",{\"1\":{\"794\":1}}],[\"permutation\",{\"1\":{\"564\":1,\"794\":1,\"1132\":1,\"1156\":1,\"1157\":1,\"1167\":1,\"1204\":2,\"1209\":6,\"1228\":5}}],[\"permute\",{\"1\":{\"794\":2,\"974\":1}}],[\"perm\",{\"0\":{\"564\":1},\"1\":{\"564\":1,\"794\":1,\"1156\":1,\"1157\":3,\"1209\":1,\"1228\":1}}],[\"perceptual\",{\"1\":{\"267\":2,\"276\":2,\"285\":1,\"286\":2}}],[\"percentage\",{\"1\":{\"173\":1}}],[\"percent\",{\"1\":{\"147\":1,\"1676\":2}}],[\"periods\",{\"1\":{\"1389\":1,\"1390\":1,\"1401\":1,\"1402\":1,\"1408\":2,\"1409\":4,\"1420\":3,\"1526\":1,\"1549\":3,\"1553\":1,\"1593\":3,\"1595\":3,\"1598\":1,\"1600\":1,\"1625\":1}}],[\"period\",{\"1\":{\"242\":1,\"1389\":1,\"1390\":2,\"1401\":1,\"1402\":2,\"1409\":3,\"1420\":1,\"1467\":1,\"1526\":2,\"1549\":4,\"1553\":3,\"1593\":3,\"1595\":7,\"1596\":4,\"1598\":2,\"1600\":2,\"1625\":2}}],[\"perturbed\",{\"1\":{\"1697\":1,\"1698\":1}}],[\"perturb\",{\"0\":{\"535\":1,\"1697\":1,\"1717\":1,\"1802\":1,\"1813\":1,\"1833\":1,\"1852\":1},\"1\":{\"223\":2,\"535\":2,\"1655\":2,\"1697\":1,\"1717\":1,\"1802\":1,\"1813\":1,\"1833\":2,\"1852\":1}}],[\"perturbation\",{\"1\":{\"197\":1,\"199\":1,\"200\":2,\"204\":1,\"205\":2,\"222\":1,\"223\":5,\"228\":1,\"234\":1,\"235\":2,\"240\":1,\"242\":2,\"253\":1,\"254\":1,\"535\":1,\"1697\":1,\"1833\":2,\"1852\":1}}],[\"perhaps\",{\"1\":{\"212\":1}}],[\"perplexity\",{\"0\":{\"385\":1},\"1\":{\"199\":1,\"200\":2,\"204\":1,\"205\":2,\"228\":1,\"240\":1,\"242\":2,\"246\":1,\"377\":2,\"385\":2,\"541\":1,\"1943\":1}}],[\"perl\",{\"1\":{\"166\":1}}],[\"per\",{\"0\":{\"940\":1,\"2080\":1},\"1\":{\"63\":1,\"91\":2,\"235\":2,\"377\":4,\"449\":4,\"689\":1,\"692\":2,\"790\":1,\"804\":2,\"820\":2,\"821\":1,\"824\":1,\"828\":2,\"830\":1,\"847\":1,\"850\":1,\"932\":2,\"934\":2,\"940\":1,\"1000\":1,\"1008\":1,\"1153\":1,\"1155\":2,\"1157\":3,\"1382\":1,\"1441\":2,\"1643\":1,\"1644\":1,\"1645\":3,\"1646\":1,\"1647\":2,\"1650\":1,\"1677\":1,\"1678\":1,\"1692\":5,\"1815\":1,\"1936\":1,\"1957\":1,\"1961\":5,\"1992\":1,\"2000\":1,\"2015\":1,\"2021\":1,\"2131\":1,\"2133\":1,\"2134\":1,\"2136\":3,\"2145\":1,\"2146\":1,\"2147\":1,\"2176\":2,\"2249\":3,\"2253\":3,\"2258\":2,\"2343\":2,\"2352\":2,\"2354\":6}}],[\"peter\",{\"1\":{\"9\":1,\"10\":1,\"13\":1}}],[\"pulsegen\",{\"1\":{\"1545\":1}}],[\"pulse\",{\"1\":{\"1545\":2}}],[\"pulse=false\",{\"1\":{\"1545\":2}}],[\"pull\",{\"1\":{\"615\":1,\"644\":1,\"709\":1,\"774\":1,\"780\":1,\"1785\":1,\"1786\":1,\"1817\":1,\"1818\":1,\"2191\":1}}],[\"pulling\",{\"1\":{\"3\":1}}],[\"pubmed\",{\"1\":{\"1117\":1}}],[\"public\",{\"1\":{\"249\":1,\"2016\":1}}],[\"publicly\",{\"1\":{\"6\":1,\"244\":1}}],[\"publisher\",{\"1\":{\"202\":1}}],[\"pu3\",{\"1\":{\"287\":1}}],[\"punc\",{\"1\":{\"267\":2,\"276\":2,\"286\":3,\"2281\":1}}],[\"punctuations\",{\"1\":{\"267\":1,\"276\":1,\"286\":1}}],[\"punctuation\",{\"1\":{\"267\":2,\"276\":2,\"286\":2}}],[\"pushing\",{\"1\":{\"256\":1,\"2192\":1}}],[\"put\",{\"0\":{\"910\":1},\"1\":{\"162\":2,\"195\":1,\"224\":1,\"242\":3,\"269\":1,\"278\":1,\"910\":1,\"2355\":1}}],[\"pure\",{\"1\":{\"175\":1,\"262\":2}}],[\"purely\",{\"1\":{\"138\":1,\"232\":1,\"258\":1}}],[\"purposes\",{\"1\":{\"78\":1,\"827\":1}}],[\"purpose\",{\"1\":{\"26\":1,\"39\":1,\"81\":1,\"162\":1,\"257\":1}}],[\"pause\",{\"0\":{\"2296\":1},\"1\":{\"287\":2,\"290\":1,\"481\":1,\"2296\":1}}],[\"pauses\",{\"1\":{\"269\":2,\"278\":2}}],[\"pau\",{\"1\":{\"271\":1,\"280\":1,\"287\":2}}],[\"pair\",{\"1\":{\"269\":1,\"278\":1,\"1000\":1,\"2039\":1}}],[\"pairs\",{\"1\":{\"265\":1,\"269\":5,\"274\":1,\"278\":5,\"514\":2,\"847\":2,\"1749\":2,\"1815\":2,\"1843\":2,\"2039\":2,\"2139\":1,\"2144\":1}}],[\"papers\",{\"1\":{\"974\":1,\"1125\":1,\"1717\":1,\"2176\":3}}],[\"paper\",{\"1\":{\"129\":1,\"146\":1,\"240\":1,\"262\":1,\"290\":2,\"780\":1,\"922\":1,\"927\":1,\"1168\":1,\"1385\":1,\"1717\":1,\"1785\":1,\"1817\":1,\"2187\":1,\"2191\":1,\"2192\":1,\"2198\":1,\"2203\":1,\"2209\":1,\"2239\":1}}],[\"packet\",{\"1\":{\"285\":1}}],[\"packed\",{\"1\":{\"127\":2,\"2147\":1}}],[\"packs\",{\"1\":{\"200\":1,\"205\":1,\"211\":1,\"217\":1,\"235\":1,\"242\":1,\"254\":1,\"266\":1,\"275\":1,\"285\":1}}],[\"pack\",{\"0\":{\"402\":1,\"526\":1,\"1948\":1,\"1952\":1,\"1953\":1,\"1954\":2,\"1955\":1,\"2147\":1},\"1\":{\"199\":1,\"200\":1,\"204\":1,\"205\":1,\"216\":1,\"217\":1,\"222\":1,\"223\":2,\"227\":1,\"228\":1,\"234\":1,\"235\":1,\"240\":1,\"242\":1,\"253\":1,\"254\":1,\"284\":1,\"285\":1,\"402\":2,\"403\":14,\"526\":1,\"1948\":1,\"1952\":1,\"1953\":1,\"1954\":2,\"1955\":1,\"2134\":2,\"2145\":1,\"2147\":2}}],[\"packaging\",{\"1\":{\"162\":1}}],[\"packages\",{\"1\":{\"153\":1,\"159\":1,\"162\":1,\"163\":1,\"2311\":1}}],[\"package\",{\"1\":{\"3\":1,\"19\":1,\"249\":1}}],[\"packing\",{\"0\":{\"127\":1},\"1\":{\"118\":2,\"127\":1,\"200\":1,\"205\":1,\"210\":1,\"211\":2,\"217\":1,\"220\":1,\"223\":1,\"242\":1,\"254\":1,\"265\":1,\"266\":2,\"274\":1,\"275\":2,\"285\":1}}],[\"pad1d\",{\"0\":{\"1493\":1},\"1\":{\"1493\":1}}],[\"pad=\",{\"1\":{\"1369\":1}}],[\"pad=0\",{\"1\":{\"1300\":1,\"1302\":2}}],[\"pad=nan\",{\"1\":{\"993\":1,\"994\":1}}],[\"pad2d\",{\"0\":{\"1494\":1},\"1\":{\"1494\":1}}],[\"pad2\",{\"1\":{\"1269\":1,\"1270\":1,\"1334\":1}}],[\"pads\",{\"1\":{\"787\":1,\"1760\":1,\"2143\":1}}],[\"pad\",{\"0\":{\"1495\":1,\"1901\":1,\"1903\":1,\"1908\":1,\"1910\":1,\"2155\":1},\"1\":{\"82\":4,\"286\":3,\"634\":2,\"641\":2,\"643\":2,\"651\":2,\"676\":2,\"678\":1,\"692\":4,\"699\":2,\"700\":2,\"706\":14,\"709\":2,\"710\":6,\"711\":6,\"733\":2,\"734\":2,\"745\":2,\"746\":4,\"747\":2,\"748\":4,\"749\":2,\"755\":2,\"760\":5,\"770\":3,\"771\":2,\"774\":2,\"775\":4,\"777\":8,\"780\":2,\"785\":2,\"790\":4,\"791\":2,\"792\":2,\"796\":2,\"797\":1,\"798\":1,\"820\":4,\"846\":4,\"847\":2,\"849\":2,\"850\":4,\"862\":1,\"941\":1,\"956\":1,\"959\":2,\"962\":1,\"994\":6,\"1029\":2,\"1119\":1,\"1156\":8,\"1164\":1,\"1212\":1,\"1235\":2,\"1356\":3,\"1370\":4,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1442\":1,\"1444\":1,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1458\":2,\"1460\":2,\"1466\":1,\"1468\":1,\"1484\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1529\":2,\"1604\":4,\"1605\":4,\"1606\":5,\"1615\":4,\"1618\":2,\"1640\":4,\"1641\":4,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1760\":6,\"1768\":2,\"1788\":2,\"1801\":2,\"1814\":2,\"1816\":2,\"1835\":1,\"1858\":2,\"1868\":3,\"1870\":3,\"1900\":1,\"1901\":1,\"1902\":5,\"1903\":1,\"1904\":5,\"1907\":2,\"1908\":3,\"1909\":1,\"1910\":4,\"1924\":1,\"1928\":4,\"2026\":1,\"2028\":1,\"2127\":2,\"2129\":2,\"2155\":5,\"2335\":2,\"2336\":1,\"2350\":3,\"2376\":2,\"2451\":1,\"2453\":1,\"2455\":2,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":2}}],[\"paddings\",{\"1\":{\"1493\":1,\"1494\":1,\"1505\":1,\"1506\":1,\"2209\":1}}],[\"padding=1\",{\"1\":{\"1304\":1,\"1347\":1}}],[\"padding=0\",{\"1\":{\"1180\":1,\"1181\":1,\"1303\":1,\"1346\":1,\"1383\":1}}],[\"padding=false\",{\"1\":{\"1103\":1,\"1114\":1,\"1236\":1}}],[\"padding=\",{\"1\":{\"1080\":1,\"1082\":2,\"1108\":1,\"1120\":1,\"1124\":3,\"1147\":3,\"1181\":1}}],[\"padding\",{\"0\":{\"1482\":1,\"1484\":1,\"1486\":1,\"1563\":1},\"1\":{\"82\":2,\"286\":1,\"616\":1,\"625\":1,\"634\":1,\"641\":1,\"643\":1,\"667\":1,\"675\":6,\"699\":5,\"700\":1,\"709\":3,\"710\":3,\"711\":3,\"716\":1,\"717\":2,\"733\":1,\"734\":1,\"749\":1,\"770\":1,\"771\":3,\"774\":3,\"780\":3,\"787\":3,\"831\":5,\"849\":3,\"851\":1,\"959\":1,\"973\":1,\"981\":1,\"1080\":3,\"1107\":3,\"1120\":1,\"1122\":1,\"1124\":6,\"1125\":9,\"1147\":6,\"1148\":1,\"1164\":1,\"1180\":2,\"1181\":3,\"1272\":1,\"1301\":1,\"1356\":4,\"1368\":2,\"1372\":1,\"1401\":1,\"1402\":1,\"1408\":1,\"1409\":1,\"1442\":1,\"1444\":2,\"1446\":1,\"1448\":3,\"1450\":1,\"1452\":2,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":1,\"1467\":1,\"1482\":1,\"1484\":8,\"1486\":1,\"1493\":2,\"1494\":2,\"1505\":2,\"1506\":2,\"1524\":3,\"1525\":3,\"1526\":1,\"1529\":1,\"1549\":1,\"1553\":1,\"1563\":1,\"1594\":1,\"1595\":1,\"1598\":1,\"1600\":1,\"1604\":2,\"1605\":2,\"1606\":3,\"1615\":2,\"1625\":1,\"1668\":2,\"1680\":1,\"1692\":1,\"1698\":1,\"1758\":1,\"1782\":3,\"1788\":1,\"1858\":1,\"1863\":3,\"1866\":3,\"1868\":1,\"1870\":1,\"1907\":1,\"1908\":2,\"1910\":1,\"1987\":1,\"2003\":1,\"2004\":1,\"2007\":2,\"2126\":1,\"2129\":3,\"2130\":1,\"2143\":2,\"2155\":3,\"2183\":1,\"2190\":1,\"2191\":3,\"2208\":1,\"2220\":3,\"2227\":1,\"2231\":1,\"2451\":1,\"2458\":1,\"2460\":2,\"2463\":2,\"2464\":2,\"2473\":3}}],[\"padded\",{\"1\":{\"48\":1,\"616\":1,\"703\":4,\"706\":4,\"716\":1,\"717\":1,\"786\":1,\"800\":1,\"867\":1,\"878\":3,\"879\":3,\"881\":3,\"882\":3,\"883\":3,\"884\":3,\"921\":1,\"935\":1,\"994\":2,\"1029\":1,\"1235\":1,\"1306\":1,\"1309\":1,\"1371\":1,\"1444\":1,\"1448\":1,\"1526\":13,\"1546\":1,\"1552\":18,\"1553\":14,\"1599\":3,\"1627\":1,\"1700\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1750\":4,\"1753\":2,\"1758\":2,\"1764\":1,\"1768\":2,\"1801\":2,\"1810\":2,\"1814\":1,\"1816\":1,\"1839\":2,\"1858\":3,\"1901\":2,\"1903\":2,\"1907\":3,\"1908\":1,\"1910\":1,\"1991\":1,\"1992\":2,\"1993\":2,\"1995\":2,\"2007\":1,\"2061\":1,\"2137\":1,\"2155\":1,\"2184\":1,\"2220\":1,\"2223\":2,\"2226\":1,\"2231\":2,\"2235\":20,\"2236\":20,\"2237\":2,\"2239\":20,\"2240\":18,\"2241\":1,\"2245\":18,\"2379\":1,\"2411\":4,\"2412\":6,\"2413\":1,\"2423\":6,\"2424\":1,\"2425\":1,\"2429\":1,\"2431\":3,\"2432\":3,\"2433\":1,\"2447\":5,\"2448\":1,\"2460\":1}}],[\"pan\",{\"1\":{\"74\":2}}],[\"pan=\",{\"1\":{\"74\":2}}],[\"past\",{\"1\":{\"1334\":3,\"1735\":5,\"1966\":1}}],[\"paser\",{\"1\":{\"40\":1}}],[\"passes\",{\"1\":{\"1668\":1,\"2136\":1,\"2188\":1,\"2355\":1}}],[\"passed\",{\"1\":{\"49\":1,\"141\":1,\"144\":1,\"756\":1,\"773\":1,\"830\":1,\"866\":1,\"867\":1,\"912\":1,\"1051\":1,\"1938\":1,\"2131\":1,\"2140\":1}}],[\"passing\",{\"1\":{\"141\":1,\"142\":1,\"821\":1,\"822\":1}}],[\"pass\",{\"0\":{\"412\":1},\"1\":{\"22\":1,\"23\":1,\"128\":1,\"150\":1,\"180\":1,\"263\":1,\"653\":1,\"654\":2,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"724\":3,\"725\":3,\"726\":2,\"727\":2,\"728\":3,\"729\":2,\"730\":1,\"739\":1,\"742\":1,\"744\":3,\"753\":1,\"754\":1,\"756\":6,\"758\":1,\"759\":1,\"773\":6,\"779\":1,\"782\":1,\"784\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"819\":1,\"822\":1,\"823\":1,\"824\":2,\"826\":1,\"827\":1,\"828\":3,\"829\":4,\"830\":3,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"841\":1,\"843\":1,\"845\":1,\"846\":1,\"851\":1,\"853\":1,\"855\":1,\"857\":1,\"859\":2,\"861\":1,\"863\":1,\"865\":1,\"866\":2,\"867\":2,\"878\":2,\"879\":2,\"881\":1,\"882\":2,\"883\":2,\"884\":1,\"912\":1,\"951\":1,\"953\":1,\"957\":1,\"958\":2,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1514\":1,\"1516\":1,\"1518\":1,\"1519\":1,\"1520\":1,\"1523\":1,\"1528\":1,\"1529\":1,\"1531\":1,\"1534\":1,\"1536\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1556\":1,\"1639\":1,\"1645\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1702\":1,\"1742\":1,\"1842\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1965\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2040\":1,\"2043\":2,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":2,\"2056\":2,\"2066\":2,\"2125\":1,\"2127\":1,\"2134\":1,\"2167\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2176\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2183\":1,\"2184\":1,\"2186\":1,\"2189\":1,\"2190\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2207\":1,\"2208\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2246\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2306\":1,\"2326\":1,\"2334\":2,\"2355\":3,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1,\"2488\":1}}],[\"par\",{\"1\":{\"1794\":3}}],[\"parenthesis\",{\"0\":{\"2496\":1},\"1\":{\"2496\":1}}],[\"parent\",{\"1\":{\"1695\":1,\"2008\":1}}],[\"park\",{\"1\":{\"833\":1}}],[\"pariente\",{\"1\":{\"691\":1}}],[\"parsing\",{\"1\":{\"84\":1}}],[\"parser\",{\"0\":{\"2152\":1},\"1\":{\"78\":2,\"290\":1,\"1844\":2,\"2152\":2,\"2246\":1,\"2247\":3,\"2248\":1,\"2249\":4,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2334\":4,\"2347\":1,\"2369\":2,\"2371\":1,\"2479\":6,\"2485\":6,\"2493\":6,\"2500\":3,\"2505\":6}}],[\"parse\",{\"0\":{\"117\":1,\"1911\":1,\"2114\":1,\"2156\":1,\"2491\":1},\"1\":{\"40\":2,\"106\":1,\"117\":2,\"1892\":1,\"1911\":2,\"1912\":1,\"2156\":2,\"2334\":1,\"2443\":1,\"2474\":1,\"2479\":4,\"2485\":4,\"2491\":1,\"2493\":4,\"2500\":1,\"2505\":4}}],[\"paradigms\",{\"1\":{\"2131\":1}}],[\"parase\",{\"1\":{\"117\":1}}],[\"parallelizes\",{\"1\":{\"2304\":1}}],[\"parallelize\",{\"1\":{\"2304\":1}}],[\"parallelization\",{\"1\":{\"165\":1,\"167\":1}}],[\"parallelllm\",{\"1\":{\"2148\":1}}],[\"parallelhfmodel\",{\"0\":{\"2140\":1},\"1\":{\"2140\":1}}],[\"parallelwaveganpretrainedvocoder\",{\"0\":{\"2422\":1},\"1\":{\"2422\":2}}],[\"parallelwavegangenerator\",{\"0\":{\"1610\":1},\"1\":{\"1610\":2}}],[\"parallelwavegandiscriminator\",{\"0\":{\"1609\":1},\"1\":{\"1609\":2}}],[\"parallelwavegan\",{\"1\":{\"267\":1,\"276\":1,\"286\":2,\"290\":1,\"1608\":1}}],[\"parallels\",{\"1\":{\"120\":2}}],[\"parallel\",{\"0\":{\"120\":1,\"1580\":1,\"1582\":1,\"1609\":2,\"1610\":2,\"1617\":1,\"1624\":1,\"2140\":1,\"2148\":2,\"2422\":1},\"1\":{\"54\":1,\"56\":1,\"68\":1,\"69\":1,\"106\":1,\"286\":5,\"290\":3,\"301\":2,\"421\":2,\"516\":1,\"520\":1,\"521\":1,\"523\":1,\"524\":1,\"525\":1,\"535\":1,\"536\":9,\"537\":1,\"600\":1,\"705\":1,\"795\":1,\"1577\":1,\"1580\":1,\"1582\":1,\"1609\":3,\"1610\":3,\"1617\":1,\"1624\":1,\"1793\":1,\"2130\":3,\"2132\":1,\"2136\":1,\"2140\":3,\"2148\":4,\"2149\":1,\"2157\":1,\"2412\":1,\"2422\":2}}],[\"paramref\",{\"1\":{\"2355\":2}}],[\"parametrization\",{\"0\":{\"1476\":1},\"1\":{\"1476\":1}}],[\"parameterization\",{\"1\":{\"938\":1}}],[\"parameterizations\",{\"1\":{\"821\":1}}],[\"parameterized\",{\"1\":{\"144\":1,\"691\":1}}],[\"parameter\",{\"1\":{\"43\":2,\"45\":1,\"46\":1,\"135\":1,\"139\":3,\"141\":3,\"142\":1,\"143\":1,\"145\":2,\"147\":1,\"148\":1,\"218\":4,\"267\":4,\"276\":4,\"286\":4,\"652\":1,\"785\":1,\"786\":1,\"820\":1,\"821\":1,\"828\":1,\"858\":1,\"866\":1,\"882\":1,\"883\":1,\"884\":1,\"921\":1,\"1130\":1,\"1224\":1,\"1327\":1,\"1330\":1,\"1409\":1,\"1513\":1,\"1526\":9,\"1548\":1,\"1551\":1,\"1552\":3,\"1553\":7,\"1592\":1,\"1593\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":6,\"1599\":1,\"1600\":8,\"1604\":1,\"1605\":1,\"1606\":1,\"1609\":1,\"1610\":1,\"1614\":1,\"1615\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1625\":7,\"1626\":3,\"1628\":1,\"1670\":1,\"1699\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1715\":1,\"1730\":1,\"1768\":1,\"1784\":1,\"2016\":1}}],[\"parameters\",{\"0\":{\"89\":1,\"661\":1,\"665\":1,\"2113\":1,\"2117\":1},\"1\":{\"43\":4,\"45\":4,\"47\":1,\"50\":2,\"51\":3,\"52\":1,\"84\":3,\"86\":3,\"88\":6,\"104\":1,\"139\":1,\"140\":2,\"141\":5,\"142\":2,\"143\":2,\"145\":3,\"146\":1,\"147\":2,\"148\":1,\"150\":3,\"218\":1,\"243\":1,\"262\":2,\"267\":1,\"276\":1,\"284\":1,\"286\":1,\"290\":7,\"377\":2,\"449\":2,\"536\":1,\"614\":7,\"615\":2,\"616\":9,\"617\":4,\"618\":4,\"619\":2,\"620\":6,\"621\":2,\"622\":2,\"623\":2,\"624\":4,\"625\":4,\"626\":4,\"627\":4,\"629\":1,\"630\":5,\"631\":1,\"632\":5,\"633\":6,\"634\":10,\"635\":1,\"636\":4,\"637\":9,\"638\":5,\"639\":6,\"640\":2,\"641\":9,\"642\":2,\"643\":9,\"644\":7,\"645\":3,\"646\":5,\"647\":7,\"648\":2,\"649\":6,\"650\":1,\"651\":8,\"652\":1,\"653\":1,\"654\":2,\"655\":2,\"656\":2,\"657\":2,\"658\":1,\"659\":2,\"660\":1,\"661\":7,\"662\":1,\"663\":1,\"664\":1,\"665\":3,\"666\":2,\"667\":1,\"668\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":1,\"673\":1,\"675\":1,\"691\":2,\"692\":5,\"696\":9,\"697\":8,\"699\":4,\"700\":1,\"701\":2,\"702\":1,\"703\":3,\"705\":1,\"706\":6,\"709\":2,\"710\":4,\"711\":4,\"712\":1,\"715\":1,\"716\":1,\"717\":1,\"733\":1,\"734\":1,\"735\":2,\"736\":1,\"737\":2,\"738\":1,\"739\":1,\"740\":4,\"745\":3,\"746\":3,\"747\":3,\"748\":3,\"749\":2,\"750\":1,\"755\":3,\"759\":2,\"760\":6,\"761\":1,\"762\":1,\"768\":2,\"770\":2,\"771\":2,\"774\":2,\"775\":1,\"777\":2,\"780\":2,\"783\":1,\"784\":4,\"785\":2,\"786\":2,\"787\":3,\"790\":3,\"793\":1,\"794\":1,\"797\":2,\"798\":1,\"800\":1,\"804\":1,\"816\":1,\"820\":4,\"821\":5,\"824\":1,\"828\":1,\"830\":1,\"831\":2,\"832\":1,\"846\":4,\"847\":9,\"849\":2,\"850\":3,\"862\":1,\"866\":2,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"919\":1,\"921\":1,\"922\":1,\"927\":1,\"930\":1,\"932\":1,\"934\":1,\"935\":1,\"936\":1,\"937\":1,\"939\":1,\"947\":2,\"948\":1,\"949\":2,\"954\":2,\"955\":1,\"958\":3,\"959\":2,\"971\":2,\"972\":1,\"973\":1,\"974\":2,\"975\":2,\"976\":1,\"977\":1,\"978\":2,\"979\":1,\"980\":2,\"981\":1,\"982\":2,\"994\":1,\"1009\":1,\"1029\":3,\"1031\":1,\"1035\":1,\"1050\":1,\"1051\":1,\"1053\":2,\"1054\":1,\"1061\":2,\"1062\":2,\"1063\":1,\"1064\":2,\"1066\":1,\"1070\":2,\"1071\":2,\"1072\":2,\"1073\":2,\"1074\":2,\"1075\":1,\"1079\":1,\"1086\":1,\"1107\":2,\"1112\":2,\"1113\":2,\"1116\":1,\"1117\":2,\"1118\":5,\"1124\":2,\"1125\":2,\"1126\":3,\"1127\":2,\"1130\":2,\"1131\":2,\"1132\":1,\"1133\":2,\"1134\":1,\"1136\":2,\"1137\":1,\"1139\":1,\"1141\":2,\"1145\":1,\"1147\":2,\"1148\":1,\"1155\":2,\"1156\":5,\"1157\":3,\"1158\":1,\"1161\":1,\"1162\":2,\"1167\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1175\":1,\"1176\":2,\"1179\":2,\"1180\":2,\"1181\":2,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1189\":1,\"1198\":1,\"1199\":1,\"1202\":1,\"1204\":2,\"1207\":1,\"1208\":2,\"1209\":2,\"1210\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1224\":3,\"1225\":3,\"1228\":2,\"1229\":1,\"1232\":2,\"1235\":3,\"1244\":1,\"1245\":5,\"1246\":1,\"1247\":1,\"1250\":3,\"1251\":3,\"1252\":2,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":2,\"1262\":1,\"1264\":2,\"1265\":1,\"1267\":2,\"1268\":3,\"1269\":2,\"1270\":2,\"1271\":2,\"1272\":1,\"1273\":2,\"1274\":2,\"1275\":1,\"1277\":1,\"1278\":2,\"1279\":2,\"1280\":2,\"1281\":2,\"1282\":2,\"1283\":2,\"1290\":2,\"1293\":1,\"1301\":1,\"1306\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1314\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1330\":1,\"1332\":1,\"1334\":5,\"1351\":1,\"1352\":1,\"1354\":1,\"1356\":1,\"1361\":1,\"1368\":1,\"1371\":1,\"1372\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1385\":2,\"1386\":1,\"1389\":5,\"1390\":1,\"1391\":4,\"1392\":2,\"1395\":6,\"1396\":1,\"1397\":2,\"1400\":2,\"1401\":5,\"1402\":5,\"1403\":4,\"1406\":2,\"1408\":4,\"1409\":6,\"1410\":4,\"1413\":1,\"1419\":2,\"1420\":1,\"1422\":1,\"1441\":2,\"1450\":4,\"1452\":4,\"1454\":3,\"1456\":3,\"1458\":3,\"1460\":3,\"1466\":5,\"1467\":5,\"1468\":4,\"1469\":1,\"1478\":1,\"1502\":1,\"1513\":4,\"1514\":2,\"1515\":1,\"1516\":1,\"1519\":2,\"1520\":2,\"1521\":3,\"1524\":2,\"1525\":2,\"1526\":3,\"1529\":4,\"1533\":2,\"1534\":2,\"1535\":2,\"1536\":2,\"1540\":1,\"1546\":2,\"1548\":6,\"1549\":5,\"1551\":4,\"1552\":3,\"1553\":3,\"1556\":4,\"1558\":1,\"1559\":1,\"1577\":2,\"1578\":2,\"1580\":2,\"1581\":2,\"1582\":2,\"1583\":2,\"1584\":2,\"1585\":2,\"1586\":2,\"1587\":2,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":2,\"1592\":5,\"1593\":3,\"1594\":4,\"1595\":5,\"1596\":2,\"1597\":2,\"1598\":3,\"1599\":4,\"1600\":3,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":2,\"1605\":5,\"1606\":5,\"1607\":2,\"1608\":4,\"1609\":3,\"1610\":4,\"1611\":3,\"1612\":2,\"1613\":2,\"1614\":2,\"1615\":2,\"1616\":2,\"1617\":2,\"1618\":6,\"1619\":5,\"1620\":2,\"1621\":2,\"1622\":2,\"1624\":2,\"1625\":3,\"1626\":3,\"1627\":2,\"1628\":2,\"1629\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1637\":1,\"1638\":1,\"1640\":2,\"1641\":2,\"1645\":1,\"1654\":1,\"1655\":1,\"1656\":2,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":2,\"1669\":2,\"1670\":2,\"1671\":1,\"1672\":1,\"1673\":1,\"1674\":1,\"1676\":1,\"1678\":1,\"1679\":1,\"1680\":1,\"1681\":1,\"1683\":1,\"1686\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1694\":1,\"1697\":1,\"1698\":1,\"1699\":1,\"1700\":1,\"1702\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1719\":8,\"1720\":4,\"1721\":7,\"1723\":1,\"1724\":2,\"1725\":11,\"1726\":2,\"1727\":2,\"1730\":2,\"1731\":8,\"1733\":1,\"1735\":3,\"1736\":4,\"1737\":2,\"1738\":2,\"1739\":2,\"1740\":2,\"1741\":2,\"1742\":2,\"1743\":2,\"1744\":2,\"1745\":2,\"1746\":2,\"1747\":2,\"1748\":2,\"1749\":9,\"1750\":4,\"1751\":2,\"1753\":3,\"1754\":2,\"1756\":2,\"1757\":2,\"1758\":3,\"1759\":2,\"1760\":3,\"1764\":2,\"1768\":1,\"1770\":2,\"1771\":3,\"1779\":2,\"1782\":2,\"1783\":2,\"1784\":2,\"1785\":3,\"1786\":2,\"1787\":3,\"1788\":2,\"1789\":2,\"1790\":2,\"1794\":4,\"1795\":2,\"1796\":1,\"1798\":2,\"1799\":2,\"1800\":2,\"1801\":1,\"1803\":2,\"1805\":1,\"1806\":7,\"1808\":2,\"1809\":1,\"1810\":2,\"1812\":2,\"1814\":2,\"1815\":9,\"1816\":2,\"1817\":3,\"1818\":2,\"1820\":4,\"1822\":4,\"1837\":2,\"1839\":2,\"1842\":1,\"1843\":5,\"1847\":2,\"1848\":2,\"1849\":2,\"1851\":3,\"1854\":3,\"1856\":2,\"1858\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":2,\"1864\":2,\"1865\":2,\"1866\":1,\"1867\":2,\"1868\":1,\"1870\":1,\"1871\":1,\"1872\":2,\"1873\":1,\"1874\":1,\"1876\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1881\":1,\"1883\":1,\"1888\":1,\"1891\":1,\"1892\":1,\"1893\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1908\":1,\"1910\":1,\"1913\":2,\"1914\":3,\"1915\":1,\"1917\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1926\":1,\"1927\":1,\"1928\":1,\"1931\":1,\"1932\":1,\"1936\":1,\"1937\":2,\"1944\":4,\"1945\":1,\"1946\":1,\"1947\":3,\"1949\":1,\"1950\":1,\"1957\":1,\"1959\":2,\"1960\":2,\"1961\":2,\"1963\":4,\"1966\":3,\"1973\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1979\":2,\"1981\":2,\"1983\":2,\"1984\":1,\"1991\":1,\"1992\":4,\"1993\":3,\"1995\":2,\"1996\":1,\"1997\":3,\"2000\":1,\"2001\":1,\"2005\":1,\"2006\":1,\"2007\":1,\"2015\":1,\"2018\":1,\"2039\":3,\"2040\":1,\"2043\":1,\"2044\":6,\"2045\":2,\"2049\":2,\"2054\":2,\"2055\":1,\"2056\":1,\"2065\":2,\"2066\":1,\"2101\":1,\"2127\":2,\"2129\":2,\"2130\":5,\"2131\":2,\"2132\":1,\"2133\":4,\"2134\":4,\"2136\":6,\"2137\":4,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":2,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2148\":1,\"2149\":1,\"2150\":1,\"2151\":1,\"2155\":1,\"2156\":1,\"2157\":1,\"2162\":1,\"2167\":2,\"2176\":2,\"2183\":2,\"2184\":1,\"2187\":2,\"2188\":1,\"2190\":2,\"2191\":2,\"2192\":1,\"2198\":1,\"2203\":1,\"2207\":2,\"2208\":2,\"2209\":1,\"2215\":1,\"2218\":2,\"2219\":2,\"2220\":2,\"2221\":2,\"2223\":2,\"2224\":1,\"2226\":2,\"2227\":2,\"2228\":3,\"2229\":3,\"2231\":3,\"2232\":3,\"2235\":4,\"2236\":4,\"2237\":2,\"2238\":2,\"2239\":4,\"2240\":4,\"2241\":2,\"2242\":1,\"2243\":1,\"2245\":3,\"2247\":6,\"2249\":1,\"2298\":1,\"2307\":2,\"2308\":1,\"2310\":1,\"2311\":1,\"2312\":2,\"2314\":1,\"2327\":1,\"2348\":1,\"2353\":1,\"2355\":2,\"2364\":1,\"2370\":2,\"2372\":1,\"2378\":1,\"2402\":1,\"2406\":1,\"2407\":1,\"2408\":3,\"2410\":1,\"2411\":4,\"2412\":4,\"2413\":2,\"2415\":2,\"2417\":2,\"2419\":2,\"2420\":1,\"2422\":1,\"2423\":4,\"2424\":2,\"2425\":2,\"2426\":2,\"2427\":3,\"2428\":5,\"2429\":2,\"2430\":2,\"2431\":3,\"2432\":4,\"2433\":2,\"2435\":4,\"2436\":1,\"2438\":1,\"2440\":1,\"2441\":1,\"2442\":1,\"2446\":3,\"2447\":4,\"2448\":2,\"2469\":1,\"2470\":1,\"2471\":1,\"2473\":1,\"2482\":1,\"2490\":1,\"2495\":1}}],[\"parameers\",{\"1\":{\"821\":1}}],[\"params=\",{\"1\":{\"1548\":1}}],[\"params\",{\"0\":{\"911\":1},\"1\":{\"286\":3,\"655\":2,\"656\":2,\"657\":2,\"659\":2,\"911\":1,\"921\":1,\"1389\":13,\"1390\":4,\"1391\":3,\"1392\":2,\"1396\":10,\"1397\":4,\"1401\":15,\"1402\":9,\"1403\":3,\"1408\":13,\"1409\":11,\"1420\":3,\"1450\":6,\"1452\":6,\"1454\":4,\"1456\":4,\"1458\":4,\"1460\":4,\"1466\":13,\"1467\":7,\"1468\":3,\"1502\":1,\"1511\":1,\"1513\":2,\"1526\":22,\"1548\":1,\"1549\":10,\"1551\":2,\"1553\":17,\"1582\":2,\"1592\":2,\"1593\":3,\"1594\":5,\"1595\":8,\"1596\":2,\"1597\":2,\"1598\":18,\"1599\":2,\"1600\":22,\"1604\":4,\"1605\":4,\"1606\":6,\"1609\":2,\"1610\":2,\"1614\":2,\"1615\":4,\"1618\":6,\"1619\":2,\"1624\":2,\"1625\":17,\"1962\":1,\"1963\":1}}],[\"paramters\",{\"1\":{\"262\":1,\"1612\":1,\"1613\":1}}],[\"param\",{\"0\":{\"2080\":1},\"1\":{\"51\":2,\"88\":7,\"89\":1,\"223\":1,\"243\":1,\"286\":1,\"290\":8,\"377\":2,\"628\":3,\"675\":2,\"699\":1,\"777\":5,\"921\":3,\"1119\":1,\"1156\":5,\"1301\":1,\"1306\":1,\"1368\":1,\"1371\":1,\"1372\":1,\"1469\":3,\"1533\":4,\"1730\":3,\"1768\":6,\"1824\":4,\"1907\":4,\"1940\":3,\"1941\":3,\"1942\":3,\"1943\":3,\"1962\":1,\"2314\":2,\"2439\":3}}],[\"partly\",{\"1\":{\"821\":1}}],[\"parts\",{\"1\":{\"52\":1,\"78\":1,\"140\":1,\"142\":1,\"145\":1,\"150\":1,\"197\":1,\"287\":1,\"600\":2,\"1124\":1,\"1280\":1,\"1283\":1,\"1526\":1,\"1600\":1}}],[\"particular\",{\"1\":{\"1050\":1,\"1116\":1,\"1161\":1,\"1189\":1,\"1218\":1,\"1221\":1,\"1229\":1,\"1244\":1}}],[\"particularly\",{\"1\":{\"3\":1,\"2000\":1}}],[\"partition\",{\"0\":{\"1374\":1},\"1\":{\"168\":2,\"1374\":2}}],[\"partialscorer\",{\"1\":{\"1799\":1}}],[\"partialscorerinterface\",{\"0\":{\"1804\":1},\"1\":{\"1723\":1,\"1799\":1,\"1804\":1}}],[\"partial=false\",{\"1\":{\"912\":1}}],[\"partial\",{\"1\":{\"50\":1,\"141\":12,\"145\":1,\"301\":2,\"421\":2,\"640\":2,\"661\":2,\"666\":2,\"1719\":5,\"1723\":2,\"1725\":6,\"1731\":2,\"1798\":1,\"1799\":2,\"1800\":2,\"1804\":3,\"1805\":1,\"1806\":2}}],[\"partiallyarhypothesis\",{\"0\":{\"1807\":1},\"1\":{\"1806\":7,\"1807\":2}}],[\"partiallyarbeamsearch\",{\"0\":{\"1806\":1},\"1\":{\"1806\":1}}],[\"partiallyarinference\",{\"0\":{\"795\":1},\"1\":{\"795\":1}}],[\"partially\",{\"0\":{\"308\":1,\"428\":1,\"795\":1,\"1806\":1,\"1807\":1},\"1\":{\"48\":1,\"692\":2,\"795\":2,\"1751\":2,\"1794\":2,\"1806\":4,\"1807\":2}}],[\"part\",{\"1\":{\"43\":3,\"52\":4,\"139\":2,\"197\":2,\"208\":1,\"286\":1,\"290\":1,\"536\":1,\"640\":1,\"666\":1,\"709\":1,\"774\":1,\"780\":1,\"824\":3,\"980\":1,\"1118\":2,\"1319\":1,\"1361\":1,\"1546\":1,\"1597\":1,\"1599\":1,\"1627\":1,\"1719\":7,\"1725\":17,\"1753\":2,\"1764\":1,\"1785\":1,\"1817\":1,\"1839\":1,\"1863\":4,\"1867\":3,\"1891\":3,\"1901\":2,\"1903\":2,\"1905\":1,\"1913\":4,\"1914\":1,\"1937\":3,\"2191\":1,\"2226\":1,\"2235\":1,\"2236\":1,\"2237\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2245\":1,\"2411\":1,\"2412\":1,\"2413\":1,\"2423\":1,\"2424\":1,\"2431\":1,\"2432\":1,\"2433\":1,\"2447\":1,\"2448\":1}}],[\"pattern\",{\"1\":{\"1938\":1,\"2151\":1,\"2310\":1,\"2325\":1,\"2327\":1,\"2355\":1}}],[\"patchembedding\",{\"0\":{\"1961\":1},\"1\":{\"1961\":1}}],[\"patch\",{\"1\":{\"737\":1,\"1959\":1}}],[\"patience=10\",{\"1\":{\"2020\":1}}],[\"patience\",{\"1\":{\"84\":1,\"243\":1,\"2020\":1,\"2348\":1,\"2359\":1,\"2370\":2,\"2372\":1}}],[\"patent\",{\"1\":{\"71\":1}}],[\"pathlib\",{\"1\":{\"2339\":2,\"2370\":1}}],[\"path=none\",{\"1\":{\"1948\":1}}],[\"path=0\",{\"1\":{\"828\":1,\"830\":1,\"1029\":1,\"1064\":1,\"1235\":1,\"1262\":1,\"1281\":1,\"1282\":1}}],[\"pathways\",{\"1\":{\"1124\":3,\"1125\":3}}],[\"paths\",{\"1\":{\"110\":1,\"150\":1,\"262\":1,\"505\":2,\"912\":1,\"1153\":1,\"1273\":1,\"1274\":1,\"2000\":1,\"2001\":1,\"2132\":2}}],[\"path>\",{\"1\":{\"79\":1,\"88\":1,\"2314\":1}}],[\"path\",{\"0\":{\"1952\":1,\"2079\":1},\"1\":{\"37\":1,\"50\":4,\"79\":15,\"80\":7,\"81\":5,\"82\":1,\"96\":4,\"97\":4,\"98\":8,\"99\":4,\"100\":4,\"101\":2,\"109\":2,\"110\":4,\"126\":3,\"134\":1,\"139\":1,\"162\":1,\"164\":1,\"168\":2,\"175\":1,\"196\":4,\"197\":2,\"212\":1,\"213\":3,\"224\":1,\"235\":1,\"236\":1,\"243\":8,\"259\":2,\"267\":15,\"268\":7,\"276\":11,\"277\":7,\"286\":10,\"290\":12,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"368\":2,\"372\":2,\"377\":2,\"385\":2,\"389\":2,\"396\":2,\"404\":4,\"406\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":2,\"457\":2,\"463\":2,\"469\":2,\"475\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":2,\"505\":2,\"514\":1,\"523\":1,\"527\":4,\"699\":4,\"706\":2,\"738\":1,\"745\":2,\"746\":4,\"747\":2,\"760\":6,\"761\":1,\"762\":1,\"820\":2,\"828\":2,\"830\":1,\"889\":1,\"890\":1,\"891\":1,\"930\":3,\"985\":1,\"987\":1,\"989\":1,\"991\":1,\"992\":5,\"994\":10,\"996\":1,\"997\":5,\"998\":2,\"999\":5,\"1003\":1,\"1004\":5,\"1005\":2,\"1006\":5,\"1008\":9,\"1009\":2,\"1010\":1,\"1013\":1,\"1015\":1,\"1016\":5,\"1017\":2,\"1018\":5,\"1019\":2,\"1021\":2,\"1022\":2,\"1023\":4,\"1024\":2,\"1026\":2,\"1027\":12,\"1029\":4,\"1053\":2,\"1064\":1,\"1128\":2,\"1133\":1,\"1134\":1,\"1136\":2,\"1137\":1,\"1139\":1,\"1141\":2,\"1153\":1,\"1162\":1,\"1185\":1,\"1235\":2,\"1252\":1,\"1262\":1,\"1280\":3,\"1281\":2,\"1282\":2,\"1548\":1,\"1656\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1872\":3,\"1876\":2,\"1949\":1,\"1951\":1,\"1952\":1,\"1953\":1,\"1954\":4,\"1955\":2,\"1965\":1,\"1997\":1,\"2000\":3,\"2001\":1,\"2128\":1,\"2132\":1,\"2134\":6,\"2137\":1,\"2138\":3,\"2141\":1,\"2144\":1,\"2156\":3,\"2157\":2,\"2160\":1,\"2161\":1,\"2249\":3,\"2258\":2,\"2263\":2,\"2268\":2,\"2270\":2,\"2271\":2,\"2275\":1,\"2278\":1,\"2279\":1,\"2285\":1,\"2288\":1,\"2291\":1,\"2292\":1,\"2293\":2,\"2336\":3,\"2337\":3,\"2338\":1,\"2339\":4,\"2342\":1,\"2344\":1,\"2348\":1,\"2349\":1,\"2351\":1,\"2353\":3,\"2356\":3,\"2359\":1,\"2360\":3,\"2361\":3,\"2362\":4,\"2363\":3,\"2364\":3,\"2366\":1,\"2369\":2,\"2370\":2,\"2372\":1,\"2390\":1,\"2391\":1,\"2393\":1,\"2396\":1,\"2397\":1,\"2399\":1,\"2422\":2,\"2462\":1}}],[\"patrick\",{\"1\":{\"10\":1}}],[\"paola\",{\"1\":{\"14\":1}}],[\"pavel\",{\"1\":{\"12\":1}}],[\"page\",{\"1\":{\"3\":1,\"260\":1,\"267\":1,\"276\":1,\"286\":1,\"290\":1,\"2168\":1}}],[\"pages=\",{\"1\":{\"207\":1}}],[\"pages\",{\"1\":{\"3\":1,\"156\":1,\"202\":1}}],[\"population\",{\"1\":{\"1701\":1}}],[\"poisson\",{\"1\":{\"846\":2,\"2220\":2}}],[\"pointwise\",{\"1\":{\"768\":3,\"1168\":1}}],[\"pointer\",{\"1\":{\"716\":1,\"755\":1,\"1705\":1}}],[\"pointing\",{\"1\":{\"224\":1}}],[\"point\",{\"1\":{\"67\":2,\"109\":1,\"154\":1,\"224\":1,\"265\":1,\"269\":1,\"274\":1,\"278\":1,\"286\":1,\"516\":2,\"523\":1,\"537\":2,\"831\":1,\"1145\":2,\"1271\":1,\"1556\":2,\"1678\":1,\"1705\":1,\"2043\":1,\"2055\":1,\"2056\":1,\"2065\":1,\"2066\":1,\"2101\":1,\"2153\":1}}],[\"points\",{\"0\":{\"2\":1},\"1\":{\"267\":1,\"516\":1,\"523\":1,\"620\":1,\"1607\":1,\"1645\":1,\"1650\":1,\"1736\":1,\"1883\":1,\"2482\":3,\"2490\":3,\"2495\":1}}],[\"pool=false\",{\"1\":{\"2179\":1}}],[\"pool=none\",{\"1\":{\"820\":1,\"828\":1,\"830\":1}}],[\"pooling\",{\"0\":{\"2172\":2,\"2183\":2,\"2190\":2,\"2208\":2},\"1\":{\"768\":1,\"770\":1,\"820\":2,\"828\":2,\"830\":1,\"959\":1,\"1301\":1,\"1306\":1,\"1401\":2,\"1402\":6,\"1408\":2,\"1409\":6,\"1466\":2,\"1467\":6,\"1526\":2,\"1549\":6,\"1553\":2,\"1594\":6,\"1595\":6,\"1598\":2,\"1600\":2,\"1606\":6,\"1625\":2,\"1702\":2,\"2172\":2,\"2176\":1,\"2183\":3,\"2184\":5,\"2188\":1,\"2190\":3,\"2208\":3}}],[\"pool\",{\"0\":{\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"859\":1,\"860\":1,\"892\":1,\"945\":1},\"1\":{\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"807\":1,\"820\":1,\"828\":1,\"830\":1,\"859\":1,\"860\":1,\"892\":1,\"945\":1,\"2458\":1,\"2463\":2,\"2464\":2}}],[\"poorest\",{\"1\":{\"242\":1}}],[\"poorly\",{\"1\":{\"32\":1}}],[\"powers=none\",{\"1\":{\"1354\":1}}],[\"powers\",{\"1\":{\"1126\":2,\"1354\":2}}],[\"power=0\",{\"1\":{\"718\":1}}],[\"power\",{\"0\":{\"926\":1,\"1325\":1,\"1326\":1,\"2000\":1,\"2001\":1},\"1\":{\"537\":1,\"606\":1,\"689\":2,\"720\":1,\"926\":1,\"1127\":1,\"1217\":1,\"1309\":2,\"1310\":1,\"1311\":2,\"1314\":2,\"1315\":3,\"1318\":5,\"1322\":1,\"1323\":1,\"1325\":3,\"1326\":2,\"1327\":1,\"1328\":6,\"1330\":1,\"1377\":4,\"1419\":2,\"1645\":4,\"2000\":2,\"2001\":3,\"2378\":1}}],[\"polarity\",{\"0\":{\"1693\":1},\"1\":{\"1693\":1}}],[\"polak\",{\"1\":{\"10\":1}}],[\"pole\",{\"1\":{\"1673\":1}}],[\"poly1d\",{\"0\":{\"1353\":1},\"1\":{\"1353\":1}}],[\"polyfit\",{\"1\":{\"1128\":1}}],[\"polyphone\",{\"1\":{\"269\":1,\"278\":1}}],[\"polish\",{\"1\":{\"287\":1,\"481\":1}}],[\"portable\",{\"1\":{\"1656\":1}}],[\"ported\",{\"1\":{\"922\":1,\"936\":1,\"937\":1,\"1153\":1,\"1308\":1,\"1332\":1,\"1373\":1}}],[\"portion\",{\"1\":{\"276\":1}}],[\"ports\",{\"1\":{\"66\":1}}],[\"port=<any\",{\"1\":{\"60\":1}}],[\"port>\",{\"1\":{\"58\":2,\"60\":3,\"61\":3}}],[\"port\",{\"0\":{\"197\":1,\"2380\":1,\"2383\":1},\"1\":{\"58\":2,\"60\":1,\"61\":3,\"62\":1,\"194\":1,\"197\":1,\"290\":1,\"374\":2,\"377\":2,\"449\":2,\"2340\":2,\"2380\":5,\"2383\":1}}],[\"positive\",{\"1\":{\"1308\":1,\"1839\":1,\"2134\":1,\"2151\":1,\"2167\":1,\"2176\":1,\"2245\":1,\"2310\":1,\"2355\":2,\"2431\":1,\"2432\":1}}],[\"positionwisefeedforward\",{\"0\":{\"1809\":1},\"1\":{\"1735\":1,\"1751\":1,\"1759\":1,\"1809\":2,\"1847\":1}}],[\"positionwise\",{\"0\":{\"1809\":1},\"1\":{\"243\":1,\"709\":6,\"710\":5,\"711\":5,\"733\":1,\"734\":1,\"774\":6,\"780\":6,\"849\":5,\"1107\":6,\"1278\":5,\"1526\":2,\"1535\":4,\"1536\":6,\"1546\":7,\"1552\":4,\"1553\":2,\"1598\":2,\"1599\":2,\"1600\":2,\"1622\":7,\"1625\":2,\"1626\":4,\"1749\":6,\"1795\":1,\"1809\":2,\"1863\":6,\"1864\":2,\"1867\":2,\"2126\":2,\"2129\":5,\"2191\":6,\"2239\":4,\"2240\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2432\":4,\"2447\":2}}],[\"positions\",{\"1\":{\"142\":2,\"633\":2,\"634\":2,\"646\":3,\"647\":8,\"674\":2,\"699\":3,\"710\":3,\"711\":3,\"771\":1,\"849\":1,\"2129\":1}}],[\"position\",{\"0\":{\"818\":1},\"1\":{\"141\":2,\"142\":2,\"633\":2,\"634\":2,\"646\":2,\"647\":2,\"661\":1,\"692\":1,\"709\":1,\"710\":4,\"711\":4,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"771\":2,\"774\":1,\"780\":1,\"787\":2,\"846\":1,\"849\":2,\"1107\":1,\"1278\":1,\"1290\":2,\"1387\":1,\"1519\":3,\"1529\":3,\"1535\":3,\"1552\":2,\"1626\":2,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1785\":1,\"1808\":1,\"1817\":1,\"1992\":1,\"1995\":1,\"2124\":1,\"2128\":1,\"2129\":2,\"2191\":1,\"2432\":2}}],[\"positionalencoding\",{\"0\":{\"1808\":1},\"1\":{\"692\":2,\"710\":1,\"711\":1,\"731\":1,\"732\":1,\"766\":1,\"767\":1,\"775\":1,\"848\":1,\"849\":2,\"850\":1,\"1786\":1,\"1808\":2,\"1818\":1,\"1820\":1,\"1837\":1,\"1891\":1,\"1957\":2,\"1960\":2,\"1961\":2,\"1992\":2,\"1995\":2,\"2129\":2}}],[\"positional\",{\"0\":{\"375\":1,\"539\":1,\"546\":1,\"549\":1,\"552\":1,\"555\":1,\"557\":1,\"559\":1,\"562\":1,\"565\":1,\"568\":1,\"571\":1,\"573\":1,\"576\":1,\"579\":1,\"582\":1,\"584\":1,\"587\":1,\"590\":1,\"593\":1,\"601\":1,\"604\":1,\"607\":1,\"610\":1,\"612\":1,\"645\":1,\"646\":1,\"647\":1,\"662\":1},\"1\":{\"43\":2,\"130\":1,\"141\":2,\"243\":2,\"617\":4,\"618\":4,\"620\":7,\"624\":4,\"636\":2,\"644\":5,\"645\":5,\"646\":1,\"647\":4,\"661\":2,\"662\":4,\"674\":2,\"692\":1,\"699\":5,\"700\":1,\"709\":6,\"710\":4,\"711\":4,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"766\":1,\"767\":1,\"774\":6,\"775\":1,\"780\":6,\"846\":2,\"848\":1,\"849\":3,\"850\":1,\"1107\":4,\"1278\":4,\"1519\":4,\"1526\":2,\"1535\":6,\"1536\":6,\"1546\":7,\"1552\":6,\"1553\":2,\"1598\":2,\"1599\":8,\"1600\":2,\"1622\":7,\"1625\":2,\"1626\":7,\"1736\":3,\"1748\":2,\"1749\":3,\"1784\":4,\"1785\":3,\"1786\":3,\"1808\":3,\"1817\":3,\"1818\":3,\"1820\":2,\"1837\":3,\"1851\":1,\"1863\":3,\"1866\":1,\"1891\":3,\"1947\":1,\"1957\":3,\"1960\":3,\"1961\":3,\"1992\":1,\"1995\":1,\"2126\":1,\"2129\":3,\"2191\":6,\"2239\":10,\"2240\":8,\"2411\":8,\"2412\":8,\"2423\":7,\"2432\":8,\"2447\":8}}],[\"pos=false\",{\"1\":{\"787\":1}}],[\"posenc\",{\"0\":{\"1746\":1},\"1\":{\"1746\":1}}],[\"pose\",{\"1\":{\"290\":1}}],[\"possibly\",{\"1\":{\"1050\":1,\"1116\":1,\"1161\":1,\"1189\":1,\"1218\":1,\"1221\":1,\"1229\":1,\"1244\":1,\"2184\":1}}],[\"possible\",{\"1\":{\"26\":1,\"46\":1,\"99\":1,\"100\":1,\"135\":1,\"223\":1,\"224\":1,\"242\":1,\"262\":1,\"290\":1,\"703\":2,\"755\":2,\"785\":2,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"919\":2,\"1155\":1,\"1157\":1}}],[\"possibilities\",{\"1\":{\"104\":1}}],[\"postdecoder\",{\"0\":{\"2124\":2,\"2128\":2},\"1\":{\"2124\":2,\"2127\":1,\"2128\":2}}],[\"posteriorencoder\",{\"0\":{\"1611\":1},\"1\":{\"1611\":2}}],[\"posterior\",{\"0\":{\"1611\":1,\"1934\":1},\"1\":{\"1552\":21,\"1553\":6,\"1601\":1,\"1602\":2,\"1611\":3,\"1625\":6,\"1626\":21,\"1730\":2,\"1934\":2}}],[\"postencoder\",{\"0\":{\"682\":2,\"762\":2,\"765\":2,\"2126\":2,\"2129\":2},\"1\":{\"138\":1,\"682\":2,\"736\":1,\"737\":1,\"761\":1,\"762\":3,\"765\":3,\"777\":1,\"1959\":1,\"1975\":1,\"1997\":1,\"2126\":3,\"2127\":2,\"2128\":1,\"2129\":2,\"2221\":1}}],[\"postnets\",{\"1\":{\"1764\":2,\"1839\":2,\"1991\":2,\"2226\":2,\"2237\":2,\"2241\":2,\"2413\":2,\"2424\":2,\"2448\":2}}],[\"postnet\",{\"0\":{\"1810\":1},\"1\":{\"1526\":4,\"1600\":4,\"1750\":11,\"1810\":5,\"1993\":9,\"1994\":3,\"2223\":11,\"2235\":12,\"2236\":12,\"2239\":12,\"2240\":12,\"2245\":9,\"2411\":12,\"2412\":12,\"2423\":12,\"2431\":9,\"2432\":12,\"2447\":12}}],[\"postfrontends=none\",{\"1\":{\"1522\":1}}],[\"postfrontend\",{\"1\":{\"1521\":1,\"1539\":1}}],[\"postfix\",{\"1\":{\"760\":4}}],[\"postprocess\",{\"1\":{\"1400\":1}}],[\"postact\",{\"1\":{\"818\":1}}],[\"postact=\",{\"1\":{\"817\":1}}],[\"posting\",{\"1\":{\"153\":1}}],[\"post\",{\"0\":{\"1522\":1,\"1539\":1},\"1\":{\"66\":1,\"738\":1,\"820\":1,\"828\":1,\"1141\":1,\"1145\":2,\"1293\":1,\"1522\":1,\"1539\":1,\"1719\":2,\"1725\":2,\"1806\":3,\"2366\":1}}],[\"pos\",{\"0\":{\"1891\":1},\"1\":{\"43\":2,\"130\":1,\"141\":7,\"142\":2,\"243\":3,\"290\":2,\"617\":6,\"618\":6,\"620\":10,\"624\":6,\"633\":2,\"634\":2,\"636\":4,\"644\":6,\"645\":1,\"661\":6,\"674\":6,\"692\":2,\"700\":3,\"701\":3,\"709\":6,\"710\":4,\"711\":4,\"731\":1,\"732\":1,\"733\":4,\"734\":4,\"735\":3,\"766\":1,\"767\":1,\"774\":5,\"775\":1,\"780\":6,\"846\":4,\"848\":1,\"849\":2,\"850\":1,\"851\":2,\"1107\":3,\"1278\":2,\"1484\":1,\"1519\":3,\"1526\":4,\"1535\":1,\"1536\":2,\"1546\":1,\"1552\":1,\"1553\":1,\"1598\":4,\"1599\":12,\"1600\":4,\"1622\":1,\"1625\":1,\"1626\":1,\"1736\":4,\"1738\":2,\"1739\":2,\"1740\":2,\"1741\":2,\"1742\":2,\"1743\":2,\"1744\":2,\"1745\":2,\"1749\":1,\"1784\":2,\"1785\":2,\"1817\":2,\"1839\":2,\"1851\":2,\"1863\":4,\"1866\":2,\"1891\":4,\"1914\":3,\"1934\":1,\"1947\":1,\"1957\":2,\"1960\":2,\"1961\":2,\"1991\":1,\"1992\":2,\"1994\":3,\"1995\":2,\"2126\":3,\"2129\":2,\"2191\":6,\"2239\":9,\"2240\":8,\"2245\":2,\"2363\":2,\"2411\":12,\"2412\":12,\"2423\":12,\"2431\":2,\"2432\":7,\"2447\":12}}],[\"po\",{\"1\":{\"14\":1,\"2427\":1}}],[\"potentially\",{\"1\":{\"1084\":1,\"1350\":1}}],[\"potential\",{\"1\":{\"3\":1}}],[\"pysize\",{\"1\":{\"2489\":1}}],[\"pyscripts\",{\"1\":{\"109\":1,\"212\":1,\"267\":1,\"276\":1,\"286\":9}}],[\"pyf98\",{\"1\":{\"2055\":2}}],[\"pydub\",{\"1\":{\"2054\":1}}],[\"pyopenjtalk\",{\"0\":{\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":1},\"1\":{\"163\":2,\"269\":2,\"271\":2,\"278\":2,\"280\":2,\"287\":10,\"290\":1,\"481\":5,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":1,\"2299\":2}}],[\"pyproject\",{\"1\":{\"162\":3}}],[\"pypinyin\",{\"0\":{\"2300\":1,\"2301\":1,\"2302\":1},\"1\":{\"269\":1,\"278\":1,\"287\":2,\"481\":3,\"2300\":1,\"2301\":1,\"2302\":1}}],[\"pypi\",{\"1\":{\"153\":1}}],[\"pytest\",{\"1\":{\"1965\":1}}],[\"pytorchlightning\",{\"1\":{\"2151\":1,\"2310\":1}}],[\"pytorch1\",{\"1\":{\"159\":1}}],[\"pytorch\",{\"0\":{\"53\":1,\"82\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1733\":1,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1764\":1,\"1766\":1,\"1768\":1,\"1769\":1,\"1770\":1,\"1771\":1,\"1779\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1801\":1,\"1803\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1820\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1842\":1,\"1847\":1,\"1849\":1,\"1851\":1,\"1854\":1,\"1855\":1,\"1858\":1,\"1860\":1,\"1861\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1885\":1,\"1886\":1,\"1888\":1,\"1891\":1,\"1892\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1907\":1,\"1908\":1,\"1910\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1926\":1,\"1927\":1,\"1928\":1,\"1931\":1,\"1932\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"1937\":1,\"2317\":2},\"1\":{\"29\":2,\"38\":3,\"42\":1,\"46\":2,\"50\":1,\"53\":1,\"60\":1,\"79\":1,\"96\":1,\"104\":2,\"162\":2,\"526\":1,\"527\":3,\"635\":1,\"652\":1,\"692\":5,\"710\":1,\"711\":1,\"731\":1,\"732\":1,\"747\":1,\"766\":1,\"767\":1,\"775\":1,\"790\":2,\"846\":1,\"848\":1,\"849\":1,\"850\":3,\"929\":1,\"1050\":4,\"1116\":4,\"1153\":1,\"1161\":4,\"1189\":4,\"1218\":4,\"1221\":4,\"1229\":4,\"1244\":4,\"1347\":1,\"1411\":2,\"1645\":1,\"1650\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1731\":1,\"1733\":1,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":2,\"1757\":2,\"1758\":1,\"1759\":1,\"1764\":1,\"1766\":1,\"1768\":1,\"1770\":1,\"1771\":1,\"1779\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1788\":1,\"1789\":2,\"1790\":2,\"1794\":2,\"1795\":1,\"1796\":1,\"1801\":1,\"1803\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1820\":1,\"1822\":5,\"1837\":1,\"1838\":1,\"1839\":1,\"1842\":1,\"1847\":1,\"1849\":1,\"1851\":1,\"1854\":1,\"1855\":2,\"1856\":1,\"1858\":1,\"1860\":1,\"1861\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1885\":1,\"1886\":1,\"1888\":1,\"1891\":1,\"1892\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1901\":3,\"1903\":3,\"1905\":1,\"1907\":1,\"1908\":1,\"1910\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1926\":1,\"1927\":1,\"1928\":1,\"1931\":1,\"1932\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"1937\":1,\"1945\":1,\"1957\":1,\"1960\":1,\"1961\":1,\"1963\":1,\"1992\":3,\"1995\":1,\"2016\":2,\"2129\":1,\"2131\":3,\"2151\":2,\"2310\":2,\"2317\":2,\"2342\":1,\"2351\":1,\"2355\":3,\"2427\":1,\"2439\":1}}],[\"python3\",{\"1\":{\"162\":3,\"164\":1,\"212\":1,\"1701\":1,\"2311\":1}}],[\"python=3\",{\"1\":{\"31\":1}}],[\"python\",{\"1\":{\"1\":4,\"3\":1,\"5\":1,\"19\":1,\"31\":1,\"37\":1,\"56\":2,\"57\":1,\"58\":2,\"61\":3,\"62\":1,\"63\":1,\"64\":1,\"71\":1,\"78\":1,\"79\":1,\"80\":1,\"81\":3,\"82\":1,\"84\":4,\"85\":2,\"86\":2,\"87\":1,\"88\":6,\"89\":1,\"90\":1,\"91\":1,\"92\":1,\"93\":2,\"94\":4,\"96\":1,\"97\":1,\"98\":2,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":1,\"107\":2,\"109\":1,\"110\":5,\"153\":1,\"154\":4,\"159\":1,\"162\":15,\"163\":6,\"164\":1,\"201\":6,\"223\":5,\"225\":1,\"228\":2,\"243\":1,\"263\":3,\"284\":1,\"287\":4,\"290\":8,\"907\":1,\"960\":1,\"1350\":1,\"1701\":1}}],[\"py\",{\"0\":{\"68\":1,\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"374\":1,\"377\":1,\"385\":1,\"389\":1,\"396\":1,\"402\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":1,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"511\":1,\"538\":1,\"541\":1,\"543\":1,\"545\":1,\"548\":1,\"551\":1,\"554\":1,\"556\":1,\"558\":1,\"561\":1,\"564\":1,\"567\":1,\"570\":1,\"572\":1,\"575\":1,\"578\":1,\"581\":1,\"583\":1,\"586\":1,\"589\":1,\"592\":1,\"594\":1,\"596\":1,\"598\":1,\"600\":1,\"603\":1,\"606\":1,\"609\":1,\"611\":1,\"2513\":1},\"1\":{\"3\":3,\"32\":4,\"41\":1,\"46\":1,\"47\":4,\"49\":1,\"68\":5,\"71\":2,\"119\":1,\"133\":1,\"134\":1,\"136\":1,\"141\":1,\"150\":2,\"162\":1,\"164\":1,\"201\":6,\"212\":1,\"223\":65,\"225\":14,\"228\":9,\"243\":1,\"246\":1,\"263\":6,\"267\":1,\"269\":3,\"276\":1,\"278\":3,\"286\":9,\"290\":6,\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"374\":1,\"377\":1,\"385\":1,\"389\":1,\"396\":1,\"402\":1,\"403\":14,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":1,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"511\":1,\"538\":1,\"541\":1,\"543\":1,\"545\":1,\"548\":1,\"551\":1,\"554\":1,\"556\":1,\"558\":1,\"561\":1,\"564\":1,\"567\":1,\"570\":1,\"572\":1,\"575\":1,\"578\":1,\"581\":1,\"583\":1,\"586\":1,\"589\":1,\"592\":1,\"594\":1,\"596\":1,\"598\":1,\"600\":1,\"603\":1,\"606\":1,\"609\":1,\"611\":1,\"661\":1,\"669\":1,\"670\":1,\"733\":1,\"737\":1,\"747\":1,\"750\":1,\"768\":1,\"794\":1,\"974\":1,\"1051\":1,\"1053\":1,\"1153\":1,\"1155\":3,\"1156\":3,\"1157\":3,\"1158\":1,\"1308\":1,\"1316\":1,\"1320\":1,\"1332\":1,\"1334\":1,\"1350\":1,\"1513\":1,\"1548\":1,\"1551\":1,\"1592\":1,\"1605\":1,\"1606\":1,\"1640\":1,\"1641\":1,\"1701\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1,\"1945\":1,\"1959\":1,\"1975\":1,\"1997\":1,\"2018\":1,\"2127\":1,\"2141\":1,\"2151\":1,\"2198\":1,\"2221\":1,\"2235\":1,\"2236\":1,\"2286\":1,\"2310\":1,\"2311\":1,\"2354\":3,\"2435\":1}}],[\"pruning\",{\"1\":{\"1730\":1}}],[\"pruned\",{\"1\":{\"139\":6,\"625\":7,\"1731\":1,\"1804\":1,\"1822\":1,\"1966\":1}}],[\"prune\",{\"1\":{\"45\":1,\"139\":1,\"145\":1,\"616\":2,\"696\":1,\"697\":1,\"1920\":2}}],[\"prs\",{\"1\":{\"290\":1}}],[\"pr\",{\"1\":{\"34\":1,\"211\":1,\"223\":2,\"260\":1,\"290\":1,\"696\":1,\"697\":1}}],[\"preservation`\",{\"1\":{\"1994\":2}}],[\"preserving\",{\"1\":{\"1225\":1,\"1855\":1}}],[\"presented\",{\"1\":{\"260\":1}}],[\"present\",{\"1\":{\"211\":1}}],[\"press\",{\"1\":{\"1126\":1}}],[\"prenorm=true\",{\"1\":{\"828\":1,\"830\":1}}],[\"prenorm\",{\"1\":{\"820\":2,\"828\":1}}],[\"prenet\",{\"0\":{\"1811\":1,\"1985\":1},\"1\":{\"290\":1,\"1599\":1,\"1750\":6,\"1811\":4,\"1812\":3,\"1985\":2,\"1993\":6,\"1994\":3,\"2223\":6,\"2235\":4,\"2236\":4,\"2239\":1,\"2240\":1,\"2245\":6,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":6,\"2432\":8,\"2447\":1}}],[\"precomputed\",{\"1\":{\"768\":1}}],[\"precisely\",{\"1\":{\"286\":1}}],[\"precision=16\",{\"1\":{\"2355\":1}}],[\"precision=true\",{\"1\":{\"924\":1}}],[\"precision\",{\"0\":{\"103\":1,\"2090\":1},\"1\":{\"70\":1,\"748\":1,\"1126\":1,\"1127\":1,\"2043\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2355\":1}}],[\"preemp\",{\"1\":{\"778\":1}}],[\"preempahsis\",{\"1\":{\"691\":1}}],[\"preemphasis\",{\"0\":{\"1694\":1},\"1\":{\"1694\":1}}],[\"preemph\",{\"1\":{\"691\":2}}],[\"preencoder\",{\"0\":{\"684\":2,\"768\":1,\"772\":1,\"832\":1},\"1\":{\"128\":5,\"138\":1,\"684\":2,\"691\":1,\"736\":1,\"737\":1,\"768\":2,\"772\":2,\"777\":1,\"832\":1,\"954\":1,\"958\":1,\"1640\":1,\"1641\":1,\"1959\":1,\"1975\":1,\"1997\":1,\"2127\":1,\"2216\":1,\"2221\":1}}],[\"prelu\",{\"1\":{\"674\":2,\"1182\":1,\"1183\":1,\"1184\":1,\"1267\":1,\"1268\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1274\":1}}],[\"pretraining\",{\"1\":{\"748\":2}}],[\"pretrain\",{\"1\":{\"267\":1,\"675\":2,\"746\":1,\"748\":2,\"846\":3,\"1640\":1,\"1641\":1}}],[\"pretrained\",{\"0\":{\"88\":1,\"125\":1,\"281\":1,\"1944\":1,\"2308\":1,\"2314\":2,\"2422\":1},\"1\":{\"125\":2,\"136\":1,\"193\":1,\"220\":1,\"222\":2,\"223\":3,\"267\":7,\"273\":1,\"274\":2,\"275\":3,\"276\":7,\"284\":3,\"285\":1,\"286\":10,\"289\":1,\"290\":21,\"536\":2,\"699\":4,\"739\":1,\"745\":3,\"746\":2,\"747\":4,\"748\":1,\"759\":3,\"760\":3,\"761\":1,\"762\":1,\"815\":1,\"816\":1,\"846\":1,\"1053\":4,\"1539\":2,\"1540\":1,\"1944\":2,\"2137\":1,\"2140\":1,\"2308\":1,\"2314\":2,\"2315\":5,\"2422\":2}}],[\"prevents\",{\"1\":{\"787\":1}}],[\"prevent\",{\"1\":{\"691\":1,\"747\":1,\"2220\":1}}],[\"prev\",{\"1\":{\"242\":7,\"243\":3,\"637\":1,\"678\":1,\"699\":2,\"700\":2,\"709\":2,\"710\":6,\"711\":6,\"733\":2,\"734\":2,\"745\":2,\"746\":2,\"747\":2,\"748\":2,\"771\":2,\"774\":2,\"780\":2,\"791\":1,\"797\":2,\"798\":1,\"846\":2,\"849\":2,\"862\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":4,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1719\":2,\"1725\":2,\"1768\":2,\"1801\":2,\"1806\":2,\"1814\":2,\"1816\":2,\"1996\":8,\"1997\":6,\"2129\":2,\"2360\":3,\"2361\":3}}],[\"previously\",{\"1\":{\"51\":1,\"127\":1,\"285\":1,\"2044\":3}}],[\"previous\",{\"1\":{\"26\":1,\"60\":1,\"148\":1,\"190\":1,\"240\":1,\"242\":1,\"243\":1,\"617\":2,\"618\":2,\"620\":2,\"624\":2,\"626\":2,\"636\":2,\"637\":2,\"644\":4,\"645\":2,\"672\":3,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"1704\":1,\"1705\":2,\"1706\":2,\"1707\":1,\"1708\":2,\"1709\":3,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":1,\"1714\":1,\"1715\":2,\"1716\":2,\"1725\":1,\"1735\":2,\"1750\":1,\"1768\":2,\"1798\":2,\"1799\":2,\"1800\":2,\"1801\":1,\"1814\":1,\"1816\":1,\"1847\":1,\"1993\":1,\"2223\":1,\"2245\":1,\"2431\":1}}],[\"preview\",{\"1\":{\"3\":1}}],[\"preds\",{\"1\":{\"2167\":1,\"2207\":1}}],[\"predcited\",{\"1\":{\"1217\":1}}],[\"predefined\",{\"1\":{\"233\":1}}],[\"pred\",{\"1\":{\"201\":1,\"286\":1,\"627\":7,\"706\":1,\"740\":7,\"948\":3,\"951\":1,\"974\":3,\"1598\":4,\"1625\":4,\"1638\":4,\"1640\":2,\"1702\":1,\"2176\":1}}],[\"predicts\",{\"1\":{\"1752\":1,\"1810\":1}}],[\"predicting\",{\"1\":{\"2443\":1}}],[\"predictive\",{\"1\":{\"1638\":2}}],[\"predictions\",{\"1\":{\"235\":1,\"960\":2,\"1702\":1,\"1750\":1,\"1758\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"2431\":1}}],[\"prediction\",{\"1\":{\"134\":3,\"135\":1,\"201\":1,\"235\":1,\"240\":1,\"286\":2,\"543\":1,\"627\":4,\"740\":4,\"828\":1,\"829\":1,\"830\":1,\"947\":1,\"948\":1,\"949\":1,\"1556\":1,\"1598\":1,\"1625\":1,\"1626\":1,\"1750\":3,\"1754\":1,\"1758\":2,\"1760\":2,\"1782\":1,\"1810\":2,\"1811\":2,\"1928\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"2220\":1,\"2223\":3,\"2227\":2,\"2231\":2,\"2236\":1,\"2240\":1,\"2245\":1,\"2412\":1,\"2431\":1}}],[\"predicted\",{\"1\":{\"211\":1,\"235\":1,\"286\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"978\":1,\"1053\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1125\":1,\"1130\":2,\"1131\":1,\"1136\":1,\"1141\":1,\"1162\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1268\":1,\"1278\":1,\"1280\":1,\"1283\":1,\"1515\":1,\"1520\":1,\"1529\":4,\"1535\":1,\"1556\":2,\"1598\":2,\"1599\":3,\"1625\":2,\"1753\":2,\"1760\":1,\"1788\":1,\"1810\":1,\"1971\":1,\"1992\":1,\"2167\":1,\"2176\":1,\"2207\":1,\"2222\":1,\"2354\":3,\"2403\":1,\"2433\":1,\"2445\":1,\"2469\":2,\"2471\":1}}],[\"predict\",{\"1\":{\"200\":1,\"225\":1,\"235\":1,\"290\":1,\"421\":2,\"1062\":3,\"1107\":2,\"1125\":2,\"1126\":2,\"1127\":2,\"1130\":2,\"1136\":2,\"1141\":2,\"1155\":1,\"1157\":1,\"1162\":2,\"1232\":2,\"1261\":1,\"1267\":2,\"1278\":2,\"2226\":3,\"2235\":1,\"2236\":1,\"2239\":3}}],[\"predictor=\",{\"1\":{\"1253\":1}}],[\"predictors\",{\"0\":{\"1161\":1,\"1221\":1,\"1229\":1,\"1244\":1},\"1\":{\"1161\":1,\"1221\":1,\"1229\":1,\"1244\":1}}],[\"predictor\",{\"0\":{\"1229\":1,\"1519\":1,\"1520\":1,\"1535\":1,\"1616\":1,\"1752\":1,\"1754\":1,\"2433\":1},\"1\":{\"46\":1,\"1050\":1,\"1161\":2,\"1221\":3,\"1229\":3,\"1244\":2,\"1253\":3,\"1254\":1,\"1519\":1,\"1520\":3,\"1526\":5,\"1535\":2,\"1552\":4,\"1553\":2,\"1598\":14,\"1599\":42,\"1600\":14,\"1616\":3,\"1625\":5,\"1626\":13,\"1627\":6,\"1752\":4,\"1753\":1,\"1754\":3,\"1764\":2,\"1974\":2,\"1994\":3,\"2226\":2,\"2236\":12,\"2239\":12,\"2240\":12,\"2241\":4,\"2411\":13,\"2412\":42,\"2413\":6,\"2423\":42,\"2424\":6,\"2433\":4,\"2447\":42,\"2448\":6}}],[\"prepended\",{\"1\":{\"2039\":1}}],[\"prepend\",{\"1\":{\"2039\":1}}],[\"prep\",{\"1\":{\"118\":1,\"124\":2,\"136\":1,\"190\":1,\"196\":1,\"211\":1,\"266\":1,\"267\":1,\"269\":3,\"275\":1,\"276\":1,\"278\":3,\"286\":3}}],[\"preprocessing\",{\"1\":{\"106\":2,\"242\":1,\"561\":1,\"2130\":3,\"2131\":5,\"2133\":1,\"2143\":2,\"2262\":1,\"2344\":1}}],[\"preprocessor\",{\"0\":{\"2328\":1,\"2336\":1,\"2337\":1,\"2341\":1,\"2346\":1,\"2353\":1,\"2356\":1,\"2357\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2364\":1,\"2368\":1,\"2375\":1,\"2378\":1,\"2379\":1},\"1\":{\"101\":2,\"223\":2,\"243\":2,\"377\":2,\"449\":2,\"2131\":6,\"2142\":1,\"2143\":1,\"2149\":3,\"2166\":1,\"2328\":1,\"2336\":1,\"2337\":1,\"2341\":1,\"2346\":2,\"2353\":2,\"2356\":1,\"2357\":2,\"2360\":2,\"2361\":1,\"2362\":1,\"2363\":2,\"2364\":2,\"2368\":2,\"2375\":1,\"2378\":1,\"2379\":1}}],[\"preprocess\",{\"1\":{\"78\":1,\"276\":1,\"521\":2,\"561\":2,\"567\":2,\"699\":2,\"1400\":1,\"1421\":1,\"2130\":3,\"2133\":2,\"2136\":2,\"2137\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":2,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":2,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2342\":1,\"2351\":2,\"2366\":1}}],[\"preprint\",{\"1\":{\"9\":1,\"207\":1,\"244\":1,\"1269\":2}}],[\"preparation\",{\"0\":{\"269\":1,\"278\":1},\"1\":{\"38\":2,\"107\":1,\"111\":1,\"118\":2,\"120\":1,\"174\":1,\"197\":1,\"199\":1,\"200\":2,\"204\":1,\"205\":3,\"210\":2,\"211\":6,\"216\":2,\"217\":4,\"222\":2,\"223\":1,\"224\":1,\"227\":2,\"228\":2,\"234\":1,\"235\":1,\"236\":1,\"240\":1,\"242\":4,\"243\":2,\"253\":1,\"254\":2,\"265\":3,\"266\":7,\"267\":6,\"269\":1,\"274\":3,\"275\":8,\"276\":7,\"278\":1,\"284\":2,\"285\":5,\"2152\":1,\"2153\":1,\"2355\":1}}],[\"prepares\",{\"1\":{\"235\":1,\"285\":1,\"2133\":1}}],[\"prepared\",{\"1\":{\"70\":1,\"242\":1,\"243\":2,\"286\":1,\"290\":1}}],[\"prepare\",{\"0\":{\"1354\":1,\"1913\":1,\"1914\":1,\"2149\":1,\"2150\":1,\"2152\":1,\"2153\":1,\"2156\":1,\"2157\":2,\"2158\":2,\"2159\":1,\"2160\":1,\"2161\":1,\"2165\":1,\"2166\":1},\"1\":{\"38\":1,\"69\":1,\"96\":2,\"160\":1,\"190\":1,\"197\":4,\"200\":1,\"205\":1,\"222\":2,\"224\":4,\"240\":2,\"243\":4,\"269\":1,\"278\":1,\"286\":2,\"1354\":2,\"1644\":1,\"1647\":1,\"1888\":1,\"1913\":2,\"1914\":2,\"2141\":1,\"2149\":1,\"2150\":1,\"2152\":1,\"2153\":1,\"2156\":1,\"2157\":2,\"2159\":1,\"2160\":1,\"2161\":1,\"2166\":1,\"2355\":5}}],[\"prebuilt\",{\"1\":{\"26\":2,\"29\":3,\"161\":1}}],[\"pref\",{\"1\":{\"1897\":4}}],[\"preforms\",{\"1\":{\"1811\":1}}],[\"prefetching\",{\"1\":{\"173\":2}}],[\"preferable\",{\"1\":{\"26\":1}}],[\"preferred\",{\"1\":{\"19\":1}}],[\"prefixes\",{\"1\":{\"1643\":1,\"1646\":1}}],[\"prefix\",{\"0\":{\"1729\":1,\"1730\":1,\"1897\":1,\"2400\":1},\"1\":{\"22\":1,\"45\":7,\"49\":1,\"50\":1,\"67\":2,\"145\":1,\"162\":2,\"175\":1,\"374\":2,\"616\":1,\"692\":2,\"696\":6,\"697\":6,\"733\":2,\"760\":8,\"790\":2,\"797\":2,\"820\":4,\"850\":2,\"1719\":2,\"1720\":1,\"1723\":2,\"1724\":2,\"1725\":4,\"1729\":1,\"1730\":3,\"1731\":5,\"1745\":4,\"1787\":4,\"1805\":3,\"1806\":1,\"1822\":4,\"1888\":1,\"1897\":4,\"1916\":4,\"1944\":4,\"1945\":2,\"1946\":2,\"1947\":4,\"1966\":2,\"1992\":2,\"1996\":6,\"2341\":1,\"2346\":3,\"2351\":1,\"2368\":3,\"2400\":1}}],[\"pre\",{\"0\":{\"259\":1},\"1\":{\"7\":1,\"46\":2,\"50\":1,\"52\":2,\"178\":3,\"179\":1,\"180\":1,\"181\":1,\"185\":1,\"187\":1,\"188\":1,\"193\":2,\"200\":1,\"207\":1,\"220\":3,\"232\":5,\"240\":1,\"243\":8,\"258\":4,\"259\":1,\"260\":1,\"261\":1,\"262\":5,\"699\":1,\"746\":2,\"760\":2,\"768\":2,\"820\":1,\"828\":1,\"831\":1,\"1145\":2,\"1157\":2,\"1158\":2,\"1267\":2,\"1268\":2,\"1273\":2,\"1274\":2,\"1330\":1,\"1354\":1,\"1582\":2,\"1694\":3,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1719\":16,\"1720\":2,\"1721\":7,\"1725\":16,\"1726\":5,\"1727\":5,\"1751\":5,\"1768\":1,\"1804\":1,\"1862\":7,\"2040\":1,\"2043\":2,\"2045\":5,\"2049\":2,\"2054\":1,\"2055\":2,\"2056\":2,\"2066\":1,\"2127\":1,\"2130\":3,\"2138\":1,\"2247\":2,\"2455\":1,\"2463\":1,\"2464\":1}}],[\"prior=none\",{\"1\":{\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1}}],[\"prior=true\",{\"1\":{\"1577\":1}}],[\"priordecoder\",{\"0\":{\"1536\":1},\"1\":{\"1536\":2}}],[\"prior\",{\"0\":{\"1536\":1},\"1\":{\"1224\":4,\"1225\":4,\"1245\":4,\"1246\":1,\"1247\":1,\"1536\":3,\"1577\":2}}],[\"priors\",{\"1\":{\"1170\":1}}],[\"primer\",{\"1\":{\"795\":2,\"1719\":1,\"1721\":1,\"1725\":4,\"1806\":3}}],[\"primitives\",{\"1\":{\"705\":2,\"804\":2,\"932\":2,\"934\":2}}],[\"primaryclass=\",{\"1\":{\"202\":1}}],[\"primary\",{\"1\":{\"2\":1,\"356\":2,\"1128\":1}}],[\"privilege\",{\"1\":{\"161\":2}}],[\"private\",{\"1\":{\"3\":1}}],[\"print=true\",{\"1\":{\"290\":1}}],[\"printable\",{\"1\":{\"290\":1}}],[\"print\",{\"0\":{\"2159\":1},\"1\":{\"71\":1,\"84\":5,\"126\":1,\"243\":1,\"286\":1,\"295\":4,\"415\":4,\"1064\":1,\"1078\":1,\"1153\":1,\"1202\":1,\"1262\":1,\"1290\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"2143\":1,\"2159\":2,\"2232\":1,\"2238\":1,\"2249\":3,\"2462\":1}}],[\"prosoody\",{\"1\":{\"2298\":1}}],[\"prosodic\",{\"1\":{\"287\":1,\"2298\":1}}],[\"prosody\",{\"0\":{\"2298\":1,\"2302\":1},\"1\":{\"286\":1,\"287\":2,\"481\":2,\"2298\":2,\"2299\":2,\"2302\":1}}],[\"pronunciation\",{\"1\":{\"2280\":1}}],[\"pronounce\",{\"1\":{\"269\":1,\"278\":1}}],[\"proability\",{\"1\":{\"1759\":1}}],[\"prototypes\",{\"1\":{\"2176\":1}}],[\"prototype\",{\"0\":{\"1631\":1},\"1\":{\"1631\":4}}],[\"protocol\",{\"1\":{\"223\":1,\"254\":1}}],[\"prodiffloss\",{\"0\":{\"2424\":1},\"1\":{\"2424\":2}}],[\"prodiff\",{\"0\":{\"2420\":1,\"2423\":3,\"2424\":1,\"2426\":1,\"2427\":1,\"2428\":1,\"2441\":1,\"2442\":1},\"1\":{\"2420\":1,\"2423\":7,\"2424\":1,\"2426\":1,\"2427\":1,\"2428\":1,\"2441\":1,\"2442\":1}}],[\"prod\",{\"1\":{\"1548\":1,\"1582\":1,\"1604\":1,\"1605\":2,\"1619\":2}}],[\"produced\",{\"1\":{\"2355\":2}}],[\"produces\",{\"1\":{\"211\":1,\"262\":1}}],[\"produce\",{\"1\":{\"211\":1,\"2133\":1,\"2355\":1}}],[\"product\",{\"1\":{\"100\":1,\"637\":2,\"644\":1,\"784\":1,\"927\":1,\"1707\":1,\"1714\":1,\"1785\":1,\"1794\":3,\"1817\":1}}],[\"production\",{\"1\":{\"16\":1}}],[\"providing\",{\"1\":{\"992\":1,\"994\":1,\"997\":1,\"1000\":1,\"1002\":1,\"1004\":1,\"1008\":1,\"1012\":1,\"1016\":1,\"2329\":1,\"2330\":1,\"2331\":1}}],[\"provided\",{\"1\":{\"47\":1,\"57\":1,\"150\":1,\"235\":1,\"261\":1,\"269\":2,\"278\":2,\"286\":1,\"287\":13,\"289\":1,\"536\":1,\"614\":1,\"699\":3,\"760\":1,\"1126\":1,\"1224\":1,\"1225\":1,\"1246\":2,\"1552\":3,\"1599\":3,\"1607\":1,\"1626\":4,\"1863\":1,\"1866\":1,\"1871\":1,\"1921\":1,\"1936\":1,\"1992\":3,\"1993\":3,\"1995\":3,\"2044\":3,\"2049\":1,\"2131\":1,\"2235\":3,\"2236\":3,\"2239\":3,\"2240\":3,\"2245\":3,\"2411\":3,\"2412\":3,\"2423\":3,\"2431\":3,\"2432\":3,\"2447\":3}}],[\"provide\",{\"1\":{\"39\":1,\"47\":1,\"78\":1,\"82\":1,\"86\":1,\"197\":1,\"211\":1,\"220\":1,\"224\":2,\"247\":2,\"261\":1,\"267\":1,\"268\":1,\"276\":1,\"277\":1,\"286\":1,\"290\":1,\"852\":1,\"1246\":1,\"1392\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1450\":3,\"1452\":3,\"1454\":2,\"1456\":2,\"1458\":2,\"1460\":2,\"2016\":1}}],[\"provides\",{\"1\":{\"0\":1,\"54\":1,\"79\":1,\"126\":1,\"198\":1,\"245\":1,\"290\":1,\"691\":1,\"831\":2,\"2001\":1,\"2044\":1,\"2130\":1,\"2134\":1,\"2137\":1}}],[\"projs\",{\"1\":{\"1199\":1}}],[\"proj\",{\"1\":{\"674\":2,\"752\":1,\"911\":1,\"1290\":2,\"1522\":1}}],[\"projector\",{\"0\":{\"2174\":2,\"2194\":2,\"2205\":2,\"2211\":2},\"1\":{\"1702\":1,\"2174\":2,\"2184\":4,\"2194\":2,\"2205\":2,\"2211\":2}}],[\"projected\",{\"1\":{\"846\":1,\"1552\":4,\"1601\":3,\"1602\":4,\"1611\":2,\"1622\":2,\"1626\":4}}],[\"projection\",{\"0\":{\"1537\":1},\"1\":{\"632\":1,\"706\":1,\"752\":1,\"772\":1,\"798\":3,\"846\":2,\"862\":3,\"1070\":1,\"1071\":1,\"1215\":1,\"1509\":1,\"1511\":1,\"1513\":2,\"1522\":1,\"1537\":1,\"1548\":2,\"1552\":6,\"1553\":2,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1768\":1,\"1814\":1,\"1816\":2,\"1895\":1,\"2219\":1}}],[\"project\",{\"1\":{\"260\":1,\"377\":2,\"449\":2,\"846\":1,\"911\":1,\"1702\":1,\"2184\":1,\"2218\":1}}],[\"prominent\",{\"1\":{\"262\":2}}],[\"prompt\",{\"1\":{\"198\":1,\"200\":21,\"201\":3,\"242\":1,\"243\":1,\"301\":6,\"1996\":1,\"2039\":2,\"2043\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2262\":2,\"2336\":2,\"2337\":2,\"2345\":2}}],[\"proof\",{\"1\":{\"232\":1,\"258\":1}}],[\"proposing\",{\"1\":{\"2240\":1}}],[\"proposed\",{\"1\":{\"146\":2,\"1185\":1,\"1577\":1,\"2016\":1}}],[\"propose\",{\"1\":{\"141\":1,\"143\":1}}],[\"properties\",{\"1\":{\"2130\":1}}],[\"property\",{\"1\":{\"225\":1,\"262\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"742\":1,\"809\":1,\"819\":1,\"828\":2,\"829\":4,\"830\":3,\"859\":1,\"860\":1,\"951\":1,\"966\":1,\"970\":1,\"977\":1,\"978\":1,\"980\":2,\"1035\":1,\"1037\":2,\"1045\":1,\"1053\":1,\"1062\":1,\"1107\":1,\"1113\":1,\"1117\":1,\"1118\":1,\"1125\":1,\"1130\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1162\":1,\"1170\":2,\"1171\":2,\"1172\":2,\"1173\":2,\"1174\":6,\"1175\":2,\"1204\":1,\"1210\":1,\"1217\":1,\"1223\":1,\"1224\":1,\"1225\":1,\"1232\":1,\"1245\":1,\"1251\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1276\":4,\"1278\":1,\"1280\":1,\"1283\":1,\"1334\":1,\"1382\":1,\"1416\":1,\"1423\":1,\"1469\":1,\"1526\":2,\"1553\":2,\"1598\":2,\"1600\":2,\"1610\":1,\"1625\":2,\"1628\":1,\"1968\":1,\"1970\":1,\"1971\":2,\"1976\":1,\"2020\":1,\"2132\":1,\"2141\":2,\"2222\":2,\"2403\":2,\"2445\":2,\"2457\":1,\"2462\":1}}],[\"properly\",{\"1\":{\"1505\":1,\"1506\":1,\"2040\":1,\"2130\":1}}],[\"proper\",{\"1\":{\"1485\":1,\"2130\":3,\"2133\":1,\"2184\":1}}],[\"propagation\",{\"1\":{\"700\":1,\"709\":1,\"733\":1,\"734\":1,\"774\":1,\"780\":1,\"1385\":1,\"1390\":1,\"1391\":1,\"1397\":1,\"1402\":1,\"1403\":1,\"1409\":1,\"1410\":1,\"1467\":1,\"1468\":1,\"1513\":1,\"1535\":1,\"1546\":1,\"1548\":1,\"1551\":1,\"1552\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1589\":1,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1599\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1606\":1,\"1609\":1,\"1610\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1622\":1,\"1624\":1,\"1626\":1,\"1627\":1,\"1628\":1,\"1705\":1,\"1706\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1733\":1,\"1735\":1,\"1737\":1,\"1750\":1,\"1753\":1,\"1754\":1,\"1758\":1,\"1764\":1,\"1766\":1,\"1768\":1,\"1770\":1,\"1771\":1,\"1788\":1,\"1795\":1,\"1803\":1,\"1810\":1,\"1812\":1,\"1816\":1,\"1839\":1,\"1849\":1,\"1854\":1,\"1856\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"2183\":1,\"2187\":2,\"2191\":1,\"2223\":1,\"2226\":1,\"2227\":1,\"2231\":1,\"2235\":2,\"2236\":2,\"2237\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2245\":1,\"2411\":1,\"2412\":1,\"2413\":1,\"2420\":1,\"2423\":1,\"2424\":1,\"2425\":1,\"2426\":1,\"2427\":1,\"2428\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":1,\"2433\":1,\"2447\":1,\"2448\":1}}],[\"propagated\",{\"1\":{\"2377\":1}}],[\"propagate\",{\"1\":{\"699\":1}}],[\"proc\",{\"1\":{\"156\":1,\"256\":1,\"691\":1,\"1061\":2,\"1062\":2,\"1145\":2,\"1279\":1,\"1280\":2,\"1281\":2,\"1282\":2,\"1283\":1,\"2187\":1,\"2191\":1,\"2192\":1,\"2203\":1,\"2209\":1}}],[\"proceed\",{\"1\":{\"260\":1}}],[\"proceedings\",{\"1\":{\"202\":2,\"1126\":1}}],[\"procedures\",{\"1\":{\"118\":1}}],[\"procedure\",{\"1\":{\"38\":1,\"139\":1,\"146\":1,\"201\":1,\"206\":1,\"212\":1,\"218\":1,\"223\":1,\"255\":1,\"267\":1,\"276\":1,\"286\":3,\"536\":1,\"699\":1}}],[\"processor\",{\"0\":{\"639\":1,\"976\":1},\"1\":{\"639\":1,\"759\":1,\"976\":1,\"2133\":1,\"2287\":1}}],[\"processed\",{\"1\":{\"626\":2,\"759\":1,\"1031\":2,\"1112\":2,\"1202\":1,\"1250\":2,\"1259\":1,\"1261\":1,\"1330\":1,\"2054\":1,\"2130\":3,\"2143\":1,\"2159\":1,\"2235\":1,\"2236\":1,\"2354\":1}}],[\"processes\",{\"1\":{\"59\":1,\"153\":1,\"173\":2,\"1050\":1,\"1116\":1,\"1161\":1,\"1189\":1,\"1218\":1,\"1221\":1,\"1229\":1,\"1244\":1,\"1702\":1,\"2044\":1,\"2054\":1,\"2130\":1,\"2132\":1,\"2133\":1,\"2141\":1,\"2143\":1,\"2149\":1,\"2276\":1,\"2277\":1}}],[\"processe\",{\"1\":{\"59\":1}}],[\"processing\",{\"1\":{\"13\":1,\"54\":2,\"68\":1,\"130\":3,\"156\":1,\"207\":1,\"211\":1,\"212\":1,\"217\":1,\"218\":1,\"240\":1,\"246\":1,\"266\":2,\"267\":1,\"269\":1,\"275\":2,\"276\":1,\"278\":1,\"285\":3,\"286\":1,\"287\":1,\"600\":1,\"710\":3,\"711\":4,\"1061\":1,\"1062\":1,\"1145\":1,\"1279\":5,\"1280\":5,\"1281\":5,\"1283\":5,\"1309\":1,\"1311\":1,\"1319\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1725\":1,\"1730\":1,\"1806\":1,\"1918\":1,\"2044\":4,\"2130\":4,\"2131\":2,\"2142\":1,\"2148\":1,\"2157\":1,\"2247\":2,\"2355\":2,\"2357\":1,\"2411\":1}}],[\"process\",{\"0\":{\"87\":1,\"2160\":1},\"1\":{\"0\":2,\"3\":1,\"26\":1,\"39\":1,\"87\":2,\"91\":1,\"127\":1,\"139\":1,\"148\":2,\"161\":1,\"174\":1,\"205\":1,\"211\":4,\"247\":1,\"266\":2,\"267\":1,\"269\":1,\"275\":2,\"276\":1,\"278\":1,\"286\":1,\"374\":1,\"686\":1,\"768\":1,\"1029\":3,\"1061\":1,\"1063\":1,\"1130\":1,\"1140\":2,\"1395\":4,\"1645\":1,\"1650\":1,\"1719\":1,\"1720\":2,\"1725\":1,\"1806\":3,\"1846\":1,\"1875\":1,\"1899\":1,\"1924\":1,\"1957\":1,\"2044\":3,\"2065\":1,\"2132\":1,\"2141\":1,\"2157\":1,\"2160\":2,\"2287\":1,\"2338\":1,\"2355\":1,\"2366\":1,\"2369\":1,\"2380\":1,\"2428\":1}}],[\"prob=0\",{\"1\":{\"2350\":1}}],[\"probablities\",{\"1\":{\"1729\":1,\"1730\":1}}],[\"probabilistic\",{\"1\":{\"2470\":1}}],[\"probabiliy\",{\"1\":{\"1245\":1}}],[\"probabilities=false\",{\"1\":{\"211\":1}}],[\"probabilities\",{\"1\":{\"211\":2,\"331\":2,\"616\":2,\"628\":1,\"770\":1,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"1720\":1,\"1721\":1,\"1750\":1,\"1920\":2,\"1993\":1,\"1997\":1,\"2000\":1,\"2224\":1,\"2245\":1,\"2431\":1,\"2432\":1}}],[\"probability\",{\"1\":{\"141\":1,\"286\":1,\"301\":2,\"309\":2,\"421\":2,\"631\":1,\"636\":1,\"661\":1,\"701\":3,\"703\":1,\"706\":1,\"735\":1,\"755\":1,\"768\":3,\"776\":1,\"785\":1,\"795\":1,\"796\":1,\"798\":1,\"832\":3,\"846\":7,\"862\":1,\"919\":1,\"939\":1,\"948\":1,\"958\":1,\"1161\":1,\"1224\":2,\"1225\":2,\"1229\":1,\"1244\":1,\"1245\":5,\"1577\":1,\"1589\":2,\"1599\":1,\"1637\":2,\"1667\":1,\"1683\":1,\"1759\":1,\"1796\":1,\"1856\":1,\"1917\":1,\"1997\":1,\"2000\":3,\"2001\":2,\"2220\":1,\"2353\":3,\"2364\":3}}],[\"probs\",{\"1\":{\"286\":1,\"703\":7,\"717\":5,\"786\":1,\"800\":1,\"867\":1,\"873\":1,\"1497\":1,\"1731\":1,\"1934\":1}}],[\"prob\",{\"1\":{\"243\":2,\"674\":6,\"747\":2,\"846\":4,\"974\":1,\"979\":1,\"1153\":1,\"1156\":1,\"1224\":2,\"1225\":2,\"1245\":1,\"1589\":2,\"1730\":2,\"1731\":1,\"1993\":1,\"2220\":5,\"2245\":1,\"2336\":3,\"2337\":3,\"2346\":3,\"2348\":2,\"2350\":2,\"2353\":6,\"2356\":3,\"2360\":4,\"2361\":4,\"2362\":3,\"2364\":6,\"2368\":3,\"2431\":1,\"2432\":1}}],[\"problems\",{\"1\":{\"210\":1,\"213\":1,\"265\":1,\"269\":2,\"274\":1,\"278\":2,\"290\":1}}],[\"problem\",{\"0\":{\"154\":1},\"1\":{\"153\":1,\"173\":1,\"262\":2,\"269\":1,\"278\":1}}],[\"proxy=\",{\"1\":{\"92\":1}}],[\"proxy\",{\"1\":{\"92\":2}}],[\"progressive\",{\"1\":{\"1211\":2,\"2423\":1}}],[\"progressive=\",{\"1\":{\"1211\":1}}],[\"progressbar\",{\"1\":{\"335\":2,\"342\":2,\"361\":2}}],[\"progress\",{\"1\":{\"39\":1,\"2044\":3,\"2355\":2}}],[\"program\",{\"1\":{\"22\":1,\"1860\":1,\"1878\":1,\"1885\":1}}],[\"programming\",{\"1\":{\"3\":1,\"66\":1}}],[\"profile\",{\"1\":{\"31\":1,\"162\":1}}],[\"fprs\",{\"1\":{\"2476\":1}}],[\"fp16\",{\"1\":{\"126\":1}}],[\"fwskattention\",{\"0\":{\"2214\":1},\"1\":{\"2214\":1}}],[\"fb\",{\"0\":{\"1558\":1},\"1\":{\"1533\":2,\"1558\":3}}],[\"fbankdir\",{\"1\":{\"535\":1}}],[\"fbank\",{\"0\":{\"516\":1,\"524\":1,\"548\":1,\"558\":1,\"575\":1,\"1980\":1,\"2416\":1},\"1\":{\"266\":1,\"267\":3,\"275\":1,\"516\":4,\"523\":2,\"524\":5,\"535\":1,\"548\":2,\"558\":2,\"575\":2,\"699\":2,\"720\":1,\"1662\":1,\"1846\":1,\"1980\":2,\"2416\":2}}],[\"fgnt\",{\"1\":{\"1332\":2}}],[\"f2\",{\"1\":{\"1051\":3}}],[\"fmin=none\",{\"1\":{\"1791\":1,\"1836\":1,\"1900\":1,\"1925\":1}}],[\"fmin\",{\"1\":{\"516\":2,\"548\":2,\"558\":2,\"720\":1,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1419\":2,\"1466\":1,\"1526\":1,\"1553\":1,\"1598\":1,\"1600\":1,\"1607\":2,\"1625\":1,\"1662\":2,\"1980\":1,\"2416\":1,\"2482\":1,\"2495\":1}}],[\"fmax=none\",{\"1\":{\"1791\":1,\"1836\":1,\"1900\":1,\"1925\":1}}],[\"fmax\",{\"1\":{\"516\":2,\"548\":2,\"558\":2,\"720\":1,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1419\":2,\"1466\":1,\"1526\":1,\"1553\":1,\"1598\":1,\"1600\":1,\"1607\":2,\"1625\":1,\"1662\":3,\"1980\":1,\"2416\":1,\"2482\":1,\"2495\":1}}],[\"fd\",{\"1\":{\"449\":2,\"2258\":2,\"2342\":1,\"2390\":1}}],[\"fʲ\",{\"1\":{\"287\":1}}],[\"f0=none\",{\"1\":{\"1548\":2}}],[\"f0max\",{\"1\":{\"286\":1,\"2404\":1}}],[\"f0min\",{\"1\":{\"286\":1,\"2404\":1}}],[\"f0\",{\"0\":{\"1559\":2},\"1\":{\"267\":4,\"276\":4,\"285\":7,\"286\":9,\"1521\":2,\"1524\":4,\"1526\":2,\"1545\":5,\"1548\":1,\"1552\":5,\"1553\":3,\"1559\":6,\"2226\":2,\"2228\":2,\"2229\":2,\"2235\":3,\"2236\":3,\"2239\":3,\"2240\":5,\"2241\":2,\"2245\":3,\"2404\":5,\"2422\":3}}],[\"ftswish\",{\"0\":{\"629\":1},\"1\":{\"141\":4,\"629\":5,\"664\":6}}],[\"fnrs\",{\"1\":{\"2476\":1}}],[\"fname\",{\"1\":{\"991\":1,\"993\":1,\"996\":1,\"1001\":1,\"1003\":1,\"1007\":1,\"1011\":1,\"1015\":1}}],[\"fn=cls\",{\"1\":{\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1}}],[\"fn=collate\",{\"1\":{\"82\":1}}],[\"fn=<function\",{\"1\":{\"1489\":2}}],[\"fn=none\",{\"1\":{\"925\":1,\"1643\":1,\"1645\":1,\"1646\":1,\"1650\":1,\"1750\":1,\"2223\":1}}],[\"fn\",{\"0\":{\"902\":1,\"1651\":1,\"1682\":1,\"1683\":1,\"1684\":1,\"1685\":1,\"2335\":1,\"2350\":1,\"2376\":2},\"1\":{\"78\":2,\"82\":12,\"674\":2,\"706\":1,\"715\":1,\"749\":1,\"768\":1,\"778\":1,\"783\":1,\"794\":1,\"851\":1,\"902\":1,\"925\":1,\"929\":1,\"1050\":2,\"1116\":2,\"1161\":2,\"1189\":2,\"1218\":1,\"1221\":1,\"1229\":3,\"1244\":2,\"1254\":1,\"1645\":1,\"1651\":1,\"1682\":1,\"1683\":1,\"1684\":1,\"1685\":1,\"1750\":1,\"1796\":1,\"1917\":3,\"2131\":5,\"2134\":2,\"2143\":1,\"2223\":1,\"2246\":5,\"2247\":2,\"2248\":5,\"2249\":7,\"2250\":5,\"2251\":5,\"2252\":5,\"2253\":5,\"2254\":5,\"2255\":5,\"2256\":5,\"2257\":5,\"2258\":4,\"2259\":5,\"2260\":5,\"2261\":5,\"2262\":2,\"2263\":5,\"2264\":5,\"2265\":1,\"2266\":5,\"2267\":5,\"2268\":5,\"2269\":5,\"2270\":5,\"2271\":5,\"2272\":5,\"2273\":5,\"2312\":1,\"2335\":2,\"2350\":2,\"2376\":2,\"2377\":1}}],[\"fsbd\",{\"1\":{\"1509\":1,\"1511\":1,\"1553\":1}}],[\"fst\",{\"0\":{\"1378\":1,\"1379\":1,\"1380\":1,\"2523\":1}}],[\"fs=24000\",{\"1\":{\"1522\":1}}],[\"fs=16000\",{\"1\":{\"1522\":1}}],[\"fs=16k\",{\"1\":{\"69\":1}}],[\"fs=48000\",{\"1\":{\"1061\":1,\"1063\":2,\"1316\":1,\"1320\":1}}],[\"fs=none\",{\"1\":{\"831\":1,\"1061\":1,\"1063\":1}}],[\"fsa\",{\"1\":{\"615\":1,\"644\":1,\"669\":1,\"670\":1}}],[\"fsc\",{\"1\":{\"201\":1}}],[\"fs\",{\"1\":{\"69\":2,\"223\":1,\"243\":1,\"267\":17,\"276\":7,\"286\":5,\"295\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"415\":2,\"516\":2,\"523\":2,\"536\":2,\"537\":2,\"548\":2,\"551\":2,\"558\":2,\"575\":2,\"606\":2,\"702\":1,\"720\":1,\"738\":2,\"752\":1,\"759\":1,\"768\":2,\"815\":1,\"831\":1,\"864\":1,\"1030\":1,\"1034\":1,\"1061\":3,\"1062\":3,\"1063\":3,\"1112\":2,\"1113\":2,\"1157\":2,\"1222\":2,\"1223\":2,\"1250\":3,\"1251\":3,\"1316\":3,\"1320\":2,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1419\":2,\"1466\":1,\"1526\":1,\"1539\":2,\"1552\":2,\"1553\":1,\"1598\":1,\"1600\":1,\"1607\":2,\"1625\":1,\"1643\":1,\"1646\":1,\"1654\":2,\"1662\":3,\"1666\":2,\"1668\":2,\"1674\":1,\"1791\":1,\"1827\":2,\"1830\":2,\"1836\":1,\"1846\":1,\"1900\":1,\"1925\":1,\"1980\":1,\"2232\":1,\"2238\":1,\"2336\":1,\"2337\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2404\":1,\"2409\":1,\"2416\":1,\"2434\":1,\"2435\":3,\"2482\":2,\"2495\":2}}],[\"fa\",{\"1\":{\"2476\":1,\"2506\":1}}],[\"fancier\",{\"1\":{\"2355\":1}}],[\"fatory\",{\"1\":{\"2249\":1,\"2253\":1}}],[\"fake\",{\"1\":{\"949\":1,\"1515\":2,\"1584\":1,\"2470\":2}}],[\"fashion\",{\"1\":{\"1751\":1}}],[\"fasnetseparator\",{\"0\":{\"1162\":1},\"1\":{\"1162\":1}}],[\"fasnet\",{\"0\":{\"1059\":1,\"1162\":1,\"1163\":2,\"1164\":2},\"1\":{\"223\":2,\"1059\":2,\"1162\":7,\"1163\":3,\"1164\":2,\"1333\":1}}],[\"fastselfattention\",{\"0\":{\"749\":1},\"1\":{\"749\":1}}],[\"fastspeech1\",{\"1\":{\"2239\":1,\"2240\":1}}],[\"fastspeech2discrete\",{\"0\":{\"2447\":1},\"1\":{\"2447\":1}}],[\"fastspeech2lossdiscrete\",{\"0\":{\"2448\":1},\"1\":{\"2448\":1}}],[\"fastspeech2loss\",{\"0\":{\"2413\":1},\"1\":{\"2413\":1}}],[\"fastspeech2\",{\"0\":{\"2412\":3,\"2413\":1,\"2433\":1,\"2447\":2,\"2448\":1},\"1\":{\"284\":2,\"286\":18,\"289\":2,\"290\":3,\"1598\":1,\"1600\":1,\"2241\":1,\"2412\":6,\"2413\":2,\"2433\":1,\"2447\":6,\"2448\":2}}],[\"fastspeech\",{\"0\":{\"1752\":1,\"1754\":1,\"1764\":1,\"1788\":1,\"2411\":3},\"1\":{\"284\":1,\"286\":8,\"289\":2,\"290\":3,\"536\":8,\"1752\":2,\"1754\":1,\"1764\":1,\"1788\":2,\"1795\":1,\"2240\":1,\"2411\":7,\"2412\":1,\"2433\":1}}],[\"fastformer\",{\"0\":{\"749\":1},\"1\":{\"749\":2}}],[\"fastpitch\",{\"1\":{\"289\":1,\"2412\":1}}],[\"fastemit\",{\"1\":{\"46\":2,\"139\":4,\"625\":3,\"703\":4,\"755\":4,\"785\":4,\"786\":4,\"800\":4,\"866\":1,\"867\":4,\"880\":1,\"881\":4,\"884\":4,\"921\":1,\"922\":4,\"935\":1,\"936\":4,\"937\":4}}],[\"faster\",{\"1\":{\"6\":1,\"48\":2,\"261\":1,\"262\":1,\"722\":1}}],[\"fast\",{\"1\":{\"41\":1,\"749\":1,\"1247\":1,\"1502\":1,\"1752\":1,\"1788\":1,\"1795\":1,\"1854\":2,\"2411\":2,\"2412\":1,\"2423\":1,\"2433\":1}}],[\"falg\",{\"1\":{\"2239\":1}}],[\"fall2021\",{\"0\":{\"192\":1}}],[\"fall2022\",{\"0\":{\"191\":1}}],[\"false\",{\"1\":{\"43\":2,\"54\":2,\"56\":2,\"61\":3,\"63\":1,\"78\":2,\"81\":2,\"84\":2,\"104\":1,\"118\":3,\"126\":1,\"127\":2,\"136\":1,\"141\":2,\"147\":1,\"200\":1,\"205\":1,\"211\":2,\"242\":1,\"243\":6,\"263\":1,\"267\":1,\"285\":1,\"616\":2,\"619\":1,\"620\":2,\"622\":1,\"623\":1,\"625\":3,\"627\":2,\"632\":1,\"635\":1,\"644\":1,\"652\":1,\"661\":2,\"674\":26,\"692\":5,\"696\":1,\"697\":1,\"699\":3,\"700\":3,\"702\":2,\"709\":8,\"710\":3,\"711\":2,\"720\":4,\"731\":2,\"732\":2,\"733\":6,\"734\":7,\"736\":1,\"737\":3,\"738\":3,\"740\":2,\"745\":1,\"746\":4,\"747\":1,\"748\":2,\"760\":5,\"765\":1,\"766\":2,\"767\":2,\"774\":6,\"775\":1,\"777\":1,\"778\":1,\"780\":6,\"787\":4,\"791\":2,\"796\":3,\"815\":3,\"819\":1,\"820\":2,\"846\":7,\"848\":2,\"849\":2,\"850\":3,\"851\":6,\"869\":1,\"958\":1,\"980\":2,\"993\":1,\"1007\":2,\"1009\":1,\"1026\":1,\"1028\":2,\"1062\":2,\"1107\":4,\"1118\":6,\"1119\":1,\"1125\":3,\"1126\":2,\"1127\":2,\"1130\":1,\"1133\":2,\"1136\":1,\"1137\":1,\"1141\":1,\"1147\":1,\"1155\":1,\"1157\":6,\"1158\":2,\"1162\":1,\"1202\":1,\"1209\":2,\"1217\":4,\"1228\":2,\"1232\":1,\"1250\":1,\"1251\":1,\"1252\":2,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":3,\"1267\":3,\"1268\":3,\"1278\":4,\"1280\":1,\"1315\":1,\"1327\":1,\"1330\":1,\"1356\":1,\"1385\":1,\"1389\":9,\"1390\":2,\"1391\":3,\"1396\":9,\"1400\":1,\"1401\":11,\"1402\":3,\"1403\":4,\"1408\":11,\"1409\":5,\"1410\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1428\":1,\"1430\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1458\":1,\"1460\":1,\"1466\":10,\"1467\":2,\"1468\":3,\"1469\":1,\"1485\":1,\"1501\":1,\"1509\":3,\"1511\":3,\"1519\":2,\"1526\":15,\"1535\":2,\"1536\":4,\"1539\":3,\"1545\":1,\"1546\":2,\"1549\":1,\"1552\":2,\"1553\":12,\"1581\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":16,\"1599\":9,\"1600\":16,\"1603\":1,\"1606\":1,\"1607\":2,\"1612\":1,\"1613\":1,\"1616\":1,\"1622\":2,\"1625\":9,\"1626\":1,\"1627\":1,\"1628\":4,\"1640\":3,\"1643\":2,\"1645\":2,\"1646\":2,\"1648\":1,\"1650\":3,\"1662\":1,\"1669\":1,\"1671\":1,\"1672\":2,\"1700\":1,\"1702\":1,\"1719\":2,\"1721\":2,\"1725\":2,\"1726\":1,\"1727\":5,\"1735\":1,\"1736\":1,\"1748\":1,\"1751\":1,\"1759\":1,\"1766\":2,\"1779\":2,\"1806\":1,\"1881\":1,\"1883\":1,\"1942\":1,\"1945\":1,\"1949\":1,\"1959\":3,\"1962\":1,\"1965\":1,\"1975\":1,\"1976\":2,\"1978\":1,\"1980\":2,\"1982\":1,\"1987\":1,\"1991\":1,\"1992\":4,\"1993\":3,\"1994\":3,\"1995\":4,\"1996\":1,\"1997\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":1,\"2008\":2,\"2015\":1,\"2049\":1,\"2065\":2,\"2126\":3,\"2127\":5,\"2129\":2,\"2130\":1,\"2134\":2,\"2136\":2,\"2167\":1,\"2176\":2,\"2184\":1,\"2191\":2,\"2217\":1,\"2220\":2,\"2221\":2,\"2235\":3,\"2236\":5,\"2239\":12,\"2240\":8,\"2241\":1,\"2245\":7,\"2246\":2,\"2247\":2,\"2248\":2,\"2249\":6,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2262\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2275\":1,\"2276\":1,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2292\":1,\"2293\":2,\"2318\":1,\"2320\":1,\"2327\":1,\"2331\":1,\"2334\":1,\"2336\":3,\"2337\":2,\"2340\":2,\"2342\":1,\"2346\":4,\"2350\":3,\"2354\":2,\"2355\":1,\"2365\":1,\"2368\":6,\"2409\":1,\"2411\":8,\"2412\":10,\"2413\":1,\"2414\":1,\"2416\":2,\"2418\":1,\"2423\":9,\"2424\":1,\"2428\":1,\"2431\":6,\"2432\":7,\"2434\":1,\"2447\":9,\"2448\":1,\"2458\":5,\"2460\":1,\"2462\":5,\"2463\":2,\"2464\":2,\"2469\":1,\"2470\":1,\"2481\":1}}],[\"fact\",{\"1\":{\"242\":1}}],[\"factor=0\",{\"1\":{\"1854\":1,\"2020\":1}}],[\"factor=1\",{\"1\":{\"1750\":1,\"2223\":1}}],[\"factor=2\",{\"1\":{\"1301\":1,\"1306\":1,\"1344\":1,\"1345\":1,\"1371\":1,\"1372\":1}}],[\"factors\",{\"1\":{\"235\":1,\"1511\":1,\"1549\":1,\"1614\":1,\"1878\":2,\"1892\":2,\"2134\":1}}],[\"factory\",{\"0\":{\"1642\":1,\"1643\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1},\"1\":{\"91\":3,\"974\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1642\":1,\"1643\":1,\"1644\":2,\"1645\":1,\"1646\":1,\"1647\":2,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1,\"2134\":3,\"2140\":1,\"2249\":12,\"2253\":4,\"2338\":2,\"2369\":3}}],[\"factor\",{\"0\":{\"133\":1},\"1\":{\"84\":1,\"133\":2,\"134\":1,\"139\":2,\"141\":3,\"200\":1,\"205\":1,\"242\":1,\"290\":1,\"543\":1,\"621\":3,\"637\":3,\"639\":3,\"663\":4,\"665\":3,\"703\":1,\"706\":1,\"755\":1,\"785\":1,\"786\":1,\"800\":1,\"823\":1,\"824\":1,\"846\":2,\"867\":1,\"881\":1,\"884\":1,\"922\":1,\"936\":1,\"937\":1,\"1225\":1,\"1250\":1,\"1251\":1,\"1301\":8,\"1306\":7,\"1314\":2,\"1315\":1,\"1354\":1,\"1361\":1,\"1371\":7,\"1372\":8,\"1389\":1,\"1390\":1,\"1392\":1,\"1413\":3,\"1420\":1,\"1526\":1,\"1552\":1,\"1571\":1,\"1575\":1,\"1592\":1,\"1598\":1,\"1599\":4,\"1600\":1,\"1609\":4,\"1611\":1,\"1612\":1,\"1613\":1,\"1615\":1,\"1617\":2,\"1619\":1,\"1620\":5,\"1621\":5,\"1626\":1,\"1628\":1,\"1655\":2,\"1672\":1,\"1673\":1,\"1680\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1697\":3,\"1698\":3,\"1750\":2,\"1758\":2,\"1849\":4,\"1854\":2,\"1863\":2,\"1866\":2,\"1993\":3,\"2000\":8,\"2001\":6,\"2008\":4,\"2015\":2,\"2020\":1,\"2134\":2,\"2167\":1,\"2176\":1,\"2223\":2,\"2231\":2,\"2235\":3,\"2236\":3,\"2239\":3,\"2240\":3,\"2245\":3,\"2307\":2,\"2404\":1,\"2409\":1,\"2411\":3,\"2412\":3,\"2423\":3,\"2431\":3,\"2432\":3,\"2447\":3}}],[\"faces\",{\"1\":{\"2176\":1}}],[\"facebookresearch\",{\"1\":{\"2018\":1,\"2462\":1}}],[\"face\",{\"0\":{\"372\":1,\"760\":1,\"761\":1,\"762\":1,\"904\":2,\"905\":2,\"930\":1,\"2049\":1,\"2128\":1,\"2278\":1,\"2279\":1},\"1\":{\"125\":1,\"127\":2,\"200\":2,\"205\":2,\"217\":3,\"222\":1,\"223\":2,\"228\":1,\"235\":1,\"242\":2,\"243\":1,\"284\":1,\"285\":3,\"290\":1,\"301\":4,\"372\":2,\"463\":4,\"469\":2,\"759\":1,\"760\":3,\"761\":2,\"762\":2,\"904\":2,\"905\":2,\"930\":1,\"2043\":1,\"2044\":1,\"2049\":3,\"2055\":1,\"2056\":1,\"2126\":1,\"2128\":2,\"2167\":1,\"2176\":1,\"2278\":1,\"2279\":1}}],[\"far\",{\"1\":{\"79\":1,\"1966\":1}}],[\"fault\",{\"1\":{\"71\":1}}],[\"faq\",{\"0\":{\"132\":1,\"149\":1,\"151\":1,\"220\":1,\"290\":1},\"1\":{\"67\":1,\"216\":1,\"227\":1,\"284\":1}}],[\"fails\",{\"1\":{\"2134\":1}}],[\"failure\",{\"1\":{\"755\":1,\"785\":1}}],[\"fail\",{\"1\":{\"224\":1,\"225\":1}}],[\"failed\",{\"1\":{\"60\":1,\"66\":1,\"174\":1,\"290\":1}}],[\"fairseqhubertpretrainencoder\",{\"0\":{\"748\":1},\"1\":{\"748\":1}}],[\"fairseqhubertencoder\",{\"0\":{\"747\":1},\"1\":{\"747\":1}}],[\"fairseqavhubertencoder\",{\"0\":{\"746\":1},\"1\":{\"746\":1}}],[\"fairseqwav2vec2\",{\"1\":{\"745\":1}}],[\"fairseqwav2vec2encoder\",{\"0\":{\"745\":1},\"1\":{\"745\":1}}],[\"fairseq\",{\"1\":{\"78\":1,\"79\":1,\"128\":2,\"745\":1,\"746\":1,\"747\":3,\"748\":1,\"1756\":2,\"1757\":2,\"1789\":2,\"1790\":2,\"2018\":3,\"2462\":2}}],[\"fairscale\",{\"1\":{\"57\":2}}],[\"ffts\",{\"1\":{\"1396\":1,\"1397\":1,\"1408\":1,\"1409\":1,\"1422\":2}}],[\"fftsinger\",{\"1\":{\"267\":2}}],[\"fft=2048\",{\"1\":{\"1560\":1}}],[\"fft=512\",{\"1\":{\"1334\":1}}],[\"fft=128\",{\"1\":{\"1269\":1,\"1270\":1}}],[\"fft\",{\"0\":{\"1562\":1,\"1569\":1},\"1\":{\"243\":1,\"267\":2,\"286\":5,\"516\":3,\"523\":3,\"536\":2,\"548\":2,\"551\":2,\"558\":2,\"575\":2,\"720\":1,\"778\":1,\"1250\":1,\"1251\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1316\":3,\"1320\":3,\"1334\":1,\"1385\":3,\"1389\":1,\"1390\":1,\"1392\":3,\"1397\":2,\"1401\":1,\"1402\":1,\"1420\":3,\"1422\":1,\"1466\":1,\"1467\":1,\"1525\":4,\"1526\":1,\"1533\":1,\"1547\":1,\"1552\":3,\"1553\":1,\"1562\":1,\"1566\":1,\"1569\":1,\"1598\":1,\"1600\":1,\"1607\":4,\"1625\":1,\"1662\":3,\"1669\":1,\"1680\":4,\"1692\":4,\"1698\":4,\"1791\":1,\"1832\":1,\"1835\":1,\"1836\":1,\"1900\":1,\"1923\":1,\"1924\":1,\"1925\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"2232\":1,\"2238\":1,\"2404\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2440\":1,\"2482\":3,\"2490\":4,\"2495\":4}}],[\"ffn\",{\"1\":{\"142\":2,\"243\":2,\"634\":2,\"642\":2,\"643\":2,\"674\":4,\"733\":3,\"734\":3,\"746\":1,\"851\":1,\"1552\":4,\"1553\":1,\"1599\":1,\"1625\":1,\"1626\":4,\"2239\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2447\":1}}],[\"ff\",{\"0\":{\"744\":2},\"1\":{\"43\":4,\"744\":2,\"818\":2,\"846\":4}}],[\"ffmpeg\",{\"1\":{\"31\":1,\"74\":3}}],[\"flipped\",{\"1\":{\"1588\":1}}],[\"flip\",{\"1\":{\"1588\":1}}],[\"flipflow\",{\"0\":{\"1588\":1},\"1\":{\"1588\":1}}],[\"fleurs\",{\"1\":{\"2000\":1}}],[\"flens\",{\"1\":{\"1113\":1,\"1251\":1}}],[\"flexible\",{\"1\":{\"356\":2,\"978\":1,\"1155\":1,\"1157\":2,\"1158\":1,\"1228\":3,\"2346\":1,\"2368\":1}}],[\"flexibly\",{\"1\":{\"79\":1}}],[\"flexibility\",{\"1\":{\"43\":1,\"2184\":1}}],[\"flops\",{\"0\":{\"2076\":1,\"2080\":1}}],[\"floor=1e\",{\"1\":{\"1728\":1,\"1850\":1}}],[\"flooring\",{\"1\":{\"1126\":2,\"1127\":2,\"1217\":3}}],[\"flo\",{\"1\":{\"696\":1,\"697\":1}}],[\"flows\",{\"1\":{\"1552\":3,\"1553\":1,\"1612\":3,\"1616\":3,\"1625\":2,\"1626\":6}}],[\"flow=false\",{\"1\":{\"1161\":1,\"1229\":1,\"1244\":1,\"1245\":1}}],[\"flow\",{\"0\":{\"200\":1,\"205\":1,\"211\":1,\"217\":1,\"235\":1,\"242\":1,\"254\":1,\"266\":1,\"275\":1,\"285\":1,\"1581\":1,\"1583\":1,\"1586\":1,\"1588\":1,\"1603\":1,\"1623\":1},\"1\":{\"199\":1,\"204\":1,\"210\":1,\"216\":1,\"227\":1,\"234\":1,\"240\":1,\"253\":1,\"265\":1,\"269\":2,\"274\":1,\"278\":2,\"284\":1,\"686\":1,\"768\":1,\"1224\":1,\"1225\":1,\"1245\":4,\"1526\":1,\"1552\":23,\"1553\":8,\"1581\":3,\"1583\":1,\"1586\":3,\"1588\":3,\"1601\":1,\"1602\":2,\"1603\":3,\"1612\":2,\"1613\":1,\"1616\":1,\"1623\":1,\"1625\":8,\"1626\":23,\"2131\":1,\"2216\":1,\"2480\":1}}],[\"floating\",{\"1\":{\"1678\":1,\"2043\":1,\"2055\":1,\"2056\":1,\"2065\":1,\"2066\":1,\"2101\":1}}],[\"floatornone\",{\"1\":{\"1678\":1,\"1680\":1,\"1692\":1,\"1698\":1}}],[\"floatortorch\",{\"1\":{\"1672\":1,\"1673\":1,\"1687\":2,\"1689\":1,\"1690\":1}}],[\"float|tuple\",{\"1\":{\"1064\":1}}],[\"float|none\",{\"1\":{\"1064\":1,\"1262\":1,\"1290\":1}}],[\"floatrandomgeneratedataset\",{\"0\":{\"987\":1},\"1\":{\"987\":1,\"988\":1}}],[\"floattensor\",{\"1\":{\"750\":2,\"1526\":3,\"1552\":2,\"1553\":3,\"2235\":2,\"2236\":2,\"2239\":2,\"2240\":2,\"2245\":2}}],[\"float=\",{\"1\":{\"674\":19}}],[\"float64\",{\"1\":{\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"377\":2,\"385\":1,\"389\":1,\"396\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":2,\"457\":1,\"463\":1,\"469\":1,\"475\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"1880\":1}}],[\"float16\",{\"1\":{\"295\":1,\"301\":2,\"309\":1,\"315\":1,\"321\":2,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"377\":2,\"385\":1,\"389\":2,\"396\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":2,\"457\":1,\"463\":1,\"469\":1,\"475\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":2,\"505\":1,\"2043\":2,\"2049\":2,\"2054\":1,\"2055\":2,\"2056\":2,\"2066\":2,\"2133\":1}}],[\"float32\",{\"1\":{\"70\":1,\"220\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"377\":2,\"385\":1,\"389\":1,\"396\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":2,\"457\":1,\"463\":1,\"469\":1,\"475\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"692\":3,\"760\":1,\"775\":1,\"790\":2,\"820\":1,\"850\":3,\"895\":1,\"924\":1,\"928\":1,\"987\":1,\"1011\":1,\"1373\":1,\"1751\":1,\"1787\":1,\"1846\":1,\"1933\":1,\"1944\":1,\"1946\":1,\"1947\":1,\"1992\":1,\"2101\":2,\"2249\":1,\"2342\":1,\"2351\":1}}],[\"float20\",{\"1\":{\"70\":1}}],[\"float\",{\"0\":{\"1365\":1,\"2484\":1},\"1\":{\"43\":8,\"44\":8,\"45\":1,\"82\":3,\"104\":1,\"139\":4,\"141\":30,\"142\":2,\"144\":5,\"145\":1,\"615\":1,\"616\":2,\"617\":1,\"618\":1,\"620\":1,\"622\":1,\"624\":1,\"625\":6,\"627\":2,\"628\":1,\"629\":2,\"631\":2,\"633\":3,\"634\":5,\"635\":1,\"637\":3,\"638\":3,\"640\":2,\"641\":2,\"642\":2,\"643\":3,\"644\":1,\"645\":1,\"646\":2,\"647\":2,\"648\":1,\"650\":2,\"651\":1,\"652\":1,\"661\":5,\"664\":7,\"666\":2,\"674\":25,\"691\":2,\"692\":2,\"696\":1,\"697\":2,\"699\":2,\"700\":9,\"701\":8,\"703\":4,\"705\":1,\"706\":2,\"709\":9,\"710\":3,\"711\":3,\"713\":1,\"715\":1,\"730\":1,\"731\":4,\"732\":4,\"733\":4,\"734\":4,\"735\":2,\"736\":3,\"737\":3,\"740\":2,\"743\":1,\"746\":10,\"747\":7,\"748\":3,\"755\":4,\"763\":2,\"765\":1,\"766\":4,\"767\":4,\"768\":2,\"770\":1,\"771\":1,\"772\":1,\"774\":6,\"775\":4,\"776\":1,\"777\":3,\"780\":9,\"781\":1,\"783\":1,\"784\":1,\"785\":5,\"786\":5,\"790\":1,\"791\":1,\"795\":2,\"796\":2,\"798\":1,\"800\":4,\"820\":3,\"824\":1,\"832\":1,\"833\":2,\"837\":1,\"846\":11,\"847\":2,\"848\":5,\"849\":3,\"850\":4,\"851\":6,\"854\":1,\"862\":1,\"867\":1,\"880\":1,\"881\":4,\"882\":1,\"883\":1,\"884\":5,\"916\":2,\"921\":2,\"922\":5,\"935\":2,\"936\":4,\"937\":4,\"939\":2,\"947\":3,\"948\":1,\"949\":4,\"958\":2,\"959\":1,\"962\":2,\"974\":2,\"979\":1,\"987\":1,\"989\":1,\"994\":1,\"1019\":1,\"1021\":2,\"1024\":1,\"1029\":5,\"1054\":2,\"1064\":4,\"1065\":2,\"1070\":1,\"1071\":1,\"1073\":1,\"1107\":6,\"1117\":2,\"1126\":5,\"1127\":4,\"1130\":6,\"1131\":2,\"1133\":2,\"1134\":1,\"1136\":2,\"1137\":1,\"1139\":1,\"1141\":2,\"1153\":1,\"1156\":1,\"1162\":1,\"1185\":1,\"1202\":1,\"1204\":2,\"1208\":2,\"1209\":2,\"1210\":2,\"1217\":6,\"1228\":1,\"1232\":2,\"1235\":5,\"1245\":1,\"1246\":1,\"1247\":2,\"1250\":2,\"1251\":2,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":2,\"1262\":4,\"1278\":6,\"1279\":2,\"1280\":10,\"1281\":5,\"1282\":5,\"1283\":4,\"1290\":2,\"1293\":1,\"1309\":4,\"1310\":4,\"1311\":4,\"1317\":1,\"1318\":4,\"1319\":4,\"1321\":4,\"1322\":4,\"1323\":4,\"1326\":2,\"1327\":6,\"1329\":2,\"1330\":6,\"1334\":1,\"1361\":4,\"1365\":1,\"1377\":1,\"1382\":3,\"1389\":7,\"1391\":4,\"1395\":1,\"1396\":7,\"1400\":4,\"1401\":7,\"1403\":4,\"1406\":2,\"1408\":8,\"1410\":3,\"1413\":2,\"1419\":2,\"1441\":6,\"1446\":1,\"1448\":1,\"1450\":2,\"1452\":2,\"1466\":8,\"1468\":4,\"1469\":5,\"1477\":1,\"1481\":1,\"1491\":1,\"1493\":1,\"1494\":1,\"1519\":6,\"1520\":1,\"1521\":1,\"1524\":2,\"1525\":2,\"1526\":15,\"1529\":1,\"1533\":3,\"1535\":6,\"1536\":6,\"1546\":6,\"1552\":16,\"1553\":23,\"1558\":4,\"1581\":2,\"1583\":4,\"1585\":1,\"1589\":2,\"1598\":11,\"1599\":28,\"1600\":9,\"1603\":2,\"1607\":2,\"1608\":4,\"1610\":2,\"1611\":2,\"1612\":2,\"1613\":2,\"1616\":4,\"1622\":6,\"1625\":17,\"1626\":18,\"1628\":2,\"1631\":4,\"1638\":3,\"1640\":4,\"1643\":1,\"1646\":1,\"1654\":1,\"1655\":5,\"1656\":1,\"1662\":5,\"1665\":2,\"1666\":1,\"1667\":1,\"1668\":2,\"1671\":1,\"1672\":1,\"1673\":1,\"1676\":4,\"1677\":1,\"1679\":2,\"1680\":7,\"1683\":2,\"1685\":1,\"1686\":2,\"1687\":2,\"1689\":1,\"1690\":1,\"1692\":5,\"1694\":2,\"1697\":2,\"1698\":7,\"1700\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1715\":1,\"1719\":5,\"1720\":4,\"1721\":8,\"1725\":11,\"1726\":5,\"1727\":4,\"1732\":2,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1748\":2,\"1749\":1,\"1750\":4,\"1751\":1,\"1753\":2,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":2,\"1760\":2,\"1762\":1,\"1766\":1,\"1768\":1,\"1770\":2,\"1771\":4,\"1775\":2,\"1782\":2,\"1784\":2,\"1785\":1,\"1786\":1,\"1788\":2,\"1789\":1,\"1790\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1803\":1,\"1806\":2,\"1807\":2,\"1808\":1,\"1809\":1,\"1810\":1,\"1814\":1,\"1815\":2,\"1816\":1,\"1817\":1,\"1818\":1,\"1820\":1,\"1822\":2,\"1837\":1,\"1839\":1,\"1847\":1,\"1856\":1,\"1862\":8,\"1863\":2,\"1908\":1,\"1914\":2,\"1917\":1,\"1920\":1,\"1928\":1,\"1942\":1,\"1945\":1,\"1947\":3,\"1957\":1,\"1959\":1,\"1960\":1,\"1961\":1,\"1962\":4,\"1966\":1,\"1976\":3,\"1980\":1,\"1984\":1,\"1987\":2,\"1988\":1,\"1990\":3,\"1991\":2,\"1992\":5,\"1993\":10,\"1994\":4,\"1995\":5,\"1996\":1,\"1997\":3,\"2000\":3,\"2001\":2,\"2008\":4,\"2014\":8,\"2015\":4,\"2016\":4,\"2017\":2,\"2018\":6,\"2019\":1,\"2020\":1,\"2021\":2,\"2044\":9,\"2065\":4,\"2126\":3,\"2127\":3,\"2129\":3,\"2130\":2,\"2136\":2,\"2137\":1,\"2150\":2,\"2155\":1,\"2156\":2,\"2167\":1,\"2176\":4,\"2191\":9,\"2220\":8,\"2221\":6,\"2223\":2,\"2224\":2,\"2227\":1,\"2228\":1,\"2229\":1,\"2231\":1,\"2235\":8,\"2236\":10,\"2239\":31,\"2240\":29,\"2245\":16,\"2258\":2,\"2307\":3,\"2308\":2,\"2309\":1,\"2327\":1,\"2329\":1,\"2332\":2,\"2335\":2,\"2336\":6,\"2337\":5,\"2341\":1,\"2342\":2,\"2346\":5,\"2348\":4,\"2350\":8,\"2351\":1,\"2353\":17,\"2356\":5,\"2359\":2,\"2360\":8,\"2361\":9,\"2362\":5,\"2363\":1,\"2364\":17,\"2367\":3,\"2368\":5,\"2370\":4,\"2372\":2,\"2373\":6,\"2374\":1,\"2376\":2,\"2378\":1,\"2390\":1,\"2393\":1,\"2397\":1,\"2398\":2,\"2399\":1,\"2408\":1,\"2411\":22,\"2412\":32,\"2416\":1,\"2423\":30,\"2427\":2,\"2428\":4,\"2430\":2,\"2431\":16,\"2432\":37,\"2433\":2,\"2435\":1,\"2441\":2,\"2442\":6,\"2446\":1,\"2447\":32,\"2458\":1,\"2460\":2,\"2462\":3,\"2463\":1,\"2464\":1,\"2469\":2,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1,\"2484\":3,\"2491\":1}}],[\"fly\",{\"1\":{\"106\":1,\"223\":1,\"246\":1,\"2136\":1,\"2345\":1}}],[\"flatened\",{\"1\":{\"804\":2,\"932\":2,\"934\":2}}],[\"flat\",{\"1\":{\"703\":2,\"755\":2,\"785\":1}}],[\"flattented\",{\"1\":{\"703\":1,\"717\":1,\"755\":1,\"785\":1}}],[\"flattened\",{\"1\":{\"703\":2,\"716\":1,\"755\":2,\"785\":2,\"804\":1,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"919\":2,\"932\":1,\"934\":1,\"2116\":2}}],[\"flatten\",{\"0\":{\"899\":1},\"1\":{\"629\":1,\"899\":1,\"1118\":1,\"1215\":1}}],[\"flash\",{\"0\":{\"2313\":2},\"1\":{\"700\":1,\"709\":1,\"733\":1,\"734\":1,\"848\":1,\"1794\":3,\"2313\":2}}],[\"flac\",{\"1\":{\"70\":1,\"71\":5,\"80\":1,\"159\":3,\"197\":3,\"205\":1,\"235\":1,\"242\":1,\"243\":1,\"254\":1,\"1678\":1,\"1824\":2}}],[\"flake8\",{\"0\":{\"32\":1},\"1\":{\"31\":2,\"32\":10,\"225\":1}}],[\"flags\",{\"1\":{\"127\":1,\"259\":1,\"285\":1}}],[\"flag\",{\"1\":{\"22\":1,\"23\":2,\"25\":1,\"26\":1,\"27\":1,\"200\":1,\"211\":1,\"246\":1,\"286\":1,\"716\":1,\"804\":1,\"932\":1,\"934\":1,\"1536\":3,\"1545\":4,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1768\":1,\"1895\":1,\"2044\":1,\"2228\":1,\"2229\":1,\"2235\":1,\"2236\":1,\"2239\":3,\"2240\":1,\"2245\":1}}],[\"f\",{\"1\":{\"39\":1,\"74\":2,\"75\":2,\"98\":2,\"113\":1,\"162\":1,\"626\":2,\"639\":1,\"675\":1,\"768\":1,\"778\":2,\"955\":1,\"977\":1,\"979\":3,\"1029\":1,\"1051\":5,\"1054\":1,\"1061\":2,\"1062\":2,\"1063\":1,\"1112\":1,\"1117\":2,\"1118\":7,\"1119\":2,\"1124\":2,\"1125\":3,\"1126\":18,\"1127\":7,\"1130\":1,\"1131\":3,\"1132\":2,\"1147\":2,\"1170\":4,\"1172\":2,\"1173\":4,\"1175\":4,\"1198\":2,\"1199\":3,\"1235\":1,\"1245\":2,\"1250\":2,\"1251\":2,\"1253\":2,\"1264\":2,\"1268\":2,\"1271\":1,\"1280\":4,\"1281\":1,\"1282\":1,\"1283\":3,\"1293\":3,\"1309\":4,\"1310\":3,\"1311\":3,\"1314\":4,\"1315\":4,\"1317\":3,\"1318\":3,\"1319\":3,\"1321\":3,\"1322\":4,\"1323\":3,\"1325\":2,\"1326\":3,\"1327\":3,\"1328\":3,\"1330\":3,\"1332\":1,\"1334\":5,\"1351\":3,\"1352\":1,\"1354\":4,\"1376\":2,\"1493\":1,\"1494\":1,\"1509\":1,\"1511\":1,\"1516\":1,\"1533\":6,\"1553\":1,\"1558\":4,\"1617\":2,\"1654\":1,\"1666\":1,\"1669\":2,\"1678\":1,\"1774\":2,\"1824\":3,\"1827\":2,\"1830\":2,\"1851\":8,\"1883\":4,\"2213\":4,\"2214\":5,\"2355\":2,\"2482\":2,\"2495\":2}}],[\"f1\",{\"1\":{\"19\":1,\"201\":4,\"1051\":3}}],[\"few\",{\"1\":{\"2162\":1}}],[\"fewer\",{\"1\":{\"262\":1,\"514\":1,\"2162\":1}}],[\"fed\",{\"1\":{\"223\":1,\"846\":1}}],[\"featurizer\",{\"1\":{\"750\":1}}],[\"featurematchloss\",{\"0\":{\"1587\":1},\"1\":{\"1587\":2}}],[\"featuremapdense\",{\"0\":{\"1165\":1},\"1\":{\"1165\":1}}],[\"featureizer\",{\"0\":{\"750\":1},\"1\":{\"750\":1}}],[\"featureextractor\",{\"1\":{\"712\":1}}],[\"feature=\",{\"1\":{\"276\":1}}],[\"feature=true\",{\"1\":{\"128\":1}}],[\"features=none\",{\"1\":{\"1205\":2,\"1966\":1}}],[\"features\",{\"0\":{\"246\":1},\"1\":{\"57\":1,\"82\":1,\"95\":5,\"106\":2,\"128\":1,\"138\":3,\"203\":1,\"205\":2,\"217\":1,\"218\":1,\"243\":1,\"245\":1,\"262\":1,\"267\":3,\"273\":2,\"276\":3,\"285\":1,\"286\":5,\"287\":1,\"290\":1,\"570\":1,\"625\":4,\"626\":4,\"639\":9,\"674\":2,\"675\":4,\"699\":3,\"701\":1,\"735\":1,\"746\":1,\"768\":1,\"784\":1,\"798\":3,\"846\":4,\"862\":3,\"1065\":1,\"1068\":1,\"1078\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1105\":1,\"1124\":1,\"1187\":1,\"1196\":1,\"1205\":1,\"1219\":1,\"1230\":1,\"1233\":1,\"1259\":1,\"1261\":1,\"1288\":1,\"1290\":1,\"1395\":2,\"1521\":3,\"1526\":1,\"1552\":2,\"1553\":1,\"1585\":2,\"1610\":4,\"1628\":2,\"1629\":1,\"1645\":1,\"1650\":1,\"1720\":1,\"1721\":1,\"1735\":2,\"1750\":5,\"1751\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1764\":1,\"1784\":1,\"1785\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1794\":1,\"1817\":1,\"1839\":1,\"1914\":1,\"1965\":2,\"1991\":1,\"1992\":1,\"1993\":3,\"1995\":1,\"2007\":1,\"2124\":1,\"2128\":1,\"2130\":5,\"2133\":4,\"2134\":1,\"2136\":3,\"2138\":2,\"2143\":2,\"2167\":1,\"2176\":1,\"2183\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2198\":1,\"2203\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2223\":2,\"2224\":2,\"2226\":1,\"2227\":1,\"2228\":3,\"2229\":3,\"2231\":1,\"2235\":5,\"2236\":5,\"2237\":1,\"2239\":3,\"2240\":4,\"2241\":1,\"2245\":3,\"2298\":1,\"2408\":3,\"2411\":3,\"2412\":3,\"2413\":1,\"2423\":3,\"2424\":1,\"2425\":1,\"2429\":1,\"2431\":3,\"2432\":3,\"2434\":1,\"2446\":3,\"2447\":2,\"2448\":1,\"2474\":1}}],[\"feature\",{\"0\":{\"128\":1,\"496\":1,\"1343\":1,\"1358\":1},\"1\":{\"18\":1,\"38\":1,\"43\":1,\"81\":2,\"82\":1,\"84\":1,\"96\":3,\"97\":3,\"98\":3,\"100\":1,\"106\":2,\"128\":1,\"135\":2,\"161\":1,\"197\":1,\"211\":2,\"217\":1,\"218\":2,\"235\":1,\"266\":2,\"267\":4,\"275\":2,\"276\":12,\"285\":1,\"286\":6,\"496\":1,\"536\":2,\"548\":1,\"551\":1,\"561\":1,\"567\":1,\"639\":4,\"674\":4,\"675\":1,\"691\":2,\"692\":1,\"702\":3,\"738\":2,\"746\":1,\"747\":1,\"759\":1,\"760\":2,\"768\":1,\"779\":1,\"790\":1,\"797\":2,\"815\":1,\"820\":2,\"821\":1,\"824\":1,\"830\":1,\"831\":1,\"846\":7,\"850\":1,\"980\":2,\"982\":1,\"1029\":6,\"1053\":1,\"1059\":1,\"1062\":1,\"1064\":2,\"1070\":3,\"1071\":3,\"1073\":3,\"1107\":2,\"1113\":2,\"1117\":3,\"1118\":1,\"1124\":1,\"1125\":2,\"1127\":1,\"1130\":3,\"1131\":3,\"1133\":2,\"1134\":1,\"1136\":2,\"1137\":1,\"1139\":1,\"1141\":4,\"1155\":5,\"1157\":6,\"1158\":2,\"1162\":3,\"1164\":1,\"1165\":1,\"1185\":1,\"1208\":5,\"1232\":2,\"1235\":6,\"1252\":1,\"1253\":4,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":2,\"1265\":2,\"1267\":2,\"1268\":3,\"1278\":2,\"1279\":4,\"1280\":4,\"1281\":5,\"1282\":5,\"1283\":3,\"1343\":1,\"1358\":1,\"1374\":2,\"1375\":3,\"1515\":1,\"1516\":1,\"1526\":4,\"1534\":1,\"1539\":1,\"1552\":3,\"1553\":6,\"1577\":2,\"1587\":3,\"1598\":5,\"1599\":4,\"1600\":5,\"1610\":2,\"1625\":5,\"1626\":7,\"1628\":2,\"1629\":3,\"1637\":1,\"1645\":1,\"1650\":1,\"1719\":7,\"1720\":4,\"1721\":2,\"1723\":1,\"1724\":2,\"1725\":8,\"1731\":5,\"1748\":5,\"1758\":2,\"1787\":2,\"1788\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1805\":1,\"1806\":4,\"1822\":2,\"1848\":2,\"1851\":4,\"1854\":3,\"1862\":1,\"1944\":2,\"1945\":1,\"1946\":1,\"1947\":2,\"1958\":1,\"1960\":1,\"1961\":1,\"1965\":1,\"1966\":2,\"1992\":1,\"1993\":1,\"2130\":6,\"2133\":7,\"2143\":1,\"2167\":1,\"2168\":1,\"2176\":2,\"2183\":4,\"2184\":1,\"2187\":1,\"2188\":2,\"2190\":4,\"2191\":1,\"2192\":1,\"2198\":1,\"2203\":1,\"2208\":4,\"2209\":1,\"2218\":1,\"2220\":3,\"2227\":1,\"2231\":2,\"2239\":1,\"2240\":1,\"2245\":1,\"2411\":1,\"2412\":1,\"2422\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2460\":1}}],[\"featured\",{\"1\":{\"18\":1}}],[\"featuires\",{\"1\":{\"699\":1}}],[\"feat\",{\"0\":{\"521\":1,\"567\":1},\"1\":{\"99\":2,\"100\":2,\"259\":1,\"315\":2,\"469\":2,\"515\":1,\"521\":2,\"567\":1,\"674\":2,\"692\":4,\"746\":1,\"760\":2,\"775\":1,\"784\":2,\"790\":3,\"820\":2,\"850\":4,\"969\":1,\"978\":2,\"999\":1,\"1010\":1,\"1159\":1,\"1389\":3,\"1396\":3,\"1401\":3,\"1408\":3,\"1466\":3,\"1526\":10,\"1553\":6,\"1590\":1,\"1598\":6,\"1600\":10,\"1625\":6,\"1662\":1,\"1702\":1,\"1720\":1,\"1723\":1,\"1724\":1,\"1756\":2,\"1757\":2,\"1785\":2,\"1787\":1,\"1789\":2,\"1790\":2,\"1794\":2,\"1817\":2,\"1875\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1984\":1,\"1992\":2,\"1993\":1,\"2130\":2,\"2133\":2,\"2136\":2,\"2137\":2,\"2183\":2,\"2184\":1,\"2190\":2,\"2208\":2,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2411\":1,\"2412\":1,\"2421\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"feats2npy\",{\"0\":{\"570\":1},\"1\":{\"570\":1}}],[\"feats2\",{\"1\":{\"97\":2,\"98\":4,\"99\":2,\"100\":2}}],[\"feats\",{\"0\":{\"548\":1,\"551\":1,\"561\":1,\"1972\":2,\"1978\":1,\"1980\":1,\"1982\":1,\"2232\":2,\"2233\":2,\"2238\":2,\"2244\":2,\"2401\":2,\"2404\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2434\":1,\"2436\":1,\"2437\":1,\"2438\":1,\"2439\":1,\"2440\":1,\"2443\":2,\"2449\":1},\"1\":{\"96\":2,\"97\":2,\"98\":4,\"99\":4,\"100\":4,\"101\":2,\"128\":1,\"196\":3,\"211\":1,\"213\":3,\"218\":2,\"220\":1,\"243\":4,\"259\":1,\"266\":5,\"267\":26,\"268\":3,\"275\":5,\"276\":11,\"277\":3,\"286\":15,\"521\":3,\"524\":1,\"525\":1,\"548\":1,\"551\":1,\"561\":1,\"621\":1,\"625\":7,\"639\":13,\"736\":1,\"737\":1,\"738\":3,\"777\":1,\"954\":2,\"958\":1,\"974\":4,\"980\":2,\"1155\":2,\"1156\":1,\"1157\":3,\"1158\":2,\"1395\":1,\"1521\":5,\"1526\":7,\"1552\":16,\"1553\":19,\"1577\":7,\"1582\":2,\"1585\":2,\"1587\":4,\"1589\":1,\"1590\":1,\"1598\":10,\"1599\":13,\"1600\":5,\"1601\":5,\"1602\":4,\"1610\":2,\"1611\":5,\"1624\":1,\"1625\":9,\"1626\":19,\"1629\":3,\"1637\":3,\"1640\":1,\"1641\":1,\"1662\":1,\"1702\":4,\"1851\":7,\"1875\":1,\"1881\":2,\"1882\":2,\"1914\":2,\"1940\":1,\"1942\":1,\"1951\":1,\"1959\":2,\"1965\":1,\"1971\":2,\"1972\":2,\"1975\":3,\"1978\":1,\"1980\":1,\"1982\":1,\"1992\":5,\"1993\":10,\"1995\":5,\"1996\":2,\"1997\":2,\"2044\":1,\"2127\":2,\"2184\":4,\"2216\":2,\"2221\":2,\"2222\":2,\"2228\":5,\"2229\":5,\"2232\":2,\"2233\":2,\"2235\":7,\"2236\":7,\"2238\":2,\"2239\":7,\"2240\":8,\"2241\":3,\"2244\":2,\"2245\":10,\"2325\":1,\"2327\":1,\"2401\":2,\"2403\":2,\"2404\":1,\"2405\":1,\"2407\":2,\"2408\":2,\"2409\":2,\"2411\":8,\"2412\":8,\"2413\":3,\"2414\":1,\"2416\":1,\"2418\":1,\"2422\":4,\"2423\":8,\"2424\":3,\"2431\":10,\"2432\":10,\"2434\":2,\"2436\":1,\"2437\":1,\"2438\":1,\"2439\":1,\"2440\":1,\"2443\":2,\"2445\":2,\"2446\":2,\"2447\":5,\"2448\":3,\"2449\":1,\"2460\":2,\"2462\":2}}],[\"feel\",{\"1\":{\"81\":1,\"96\":1}}],[\"feedforwardtransformerloss\",{\"0\":{\"1764\":1},\"1\":{\"1764\":1}}],[\"feedforward\",{\"0\":{\"630\":1,\"751\":1,\"818\":1},\"1\":{\"630\":5,\"643\":1,\"748\":1,\"751\":1,\"846\":1}}],[\"feedback\",{\"1\":{\"245\":1,\"246\":1,\"247\":2}}],[\"feed\",{\"0\":{\"630\":1,\"638\":1,\"1809\":1},\"1\":{\"43\":2,\"68\":1,\"141\":4,\"142\":2,\"618\":6,\"624\":6,\"630\":1,\"634\":1,\"638\":2,\"642\":2,\"643\":1,\"661\":1,\"692\":1,\"709\":1,\"710\":1,\"711\":1,\"735\":6,\"771\":1,\"774\":1,\"780\":1,\"846\":2,\"849\":1,\"1107\":1,\"1278\":1,\"1735\":3,\"1751\":3,\"1759\":3,\"1764\":2,\"1788\":1,\"1795\":1,\"1809\":2,\"1847\":3,\"1992\":1,\"1995\":1,\"2129\":1,\"2184\":1,\"2191\":1,\"2226\":2,\"2241\":1,\"2411\":1,\"2413\":1,\"2424\":1,\"2448\":1}}],[\"fernandes\",{\"1\":{\"10\":1}}],[\"fr=none\",{\"1\":{\"2506\":1}}],[\"fr\",{\"1\":{\"1327\":1,\"1330\":1}}],[\"friendly\",{\"1\":{\"907\":1,\"1387\":1,\"1807\":1}}],[\"fréchet\",{\"1\":{\"286\":1}}],[\"fresh\",{\"1\":{\"2018\":1}}],[\"french\",{\"1\":{\"287\":1,\"481\":1}}],[\"freq=40\",{\"1\":{\"2213\":1,\"2214\":1}}],[\"frequent\",{\"1\":{\"2000\":1,\"2001\":1}}],[\"frequencíes\",{\"1\":{\"1654\":2,\"1666\":1}}],[\"frequence\",{\"1\":{\"1174\":1,\"1514\":1}}],[\"frequencies\",{\"1\":{\"1145\":1,\"1264\":3,\"1334\":5,\"1558\":2,\"1654\":1,\"1666\":1}}],[\"frequencydomainl1\",{\"0\":{\"1173\":1},\"1\":{\"1173\":1}}],[\"frequencydomainloss\",{\"0\":{\"1174\":1},\"1\":{\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1}}],[\"frequencydomaindpcl\",{\"0\":{\"1172\":1},\"1\":{\"1172\":1}}],[\"frequencydomaincrossentropy\",{\"0\":{\"1171\":1},\"1\":{\"1171\":1}}],[\"frequencydomainabscoherence\",{\"0\":{\"1170\":1},\"1\":{\"1170\":1}}],[\"frequencydomainmse\",{\"0\":{\"1175\":1},\"1\":{\"225\":1,\"1175\":1}}],[\"frequency\",{\"1\":{\"69\":1,\"70\":2,\"71\":1,\"225\":1,\"267\":1,\"276\":1,\"516\":2,\"537\":1,\"1029\":3,\"1061\":7,\"1062\":3,\"1063\":2,\"1072\":1,\"1119\":1,\"1145\":3,\"1155\":1,\"1157\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1175\":1,\"1235\":2,\"1269\":1,\"1270\":1,\"1271\":3,\"1280\":4,\"1281\":2,\"1282\":2,\"1283\":1,\"1316\":6,\"1320\":3,\"1321\":1,\"1322\":1,\"1325\":1,\"1419\":2,\"1444\":1,\"1448\":1,\"1514\":1,\"1533\":4,\"1534\":4,\"1549\":1,\"1558\":3,\"1582\":1,\"1607\":2,\"1608\":1,\"1617\":1,\"1624\":1,\"1631\":1,\"1645\":1,\"1654\":1,\"1662\":2,\"1666\":1,\"1672\":1,\"1673\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1717\":2,\"2001\":1,\"2203\":1,\"2355\":1,\"2482\":3,\"2495\":3}}],[\"freqmask\",{\"0\":{\"1765\":1}}],[\"freqwiseblock\",{\"0\":{\"1168\":1},\"1\":{\"1168\":2}}],[\"freqs=257\",{\"1\":{\"1264\":1}}],[\"freqs\",{\"1\":{\"1145\":3,\"1168\":1,\"1182\":1,\"1183\":1,\"1264\":1,\"1558\":4}}],[\"freq\",{\"0\":{\"1419\":1,\"1884\":1},\"1\":{\"219\":1,\"243\":3,\"833\":3,\"978\":3,\"1029\":5,\"1062\":3,\"1070\":2,\"1071\":2,\"1073\":2,\"1107\":3,\"1117\":3,\"1118\":3,\"1125\":3,\"1126\":1,\"1127\":1,\"1130\":3,\"1136\":3,\"1141\":3,\"1145\":2,\"1162\":3,\"1168\":1,\"1198\":1,\"1217\":7,\"1232\":3,\"1235\":3,\"1252\":3,\"1261\":3,\"1267\":3,\"1268\":1,\"1278\":3,\"1279\":2,\"1280\":3,\"1281\":2,\"1282\":2,\"1283\":3,\"1316\":4,\"1317\":1,\"1403\":2,\"1419\":1,\"1511\":1,\"1514\":1,\"1533\":1,\"1549\":2,\"1582\":2,\"1624\":2,\"1655\":3,\"1664\":1,\"1665\":1,\"1669\":2,\"1670\":1,\"1672\":4,\"1673\":4,\"1687\":2,\"1689\":2,\"1690\":2,\"1691\":1,\"1699\":1,\"2196\":1}}],[\"freesound\",{\"1\":{\"201\":1}}],[\"free=$0\",{\"1\":{\"168\":2}}],[\"free\",{\"0\":{\"522\":1,\"564\":1,\"2380\":1},\"1\":{\"58\":2,\"60\":3,\"61\":3,\"62\":1,\"66\":1,\"106\":2,\"522\":3,\"564\":2,\"2380\":2}}],[\"freeze\",{\"0\":{\"89\":1},\"1\":{\"51\":8,\"89\":1,\"738\":1,\"745\":2,\"746\":1,\"747\":3,\"846\":3,\"864\":1,\"1334\":2}}],[\"freezing\",{\"0\":{\"51\":1},\"1\":{\"49\":1,\"51\":1,\"1334\":1}}],[\"freely\",{\"1\":{\"32\":1,\"37\":1,\"108\":1,\"110\":1}}],[\"fraction\",{\"1\":{\"2018\":3}}],[\"fractional\",{\"1\":{\"1692\":1}}],[\"framing\",{\"0\":{\"1356\":1,\"2379\":1},\"1\":{\"1356\":1,\"2379\":1}}],[\"framed\",{\"1\":{\"1315\":1,\"1350\":1}}],[\"framework\",{\"1\":{\"191\":1,\"232\":1,\"258\":1,\"262\":1,\"404\":1,\"1577\":1}}],[\"framescorefeats\",{\"0\":{\"2232\":1},\"1\":{\"2232\":2}}],[\"frames=none\",{\"1\":{\"1773\":1,\"1781\":1,\"1826\":1,\"1829\":1}}],[\"frameshift\",{\"1\":{\"128\":2}}],[\"frames\",{\"0\":{\"1889\":1},\"1\":{\"48\":7,\"135\":1,\"147\":1,\"148\":1,\"175\":1,\"535\":2,\"548\":2,\"551\":2,\"561\":2,\"617\":2,\"618\":2,\"620\":2,\"624\":2,\"626\":5,\"636\":2,\"644\":4,\"645\":2,\"661\":1,\"669\":1,\"696\":1,\"712\":1,\"846\":2,\"941\":1,\"976\":1,\"978\":3,\"1062\":3,\"1107\":3,\"1117\":3,\"1118\":3,\"1125\":3,\"1130\":3,\"1136\":3,\"1141\":3,\"1145\":1,\"1162\":3,\"1168\":1,\"1217\":7,\"1232\":3,\"1252\":3,\"1261\":3,\"1264\":2,\"1267\":3,\"1268\":1,\"1278\":3,\"1279\":1,\"1280\":5,\"1281\":1,\"1283\":5,\"1334\":7,\"1350\":7,\"1356\":2,\"1484\":1,\"1552\":1,\"1556\":1,\"1638\":2,\"1660\":1,\"1669\":2,\"1711\":1,\"1748\":1,\"1883\":3,\"1889\":3,\"1890\":2,\"1934\":1,\"2133\":1,\"2136\":1,\"2184\":1,\"2218\":2,\"2232\":2,\"2238\":1}}],[\"frame\",{\"0\":{\"2244\":1},\"1\":{\"48\":1,\"99\":1,\"135\":2,\"139\":1,\"175\":1,\"267\":2,\"275\":1,\"276\":6,\"515\":1,\"691\":1,\"702\":1,\"706\":3,\"712\":1,\"748\":1,\"831\":2,\"1031\":5,\"1035\":4,\"1045\":1,\"1112\":5,\"1113\":4,\"1128\":1,\"1232\":1,\"1250\":5,\"1251\":5,\"1260\":1,\"1261\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1334\":1,\"1350\":10,\"1356\":9,\"1406\":1,\"1529\":3,\"1548\":1,\"1559\":3,\"1629\":1,\"1702\":1,\"1711\":1,\"1748\":2,\"1752\":1,\"1788\":2,\"1957\":1,\"1961\":5,\"1977\":1,\"1997\":1,\"2133\":3,\"2136\":8,\"2143\":3,\"2183\":2,\"2184\":3,\"2187\":1,\"2190\":2,\"2192\":1,\"2198\":1,\"2203\":1,\"2208\":2,\"2209\":1,\"2218\":4,\"2228\":6,\"2239\":9,\"2244\":1,\"2378\":2,\"2379\":2}}],[\"frank\",{\"1\":{\"13\":1}}],[\"frontends\",{\"0\":{\"1766\":1,\"1885\":1},\"1\":{\"752\":1,\"1766\":1,\"1885\":1}}],[\"frontend\",{\"0\":{\"271\":1,\"280\":1,\"287\":1,\"639\":1,\"680\":2,\"691\":2,\"702\":1,\"712\":1,\"720\":1,\"722\":1,\"738\":1,\"750\":1,\"752\":1,\"759\":1,\"778\":1,\"815\":1,\"831\":1,\"854\":1,\"864\":1,\"887\":1,\"1522\":1,\"1539\":1,\"1766\":2,\"1885\":2,\"1957\":1,\"1960\":1,\"1961\":1,\"2230\":1,\"2234\":1},\"1\":{\"128\":4,\"222\":1,\"226\":1,\"243\":1,\"265\":1,\"266\":1,\"274\":1,\"275\":1,\"284\":1,\"285\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"536\":2,\"625\":3,\"639\":4,\"675\":1,\"680\":2,\"686\":1,\"691\":5,\"702\":1,\"712\":1,\"720\":3,\"722\":1,\"736\":2,\"737\":3,\"738\":4,\"750\":1,\"752\":1,\"759\":1,\"768\":5,\"777\":2,\"778\":1,\"815\":3,\"831\":1,\"854\":1,\"864\":1,\"887\":1,\"954\":3,\"958\":1,\"974\":3,\"1155\":2,\"1156\":3,\"1157\":2,\"1158\":2,\"1522\":1,\"1539\":1,\"1640\":3,\"1641\":3,\"1702\":2,\"1766\":5,\"1863\":1,\"1885\":3,\"1893\":1,\"1957\":1,\"1959\":3,\"1960\":2,\"1961\":2,\"1975\":2,\"1980\":1,\"1982\":1,\"1996\":2,\"1997\":3,\"2127\":3,\"2184\":1,\"2188\":2,\"2216\":1,\"2221\":3,\"2416\":1,\"2418\":1,\"2462\":2}}],[\"front\",{\"1\":{\"8\":1,\"135\":1,\"228\":1,\"260\":1,\"262\":3}}],[\"from=\",{\"1\":{\"1334\":1}}],[\"from\",{\"0\":{\"74\":1,\"106\":1,\"118\":1,\"174\":1,\"232\":1,\"258\":1,\"518\":1,\"575\":1,\"898\":1,\"1366\":1,\"1953\":1,\"2150\":1,\"2311\":1},\"1\":{\"1\":1,\"3\":4,\"19\":2,\"25\":1,\"26\":3,\"32\":1,\"37\":2,\"43\":2,\"45\":1,\"47\":1,\"48\":1,\"55\":1,\"68\":1,\"70\":1,\"76\":2,\"78\":2,\"82\":4,\"87\":2,\"94\":1,\"96\":2,\"104\":1,\"106\":2,\"109\":2,\"110\":1,\"123\":1,\"128\":1,\"136\":1,\"139\":1,\"141\":1,\"147\":1,\"152\":1,\"161\":1,\"162\":2,\"167\":1,\"173\":2,\"174\":3,\"175\":2,\"184\":1,\"197\":2,\"200\":1,\"201\":1,\"203\":1,\"205\":3,\"207\":1,\"211\":2,\"217\":1,\"219\":1,\"220\":2,\"223\":1,\"224\":1,\"235\":1,\"236\":1,\"242\":3,\"243\":4,\"246\":1,\"247\":1,\"259\":3,\"261\":1,\"262\":3,\"266\":2,\"267\":8,\"269\":6,\"270\":1,\"271\":1,\"273\":1,\"275\":4,\"276\":9,\"278\":6,\"279\":1,\"280\":1,\"285\":2,\"286\":15,\"287\":1,\"288\":1,\"290\":14,\"518\":2,\"527\":1,\"536\":1,\"541\":1,\"548\":1,\"551\":1,\"575\":2,\"614\":1,\"616\":2,\"620\":1,\"634\":1,\"639\":1,\"641\":1,\"643\":1,\"651\":1,\"674\":1,\"696\":3,\"697\":3,\"699\":4,\"704\":1,\"759\":1,\"768\":2,\"777\":2,\"786\":2,\"787\":1,\"790\":1,\"791\":1,\"800\":2,\"817\":1,\"821\":1,\"823\":1,\"824\":1,\"829\":1,\"847\":1,\"864\":1,\"867\":2,\"898\":1,\"919\":1,\"921\":2,\"922\":1,\"929\":1,\"930\":1,\"935\":2,\"936\":1,\"937\":1,\"939\":2,\"961\":1,\"980\":1,\"987\":1,\"989\":1,\"1000\":1,\"1011\":1,\"1029\":1,\"1051\":2,\"1053\":2,\"1057\":1,\"1076\":1,\"1086\":1,\"1107\":1,\"1117\":2,\"1130\":2,\"1131\":2,\"1134\":1,\"1136\":2,\"1137\":1,\"1139\":1,\"1141\":2,\"1153\":1,\"1156\":2,\"1162\":1,\"1185\":1,\"1207\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1224\":1,\"1225\":1,\"1228\":2,\"1232\":2,\"1235\":1,\"1242\":1,\"1245\":2,\"1247\":1,\"1257\":1,\"1261\":1,\"1264\":1,\"1267\":2,\"1268\":3,\"1278\":1,\"1279\":3,\"1280\":4,\"1281\":2,\"1282\":1,\"1283\":3,\"1308\":1,\"1312\":1,\"1324\":1,\"1332\":1,\"1334\":2,\"1350\":1,\"1366\":1,\"1373\":1,\"1381\":2,\"1385\":1,\"1391\":1,\"1400\":1,\"1403\":1,\"1410\":1,\"1441\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":2,\"1460\":2,\"1468\":1,\"1469\":1,\"1478\":1,\"1505\":1,\"1506\":1,\"1513\":2,\"1514\":1,\"1515\":2,\"1524\":1,\"1530\":1,\"1533\":1,\"1541\":1,\"1548\":2,\"1551\":2,\"1584\":2,\"1587\":2,\"1592\":2,\"1596\":2,\"1597\":4,\"1598\":2,\"1599\":7,\"1600\":2,\"1605\":2,\"1606\":2,\"1609\":2,\"1610\":2,\"1618\":1,\"1619\":2,\"1628\":2,\"1637\":2,\"1643\":1,\"1644\":1,\"1645\":1,\"1646\":1,\"1650\":1,\"1701\":1,\"1717\":1,\"1720\":1,\"1721\":1,\"1726\":1,\"1727\":1,\"1734\":1,\"1736\":1,\"1749\":1,\"1750\":1,\"1752\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1815\":1,\"1843\":1,\"1855\":1,\"1856\":1,\"1873\":2,\"1877\":2,\"1882\":2,\"1892\":1,\"1900\":1,\"1907\":1,\"1920\":1,\"1921\":1,\"1923\":1,\"1924\":1,\"1932\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":1,\"1947\":1,\"1948\":1,\"1949\":1,\"1950\":1,\"1951\":1,\"1953\":1,\"1965\":1,\"1977\":2,\"1992\":1,\"1995\":1,\"1997\":1,\"2001\":1,\"2015\":4,\"2016\":1,\"2018\":3,\"2043\":1,\"2049\":3,\"2055\":1,\"2056\":1,\"2066\":1,\"2101\":1,\"2116\":1,\"2130\":2,\"2131\":2,\"2132\":1,\"2134\":3,\"2136\":3,\"2138\":1,\"2140\":2,\"2148\":1,\"2149\":1,\"2150\":2,\"2151\":1,\"2162\":1,\"2187\":1,\"2192\":1,\"2198\":2,\"2203\":1,\"2209\":1,\"2215\":1,\"2219\":1,\"2223\":1,\"2235\":3,\"2236\":3,\"2240\":1,\"2246\":3,\"2248\":3,\"2249\":5,\"2250\":3,\"2251\":3,\"2252\":3,\"2253\":3,\"2254\":3,\"2255\":3,\"2256\":3,\"2257\":3,\"2259\":3,\"2260\":3,\"2261\":3,\"2263\":4,\"2264\":3,\"2265\":2,\"2266\":3,\"2267\":3,\"2268\":4,\"2269\":3,\"2270\":4,\"2271\":4,\"2272\":3,\"2273\":3,\"2287\":1,\"2298\":1,\"2299\":1,\"2307\":1,\"2309\":1,\"2310\":1,\"2311\":4,\"2325\":1,\"2327\":1,\"2344\":1,\"2354\":4,\"2355\":1,\"2366\":1,\"2377\":3,\"2412\":6,\"2423\":6,\"2427\":1,\"2435\":4,\"2447\":6,\"2462\":1,\"2470\":1,\"2474\":1,\"2483\":1,\"2489\":1}}],[\"fues\",{\"1\":{\"2202\":1,\"2214\":1}}],[\"fuction\",{\"1\":{\"777\":1,\"1156\":1,\"1940\":1,\"1942\":1}}],[\"fusedpostfrontends\",{\"0\":{\"1522\":1},\"1\":{\"1522\":1}}],[\"fusedfrontends\",{\"0\":{\"752\":1},\"1\":{\"752\":1}}],[\"fused\",{\"0\":{\"752\":1,\"1522\":1},\"1\":{\"675\":1,\"752\":1,\"1301\":2,\"1372\":2,\"1522\":1,\"1555\":1}}],[\"fuse\",{\"1\":{\"674\":2,\"2213\":1}}],[\"fusion\",{\"1\":{\"616\":1,\"675\":1,\"696\":1,\"697\":1,\"746\":1,\"780\":3}}],[\"fundamental\",{\"1\":{\"267\":1,\"276\":1}}],[\"functor\",{\"1\":{\"2335\":1,\"2350\":1}}],[\"functrans\",{\"0\":{\"1767\":1}}],[\"functioin\",{\"1\":{\"269\":1,\"278\":1}}],[\"function`\",{\"1\":{\"2420\":1}}],[\"functional\",{\"0\":{\"1767\":1},\"1\":{\"833\":1,\"887\":1,\"921\":1,\"935\":1,\"1051\":1,\"1668\":3}}],[\"functionality\",{\"1\":{\"246\":1,\"2354\":1}}],[\"functionalities\",{\"1\":{\"194\":1}}],[\"functions\",{\"1\":{\"3\":2,\"143\":1,\"191\":1,\"222\":1,\"225\":1,\"263\":2,\"290\":1,\"661\":1,\"912\":1,\"1245\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1312\":1,\"1845\":1,\"2215\":1,\"2239\":1,\"2240\":1,\"2435\":1}}],[\"function\",{\"0\":{\"1573\":1},\"1\":{\"2\":1,\"3\":2,\"68\":1,\"82\":1,\"132\":1,\"141\":1,\"142\":2,\"143\":1,\"225\":1,\"254\":2,\"262\":1,\"619\":1,\"632\":1,\"633\":1,\"634\":2,\"635\":2,\"637\":1,\"638\":1,\"652\":1,\"654\":4,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"664\":4,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"691\":3,\"694\":1,\"699\":2,\"709\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"747\":1,\"750\":1,\"753\":1,\"756\":8,\"758\":1,\"768\":3,\"773\":8,\"774\":1,\"777\":1,\"779\":1,\"780\":2,\"782\":1,\"784\":1,\"789\":1,\"792\":1,\"794\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"824\":1,\"826\":1,\"828\":1,\"829\":1,\"830\":2,\"831\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"841\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"866\":5,\"867\":5,\"902\":1,\"929\":3,\"951\":1,\"953\":1,\"957\":1,\"960\":3,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"978\":2,\"1029\":1,\"1031\":2,\"1033\":1,\"1035\":3,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1051\":1,\"1052\":1,\"1054\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1066\":1,\"1067\":1,\"1069\":1,\"1070\":1,\"1071\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1107\":2,\"1109\":1,\"1111\":1,\"1112\":1,\"1113\":2,\"1115\":1,\"1117\":1,\"1121\":1,\"1123\":1,\"1126\":1,\"1127\":1,\"1130\":1,\"1131\":1,\"1135\":1,\"1136\":1,\"1138\":1,\"1139\":1,\"1140\":1,\"1141\":2,\"1143\":1,\"1145\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1156\":1,\"1160\":1,\"1163\":1,\"1164\":2,\"1166\":1,\"1169\":1,\"1178\":1,\"1185\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1199\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1232\":1,\"1234\":1,\"1235\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1245\":1,\"1249\":1,\"1250\":1,\"1251\":2,\"1253\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1261\":1,\"1263\":1,\"1264\":1,\"1265\":1,\"1266\":1,\"1267\":2,\"1268\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1273\":2,\"1274\":2,\"1278\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1328\":1,\"1329\":1,\"1333\":1,\"1334\":1,\"1335\":1,\"1384\":1,\"1388\":1,\"1392\":2,\"1393\":1,\"1397\":2,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1450\":4,\"1451\":1,\"1452\":4,\"1453\":1,\"1454\":2,\"1455\":1,\"1456\":2,\"1457\":1,\"1458\":2,\"1459\":1,\"1460\":2,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1513\":2,\"1518\":1,\"1519\":1,\"1523\":1,\"1526\":1,\"1528\":1,\"1531\":1,\"1535\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1546\":1,\"1548\":2,\"1550\":1,\"1551\":2,\"1552\":1,\"1555\":1,\"1573\":1,\"1582\":2,\"1592\":2,\"1596\":2,\"1597\":2,\"1599\":3,\"1604\":4,\"1605\":5,\"1606\":4,\"1609\":2,\"1614\":2,\"1615\":4,\"1619\":5,\"1621\":3,\"1622\":1,\"1624\":2,\"1626\":1,\"1639\":1,\"1645\":1,\"1653\":1,\"1656\":1,\"1658\":1,\"1660\":1,\"1661\":2,\"1663\":1,\"1664\":1,\"1665\":1,\"1668\":8,\"1669\":1,\"1670\":1,\"1671\":1,\"1677\":1,\"1678\":1,\"1680\":1,\"1692\":2,\"1697\":1,\"1698\":2,\"1720\":1,\"1721\":1,\"1725\":1,\"1750\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1764\":1,\"1770\":1,\"1771\":1,\"1777\":1,\"1782\":1,\"1789\":1,\"1790\":1,\"1806\":2,\"1809\":1,\"1833\":1,\"1838\":1,\"1839\":1,\"1862\":1,\"1864\":2,\"1865\":2,\"1867\":2,\"1881\":1,\"1886\":1,\"1917\":1,\"1939\":1,\"1941\":2,\"1943\":2,\"1946\":1,\"1958\":1,\"1966\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"1993\":1,\"2007\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2101\":1,\"2125\":1,\"2134\":1,\"2140\":1,\"2142\":2,\"2166\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2176\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2191\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2202\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2213\":1,\"2214\":1,\"2217\":1,\"2223\":1,\"2226\":1,\"2232\":2,\"2235\":1,\"2237\":1,\"2238\":1,\"2239\":2,\"2240\":2,\"2241\":1,\"2245\":2,\"2246\":2,\"2247\":4,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2258\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2262\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2293\":1,\"2306\":1,\"2312\":1,\"2326\":1,\"2354\":2,\"2355\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2411\":1,\"2412\":1,\"2413\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2420\":1,\"2423\":1,\"2424\":1,\"2431\":2,\"2435\":2,\"2436\":3,\"2438\":2,\"2440\":2,\"2444\":1,\"2447\":1,\"2448\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1,\"2482\":1,\"2483\":1,\"2487\":1,\"2490\":1}}],[\"funcs\",{\"0\":{\"1948\":2,\"1949\":1,\"1950\":1,\"1951\":1,\"1952\":2,\"1953\":2,\"1954\":2,\"1955\":2,\"2309\":1,\"2323\":1,\"2533\":1},\"1\":{\"1648\":1,\"1948\":2,\"1949\":1,\"1950\":1,\"1951\":1,\"1952\":2,\"1953\":2,\"1954\":2,\"1955\":2,\"2309\":1,\"2323\":1}}],[\"func\",{\"0\":{\"2486\":1},\"1\":{\"1251\":1,\"1668\":2,\"2235\":1,\"2236\":1,\"2486\":1,\"2487\":1,\"2488\":2,\"2494\":1}}],[\"func=true\",{\"1\":{\"254\":2}}],[\"funcodecgenerator\",{\"0\":{\"1403\":1},\"1\":{\"1403\":1}}],[\"funcodecdiscriminator\",{\"0\":{\"1402\":1},\"1\":{\"1402\":1}}],[\"funcodec\",{\"0\":{\"1401\":3,\"1402\":2,\"1403\":2},\"1\":{\"219\":1,\"1385\":2,\"1401\":5,\"1402\":4,\"1403\":7}}],[\"furthermore\",{\"1\":{\"1716\":1}}],[\"further\",{\"1\":{\"240\":1,\"242\":1,\"262\":1,\"267\":1,\"276\":1,\"286\":1,\"696\":1,\"697\":1}}],[\"future\",{\"1\":{\"213\":1,\"232\":1,\"258\":1,\"268\":1,\"277\":1,\"709\":1,\"774\":1,\"780\":1,\"1334\":3,\"2184\":2,\"2191\":1,\"2369\":1}}],[\"futami\",{\"1\":{\"202\":1}}],[\"fujita\",{\"1\":{\"207\":1}}],[\"fullscorer\",{\"1\":{\"1798\":1}}],[\"fully\",{\"1\":{\"26\":1,\"107\":2,\"267\":1,\"276\":1,\"286\":1,\"691\":1,\"1165\":1,\"1331\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"2249\":1,\"2253\":1}}],[\"full\",{\"1\":{\"8\":1,\"18\":2,\"38\":1,\"71\":1,\"97\":1,\"107\":1,\"142\":1,\"147\":1,\"217\":1,\"236\":1,\"242\":1,\"286\":4,\"633\":1,\"634\":1,\"661\":1,\"669\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"821\":1,\"829\":2,\"830\":1,\"832\":1,\"859\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1484\":1,\"1719\":11,\"1720\":4,\"1725\":15,\"1798\":1,\"1799\":1,\"1800\":1,\"1806\":6,\"1853\":1,\"1862\":1,\"1993\":2,\"2130\":1,\"2131\":1,\"2143\":1,\"2298\":1}}],[\"fit\",{\"1\":{\"2147\":2,\"2333\":2,\"2355\":3}}],[\"fig\",{\"1\":{\"1124\":1}}],[\"figdir\",{\"1\":{\"606\":2}}],[\"five\",{\"1\":{\"286\":1}}],[\"fidelity\",{\"1\":{\"267\":1,\"276\":1,\"286\":1,\"1061\":1,\"1062\":1}}],[\"fixme\",{\"1\":{\"2016\":1}}],[\"fixing\",{\"1\":{\"269\":1,\"278\":1}}],[\"fixes\",{\"1\":{\"269\":1,\"278\":1}}],[\"fixedordersolver\",{\"0\":{\"1167\":1},\"1\":{\"1167\":1}}],[\"fixed\",{\"0\":{\"1167\":1},\"1\":{\"91\":1,\"104\":1,\"135\":1,\"223\":1,\"242\":1,\"269\":1,\"278\":1,\"295\":1,\"415\":1,\"701\":2,\"1163\":1,\"1164\":1,\"1167\":2,\"1250\":1,\"1251\":1,\"1264\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1333\":1,\"1334\":1,\"1590\":1,\"1919\":2}}],[\"fix\",{\"1\":{\"242\":1,\"290\":2,\"368\":2,\"377\":2,\"406\":2,\"484\":2,\"490\":2,\"1119\":3,\"2235\":1,\"2236\":1,\"2353\":4}}],[\"fi\",{\"1\":{\"197\":1}}],[\"field\",{\"1\":{\"142\":1,\"481\":2,\"1002\":1,\"1610\":2,\"1628\":2,\"1722\":6,\"1807\":5,\"2184\":1,\"2325\":1,\"2327\":1}}],[\"fist\",{\"1\":{\"97\":1}}],[\"filts=5\",{\"1\":{\"1750\":1,\"1758\":1,\"1810\":1,\"2223\":1,\"2231\":1}}],[\"filts\",{\"1\":{\"796\":2,\"869\":2,\"1526\":1,\"1600\":1,\"1706\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1715\":2,\"1716\":2,\"1750\":1,\"1758\":1,\"1768\":2,\"1810\":1,\"1895\":2,\"1993\":4,\"2223\":1,\"2231\":1,\"2235\":4,\"2236\":4,\"2239\":2,\"2240\":2,\"2245\":6,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":6,\"2432\":4,\"2447\":2}}],[\"filt\",{\"0\":{\"572\":1},\"1\":{\"572\":2}}],[\"filter=true\",{\"1\":{\"2224\":1}}],[\"filter=asteroid\",{\"1\":{\"1053\":1}}],[\"filterw\",{\"1\":{\"1301\":1,\"1372\":1}}],[\"filterh\",{\"1\":{\"1301\":1,\"1372\":1}}],[\"filterbanks\",{\"1\":{\"1533\":1,\"1558\":2,\"1631\":1,\"1993\":1,\"2431\":1,\"2432\":1}}],[\"filterbank\",{\"1\":{\"691\":4,\"768\":1,\"1558\":1,\"1668\":1,\"1810\":1,\"2187\":1,\"2198\":1,\"2495\":2}}],[\"filters\",{\"1\":{\"691\":2,\"768\":3,\"978\":1,\"982\":1,\"1246\":1,\"1273\":1,\"1274\":1,\"1389\":1,\"1391\":1,\"1392\":6,\"1396\":2,\"1397\":3,\"1401\":1,\"1403\":1,\"1408\":1,\"1409\":1,\"1422\":3,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1466\":1,\"1468\":1,\"1509\":2,\"1511\":2,\"1513\":1,\"1543\":1,\"1548\":1,\"1552\":2,\"1553\":2,\"1631\":1,\"1662\":1,\"1668\":5}}],[\"filtering\",{\"0\":{\"1351\":1,\"1672\":1,\"1673\":1,\"1687\":1,\"1689\":1,\"1690\":1},\"1\":{\"210\":1,\"211\":2,\"218\":4,\"265\":1,\"266\":2,\"267\":4,\"274\":1,\"275\":2,\"276\":4,\"286\":4,\"1066\":1,\"1293\":1,\"1321\":1,\"1322\":1,\"1330\":1,\"1351\":2,\"1668\":1,\"1672\":1,\"1673\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"2308\":2}}],[\"filter\",{\"0\":{\"925\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1317\":1,\"1352\":1,\"1631\":1,\"2308\":1},\"1\":{\"74\":1,\"514\":3,\"572\":1,\"691\":1,\"768\":1,\"925\":1,\"1066\":3,\"1162\":1,\"1246\":5,\"1301\":1,\"1306\":5,\"1309\":2,\"1310\":3,\"1311\":1,\"1317\":3,\"1323\":1,\"1327\":3,\"1330\":3,\"1334\":1,\"1351\":3,\"1352\":3,\"1354\":1,\"1371\":5,\"1372\":1,\"1376\":1,\"1377\":1,\"1520\":3,\"1533\":2,\"1552\":1,\"1558\":1,\"1608\":1,\"1631\":4,\"1654\":2,\"1666\":2,\"1668\":4,\"1672\":2,\"1673\":2,\"1687\":2,\"1689\":2,\"1690\":2,\"1706\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1715\":1,\"1716\":3,\"1750\":2,\"1768\":2,\"1810\":2,\"1854\":3,\"1895\":1,\"1993\":4,\"2223\":3,\"2224\":2,\"2231\":2,\"2235\":4,\"2236\":4,\"2245\":13,\"2308\":2,\"2431\":6,\"2432\":2}}],[\"filtered\",{\"1\":{\"67\":1,\"1672\":1,\"1673\":1,\"1679\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1692\":1}}],[\"filled\",{\"1\":{\"1946\":1,\"2136\":1}}],[\"fill=0\",{\"1\":{\"1905\":1}}],[\"fill\",{\"1\":{\"125\":1,\"1356\":1,\"1905\":2,\"2130\":1}}],[\"filenotfounderror\",{\"1\":{\"2134\":1}}],[\"filename\",{\"1\":{\"60\":1,\"1948\":1}}],[\"fileobj\",{\"1\":{\"1948\":1}}],[\"filepath\",{\"1\":{\"1824\":2,\"2394\":1}}],[\"file1\",{\"1\":{\"1002\":5}}],[\"fileio\",{\"0\":{\"985\":1,\"987\":1,\"989\":1,\"991\":1,\"993\":1,\"995\":1,\"996\":1,\"998\":1,\"1000\":1,\"1001\":1,\"1003\":1,\"1005\":1,\"1007\":1,\"1009\":1,\"1011\":1,\"1013\":1,\"1015\":1,\"1017\":1,\"1019\":1,\"1021\":1,\"1022\":1,\"1024\":1,\"1026\":1,\"1028\":1,\"2521\":1},\"1\":{\"985\":1,\"987\":1,\"989\":1,\"991\":1,\"993\":1,\"995\":1,\"996\":1,\"998\":1,\"1000\":1,\"1001\":1,\"1003\":1,\"1005\":1,\"1007\":1,\"1009\":1,\"1011\":1,\"1013\":1,\"1015\":1,\"1017\":1,\"1019\":1,\"1021\":1,\"1022\":1,\"1024\":1,\"1026\":1,\"1028\":1}}],[\"filetype=\",{\"1\":{\"1728\":1,\"1802\":1,\"1813\":1}}],[\"filetype\",{\"1\":{\"521\":1,\"524\":1,\"525\":1,\"548\":1,\"551\":1,\"558\":1,\"561\":2,\"567\":1,\"575\":1,\"1881\":2,\"1883\":2}}],[\"file=<\",{\"1\":{\"2249\":1}}],[\"file=\",{\"1\":{\"290\":10}}],[\"files\",{\"0\":{\"75\":1,\"2074\":1},\"1\":{\"2\":2,\"3\":12,\"24\":1,\"32\":5,\"37\":1,\"68\":2,\"69\":4,\"71\":4,\"76\":3,\"85\":1,\"98\":1,\"109\":1,\"133\":1,\"150\":1,\"195\":1,\"200\":3,\"201\":2,\"205\":3,\"211\":3,\"217\":1,\"223\":10,\"224\":5,\"228\":2,\"235\":1,\"236\":1,\"242\":4,\"243\":3,\"254\":2,\"263\":1,\"266\":1,\"269\":1,\"275\":1,\"278\":1,\"285\":1,\"286\":2,\"290\":1,\"374\":2,\"402\":1,\"461\":1,\"514\":1,\"515\":2,\"554\":1,\"556\":1,\"592\":1,\"1008\":1,\"1009\":3,\"1824\":2,\"1949\":1,\"1954\":2,\"1955\":2,\"2000\":2,\"2001\":2,\"2002\":1,\"2003\":1,\"2004\":1,\"2007\":3,\"2008\":1,\"2134\":1,\"2139\":1,\"2157\":1,\"2208\":1,\"2249\":1,\"2258\":2,\"2354\":1,\"2474\":1}}],[\"file\",{\"0\":{\"68\":1,\"70\":1,\"71\":1,\"80\":1,\"85\":1,\"113\":1,\"114\":1,\"1881\":1,\"1883\":1,\"2156\":1},\"1\":{\"1\":1,\"3\":1,\"23\":1,\"26\":1,\"47\":6,\"60\":5,\"62\":2,\"63\":1,\"64\":1,\"68\":1,\"69\":2,\"70\":1,\"71\":1,\"76\":2,\"79\":5,\"80\":4,\"96\":5,\"97\":4,\"98\":8,\"99\":4,\"100\":4,\"101\":2,\"106\":3,\"110\":1,\"119\":1,\"126\":1,\"127\":1,\"128\":1,\"130\":1,\"136\":1,\"166\":2,\"167\":1,\"168\":1,\"196\":6,\"200\":5,\"201\":1,\"211\":6,\"213\":5,\"218\":2,\"223\":7,\"224\":1,\"225\":1,\"235\":1,\"242\":2,\"243\":3,\"249\":1,\"259\":1,\"263\":1,\"267\":16,\"268\":9,\"276\":13,\"277\":9,\"286\":7,\"290\":3,\"295\":2,\"301\":12,\"309\":4,\"315\":8,\"321\":6,\"327\":4,\"331\":4,\"335\":4,\"342\":4,\"349\":4,\"356\":2,\"361\":4,\"368\":4,\"374\":2,\"377\":4,\"385\":4,\"389\":8,\"396\":10,\"403\":34,\"404\":4,\"406\":6,\"415\":2,\"421\":10,\"429\":10,\"436\":4,\"442\":10,\"449\":4,\"457\":4,\"463\":16,\"469\":8,\"475\":4,\"484\":6,\"490\":6,\"496\":4,\"498\":10,\"505\":8,\"511\":2,\"513\":1,\"518\":5,\"521\":1,\"524\":1,\"525\":1,\"527\":4,\"536\":1,\"545\":1,\"570\":1,\"572\":1,\"583\":1,\"589\":1,\"594\":1,\"600\":1,\"606\":1,\"699\":1,\"760\":1,\"768\":1,\"930\":1,\"987\":1,\"989\":1,\"994\":1,\"996\":2,\"998\":2,\"1000\":1,\"1008\":2,\"1019\":1,\"1021\":1,\"1022\":1,\"1024\":1,\"1026\":1,\"1312\":1,\"1324\":1,\"1548\":1,\"1656\":3,\"1678\":1,\"1721\":2,\"1824\":5,\"1881\":1,\"1882\":4,\"1883\":6,\"1948\":1,\"1949\":2,\"1955\":1,\"1956\":3,\"1999\":1,\"2000\":11,\"2001\":5,\"2002\":1,\"2005\":2,\"2006\":3,\"2007\":1,\"2008\":1,\"2134\":7,\"2138\":1,\"2141\":2,\"2144\":4,\"2156\":3,\"2157\":2,\"2161\":2,\"2249\":8,\"2263\":3,\"2268\":3,\"2270\":3,\"2271\":3,\"2344\":1,\"2351\":1,\"2353\":3,\"2364\":3,\"2366\":1,\"2422\":2,\"2474\":1}}],[\"firn\",{\"1\":{\"1301\":1,\"1306\":1,\"1371\":1,\"1372\":1}}],[\"firw\",{\"1\":{\"1301\":1,\"1306\":1,\"1371\":1,\"1372\":1}}],[\"firh\",{\"1\":{\"1301\":1,\"1306\":1,\"1371\":1,\"1372\":1}}],[\"fir=false\",{\"1\":{\"1238\":1}}],[\"fir=true\",{\"1\":{\"1211\":1}}],[\"fir\",{\"1\":{\"1211\":1,\"1238\":1,\"1301\":1,\"1306\":1,\"1371\":1,\"1372\":1}}],[\"firefox\",{\"1\":{\"248\":1}}],[\"firewall\",{\"1\":{\"66\":1}}],[\"first=false\",{\"1\":{\"1215\":1}}],[\"firstly\",{\"1\":{\"153\":1}}],[\"first\",{\"1\":{\"19\":1,\"24\":1,\"50\":1,\"51\":1,\"67\":2,\"69\":1,\"79\":1,\"80\":1,\"82\":1,\"97\":2,\"98\":1,\"127\":1,\"138\":2,\"141\":1,\"171\":1,\"172\":1,\"173\":1,\"175\":1,\"201\":1,\"212\":1,\"218\":1,\"224\":1,\"243\":2,\"267\":10,\"269\":1,\"276\":8,\"278\":1,\"286\":6,\"287\":1,\"436\":2,\"536\":1,\"649\":3,\"654\":5,\"665\":1,\"673\":3,\"674\":2,\"691\":1,\"692\":1,\"703\":4,\"709\":1,\"710\":1,\"711\":1,\"716\":3,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"755\":1,\"756\":3,\"771\":1,\"773\":3,\"774\":1,\"780\":1,\"785\":1,\"846\":2,\"849\":1,\"851\":1,\"866\":2,\"867\":2,\"878\":1,\"879\":1,\"881\":1,\"884\":1,\"939\":1,\"1107\":1,\"1133\":1,\"1278\":1,\"1279\":1,\"1280\":2,\"1281\":1,\"1283\":2,\"1400\":1,\"1402\":1,\"1419\":1,\"1467\":1,\"1514\":2,\"1533\":1,\"1545\":1,\"1594\":1,\"1595\":1,\"1597\":2,\"1604\":3,\"1606\":2,\"1628\":4,\"1638\":1,\"1647\":1,\"1655\":1,\"1719\":2,\"1725\":3,\"1735\":1,\"1751\":2,\"1759\":1,\"1794\":1,\"1806\":1,\"1957\":1,\"1992\":1,\"1995\":1,\"2014\":4,\"2018\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2136\":1,\"2143\":2,\"2155\":1,\"2191\":1,\"2198\":1,\"2348\":2,\"2355\":1,\"2361\":1,\"2372\":2}}],[\"finnish\",{\"1\":{\"287\":1,\"481\":1}}],[\"final=true\",{\"1\":{\"710\":1,\"711\":1}}],[\"finally\",{\"1\":{\"224\":1,\"225\":1,\"276\":1}}],[\"final\",{\"1\":{\"45\":1,\"128\":1,\"141\":2,\"145\":1,\"262\":1,\"616\":3,\"639\":9,\"661\":2,\"674\":4,\"696\":2,\"697\":2,\"710\":1,\"711\":1,\"738\":3,\"828\":1,\"829\":1,\"830\":1,\"846\":4,\"978\":1,\"1389\":3,\"1391\":2,\"1396\":3,\"1401\":3,\"1403\":2,\"1408\":1,\"1450\":5,\"1452\":5,\"1466\":3,\"1468\":2,\"1513\":1,\"1526\":1,\"1548\":1,\"1551\":1,\"1553\":1,\"1587\":3,\"1592\":1,\"1596\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1605\":4,\"1625\":1,\"1720\":3,\"1727\":1,\"1814\":1,\"1822\":2,\"1868\":1,\"1870\":1,\"1910\":1,\"1915\":1,\"1927\":1,\"1966\":2,\"2000\":2,\"2001\":1,\"2015\":2,\"2018\":5,\"2216\":1,\"2217\":1,\"2218\":6,\"2219\":3}}],[\"finalize\",{\"1\":{\"3\":1}}],[\"finished\",{\"1\":{\"133\":1,\"164\":1,\"286\":2,\"1804\":1,\"2147\":1,\"2367\":1}}],[\"finish\",{\"1\":{\"39\":1,\"113\":1,\"2359\":1}}],[\"finds\",{\"1\":{\"2489\":1}}],[\"finding\",{\"1\":{\"2147\":1,\"2380\":1}}],[\"find\",{\"0\":{\"1952\":1,\"2078\":1,\"2079\":1},\"1\":{\"18\":1,\"19\":1,\"31\":1,\"69\":1,\"85\":1,\"106\":1,\"107\":1,\"263\":1,\"267\":2,\"272\":1,\"276\":2,\"282\":1,\"285\":2,\"286\":6,\"289\":2,\"290\":2,\"536\":1,\"1209\":1,\"1228\":1,\"1720\":1,\"1721\":1,\"1725\":1,\"1806\":1,\"1862\":1,\"1952\":1,\"2130\":2,\"2131\":2,\"2133\":2,\"2136\":1,\"2137\":1,\"2143\":1,\"2380\":1}}],[\"finetuned\",{\"1\":{\"745\":1}}],[\"finetune\",{\"1\":{\"184\":1,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"284\":1,\"286\":3,\"290\":2,\"675\":1,\"745\":3,\"746\":1,\"747\":2}}],[\"finetuning\",{\"0\":{\"49\":1},\"1\":{\"49\":1,\"290\":3,\"747\":1,\"846\":4}}],[\"fine\",{\"0\":{\"88\":1},\"1\":{\"5\":1,\"178\":1,\"184\":1,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"240\":2,\"243\":8,\"286\":3,\"699\":1,\"2176\":1,\"2249\":1}}],[\"fomulation\",{\"1\":{\"141\":1,\"650\":1,\"664\":1}}],[\"focuses\",{\"1\":{\"101\":1}}],[\"focus\",{\"1\":{\"45\":1,\"78\":1,\"145\":1,\"286\":2,\"1011\":2,\"2407\":1}}],[\"fout\",{\"1\":{\"821\":1}}],[\"found\",{\"1\":{\"66\":3,\"67\":1,\"136\":1,\"210\":1,\"213\":1,\"220\":1,\"240\":1,\"243\":2,\"260\":1,\"262\":7,\"263\":1,\"269\":2,\"275\":1,\"278\":2,\"530\":1,\"532\":1,\"534\":1,\"709\":1,\"760\":1,\"774\":1,\"780\":1,\"1334\":1,\"1785\":1,\"1786\":1,\"1817\":1,\"1818\":1,\"2065\":1,\"2191\":1,\"2309\":1,\"2355\":1,\"2508\":1}}],[\"foundation\",{\"1\":{\"6\":1,\"242\":1}}],[\"fourier\",{\"1\":{\"1177\":1,\"1211\":2,\"1784\":1}}],[\"four\",{\"1\":{\"44\":1,\"142\":1,\"266\":1,\"267\":1,\"268\":1,\"275\":1,\"276\":1,\"277\":1,\"1597\":1,\"2151\":1,\"2310\":1}}],[\"fooiterfactory\",{\"1\":{\"2249\":1}}],[\"foo=none\",{\"1\":{\"2485\":3,\"2493\":3,\"2505\":3}}],[\"foo=456\",{\"1\":{\"2493\":1}}],[\"foo=4\",{\"1\":{\"2334\":1,\"2485\":1}}],[\"foo=3\",{\"1\":{\"2334\":1}}],[\"foo=\",{\"1\":{\"168\":1,\"2500\":1,\"2505\":1}}],[\"foo=path\",{\"1\":{\"22\":1}}],[\"foo\",{\"1\":{\"24\":1,\"82\":2,\"85\":2,\"119\":1,\"195\":3,\"197\":2,\"2305\":2,\"2334\":1,\"2485\":5,\"2493\":5,\"2500\":2,\"2505\":5}}],[\"follwing\",{\"1\":{\"275\":1}}],[\"follow\",{\"1\":{\"24\":1,\"44\":1,\"106\":1,\"200\":2,\"223\":1,\"224\":2,\"225\":1,\"249\":1,\"259\":1,\"1389\":1,\"1390\":1,\"1401\":1,\"1402\":3,\"1408\":1,\"1409\":1,\"1466\":1,\"1467\":3,\"1526\":1,\"1549\":3,\"1553\":1,\"1594\":3,\"1595\":3,\"1598\":1,\"1600\":1,\"1625\":1}}],[\"followed\",{\"1\":{\"22\":1,\"756\":2,\"773\":2,\"866\":1,\"867\":1,\"980\":1,\"1301\":1,\"1372\":1,\"2039\":1,\"2136\":1}}],[\"follows\",{\"1\":{\"22\":1,\"41\":1,\"56\":1,\"94\":1,\"98\":1,\"117\":1,\"127\":1,\"174\":1,\"197\":1,\"220\":1,\"233\":1,\"240\":1,\"252\":1,\"267\":1,\"276\":1,\"286\":1,\"290\":2,\"1309\":1,\"1311\":1,\"1439\":1,\"1513\":1,\"1548\":1,\"1551\":1,\"1592\":1,\"1605\":1,\"1606\":1,\"2240\":1,\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1}}],[\"following\",{\"1\":{\"1\":2,\"22\":1,\"24\":1,\"25\":1,\"38\":3,\"39\":3,\"40\":1,\"41\":1,\"42\":1,\"49\":1,\"71\":2,\"74\":1,\"75\":1,\"76\":3,\"79\":1,\"81\":2,\"85\":1,\"87\":1,\"91\":1,\"102\":1,\"107\":1,\"119\":1,\"121\":1,\"125\":1,\"136\":1,\"138\":1,\"139\":2,\"140\":1,\"141\":2,\"142\":1,\"143\":1,\"144\":1,\"150\":1,\"159\":1,\"160\":1,\"162\":1,\"166\":1,\"167\":1,\"168\":2,\"175\":1,\"201\":3,\"205\":1,\"218\":1,\"219\":1,\"224\":2,\"226\":1,\"236\":1,\"242\":1,\"243\":3,\"247\":1,\"259\":1,\"267\":1,\"269\":2,\"272\":1,\"276\":2,\"278\":2,\"282\":1,\"286\":3,\"289\":3,\"290\":3,\"536\":1,\"755\":1,\"785\":1,\"829\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"974\":1,\"994\":1,\"1008\":1,\"1397\":1,\"1645\":1,\"1650\":1,\"1717\":1,\"1883\":1,\"1993\":1,\"2019\":1,\"2216\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2246\":1,\"2248\":1,\"2249\":3,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":3,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2355\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"fold\",{\"1\":{\"98\":5,\"377\":2,\"449\":2,\"2002\":1,\"2007\":2}}],[\"foldedbatchsampler\",{\"0\":{\"2002\":1},\"1\":{\"2002\":1}}],[\"folded\",{\"0\":{\"2002\":1},\"1\":{\"95\":1,\"97\":1,\"98\":2,\"449\":2,\"2002\":1,\"2007\":4}}],[\"folders\",{\"1\":{\"22\":2,\"24\":3,\"201\":1}}],[\"folder\",{\"1\":{\"18\":1,\"24\":1,\"39\":1,\"199\":1,\"200\":3,\"204\":1,\"205\":3}}],[\"forum\",{\"1\":{\"1589\":1,\"1833\":1}}],[\"forcingtrue\",{\"1\":{\"286\":4}}],[\"forcing\",{\"1\":{\"286\":4,\"406\":2,\"475\":2,\"484\":2,\"490\":2,\"1526\":3,\"1552\":3,\"1553\":3,\"1598\":3,\"1599\":3,\"1625\":3,\"1626\":4,\"1750\":2,\"1976\":1,\"1993\":3,\"2224\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":3,\"2411\":3,\"2412\":3,\"2423\":3,\"2431\":3,\"2432\":3,\"2447\":3}}],[\"forces\",{\"1\":{\"1770\":1}}],[\"force\",{\"0\":{\"2309\":1},\"1\":{\"224\":1,\"706\":1,\"1726\":1,\"2309\":1,\"2336\":1,\"2346\":1,\"2368\":1}}],[\"forced\",{\"1\":{\"205\":1,\"285\":1,\"706\":2,\"1997\":2}}],[\"formam\",{\"1\":{\"1009\":2}}],[\"formatted\",{\"1\":{\"768\":1,\"2151\":1,\"2310\":1}}],[\"formatting\",{\"0\":{\"70\":1},\"1\":{\"70\":1,\"162\":1}}],[\"format=none\",{\"1\":{\"1824\":1}}],[\"format=npy\",{\"1\":{\"80\":1}}],[\"format=\",{\"1\":{\"1009\":1,\"1826\":1,\"1829\":1}}],[\"format=text\",{\"1\":{\"80\":2}}],[\"format=kaldi\",{\"1\":{\"80\":1}}],[\"format=sound\",{\"1\":{\"80\":1}}],[\"format=flac\",{\"1\":{\"69\":1}}],[\"format>\",{\"1\":{\"79\":1}}],[\"formats\",{\"0\":{\"68\":1,\"71\":1},\"1\":{\"70\":1,\"71\":2,\"80\":1,\"235\":1,\"1678\":3,\"2144\":1,\"2474\":1}}],[\"format\",{\"0\":{\"68\":1,\"74\":1},\"1\":{\"38\":1,\"68\":14,\"69\":3,\"70\":2,\"71\":11,\"73\":3,\"76\":2,\"79\":2,\"80\":1,\"84\":1,\"85\":1,\"86\":1,\"106\":2,\"182\":1,\"190\":1,\"196\":7,\"200\":2,\"204\":1,\"205\":4,\"211\":1,\"213\":5,\"217\":1,\"222\":1,\"223\":2,\"225\":1,\"228\":1,\"234\":1,\"235\":2,\"240\":3,\"242\":6,\"243\":4,\"253\":1,\"254\":4,\"268\":10,\"276\":2,\"277\":10,\"284\":1,\"285\":1,\"286\":1,\"342\":2,\"349\":2,\"361\":2,\"402\":1,\"513\":2,\"514\":2,\"521\":1,\"524\":1,\"525\":1,\"1002\":2,\"1009\":8,\"1395\":1,\"1521\":1,\"1585\":1,\"1677\":1,\"1678\":2,\"1824\":1,\"1883\":2,\"2000\":1,\"2001\":1,\"2065\":1,\"2101\":3,\"2130\":3,\"2134\":2,\"2143\":2,\"2144\":4,\"2443\":1}}],[\"formula\",{\"1\":{\"756\":1,\"773\":1,\"866\":1,\"867\":1,\"1051\":1,\"1662\":1,\"1665\":1,\"2000\":2,\"2001\":1}}],[\"formulation\",{\"1\":{\"141\":6,\"200\":3,\"629\":2,\"635\":1,\"650\":1,\"664\":6}}],[\"form\",{\"1\":{\"254\":1,\"921\":1,\"927\":1,\"935\":1,\"1245\":1,\"1668\":2,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2021\":1,\"2039\":1}}],[\"former\",{\"1\":{\"47\":1,\"141\":1,\"262\":1,\"661\":3,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"forwardadaptor\",{\"0\":{\"2304\":1},\"1\":{\"2304\":1,\"2305\":1}}],[\"forwardsum\",{\"1\":{\"1589\":3}}],[\"forwardsumloss\",{\"0\":{\"1589\":1},\"1\":{\"1589\":1}}],[\"forwarding\",{\"1\":{\"102\":1}}],[\"forward\",{\"0\":{\"630\":1,\"638\":1,\"1809\":1,\"2304\":1},\"1\":{\"43\":2,\"78\":1,\"79\":1,\"102\":1,\"113\":2,\"132\":1,\"141\":4,\"142\":2,\"150\":2,\"263\":1,\"406\":2,\"449\":2,\"484\":2,\"490\":2,\"614\":3,\"615\":1,\"617\":2,\"618\":8,\"619\":1,\"620\":2,\"621\":1,\"622\":1,\"623\":1,\"624\":8,\"625\":2,\"626\":2,\"629\":2,\"630\":2,\"632\":1,\"633\":1,\"634\":4,\"635\":2,\"636\":4,\"637\":1,\"638\":3,\"640\":1,\"641\":4,\"642\":3,\"643\":4,\"644\":3,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":2,\"651\":3,\"652\":2,\"654\":2,\"661\":1,\"675\":7,\"676\":1,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"689\":1,\"690\":1,\"691\":1,\"692\":7,\"693\":1,\"694\":1,\"699\":3,\"700\":2,\"701\":1,\"702\":2,\"703\":6,\"706\":1,\"709\":3,\"710\":6,\"711\":6,\"712\":2,\"713\":1,\"714\":1,\"715\":2,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":3,\"725\":3,\"726\":2,\"727\":2,\"728\":3,\"729\":3,\"730\":2,\"733\":2,\"734\":2,\"735\":7,\"736\":1,\"737\":1,\"738\":1,\"739\":1,\"741\":1,\"742\":1,\"744\":3,\"745\":2,\"746\":3,\"747\":2,\"748\":2,\"749\":2,\"750\":2,\"752\":1,\"753\":1,\"754\":2,\"755\":2,\"756\":17,\"757\":1,\"758\":1,\"759\":2,\"760\":2,\"761\":2,\"762\":2,\"765\":2,\"768\":1,\"770\":2,\"771\":2,\"772\":2,\"773\":17,\"774\":3,\"775\":2,\"776\":1,\"777\":1,\"778\":1,\"779\":1,\"780\":3,\"781\":1,\"782\":1,\"783\":2,\"784\":4,\"785\":2,\"786\":2,\"787\":2,\"788\":1,\"789\":1,\"790\":4,\"791\":1,\"792\":1,\"794\":1,\"795\":1,\"796\":1,\"797\":2,\"798\":1,\"799\":1,\"800\":2,\"805\":1,\"806\":1,\"807\":1,\"808\":1,\"809\":1,\"810\":1,\"811\":1,\"812\":1,\"813\":1,\"814\":1,\"815\":1,\"816\":1,\"819\":2,\"820\":2,\"821\":2,\"822\":3,\"823\":3,\"824\":3,\"825\":1,\"826\":1,\"827\":2,\"828\":3,\"829\":5,\"830\":3,\"831\":1,\"832\":2,\"833\":1,\"834\":1,\"835\":1,\"836\":1,\"837\":1,\"838\":1,\"839\":1,\"840\":1,\"841\":2,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"846\":5,\"847\":4,\"849\":2,\"850\":4,\"851\":2,\"852\":1,\"853\":1,\"854\":1,\"855\":1,\"856\":1,\"857\":1,\"859\":2,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":1,\"866\":7,\"867\":7,\"878\":6,\"881\":3,\"882\":6,\"884\":3,\"947\":2,\"948\":2,\"949\":2,\"950\":1,\"951\":1,\"952\":1,\"953\":1,\"954\":1,\"955\":2,\"956\":1,\"957\":1,\"958\":2,\"959\":2,\"963\":1,\"964\":1,\"965\":1,\"966\":1,\"967\":1,\"968\":2,\"969\":1,\"970\":1,\"971\":2,\"972\":2,\"973\":2,\"974\":1,\"975\":2,\"976\":2,\"977\":2,\"978\":1,\"979\":2,\"980\":2,\"981\":2,\"982\":1,\"1029\":2,\"1030\":1,\"1031\":2,\"1032\":1,\"1033\":1,\"1034\":1,\"1035\":2,\"1036\":1,\"1037\":1,\"1038\":1,\"1039\":2,\"1040\":1,\"1041\":1,\"1042\":1,\"1043\":1,\"1044\":1,\"1045\":2,\"1046\":1,\"1047\":1,\"1048\":1,\"1049\":1,\"1051\":2,\"1052\":1,\"1053\":3,\"1054\":2,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1059\":1,\"1060\":1,\"1061\":2,\"1062\":2,\"1063\":2,\"1064\":2,\"1066\":1,\"1067\":1,\"1068\":1,\"1069\":1,\"1070\":2,\"1071\":2,\"1072\":2,\"1073\":2,\"1074\":2,\"1075\":2,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1083\":1,\"1084\":1,\"1085\":1,\"1086\":2,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":1,\"1094\":1,\"1095\":1,\"1096\":1,\"1097\":1,\"1098\":1,\"1099\":1,\"1100\":1,\"1101\":1,\"1102\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1106\":1,\"1107\":3,\"1108\":1,\"1109\":1,\"1110\":1,\"1111\":1,\"1112\":3,\"1113\":3,\"1114\":1,\"1115\":1,\"1117\":2,\"1118\":2,\"1119\":1,\"1120\":1,\"1121\":1,\"1122\":1,\"1123\":1,\"1124\":2,\"1125\":2,\"1126\":2,\"1127\":2,\"1130\":2,\"1131\":2,\"1132\":1,\"1133\":1,\"1134\":1,\"1135\":1,\"1136\":2,\"1137\":1,\"1138\":1,\"1139\":1,\"1140\":1,\"1141\":2,\"1142\":1,\"1143\":1,\"1145\":1,\"1146\":1,\"1147\":2,\"1148\":2,\"1149\":1,\"1150\":1,\"1151\":1,\"1152\":1,\"1153\":1,\"1154\":1,\"1155\":6,\"1156\":1,\"1157\":8,\"1158\":3,\"1159\":1,\"1160\":1,\"1162\":2,\"1163\":2,\"1164\":2,\"1165\":1,\"1166\":1,\"1167\":1,\"1168\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1175\":1,\"1176\":2,\"1177\":1,\"1178\":1,\"1179\":2,\"1180\":2,\"1181\":2,\"1182\":2,\"1183\":2,\"1184\":2,\"1185\":1,\"1186\":1,\"1187\":1,\"1188\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1193\":1,\"1194\":1,\"1195\":1,\"1196\":1,\"1197\":1,\"1198\":2,\"1199\":2,\"1200\":1,\"1201\":1,\"1202\":1,\"1203\":2,\"1204\":1,\"1205\":1,\"1206\":1,\"1207\":2,\"1208\":1,\"1209\":2,\"1210\":2,\"1211\":1,\"1212\":1,\"1213\":1,\"1214\":1,\"1215\":1,\"1216\":1,\"1217\":2,\"1219\":1,\"1220\":1,\"1222\":2,\"1223\":2,\"1226\":1,\"1227\":1,\"1228\":3,\"1230\":1,\"1231\":1,\"1232\":3,\"1233\":1,\"1234\":1,\"1235\":2,\"1236\":1,\"1237\":1,\"1238\":1,\"1239\":1,\"1240\":1,\"1241\":1,\"1242\":1,\"1243\":1,\"1246\":2,\"1247\":2,\"1248\":1,\"1249\":1,\"1250\":4,\"1251\":4,\"1252\":2,\"1253\":1,\"1254\":1,\"1255\":1,\"1256\":1,\"1257\":1,\"1258\":1,\"1259\":1,\"1260\":2,\"1261\":3,\"1262\":1,\"1263\":1,\"1264\":2,\"1265\":1,\"1266\":1,\"1267\":3,\"1268\":2,\"1269\":2,\"1270\":2,\"1271\":2,\"1272\":2,\"1273\":1,\"1274\":2,\"1275\":2,\"1277\":2,\"1278\":3,\"1279\":2,\"1280\":2,\"1281\":2,\"1282\":2,\"1283\":2,\"1284\":1,\"1285\":1,\"1286\":1,\"1287\":1,\"1288\":1,\"1289\":1,\"1290\":2,\"1333\":2,\"1334\":2,\"1381\":2,\"1383\":1,\"1384\":1,\"1385\":2,\"1387\":1,\"1388\":1,\"1389\":5,\"1390\":2,\"1391\":2,\"1392\":1,\"1393\":1,\"1395\":4,\"1397\":2,\"1398\":1,\"1399\":1,\"1400\":2,\"1401\":5,\"1402\":2,\"1403\":2,\"1404\":1,\"1405\":1,\"1406\":1,\"1407\":1,\"1408\":5,\"1409\":2,\"1410\":2,\"1411\":1,\"1412\":1,\"1413\":1,\"1414\":1,\"1415\":1,\"1416\":1,\"1417\":1,\"1418\":1,\"1419\":1,\"1420\":1,\"1421\":1,\"1422\":1,\"1423\":1,\"1424\":1,\"1425\":1,\"1426\":1,\"1427\":1,\"1428\":1,\"1429\":1,\"1430\":1,\"1431\":1,\"1433\":1,\"1434\":1,\"1435\":1,\"1436\":1,\"1437\":1,\"1438\":1,\"1439\":1,\"1440\":1,\"1441\":1,\"1442\":1,\"1443\":1,\"1444\":1,\"1445\":1,\"1446\":1,\"1447\":1,\"1448\":1,\"1449\":1,\"1450\":1,\"1451\":1,\"1452\":1,\"1453\":1,\"1454\":1,\"1455\":1,\"1456\":1,\"1457\":1,\"1458\":1,\"1459\":1,\"1460\":1,\"1461\":1,\"1462\":1,\"1463\":1,\"1464\":1,\"1465\":1,\"1466\":5,\"1467\":2,\"1468\":2,\"1469\":1,\"1470\":1,\"1508\":2,\"1509\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1513\":2,\"1514\":2,\"1515\":2,\"1516\":2,\"1517\":1,\"1518\":1,\"1519\":2,\"1520\":2,\"1521\":4,\"1522\":1,\"1523\":1,\"1524\":1,\"1525\":2,\"1526\":5,\"1527\":1,\"1528\":1,\"1529\":2,\"1530\":1,\"1531\":1,\"1533\":2,\"1534\":2,\"1535\":2,\"1536\":2,\"1537\":1,\"1538\":1,\"1539\":1,\"1540\":1,\"1541\":1,\"1542\":1,\"1543\":1,\"1544\":1,\"1545\":3,\"1546\":2,\"1548\":2,\"1549\":1,\"1550\":1,\"1551\":2,\"1552\":2,\"1553\":5,\"1554\":1,\"1555\":1,\"1556\":2,\"1576\":2,\"1577\":1,\"1581\":2,\"1582\":2,\"1583\":2,\"1584\":1,\"1585\":4,\"1586\":2,\"1587\":1,\"1588\":2,\"1589\":2,\"1590\":1,\"1591\":1,\"1592\":2,\"1593\":2,\"1594\":2,\"1595\":2,\"1596\":2,\"1597\":2,\"1598\":5,\"1599\":2,\"1600\":5,\"1601\":1,\"1602\":1,\"1603\":2,\"1604\":2,\"1605\":2,\"1606\":2,\"1607\":1,\"1609\":2,\"1610\":2,\"1611\":2,\"1612\":2,\"1613\":2,\"1614\":2,\"1615\":2,\"1616\":2,\"1617\":2,\"1618\":2,\"1619\":2,\"1620\":2,\"1621\":2,\"1622\":2,\"1623\":1,\"1624\":2,\"1625\":5,\"1626\":2,\"1627\":2,\"1628\":2,\"1638\":1,\"1639\":1,\"1640\":1,\"1641\":1,\"1652\":1,\"1653\":1,\"1656\":2,\"1657\":1,\"1658\":1,\"1660\":2,\"1661\":2,\"1662\":1,\"1663\":1,\"1664\":2,\"1665\":2,\"1667\":1,\"1668\":2,\"1669\":2,\"1670\":2,\"1671\":2,\"1702\":2,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":7,\"1709\":7,\"1710\":7,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1720\":1,\"1721\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1733\":2,\"1735\":7,\"1736\":2,\"1737\":2,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":2,\"1749\":3,\"1750\":5,\"1751\":6,\"1753\":4,\"1754\":2,\"1756\":2,\"1757\":2,\"1758\":2,\"1759\":4,\"1764\":4,\"1766\":2,\"1768\":7,\"1770\":2,\"1771\":2,\"1779\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1788\":3,\"1789\":2,\"1790\":2,\"1794\":3,\"1795\":3,\"1796\":1,\"1801\":2,\"1803\":2,\"1806\":1,\"1808\":1,\"1809\":4,\"1810\":2,\"1812\":2,\"1814\":2,\"1815\":4,\"1816\":2,\"1817\":1,\"1818\":1,\"1820\":1,\"1837\":1,\"1838\":1,\"1839\":2,\"1843\":2,\"1847\":4,\"1849\":2,\"1851\":2,\"1854\":2,\"1856\":2,\"1938\":1,\"1939\":1,\"1940\":1,\"1941\":1,\"1942\":1,\"1943\":1,\"1944\":1,\"1945\":1,\"1946\":1,\"1947\":1,\"1950\":1,\"1957\":1,\"1958\":1,\"1959\":1,\"1960\":1,\"1961\":1,\"1965\":2,\"1966\":1,\"1967\":1,\"1968\":1,\"1969\":1,\"1970\":1,\"1971\":1,\"1972\":1,\"1973\":1,\"1974\":2,\"1975\":1,\"1976\":2,\"1977\":1,\"1978\":1,\"1979\":1,\"1980\":1,\"1981\":1,\"1982\":1,\"1983\":1,\"1984\":2,\"1985\":1,\"1986\":1,\"1987\":2,\"1988\":1,\"1989\":1,\"1990\":2,\"1991\":2,\"1992\":5,\"1993\":5,\"1995\":3,\"1996\":1,\"1997\":1,\"2016\":1,\"2026\":1,\"2027\":1,\"2028\":1,\"2029\":1,\"2030\":1,\"2031\":1,\"2032\":1,\"2033\":1,\"2034\":1,\"2035\":1,\"2040\":2,\"2043\":3,\"2044\":1,\"2045\":2,\"2049\":2,\"2054\":2,\"2055\":3,\"2056\":3,\"2065\":1,\"2066\":3,\"2124\":1,\"2125\":1,\"2126\":2,\"2127\":1,\"2128\":2,\"2129\":2,\"2131\":1,\"2143\":1,\"2167\":2,\"2168\":1,\"2169\":1,\"2170\":1,\"2171\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2175\":1,\"2176\":2,\"2177\":1,\"2178\":1,\"2179\":1,\"2180\":1,\"2181\":1,\"2182\":1,\"2183\":2,\"2184\":2,\"2185\":1,\"2186\":1,\"2187\":2,\"2188\":1,\"2189\":1,\"2190\":2,\"2191\":3,\"2192\":1,\"2193\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2197\":1,\"2198\":1,\"2199\":1,\"2200\":1,\"2201\":1,\"2202\":2,\"2203\":1,\"2204\":1,\"2205\":1,\"2206\":1,\"2207\":2,\"2208\":2,\"2209\":1,\"2210\":1,\"2211\":1,\"2212\":1,\"2213\":2,\"2214\":2,\"2215\":2,\"2216\":1,\"2217\":1,\"2218\":2,\"2219\":2,\"2220\":2,\"2221\":1,\"2222\":1,\"2223\":2,\"2224\":3,\"2226\":4,\"2227\":2,\"2228\":1,\"2229\":1,\"2231\":2,\"2232\":2,\"2235\":4,\"2236\":4,\"2237\":2,\"2238\":2,\"2239\":2,\"2240\":2,\"2241\":3,\"2245\":5,\"2246\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2304\":2,\"2305\":1,\"2306\":1,\"2325\":3,\"2326\":1,\"2327\":12,\"2348\":1,\"2355\":2,\"2370\":2,\"2372\":1,\"2401\":1,\"2402\":1,\"2403\":1,\"2405\":1,\"2406\":1,\"2407\":1,\"2408\":1,\"2409\":1,\"2410\":1,\"2411\":3,\"2412\":2,\"2413\":3,\"2414\":1,\"2415\":1,\"2416\":1,\"2417\":1,\"2418\":1,\"2419\":1,\"2420\":2,\"2422\":1,\"2423\":2,\"2424\":3,\"2425\":2,\"2426\":2,\"2427\":2,\"2428\":5,\"2429\":2,\"2430\":2,\"2431\":5,\"2432\":2,\"2433\":2,\"2434\":1,\"2435\":1,\"2443\":1,\"2444\":1,\"2445\":1,\"2446\":1,\"2447\":2,\"2448\":3,\"2449\":1,\"2450\":1,\"2451\":1,\"2452\":1,\"2453\":1,\"2454\":1,\"2456\":1,\"2457\":1,\"2458\":1,\"2459\":1,\"2460\":1,\"2461\":1,\"2462\":1,\"2465\":1,\"2466\":1,\"2467\":1,\"2468\":1,\"2469\":2,\"2470\":2,\"2471\":2,\"2472\":2,\"2473\":2}}],[\"forked\",{\"1\":{\"138\":1}}],[\"fork\",{\"1\":{\"42\":1,\"139\":1}}],[\"forget\",{\"1\":{\"132\":1}}],[\"forge\",{\"1\":{\"31\":2}}],[\"for\",{\"0\":{\"58\":1,\"62\":1,\"64\":1,\"66\":1,\"76\":1,\"77\":1,\"82\":1,\"83\":1,\"86\":1,\"116\":1,\"119\":1,\"124\":1,\"197\":1,\"224\":1,\"1484\":1,\"1860\":1,\"1878\":1,\"1885\":1},\"1\":{\"0\":1,\"2\":1,\"3\":6,\"5\":1,\"11\":2,\"13\":1,\"22\":1,\"24\":2,\"26\":3,\"28\":1,\"31\":1,\"32\":2,\"37\":5,\"39\":2,\"41\":3,\"43\":12,\"44\":4,\"45\":5,\"46\":2,\"47\":1,\"48\":1,\"50\":3,\"51\":1,\"52\":7,\"54\":3,\"55\":1,\"58\":1,\"67\":2,\"69\":1,\"70\":1,\"71\":3,\"78\":3,\"79\":3,\"80\":1,\"81\":3,\"82\":6,\"84\":4,\"85\":2,\"91\":3,\"94\":3,\"96\":1,\"97\":1,\"98\":2,\"99\":2,\"100\":2,\"102\":4,\"104\":3,\"106\":2,\"107\":3,\"108\":1,\"109\":3,\"110\":5,\"119\":5,\"120\":2,\"123\":1,\"124\":2,\"125\":1,\"126\":2,\"127\":2,\"128\":3,\"129\":1,\"130\":4,\"133\":2,\"135\":3,\"136\":1,\"138\":3,\"139\":4,\"140\":1,\"141\":44,\"142\":14,\"144\":2,\"145\":4,\"146\":2,\"147\":1,\"150\":3,\"159\":7,\"160\":1,\"162\":9,\"163\":1,\"168\":3,\"173\":2,\"174\":2,\"175\":6,\"178\":1,\"179\":1,\"190\":1,\"193\":1,\"194\":1,\"195\":1,\"196\":2,\"197\":3,\"198\":2,\"199\":1,\"200\":20,\"201\":3,\"202\":1,\"203\":1,\"204\":1,\"205\":15,\"207\":1,\"208\":2,\"209\":1,\"211\":10,\"212\":1,\"213\":5,\"215\":1,\"216\":1,\"217\":9,\"218\":2,\"219\":1,\"220\":1,\"221\":1,\"222\":5,\"223\":14,\"224\":7,\"225\":14,\"226\":1,\"227\":1,\"228\":1,\"229\":1,\"230\":1,\"231\":2,\"232\":4,\"233\":3,\"235\":5,\"237\":1,\"238\":1,\"239\":1,\"240\":4,\"242\":17,\"243\":8,\"245\":3,\"246\":2,\"247\":6,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"252\":2,\"253\":1,\"254\":8,\"257\":2,\"258\":4,\"259\":3,\"260\":1,\"261\":4,\"262\":8,\"263\":1,\"264\":1,\"266\":4,\"267\":12,\"268\":5,\"269\":5,\"272\":1,\"273\":1,\"275\":4,\"276\":12,\"277\":5,\"278\":5,\"282\":1,\"283\":1,\"284\":3,\"285\":9,\"286\":9,\"287\":2,\"289\":1,\"290\":7,\"291\":1,\"292\":1,\"377\":2,\"513\":1,\"514\":1,\"515\":2,\"526\":1,\"536\":3,\"554\":1,\"600\":1,\"614\":1,\"615\":1,\"616\":7,\"617\":2,\"618\":3,\"620\":3,\"624\":3,\"625\":3,\"626\":1,\"627\":1,\"628\":2,\"629\":2,\"631\":1,\"632\":2,\"633\":9,\"634\":11,\"635\":1,\"636\":1,\"637\":2,\"639\":2,\"640\":2,\"641\":3,\"642\":3,\"643\":5,\"644\":4,\"648\":1,\"649\":2,\"650\":2,\"651\":2,\"652\":1,\"653\":2,\"654\":4,\"661\":3,\"663\":3,\"664\":9,\"665\":1,\"666\":1,\"669\":2,\"670\":2,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"686\":1,\"687\":1,\"690\":1,\"691\":4,\"692\":5,\"694\":1,\"696\":8,\"697\":6,\"699\":9,\"701\":1,\"703\":4,\"705\":3,\"706\":3,\"709\":3,\"710\":5,\"711\":5,\"714\":1,\"717\":3,\"719\":1,\"720\":2,\"721\":1,\"723\":1,\"724\":5,\"725\":5,\"726\":2,\"727\":1,\"728\":5,\"729\":4,\"733\":2,\"735\":1,\"738\":3,\"739\":1,\"740\":1,\"742\":1,\"743\":1,\"744\":4,\"746\":2,\"747\":1,\"748\":3,\"749\":1,\"753\":1,\"755\":1,\"756\":9,\"758\":1,\"759\":2,\"760\":6,\"763\":1,\"768\":3,\"771\":1,\"773\":9,\"774\":6,\"777\":1,\"779\":1,\"780\":2,\"782\":1,\"784\":4,\"785\":2,\"786\":6,\"787\":2,\"789\":1,\"790\":5,\"792\":1,\"793\":1,\"797\":5,\"799\":1,\"800\":1,\"804\":4,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"815\":2,\"816\":1,\"817\":2,\"819\":1,\"820\":8,\"821\":8,\"822\":1,\"826\":1,\"827\":2,\"828\":9,\"829\":9,\"830\":14,\"831\":2,\"833\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"846\":7,\"847\":8,\"849\":1,\"850\":5,\"853\":1,\"855\":1,\"857\":1,\"859\":3,\"861\":1,\"863\":1,\"865\":1,\"866\":12,\"867\":4,\"881\":1,\"882\":3,\"883\":3,\"884\":4,\"911\":1,\"918\":1,\"921\":5,\"922\":7,\"927\":5,\"936\":3,\"937\":3,\"938\":1,\"939\":1,\"944\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"951\":1,\"953\":1,\"954\":1,\"955\":1,\"957\":1,\"960\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"974\":3,\"976\":1,\"977\":1,\"979\":1,\"980\":1,\"991\":1,\"993\":2,\"996\":1,\"998\":1,\"1000\":5,\"1001\":1,\"1002\":2,\"1003\":1,\"1005\":1,\"1007\":1,\"1009\":1,\"1011\":2,\"1013\":1,\"1015\":1,\"1017\":1,\"1029\":3,\"1031\":1,\"1033\":1,\"1035\":2,\"1036\":1,\"1037\":2,\"1039\":1,\"1041\":1,\"1042\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1050\":2,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1061\":1,\"1062\":2,\"1064\":1,\"1066\":2,\"1067\":1,\"1069\":1,\"1070\":2,\"1071\":2,\"1073\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1107\":3,\"1109\":1,\"1111\":1,\"1112\":1,\"1113\":2,\"1115\":1,\"1116\":3,\"1117\":3,\"1118\":1,\"1121\":1,\"1123\":1,\"1124\":11,\"1125\":12,\"1126\":5,\"1127\":1,\"1130\":2,\"1131\":3,\"1132\":1,\"1135\":1,\"1136\":1,\"1138\":1,\"1140\":1,\"1141\":1,\"1143\":1,\"1146\":1,\"1147\":6,\"1150\":1,\"1152\":1,\"1154\":1,\"1155\":19,\"1156\":5,\"1157\":17,\"1158\":3,\"1160\":1,\"1161\":2,\"1163\":2,\"1164\":2,\"1166\":1,\"1167\":1,\"1169\":1,\"1170\":1,\"1171\":1,\"1172\":2,\"1173\":1,\"1174\":3,\"1175\":1,\"1177\":1,\"1178\":1,\"1185\":2,\"1186\":1,\"1188\":1,\"1189\":2,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1202\":1,\"1203\":1,\"1204\":2,\"1206\":1,\"1209\":3,\"1210\":2,\"1212\":1,\"1214\":1,\"1216\":1,\"1217\":1,\"1218\":2,\"1220\":1,\"1221\":2,\"1224\":1,\"1225\":1,\"1227\":1,\"1228\":2,\"1229\":3,\"1231\":1,\"1232\":1,\"1234\":1,\"1235\":3,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1244\":2,\"1245\":4,\"1246\":3,\"1247\":2,\"1248\":1,\"1249\":1,\"1250\":2,\"1251\":3,\"1252\":1,\"1253\":3,\"1254\":1,\"1255\":1,\"1256\":1,\"1257\":1,\"1258\":1,\"1259\":3,\"1260\":1,\"1261\":3,\"1262\":1,\"1263\":1,\"1266\":1,\"1267\":1,\"1268\":4,\"1269\":6,\"1270\":6,\"1271\":7,\"1274\":1,\"1275\":1,\"1276\":3,\"1277\":1,\"1278\":1,\"1279\":5,\"1280\":9,\"1281\":6,\"1282\":3,\"1283\":8,\"1285\":1,\"1287\":1,\"1289\":1,\"1293\":1,\"1301\":1,\"1306\":1,\"1309\":1,\"1311\":1,\"1314\":1,\"1317\":1,\"1318\":1,\"1321\":1,\"1322\":1,\"1325\":1,\"1327\":2,\"1328\":1,\"1330\":3,\"1331\":1,\"1333\":2,\"1334\":4,\"1354\":7,\"1356\":2,\"1368\":1,\"1371\":1,\"1372\":1,\"1377\":1,\"1384\":1,\"1388\":1,\"1389\":2,\"1391\":1,\"1392\":4,\"1393\":1,\"1395\":3,\"1396\":1,\"1397\":5,\"1399\":1,\"1400\":5,\"1401\":2,\"1402\":4,\"1403\":1,\"1405\":1,\"1407\":1,\"1408\":2,\"1409\":5,\"1410\":1,\"1412\":1,\"1414\":1,\"1415\":1,\"1416\":1,\"1418\":1,\"1419\":4,\"1421\":1,\"1422\":5,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1441\":7,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1450\":6,\"1451\":1,\"1452\":6,\"1453\":1,\"1454\":5,\"1455\":1,\"1456\":5,\"1457\":1,\"1458\":3,\"1459\":1,\"1460\":3,\"1461\":1,\"1463\":1,\"1465\":1,\"1466\":2,\"1467\":4,\"1468\":1,\"1469\":4,\"1470\":1,\"1484\":3,\"1493\":1,\"1494\":1,\"1501\":1,\"1502\":1,\"1505\":1,\"1506\":1,\"1510\":1,\"1512\":1,\"1513\":4,\"1514\":2,\"1515\":2,\"1518\":1,\"1519\":1,\"1521\":3,\"1523\":1,\"1524\":1,\"1526\":20,\"1528\":1,\"1529\":1,\"1531\":1,\"1534\":2,\"1535\":3,\"1536\":7,\"1538\":1,\"1539\":2,\"1540\":1,\"1542\":1,\"1544\":1,\"1545\":6,\"1546\":6,\"1548\":5,\"1549\":6,\"1550\":1,\"1551\":4,\"1552\":12,\"1553\":21,\"1555\":1,\"1577\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1585\":3,\"1586\":1,\"1587\":1,\"1588\":1,\"1592\":4,\"1593\":1,\"1594\":3,\"1595\":4,\"1596\":1,\"1597\":6,\"1598\":16,\"1599\":8,\"1600\":18,\"1603\":2,\"1604\":7,\"1605\":4,\"1606\":8,\"1607\":2,\"1608\":2,\"1609\":1,\"1610\":2,\"1611\":2,\"1612\":3,\"1613\":3,\"1614\":2,\"1615\":2,\"1616\":1,\"1618\":2,\"1619\":4,\"1622\":4,\"1623\":1,\"1624\":1,\"1625\":18,\"1626\":10,\"1627\":1,\"1628\":2,\"1631\":3,\"1638\":5,\"1639\":1,\"1644\":3,\"1645\":3,\"1647\":3,\"1650\":3,\"1651\":1,\"1653\":1,\"1654\":1,\"1655\":1,\"1658\":1,\"1663\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1677\":1,\"1678\":6,\"1679\":1,\"1680\":3,\"1681\":3,\"1683\":6,\"1684\":1,\"1685\":1,\"1692\":4,\"1697\":1,\"1698\":4,\"1702\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1713\":1,\"1715\":1,\"1716\":2,\"1717\":1,\"1719\":9,\"1720\":1,\"1721\":7,\"1722\":6,\"1723\":4,\"1724\":4,\"1725\":14,\"1726\":2,\"1727\":2,\"1729\":1,\"1730\":4,\"1731\":15,\"1732\":1,\"1735\":2,\"1736\":1,\"1737\":1,\"1749\":9,\"1750\":1,\"1751\":2,\"1754\":1,\"1756\":2,\"1757\":2,\"1759\":1,\"1760\":2,\"1762\":1,\"1764\":2,\"1766\":1,\"1768\":2,\"1771\":1,\"1775\":1,\"1779\":1,\"1784\":2,\"1787\":6,\"1788\":2,\"1789\":2,\"1790\":2,\"1794\":5,\"1795\":1,\"1798\":4,\"1799\":5,\"1800\":3,\"1804\":1,\"1805\":4,\"1806\":5,\"1807\":6,\"1808\":1,\"1810\":1,\"1811\":1,\"1815\":9,\"1821\":1,\"1822\":6,\"1833\":1,\"1839\":2,\"1842\":4,\"1843\":8,\"1846\":1,\"1847\":1,\"1848\":3,\"1851\":1,\"1854\":2,\"1860\":1,\"1862\":2,\"1863\":4,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1869\":1,\"1876\":1,\"1878\":1,\"1880\":1,\"1881\":2,\"1882\":2,\"1883\":4,\"1885\":1,\"1892\":1,\"1908\":2,\"1913\":1,\"1914\":3,\"1919\":1,\"1920\":3,\"1921\":1,\"1926\":1,\"1935\":1,\"1938\":1,\"1939\":1,\"1940\":1,\"1941\":1,\"1942\":1,\"1943\":1,\"1944\":6,\"1945\":3,\"1946\":4,\"1947\":6,\"1949\":1,\"1951\":1,\"1957\":1,\"1958\":1,\"1960\":1,\"1961\":1,\"1962\":1,\"1963\":6,\"1965\":4,\"1966\":3,\"1967\":1,\"1968\":1,\"1969\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1980\":1,\"1981\":1,\"1982\":1,\"1983\":1,\"1984\":1,\"1986\":1,\"1987\":1,\"1988\":1,\"1989\":1,\"1990\":1,\"1991\":1,\"1992\":6,\"1993\":2,\"1995\":3,\"1997\":2,\"2000\":6,\"2001\":3,\"2006\":1,\"2007\":5,\"2016\":1,\"2018\":3,\"2019\":1,\"2020\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2039\":1,\"2043\":1,\"2044\":14,\"2045\":2,\"2049\":4,\"2054\":1,\"2055\":1,\"2056\":1,\"2065\":5,\"2066\":1,\"2125\":1,\"2129\":1,\"2130\":44,\"2131\":7,\"2132\":4,\"2133\":10,\"2134\":10,\"2136\":17,\"2137\":8,\"2141\":1,\"2142\":4,\"2143\":8,\"2144\":1,\"2150\":3,\"2151\":1,\"2152\":1,\"2153\":1,\"2155\":1,\"2157\":1,\"2159\":1,\"2162\":1,\"2166\":1,\"2167\":2,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2176\":4,\"2178\":1,\"2180\":1,\"2182\":1,\"2184\":6,\"2186\":1,\"2188\":1,\"2189\":1,\"2191\":4,\"2193\":1,\"2195\":1,\"2197\":1,\"2198\":1,\"2199\":1,\"2201\":1,\"2203\":1,\"2204\":1,\"2206\":1,\"2208\":1,\"2209\":1,\"2210\":1,\"2212\":1,\"2215\":2,\"2217\":1,\"2219\":3,\"2220\":8,\"2223\":3,\"2224\":1,\"2226\":2,\"2228\":1,\"2229\":1,\"2231\":2,\"2235\":1,\"2236\":1,\"2237\":2,\"2239\":5,\"2240\":8,\"2241\":2,\"2245\":5,\"2249\":9,\"2253\":8,\"2262\":5,\"2280\":1,\"2286\":2,\"2287\":1,\"2298\":1,\"2304\":1,\"2306\":1,\"2308\":2,\"2310\":1,\"2311\":3,\"2325\":1,\"2326\":1,\"2327\":6,\"2334\":1,\"2342\":1,\"2344\":1,\"2345\":1,\"2346\":1,\"2347\":2,\"2348\":1,\"2351\":1,\"2352\":1,\"2353\":1,\"2354\":9,\"2355\":13,\"2357\":3,\"2359\":1,\"2360\":1,\"2363\":1,\"2364\":2,\"2365\":1,\"2367\":2,\"2368\":1,\"2369\":1,\"2371\":3,\"2372\":1,\"2377\":1,\"2384\":1,\"2385\":1,\"2402\":1,\"2404\":1,\"2406\":1,\"2408\":1,\"2410\":1,\"2411\":2,\"2412\":3,\"2413\":2,\"2415\":1,\"2416\":1,\"2417\":1,\"2418\":1,\"2419\":1,\"2423\":4,\"2424\":2,\"2426\":1,\"2428\":2,\"2431\":3,\"2432\":3,\"2435\":2,\"2443\":1,\"2444\":1,\"2446\":1,\"2447\":3,\"2448\":2,\"2450\":1,\"2452\":1,\"2454\":1,\"2456\":1,\"2457\":1,\"2458\":1,\"2459\":1,\"2460\":1,\"2461\":1,\"2466\":1,\"2468\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1,\"2477\":1}}],[\"axes=\",{\"1\":{\"1717\":1}}],[\"ax\",{\"1\":{\"1357\":1}}],[\"axis=1\",{\"1\":{\"1007\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1373\":1,\"1734\":1}}],[\"axis=0\",{\"1\":{\"993\":1,\"994\":2,\"1373\":1}}],[\"axis\",{\"0\":{\"1664\":1,\"1665\":1,\"1691\":2},\"1\":{\"82\":2,\"792\":1,\"819\":1,\"827\":1,\"852\":1,\"856\":1,\"994\":2,\"1028\":1,\"1145\":5,\"1168\":1,\"1297\":1,\"1325\":2,\"1444\":2,\"1448\":2,\"1582\":3,\"1617\":2,\"1624\":3,\"1664\":1,\"1665\":2,\"1691\":2,\"1717\":1}}],[\"aaa\",{\"1\":{\"2505\":2}}],[\"aaaa\",{\"1\":{\"2334\":1}}],[\"aamsoftmax\",{\"0\":{\"2167\":2,\"2176\":1},\"1\":{\"2167\":3,\"2176\":3}}],[\"aa\",{\"0\":{\"2082\":1},\"1\":{\"999\":1,\"1006\":1,\"1010\":7,\"1014\":1,\"1018\":1}}],[\"aand\",{\"1\":{\"69\":1}}],[\"a^i\",{\"1\":{\"926\":1}}],[\"a^l\",{\"1\":{\"926\":1}}],[\"a^dt\",{\"1\":{\"824\":1}}],[\"aes\",{\"1\":{\"616\":1}}],[\"agent\",{\"1\":{\"1709\":1}}],[\"aggregation\",{\"0\":{\"1660\":1},\"1\":{\"1660\":1,\"2183\":1,\"2187\":1,\"2191\":1}}],[\"aggregator\",{\"1\":{\"974\":1,\"976\":1}}],[\"aggregates\",{\"1\":{\"2183\":1,\"2208\":1}}],[\"aggregated\",{\"1\":{\"2149\":1}}],[\"aggregate\",{\"0\":{\"293\":1,\"2374\":1},\"1\":{\"293\":2,\"2184\":2,\"2232\":2,\"2374\":1}}],[\"against\",{\"1\":{\"200\":1,\"205\":1,\"242\":1,\"265\":1,\"269\":1,\"274\":1,\"278\":1}}],[\"again\",{\"1\":{\"81\":1,\"153\":1,\"1269\":1,\"1270\":1,\"1271\":1}}],[\"aː\",{\"1\":{\"287\":2}}],[\"a3\",{\"1\":{\"287\":1,\"994\":1}}],[\"a=3\",{\"1\":{\"2479\":1}}],[\"a=a\",{\"1\":{\"2334\":1}}],[\"a=\",{\"1\":{\"276\":1}}],[\"ai\",{\"1\":{\"269\":1,\"278\":1}}],[\"aishell\",{\"1\":{\"243\":8}}],[\"aim\",{\"1\":{\"79\":1}}],[\"ah0\",{\"1\":{\"271\":2,\"280\":2,\"287\":4}}],[\"ahead=16\",{\"1\":{\"1720\":1}}],[\"aheads=4\",{\"1\":{\"1716\":1}}],[\"aheads\",{\"1\":{\"796\":1,\"869\":1,\"1107\":2,\"1278\":2,\"1526\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1713\":3,\"1714\":3,\"1715\":4,\"1716\":5,\"1895\":2,\"1993\":1,\"1994\":1,\"2239\":1,\"2240\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2432\":2,\"2447\":1}}],[\"ahead\",{\"1\":{\"130\":2,\"710\":3,\"711\":3,\"1721\":5}}],[\"ahmed\",{\"1\":{\"8\":1}}],[\"a2\",{\"0\":{\"2022\":1,\"2023\":1,\"2062\":1},\"1\":{\"76\":1,\"994\":1,\"1008\":2,\"1027\":2}}],[\"a1t2\",{\"0\":{\"2110\":1}}],[\"a1t1\",{\"0\":{\"2109\":1}}],[\"a1\",{\"0\":{\"2022\":1,\"2023\":1,\"2024\":1,\"2025\":1},\"1\":{\"76\":1,\"994\":1,\"1027\":2}}],[\"afms\",{\"0\":{\"2168\":1},\"1\":{\"2168\":1}}],[\"affect\",{\"1\":{\"821\":2,\"846\":1}}],[\"affine=true\",{\"1\":{\"722\":1,\"1078\":1}}],[\"affine\",{\"0\":{\"689\":1},\"1\":{\"689\":1,\"809\":1,\"854\":1,\"1065\":1,\"1586\":1,\"1612\":2,\"1613\":1}}],[\"af\",{\"1\":{\"74\":3}}],[\"after=false\",{\"1\":{\"1735\":1,\"1751\":1,\"1759\":1}}],[\"afterwards\",{\"1\":{\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"after\",{\"1\":{\"3\":1,\"26\":1,\"43\":4,\"62\":1,\"91\":2,\"133\":1,\"139\":1,\"141\":4,\"148\":1,\"168\":1,\"175\":1,\"200\":1,\"205\":1,\"218\":2,\"243\":1,\"252\":1,\"267\":2,\"276\":2,\"286\":3,\"620\":2,\"626\":2,\"661\":4,\"692\":1,\"700\":1,\"709\":3,\"710\":3,\"711\":3,\"713\":1,\"715\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"766\":1,\"767\":1,\"774\":3,\"775\":1,\"780\":6,\"781\":1,\"783\":1,\"818\":1,\"822\":1,\"846\":2,\"848\":1,\"849\":3,\"850\":1,\"922\":1,\"1053\":1,\"1107\":3,\"1113\":1,\"1124\":1,\"1125\":1,\"1133\":1,\"1139\":1,\"1141\":1,\"1145\":1,\"1155\":1,\"1157\":1,\"1176\":1,\"1208\":2,\"1278\":3,\"1279\":1,\"1280\":3,\"1281\":1,\"1283\":3,\"1334\":1,\"1387\":1,\"1392\":1,\"1450\":1,\"1452\":1,\"1502\":1,\"1519\":1,\"1526\":2,\"1598\":2,\"1599\":6,\"1600\":2,\"1609\":1,\"1680\":1,\"1692\":1,\"1698\":1,\"1735\":1,\"1736\":2,\"1750\":1,\"1751\":1,\"1759\":1,\"1764\":3,\"1839\":3,\"1957\":1,\"1960\":1,\"1961\":1,\"1991\":3,\"1992\":2,\"1993\":1,\"1995\":2,\"2015\":1,\"2126\":1,\"2129\":3,\"2130\":2,\"2133\":5,\"2136\":2,\"2137\":1,\"2183\":1,\"2191\":1,\"2220\":2,\"2223\":1,\"2226\":3,\"2237\":3,\"2239\":6,\"2240\":6,\"2241\":3,\"2355\":2,\"2411\":6,\"2412\":6,\"2413\":3,\"2423\":5,\"2424\":3,\"2432\":6,\"2447\":6,\"2448\":3,\"2458\":1}}],[\"awin\",{\"1\":{\"796\":1,\"869\":1,\"1895\":2}}],[\"awk\",{\"1\":{\"286\":1}}],[\"awkward\",{\"1\":{\"246\":1}}],[\"aws\",{\"0\":{\"152\":1},\"1\":{\"66\":1}}],[\"aware\",{\"1\":{\"36\":1,\"82\":1,\"262\":1,\"1131\":1,\"1172\":1,\"1185\":1,\"1706\":2,\"1710\":1,\"1711\":2,\"1712\":2,\"1715\":1,\"1716\":1}}],[\"amfs\",{\"1\":{\"2168\":1}}],[\"amuse\",{\"1\":{\"1957\":1}}],[\"amb\",{\"1\":{\"1678\":1}}],[\"amr\",{\"1\":{\"1678\":1}}],[\"ameboshi\",{\"1\":{\"269\":1,\"278\":1}}],[\"amplitudes\",{\"1\":{\"1565\":1,\"1570\":1}}],[\"amplitude\",{\"1\":{\"1545\":1,\"1607\":1,\"1978\":2,\"1980\":1,\"1982\":1,\"2414\":2,\"2416\":1,\"2418\":1}}],[\"amp=0\",{\"1\":{\"1545\":1}}],[\"amp\",{\"0\":{\"1557\":1},\"1\":{\"87\":1,\"103\":1,\"243\":1,\"449\":2,\"748\":2,\"1545\":2,\"1557\":2,\"2348\":1,\"2370\":2,\"2372\":1}}],[\"amsgrad\",{\"1\":{\"84\":1}}],[\"amongst\",{\"1\":{\"262\":1}}],[\"among\",{\"1\":{\"78\":1,\"736\":1,\"737\":1,\"954\":1,\"974\":1,\"1155\":1,\"1157\":1,\"1158\":1,\"1395\":1,\"1521\":1,\"1585\":1,\"1640\":1,\"1641\":1,\"1938\":1,\"1959\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2221\":1,\"2228\":1,\"2229\":1,\"2325\":1,\"2327\":1,\"2408\":1,\"2446\":1}}],[\"amounts\",{\"1\":{\"1919\":6}}],[\"amount\",{\"1\":{\"71\":1,\"927\":1,\"1008\":1,\"1524\":1,\"1679\":5}}],[\"am\",{\"0\":{\"947\":1,\"1378\":1,\"1379\":1},\"1\":{\"46\":1,\"139\":2,\"947\":1}}],[\"aug\",{\"1\":{\"2336\":3,\"2337\":3,\"2346\":3,\"2356\":3,\"2362\":3,\"2368\":3}}],[\"augmentation\",{\"0\":{\"1655\":1,\"1667\":1,\"1672\":1,\"1673\":1,\"1674\":1,\"1676\":1,\"1677\":1,\"1679\":1,\"1680\":1,\"1686\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1692\":1,\"1693\":1,\"1694\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1701\":1},\"1\":{\"254\":1,\"554\":1,\"686\":1,\"699\":2,\"746\":1,\"768\":1,\"833\":1,\"1655\":3,\"1667\":3,\"1672\":1,\"1673\":1,\"1674\":1,\"1676\":1,\"1677\":1,\"1679\":1,\"1680\":1,\"1686\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1692\":1,\"1693\":1,\"1694\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1701\":1,\"1919\":1}}],[\"augment\",{\"0\":{\"1765\":1,\"1831\":1,\"1840\":1,\"1841\":1,\"1884\":1,\"1922\":2,\"1929\":1,\"1930\":1},\"1\":{\"200\":1,\"205\":1,\"242\":1,\"699\":2}}],[\"augmented\",{\"1\":{\"44\":1}}],[\"auprc\",{\"1\":{\"960\":1}}],[\"audiocoding\",{\"1\":{\"220\":2}}],[\"audioset\",{\"1\":{\"212\":2}}],[\"audios\",{\"1\":{\"203\":1,\"993\":1,\"994\":2}}],[\"audio\",{\"0\":{\"68\":1,\"70\":1,\"71\":1,\"74\":1,\"639\":1,\"2088\":1,\"2103\":1,\"2133\":1,\"2136\":1,\"2138\":1,\"2139\":1,\"2150\":1,\"2156\":1,\"2157\":2,\"2159\":1,\"2160\":1},\"1\":{\"68\":5,\"69\":7,\"70\":7,\"71\":9,\"73\":1,\"76\":6,\"135\":1,\"136\":1,\"148\":1,\"196\":2,\"211\":1,\"220\":1,\"223\":1,\"235\":2,\"242\":1,\"243\":1,\"246\":1,\"247\":1,\"249\":1,\"268\":2,\"277\":2,\"286\":1,\"295\":1,\"404\":1,\"415\":1,\"527\":1,\"537\":1,\"639\":3,\"674\":10,\"675\":4,\"691\":1,\"699\":4,\"702\":2,\"746\":4,\"768\":2,\"792\":2,\"831\":3,\"846\":2,\"865\":1,\"1009\":6,\"1031\":3,\"1035\":5,\"1112\":3,\"1113\":5,\"1128\":2,\"1155\":2,\"1158\":2,\"1162\":1,\"1250\":3,\"1251\":5,\"1269\":3,\"1270\":3,\"1271\":3,\"1334\":3,\"1389\":5,\"1391\":3,\"1395\":12,\"1401\":5,\"1403\":4,\"1406\":1,\"1408\":5,\"1410\":3,\"1413\":1,\"1420\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1466\":5,\"1468\":3,\"1524\":2,\"1533\":1,\"1539\":1,\"1552\":1,\"1558\":1,\"1672\":1,\"1673\":1,\"1676\":1,\"1678\":1,\"1679\":1,\"1680\":1,\"1686\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1692\":1,\"1694\":1,\"1697\":1,\"1698\":1,\"1873\":2,\"1877\":2,\"1965\":3,\"2000\":1,\"2001\":1,\"2040\":3,\"2043\":3,\"2044\":13,\"2045\":3,\"2054\":4,\"2055\":3,\"2056\":3,\"2061\":2,\"2065\":8,\"2066\":3,\"2088\":1,\"2101\":4,\"2130\":4,\"2133\":15,\"2136\":19,\"2138\":1,\"2139\":5,\"2143\":4,\"2150\":1,\"2156\":1,\"2157\":2,\"2159\":1,\"2160\":2,\"2262\":1,\"2287\":7,\"2336\":1,\"2350\":1,\"2353\":1,\"2357\":1,\"2435\":2,\"2438\":1,\"2440\":1}}],[\"aux=28\",{\"1\":{\"1854\":1}}],[\"auxlm\",{\"1\":{\"144\":2}}],[\"auxctc\",{\"1\":{\"144\":2}}],[\"auxirialy\",{\"1\":{\"1620\":1,\"1621\":2}}],[\"auxially\",{\"1\":{\"81\":2}}],[\"auxiliary\",{\"1\":{\"44\":7,\"144\":7,\"260\":1,\"262\":2,\"625\":11,\"1268\":2,\"1610\":4,\"1619\":2,\"1620\":1,\"1621\":1,\"1779\":1,\"1854\":2,\"1936\":3,\"1967\":1,\"2472\":1}}],[\"aux\",{\"0\":{\"1936\":1,\"1967\":2,\"1984\":1},\"1\":{\"44\":6,\"1040\":2,\"1155\":2,\"1158\":2,\"1268\":4,\"1526\":1,\"1552\":2,\"1553\":1,\"1582\":4,\"1598\":2,\"1599\":2,\"1600\":1,\"1610\":4,\"1619\":2,\"1620\":5,\"1621\":3,\"1625\":2,\"1626\":4,\"1628\":3,\"1779\":2,\"1854\":4,\"1936\":3,\"1967\":2,\"1975\":1,\"1984\":1,\"2336\":1,\"2337\":1}}],[\"auth\",{\"1\":{\"356\":2,\"1129\":1}}],[\"authors\",{\"1\":{\"1225\":1}}],[\"author\",{\"1\":{\"156\":1,\"202\":1}}],[\"author=\",{\"1\":{\"5\":1,\"6\":3,\"7\":1,\"8\":1,\"9\":2,\"10\":2,\"11\":2,\"12\":1,\"13\":1,\"14\":1,\"15\":1,\"16\":1,\"202\":1,\"207\":2,\"244\":1,\"256\":1}}],[\"autoregression\",{\"1\":{\"1751\":1}}],[\"autoregressive\",{\"1\":{\"262\":3,\"286\":1,\"290\":1,\"776\":1,\"784\":1,\"795\":1,\"1794\":2,\"1806\":3,\"1807\":1}}],[\"autoencoder\",{\"1\":{\"978\":1,\"982\":1,\"1273\":1,\"1274\":1,\"1546\":1,\"1553\":1,\"1611\":1,\"1612\":1,\"1616\":1,\"1622\":1,\"1625\":1,\"1626\":1}}],[\"autograd\",{\"1\":{\"756\":3,\"773\":3}}],[\"automated\",{\"1\":{\"245\":1}}],[\"automatic\",{\"0\":{\"103\":1,\"198\":1,\"203\":1,\"292\":1},\"1\":{\"247\":1,\"268\":1,\"269\":1,\"277\":1,\"278\":1,\"286\":1,\"748\":1,\"756\":1,\"773\":1,\"833\":1,\"866\":1,\"867\":1,\"2134\":1,\"2191\":1,\"2355\":5}}],[\"automatically\",{\"1\":{\"0\":1,\"19\":1,\"34\":1,\"39\":1,\"101\":1,\"133\":1,\"137\":1,\"243\":1,\"285\":1,\"1720\":1,\"1721\":1,\"1725\":1,\"1806\":1,\"1862\":1,\"2145\":1,\"2245\":1,\"2355\":5,\"2474\":1}}],[\"auto\",{\"1\":{\"47\":1,\"220\":1,\"262\":1,\"269\":1,\"278\":1,\"295\":1,\"415\":1,\"1750\":1,\"1811\":1,\"2225\":1,\"2411\":2}}],[\"aka\",{\"1\":{\"42\":1,\"138\":1}}],[\"api\",{\"1\":{\"175\":2,\"220\":2,\"263\":1,\"527\":5,\"978\":1,\"982\":1,\"1126\":1,\"1217\":1,\"1273\":1}}],[\"apt\",{\"1\":{\"159\":3,\"161\":1}}],[\"appears\",{\"1\":{\"269\":1,\"278\":1}}],[\"appendix\",{\"1\":{\"1786\":1,\"1818\":1}}],[\"append\",{\"1\":{\"242\":1,\"1268\":1,\"1725\":3,\"1778\":1,\"1857\":1,\"2039\":1,\"2478\":1}}],[\"appended\",{\"1\":{\"82\":1}}],[\"appreciably\",{\"1\":{\"262\":1}}],[\"approximation\",{\"1\":{\"1327\":2,\"1330\":2}}],[\"approximates\",{\"1\":{\"2001\":1}}],[\"approximately\",{\"1\":{\"1441\":1}}],[\"approximate\",{\"1\":{\"1269\":1,\"1270\":1,\"2000\":1,\"2001\":1}}],[\"approx\",{\"1\":{\"1182\":1,\"1183\":1,\"1269\":2,\"1270\":2,\"1327\":3,\"1330\":3,\"2220\":1}}],[\"approaches\",{\"1\":{\"262\":2,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1}}],[\"approach\",{\"1\":{\"262\":3,\"1631\":1}}],[\"appropriate\",{\"1\":{\"160\":1,\"162\":1,\"374\":1,\"1441\":1}}],[\"appropriately\",{\"1\":{\"41\":1,\"175\":1,\"821\":1}}],[\"app\",{\"1\":{\"246\":1}}],[\"applications\",{\"1\":{\"1031\":1,\"1112\":1,\"1250\":1,\"2404\":1}}],[\"application\",{\"1\":{\"275\":1,\"276\":1,\"1035\":1,\"1113\":1,\"1251\":1,\"1327\":1,\"1330\":1}}],[\"applies\",{\"1\":{\"235\":1,\"290\":1,\"927\":1,\"1168\":1,\"1645\":1,\"1661\":1,\"1667\":1,\"1812\":1,\"2143\":1}}],[\"applied\",{\"1\":{\"98\":1,\"106\":1,\"262\":1,\"629\":1,\"692\":2,\"699\":2,\"706\":3,\"709\":2,\"710\":2,\"711\":2,\"733\":1,\"734\":1,\"774\":2,\"780\":2,\"786\":1,\"800\":1,\"820\":1,\"828\":1,\"831\":1,\"846\":5,\"849\":2,\"921\":1,\"935\":1,\"1029\":1,\"1107\":2,\"1137\":1,\"1139\":1,\"1141\":1,\"1153\":1,\"1185\":1,\"1235\":1,\"1278\":2,\"1279\":1,\"1281\":1,\"1282\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1513\":1,\"1525\":1,\"1548\":1,\"1551\":1,\"1558\":1,\"1592\":1,\"1596\":2,\"1597\":2,\"1599\":1,\"1605\":1,\"1609\":1,\"1610\":2,\"1619\":1,\"1628\":2,\"1655\":4,\"1668\":2,\"1680\":1,\"1692\":1,\"1698\":1,\"1735\":2,\"1751\":2,\"1759\":2,\"1806\":1,\"1992\":2,\"1995\":2,\"2001\":1,\"2129\":2,\"2304\":1,\"2432\":6}}],[\"applying\",{\"1\":{\"820\":1,\"828\":1,\"1051\":1,\"1645\":1,\"1655\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1715\":1,\"1768\":1,\"2353\":3,\"2364\":3}}],[\"apply\",{\"0\":{\"1291\":1,\"1476\":1,\"2067\":1},\"1\":{\"139\":1,\"173\":1,\"243\":5,\"267\":1,\"276\":1,\"286\":1,\"521\":1,\"691\":1,\"699\":1,\"720\":1,\"747\":1,\"768\":1,\"786\":1,\"787\":1,\"800\":1,\"831\":1,\"832\":1,\"833\":3,\"921\":1,\"935\":1,\"939\":2,\"1062\":1,\"1086\":2,\"1118\":2,\"1124\":1,\"1125\":1,\"1126\":1,\"1133\":2,\"1176\":1,\"1207\":2,\"1252\":1,\"1291\":1,\"1476\":1,\"1513\":2,\"1535\":1,\"1546\":1,\"1548\":2,\"1551\":2,\"1552\":4,\"1558\":2,\"1592\":2,\"1596\":4,\"1597\":4,\"1599\":4,\"1605\":2,\"1606\":2,\"1609\":2,\"1610\":2,\"1611\":1,\"1618\":4,\"1619\":2,\"1622\":1,\"1626\":4,\"1627\":1,\"1628\":2,\"1655\":3,\"1656\":3,\"1674\":1,\"1676\":1,\"1677\":1,\"1679\":1,\"1683\":1,\"1691\":1,\"1700\":1,\"1750\":1,\"1764\":1,\"1783\":1,\"1784\":2,\"1794\":1,\"1833\":1,\"1839\":2,\"1845\":1,\"1919\":1,\"1960\":1,\"1961\":1,\"1993\":1,\"2136\":1,\"2138\":1,\"2224\":2,\"2226\":1,\"2235\":1,\"2236\":1,\"2237\":2,\"2239\":4,\"2240\":4,\"2241\":1,\"2245\":3,\"2336\":2,\"2337\":2,\"2346\":2,\"2350\":2,\"2353\":4,\"2356\":2,\"2360\":4,\"2361\":4,\"2362\":2,\"2364\":4,\"2368\":2,\"2411\":4,\"2412\":4,\"2413\":1,\"2423\":4,\"2424\":1,\"2431\":2,\"2432\":7,\"2447\":4,\"2448\":1}}],[\"apsipa\",{\"1\":{\"16\":1}}],[\"abbreviates\",{\"1\":{\"2151\":1,\"2310\":1}}],[\"abbreviation\",{\"1\":{\"290\":1}}],[\"abel\",{\"1\":{\"1521\":1,\"2228\":1,\"2229\":1}}],[\"abc\",{\"1\":{\"614\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"950\":1,\"952\":1,\"956\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1116\":1,\"1174\":1,\"1229\":1,\"1245\":1,\"1276\":1,\"1381\":1,\"1415\":1,\"1508\":1,\"1576\":1,\"1642\":1,\"1652\":1,\"1659\":1,\"1800\":1,\"1938\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1998\":1,\"2012\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2130\":1,\"2131\":1,\"2172\":1,\"2174\":1,\"2215\":1,\"2222\":1,\"2249\":1,\"2274\":1,\"2324\":1,\"2325\":1,\"2327\":1,\"2328\":1,\"2401\":1,\"2403\":1,\"2443\":1,\"2445\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2500\":4,\"2502\":2}}],[\"ability\",{\"1\":{\"262\":1}}],[\"abdelrahman\",{\"1\":{\"207\":1}}],[\"abdelaziz\",{\"1\":{\"8\":1}}],[\"able\",{\"1\":{\"107\":1}}],[\"absuasrloss\",{\"0\":{\"2456\":1},\"1\":{\"2456\":1,\"2462\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1}}],[\"absgenerator\",{\"0\":{\"2453\":1},\"1\":{\"2453\":1,\"2460\":1,\"2462\":1}}],[\"absgantts\",{\"0\":{\"1576\":1},\"1\":{\"1576\":1,\"1585\":1,\"1598\":1,\"1600\":1,\"1625\":1}}],[\"absgansvs\",{\"0\":{\"1508\":1},\"1\":{\"1508\":1,\"1521\":1,\"1526\":1,\"1553\":1}}],[\"absganespnetmodel\",{\"0\":{\"2327\":1},\"1\":{\"1395\":1,\"1521\":1,\"1585\":1,\"2327\":2}}],[\"absgancodec\",{\"0\":{\"1381\":1},\"1\":{\"1381\":1,\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1}}],[\"absjobtemplate\",{\"0\":{\"2131\":1},\"1\":{\"2131\":1,\"2142\":1}}],[\"absio\",{\"0\":{\"2130\":1},\"1\":{\"2130\":2,\"2133\":1,\"2136\":1,\"2137\":1}}],[\"absiterfactory\",{\"0\":{\"1642\":1},\"1\":{\"1642\":1,\"1643\":1,\"1645\":1,\"1646\":1,\"1648\":2,\"1650\":1,\"2249\":6,\"2253\":1,\"2338\":2,\"2369\":3}}],[\"absvad\",{\"0\":{\"2034\":1},\"1\":{\"2034\":1,\"2065\":1}}],[\"absvalepochstepscheduler\",{\"0\":{\"2013\":1},\"1\":{\"2013\":1,\"2020\":1}}],[\"absbatchstepscheduler\",{\"0\":{\"2010\":1},\"1\":{\"2010\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1}}],[\"absllm\",{\"0\":{\"2030\":1},\"1\":{\"2030\":1,\"2049\":1}}],[\"abslm\",{\"0\":{\"1938\":1},\"1\":{\"1938\":3,\"1940\":1,\"1942\":1,\"1944\":1,\"1945\":1,\"1947\":1}}],[\"absloss\",{\"0\":{\"2170\":1},\"1\":{\"1702\":1,\"2167\":1,\"2170\":1,\"2176\":1,\"2184\":1,\"2207\":1}}],[\"abslosswrapper\",{\"0\":{\"1042\":1},\"1\":{\"794\":1,\"1042\":1,\"1132\":1,\"1157\":1,\"1158\":1,\"1167\":1,\"1204\":1,\"1209\":1,\"1228\":1}}],[\"absfeatsextractdiscrete\",{\"0\":{\"2443\":1},\"1\":{\"2443\":1,\"2446\":1,\"2449\":1}}],[\"absfeatsextract\",{\"0\":{\"2401\":1},\"1\":{\"1521\":8,\"1585\":3,\"1972\":1,\"2228\":8,\"2229\":8,\"2232\":1,\"2238\":1,\"2401\":1,\"2404\":1,\"2408\":3,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2434\":1,\"2446\":2}}],[\"absfrontend\",{\"0\":{\"680\":1},\"1\":{\"625\":1,\"680\":1,\"691\":1,\"702\":1,\"720\":1,\"736\":1,\"737\":1,\"738\":1,\"752\":1,\"759\":1,\"777\":1,\"778\":1,\"815\":1,\"831\":1,\"864\":1,\"954\":1,\"958\":1,\"974\":1,\"1521\":1,\"1522\":1,\"1539\":1,\"1640\":1,\"1641\":1,\"1702\":1,\"1957\":1,\"1959\":1,\"1960\":1,\"1961\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2184\":1,\"2216\":1,\"2221\":1,\"2462\":1}}],[\"absolute\",{\"1\":{\"1002\":1,\"1170\":1,\"1385\":1,\"1725\":2,\"1806\":1}}],[\"absorb\",{\"1\":{\"197\":1,\"858\":1}}],[\"abssegmenter\",{\"0\":{\"2455\":1},\"1\":{\"2455\":1,\"2462\":1,\"2463\":1,\"2464\":1}}],[\"absseprator\",{\"1\":{\"1053\":1}}],[\"absseparator\",{\"0\":{\"1044\":1},\"1\":{\"980\":1,\"1044\":1,\"1053\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1125\":1,\"1130\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1157\":1,\"1162\":2,\"1217\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":1,\"1280\":1,\"1283\":1,\"1334\":1}}],[\"abssslloss\",{\"0\":{\"2215\":1},\"1\":{\"2215\":1,\"2216\":1,\"2219\":1}}],[\"absscheduler\",{\"0\":{\"2012\":1},\"1\":{\"2010\":1,\"2011\":1,\"2012\":1,\"2347\":1,\"2369\":3,\"2371\":1}}],[\"abssynthesizer\",{\"0\":{\"1971\":1},\"1\":{\"1971\":1,\"1975\":1,\"1992\":1,\"1993\":1,\"1994\":1,\"1995\":1}}],[\"abss2stloss\",{\"0\":{\"1969\":1},\"1\":{\"1969\":1,\"1975\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1}}],[\"abss2stauxattention\",{\"0\":{\"1967\":1},\"1\":{\"1967\":1,\"1975\":1,\"1984\":1}}],[\"abssampler\",{\"0\":{\"1998\":1},\"1\":{\"1643\":1,\"1645\":1,\"1646\":1,\"1649\":1,\"1650\":1,\"1998\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":1,\"2008\":1}}],[\"abssvs\",{\"0\":{\"2222\":1},\"1\":{\"1508\":1,\"2222\":1,\"2228\":1,\"2229\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1}}],[\"absspecaug\",{\"0\":{\"686\":1},\"1\":{\"625\":1,\"686\":1,\"736\":1,\"737\":1,\"777\":1,\"833\":1,\"954\":1,\"958\":1,\"974\":1,\"1640\":1,\"1641\":1,\"1702\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2184\":1,\"2216\":1,\"2221\":1}}],[\"absmask\",{\"0\":{\"969\":1},\"1\":{\"969\":1,\"978\":1,\"1157\":1}}],[\"absdataset\",{\"0\":{\"2324\":1},\"1\":{\"2249\":2,\"2324\":1,\"2342\":1,\"2344\":1}}],[\"absdiscriminator\",{\"0\":{\"2451\":1},\"1\":{\"2451\":1,\"2458\":1,\"2462\":1,\"2470\":1}}],[\"absdiffusion\",{\"0\":{\"1032\":1},\"1\":{\"1032\":1,\"1155\":1,\"1253\":1}}],[\"absdiarization\",{\"0\":{\"967\":1},\"1\":{\"967\":1}}],[\"absdecoder\",{\"0\":{\"614\":1,\"676\":1,\"952\":1,\"956\":1,\"965\":1,\"1030\":1},\"1\":{\"614\":1,\"616\":1,\"625\":1,\"627\":1,\"634\":1,\"641\":1,\"643\":1,\"651\":1,\"676\":1,\"692\":1,\"696\":1,\"697\":1,\"736\":1,\"737\":1,\"740\":1,\"760\":1,\"770\":1,\"775\":1,\"790\":1,\"795\":1,\"796\":1,\"820\":1,\"847\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":1,\"959\":1,\"965\":1,\"974\":1,\"977\":1,\"1030\":1,\"1112\":1,\"1155\":1,\"1157\":1,\"1158\":1,\"1222\":1,\"1250\":1,\"1959\":1,\"1975\":2,\"1997\":1,\"2127\":1,\"2221\":3}}],[\"absasr\",{\"0\":{\"2026\":1},\"1\":{\"2026\":1,\"2043\":1,\"2055\":1,\"2056\":1,\"2066\":1}}],[\"absasvspoofloss\",{\"0\":{\"950\":1},\"1\":{\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"954\":1}}],[\"absattractor\",{\"0\":{\"963\":1},\"1\":{\"963\":1,\"974\":1,\"979\":1}}],[\"abspreprocessor\",{\"0\":{\"2328\":1},\"1\":{\"2328\":1,\"2336\":1,\"2341\":1,\"2357\":1,\"2363\":1}}],[\"abspreencoder\",{\"0\":{\"684\":1},\"1\":{\"684\":1,\"736\":1,\"737\":1,\"768\":1,\"772\":1,\"777\":1,\"954\":1,\"958\":1,\"1640\":1,\"1641\":1,\"1959\":1,\"1975\":1,\"1997\":1,\"2127\":1,\"2216\":1,\"2221\":1}}],[\"absprojector\",{\"0\":{\"2174\":1},\"1\":{\"1702\":1,\"2174\":1,\"2184\":1,\"2194\":1,\"2205\":1,\"2211\":1}}],[\"abspostdecoder\",{\"0\":{\"2124\":1},\"1\":{\"2124\":1,\"2127\":1,\"2128\":1}}],[\"abspostencoder\",{\"0\":{\"682\":1},\"1\":{\"682\":1,\"736\":1,\"737\":1,\"762\":1,\"765\":1,\"777\":1,\"1959\":1,\"1975\":1,\"1997\":1,\"2126\":1,\"2127\":2,\"2129\":1,\"2221\":1}}],[\"abspooling\",{\"0\":{\"2172\":1},\"1\":{\"1702\":1,\"2172\":1,\"2183\":1,\"2184\":1,\"2190\":1,\"2208\":1}}],[\"abse2e\",{\"0\":{\"2028\":1},\"1\":{\"2028\":1,\"2054\":1}}],[\"absepochstepscheduler\",{\"0\":{\"2011\":1},\"1\":{\"2011\":1,\"2013\":1}}],[\"absextractor\",{\"0\":{\"1040\":1},\"1\":{\"1040\":1,\"1158\":1,\"1268\":1}}],[\"absenhancement\",{\"0\":{\"1038\":1},\"1\":{\"1038\":1}}],[\"absenhloss\",{\"0\":{\"1036\":1},\"1\":{\"1036\":1,\"1132\":1,\"1167\":1,\"1174\":1,\"1204\":3,\"1209\":3,\"1228\":3,\"1276\":1}}],[\"absencoder\",{\"0\":{\"678\":1,\"1034\":1},\"1\":{\"678\":1,\"699\":2,\"700\":1,\"709\":1,\"710\":1,\"711\":1,\"733\":1,\"734\":1,\"736\":1,\"737\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"761\":1,\"771\":1,\"777\":1,\"780\":1,\"791\":1,\"798\":1,\"846\":1,\"849\":1,\"862\":1,\"954\":1,\"958\":1,\"974\":1,\"1034\":1,\"1113\":1,\"1155\":1,\"1157\":1,\"1158\":1,\"1223\":1,\"1251\":1,\"1640\":1,\"1641\":1,\"1702\":1,\"1959\":1,\"1975\":2,\"1996\":2,\"1997\":1,\"2127\":1,\"2184\":1,\"2187\":1,\"2188\":1,\"2191\":1,\"2192\":1,\"2198\":1,\"2203\":1,\"2209\":1,\"2216\":1,\"2221\":4}}],[\"absespnetmodel\",{\"0\":{\"2325\":1},\"1\":{\"78\":1,\"223\":1,\"228\":1,\"625\":1,\"954\":1,\"958\":1,\"960\":1,\"974\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1640\":1,\"1641\":1,\"1702\":1,\"1940\":1,\"1942\":1,\"1950\":1,\"1951\":1,\"1959\":1,\"1965\":2,\"1975\":1,\"1996\":1,\"1997\":1,\"2044\":1,\"2184\":1,\"2216\":1,\"2221\":1,\"2229\":1,\"2246\":4,\"2248\":4,\"2249\":6,\"2250\":4,\"2251\":4,\"2252\":4,\"2253\":4,\"2254\":4,\"2255\":4,\"2256\":4,\"2257\":4,\"2259\":4,\"2260\":4,\"2261\":4,\"2263\":4,\"2264\":4,\"2265\":4,\"2266\":4,\"2267\":4,\"2268\":4,\"2269\":4,\"2270\":4,\"2271\":4,\"2272\":4,\"2273\":4,\"2325\":2,\"2327\":1,\"2338\":1,\"2369\":1,\"2408\":1,\"2446\":1,\"2462\":1}}],[\"absnormalize\",{\"0\":{\"1652\":1},\"1\":{\"625\":1,\"736\":1,\"777\":1,\"954\":1,\"958\":1,\"974\":1,\"1640\":1,\"1641\":1,\"1652\":1,\"1656\":1,\"1671\":1,\"1702\":1,\"1975\":2,\"1996\":1,\"1997\":1,\"2127\":1,\"2184\":1,\"2216\":1,\"2221\":1}}],[\"abstokenizer\",{\"0\":{\"2274\":1},\"1\":{\"2274\":1,\"2275\":1,\"2279\":1,\"2284\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2292\":1,\"2293\":1}}],[\"abstgtfeatsextract\",{\"0\":{\"1972\":1},\"1\":{\"1972\":1,\"1975\":1,\"1978\":1,\"1980\":1,\"1982\":1}}],[\"abstts2\",{\"0\":{\"2445\":1},\"1\":{\"2445\":1,\"2446\":1,\"2447\":1}}],[\"abstts\",{\"0\":{\"2032\":1,\"2403\":1},\"1\":{\"1576\":1,\"2032\":1,\"2040\":1,\"2045\":1,\"2403\":1,\"2408\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1}}],[\"abstraction\",{\"1\":{\"2131\":1}}],[\"abstract\",{\"1\":{\"614\":8,\"676\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"829\":1,\"950\":1,\"951\":1,\"952\":1,\"956\":1,\"963\":1,\"965\":1,\"966\":1,\"967\":1,\"968\":1,\"969\":1,\"970\":1,\"1030\":1,\"1032\":2,\"1034\":1,\"1035\":1,\"1036\":1,\"1038\":1,\"1039\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1045\":1,\"1116\":2,\"1163\":1,\"1164\":1,\"1174\":2,\"1229\":2,\"1245\":7,\"1333\":1,\"1381\":5,\"1415\":1,\"1416\":1,\"1508\":2,\"1576\":2,\"1642\":1,\"1652\":1,\"1659\":1,\"1938\":2,\"1967\":1,\"1969\":1,\"1971\":3,\"1972\":1,\"1973\":3,\"2010\":3,\"2011\":3,\"2012\":3,\"2013\":3,\"2026\":1,\"2027\":1,\"2028\":1,\"2029\":1,\"2030\":1,\"2031\":1,\"2032\":1,\"2033\":1,\"2034\":1,\"2035\":1,\"2124\":2,\"2125\":1,\"2130\":1,\"2131\":3,\"2170\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2175\":1,\"2215\":2,\"2222\":3,\"2249\":6,\"2274\":2,\"2324\":2,\"2325\":3,\"2327\":3,\"2401\":1,\"2402\":2,\"2403\":3,\"2443\":1,\"2445\":3,\"2451\":1,\"2453\":1,\"2454\":1,\"2455\":2,\"2456\":1}}],[\"abstask\",{\"0\":{\"2249\":1},\"1\":{\"78\":3,\"81\":1,\"82\":1,\"223\":1,\"1962\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":2,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2325\":2,\"2327\":2}}],[\"abs\",{\"0\":{\"614\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"950\":1,\"952\":1,\"956\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1381\":1,\"1508\":1,\"1576\":1,\"1642\":1,\"1652\":1,\"1938\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1998\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2130\":1,\"2131\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2215\":1,\"2222\":1,\"2249\":1,\"2258\":1,\"2274\":1,\"2325\":1,\"2327\":1,\"2401\":1,\"2403\":1,\"2443\":1,\"2445\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1},\"1\":{\"78\":2,\"223\":6,\"243\":1,\"614\":1,\"616\":1,\"629\":1,\"635\":1,\"652\":3,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"696\":1,\"699\":1,\"711\":1,\"768\":1,\"774\":1,\"780\":1,\"950\":1,\"952\":1,\"956\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1066\":1,\"1155\":1,\"1157\":1,\"1170\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1330\":1,\"1381\":1,\"1385\":2,\"1396\":1,\"1401\":1,\"1411\":1,\"1466\":1,\"1508\":1,\"1515\":1,\"1576\":1,\"1577\":1,\"1590\":1,\"1642\":1,\"1643\":1,\"1646\":1,\"1652\":1,\"1661\":1,\"1668\":1,\"1705\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1720\":1,\"1721\":1,\"1731\":2,\"1748\":1,\"1749\":1,\"1768\":1,\"1785\":1,\"1786\":1,\"1817\":1,\"1818\":1,\"1820\":1,\"1863\":1,\"1938\":2,\"1967\":2,\"1969\":1,\"1971\":1,\"1972\":1,\"1977\":1,\"1998\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2130\":1,\"2131\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2215\":1,\"2222\":1,\"2227\":1,\"2231\":1,\"2239\":1,\"2246\":2,\"2248\":2,\"2249\":3,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2258\":1,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2274\":1,\"2325\":2,\"2327\":2,\"2377\":1,\"2401\":1,\"2403\":1,\"2443\":1,\"2445\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1}}],[\"above\",{\"0\":{\"1570\":1},\"1\":{\"31\":1,\"132\":1,\"141\":1,\"162\":1,\"164\":1,\"167\":1,\"175\":1,\"206\":1,\"211\":1,\"212\":1,\"218\":1,\"242\":2,\"255\":1,\"267\":1,\"272\":1,\"276\":1,\"282\":1,\"286\":3,\"289\":2,\"290\":1,\"635\":1,\"664\":1,\"786\":1,\"866\":1,\"1008\":1,\"1051\":1,\"1402\":1,\"1409\":1,\"1467\":1,\"1552\":1,\"1570\":1,\"1594\":1,\"1595\":1,\"1606\":1,\"1619\":1,\"1626\":1,\"1655\":2,\"1901\":1,\"1903\":1,\"2151\":1,\"2310\":1,\"2355\":1}}],[\"about\",{\"0\":{\"60\":1,\"196\":1,\"213\":1,\"268\":1,\"277\":1},\"1\":{\"18\":1,\"69\":2,\"92\":1,\"93\":1,\"110\":1,\"119\":1,\"125\":1,\"132\":1,\"175\":1,\"194\":1,\"196\":2,\"200\":1,\"205\":1,\"210\":1,\"211\":1,\"213\":2,\"214\":1,\"217\":1,\"242\":3,\"254\":1,\"262\":1,\"265\":1,\"266\":1,\"267\":1,\"268\":3,\"274\":1,\"275\":1,\"277\":3,\"285\":1,\"286\":1,\"974\":1,\"1209\":1,\"1228\":1,\"1462\":1,\"2355\":2}}],[\"ac\",{\"1\":{\"1717\":1}}],[\"academy\",{\"1\":{\"1385\":2}}],[\"academic\",{\"1\":{\"7\":1}}],[\"acm\",{\"1\":{\"1330\":1}}],[\"aconv\",{\"1\":{\"796\":2,\"869\":2,\"1706\":4,\"1708\":4,\"1709\":4,\"1710\":4,\"1711\":4,\"1712\":4,\"1715\":4,\"1716\":6,\"1768\":4,\"1895\":4,\"1993\":4,\"2245\":4,\"2431\":4}}],[\"acoustic\",{\"1\":{\"276\":1,\"285\":3,\"287\":1,\"290\":1,\"699\":1,\"703\":3,\"717\":1,\"755\":2,\"785\":2,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"919\":2,\"922\":1,\"936\":1,\"937\":1,\"1318\":1,\"1526\":2,\"1552\":2,\"1553\":2,\"1577\":1,\"1598\":2,\"1600\":2,\"1625\":2,\"1626\":2,\"1708\":1,\"1709\":1,\"1758\":3,\"2130\":2,\"2136\":2,\"2231\":3,\"2298\":1}}],[\"acoustics\",{\"1\":{\"207\":1}}],[\"acodec\",{\"1\":{\"74\":2}}],[\"acesinger\",{\"1\":{\"267\":1}}],[\"ace\",{\"1\":{\"188\":2,\"267\":1}}],[\"across\",{\"1\":{\"141\":1,\"142\":1,\"150\":1,\"173\":1,\"200\":3,\"201\":1,\"730\":1,\"818\":1,\"820\":1,\"821\":1,\"828\":1,\"830\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"919\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1477\":1,\"2000\":3,\"2001\":3,\"2130\":1,\"2134\":1,\"2136\":1,\"2162\":1}}],[\"action=nesteddictaction\",{\"1\":{\"2479\":1}}],[\"action\",{\"0\":{\"2478\":1},\"1\":{\"2478\":3}}],[\"acting\",{\"1\":{\"1415\":1}}],[\"active\",{\"1\":{\"2044\":4}}],[\"activity\",{\"1\":{\"200\":1,\"2044\":1,\"2378\":1}}],[\"activatio\",{\"1\":{\"703\":1}}],[\"activation=relu\",{\"1\":{\"1747\":1,\"1809\":1}}],[\"activation=<class\",{\"1\":{\"1108\":1,\"1145\":1,\"1168\":1,\"1264\":1,\"1265\":1}}],[\"activation=\",{\"1\":{\"744\":1,\"817\":1,\"1029\":1,\"1070\":1,\"1071\":1,\"1120\":2,\"1122\":2,\"1139\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1235\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1279\":1,\"1281\":1,\"1282\":1,\"1334\":1,\"1548\":1}}],[\"activation=none\",{\"1\":{\"688\":1,\"726\":1,\"769\":1,\"859\":1,\"906\":1}}],[\"activations\",{\"1\":{\"706\":3,\"748\":1,\"1086\":1,\"1207\":1,\"1855\":1}}],[\"activation\",{\"0\":{\"629\":1,\"635\":1,\"650\":1,\"652\":1,\"664\":2,\"688\":1,\"902\":1,\"1313\":1,\"1464\":1,\"1483\":1,\"1500\":1,\"1886\":1},\"1\":{\"43\":2,\"141\":14,\"142\":4,\"143\":3,\"243\":1,\"619\":3,\"620\":1,\"629\":4,\"632\":5,\"633\":3,\"634\":6,\"635\":4,\"637\":3,\"638\":4,\"650\":4,\"652\":3,\"661\":6,\"664\":13,\"674\":6,\"688\":1,\"700\":1,\"703\":1,\"705\":1,\"709\":3,\"710\":1,\"713\":1,\"715\":1,\"717\":1,\"733\":2,\"734\":2,\"746\":1,\"747\":3,\"748\":1,\"755\":4,\"768\":3,\"769\":1,\"774\":3,\"780\":6,\"781\":2,\"783\":2,\"785\":2,\"804\":2,\"818\":3,\"819\":1,\"841\":1,\"851\":2,\"878\":4,\"879\":4,\"881\":4,\"882\":4,\"883\":4,\"884\":4,\"902\":4,\"919\":3,\"922\":1,\"932\":2,\"934\":2,\"936\":1,\"937\":1,\"1029\":2,\"1070\":2,\"1071\":2,\"1107\":3,\"1108\":2,\"1119\":2,\"1139\":2,\"1141\":3,\"1145\":4,\"1168\":1,\"1185\":2,\"1205\":1,\"1235\":2,\"1262\":2,\"1264\":4,\"1265\":4,\"1269\":3,\"1270\":3,\"1271\":3,\"1279\":2,\"1280\":3,\"1281\":2,\"1282\":2,\"1283\":3,\"1312\":1,\"1313\":1,\"1334\":3,\"1389\":6,\"1390\":2,\"1391\":4,\"1392\":6,\"1396\":6,\"1397\":6,\"1401\":8,\"1402\":4,\"1403\":4,\"1408\":6,\"1409\":6,\"1420\":2,\"1450\":12,\"1452\":12,\"1454\":6,\"1456\":6,\"1458\":6,\"1460\":6,\"1464\":1,\"1466\":6,\"1467\":2,\"1468\":4,\"1483\":2,\"1500\":1,\"1513\":6,\"1519\":3,\"1526\":7,\"1535\":3,\"1536\":3,\"1546\":3,\"1548\":5,\"1549\":4,\"1551\":6,\"1552\":3,\"1553\":5,\"1582\":6,\"1592\":6,\"1593\":2,\"1594\":2,\"1595\":4,\"1596\":6,\"1597\":6,\"1598\":7,\"1599\":9,\"1600\":7,\"1604\":6,\"1605\":9,\"1606\":6,\"1609\":4,\"1614\":6,\"1615\":6,\"1618\":2,\"1619\":6,\"1622\":3,\"1624\":6,\"1625\":5,\"1626\":3,\"1661\":2,\"1668\":1,\"1736\":1,\"1749\":6,\"1750\":3,\"1779\":3,\"1838\":1,\"1863\":6,\"1864\":6,\"1867\":3,\"1886\":2,\"1993\":3,\"1994\":1,\"2126\":1,\"2191\":3,\"2223\":3,\"2239\":2,\"2240\":1,\"2245\":3,\"2411\":3,\"2412\":3,\"2420\":2,\"2423\":3,\"2431\":3,\"2447\":3}}],[\"activate=false\",{\"1\":{\"769\":1}}],[\"activate\",{\"1\":{\"1\":5,\"31\":3,\"153\":1,\"154\":1,\"162\":5,\"163\":4,\"164\":1}}],[\"act=elu\",{\"1\":{\"1103\":1,\"1236\":1}}],[\"act=relu\",{\"1\":{\"1068\":1,\"1087\":1,\"1091\":1,\"1093\":1,\"1230\":1,\"1233\":1}}],[\"act=none\",{\"1\":{\"817\":1}}],[\"acts\",{\"1\":{\"755\":8,\"773\":1,\"785\":5,\"786\":1,\"800\":1,\"803\":1,\"804\":1,\"866\":1,\"867\":1,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"919\":2,\"921\":2,\"922\":2,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"935\":2,\"936\":2,\"937\":2}}],[\"act\",{\"0\":{\"1312\":1},\"1\":{\"141\":4,\"143\":1,\"632\":1,\"661\":4,\"786\":2,\"800\":2,\"818\":1,\"866\":1,\"867\":2,\"921\":2,\"935\":2,\"1205\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1262\":2,\"1312\":1,\"1886\":1,\"2458\":1}}],[\"actual\",{\"1\":{\"94\":2,\"147\":1,\"243\":1,\"653\":2,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"1029\":1,\"1235\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1526\":1,\"1553\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1842\":2,\"2130\":1,\"2133\":1,\"2143\":1,\"2220\":1}}],[\"actually\",{\"1\":{\"68\":1,\"196\":1,\"213\":1,\"242\":1,\"268\":1,\"277\":1}}],[\"achieved\",{\"1\":{\"150\":1,\"691\":1,\"1271\":1}}],[\"achieve\",{\"1\":{\"41\":1,\"130\":1,\"193\":1}}],[\"acc=0\",{\"1\":{\"113\":2}}],[\"accurate\",{\"0\":{\"901\":1},\"1\":{\"901\":1,\"2245\":1}}],[\"accuracy\",{\"0\":{\"1928\":1},\"1\":{\"39\":1,\"46\":1,\"114\":1,\"175\":1,\"201\":1,\"211\":1,\"235\":2,\"246\":1,\"267\":2,\"276\":2,\"1246\":1,\"1928\":3,\"2167\":2,\"2176\":2,\"2207\":2,\"2355\":1}}],[\"accum\",{\"1\":{\"102\":6,\"136\":3,\"243\":1,\"2348\":1,\"2370\":2,\"2372\":1}}],[\"accumulated\",{\"1\":{\"102\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"2240\":1}}],[\"accumulate\",{\"1\":{\"102\":1,\"961\":1,\"2355\":2}}],[\"accumulating\",{\"0\":{\"102\":1},\"1\":{\"102\":2}}],[\"accomodate\",{\"1\":{\"232\":1,\"258\":1}}],[\"accordingly\",{\"1\":{\"119\":1,\"128\":1,\"2044\":1}}],[\"according\",{\"1\":{\"46\":1,\"80\":1,\"96\":1,\"124\":1,\"127\":1,\"165\":1,\"536\":1,\"616\":1,\"794\":1,\"803\":1,\"978\":1,\"1164\":1,\"1590\":1,\"1629\":2,\"1645\":1,\"1650\":1,\"1730\":1,\"1905\":1,\"1920\":1,\"2151\":1,\"2310\":1}}],[\"account\",{\"1\":{\"22\":1,\"254\":1,\"696\":1,\"1711\":1,\"1712\":1}}],[\"acc\",{\"1\":{\"39\":2,\"114\":1,\"136\":2,\"243\":1,\"267\":2,\"276\":2,\"527\":1,\"541\":1,\"1949\":1,\"2355\":8,\"2359\":1}}],[\"accent\",{\"0\":{\"2295\":1,\"2296\":1},\"1\":{\"287\":6,\"290\":1,\"481\":2,\"2295\":1,\"2296\":1}}],[\"accentdb\",{\"1\":{\"201\":1}}],[\"accelerate\",{\"1\":{\"120\":1,\"286\":1}}],[\"accessed\",{\"1\":{\"1883\":1}}],[\"accessing\",{\"1\":{\"922\":1,\"936\":1,\"937\":1,\"2044\":1}}],[\"accessible\",{\"1\":{\"205\":1}}],[\"access\",{\"1\":{\"22\":1,\"25\":1,\"147\":1,\"173\":1,\"248\":1,\"1000\":1,\"2044\":3,\"2049\":3,\"2131\":1}}],[\"acceptable\",{\"1\":{\"1064\":1,\"1078\":1,\"1153\":1,\"1202\":1,\"1262\":1,\"1290\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"2232\":1,\"2238\":1}}],[\"accepted\",{\"1\":{\"267\":1}}],[\"accepts\",{\"1\":{\"197\":1,\"756\":1,\"773\":1,\"819\":1,\"960\":1,\"1306\":1,\"1371\":1}}],[\"accept\",{\"1\":{\"22\":1,\"756\":2,\"773\":2,\"858\":1,\"866\":1,\"867\":1,\"2327\":1}}],[\"acl\",{\"1\":{\"10\":2,\"260\":1}}],[\"avocodo=false\",{\"1\":{\"1548\":1,\"1552\":1}}],[\"avocodogenerator\",{\"0\":{\"1513\":1},\"1\":{\"1513\":2}}],[\"avocododiscriminatorplus\",{\"0\":{\"1511\":1},\"1\":{\"1511\":1}}],[\"avocododiscriminator\",{\"0\":{\"1509\":1},\"1\":{\"1509\":1}}],[\"avocodo\",{\"0\":{\"1509\":2,\"1511\":2,\"1513\":2,\"1515\":2,\"1516\":2,\"1530\":2,\"1532\":2,\"1541\":2,\"1543\":2,\"1563\":2},\"1\":{\"1509\":3,\"1511\":3,\"1513\":3,\"1515\":2,\"1516\":2,\"1530\":2,\"1532\":2,\"1541\":2,\"1543\":2,\"1552\":2,\"1553\":1,\"1563\":2}}],[\"avoids\",{\"1\":{\"262\":1,\"2044\":4}}],[\"avoid\",{\"1\":{\"3\":1,\"32\":1,\"60\":1,\"71\":2,\"84\":1,\"696\":1,\"722\":1,\"777\":2,\"1124\":1,\"1156\":2,\"1655\":1,\"1753\":1,\"1754\":1,\"1940\":2,\"1942\":2,\"2039\":1,\"2131\":1,\"2346\":1,\"2368\":1}}],[\"avhubertmodel\",{\"0\":{\"675\":1},\"1\":{\"675\":1}}],[\"avhubertconfig\",{\"0\":{\"674\":1},\"1\":{\"674\":1,\"675\":2}}],[\"avhubert\",{\"0\":{\"674\":1,\"675\":1,\"693\":1,\"746\":1,\"756\":1,\"805\":1,\"807\":1,\"825\":1,\"839\":1,\"885\":1,\"889\":2,\"893\":1,\"894\":1,\"910\":1,\"915\":1,\"941\":1},\"1\":{\"674\":2,\"675\":3,\"693\":1,\"746\":9,\"756\":1,\"805\":1,\"807\":1,\"825\":1,\"839\":1,\"885\":1,\"889\":2,\"893\":1,\"894\":1,\"910\":1,\"915\":1,\"941\":1}}],[\"avgpool1d\",{\"1\":{\"1401\":1,\"1402\":1,\"1408\":1,\"1409\":1,\"1466\":1,\"1467\":1,\"1526\":1,\"1549\":1,\"1553\":1,\"1594\":1,\"1595\":1,\"1598\":1,\"1600\":1,\"1606\":1,\"1625\":1}}],[\"avgpool\",{\"1\":{\"768\":2}}],[\"avgpool=false\",{\"1\":{\"768\":1}}],[\"avg\",{\"1\":{\"286\":1,\"377\":2,\"526\":1,\"807\":1,\"1481\":1,\"2412\":2,\"2423\":2,\"2447\":2}}],[\"averaging\",{\"1\":{\"1279\":1,\"1501\":1,\"2348\":1,\"2370\":2,\"2372\":1}}],[\"averagecheckpointscallback\",{\"0\":{\"2333\":1},\"1\":{\"2333\":1}}],[\"average=true\",{\"1\":{\"1501\":1}}],[\"averaged\",{\"1\":{\"218\":1,\"235\":1,\"267\":1,\"276\":1,\"284\":1,\"286\":6,\"787\":1,\"1599\":4,\"1627\":2,\"1629\":2,\"1949\":3,\"2404\":1,\"2409\":1,\"2412\":3,\"2413\":2,\"2423\":2,\"2424\":2,\"2434\":1,\"2447\":2,\"2448\":2}}],[\"average\",{\"0\":{\"541\":1,\"1477\":1,\"1629\":1,\"1949\":2,\"2318\":1,\"2332\":1},\"1\":{\"130\":2,\"133\":2,\"223\":1,\"449\":4,\"541\":2,\"627\":2,\"633\":1,\"637\":1,\"710\":3,\"711\":3,\"740\":2,\"768\":1,\"787\":1,\"1073\":1,\"1279\":1,\"1301\":1,\"1306\":1,\"1389\":4,\"1396\":4,\"1400\":2,\"1401\":4,\"1408\":4,\"1441\":2,\"1466\":4,\"1469\":2,\"1477\":2,\"1501\":1,\"1526\":4,\"1553\":4,\"1584\":3,\"1587\":6,\"1591\":3,\"1598\":4,\"1600\":4,\"1625\":4,\"1629\":2,\"1949\":2,\"2190\":1,\"2318\":1,\"2332\":1,\"2354\":2,\"2365\":1}}],[\"ave\",{\"1\":{\"136\":4,\"218\":2,\"243\":2,\"267\":1,\"276\":1,\"286\":9,\"290\":1,\"701\":4}}],[\"availables\",{\"1\":{\"19\":1}}],[\"available\",{\"1\":{\"6\":1,\"43\":2,\"45\":1,\"46\":1,\"71\":1,\"102\":1,\"138\":1,\"139\":3,\"142\":1,\"145\":1,\"148\":2,\"242\":1,\"244\":1,\"246\":1,\"275\":1,\"276\":1,\"290\":1,\"527\":2,\"536\":2,\"635\":1,\"652\":1,\"760\":1,\"2134\":1,\"2162\":1,\"2286\":1,\"2355\":2}}],[\"atbatch\",{\"0\":{\"2094\":1}}],[\"atfblock\",{\"0\":{\"1029\":1},\"1\":{\"1029\":1}}],[\"atype\",{\"1\":{\"796\":1,\"869\":1,\"1895\":2,\"1993\":2,\"2245\":1,\"2431\":1}}],[\"atlas\",{\"1\":{\"161\":2}}],[\"attmultiheadmultiresloc\",{\"0\":{\"1716\":1},\"1\":{\"1716\":3}}],[\"attmultiheadloc\",{\"0\":{\"1715\":1},\"1\":{\"1715\":3}}],[\"attmultiheaddot\",{\"0\":{\"1714\":1},\"1\":{\"1714\":3}}],[\"attmultiheadadd\",{\"0\":{\"1713\":1},\"1\":{\"1713\":3}}],[\"attlocrec\",{\"0\":{\"1712\":1},\"1\":{\"1712\":3}}],[\"attloc2d\",{\"0\":{\"1711\":1},\"1\":{\"1711\":3}}],[\"attloc\",{\"0\":{\"1710\":1},\"1\":{\"1710\":3,\"1768\":1}}],[\"attetion\",{\"1\":{\"1708\":1,\"1709\":1,\"1710\":1,\"1768\":1}}],[\"attenuate\",{\"1\":{\"2240\":1}}],[\"attenuation\",{\"1\":{\"1687\":1}}],[\"attending\",{\"1\":{\"1719\":2,\"1725\":3,\"1751\":1}}],[\"attended\",{\"1\":{\"1708\":3,\"1709\":3,\"1710\":3,\"1768\":3,\"1977\":1}}],[\"attentive\",{\"1\":{\"1029\":1,\"1974\":1,\"1985\":1,\"2183\":1}}],[\"attentions\",{\"0\":{\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1768\":1,\"1801\":1,\"1860\":1,\"1861\":1,\"1895\":1,\"1950\":2},\"1\":{\"1704\":1,\"1705\":1,\"1706\":2,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1750\":1,\"1768\":1,\"1801\":1,\"1811\":1,\"1860\":1,\"1861\":1,\"1895\":1,\"1950\":2}}],[\"attentionreference\",{\"0\":{\"1054\":1},\"1\":{\"1054\":1}}],[\"attention=false\",{\"1\":{\"787\":2}}],[\"attention=true\",{\"1\":{\"733\":1}}],[\"attentional\",{\"1\":{\"260\":1,\"261\":1,\"262\":8,\"1732\":1}}],[\"attention\",{\"0\":{\"175\":2,\"644\":1,\"649\":1,\"654\":1,\"668\":1,\"784\":1,\"869\":1,\"1785\":1,\"1792\":1,\"1794\":1,\"1817\":1,\"1967\":2,\"1984\":1,\"1987\":1,\"1990\":1},\"1\":{\"43\":4,\"53\":2,\"114\":1,\"138\":1,\"141\":7,\"142\":7,\"147\":2,\"148\":1,\"175\":13,\"243\":6,\"260\":1,\"261\":2,\"262\":12,\"286\":2,\"290\":5,\"617\":4,\"618\":4,\"620\":2,\"624\":4,\"626\":2,\"633\":12,\"634\":5,\"636\":2,\"637\":1,\"642\":3,\"643\":3,\"644\":18,\"645\":2,\"649\":8,\"654\":1,\"661\":2,\"668\":1,\"669\":1,\"674\":8,\"692\":6,\"700\":3,\"701\":2,\"709\":10,\"710\":8,\"711\":8,\"731\":3,\"732\":3,\"733\":4,\"734\":3,\"735\":2,\"736\":1,\"745\":1,\"746\":2,\"747\":4,\"748\":7,\"749\":2,\"760\":1,\"766\":3,\"767\":3,\"771\":1,\"774\":18,\"775\":3,\"780\":10,\"784\":7,\"787\":10,\"846\":5,\"848\":3,\"849\":8,\"850\":3,\"851\":3,\"869\":1,\"1029\":4,\"1054\":1,\"1055\":1,\"1057\":1,\"1064\":2,\"1070\":4,\"1071\":4,\"1107\":7,\"1139\":1,\"1141\":1,\"1185\":1,\"1235\":4,\"1262\":2,\"1269\":1,\"1270\":2,\"1271\":1,\"1278\":6,\"1279\":2,\"1280\":3,\"1281\":3,\"1282\":3,\"1283\":2,\"1290\":3,\"1519\":13,\"1535\":12,\"1536\":13,\"1546\":16,\"1552\":10,\"1553\":3,\"1577\":2,\"1589\":1,\"1599\":10,\"1622\":16,\"1625\":4,\"1626\":11,\"1637\":2,\"1704\":6,\"1705\":6,\"1706\":9,\"1707\":6,\"1708\":9,\"1709\":9,\"1710\":11,\"1711\":12,\"1712\":11,\"1713\":11,\"1714\":8,\"1715\":14,\"1716\":14,\"1719\":1,\"1720\":1,\"1729\":1,\"1730\":2,\"1735\":2,\"1750\":9,\"1751\":3,\"1756\":2,\"1757\":2,\"1759\":2,\"1768\":11,\"1770\":8,\"1771\":7,\"1785\":4,\"1789\":2,\"1790\":2,\"1794\":12,\"1801\":4,\"1817\":4,\"1847\":3,\"1860\":4,\"1861\":5,\"1864\":1,\"1880\":1,\"1891\":3,\"1895\":9,\"1947\":1,\"1950\":1,\"1965\":2,\"1966\":2,\"1967\":3,\"1975\":1,\"1977\":1,\"1984\":2,\"1987\":2,\"1990\":2,\"1992\":9,\"1993\":9,\"1995\":9,\"1997\":1,\"2016\":1,\"2124\":1,\"2126\":2,\"2127\":1,\"2128\":1,\"2129\":8,\"2133\":2,\"2176\":1,\"2183\":1,\"2187\":1,\"2191\":9,\"2202\":1,\"2203\":1,\"2213\":1,\"2214\":1,\"2221\":1,\"2223\":3,\"2224\":4,\"2231\":1,\"2239\":9,\"2240\":6,\"2245\":13,\"2338\":1,\"2347\":1,\"2369\":3,\"2371\":1,\"2407\":2,\"2411\":9,\"2412\":9,\"2421\":2,\"2423\":9,\"2429\":1,\"2430\":2,\"2431\":12,\"2432\":15,\"2447\":8}}],[\"attforwardta\",{\"0\":{\"1709\":1},\"1\":{\"1709\":3}}],[\"attforward\",{\"0\":{\"1708\":1},\"1\":{\"1708\":3}}],[\"attdot\",{\"0\":{\"1707\":1},\"1\":{\"1707\":2}}],[\"attcovloc\",{\"0\":{\"1706\":1},\"1\":{\"1706\":3}}],[\"attcov\",{\"0\":{\"1705\":1},\"1\":{\"1705\":3}}],[\"attadd\",{\"0\":{\"1704\":1},\"1\":{\"1704\":2}}],[\"attractor\",{\"0\":{\"963\":2,\"979\":2},\"1\":{\"963\":2,\"974\":5,\"979\":5,\"1117\":2}}],[\"attrs\",{\"0\":{\"898\":1},\"1\":{\"898\":2,\"1156\":2}}],[\"attributes\",{\"1\":{\"223\":1,\"545\":1,\"821\":1,\"829\":1,\"1156\":1,\"2215\":2,\"2216\":1}}],[\"attribute\",{\"1\":{\"132\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"756\":1,\"760\":1,\"773\":1,\"829\":2,\"830\":1,\"859\":1,\"866\":1,\"867\":1,\"1117\":1,\"1963\":1}}],[\"attn=none\",{\"1\":{\"1751\":1}}],[\"attn=false\",{\"1\":{\"787\":1,\"1794\":2}}],[\"attnblockpp\",{\"0\":{\"1057\":1},\"1\":{\"1057\":1}}],[\"attnblock\",{\"0\":{\"1055\":1},\"1\":{\"1055\":1}}],[\"attn\",{\"0\":{\"2183\":1,\"2313\":2},\"1\":{\"261\":4,\"633\":5,\"644\":1,\"700\":3,\"701\":5,\"709\":2,\"733\":1,\"734\":1,\"735\":2,\"749\":3,\"774\":4,\"780\":1,\"787\":3,\"848\":1,\"850\":1,\"851\":2,\"1064\":2,\"1107\":2,\"1185\":1,\"1211\":1,\"1262\":2,\"1269\":4,\"1270\":4,\"1271\":5,\"1290\":2,\"1526\":3,\"1589\":2,\"1598\":3,\"1599\":6,\"1600\":3,\"1637\":3,\"1683\":1,\"1719\":6,\"1725\":9,\"1735\":2,\"1751\":6,\"1759\":2,\"1794\":3,\"1863\":2,\"1864\":2,\"1891\":3,\"1994\":1,\"2133\":2,\"2183\":1,\"2191\":1,\"2239\":5,\"2240\":5,\"2245\":6,\"2313\":2,\"2411\":6,\"2412\":6,\"2423\":5,\"2431\":6,\"2432\":19,\"2447\":6}}],[\"att=1\",{\"1\":{\"1860\":1}}],[\"att=28\",{\"1\":{\"113\":1}}],[\"att=35\",{\"1\":{\"113\":1}}],[\"att\",{\"0\":{\"1860\":1,\"1861\":1,\"1891\":1,\"1895\":1},\"1\":{\"39\":2,\"43\":3,\"114\":1,\"141\":5,\"142\":2,\"243\":1,\"286\":2,\"406\":2,\"484\":2,\"490\":2,\"617\":2,\"618\":2,\"620\":1,\"624\":2,\"633\":2,\"634\":2,\"636\":1,\"642\":4,\"643\":12,\"649\":8,\"654\":10,\"661\":2,\"692\":2,\"709\":2,\"710\":2,\"711\":2,\"774\":2,\"780\":2,\"796\":2,\"849\":2,\"869\":1,\"974\":1,\"979\":1,\"1029\":9,\"1054\":1,\"1070\":4,\"1071\":4,\"1107\":2,\"1139\":3,\"1141\":2,\"1185\":2,\"1235\":9,\"1278\":2,\"1279\":7,\"1280\":10,\"1281\":10,\"1282\":10,\"1283\":7,\"1625\":1,\"1704\":4,\"1705\":4,\"1706\":4,\"1707\":4,\"1708\":4,\"1709\":4,\"1710\":4,\"1711\":8,\"1712\":8,\"1713\":6,\"1714\":6,\"1715\":6,\"1716\":6,\"1735\":2,\"1736\":4,\"1750\":6,\"1751\":2,\"1759\":2,\"1768\":4,\"1770\":2,\"1771\":2,\"1801\":2,\"1851\":1,\"1860\":2,\"1861\":5,\"1891\":1,\"1895\":1,\"1947\":1,\"1976\":1,\"1990\":1,\"1992\":2,\"1993\":6,\"1995\":2,\"2129\":2,\"2223\":4,\"2224\":2,\"2245\":5,\"2407\":2,\"2431\":5,\"2432\":1}}],[\"atsunori\",{\"1\":{\"15\":1}}],[\"at\",{\"0\":{\"118\":1,\"174\":1,\"2084\":1},\"1\":{\"3\":1,\"18\":1,\"19\":1,\"22\":1,\"26\":1,\"45\":4,\"47\":1,\"69\":1,\"79\":1,\"87\":1,\"91\":3,\"106\":1,\"108\":1,\"110\":2,\"135\":1,\"139\":1,\"145\":3,\"159\":1,\"162\":1,\"165\":1,\"168\":1,\"174\":2,\"195\":1,\"197\":1,\"200\":2,\"205\":1,\"213\":1,\"217\":1,\"223\":2,\"224\":1,\"242\":3,\"260\":1,\"263\":1,\"267\":1,\"269\":2,\"276\":2,\"278\":2,\"284\":1,\"285\":2,\"286\":2,\"290\":5,\"536\":2,\"616\":2,\"625\":1,\"627\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"693\":1,\"696\":2,\"697\":2,\"704\":1,\"705\":1,\"713\":1,\"718\":1,\"720\":1,\"722\":1,\"738\":1,\"741\":1,\"752\":1,\"757\":1,\"768\":1,\"778\":1,\"781\":1,\"788\":1,\"791\":1,\"796\":1,\"798\":1,\"804\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"821\":1,\"825\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"842\":1,\"844\":1,\"846\":1,\"852\":1,\"854\":1,\"856\":1,\"860\":1,\"862\":1,\"864\":1,\"932\":1,\"934\":1,\"950\":1,\"952\":1,\"956\":1,\"958\":1,\"960\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"1000\":6,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1066\":1,\"1068\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1086\":2,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1114\":1,\"1120\":1,\"1122\":1,\"1134\":1,\"1137\":1,\"1139\":2,\"1141\":1,\"1142\":1,\"1145\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1155\":1,\"1157\":1,\"1159\":1,\"1165\":1,\"1168\":1,\"1177\":1,\"1185\":2,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1200\":1,\"1202\":1,\"1205\":1,\"1207\":2,\"1211\":1,\"1213\":1,\"1215\":1,\"1219\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1248\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1262\":1,\"1265\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1301\":1,\"1327\":1,\"1328\":1,\"1330\":1,\"1350\":1,\"1356\":1,\"1372\":1,\"1383\":1,\"1387\":1,\"1392\":1,\"1398\":1,\"1400\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1413\":1,\"1415\":2,\"1417\":1,\"1420\":5,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":4,\"1452\":4,\"1454\":2,\"1456\":2,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1484\":1,\"1509\":1,\"1511\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1549\":1,\"1554\":1,\"1589\":1,\"1638\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1657\":1,\"1662\":1,\"1687\":1,\"1717\":1,\"1797\":1,\"1819\":1,\"1823\":1,\"1834\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1945\":1,\"1957\":1,\"1967\":1,\"1969\":1,\"1972\":1,\"1975\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"2016\":1,\"2018\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2147\":1,\"2155\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2203\":1,\"2205\":1,\"2209\":1,\"2211\":1,\"2216\":1,\"2249\":1,\"2253\":1,\"2305\":1,\"2325\":1,\"2355\":1,\"2369\":1,\"2401\":1,\"2405\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2434\":1,\"2443\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2465\":1,\"2467\":1,\"2470\":1}}],[\"adversarial\",{\"1\":{\"1526\":3,\"1546\":1,\"1553\":4,\"1584\":2,\"1591\":3,\"1598\":3,\"1600\":3,\"1611\":1,\"1612\":1,\"1616\":1,\"1622\":1,\"1625\":4,\"1626\":1}}],[\"adv\",{\"1\":{\"1389\":3,\"1396\":3,\"1401\":3,\"1408\":3,\"1466\":3,\"1526\":6,\"1553\":6,\"1598\":6,\"1600\":6,\"1625\":6}}],[\"advances\",{\"1\":{\"1717\":1}}],[\"advance\",{\"1\":{\"1647\":1}}],[\"advanced\",{\"1\":{\"220\":1}}],[\"advancing\",{\"1\":{\"12\":1}}],[\"advantage\",{\"1\":{\"18\":1}}],[\"adhere\",{\"1\":{\"829\":1}}],[\"adim\",{\"1\":{\"796\":1,\"869\":1,\"1107\":2,\"1278\":2,\"1526\":1,\"1577\":3,\"1590\":2,\"1598\":1,\"1599\":1,\"1600\":1,\"1895\":2,\"1993\":2,\"1994\":1,\"2239\":1,\"2240\":1,\"2245\":2,\"2411\":1,\"2412\":1,\"2423\":1,\"2426\":2,\"2428\":2,\"2431\":2,\"2432\":2,\"2447\":1}}],[\"adithya\",{\"1\":{\"156\":1}}],[\"adjacent\",{\"1\":{\"269\":3,\"278\":3,\"1259\":1,\"1261\":1}}],[\"adjust\",{\"1\":{\"821\":1,\"1103\":1,\"1114\":1,\"1236\":1}}],[\"adjusted\",{\"1\":{\"821\":1}}],[\"adjusts\",{\"1\":{\"101\":1,\"2018\":1}}],[\"adjustment\",{\"1\":{\"3\":1}}],[\"administrator\",{\"1\":{\"165\":1}}],[\"adopt\",{\"1\":{\"85\":1,\"95\":1,\"1061\":1}}],[\"adopts\",{\"1\":{\"71\":1,\"162\":1,\"225\":1}}],[\"adaptaion\",{\"1\":{\"1683\":1}}],[\"adaptation\",{\"1\":{\"290\":1,\"1086\":2,\"1207\":2,\"1268\":2,\"1274\":4,\"1683\":1}}],[\"adaptor\",{\"0\":{\"765\":1,\"2304\":1},\"1\":{\"762\":1,\"765\":3,\"2304\":1}}],[\"adapterforsoundscpreader\",{\"0\":{\"2331\":1},\"1\":{\"2331\":1}}],[\"adapterforsingingscorescpreader\",{\"0\":{\"2330\":1},\"1\":{\"2330\":1}}],[\"adapterforlabelscpreader\",{\"0\":{\"2329\":1},\"1\":{\"2329\":1}}],[\"adapter\",{\"0\":{\"1657\":2,\"1675\":1,\"1681\":2,\"1682\":2,\"1683\":2,\"1684\":1,\"1685\":1,\"1688\":1,\"1695\":1},\"1\":{\"699\":3,\"1657\":2,\"1675\":1,\"1681\":9,\"1682\":2,\"1683\":3,\"1684\":2,\"1685\":1,\"1688\":1,\"1695\":1,\"2348\":2,\"2370\":4,\"2372\":2}}],[\"adapted\",{\"1\":{\"267\":4,\"1051\":1,\"1086\":1,\"1207\":1,\"1211\":1,\"1242\":1,\"1385\":1,\"1668\":1,\"1681\":1,\"1683\":2,\"2018\":1,\"2198\":1,\"2354\":1}}],[\"adapt\",{\"0\":{\"1086\":1,\"1207\":1,\"1335\":1,\"1336\":1,\"1340\":2},\"1\":{\"232\":1,\"258\":1,\"1086\":1,\"1207\":1,\"1268\":9,\"1274\":10,\"1335\":1,\"1336\":1,\"1340\":2}}],[\"adaptive\",{\"1\":{\"45\":2,\"139\":1,\"145\":2,\"449\":2,\"616\":2,\"696\":2,\"697\":2}}],[\"adamw\",{\"1\":{\"243\":1}}],[\"adam\",{\"1\":{\"84\":3}}],[\"addfile\",{\"1\":{\"1948\":1}}],[\"adddeltas\",{\"0\":{\"1703\":1},\"1\":{\"1703\":1,\"1876\":1}}],[\"adds\",{\"1\":{\"1350\":1,\"2039\":1,\"2143\":1,\"2307\":1}}],[\"add=none\",{\"1\":{\"715\":1,\"783\":1}}],[\"addjson\",{\"0\":{\"538\":1},\"1\":{\"538\":1}}],[\"addr=<rank0\",{\"1\":{\"60\":1}}],[\"addr\",{\"0\":{\"2382\":1},\"1\":{\"58\":2,\"60\":1,\"61\":3,\"374\":2,\"377\":2,\"449\":2,\"2340\":2,\"2382\":1}}],[\"address\",{\"1\":{\"3\":1,\"39\":1,\"269\":1,\"278\":1,\"1645\":1,\"1883\":1}}],[\"added\",{\"1\":{\"32\":1,\"131\":1,\"145\":1,\"150\":1,\"223\":1,\"615\":1,\"640\":1,\"648\":1,\"666\":1,\"760\":2,\"1181\":1,\"1246\":1,\"1350\":1,\"1484\":1,\"1524\":1,\"1539\":1,\"1949\":1,\"2039\":1,\"2148\":1,\"2162\":1,\"2168\":1,\"2283\":1,\"2284\":1}}],[\"additive\",{\"1\":{\"1704\":2,\"1713\":2,\"2167\":2,\"2176\":1}}],[\"addition=true\",{\"1\":{\"1207\":1}}],[\"addition\",{\"1\":{\"22\":1,\"39\":1,\"44\":1,\"138\":1,\"141\":1,\"145\":1,\"163\":1,\"200\":1,\"225\":1,\"287\":2}}],[\"additionally\",{\"1\":{\"44\":1,\"50\":1,\"200\":2,\"217\":1,\"266\":1,\"267\":1,\"275\":1,\"276\":1,\"285\":2,\"286\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"733\":1,\"744\":1,\"828\":1,\"829\":1,\"830\":1,\"859\":1,\"1062\":1}}],[\"additional\",{\"0\":{\"4\":1,\"46\":1},\"1\":{\"45\":1,\"145\":1,\"150\":2,\"156\":1,\"200\":2,\"205\":1,\"217\":2,\"223\":1,\"242\":1,\"261\":1,\"262\":1,\"266\":1,\"275\":1,\"285\":2,\"286\":3,\"616\":1,\"692\":2,\"696\":1,\"697\":1,\"709\":2,\"710\":2,\"711\":2,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"733\":1,\"741\":1,\"744\":1,\"774\":2,\"780\":2,\"784\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":1,\"849\":2,\"859\":1,\"912\":1,\"1040\":1,\"1044\":1,\"1050\":1,\"1053\":2,\"1062\":2,\"1107\":4,\"1116\":1,\"1117\":2,\"1118\":2,\"1125\":1,\"1130\":1,\"1131\":2,\"1136\":2,\"1141\":2,\"1157\":1,\"1158\":1,\"1161\":1,\"1162\":2,\"1181\":1,\"1189\":1,\"1217\":2,\"1218\":1,\"1221\":1,\"1229\":1,\"1232\":2,\"1244\":1,\"1252\":2,\"1261\":2,\"1267\":2,\"1268\":3,\"1269\":3,\"1270\":3,\"1271\":3,\"1278\":4,\"1280\":3,\"1283\":3,\"1334\":3,\"1422\":1,\"1511\":1,\"1513\":3,\"1526\":1,\"1546\":1,\"1548\":3,\"1551\":3,\"1592\":3,\"1596\":2,\"1598\":1,\"1599\":3,\"1600\":1,\"1614\":3,\"1622\":1,\"1626\":1,\"1638\":1,\"1644\":1,\"1735\":2,\"1751\":2,\"1759\":2,\"1992\":2,\"1995\":2,\"2129\":2,\"2140\":1,\"2176\":1,\"2347\":1,\"2355\":2,\"2371\":1}}],[\"adding\",{\"1\":{\"26\":1,\"49\":1,\"150\":1,\"200\":1,\"208\":1,\"223\":1,\"709\":1,\"710\":1,\"711\":1,\"774\":1,\"780\":1,\"849\":1,\"1107\":1,\"1278\":1,\"1680\":1,\"1784\":1,\"1957\":1,\"1960\":1,\"1961\":1,\"2129\":1,\"2191\":1,\"2216\":1,\"2474\":1}}],[\"add\",{\"0\":{\"150\":1,\"868\":1,\"1350\":1,\"1703\":1,\"1857\":2,\"1858\":2,\"1875\":1,\"1907\":1,\"1963\":1,\"2307\":2},\"1\":{\"22\":1,\"23\":2,\"25\":1,\"32\":1,\"43\":3,\"78\":2,\"141\":1,\"150\":4,\"168\":3,\"191\":1,\"214\":1,\"222\":1,\"225\":7,\"259\":1,\"269\":3,\"278\":3,\"284\":3,\"286\":1,\"287\":2,\"290\":10,\"372\":2,\"481\":4,\"511\":2,\"538\":1,\"620\":1,\"699\":4,\"715\":1,\"760\":1,\"783\":1,\"787\":2,\"802\":1,\"803\":1,\"804\":1,\"819\":1,\"868\":1,\"932\":1,\"934\":1,\"991\":1,\"1029\":1,\"1064\":1,\"1235\":1,\"1262\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1290\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1318\":1,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1330\":1,\"1334\":1,\"1350\":1,\"1513\":1,\"1526\":1,\"1548\":1,\"1551\":1,\"1555\":1,\"1592\":1,\"1596\":1,\"1597\":1,\"1598\":1,\"1599\":2,\"1600\":1,\"1604\":1,\"1605\":1,\"1606\":1,\"1614\":1,\"1615\":1,\"1619\":1,\"1644\":1,\"1703\":2,\"1736\":1,\"1784\":1,\"1802\":1,\"1806\":4,\"1808\":1,\"1818\":1,\"1820\":1,\"1837\":1,\"1844\":2,\"1857\":2,\"1858\":3,\"1875\":1,\"1876\":1,\"1907\":2,\"1948\":1,\"1963\":1,\"1993\":2,\"1994\":1,\"2235\":3,\"2236\":3,\"2239\":1,\"2240\":3,\"2246\":1,\"2247\":2,\"2248\":1,\"2249\":2,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":2,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2307\":2,\"2334\":2,\"2347\":2,\"2355\":3,\"2359\":1,\"2367\":1,\"2369\":2,\"2371\":2,\"2411\":1,\"2412\":1,\"2423\":1,\"2432\":1,\"2447\":1,\"2479\":1,\"2485\":1,\"2493\":1,\"2500\":1,\"2505\":1}}],[\"arcmarginproduct\",{\"0\":{\"2176\":1},\"1\":{\"2176\":1}}],[\"arcface\",{\"1\":{\"2167\":1,\"2176\":4}}],[\"arcname=none\",{\"1\":{\"1948\":1}}],[\"archs\",{\"1\":{\"738\":1}}],[\"arch=\",{\"1\":{\"168\":2}}],[\"arch\",{\"1\":{\"43\":4,\"781\":1,\"783\":1,\"1749\":3,\"1892\":3}}],[\"architectures\",{\"1\":{\"43\":1,\"102\":1,\"260\":1,\"261\":1,\"262\":1,\"939\":1}}],[\"architecture\",{\"0\":{\"43\":1,\"140\":1,\"671\":1},\"1\":{\"43\":10,\"140\":1,\"141\":3,\"225\":1,\"262\":1,\"625\":1,\"630\":2,\"636\":4,\"642\":1,\"649\":2,\"655\":1,\"671\":2,\"672\":1,\"760\":2,\"1119\":1,\"1271\":1,\"1546\":1,\"1610\":1,\"1622\":1,\"1626\":1,\"1729\":1,\"1730\":1,\"1749\":1,\"1866\":1,\"1880\":1,\"2131\":3,\"2148\":3,\"2184\":1,\"2187\":1,\"2192\":1,\"2203\":1,\"2240\":1,\"2447\":1}}],[\"archiver\",{\"0\":{\"1948\":1},\"1\":{\"1948\":1}}],[\"archiveprefix=\",{\"1\":{\"202\":1}}],[\"archive\",{\"1\":{\"26\":1,\"402\":1,\"1061\":1,\"1062\":1,\"1130\":1,\"1131\":1,\"1172\":1,\"1955\":2,\"2168\":1}}],[\"arange\",{\"1\":{\"1906\":1}}],[\"arabic\",{\"1\":{\"201\":1,\"287\":1,\"481\":1}}],[\"ar\",{\"0\":{\"308\":1,\"428\":1,\"795\":1,\"1806\":1,\"1807\":1},\"1\":{\"301\":2,\"421\":2,\"692\":2,\"795\":2,\"1751\":1,\"1806\":1,\"1807\":1}}],[\"around\",{\"1\":{\"286\":2,\"821\":1,\"1225\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1493\":1,\"1494\":1,\"2134\":1}}],[\"arora2023universlu\",{\"1\":{\"202\":1}}],[\"arora2021espnet\",{\"1\":{\"12\":1}}],[\"arora\",{\"1\":{\"5\":1,\"6\":2,\"12\":1,\"15\":1,\"202\":1,\"244\":1}}],[\"articulations\",{\"1\":{\"269\":2,\"278\":2}}],[\"article\",{\"1\":{\"9\":1,\"207\":1,\"244\":1,\"2168\":1}}],[\"art\",{\"1\":{\"190\":3}}],[\"arrays\",{\"1\":{\"570\":1,\"994\":6,\"1717\":1}}],[\"array\",{\"1\":{\"167\":1,\"792\":2,\"987\":1,\"988\":4,\"989\":1,\"990\":4,\"994\":1,\"997\":1,\"999\":2,\"1008\":3,\"1010\":4,\"1012\":1,\"1020\":2,\"1025\":1,\"1028\":1,\"1264\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1319\":1,\"1334\":1,\"1760\":2,\"1774\":1,\"1824\":3,\"1827\":1,\"1830\":1,\"1861\":2,\"1881\":1,\"1882\":4,\"1883\":2,\"2040\":1,\"2043\":3,\"2044\":1,\"2045\":1,\"2054\":3,\"2055\":3,\"2056\":3,\"2065\":2,\"2066\":3,\"2101\":1,\"2133\":6,\"2136\":5,\"2137\":2,\"2139\":1,\"2143\":1,\"2155\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2343\":2,\"2352\":2,\"2376\":1}}],[\"arbitrary\",{\"1\":{\"81\":1,\"284\":1,\"290\":1,\"756\":1,\"773\":1,\"939\":1,\"1301\":1,\"1372\":1,\"2355\":2}}],[\"arbitrarily\",{\"1\":{\"69\":1}}],[\"ark\",{\"1\":{\"79\":1,\"80\":3,\"205\":1,\"235\":1,\"242\":1,\"243\":1,\"254\":1,\"286\":1,\"526\":1,\"527\":2,\"1774\":1,\"1827\":1,\"1830\":1,\"1846\":1,\"1881\":3,\"1882\":4,\"1883\":10,\"1890\":1,\"1912\":4}}],[\"arguements\",{\"1\":{\"2235\":1,\"2236\":1}}],[\"argumentparser\",{\"0\":{\"2474\":1},\"1\":{\"2152\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":3,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2334\":1,\"2347\":1,\"2369\":1,\"2371\":1,\"2474\":3,\"2479\":1,\"2485\":1,\"2493\":1,\"2500\":1,\"2505\":1}}],[\"argument\",{\"0\":{\"2089\":1},\"1\":{\"50\":1,\"51\":1,\"79\":1,\"81\":1,\"82\":2,\"85\":1,\"131\":1,\"168\":1,\"224\":3,\"290\":1,\"756\":4,\"773\":4,\"818\":1,\"846\":2,\"866\":2,\"867\":2,\"1156\":1,\"1209\":1,\"1224\":1,\"1225\":1,\"1228\":1,\"1735\":2,\"1751\":3,\"1759\":2,\"1892\":1,\"2152\":1,\"2220\":2,\"2327\":2,\"2355\":1,\"2474\":1,\"2479\":1,\"2485\":1,\"2493\":1,\"2500\":1,\"2505\":1}}],[\"arguments\",{\"0\":{\"117\":1,\"294\":1,\"296\":1,\"300\":1,\"302\":1,\"310\":1,\"316\":1,\"322\":1,\"328\":1,\"332\":1,\"336\":1,\"343\":1,\"350\":1,\"357\":1,\"362\":1,\"369\":1,\"373\":1,\"375\":1,\"376\":1,\"378\":1,\"386\":1,\"390\":1,\"397\":1,\"405\":1,\"407\":1,\"416\":1,\"420\":1,\"422\":1,\"430\":1,\"437\":1,\"443\":1,\"450\":1,\"458\":1,\"462\":1,\"464\":1,\"470\":1,\"476\":1,\"482\":1,\"485\":1,\"491\":1,\"497\":1,\"499\":1,\"506\":1,\"512\":1,\"539\":1,\"540\":1,\"542\":1,\"544\":1,\"546\":1,\"547\":1,\"549\":1,\"550\":1,\"552\":1,\"553\":1,\"555\":1,\"557\":1,\"559\":1,\"560\":1,\"562\":1,\"563\":1,\"565\":1,\"566\":1,\"568\":1,\"569\":1,\"571\":1,\"573\":1,\"574\":1,\"576\":1,\"577\":1,\"579\":1,\"580\":1,\"582\":1,\"584\":1,\"585\":1,\"587\":1,\"588\":1,\"590\":1,\"591\":1,\"593\":1,\"595\":1,\"597\":1,\"599\":1,\"601\":1,\"602\":1,\"604\":1,\"605\":1,\"607\":1,\"608\":1,\"610\":1,\"612\":1,\"613\":1,\"672\":1,\"819\":1,\"1937\":1},\"1\":{\"22\":5,\"23\":1,\"24\":2,\"40\":1,\"46\":2,\"47\":2,\"78\":2,\"79\":1,\"81\":1,\"84\":1,\"117\":1,\"119\":1,\"128\":1,\"134\":1,\"142\":2,\"150\":1,\"167\":1,\"223\":4,\"224\":1,\"235\":1,\"243\":2,\"403\":14,\"617\":1,\"618\":1,\"619\":1,\"622\":1,\"624\":1,\"625\":1,\"628\":1,\"634\":2,\"636\":1,\"642\":1,\"643\":1,\"666\":2,\"672\":2,\"756\":1,\"773\":1,\"911\":1,\"912\":1,\"1050\":1,\"1116\":1,\"1161\":1,\"1189\":1,\"1218\":1,\"1221\":1,\"1229\":1,\"1244\":1,\"1368\":4,\"1397\":1,\"1582\":1,\"1624\":1,\"1645\":1,\"1655\":1,\"1662\":1,\"1844\":2,\"1860\":2,\"1878\":2,\"1885\":1,\"1887\":1,\"1914\":1,\"1937\":2,\"1962\":2,\"2140\":1,\"2246\":1,\"2247\":6,\"2248\":1,\"2249\":2,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":2,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2334\":2,\"2347\":2,\"2369\":2,\"2371\":2}}],[\"argmax\",{\"1\":{\"211\":1,\"706\":3,\"1854\":1,\"2355\":2}}],[\"arg2\",{\"1\":{\"119\":1}}],[\"argparse\",{\"0\":{\"2474\":1},\"1\":{\"78\":1,\"82\":1,\"2249\":3,\"2325\":1,\"2327\":1,\"2334\":2,\"2474\":1,\"2479\":1,\"2485\":2,\"2493\":2,\"2500\":2,\"2505\":2}}],[\"argparse2rst\",{\"1\":{\"3\":1}}],[\"argscomplexmultiplicationwrapper\",{\"0\":{\"1051\":1},\"1\":{\"1051\":1}}],[\"args=none\",{\"1\":{\"2474\":1}}],[\"args=\",{\"1\":{\"821\":1}}],[\"args\",{\"0\":{\"1341\":1,\"1368\":1,\"1887\":1},\"1\":{\"78\":3,\"82\":3,\"119\":3,\"124\":2,\"139\":2,\"142\":2,\"200\":2,\"205\":2,\"211\":1,\"217\":3,\"223\":3,\"235\":1,\"242\":2,\"243\":2,\"254\":1,\"266\":2,\"267\":2,\"275\":2,\"276\":2,\"285\":2,\"286\":6,\"374\":2,\"614\":1,\"617\":2,\"618\":2,\"619\":2,\"622\":2,\"624\":2,\"625\":2,\"634\":4,\"636\":2,\"642\":2,\"643\":2,\"654\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"718\":1,\"744\":1,\"747\":1,\"751\":1,\"756\":3,\"757\":1,\"773\":3,\"784\":1,\"793\":1,\"794\":1,\"795\":1,\"817\":1,\"819\":1,\"821\":2,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"835\":1,\"846\":1,\"866\":1,\"867\":1,\"877\":1,\"912\":2,\"938\":1,\"950\":1,\"952\":1,\"956\":1,\"962\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1050\":2,\"1051\":4,\"1053\":1,\"1116\":2,\"1161\":2,\"1163\":1,\"1189\":2,\"1218\":3,\"1221\":3,\"1222\":1,\"1226\":1,\"1229\":3,\"1244\":2,\"1245\":4,\"1294\":1,\"1333\":1,\"1341\":3,\"1350\":1,\"1359\":1,\"1368\":1,\"1381\":4,\"1390\":1,\"1397\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1471\":1,\"1472\":1,\"1492\":1,\"1508\":2,\"1576\":2,\"1578\":1,\"1580\":1,\"1588\":2,\"1601\":1,\"1602\":1,\"1603\":1,\"1645\":2,\"1652\":1,\"1720\":1,\"1763\":1,\"1778\":2,\"1796\":2,\"1797\":1,\"1806\":1,\"1819\":1,\"1823\":1,\"1834\":2,\"1838\":1,\"1860\":2,\"1878\":2,\"1885\":1,\"1887\":1,\"1892\":3,\"1938\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1987\":1,\"1990\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2043\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2124\":1,\"2172\":1,\"2174\":1,\"2215\":1,\"2222\":1,\"2246\":4,\"2247\":6,\"2248\":4,\"2249\":20,\"2250\":4,\"2251\":4,\"2252\":4,\"2253\":5,\"2254\":5,\"2255\":5,\"2256\":5,\"2257\":4,\"2259\":4,\"2260\":4,\"2261\":4,\"2262\":3,\"2263\":4,\"2264\":4,\"2265\":2,\"2266\":4,\"2267\":4,\"2268\":4,\"2269\":4,\"2270\":4,\"2271\":4,\"2272\":4,\"2273\":5,\"2305\":1,\"2325\":2,\"2327\":2,\"2334\":6,\"2338\":1,\"2347\":1,\"2355\":1,\"2369\":1,\"2371\":1,\"2395\":1,\"2401\":1,\"2403\":1,\"2407\":1,\"2420\":1,\"2443\":1,\"2445\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2462\":1,\"2472\":1,\"2474\":2,\"2479\":4,\"2483\":2,\"2485\":4,\"2493\":4,\"2500\":1,\"2505\":4}}],[\"arg\",{\"1\":{\"24\":2,\"119\":1,\"545\":1}}],[\"arxiv\",{\"1\":{\"9\":2,\"202\":1,\"207\":2,\"244\":2,\"616\":2,\"617\":1,\"624\":1,\"629\":1,\"634\":1,\"635\":1,\"640\":1,\"643\":1,\"648\":1,\"652\":3,\"696\":4,\"697\":3,\"699\":1,\"711\":1,\"768\":1,\"780\":1,\"785\":1,\"786\":1,\"866\":2,\"882\":1,\"883\":1,\"884\":1,\"921\":2,\"922\":2,\"974\":3,\"1066\":1,\"1170\":1,\"1252\":1,\"1269\":4,\"1385\":1,\"1396\":1,\"1411\":1,\"1439\":1,\"1515\":1,\"1530\":1,\"1541\":1,\"1577\":1,\"1590\":1,\"1668\":1,\"1683\":1,\"1705\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1720\":1,\"1721\":1,\"1731\":2,\"1748\":1,\"1768\":1,\"1784\":2,\"1785\":1,\"1786\":1,\"1817\":1,\"1818\":1,\"1820\":1,\"1967\":1,\"1977\":1,\"1994\":1,\"2000\":1,\"2001\":1,\"2016\":1,\"2167\":1,\"2176\":1,\"2183\":1,\"2223\":2,\"2227\":1,\"2231\":1,\"2239\":1,\"2245\":1,\"2428\":1}}],[\"area\",{\"1\":{\"1558\":1}}],[\"arena\",{\"1\":{\"247\":1}}],[\"are\",{\"1\":{\"3\":5,\"22\":2,\"24\":1,\"25\":1,\"26\":3,\"39\":1,\"43\":2,\"44\":1,\"45\":4,\"46\":2,\"47\":2,\"52\":1,\"57\":1,\"60\":2,\"66\":1,\"67\":1,\"69\":1,\"70\":3,\"71\":1,\"76\":1,\"80\":1,\"81\":1,\"82\":2,\"84\":1,\"85\":1,\"86\":1,\"91\":1,\"94\":2,\"95\":1,\"97\":1,\"102\":2,\"104\":1,\"107\":1,\"110\":1,\"118\":2,\"128\":1,\"135\":1,\"137\":1,\"138\":1,\"139\":1,\"141\":1,\"142\":3,\"144\":1,\"145\":4,\"148\":2,\"150\":1,\"152\":2,\"159\":1,\"162\":3,\"163\":1,\"165\":1,\"167\":1,\"175\":1,\"195\":1,\"197\":2,\"200\":6,\"203\":1,\"205\":3,\"211\":1,\"213\":1,\"223\":6,\"224\":1,\"225\":1,\"226\":1,\"228\":2,\"232\":2,\"242\":5,\"243\":2,\"247\":1,\"254\":1,\"258\":1,\"259\":1,\"261\":1,\"262\":5,\"263\":1,\"266\":2,\"267\":5,\"269\":5,\"273\":1,\"275\":3,\"276\":1,\"278\":5,\"285\":2,\"286\":4,\"290\":3,\"536\":1,\"616\":1,\"691\":1,\"696\":1,\"699\":1,\"756\":4,\"759\":1,\"773\":4,\"787\":2,\"819\":2,\"821\":4,\"831\":1,\"846\":2,\"927\":1,\"1008\":3,\"1011\":1,\"1029\":1,\"1051\":1,\"1064\":1,\"1072\":1,\"1074\":1,\"1078\":1,\"1086\":1,\"1117\":1,\"1119\":2,\"1130\":1,\"1131\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1137\":1,\"1139\":1,\"1141\":1,\"1153\":1,\"1180\":1,\"1181\":1,\"1202\":2,\"1207\":1,\"1208\":1,\"1232\":1,\"1245\":1,\"1246\":1,\"1252\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1262\":1,\"1279\":2,\"1280\":2,\"1281\":2,\"1283\":2,\"1290\":1,\"1306\":2,\"1354\":1,\"1371\":2,\"1390\":1,\"1396\":1,\"1402\":1,\"1409\":1,\"1444\":1,\"1448\":1,\"1467\":1,\"1484\":2,\"1595\":1,\"1597\":1,\"1608\":1,\"1647\":1,\"1655\":3,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1678\":2,\"1719\":3,\"1725\":5,\"1753\":2,\"1794\":1,\"1806\":1,\"1833\":1,\"1863\":1,\"1919\":2,\"1920\":1,\"1934\":1,\"1936\":1,\"1937\":1,\"2000\":1,\"2001\":1,\"2007\":1,\"2044\":1,\"2054\":1,\"2130\":3,\"2134\":2,\"2136\":4,\"2143\":1,\"2145\":1,\"2146\":1,\"2147\":2,\"2162\":1,\"2232\":1,\"2235\":1,\"2236\":1,\"2238\":1,\"2249\":3,\"2253\":3,\"2312\":1,\"2325\":1,\"2345\":1,\"2354\":1,\"2355\":3,\"2377\":2,\"2380\":1}}],[\"anchor\",{\"1\":{\"2480\":1,\"2507\":1}}],[\"anchors\",{\"1\":{\"2480\":2}}],[\"angular\",{\"1\":{\"2167\":4,\"2176\":3}}],[\"angles\",{\"1\":{\"1131\":1,\"1172\":1}}],[\"annealing\",{\"1\":{\"2014\":1}}],[\"anneal\",{\"0\":{\"2014\":1},\"1\":{\"2014\":1}}],[\"annealed\",{\"1\":{\"1050\":1}}],[\"annealedlangevindynamics\",{\"0\":{\"1050\":1},\"1\":{\"1050\":1}}],[\"annotation\",{\"1\":{\"265\":3,\"269\":6,\"274\":3,\"278\":6}}],[\"announcement\",{\"1\":{\"106\":1}}],[\"anomaly\",{\"1\":{\"377\":2,\"449\":2}}],[\"another\",{\"1\":{\"84\":1,\"147\":1,\"252\":1,\"994\":5,\"2369\":1}}],[\"anecdotally\",{\"1\":{\"262\":1}}],[\"analytic\",{\"0\":{\"1293\":1},\"1\":{\"691\":1,\"1293\":2}}],[\"analyze\",{\"1\":{\"247\":1,\"2482\":2,\"2495\":2}}],[\"analysis\",{\"1\":{\"233\":1,\"1170\":1,\"1524\":1,\"1608\":2}}],[\"an4\",{\"1\":{\"23\":1,\"25\":1,\"37\":2,\"38\":2,\"39\":1,\"107\":2,\"108\":5,\"109\":1,\"196\":2,\"197\":1}}],[\"anyone\",{\"1\":{\"242\":1}}],[\"anything\",{\"1\":{\"110\":1,\"168\":2,\"2355\":1}}],[\"any\",{\"0\":{\"2375\":1},\"1\":{\"3\":1,\"18\":1,\"19\":1,\"22\":1,\"24\":1,\"42\":1,\"47\":1,\"66\":1,\"69\":1,\"70\":1,\"96\":1,\"108\":1,\"128\":1,\"138\":1,\"165\":1,\"167\":1,\"197\":1,\"200\":2,\"211\":1,\"242\":4,\"243\":2,\"267\":1,\"286\":1,\"614\":1,\"626\":3,\"628\":2,\"631\":4,\"651\":2,\"655\":2,\"656\":2,\"657\":2,\"658\":1,\"659\":2,\"661\":1,\"662\":1,\"671\":2,\"672\":1,\"673\":2,\"692\":6,\"696\":1,\"697\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"743\":2,\"744\":1,\"752\":1,\"756\":11,\"760\":5,\"763\":4,\"770\":1,\"773\":11,\"790\":4,\"797\":1,\"820\":5,\"828\":1,\"829\":1,\"830\":1,\"847\":2,\"850\":4,\"859\":1,\"930\":1,\"959\":1,\"1145\":1,\"1264\":1,\"1265\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1,\"1381\":1,\"1385\":1,\"1389\":9,\"1390\":1,\"1391\":2,\"1395\":3,\"1396\":6,\"1397\":3,\"1400\":1,\"1401\":9,\"1402\":7,\"1403\":2,\"1408\":9,\"1409\":8,\"1420\":2,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1441\":2,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":9,\"1467\":6,\"1468\":2,\"1469\":1,\"1480\":3,\"1502\":1,\"1509\":3,\"1511\":4,\"1513\":2,\"1521\":2,\"1526\":18,\"1549\":8,\"1551\":2,\"1553\":14,\"1582\":2,\"1585\":2,\"1592\":2,\"1593\":2,\"1594\":4,\"1595\":3,\"1596\":2,\"1597\":2,\"1598\":14,\"1599\":2,\"1600\":18,\"1604\":4,\"1605\":4,\"1606\":6,\"1609\":2,\"1610\":2,\"1614\":2,\"1615\":4,\"1618\":2,\"1619\":2,\"1624\":2,\"1625\":14,\"1643\":1,\"1645\":1,\"1646\":1,\"1650\":1,\"1683\":1,\"1719\":7,\"1720\":2,\"1723\":4,\"1724\":5,\"1725\":7,\"1726\":8,\"1727\":8,\"1731\":2,\"1732\":4,\"1742\":1,\"1749\":2,\"1762\":2,\"1775\":4,\"1787\":5,\"1798\":1,\"1799\":1,\"1800\":1,\"1805\":3,\"1806\":2,\"1807\":2,\"1815\":2,\"1822\":7,\"1843\":2,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1871\":4,\"1883\":1,\"1913\":1,\"1914\":2,\"1921\":4,\"1937\":1,\"1944\":7,\"1945\":2,\"1947\":7,\"1973\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1992\":4,\"2001\":1,\"2006\":1,\"2039\":1,\"2130\":6,\"2131\":5,\"2142\":1,\"2155\":1,\"2232\":1,\"2238\":1,\"2249\":2,\"2327\":1,\"2355\":4,\"2375\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2411\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2443\":1,\"2449\":1,\"2474\":1}}],[\"an\",{\"1\":{\"3\":1,\"6\":1,\"13\":1,\"26\":2,\"36\":1,\"38\":2,\"42\":1,\"43\":1,\"46\":2,\"68\":1,\"79\":3,\"81\":3,\"82\":1,\"84\":1,\"85\":1,\"91\":2,\"111\":1,\"126\":1,\"128\":1,\"130\":1,\"138\":1,\"143\":1,\"153\":1,\"162\":1,\"168\":2,\"184\":1,\"190\":1,\"200\":1,\"223\":1,\"225\":1,\"242\":2,\"243\":2,\"244\":1,\"245\":1,\"247\":3,\"254\":1,\"262\":15,\"284\":1,\"287\":1,\"290\":6,\"538\":1,\"619\":1,\"625\":1,\"626\":1,\"627\":1,\"637\":1,\"638\":1,\"639\":1,\"644\":1,\"663\":1,\"699\":1,\"703\":1,\"724\":3,\"725\":3,\"726\":1,\"727\":1,\"728\":3,\"729\":2,\"740\":1,\"744\":3,\"755\":2,\"756\":2,\"764\":1,\"768\":2,\"773\":2,\"784\":3,\"785\":3,\"797\":1,\"801\":1,\"824\":1,\"828\":3,\"829\":5,\"830\":3,\"859\":1,\"866\":2,\"867\":2,\"943\":1,\"1002\":1,\"1124\":1,\"1167\":1,\"1204\":1,\"1209\":1,\"1218\":1,\"1221\":1,\"1224\":2,\"1225\":2,\"1228\":1,\"1245\":1,\"1246\":1,\"1252\":1,\"1350\":1,\"1400\":1,\"1441\":1,\"1469\":1,\"1484\":1,\"1485\":1,\"1599\":1,\"1637\":1,\"1668\":1,\"1711\":1,\"1712\":1,\"1719\":1,\"1724\":1,\"1725\":1,\"1729\":1,\"1731\":2,\"1735\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1751\":1,\"1759\":1,\"1760\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1794\":1,\"1806\":1,\"1808\":1,\"1809\":1,\"1817\":1,\"1818\":1,\"1822\":1,\"1837\":1,\"1838\":1,\"1847\":1,\"1848\":2,\"1860\":1,\"1878\":1,\"1885\":1,\"1966\":1,\"2040\":1,\"2044\":1,\"2045\":1,\"2054\":1,\"2065\":2,\"2101\":2,\"2151\":1,\"2215\":1,\"2216\":1,\"2235\":1,\"2236\":1,\"2240\":1,\"2262\":1,\"2310\":1,\"2327\":1,\"2355\":4,\"2376\":1,\"2427\":1,\"2474\":1}}],[\"andreas\",{\"1\":{\"202\":1}}],[\"and\",{\"0\":{\"53\":1,\"58\":1,\"59\":1,\"61\":1,\"62\":1,\"64\":1,\"77\":1,\"81\":1,\"94\":1,\"104\":1,\"118\":1,\"121\":1,\"123\":1,\"127\":1,\"133\":1,\"160\":1,\"175\":1,\"208\":1,\"1350\":1,\"1378\":1,\"1380\":1,\"1891\":1,\"1952\":1},\"1\":{\"0\":1,\"1\":1,\"3\":11,\"5\":10,\"6\":27,\"7\":5,\"8\":8,\"9\":18,\"10\":21,\"11\":24,\"12\":12,\"13\":11,\"14\":6,\"15\":10,\"16\":4,\"18\":1,\"19\":4,\"22\":3,\"24\":5,\"26\":3,\"27\":1,\"28\":1,\"33\":1,\"37\":1,\"38\":3,\"39\":3,\"40\":1,\"41\":1,\"43\":9,\"44\":2,\"45\":9,\"46\":2,\"47\":7,\"48\":3,\"49\":1,\"50\":4,\"51\":1,\"52\":4,\"56\":1,\"59\":4,\"60\":1,\"66\":1,\"68\":2,\"69\":1,\"70\":2,\"71\":2,\"73\":1,\"74\":1,\"76\":2,\"78\":3,\"79\":18,\"80\":1,\"81\":15,\"82\":9,\"84\":1,\"86\":1,\"87\":1,\"88\":2,\"91\":3,\"92\":2,\"94\":3,\"96\":13,\"97\":12,\"98\":18,\"99\":8,\"100\":8,\"101\":4,\"102\":3,\"104\":1,\"106\":2,\"107\":1,\"108\":2,\"110\":2,\"111\":1,\"113\":1,\"118\":5,\"119\":1,\"124\":1,\"126\":1,\"127\":1,\"128\":3,\"133\":5,\"134\":4,\"135\":1,\"136\":1,\"138\":3,\"139\":3,\"140\":2,\"141\":7,\"142\":1,\"143\":1,\"144\":2,\"145\":4,\"146\":2,\"147\":4,\"150\":4,\"156\":11,\"160\":1,\"161\":1,\"162\":5,\"165\":1,\"166\":1,\"167\":2,\"168\":1,\"173\":1,\"174\":2,\"175\":10,\"185\":1,\"190\":1,\"194\":1,\"196\":6,\"197\":7,\"200\":12,\"201\":5,\"202\":18,\"204\":1,\"205\":8,\"206\":1,\"207\":6,\"208\":2,\"211\":15,\"212\":3,\"213\":2,\"217\":11,\"218\":3,\"220\":2,\"223\":6,\"224\":6,\"225\":10,\"226\":1,\"228\":5,\"232\":4,\"233\":1,\"234\":2,\"235\":10,\"236\":1,\"242\":25,\"243\":10,\"244\":11,\"245\":3,\"246\":8,\"247\":5,\"254\":7,\"255\":1,\"256\":5,\"258\":4,\"259\":1,\"260\":1,\"261\":1,\"262\":5,\"263\":5,\"265\":1,\"266\":15,\"267\":22,\"268\":7,\"269\":10,\"274\":1,\"275\":17,\"276\":18,\"277\":7,\"278\":10,\"285\":17,\"286\":28,\"287\":2,\"288\":2,\"289\":2,\"290\":11,\"301\":4,\"309\":4,\"315\":4,\"321\":4,\"327\":4,\"331\":4,\"335\":4,\"342\":4,\"349\":4,\"361\":4,\"368\":4,\"377\":4,\"385\":4,\"389\":4,\"396\":4,\"404\":4,\"406\":4,\"421\":4,\"429\":4,\"436\":4,\"442\":4,\"449\":4,\"457\":4,\"463\":4,\"469\":4,\"475\":4,\"484\":4,\"490\":4,\"496\":4,\"498\":4,\"505\":4,\"516\":1,\"520\":1,\"523\":1,\"524\":1,\"525\":1,\"536\":2,\"606\":1,\"614\":1,\"616\":2,\"617\":1,\"618\":1,\"624\":1,\"625\":4,\"627\":1,\"628\":1,\"629\":1,\"632\":1,\"633\":1,\"634\":1,\"635\":1,\"636\":2,\"644\":1,\"650\":2,\"652\":1,\"665\":1,\"666\":1,\"675\":3,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"691\":1,\"692\":5,\"693\":1,\"696\":2,\"697\":2,\"699\":3,\"700\":1,\"701\":2,\"702\":1,\"703\":5,\"704\":2,\"706\":3,\"709\":2,\"710\":7,\"711\":7,\"712\":1,\"713\":1,\"715\":1,\"718\":1,\"720\":1,\"724\":2,\"725\":2,\"726\":2,\"727\":2,\"728\":2,\"729\":1,\"731\":1,\"732\":1,\"733\":2,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"740\":1,\"741\":1,\"743\":1,\"744\":2,\"745\":2,\"746\":2,\"747\":3,\"748\":2,\"749\":2,\"751\":1,\"752\":1,\"754\":1,\"755\":5,\"756\":4,\"757\":1,\"759\":2,\"760\":2,\"766\":1,\"767\":1,\"768\":4,\"769\":1,\"771\":2,\"773\":4,\"774\":2,\"775\":1,\"777\":3,\"778\":1,\"780\":2,\"781\":1,\"783\":1,\"784\":2,\"785\":5,\"786\":2,\"787\":2,\"788\":1,\"790\":3,\"791\":1,\"793\":2,\"794\":2,\"796\":1,\"797\":1,\"798\":1,\"800\":2,\"803\":1,\"805\":1,\"807\":1,\"809\":2,\"811\":1,\"813\":1,\"815\":1,\"818\":1,\"819\":2,\"820\":4,\"821\":5,\"823\":1,\"824\":2,\"825\":1,\"828\":3,\"829\":3,\"830\":2,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":4,\"847\":1,\"848\":1,\"849\":3,\"850\":3,\"851\":1,\"852\":2,\"854\":1,\"856\":1,\"859\":2,\"860\":1,\"862\":1,\"864\":1,\"866\":2,\"867\":2,\"911\":1,\"912\":1,\"919\":1,\"921\":1,\"922\":1,\"926\":1,\"927\":3,\"935\":1,\"943\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":2,\"960\":3,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":2,\"975\":1,\"976\":1,\"977\":1,\"978\":1,\"979\":1,\"981\":1,\"984\":1,\"1000\":5,\"1008\":3,\"1029\":2,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":2,\"1053\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1061\":3,\"1062\":2,\"1063\":2,\"1064\":3,\"1065\":1,\"1066\":1,\"1068\":1,\"1070\":1,\"1071\":1,\"1072\":3,\"1074\":2,\"1075\":1,\"1076\":1,\"1078\":2,\"1084\":1,\"1086\":3,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1107\":1,\"1108\":1,\"1110\":2,\"1112\":2,\"1113\":2,\"1114\":1,\"1119\":2,\"1120\":1,\"1122\":1,\"1124\":3,\"1126\":1,\"1127\":1,\"1131\":1,\"1132\":1,\"1133\":2,\"1134\":2,\"1136\":1,\"1137\":2,\"1139\":2,\"1141\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":2,\"1155\":5,\"1156\":4,\"1157\":5,\"1158\":1,\"1159\":1,\"1162\":1,\"1163\":1,\"1164\":2,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":2,\"1173\":1,\"1174\":1,\"1175\":1,\"1177\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":2,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":3,\"1204\":1,\"1205\":1,\"1207\":3,\"1208\":1,\"1209\":3,\"1210\":2,\"1211\":2,\"1213\":1,\"1215\":1,\"1217\":1,\"1219\":1,\"1222\":1,\"1223\":1,\"1226\":1,\"1228\":1,\"1230\":1,\"1233\":1,\"1235\":2,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1245\":3,\"1246\":2,\"1247\":1,\"1248\":1,\"1250\":3,\"1251\":3,\"1252\":2,\"1253\":2,\"1255\":1,\"1257\":2,\"1259\":2,\"1261\":2,\"1262\":2,\"1264\":1,\"1265\":1,\"1269\":9,\"1270\":11,\"1271\":10,\"1272\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1278\":1,\"1279\":4,\"1280\":10,\"1281\":6,\"1282\":6,\"1283\":6,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":4,\"1296\":1,\"1301\":1,\"1306\":2,\"1308\":1,\"1309\":2,\"1311\":2,\"1316\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1327\":1,\"1330\":1,\"1333\":1,\"1334\":8,\"1350\":2,\"1354\":1,\"1371\":2,\"1372\":1,\"1381\":1,\"1383\":1,\"1387\":2,\"1389\":1,\"1390\":1,\"1392\":1,\"1395\":2,\"1396\":2,\"1398\":1,\"1400\":2,\"1401\":1,\"1402\":2,\"1404\":1,\"1406\":1,\"1408\":1,\"1409\":1,\"1411\":1,\"1417\":1,\"1419\":1,\"1424\":2,\"1426\":2,\"1428\":2,\"1430\":2,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":3,\"1442\":2,\"1444\":2,\"1446\":2,\"1448\":2,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1466\":1,\"1467\":2,\"1469\":1,\"1508\":1,\"1509\":1,\"1511\":1,\"1513\":1,\"1514\":1,\"1515\":4,\"1516\":2,\"1517\":1,\"1521\":3,\"1522\":1,\"1524\":2,\"1526\":2,\"1527\":1,\"1530\":1,\"1533\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1547\":1,\"1548\":1,\"1549\":1,\"1551\":1,\"1552\":2,\"1553\":1,\"1554\":1,\"1576\":1,\"1585\":2,\"1588\":1,\"1590\":1,\"1592\":1,\"1594\":1,\"1595\":2,\"1596\":1,\"1597\":2,\"1598\":2,\"1599\":6,\"1600\":2,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":3,\"1605\":1,\"1606\":2,\"1608\":1,\"1609\":1,\"1625\":1,\"1626\":2,\"1638\":1,\"1640\":1,\"1641\":1,\"1645\":1,\"1647\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":3,\"1657\":1,\"1660\":2,\"1662\":2,\"1664\":2,\"1665\":2,\"1667\":1,\"1668\":3,\"1669\":2,\"1670\":2,\"1671\":2,\"1678\":2,\"1686\":1,\"1694\":1,\"1698\":1,\"1700\":1,\"1702\":4,\"1704\":1,\"1705\":1,\"1706\":2,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":3,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1719\":10,\"1720\":5,\"1721\":2,\"1723\":1,\"1724\":1,\"1725\":12,\"1731\":2,\"1732\":1,\"1735\":1,\"1749\":1,\"1750\":2,\"1751\":1,\"1752\":1,\"1753\":1,\"1756\":1,\"1757\":1,\"1759\":2,\"1760\":1,\"1762\":1,\"1768\":2,\"1779\":1,\"1782\":1,\"1787\":2,\"1788\":1,\"1789\":1,\"1790\":1,\"1794\":2,\"1795\":1,\"1798\":2,\"1799\":2,\"1800\":2,\"1804\":1,\"1805\":1,\"1806\":9,\"1822\":1,\"1824\":1,\"1833\":2,\"1838\":1,\"1858\":1,\"1868\":1,\"1870\":1,\"1872\":1,\"1876\":1,\"1881\":1,\"1883\":1,\"1891\":2,\"1892\":1,\"1902\":1,\"1904\":1,\"1907\":2,\"1919\":1,\"1920\":1,\"1932\":1,\"1937\":1,\"1938\":1,\"1940\":3,\"1942\":3,\"1944\":3,\"1945\":2,\"1946\":1,\"1947\":3,\"1951\":1,\"1952\":1,\"1955\":1,\"1957\":1,\"1959\":1,\"1963\":1,\"1965\":1,\"1966\":1,\"1967\":1,\"1969\":1,\"1971\":2,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1992\":5,\"1993\":2,\"1994\":1,\"1995\":3,\"1996\":2,\"1997\":1,\"2000\":4,\"2007\":1,\"2018\":1,\"2020\":1,\"2021\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2039\":2,\"2040\":2,\"2043\":1,\"2044\":9,\"2045\":4,\"2049\":4,\"2054\":3,\"2055\":1,\"2056\":1,\"2065\":1,\"2066\":1,\"2101\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2129\":3,\"2130\":13,\"2131\":12,\"2132\":1,\"2133\":1,\"2134\":2,\"2136\":5,\"2138\":1,\"2139\":2,\"2142\":1,\"2143\":6,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2148\":1,\"2151\":1,\"2155\":3,\"2156\":1,\"2157\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":5,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":3,\"2184\":3,\"2185\":1,\"2187\":2,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2202\":1,\"2203\":2,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2218\":2,\"2221\":1,\"2222\":2,\"2224\":2,\"2226\":1,\"2227\":1,\"2228\":3,\"2229\":3,\"2231\":1,\"2232\":2,\"2235\":3,\"2236\":3,\"2238\":2,\"2239\":5,\"2240\":7,\"2245\":3,\"2246\":1,\"2248\":1,\"2249\":5,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":3,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":4,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2276\":1,\"2277\":1,\"2287\":2,\"2290\":2,\"2304\":1,\"2305\":1,\"2310\":1,\"2314\":1,\"2325\":6,\"2327\":8,\"2334\":1,\"2338\":1,\"2344\":3,\"2345\":2,\"2347\":1,\"2354\":3,\"2355\":18,\"2357\":1,\"2359\":3,\"2367\":1,\"2369\":2,\"2371\":1,\"2376\":1,\"2380\":2,\"2384\":1,\"2385\":1,\"2401\":1,\"2403\":2,\"2405\":1,\"2407\":1,\"2408\":3,\"2409\":1,\"2411\":7,\"2412\":8,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2423\":6,\"2425\":1,\"2429\":1,\"2430\":1,\"2431\":2,\"2432\":5,\"2433\":1,\"2434\":1,\"2435\":1,\"2443\":1,\"2445\":2,\"2446\":3,\"2447\":6,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1,\"2480\":1}}],[\"aspect\",{\"1\":{\"2130\":1}}],[\"aspects\",{\"1\":{\"247\":1}}],[\"astype\",{\"1\":{\"1846\":1}}],[\"asteroidmodel\",{\"0\":{\"1053\":1},\"1\":{\"1053\":1}}],[\"asteroidfrontend\",{\"0\":{\"691\":1},\"1\":{\"691\":1}}],[\"asteroid\",{\"0\":{\"691\":1,\"1053\":1},\"1\":{\"223\":2,\"691\":3,\"1051\":1,\"1053\":11,\"1308\":3}}],[\"asdict\",{\"1\":{\"1807\":1}}],[\"asymmetric\",{\"1\":{\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1}}],[\"asv\",{\"1\":{\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"954\":2}}],[\"asvspooftask\",{\"0\":{\"2248\":1},\"1\":{\"2248\":1}}],[\"asvspoofocsoftmaxloss\",{\"0\":{\"949\":1},\"1\":{\"949\":1}}],[\"asvspoofbinaryloss\",{\"0\":{\"948\":1},\"1\":{\"948\":1}}],[\"asvspoofamsoftmaxloss\",{\"0\":{\"947\":1},\"1\":{\"947\":1}}],[\"asvspoof\",{\"0\":{\"327\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"2248\":1,\"2518\":1},\"1\":{\"201\":1,\"327\":6,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"2248\":1}}],[\"as20k\",{\"1\":{\"211\":4,\"212\":2}}],[\"ask\",{\"1\":{\"165\":1}}],[\"assemble\",{\"1\":{\"1720\":2}}],[\"asserterror\",{\"1\":{\"1124\":1}}],[\"assert\",{\"0\":{\"1859\":1},\"1\":{\"988\":2,\"990\":2,\"994\":1,\"1020\":1,\"1025\":1,\"1859\":2,\"1933\":1,\"2378\":2}}],[\"asserts\",{\"1\":{\"962\":1}}],[\"assertion\",{\"1\":{\"81\":2}}],[\"assistant\",{\"1\":{\"247\":1,\"2039\":1,\"2049\":1,\"2143\":1}}],[\"assignemnt3\",{\"1\":{\"190\":1}}],[\"assigned\",{\"1\":{\"94\":2}}],[\"assignment8\",{\"1\":{\"190\":1}}],[\"assignment7\",{\"1\":{\"190\":1}}],[\"assignment6\",{\"1\":{\"190\":1}}],[\"assignment5\",{\"1\":{\"190\":1}}],[\"assignment4\",{\"1\":{\"190\":1}}],[\"assignment1\",{\"1\":{\"190\":1}}],[\"assignment\",{\"1\":{\"190\":1}}],[\"assignment0\",{\"1\":{\"190\":1}}],[\"assign\",{\"1\":{\"167\":1,\"269\":1,\"278\":1,\"2355\":1}}],[\"assigns\",{\"1\":{\"74\":1}}],[\"associated\",{\"1\":{\"213\":1,\"706\":1,\"1441\":1,\"2355\":1}}],[\"assuming\",{\"1\":{\"195\":1,\"1558\":1}}],[\"assumed\",{\"1\":{\"1061\":1,\"1063\":1,\"1168\":1,\"1306\":1,\"1316\":1,\"1320\":1,\"1371\":1,\"1700\":1}}],[\"assume\",{\"1\":{\"69\":1,\"70\":1,\"195\":1,\"224\":1,\"266\":1,\"275\":1,\"285\":1,\"286\":2,\"675\":1,\"696\":1,\"1552\":3,\"1599\":3,\"1626\":3,\"1992\":3,\"1993\":3,\"1995\":3,\"2235\":3,\"2236\":3,\"2239\":3,\"2240\":3,\"2245\":3,\"2345\":1,\"2411\":3,\"2412\":3,\"2423\":3,\"2431\":3,\"2432\":3,\"2447\":3}}],[\"assumes\",{\"1\":{\"58\":1,\"81\":1,\"152\":1,\"722\":1,\"770\":1,\"852\":1,\"856\":1,\"958\":1,\"959\":1,\"2155\":1}}],[\"ascending\",{\"1\":{\"449\":2,\"1308\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":2,\"2007\":1}}],[\"asc\",{\"1\":{\"16\":1}}],[\"aswin\",{\"1\":{\"11\":1}}],[\"asr2\",{\"1\":{\"162\":2,\"203\":1,\"205\":2,\"206\":2}}],[\"asrtask\",{\"0\":{\"2246\":1},\"1\":{\"138\":1,\"2246\":1,\"2265\":1}}],[\"asrtransducertask\",{\"0\":{\"2247\":1},\"1\":{\"138\":1,\"2247\":7}}],[\"asr|tts|vc\",{\"1\":{\"49\":1}}],[\"asr1\",{\"1\":{\"22\":1,\"23\":1,\"24\":3,\"25\":1,\"37\":1,\"38\":1,\"50\":1,\"107\":1,\"108\":5,\"109\":1,\"127\":1,\"136\":2,\"195\":2,\"196\":1,\"197\":1,\"198\":2,\"200\":2,\"201\":1,\"203\":1,\"205\":1,\"223\":1,\"227\":6,\"228\":1,\"240\":1,\"254\":1}}],[\"asru\",{\"1\":{\"6\":1,\"15\":1,\"240\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1}}],[\"asr\",{\"0\":{\"14\":1,\"129\":1,\"138\":1,\"178\":1,\"184\":1,\"228\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"614\":1,\"615\":1,\"616\":1,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"629\":1,\"630\":1,\"631\":1,\"632\":1,\"633\":1,\"634\":1,\"635\":1,\"636\":1,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"651\":1,\"652\":1,\"653\":1,\"654\":1,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"663\":1,\"664\":1,\"665\":1,\"666\":1,\"667\":1,\"668\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":1,\"673\":1,\"674\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"688\":1,\"689\":1,\"691\":1,\"692\":1,\"693\":1,\"695\":1,\"696\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"703\":1,\"704\":1,\"706\":1,\"707\":1,\"708\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"720\":1,\"722\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":2,\"738\":1,\"740\":1,\"741\":1,\"743\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"750\":1,\"751\":1,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"757\":1,\"759\":1,\"760\":1,\"761\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":1,\"767\":1,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"773\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"788\":1,\"790\":1,\"791\":1,\"793\":1,\"794\":1,\"795\":1,\"796\":1,\"798\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"817\":1,\"820\":1,\"821\":1,\"823\":1,\"824\":1,\"825\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":1,\"832\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"862\":1,\"864\":1,\"866\":1,\"867\":1,\"868\":1,\"869\":1,\"870\":1,\"871\":1,\"872\":1,\"873\":1,\"874\":1,\"875\":1,\"876\":1,\"877\":1,\"878\":1,\"879\":1,\"880\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":1,\"890\":1,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":1,\"900\":1,\"901\":1,\"902\":1,\"903\":1,\"904\":1,\"905\":1,\"906\":1,\"907\":1,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":1,\"914\":1,\"915\":1,\"916\":1,\"917\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"928\":1,\"929\":1,\"930\":1,\"931\":1,\"933\":1,\"935\":1,\"936\":1,\"937\":1,\"938\":1,\"939\":1,\"940\":1,\"941\":1,\"942\":1,\"943\":1,\"944\":1,\"945\":1,\"946\":1,\"1880\":1,\"1893\":1,\"2026\":2,\"2043\":2,\"2055\":2,\"2056\":2,\"2066\":2,\"2083\":1,\"2111\":1,\"2246\":1,\"2247\":1,\"2514\":1,\"2517\":1},\"1\":{\"3\":1,\"11\":1,\"14\":1,\"22\":1,\"32\":6,\"37\":1,\"38\":3,\"41\":2,\"42\":1,\"46\":1,\"47\":2,\"56\":2,\"57\":1,\"58\":2,\"61\":3,\"62\":1,\"63\":1,\"64\":1,\"70\":1,\"78\":1,\"79\":1,\"80\":2,\"81\":1,\"84\":5,\"85\":2,\"86\":2,\"87\":1,\"88\":6,\"89\":1,\"90\":1,\"91\":1,\"92\":1,\"93\":2,\"94\":4,\"96\":1,\"97\":1,\"98\":2,\"99\":1,\"100\":1,\"102\":1,\"103\":1,\"104\":1,\"107\":1,\"109\":1,\"119\":5,\"124\":5,\"126\":1,\"127\":1,\"128\":1,\"129\":1,\"130\":1,\"134\":3,\"136\":10,\"137\":3,\"138\":5,\"139\":4,\"141\":1,\"150\":1,\"161\":1,\"162\":6,\"163\":1,\"178\":6,\"184\":2,\"190\":1,\"193\":2,\"195\":3,\"196\":2,\"197\":5,\"199\":4,\"200\":12,\"201\":3,\"204\":4,\"205\":12,\"207\":2,\"222\":2,\"223\":17,\"227\":3,\"228\":6,\"242\":10,\"246\":4,\"247\":3,\"260\":1,\"261\":2,\"262\":9,\"267\":9,\"268\":1,\"276\":9,\"277\":1,\"286\":14,\"290\":1,\"295\":6,\"301\":10,\"309\":6,\"315\":6,\"321\":8,\"402\":1,\"403\":6,\"442\":3,\"463\":16,\"511\":2,\"543\":1,\"581\":1,\"614\":1,\"615\":1,\"616\":1,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"629\":1,\"630\":1,\"631\":1,\"632\":1,\"633\":1,\"634\":1,\"635\":1,\"636\":1,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"651\":1,\"652\":1,\"653\":1,\"654\":1,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":2,\"662\":1,\"663\":1,\"664\":1,\"665\":1,\"666\":1,\"667\":1,\"668\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":1,\"673\":1,\"674\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"688\":1,\"689\":1,\"691\":1,\"692\":1,\"693\":1,\"696\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"703\":2,\"704\":1,\"706\":1,\"709\":1,\"710\":1,\"711\":2,\"712\":1,\"713\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"720\":2,\"722\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":2,\"738\":1,\"740\":1,\"741\":1,\"743\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":2,\"748\":1,\"749\":1,\"750\":1,\"751\":1,\"752\":1,\"754\":1,\"755\":2,\"756\":1,\"757\":1,\"759\":2,\"760\":1,\"761\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":1,\"767\":1,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"773\":1,\"774\":1,\"775\":1,\"776\":2,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"784\":1,\"785\":2,\"786\":2,\"787\":1,\"788\":1,\"790\":1,\"791\":1,\"793\":1,\"794\":1,\"795\":1,\"796\":1,\"798\":1,\"800\":2,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":2,\"817\":1,\"820\":1,\"821\":1,\"823\":1,\"824\":1,\"825\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":1,\"832\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":3,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"862\":1,\"864\":1,\"866\":1,\"867\":2,\"868\":1,\"869\":1,\"873\":1,\"874\":1,\"875\":1,\"876\":1,\"877\":1,\"878\":1,\"879\":1,\"880\":1,\"881\":2,\"882\":1,\"883\":1,\"884\":2,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":1,\"890\":1,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":1,\"900\":1,\"901\":1,\"902\":1,\"903\":1,\"904\":1,\"905\":1,\"906\":1,\"907\":2,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":1,\"914\":1,\"915\":1,\"916\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":2,\"923\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"928\":1,\"929\":1,\"930\":1,\"931\":1,\"933\":1,\"935\":1,\"936\":2,\"937\":2,\"938\":1,\"939\":1,\"940\":1,\"941\":1,\"942\":1,\"943\":1,\"944\":1,\"945\":1,\"946\":1,\"1000\":1,\"1156\":2,\"1640\":1,\"1641\":1,\"1668\":1,\"1720\":1,\"1721\":4,\"1880\":1,\"1892\":1,\"1893\":1,\"1956\":1,\"1975\":2,\"1982\":1,\"1996\":1,\"2026\":2,\"2043\":7,\"2044\":21,\"2055\":4,\"2056\":4,\"2066\":4,\"2127\":1,\"2134\":2,\"2184\":1,\"2221\":2,\"2246\":1,\"2247\":3,\"2262\":2,\"2293\":1,\"2418\":1,\"2462\":1}}],[\"as\",{\"0\":{\"128\":1,\"152\":1},\"1\":{\"3\":1,\"18\":1,\"22\":2,\"24\":1,\"36\":1,\"39\":1,\"40\":1,\"41\":1,\"43\":1,\"44\":3,\"47\":1,\"48\":1,\"50\":1,\"51\":2,\"52\":1,\"56\":1,\"67\":1,\"69\":1,\"70\":1,\"71\":2,\"74\":1,\"75\":1,\"79\":4,\"80\":1,\"86\":1,\"87\":1,\"94\":1,\"96\":1,\"98\":3,\"99\":2,\"100\":2,\"102\":1,\"106\":1,\"107\":2,\"108\":1,\"110\":1,\"111\":1,\"117\":1,\"127\":1,\"128\":2,\"130\":2,\"133\":1,\"136\":1,\"138\":1,\"144\":1,\"145\":1,\"146\":1,\"162\":1,\"166\":1,\"167\":1,\"174\":1,\"196\":1,\"197\":3,\"200\":6,\"203\":1,\"205\":1,\"211\":1,\"213\":1,\"217\":1,\"218\":1,\"219\":1,\"220\":2,\"223\":12,\"224\":4,\"225\":2,\"228\":2,\"232\":2,\"233\":1,\"240\":1,\"242\":3,\"243\":3,\"246\":2,\"252\":2,\"254\":2,\"258\":2,\"259\":1,\"262\":4,\"267\":3,\"268\":1,\"269\":2,\"275\":1,\"276\":3,\"277\":1,\"278\":2,\"285\":3,\"286\":10,\"287\":1,\"290\":14,\"527\":1,\"536\":1,\"626\":1,\"631\":1,\"702\":1,\"703\":3,\"706\":1,\"710\":1,\"711\":1,\"724\":1,\"725\":1,\"726\":2,\"727\":1,\"728\":1,\"744\":1,\"755\":4,\"756\":8,\"768\":8,\"773\":8,\"785\":3,\"804\":1,\"817\":1,\"819\":2,\"824\":1,\"828\":1,\"829\":2,\"830\":1,\"846\":1,\"859\":2,\"866\":7,\"867\":7,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"922\":2,\"924\":1,\"927\":2,\"929\":1,\"932\":1,\"934\":1,\"936\":2,\"937\":2,\"972\":1,\"980\":1,\"986\":1,\"1009\":1,\"1022\":1,\"1026\":1,\"1035\":1,\"1075\":1,\"1113\":1,\"1145\":1,\"1149\":1,\"1156\":1,\"1180\":1,\"1181\":1,\"1224\":1,\"1225\":1,\"1251\":1,\"1265\":1,\"1268\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1280\":1,\"1283\":1,\"1301\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1316\":1,\"1327\":1,\"1330\":1,\"1334\":2,\"1346\":1,\"1347\":1,\"1372\":1,\"1376\":1,\"1377\":1,\"1395\":1,\"1397\":1,\"1400\":1,\"1419\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1477\":1,\"1484\":2,\"1502\":1,\"1521\":2,\"1546\":1,\"1548\":1,\"1552\":3,\"1585\":1,\"1590\":1,\"1599\":3,\"1612\":1,\"1622\":1,\"1626\":4,\"1655\":1,\"1662\":1,\"1668\":4,\"1680\":1,\"1692\":1,\"1697\":1,\"1698\":2,\"1720\":1,\"1721\":1,\"1725\":2,\"1731\":2,\"1735\":2,\"1751\":3,\"1759\":3,\"1774\":1,\"1806\":2,\"1827\":1,\"1830\":1,\"1881\":2,\"1883\":3,\"1901\":1,\"1903\":1,\"1931\":1,\"1950\":1,\"1955\":1,\"1971\":1,\"1992\":4,\"1993\":3,\"1995\":3,\"2007\":1,\"2016\":1,\"2019\":1,\"2039\":2,\"2040\":1,\"2043\":1,\"2044\":2,\"2045\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2065\":3,\"2066\":1,\"2130\":3,\"2131\":1,\"2132\":1,\"2133\":2,\"2136\":2,\"2137\":1,\"2147\":1,\"2184\":2,\"2222\":1,\"2228\":2,\"2229\":2,\"2235\":3,\"2236\":3,\"2239\":4,\"2240\":3,\"2245\":3,\"2246\":4,\"2248\":4,\"2249\":7,\"2250\":4,\"2251\":4,\"2252\":4,\"2253\":6,\"2254\":4,\"2255\":4,\"2256\":4,\"2257\":4,\"2259\":4,\"2260\":4,\"2261\":4,\"2262\":2,\"2263\":4,\"2264\":4,\"2265\":4,\"2266\":4,\"2267\":4,\"2268\":4,\"2269\":4,\"2270\":4,\"2271\":4,\"2272\":4,\"2273\":4,\"2276\":1,\"2277\":1,\"2280\":1,\"2298\":1,\"2312\":1,\"2325\":1,\"2327\":1,\"2348\":1,\"2355\":2,\"2359\":1,\"2365\":1,\"2370\":2,\"2372\":1,\"2377\":1,\"2384\":1,\"2385\":1,\"2403\":1,\"2408\":2,\"2411\":3,\"2412\":3,\"2423\":3,\"2431\":3,\"2432\":3,\"2443\":2,\"2445\":1,\"2446\":2,\"2447\":5,\"2449\":1,\"2474\":1}}],[\"a\",{\"0\":{\"76\":1,\"114\":1,\"118\":2,\"150\":1,\"152\":1,\"154\":1,\"224\":1,\"225\":1},\"1\":{\"2\":2,\"3\":1,\"15\":2,\"16\":1,\"18\":3,\"19\":1,\"22\":3,\"25\":3,\"26\":7,\"36\":1,\"38\":3,\"39\":1,\"41\":1,\"43\":7,\"45\":2,\"46\":1,\"47\":7,\"50\":6,\"51\":3,\"52\":2,\"60\":1,\"67\":1,\"68\":1,\"69\":4,\"70\":4,\"71\":1,\"73\":6,\"74\":2,\"75\":2,\"76\":3,\"78\":5,\"79\":3,\"80\":12,\"81\":1,\"82\":7,\"84\":2,\"86\":2,\"91\":1,\"94\":1,\"101\":1,\"102\":4,\"106\":4,\"107\":1,\"108\":1,\"110\":1,\"119\":2,\"121\":1,\"124\":1,\"126\":1,\"127\":2,\"128\":2,\"133\":1,\"135\":1,\"138\":2,\"139\":1,\"141\":7,\"145\":2,\"146\":1,\"147\":1,\"150\":3,\"153\":1,\"159\":1,\"161\":1,\"162\":3,\"163\":1,\"166\":1,\"167\":2,\"168\":3,\"173\":2,\"187\":1,\"188\":1,\"190\":2,\"191\":2,\"192\":1,\"193\":1,\"195\":2,\"196\":6,\"197\":5,\"198\":1,\"200\":3,\"201\":1,\"202\":1,\"203\":1,\"205\":2,\"209\":1,\"211\":6,\"212\":1,\"213\":3,\"215\":1,\"217\":2,\"218\":1,\"220\":1,\"221\":1,\"222\":4,\"223\":5,\"224\":2,\"225\":4,\"228\":3,\"229\":1,\"230\":1,\"231\":1,\"233\":4,\"235\":2,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":3,\"242\":9,\"243\":11,\"245\":1,\"246\":2,\"247\":3,\"248\":1,\"250\":1,\"251\":1,\"252\":4,\"254\":4,\"257\":1,\"259\":5,\"261\":1,\"262\":9,\"263\":2,\"264\":1,\"266\":2,\"267\":2,\"268\":3,\"269\":7,\"271\":1,\"273\":1,\"275\":2,\"276\":7,\"277\":3,\"278\":7,\"280\":1,\"283\":1,\"284\":4,\"285\":5,\"286\":7,\"287\":12,\"290\":9,\"291\":1,\"292\":1,\"295\":1,\"415\":1,\"522\":1,\"536\":3,\"545\":2,\"572\":1,\"583\":3,\"589\":3,\"592\":1,\"600\":1,\"614\":1,\"615\":1,\"616\":3,\"617\":1,\"618\":1,\"620\":3,\"621\":2,\"622\":1,\"623\":1,\"624\":1,\"630\":1,\"632\":1,\"633\":1,\"634\":2,\"635\":1,\"636\":1,\"637\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":2,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"651\":1,\"653\":1,\"664\":1,\"675\":1,\"691\":2,\"699\":3,\"702\":1,\"704\":2,\"706\":1,\"716\":2,\"724\":4,\"725\":4,\"726\":6,\"727\":4,\"728\":4,\"738\":1,\"744\":4,\"750\":1,\"755\":9,\"756\":9,\"760\":2,\"768\":3,\"769\":1,\"770\":2,\"773\":9,\"780\":1,\"785\":7,\"793\":2,\"797\":1,\"798\":1,\"802\":1,\"817\":4,\"818\":1,\"819\":1,\"820\":1,\"821\":10,\"822\":1,\"823\":1,\"824\":8,\"828\":9,\"829\":12,\"830\":9,\"831\":4,\"833\":1,\"846\":1,\"852\":1,\"858\":1,\"859\":6,\"862\":1,\"866\":6,\"867\":6,\"881\":3,\"884\":3,\"912\":1,\"918\":1,\"922\":3,\"924\":2,\"926\":2,\"928\":1,\"929\":2,\"930\":1,\"936\":2,\"937\":2,\"939\":1,\"943\":1,\"944\":1,\"954\":1,\"958\":1,\"959\":2,\"960\":3,\"986\":2,\"992\":3,\"994\":3,\"996\":1,\"997\":3,\"998\":1,\"999\":1,\"1000\":2,\"1002\":4,\"1004\":2,\"1008\":8,\"1011\":1,\"1012\":2,\"1016\":3,\"1018\":1,\"1019\":1,\"1021\":1,\"1022\":1,\"1023\":2,\"1024\":1,\"1026\":1,\"1029\":2,\"1035\":2,\"1050\":4,\"1051\":7,\"1064\":2,\"1066\":1,\"1084\":2,\"1113\":2,\"1116\":5,\"1130\":1,\"1131\":1,\"1132\":1,\"1139\":1,\"1155\":4,\"1157\":4,\"1161\":4,\"1165\":1,\"1189\":4,\"1199\":1,\"1209\":1,\"1218\":4,\"1221\":4,\"1229\":5,\"1235\":2,\"1244\":4,\"1245\":4,\"1246\":1,\"1247\":1,\"1250\":1,\"1251\":3,\"1257\":1,\"1262\":1,\"1279\":2,\"1280\":4,\"1281\":3,\"1282\":2,\"1283\":3,\"1290\":1,\"1292\":2,\"1306\":3,\"1308\":7,\"1309\":2,\"1310\":1,\"1311\":2,\"1314\":1,\"1316\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1327\":3,\"1330\":4,\"1331\":1,\"1334\":1,\"1342\":1,\"1350\":5,\"1354\":5,\"1355\":1,\"1357\":1,\"1367\":1,\"1368\":1,\"1371\":3,\"1376\":1,\"1377\":1,\"1395\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1441\":2,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1477\":1,\"1484\":2,\"1516\":1,\"1521\":2,\"1533\":4,\"1539\":1,\"1545\":1,\"1546\":1,\"1551\":1,\"1553\":1,\"1555\":1,\"1558\":6,\"1585\":1,\"1598\":1,\"1605\":1,\"1611\":1,\"1612\":1,\"1616\":1,\"1622\":1,\"1625\":1,\"1626\":1,\"1631\":1,\"1643\":1,\"1644\":1,\"1646\":1,\"1655\":6,\"1665\":1,\"1668\":4,\"1672\":2,\"1674\":1,\"1678\":1,\"1679\":1,\"1683\":1,\"1684\":1,\"1685\":1,\"1686\":1,\"1692\":1,\"1694\":1,\"1706\":1,\"1720\":1,\"1721\":1,\"1723\":2,\"1725\":3,\"1726\":2,\"1727\":2,\"1731\":6,\"1736\":3,\"1737\":1,\"1749\":1,\"1750\":1,\"1752\":2,\"1758\":2,\"1760\":1,\"1761\":1,\"1763\":1,\"1770\":1,\"1771\":2,\"1778\":1,\"1788\":1,\"1795\":1,\"1797\":1,\"1805\":2,\"1806\":7,\"1810\":1,\"1811\":1,\"1819\":1,\"1822\":3,\"1823\":1,\"1824\":3,\"1834\":1,\"1842\":1,\"1851\":2,\"1854\":1,\"1855\":1,\"1861\":1,\"1862\":1,\"1871\":1,\"1881\":1,\"1895\":1,\"1897\":2,\"1898\":1,\"1907\":1,\"1920\":2,\"1921\":1,\"1949\":1,\"1950\":2,\"1955\":1,\"1960\":1,\"1961\":1,\"1966\":2,\"1971\":1,\"1992\":2,\"1993\":2,\"1994\":1,\"1995\":1,\"2000\":9,\"2001\":9,\"2007\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2020\":2,\"2039\":5,\"2040\":4,\"2043\":3,\"2044\":3,\"2045\":8,\"2049\":9,\"2054\":3,\"2055\":3,\"2056\":3,\"2065\":3,\"2066\":3,\"2101\":4,\"2130\":7,\"2131\":5,\"2132\":3,\"2133\":1,\"2134\":4,\"2136\":4,\"2137\":2,\"2138\":1,\"2139\":1,\"2140\":1,\"2142\":2,\"2145\":2,\"2146\":1,\"2147\":1,\"2148\":2,\"2150\":2,\"2151\":2,\"2155\":1,\"2160\":1,\"2166\":1,\"2184\":1,\"2188\":1,\"2190\":1,\"2220\":2,\"2222\":1,\"2223\":1,\"2227\":2,\"2228\":2,\"2229\":2,\"2231\":2,\"2240\":4,\"2245\":1,\"2246\":1,\"2248\":1,\"2249\":2,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":2,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":3,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2276\":1,\"2277\":1,\"2280\":2,\"2293\":1,\"2299\":1,\"2305\":2,\"2307\":1,\"2309\":1,\"2310\":2,\"2312\":1,\"2314\":1,\"2319\":1,\"2325\":3,\"2327\":3,\"2329\":2,\"2330\":2,\"2331\":2,\"2334\":5,\"2344\":1,\"2353\":1,\"2354\":2,\"2355\":19,\"2364\":1,\"2366\":1,\"2369\":1,\"2403\":1,\"2404\":1,\"2408\":2,\"2411\":1,\"2412\":1,\"2420\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2433\":1,\"2443\":1,\"2445\":1,\"2446\":2,\"2447\":1,\"2479\":4,\"2480\":1,\"2486\":1,\"2488\":1}}],[\"alway\",{\"1\":{\"1812\":1}}],[\"always=true\",{\"1\":{\"1770\":1,\"1771\":1}}],[\"always\",{\"1\":{\"79\":1,\"82\":1,\"84\":1,\"107\":1,\"164\":1,\"175\":1,\"195\":1,\"290\":2,\"368\":2,\"406\":2,\"484\":2,\"490\":2,\"817\":1,\"993\":1,\"1007\":1,\"1028\":1,\"1155\":3,\"1157\":4,\"1185\":1,\"1444\":1,\"1448\":1,\"1545\":1,\"1770\":2,\"1771\":4,\"2139\":1,\"2249\":1,\"2253\":1,\"2480\":1}}],[\"alaw\",{\"1\":{\"1678\":1}}],[\"alan\",{\"1\":{\"12\":1}}],[\"aliases\",{\"1\":{\"2480\":4}}],[\"alias=\",{\"1\":{\"1876\":1}}],[\"alias\",{\"0\":{\"2480\":1,\"2507\":2},\"1\":{\"1722\":6,\"1807\":5,\"1876\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2480\":2,\"2507\":3}}],[\"alibaba\",{\"1\":{\"1385\":2}}],[\"aligments\",{\"1\":{\"284\":1,\"285\":5}}],[\"align\",{\"0\":{\"295\":1,\"415\":1,\"1630\":1},\"1\":{\"269\":1,\"278\":1,\"295\":1,\"415\":1,\"616\":1,\"696\":1,\"697\":1,\"706\":1,\"752\":1,\"1522\":1,\"1598\":2,\"1997\":1}}],[\"aligner\",{\"1\":{\"285\":1}}],[\"aligned\",{\"1\":{\"269\":1,\"278\":1,\"286\":1,\"290\":1,\"2136\":1}}],[\"alignement\",{\"1\":{\"45\":1,\"145\":1}}],[\"alignmentmodule\",{\"0\":{\"1577\":1},\"1\":{\"1577\":2}}],[\"alignments\",{\"0\":{\"1577\":1,\"1629\":1,\"1637\":1},\"1\":{\"285\":1,\"706\":1,\"1577\":1,\"1629\":1,\"1637\":1,\"1997\":1}}],[\"alignment\",{\"1\":{\"45\":1,\"145\":1,\"205\":1,\"262\":4,\"415\":1,\"616\":1,\"696\":1,\"697\":1,\"706\":3,\"1577\":2,\"1598\":1,\"1599\":1,\"1997\":3,\"2245\":1}}],[\"ald\",{\"1\":{\"1253\":2}}],[\"aldeneh\",{\"1\":{\"8\":1}}],[\"although\",{\"1\":{\"223\":1,\"262\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"alternative\",{\"1\":{\"1502\":1}}],[\"alternatively\",{\"1\":{\"1\":1,\"267\":1}}],[\"altenative\",{\"1\":{\"118\":1}}],[\"alexei\",{\"1\":{\"207\":1}}],[\"alec\",{\"1\":{\"202\":1}}],[\"alone\",{\"1\":{\"133\":1}}],[\"alongside\",{\"1\":{\"44\":1}}],[\"along\",{\"0\":{\"1664\":1,\"1665\":1,\"1691\":2},\"1\":{\"3\":1,\"101\":1,\"1008\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1664\":1,\"1665\":2,\"1686\":1,\"1691\":3,\"1694\":1,\"1717\":1,\"2280\":1}}],[\"almost\",{\"1\":{\"80\":1,\"102\":1,\"107\":1,\"162\":1,\"167\":1,\"197\":1,\"223\":1,\"286\":1,\"2019\":1}}],[\"alpha=1\",{\"1\":{\"809\":1,\"1103\":1,\"1236\":1,\"1770\":1,\"1771\":1,\"1788\":1}}],[\"alphas\",{\"0\":{\"878\":1,\"882\":1},\"1\":{\"703\":5,\"878\":4,\"881\":2,\"882\":4,\"884\":2,\"1419\":3}}],[\"alphabet\",{\"1\":{\"703\":2,\"705\":1,\"716\":2,\"755\":2,\"785\":2,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"919\":2}}],[\"alpha\",{\"1\":{\"45\":2,\"141\":2,\"484\":2,\"490\":2,\"650\":7,\"664\":3,\"696\":2,\"697\":2,\"703\":5,\"878\":1,\"881\":1,\"882\":1,\"884\":1,\"949\":1,\"1130\":2,\"1396\":1,\"1401\":1,\"1403\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1466\":1,\"1468\":1,\"1526\":5,\"1552\":3,\"1553\":3,\"1598\":2,\"1599\":6,\"1600\":2,\"1625\":3,\"1626\":3,\"1681\":1,\"1683\":2,\"1685\":1,\"1770\":1,\"1771\":2,\"1788\":2,\"1990\":1,\"1994\":2,\"2168\":1,\"2239\":6,\"2240\":8,\"2411\":9,\"2412\":9,\"2423\":9,\"2432\":6,\"2447\":9}}],[\"alsd\",{\"1\":{\"45\":2,\"145\":2,\"148\":1,\"616\":1,\"696\":2,\"697\":2}}],[\"also\",{\"1\":{\"26\":2,\"41\":1,\"44\":1,\"45\":1,\"47\":2,\"50\":1,\"52\":1,\"59\":1,\"69\":1,\"70\":1,\"79\":2,\"82\":1,\"86\":1,\"91\":1,\"96\":1,\"98\":1,\"101\":1,\"106\":3,\"110\":1,\"117\":1,\"118\":1,\"120\":1,\"133\":1,\"139\":2,\"143\":1,\"144\":1,\"145\":1,\"152\":2,\"160\":1,\"161\":1,\"162\":3,\"163\":1,\"168\":2,\"196\":1,\"197\":1,\"200\":6,\"205\":5,\"211\":3,\"213\":1,\"217\":4,\"220\":1,\"223\":3,\"224\":2,\"225\":1,\"232\":1,\"242\":5,\"243\":1,\"247\":1,\"254\":3,\"259\":1,\"261\":1,\"262\":3,\"266\":6,\"267\":3,\"268\":1,\"269\":1,\"275\":8,\"276\":4,\"277\":1,\"278\":1,\"285\":5,\"286\":1,\"520\":1,\"536\":3,\"756\":2,\"768\":2,\"773\":2,\"817\":2,\"821\":2,\"829\":1,\"866\":1,\"867\":1,\"960\":1,\"1209\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":2,\"1502\":1,\"1697\":1,\"1711\":1,\"1730\":1,\"1945\":1,\"2044\":1,\"2354\":1,\"2355\":2}}],[\"al\",{\"1\":{\"45\":4,\"46\":1,\"139\":2,\"145\":4,\"146\":1,\"261\":1,\"711\":1,\"768\":1,\"833\":1,\"1066\":1,\"1117\":1,\"1125\":1,\"1126\":1,\"1130\":1,\"1131\":2,\"1149\":1,\"1170\":1,\"1172\":2,\"1176\":1,\"1185\":1,\"1252\":1,\"1321\":1,\"1322\":1,\"1327\":1,\"1330\":2,\"1668\":1,\"1720\":1,\"1721\":1,\"1729\":1,\"1730\":2,\"1880\":1,\"2191\":1,\"2192\":1,\"2198\":1,\"2203\":1,\"2209\":1}}],[\"algorithm\",{\"1\":{\"45\":3,\"139\":1,\"145\":2,\"558\":1,\"616\":1,\"696\":1,\"697\":1,\"821\":1,\"1116\":1,\"1229\":1,\"1328\":1,\"1400\":2,\"1439\":1,\"1726\":1,\"1727\":1,\"1729\":1,\"1730\":1,\"1854\":2,\"2147\":1,\"2298\":1,\"2404\":2}}],[\"algorithms\",{\"1\":{\"45\":2,\"145\":4,\"148\":1,\"631\":1,\"763\":1,\"1327\":1,\"1330\":1,\"1775\":1}}],[\"already\",{\"1\":{\"19\":1,\"26\":1,\"48\":2,\"110\":1,\"124\":1,\"150\":1,\"152\":1,\"162\":1,\"286\":3,\"626\":1,\"675\":1,\"1061\":1,\"1063\":1,\"1222\":1,\"1927\":1,\"2044\":4,\"2130\":2,\"2188\":1,\"2354\":1}}],[\"allzero\",{\"0\":{\"2375\":1},\"1\":{\"2346\":1,\"2368\":1,\"2375\":1}}],[\"allheadprelulayernormalization4dcf\",{\"0\":{\"1048\":1},\"1\":{\"1048\":1}}],[\"allheadprelulayernormalization4dc\",{\"0\":{\"1046\":1},\"1\":{\"1046\":1}}],[\"allenai\",{\"1\":{\"774\":1}}],[\"alleviates\",{\"1\":{\"262\":1}}],[\"allocated\",{\"1\":{\"703\":1,\"755\":1,\"785\":2}}],[\"allowing\",{\"1\":{\"246\":1,\"262\":1,\"1051\":1,\"2130\":1}}],[\"allow\",{\"1\":{\"81\":2,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"368\":2,\"385\":2,\"389\":2,\"396\":2,\"406\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":4,\"463\":2,\"469\":2,\"475\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":2,\"505\":2,\"755\":1,\"846\":2,\"1155\":1,\"1157\":1,\"1493\":1,\"1494\":1,\"2220\":2,\"2249\":2,\"2258\":2,\"2331\":1,\"2342\":1,\"2390\":1,\"2393\":1,\"2397\":1,\"2399\":1,\"2480\":1}}],[\"allowed\",{\"1\":{\"43\":1,\"45\":1,\"81\":1,\"145\":1,\"290\":1,\"616\":1,\"696\":1,\"697\":1,\"1246\":1,\"1679\":1,\"1920\":1,\"2039\":1,\"2145\":1,\"2146\":1,\"2147\":1}}],[\"allows\",{\"1\":{\"18\":1,\"793\":1,\"829\":2,\"1066\":1,\"2345\":1}}],[\"all=c\",{\"1\":{\"224\":1}}],[\"all\",{\"0\":{\"1473\":1,\"1474\":1,\"1475\":1,\"1950\":2,\"2321\":2},\"1\":{\"1\":1,\"2\":1,\"3\":4,\"10\":1,\"22\":3,\"24\":1,\"31\":1,\"46\":1,\"50\":1,\"60\":1,\"66\":1,\"71\":1,\"79\":1,\"80\":1,\"84\":1,\"88\":3,\"101\":1,\"108\":1,\"111\":1,\"117\":1,\"128\":1,\"133\":1,\"141\":1,\"147\":1,\"148\":1,\"162\":5,\"196\":1,\"197\":1,\"200\":2,\"201\":3,\"206\":1,\"211\":3,\"212\":1,\"213\":1,\"218\":1,\"225\":1,\"236\":1,\"242\":3,\"243\":1,\"247\":1,\"254\":1,\"255\":1,\"262\":1,\"267\":1,\"268\":1,\"276\":2,\"277\":1,\"286\":2,\"290\":4,\"331\":2,\"535\":1,\"536\":1,\"655\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"692\":3,\"693\":1,\"699\":3,\"709\":3,\"713\":1,\"718\":1,\"720\":1,\"722\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"734\":1,\"738\":1,\"741\":1,\"747\":1,\"750\":2,\"752\":1,\"756\":2,\"757\":1,\"773\":2,\"774\":3,\"778\":1,\"781\":1,\"786\":1,\"787\":2,\"788\":1,\"791\":1,\"796\":1,\"798\":1,\"800\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"819\":1,\"821\":3,\"825\":1,\"829\":3,\"830\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"842\":1,\"844\":1,\"852\":1,\"854\":1,\"856\":1,\"859\":1,\"860\":1,\"862\":1,\"864\":1,\"866\":1,\"867\":2,\"921\":1,\"935\":1,\"950\":2,\"952\":1,\"956\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"994\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":2,\"1038\":1,\"1040\":1,\"1042\":2,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1066\":1,\"1068\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1114\":1,\"1120\":1,\"1122\":1,\"1124\":10,\"1125\":10,\"1131\":1,\"1132\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1145\":1,\"1147\":2,\"1149\":1,\"1151\":1,\"1153\":1,\"1155\":1,\"1157\":1,\"1159\":1,\"1165\":1,\"1168\":1,\"1174\":1,\"1176\":1,\"1177\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1200\":1,\"1202\":1,\"1205\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1219\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1262\":1,\"1265\":1,\"1271\":1,\"1276\":1,\"1279\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1292\":1,\"1350\":1,\"1354\":4,\"1383\":1,\"1387\":1,\"1392\":1,\"1398\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":3,\"1452\":3,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1477\":1,\"1478\":2,\"1489\":1,\"1509\":1,\"1511\":1,\"1513\":3,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1548\":3,\"1549\":1,\"1551\":3,\"1554\":1,\"1592\":3,\"1596\":4,\"1597\":6,\"1599\":1,\"1605\":3,\"1606\":2,\"1609\":3,\"1610\":3,\"1618\":1,\"1619\":3,\"1628\":3,\"1638\":1,\"1645\":1,\"1652\":1,\"1657\":1,\"1662\":1,\"1668\":1,\"1683\":4,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1719\":1,\"1750\":2,\"1794\":1,\"1804\":1,\"1806\":1,\"1821\":1,\"1863\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1945\":1,\"1950\":3,\"1955\":1,\"1957\":1,\"1967\":2,\"1969\":2,\"1972\":1,\"1975\":2,\"1978\":1,\"1980\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"1992\":1,\"1995\":1,\"2000\":3,\"2001\":2,\"2016\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2130\":6,\"2132\":4,\"2136\":3,\"2137\":2,\"2139\":1,\"2141\":2,\"2144\":1,\"2149\":1,\"2162\":2,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2185\":1,\"2188\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2203\":1,\"2205\":1,\"2209\":1,\"2211\":1,\"2216\":1,\"2220\":1,\"2240\":1,\"2305\":1,\"2321\":2,\"2325\":1,\"2344\":1,\"2354\":2,\"2355\":1,\"2359\":1,\"2367\":1,\"2368\":1,\"2401\":1,\"2405\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2434\":1,\"2443\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2456\":2,\"2458\":1,\"2460\":1,\"2465\":1,\"2467\":1}}],[\"bw2\",{\"1\":{\"2474\":1}}],[\"bw\",{\"1\":{\"1391\":1,\"1403\":1,\"1406\":1,\"1410\":1,\"1468\":1}}],[\"bdt\",{\"1\":{\"1179\":1}}],[\"bdtf\",{\"1\":{\"1072\":1,\"1074\":1,\"1296\":1}}],[\"bdelay=3\",{\"1\":{\"1354\":1}}],[\"bdelay\",{\"1\":{\"1126\":1,\"1315\":1,\"1351\":1,\"1354\":1,\"1356\":3}}],[\"bdropout\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1217\":1,\"1539\":1,\"1766\":1}}],[\"bfloat16\",{\"1\":{\"2133\":2}}],[\"bf\",{\"0\":{\"1059\":1},\"1\":{\"1059\":1,\"1217\":2}}],[\"b3\",{\"1\":{\"1027\":2}}],[\"bb\",{\"1\":{\"999\":1,\"1006\":1,\"1010\":3,\"1014\":1,\"1018\":1}}],[\"bnb\",{\"0\":{\"2099\":1}}],[\"bnonlinear\",{\"1\":{\"1217\":1}}],[\"bnet\",{\"1\":{\"1217\":1}}],[\"bn\",{\"1\":{\"980\":1,\"982\":1,\"1061\":1,\"1062\":1,\"1118\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1267\":1,\"1268\":1,\"1273\":1,\"1274\":1,\"2460\":1}}],[\"bnmask\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1539\":1,\"1766\":1}}],[\"b^dt\",{\"1\":{\"824\":1}}],[\"bprojs\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1126\":1,\"1217\":1,\"1539\":1,\"1766\":1}}],[\"bpemodel\",{\"1\":{\"243\":2,\"295\":2,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"389\":2,\"396\":2,\"406\":2,\"415\":2,\"421\":2,\"429\":2,\"442\":2,\"463\":4,\"469\":2,\"481\":2,\"498\":2,\"505\":2,\"2293\":1,\"2336\":1,\"2337\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1}}],[\"bpe50000\",{\"1\":{\"243\":1}}],[\"bpe5000\",{\"1\":{\"136\":2,\"2043\":2}}],[\"bpe\",{\"1\":{\"136\":3,\"175\":1,\"200\":1,\"205\":1,\"228\":1,\"242\":1,\"243\":12,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"389\":1,\"396\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"442\":1,\"463\":2,\"469\":1,\"481\":1,\"498\":1,\"578\":2}}],[\"btaps=5\",{\"1\":{\"1354\":1}}],[\"btaps+1\",{\"1\":{\"1126\":3,\"1309\":5,\"1310\":3,\"1315\":2}}],[\"btaps\",{\"1\":{\"1126\":1,\"1309\":1,\"1315\":2,\"1351\":2,\"1354\":1}}],[\"btype\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1126\":1,\"1539\":1,\"1766\":1}}],[\"bt\",{\"1\":{\"699\":1}}],[\"btc\",{\"1\":{\"699\":1}}],[\"bss\",{\"1\":{\"1247\":1}}],[\"bsrnnseparator\",{\"0\":{\"1062\":1},\"1\":{\"1062\":1}}],[\"bsrnn\",{\"0\":{\"1061\":2,\"1062\":1,\"1063\":1,\"1072\":1,\"1074\":1,\"1198\":1,\"1296\":1,\"1316\":1,\"1320\":1},\"1\":{\"1061\":4,\"1062\":3,\"1063\":1,\"1072\":1,\"1074\":1,\"1198\":1,\"1296\":1,\"1316\":1,\"1320\":1}}],[\"bsz\",{\"1\":{\"787\":1}}],[\"bs\",{\"1\":{\"639\":2,\"1320\":2,\"1846\":2,\"2354\":1,\"2365\":1}}],[\"bs3min\",{\"1\":{\"236\":1}}],[\"bce\",{\"1\":{\"290\":2,\"1839\":2,\"1991\":1,\"2245\":2,\"2431\":2,\"2432\":3}}],[\"b=3\",{\"1\":{\"2488\":1}}],[\"b=b\",{\"1\":{\"2334\":1}}],[\"b=batch\",{\"1\":{\"819\":1}}],[\"b=false\",{\"1\":{\"895\":1}}],[\"b=\",{\"1\":{\"276\":1}}],[\"b2\",{\"1\":{\"213\":1,\"994\":1,\"1008\":1,\"1027\":2}}],[\"b1\",{\"1\":{\"213\":1,\"994\":1,\"1027\":2}}],[\"b\",{\"1\":{\"69\":3,\"74\":2,\"75\":2,\"80\":6,\"162\":1,\"276\":4,\"286\":1,\"287\":1,\"615\":2,\"617\":11,\"618\":11,\"619\":3,\"620\":17,\"621\":4,\"622\":3,\"623\":3,\"624\":11,\"625\":14,\"626\":5,\"627\":8,\"628\":1,\"630\":4,\"632\":6,\"633\":7,\"634\":7,\"636\":7,\"637\":9,\"638\":2,\"640\":2,\"641\":18,\"642\":4,\"643\":12,\"644\":26,\"645\":3,\"648\":2,\"649\":8,\"651\":3,\"654\":6,\"667\":6,\"670\":2,\"675\":5,\"691\":3,\"696\":1,\"697\":1,\"699\":4,\"703\":8,\"706\":15,\"710\":6,\"711\":6,\"717\":2,\"722\":1,\"724\":4,\"725\":4,\"728\":4,\"729\":5,\"740\":8,\"744\":4,\"745\":2,\"746\":3,\"747\":2,\"748\":2,\"755\":7,\"759\":2,\"768\":3,\"770\":3,\"771\":2,\"784\":4,\"785\":5,\"804\":3,\"819\":8,\"821\":5,\"822\":3,\"823\":3,\"824\":6,\"827\":2,\"828\":4,\"829\":4,\"830\":4,\"831\":3,\"832\":1,\"846\":2,\"847\":22,\"849\":2,\"852\":1,\"856\":1,\"878\":7,\"879\":7,\"881\":9,\"882\":7,\"883\":7,\"884\":9,\"918\":1,\"919\":2,\"922\":6,\"924\":5,\"932\":3,\"934\":3,\"936\":6,\"937\":6,\"944\":1,\"959\":3,\"973\":1,\"978\":1,\"980\":3,\"981\":2,\"982\":3,\"986\":1,\"992\":1,\"997\":1,\"999\":1,\"1008\":2,\"1016\":1,\"1018\":1,\"1023\":2,\"1031\":3,\"1035\":2,\"1051\":3,\"1053\":4,\"1054\":4,\"1061\":2,\"1062\":3,\"1063\":2,\"1064\":1,\"1080\":2,\"1107\":3,\"1112\":3,\"1113\":2,\"1117\":4,\"1118\":8,\"1124\":3,\"1125\":1,\"1126\":23,\"1127\":11,\"1130\":3,\"1131\":4,\"1132\":1,\"1136\":3,\"1141\":3,\"1147\":2,\"1148\":1,\"1162\":2,\"1164\":2,\"1176\":2,\"1180\":3,\"1181\":3,\"1182\":2,\"1183\":2,\"1184\":2,\"1198\":3,\"1199\":5,\"1232\":3,\"1250\":3,\"1251\":3,\"1252\":3,\"1261\":3,\"1264\":2,\"1267\":3,\"1268\":5,\"1269\":6,\"1270\":6,\"1271\":6,\"1272\":2,\"1273\":2,\"1274\":2,\"1278\":3,\"1280\":4,\"1283\":4,\"1290\":1,\"1308\":5,\"1309\":4,\"1310\":4,\"1315\":4,\"1334\":8,\"1342\":1,\"1351\":3,\"1357\":2,\"1374\":2,\"1375\":2,\"1385\":1,\"1389\":1,\"1390\":1,\"1391\":2,\"1395\":12,\"1397\":1,\"1400\":3,\"1401\":1,\"1402\":1,\"1403\":2,\"1406\":1,\"1408\":1,\"1409\":1,\"1410\":2,\"1419\":2,\"1466\":1,\"1467\":1,\"1468\":2,\"1513\":3,\"1514\":1,\"1515\":4,\"1516\":3,\"1519\":5,\"1520\":4,\"1521\":38,\"1524\":4,\"1525\":3,\"1526\":20,\"1529\":8,\"1534\":1,\"1535\":3,\"1536\":5,\"1546\":8,\"1548\":4,\"1551\":4,\"1552\":41,\"1553\":23,\"1555\":1,\"1556\":15,\"1559\":2,\"1577\":6,\"1581\":5,\"1582\":2,\"1583\":4,\"1585\":20,\"1586\":4,\"1588\":3,\"1589\":3,\"1590\":5,\"1592\":3,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":11,\"1599\":35,\"1600\":6,\"1601\":5,\"1602\":4,\"1603\":4,\"1604\":1,\"1605\":2,\"1606\":1,\"1607\":3,\"1608\":4,\"1609\":2,\"1610\":3,\"1611\":7,\"1612\":4,\"1613\":5,\"1614\":2,\"1615\":2,\"1616\":6,\"1617\":2,\"1618\":1,\"1619\":3,\"1620\":4,\"1621\":4,\"1622\":6,\"1624\":2,\"1625\":11,\"1626\":34,\"1627\":7,\"1628\":7,\"1629\":5,\"1632\":4,\"1633\":3,\"1637\":4,\"1656\":2,\"1668\":2,\"1671\":2,\"1700\":2,\"1704\":5,\"1705\":4,\"1706\":4,\"1707\":4,\"1708\":5,\"1709\":6,\"1710\":6,\"1711\":6,\"1712\":10,\"1713\":5,\"1714\":5,\"1715\":6,\"1716\":6,\"1730\":4,\"1733\":2,\"1736\":12,\"1737\":2,\"1749\":12,\"1750\":11,\"1753\":6,\"1754\":2,\"1758\":5,\"1764\":7,\"1768\":7,\"1770\":3,\"1771\":3,\"1779\":3,\"1786\":1,\"1788\":3,\"1795\":2,\"1801\":3,\"1803\":2,\"1810\":2,\"1812\":2,\"1814\":3,\"1815\":22,\"1816\":3,\"1818\":1,\"1839\":6,\"1847\":5,\"1849\":2,\"1851\":8,\"1854\":3,\"1856\":5,\"1858\":3,\"1868\":2,\"1888\":2,\"1901\":1,\"1903\":1,\"1905\":3,\"1907\":3,\"1908\":2,\"1919\":4,\"1928\":2,\"1960\":3,\"1961\":2,\"1991\":6,\"1992\":7,\"1993\":7,\"1995\":7,\"2129\":2,\"2151\":2,\"2187\":1,\"2202\":5,\"2213\":5,\"2214\":5,\"2215\":2,\"2219\":4,\"2223\":8,\"2226\":9,\"2227\":3,\"2228\":50,\"2229\":46,\"2231\":5,\"2235\":18,\"2236\":18,\"2237\":4,\"2239\":19,\"2240\":19,\"2241\":11,\"2245\":18,\"2310\":2,\"2319\":1,\"2334\":1,\"2408\":20,\"2411\":9,\"2412\":13,\"2413\":11,\"2423\":13,\"2424\":11,\"2425\":2,\"2429\":2,\"2430\":2,\"2431\":7,\"2432\":7,\"2433\":3,\"2446\":24,\"2447\":13,\"2448\":11,\"2486\":1,\"2488\":1}}],[\"bucket\",{\"0\":{\"2146\":1},\"1\":{\"2134\":3,\"2143\":1,\"2145\":1,\"2146\":3,\"2348\":1,\"2370\":2,\"2372\":1}}],[\"buckets\",{\"1\":{\"851\":1,\"2146\":2}}],[\"buckets=32\",{\"1\":{\"787\":1}}],[\"buffer=none\",{\"1\":{\"1045\":1,\"1267\":1}}],[\"buffer\",{\"0\":{\"1501\":1},\"1\":{\"1031\":1,\"1035\":1,\"1112\":1,\"1113\":1,\"1250\":1,\"1251\":1,\"1501\":1,\"1548\":1,\"1944\":1,\"1947\":1,\"2039\":8,\"2044\":1}}],[\"buffers\",{\"1\":{\"793\":1,\"1501\":2}}],[\"bunits\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1126\":1,\"1217\":1,\"1539\":1,\"1766\":1}}],[\"bundle=\",{\"1\":{\"92\":1}}],[\"bundle=your\",{\"1\":{\"92\":1}}],[\"burden\",{\"1\":{\"262\":1}}],[\"bug\",{\"1\":{\"106\":1}}],[\"button\",{\"1\":{\"247\":1}}],[\"buttons\",{\"1\":{\"246\":1,\"247\":1}}],[\"but\",{\"1\":{\"66\":1,\"68\":1,\"71\":1,\"84\":1,\"91\":1,\"98\":1,\"106\":1,\"110\":1,\"132\":1,\"133\":1,\"141\":1,\"143\":1,\"148\":1,\"152\":1,\"159\":1,\"162\":1,\"197\":1,\"223\":1,\"232\":1,\"240\":1,\"242\":2,\"243\":2,\"259\":2,\"262\":1,\"266\":1,\"267\":1,\"275\":1,\"286\":5,\"287\":3,\"722\":1,\"821\":1,\"1008\":2,\"1147\":2,\"1224\":1,\"1225\":1,\"1397\":1,\"1484\":1,\"1526\":2,\"1553\":2,\"1598\":2,\"1600\":2,\"1625\":2,\"1711\":1,\"1729\":1,\"1730\":1,\"1742\":1,\"1753\":1,\"1755\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1,\"2000\":1,\"2001\":1,\"2016\":1,\"2354\":2,\"2355\":4,\"2447\":1,\"2474\":1}}],[\"builtin\",{\"1\":{\"635\":2,\"652\":2,\"706\":2,\"1118\":2,\"1251\":1,\"1269\":3,\"1270\":3,\"1271\":2,\"1318\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1}}],[\"built\",{\"1\":{\"2\":1,\"22\":2,\"26\":2,\"79\":1,\"91\":1}}],[\"builds\",{\"0\":{\"26\":1},\"1\":{\"26\":3,\"152\":1}}],[\"build\",{\"0\":{\"152\":1,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"869\":1,\"1630\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"2007\":2,\"2008\":2,\"2068\":1,\"2069\":1,\"2148\":1,\"2293\":2,\"2483\":2},\"1\":{\"1\":1,\"3\":6,\"19\":1,\"22\":1,\"25\":1,\"26\":6,\"27\":1,\"78\":5,\"79\":1,\"82\":3,\"91\":2,\"141\":1,\"150\":2,\"160\":1,\"161\":1,\"194\":1,\"228\":1,\"289\":1,\"655\":2,\"656\":2,\"657\":2,\"658\":2,\"659\":2,\"660\":2,\"661\":2,\"662\":2,\"675\":2,\"869\":1,\"1642\":1,\"1644\":2,\"1645\":2,\"1647\":2,\"1648\":2,\"1650\":2,\"1863\":2,\"1864\":2,\"1865\":2,\"1866\":2,\"1867\":2,\"2007\":2,\"2008\":2,\"2131\":6,\"2134\":1,\"2142\":4,\"2148\":1,\"2152\":1,\"2246\":4,\"2247\":5,\"2248\":4,\"2249\":25,\"2250\":4,\"2251\":4,\"2252\":4,\"2253\":9,\"2254\":5,\"2255\":5,\"2256\":5,\"2257\":4,\"2259\":4,\"2260\":4,\"2261\":4,\"2262\":6,\"2263\":5,\"2264\":4,\"2265\":2,\"2266\":4,\"2267\":4,\"2268\":5,\"2269\":4,\"2270\":5,\"2271\":5,\"2272\":4,\"2273\":5,\"2293\":2,\"2325\":1,\"2327\":1,\"2338\":2,\"2347\":2,\"2369\":2,\"2371\":2,\"2483\":3}}],[\"building\",{\"0\":{\"1\":1,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1},\"1\":{\"0\":1,\"26\":1,\"79\":1,\"150\":2,\"245\":1,\"261\":1,\"262\":1,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"768\":1,\"1108\":1,\"1155\":1,\"1157\":1}}],[\"bi\",{\"1\":{\"2280\":1}}],[\"bidim\",{\"1\":{\"1054\":1,\"1126\":1}}],[\"bidirectional=true\",{\"1\":{\"1029\":1,\"1059\":1,\"1124\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1185\":1,\"1259\":1,\"1279\":1,\"1281\":1}}],[\"bidirectional=false\",{\"1\":{\"817\":1,\"1124\":1,\"1176\":1,\"1202\":1,\"1215\":1,\"1255\":1,\"1257\":1}}],[\"bidirectionality\",{\"1\":{\"821\":1}}],[\"bidirectional\",{\"1\":{\"798\":3,\"817\":1,\"862\":3,\"1029\":2,\"1117\":2,\"1118\":2,\"1124\":1,\"1125\":2,\"1130\":2,\"1131\":2,\"1133\":3,\"1134\":2,\"1136\":3,\"1137\":2,\"1139\":2,\"1141\":3,\"1176\":1,\"1185\":3,\"1202\":2,\"1208\":3,\"1232\":2,\"1252\":3,\"1255\":2,\"1257\":2,\"1259\":2,\"1279\":2,\"1280\":3,\"1281\":2,\"1283\":3,\"1814\":1,\"1816\":1,\"2235\":2,\"2236\":2}}],[\"bilinear\",{\"1\":{\"823\":1}}],[\"billions\",{\"1\":{\"2151\":1,\"2310\":1}}],[\"billion\",{\"1\":{\"242\":1,\"2151\":1,\"2310\":1}}],[\"bicubic\",{\"1\":{\"243\":1,\"833\":1,\"1670\":1,\"1699\":1}}],[\"biggan\",{\"1\":{\"1211\":1}}],[\"big\",{\"1\":{\"70\":1,\"78\":1,\"696\":1,\"785\":6,\"786\":2,\"866\":2,\"882\":7,\"883\":7,\"884\":8,\"921\":2,\"922\":7}}],[\"bits\",{\"1\":{\"1677\":1,\"1678\":1}}],[\"bit\",{\"1\":{\"70\":1,\"242\":1,\"537\":2,\"927\":1,\"1678\":1,\"1881\":1,\"2065\":1,\"2355\":1}}],[\"biz\",{\"1\":{\"67\":1}}],[\"bias=none\",{\"1\":{\"851\":1}}],[\"bias=false\",{\"1\":{\"787\":1,\"1105\":1,\"1288\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1}}],[\"bias=true\",{\"1\":{\"722\":1,\"769\":1,\"787\":1,\"856\":1,\"1029\":1,\"1064\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1110\":1,\"1187\":1,\"1219\":1,\"1235\":1,\"1262\":1,\"1281\":1,\"1282\":1,\"1290\":1,\"1303\":1,\"1304\":1,\"1346\":1,\"1347\":1,\"1548\":1,\"1733\":1,\"1747\":1,\"1849\":1}}],[\"biases=true\",{\"1\":{\"1114\":1,\"1200\":1,\"1286\":1}}],[\"biases\",{\"0\":{\"92\":1},\"1\":{\"92\":1,\"1683\":1}}],[\"bias\",{\"0\":{\"646\":1,\"647\":1},\"1\":{\"43\":4,\"141\":2,\"142\":3,\"620\":3,\"632\":1,\"633\":3,\"634\":3,\"646\":3,\"647\":4,\"674\":2,\"702\":1,\"712\":1,\"754\":1,\"769\":1,\"787\":4,\"846\":3,\"851\":1,\"854\":1,\"1029\":2,\"1064\":2,\"1235\":2,\"1262\":2,\"1280\":3,\"1281\":2,\"1282\":2,\"1290\":3,\"1389\":1,\"1390\":1,\"1401\":2,\"1402\":2,\"1408\":2,\"1409\":2,\"1420\":1,\"1442\":1,\"1444\":1,\"1466\":1,\"1467\":1,\"1513\":3,\"1526\":3,\"1548\":2,\"1549\":2,\"1551\":3,\"1553\":2,\"1579\":1,\"1592\":3,\"1593\":1,\"1594\":1,\"1595\":2,\"1596\":3,\"1597\":3,\"1598\":3,\"1599\":3,\"1600\":3,\"1604\":3,\"1605\":3,\"1606\":3,\"1609\":3,\"1610\":3,\"1611\":3,\"1612\":3,\"1613\":3,\"1614\":3,\"1615\":3,\"1618\":1,\"1619\":3,\"1620\":3,\"1621\":3,\"1625\":2,\"1628\":3,\"1683\":6,\"1736\":3,\"1756\":2,\"1757\":2,\"1789\":2,\"1790\":2,\"1957\":2,\"1963\":3,\"2427\":3,\"2433\":1,\"2460\":1}}],[\"bind\",{\"1\":{\"1962\":1,\"2380\":1}}],[\"binomial\",{\"1\":{\"1577\":1}}],[\"binarization\",{\"1\":{\"1599\":1,\"1637\":1}}],[\"binaries\",{\"1\":{\"161\":1}}],[\"binary\",{\"0\":{\"948\":1},\"1\":{\"947\":1,\"948\":2,\"949\":1,\"1839\":1,\"1991\":1,\"2065\":5}}],[\"bins`\",{\"1\":{\"195\":1}}],[\"bins30000000\",{\"1\":{\"136\":3}}],[\"bins\",{\"1\":{\"48\":4,\"95\":2,\"99\":4,\"100\":5,\"101\":1,\"290\":1,\"449\":4,\"1061\":1,\"1062\":1,\"1131\":1,\"1132\":1,\"1280\":1,\"1283\":1,\"1316\":2,\"1320\":1,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1406\":1,\"1408\":1,\"1410\":1,\"1419\":1,\"1441\":2,\"1466\":1,\"1468\":1,\"1514\":1,\"1533\":1,\"1581\":3,\"1692\":4,\"2000\":9,\"2001\":6,\"2003\":1,\"2004\":1,\"2007\":2,\"2008\":1,\"2258\":2}}],[\"bin\",{\"0\":{\"2149\":1,\"2150\":1,\"2152\":1,\"2153\":1,\"2156\":1,\"2157\":1,\"2158\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2165\":1,\"2166\":1,\"2509\":1},\"1\":{\"3\":3,\"31\":1,\"40\":1,\"48\":1,\"56\":2,\"57\":1,\"58\":2,\"61\":3,\"62\":1,\"63\":1,\"64\":1,\"79\":1,\"80\":1,\"84\":5,\"85\":2,\"86\":2,\"87\":1,\"88\":6,\"89\":1,\"90\":1,\"91\":1,\"92\":1,\"93\":2,\"94\":4,\"96\":1,\"97\":1,\"98\":2,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":1,\"117\":1,\"119\":1,\"126\":1,\"137\":1,\"168\":2,\"220\":1,\"223\":7,\"228\":2,\"243\":1,\"263\":2,\"290\":11,\"1061\":1,\"1117\":1,\"1130\":1,\"1131\":1,\"1316\":1,\"1558\":1,\"1634\":2,\"1635\":2,\"1636\":2,\"2065\":1,\"2149\":1,\"2150\":1,\"2152\":1,\"2153\":1,\"2156\":1,\"2157\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2166\":1,\"2354\":2,\"2435\":1}}],[\"bhiksha\",{\"1\":{\"15\":1}}],[\"blue\",{\"1\":{\"1760\":1}}],[\"blind\",{\"0\":{\"1293\":1},\"1\":{\"1293\":2,\"1318\":1}}],[\"bleu=false\",{\"1\":{\"1760\":1}}],[\"bleu\",{\"0\":{\"530\":1},\"1\":{\"261\":2,\"541\":1,\"737\":1,\"1760\":8,\"1959\":1,\"1975\":1,\"2221\":1}}],[\"blog\",{\"1\":{\"705\":1,\"804\":1,\"932\":1,\"934\":1}}],[\"blob\",{\"1\":{\"195\":1,\"201\":1,\"289\":1,\"290\":2,\"536\":1,\"669\":1,\"670\":1,\"747\":1,\"750\":1,\"1053\":2,\"1153\":1,\"1308\":1,\"1316\":1,\"1320\":1,\"1332\":1,\"1350\":1,\"1513\":1,\"1548\":1,\"1551\":1,\"1592\":1,\"1605\":1,\"1606\":1,\"1945\":1,\"2018\":1,\"2151\":1,\"2198\":1,\"2286\":1,\"2310\":1}}],[\"blocking=false\",{\"1\":{\"2164\":1,\"2323\":1}}],[\"blockidx\",{\"1\":{\"803\":1}}],[\"blockdrop\",{\"1\":{\"141\":1,\"636\":2,\"661\":2}}],[\"blockwise\",{\"1\":{\"129\":1,\"130\":2,\"261\":4,\"1720\":1,\"1721\":1}}],[\"blocked\",{\"1\":{\"43\":2,\"141\":1,\"620\":1,\"1736\":1}}],[\"block\",{\"0\":{\"150\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"672\":1,\"673\":1,\"710\":1,\"711\":1,\"830\":1,\"893\":1,\"894\":1,\"940\":1,\"1578\":1,\"1579\":1,\"1614\":1,\"1620\":1,\"1621\":1,\"1735\":1,\"1864\":1,\"1865\":1,\"1867\":1,\"1937\":1,\"2036\":1,\"2168\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2185\":1,\"2200\":1},\"1\":{\"43\":16,\"130\":8,\"141\":21,\"142\":5,\"150\":9,\"617\":10,\"618\":10,\"621\":1,\"624\":10,\"630\":7,\"634\":2,\"636\":11,\"642\":3,\"643\":2,\"649\":7,\"656\":4,\"657\":4,\"658\":4,\"659\":4,\"660\":4,\"661\":1,\"662\":3,\"671\":4,\"672\":10,\"673\":6,\"692\":1,\"709\":1,\"710\":9,\"711\":10,\"745\":1,\"747\":1,\"748\":1,\"768\":5,\"771\":1,\"774\":1,\"780\":2,\"787\":1,\"804\":1,\"807\":1,\"830\":2,\"846\":1,\"849\":1,\"893\":1,\"894\":1,\"927\":3,\"932\":1,\"934\":1,\"940\":1,\"978\":2,\"982\":1,\"1029\":2,\"1055\":1,\"1057\":1,\"1062\":1,\"1107\":1,\"1108\":1,\"1133\":2,\"1137\":1,\"1139\":1,\"1141\":1,\"1145\":5,\"1147\":1,\"1168\":1,\"1208\":1,\"1235\":4,\"1262\":1,\"1264\":1,\"1265\":1,\"1270\":1,\"1273\":1,\"1274\":1,\"1278\":1,\"1279\":2,\"1280\":5,\"1281\":2,\"1282\":3,\"1283\":2,\"1386\":1,\"1458\":1,\"1460\":1,\"1516\":2,\"1543\":1,\"1552\":13,\"1560\":1,\"1561\":1,\"1578\":1,\"1579\":1,\"1599\":2,\"1610\":1,\"1612\":2,\"1614\":2,\"1620\":1,\"1621\":1,\"1626\":13,\"1628\":1,\"1720\":7,\"1721\":7,\"1735\":3,\"1737\":1,\"1749\":3,\"1751\":1,\"1759\":1,\"1795\":2,\"1854\":1,\"1863\":2,\"1864\":6,\"1865\":7,\"1866\":2,\"1867\":6,\"1914\":2,\"1937\":10,\"1992\":1,\"1995\":1,\"2129\":1,\"2168\":2,\"2177\":1,\"2179\":1,\"2181\":1,\"2185\":1,\"2187\":3,\"2191\":1,\"2192\":3,\"2198\":4,\"2200\":1,\"2203\":3,\"2239\":2,\"2240\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2426\":1,\"2432\":2,\"2447\":2}}],[\"blocks=4\",{\"1\":{\"1281\":1}}],[\"blocks=3\",{\"1\":{\"1279\":1,\"1282\":1}}],[\"blocks=6\",{\"1\":{\"1279\":1}}],[\"blocks=7\",{\"1\":{\"1264\":1,\"1334\":1}}],[\"blocks=1\",{\"1\":{\"1145\":1}}],[\"blocks=2\",{\"1\":{\"1145\":2,\"1211\":1,\"1259\":1,\"1281\":1,\"1282\":1}}],[\"blocks\",{\"0\":{\"617\":1,\"618\":1,\"620\":1,\"621\":1,\"624\":1,\"633\":1,\"636\":1,\"642\":1,\"655\":1,\"1863\":2,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1891\":1,\"1913\":1,\"1914\":1,\"1937\":1},\"1\":{\"3\":1,\"43\":3,\"141\":6,\"142\":7,\"150\":3,\"243\":2,\"617\":1,\"618\":1,\"620\":1,\"621\":1,\"624\":1,\"630\":6,\"633\":1,\"634\":3,\"636\":2,\"642\":4,\"643\":4,\"649\":6,\"655\":4,\"671\":1,\"692\":2,\"700\":1,\"703\":1,\"709\":3,\"710\":3,\"711\":3,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"748\":3,\"755\":1,\"766\":1,\"767\":1,\"774\":3,\"775\":1,\"780\":3,\"785\":2,\"804\":1,\"848\":1,\"849\":6,\"850\":1,\"927\":3,\"932\":1,\"934\":1,\"978\":1,\"982\":3,\"1064\":1,\"1091\":1,\"1107\":1,\"1133\":2,\"1145\":4,\"1153\":1,\"1162\":1,\"1230\":1,\"1240\":1,\"1252\":1,\"1259\":2,\"1261\":1,\"1264\":3,\"1269\":1,\"1270\":1,\"1271\":1,\"1273\":4,\"1274\":4,\"1278\":1,\"1279\":4,\"1280\":6,\"1281\":4,\"1282\":4,\"1283\":6,\"1334\":3,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1513\":3,\"1519\":3,\"1535\":3,\"1536\":3,\"1546\":3,\"1548\":3,\"1551\":3,\"1552\":3,\"1553\":1,\"1592\":3,\"1596\":1,\"1599\":3,\"1622\":3,\"1625\":1,\"1626\":3,\"1683\":1,\"1863\":8,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1891\":1,\"1913\":5,\"1914\":4,\"1937\":1,\"1992\":3,\"1995\":3,\"2126\":1,\"2129\":3,\"2191\":3,\"2198\":3,\"2239\":1}}],[\"blayers\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1126\":1,\"1217\":1,\"1539\":1,\"1766\":1}}],[\"blas\",{\"1\":{\"161\":1}}],[\"blank=0\",{\"1\":{\"800\":1,\"935\":1}}],[\"blanks\",{\"1\":{\"295\":2,\"415\":2,\"696\":1,\"785\":3,\"882\":4,\"883\":4,\"884\":5,\"922\":4}}],[\"blank\",{\"0\":{\"703\":1,\"704\":1,\"716\":1,\"717\":1,\"755\":1,\"764\":1,\"773\":1,\"785\":1,\"786\":2,\"800\":2,\"801\":1,\"802\":1,\"803\":1,\"866\":2,\"867\":2,\"868\":1,\"873\":2,\"874\":2,\"875\":2,\"876\":2,\"878\":1,\"879\":1,\"880\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"886\":1,\"888\":1,\"896\":1,\"897\":1,\"899\":1,\"908\":1,\"909\":1,\"916\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":2,\"922\":1,\"923\":1,\"931\":1,\"933\":1,\"935\":2,\"936\":1,\"937\":1,\"940\":1,\"946\":1},\"1\":{\"45\":1,\"211\":1,\"213\":1,\"276\":1,\"295\":4,\"415\":4,\"469\":2,\"625\":4,\"627\":3,\"651\":1,\"667\":3,\"696\":13,\"703\":5,\"704\":1,\"706\":3,\"716\":2,\"717\":5,\"736\":1,\"737\":1,\"740\":3,\"755\":5,\"764\":1,\"773\":1,\"777\":1,\"785\":11,\"786\":9,\"800\":4,\"801\":1,\"802\":1,\"803\":1,\"804\":1,\"847\":1,\"866\":8,\"867\":3,\"868\":1,\"873\":2,\"874\":2,\"875\":2,\"876\":2,\"878\":6,\"879\":6,\"880\":1,\"881\":6,\"882\":11,\"883\":11,\"884\":11,\"886\":1,\"888\":1,\"896\":1,\"897\":1,\"899\":1,\"908\":1,\"909\":1,\"916\":1,\"918\":1,\"919\":2,\"920\":1,\"921\":10,\"922\":12,\"923\":1,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"935\":4,\"936\":4,\"937\":4,\"940\":1,\"946\":1,\"1589\":3,\"1640\":1,\"1726\":3,\"1727\":3,\"1729\":1,\"1730\":3,\"1749\":3,\"1815\":3,\"1888\":2,\"1934\":1,\"1959\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2221\":2}}],[\"blacklist\",{\"1\":{\"67\":1}}],[\"black\",{\"1\":{\"12\":1,\"32\":5,\"67\":1,\"225\":1,\"830\":3,\"1502\":1}}],[\"blstmp\",{\"1\":{\"720\":2,\"738\":2,\"815\":2,\"1126\":1,\"1127\":1,\"1217\":2,\"1539\":2,\"1766\":2}}],[\"blstm\",{\"1\":{\"43\":1,\"1061\":1,\"1062\":1,\"1117\":2,\"1118\":1,\"1124\":1,\"1125\":1,\"1130\":2,\"1131\":2,\"1176\":1,\"1232\":2,\"1814\":1,\"1816\":1,\"2245\":2,\"2431\":2}}],[\"bosswissam\",{\"1\":{\"2489\":1}}],[\"bos\",{\"1\":{\"2143\":1}}],[\"bound=1\",{\"1\":{\"1634\":1,\"1636\":1}}],[\"bound\",{\"1\":{\"1119\":1,\"1581\":3,\"1676\":2}}],[\"bonjour\",{\"1\":{\"287\":1}}],[\"bonus\",{\"0\":{\"1787\":1},\"1\":{\"262\":1,\"1787\":2,\"1822\":1}}],[\"bong\",{\"1\":{\"256\":1}}],[\"bonding\",{\"1\":{\"67\":1}}],[\"bond\",{\"1\":{\"67\":2}}],[\"boxcar\",{\"1\":{\"2378\":1}}],[\"box\",{\"1\":{\"247\":1,\"830\":3}}],[\"bottle2neck\",{\"0\":{\"2179\":1},\"1\":{\"2179\":1,\"2192\":1,\"2203\":1}}],[\"bottleneck=none\",{\"1\":{\"817\":1}}],[\"bottleneck\",{\"0\":{\"2181\":1},\"1\":{\"173\":1,\"819\":1,\"954\":1,\"969\":1,\"974\":3,\"978\":5,\"980\":5,\"982\":2,\"1267\":3,\"1268\":3,\"1273\":1,\"1274\":1,\"1279\":3,\"1280\":3,\"1281\":3,\"1282\":3,\"1283\":3,\"1657\":1,\"1682\":1,\"1684\":1,\"1851\":1,\"2181\":1,\"2198\":1,\"2200\":1}}],[\"bottom=0\",{\"1\":{\"1635\":1}}],[\"both\",{\"1\":{\"50\":1,\"94\":1,\"106\":1,\"127\":1,\"133\":1,\"200\":2,\"205\":1,\"211\":1,\"235\":2,\"259\":1,\"262\":1,\"267\":1,\"276\":1,\"286\":2,\"614\":1,\"629\":1,\"635\":1,\"650\":1,\"652\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"692\":1,\"693\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"706\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"715\":1,\"718\":1,\"720\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"741\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"751\":1,\"752\":1,\"754\":1,\"755\":1,\"757\":1,\"759\":1,\"766\":1,\"767\":1,\"771\":1,\"774\":1,\"775\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"785\":1,\"786\":1,\"787\":1,\"788\":1,\"790\":1,\"791\":1,\"793\":1,\"794\":1,\"796\":1,\"798\":1,\"800\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"820\":1,\"821\":1,\"823\":1,\"825\":1,\"828\":1,\"829\":1,\"830\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"859\":1,\"860\":1,\"862\":1,\"864\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"979\":1,\"981\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1063\":1,\"1064\":2,\"1065\":1,\"1066\":1,\"1068\":1,\"1072\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":2,\"1080\":1,\"1084\":1,\"1086\":3,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1126\":1,\"1127\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":2,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1159\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1177\":1,\"1179\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":3,\"1205\":1,\"1207\":3,\"1208\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":1,\"1219\":1,\"1222\":1,\"1223\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":2,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":2,\"1261\":2,\"1262\":2,\"1264\":1,\"1265\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1279\":1,\"1280\":3,\"1281\":1,\"1282\":1,\"1283\":3,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":3,\"1333\":1,\"1334\":1,\"1381\":1,\"1383\":1,\"1387\":1,\"1392\":1,\"1398\":1,\"1400\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1417\":1,\"1419\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1508\":1,\"1509\":1,\"1511\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1533\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1547\":1,\"1554\":1,\"1576\":1,\"1588\":1,\"1590\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1638\":1,\"1640\":1,\"1641\":1,\"1652\":1,\"1656\":2,\"1657\":1,\"1660\":2,\"1662\":2,\"1664\":2,\"1665\":2,\"1667\":1,\"1669\":2,\"1670\":2,\"1671\":2,\"1702\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1833\":1,\"1838\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1959\":1,\"1965\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"2000\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2044\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2132\":1,\"2136\":2,\"2139\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2221\":1,\"2222\":1,\"2232\":2,\"2238\":2,\"2287\":2,\"2305\":1,\"2325\":1,\"2327\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2407\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2434\":1,\"2443\":1,\"2445\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1}}],[\"boyer\",{\"1\":{\"145\":1}}],[\"body\",{\"0\":{\"655\":1,\"1913\":1},\"1\":{\"141\":4,\"150\":1,\"626\":3,\"655\":3,\"671\":4,\"673\":3,\"1913\":2}}],[\"bootphon\",{\"1\":{\"287\":13,\"290\":2,\"2286\":2}}],[\"bootstrap\",{\"1\":{\"66\":1}}],[\"booktitle\",{\"1\":{\"156\":1,\"202\":1}}],[\"booktitle=\",{\"1\":{\"5\":1,\"6\":3,\"7\":1,\"8\":1,\"9\":1,\"10\":2,\"11\":2,\"12\":1,\"13\":1,\"14\":1,\"15\":1,\"16\":1,\"207\":1,\"256\":1}}],[\"boosting\",{\"1\":{\"2176\":1}}],[\"boost\",{\"1\":{\"128\":1,\"1687\":1}}],[\"booleans\",{\"1\":{\"756\":1,\"773\":1,\"866\":1,\"867\":1}}],[\"bool=\",{\"1\":{\"674\":14,\"2340\":2}}],[\"bool\",{\"1\":{\"43\":8,\"78\":2,\"81\":2,\"141\":4,\"147\":1,\"616\":2,\"619\":1,\"620\":4,\"621\":1,\"622\":1,\"623\":1,\"625\":4,\"627\":2,\"632\":2,\"635\":1,\"639\":3,\"644\":1,\"652\":1,\"658\":1,\"661\":2,\"663\":1,\"665\":1,\"674\":14,\"692\":10,\"696\":2,\"697\":2,\"699\":3,\"700\":6,\"702\":3,\"703\":2,\"706\":3,\"709\":16,\"710\":7,\"711\":5,\"712\":1,\"713\":1,\"715\":1,\"716\":2,\"720\":5,\"731\":3,\"732\":3,\"733\":6,\"734\":8,\"736\":4,\"737\":5,\"738\":5,\"740\":2,\"745\":1,\"746\":6,\"747\":2,\"748\":2,\"759\":1,\"760\":6,\"765\":1,\"766\":3,\"767\":3,\"771\":1,\"774\":13,\"775\":3,\"777\":4,\"778\":2,\"780\":13,\"781\":2,\"783\":2,\"787\":7,\"790\":1,\"791\":2,\"796\":2,\"798\":2,\"803\":1,\"804\":1,\"815\":1,\"820\":3,\"833\":4,\"846\":7,\"847\":1,\"848\":5,\"849\":2,\"850\":7,\"851\":6,\"854\":2,\"862\":2,\"864\":1,\"869\":1,\"908\":1,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"939\":1,\"958\":1,\"976\":1,\"979\":1,\"980\":2,\"991\":1,\"993\":1,\"1007\":2,\"1009\":1,\"1026\":1,\"1028\":2,\"1029\":4,\"1037\":1,\"1061\":1,\"1062\":4,\"1064\":2,\"1065\":2,\"1107\":9,\"1117\":1,\"1118\":10,\"1119\":1,\"1124\":2,\"1125\":5,\"1126\":5,\"1127\":5,\"1130\":2,\"1131\":1,\"1133\":4,\"1134\":1,\"1136\":3,\"1137\":1,\"1139\":1,\"1141\":4,\"1147\":1,\"1153\":1,\"1155\":1,\"1156\":1,\"1157\":6,\"1158\":3,\"1162\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":4,\"1175\":1,\"1176\":2,\"1185\":1,\"1202\":1,\"1208\":2,\"1209\":1,\"1210\":1,\"1217\":11,\"1228\":2,\"1232\":2,\"1235\":3,\"1246\":1,\"1247\":1,\"1250\":3,\"1251\":4,\"1252\":4,\"1255\":1,\"1257\":1,\"1259\":2,\"1261\":5,\"1262\":1,\"1267\":4,\"1268\":4,\"1276\":3,\"1278\":7,\"1279\":1,\"1280\":6,\"1281\":3,\"1282\":2,\"1283\":2,\"1290\":1,\"1309\":2,\"1310\":2,\"1311\":2,\"1315\":1,\"1318\":2,\"1319\":2,\"1321\":2,\"1322\":2,\"1323\":2,\"1326\":1,\"1327\":4,\"1329\":1,\"1330\":4,\"1356\":1,\"1368\":1,\"1377\":2,\"1382\":1,\"1385\":4,\"1389\":6,\"1390\":1,\"1391\":6,\"1392\":2,\"1395\":2,\"1396\":5,\"1400\":1,\"1401\":6,\"1402\":2,\"1403\":7,\"1408\":7,\"1409\":1,\"1410\":3,\"1419\":8,\"1422\":2,\"1424\":1,\"1428\":1,\"1430\":1,\"1441\":3,\"1442\":2,\"1444\":2,\"1446\":1,\"1448\":1,\"1450\":4,\"1452\":4,\"1454\":4,\"1456\":4,\"1458\":4,\"1460\":4,\"1462\":1,\"1466\":7,\"1467\":2,\"1468\":5,\"1469\":3,\"1485\":1,\"1513\":6,\"1519\":6,\"1521\":2,\"1526\":12,\"1529\":3,\"1534\":1,\"1535\":6,\"1536\":6,\"1539\":1,\"1546\":7,\"1548\":5,\"1549\":3,\"1551\":6,\"1552\":19,\"1553\":6,\"1577\":1,\"1579\":1,\"1581\":2,\"1584\":2,\"1585\":2,\"1586\":2,\"1587\":6,\"1588\":2,\"1591\":2,\"1592\":6,\"1594\":2,\"1595\":2,\"1596\":7,\"1597\":5,\"1598\":8,\"1599\":30,\"1600\":10,\"1603\":2,\"1604\":2,\"1605\":5,\"1606\":4,\"1607\":8,\"1609\":4,\"1610\":6,\"1611\":4,\"1612\":8,\"1613\":8,\"1614\":4,\"1615\":2,\"1616\":2,\"1618\":2,\"1619\":4,\"1620\":2,\"1621\":2,\"1622\":6,\"1625\":8,\"1626\":16,\"1627\":4,\"1628\":12,\"1640\":3,\"1642\":1,\"1643\":3,\"1644\":1,\"1645\":3,\"1646\":3,\"1647\":1,\"1648\":2,\"1650\":4,\"1656\":2,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":3,\"1671\":2,\"1672\":2,\"1691\":1,\"1700\":2,\"1702\":2,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1719\":4,\"1720\":1,\"1721\":4,\"1725\":4,\"1726\":1,\"1727\":5,\"1735\":2,\"1736\":3,\"1748\":1,\"1749\":1,\"1750\":4,\"1751\":3,\"1756\":2,\"1757\":2,\"1758\":2,\"1759\":2,\"1764\":2,\"1766\":3,\"1768\":1,\"1770\":1,\"1771\":2,\"1779\":2,\"1782\":1,\"1784\":1,\"1785\":1,\"1789\":2,\"1790\":2,\"1794\":7,\"1806\":1,\"1808\":1,\"1810\":1,\"1815\":1,\"1817\":1,\"1839\":2,\"1843\":1,\"1860\":1,\"1871\":1,\"1872\":1,\"1881\":1,\"1883\":1,\"1895\":1,\"1897\":1,\"1901\":1,\"1903\":1,\"1921\":1,\"1926\":1,\"1936\":1,\"1942\":1,\"1945\":1,\"1948\":1,\"1949\":1,\"1951\":1,\"1955\":1,\"1959\":5,\"1962\":1,\"1965\":1,\"1973\":1,\"1975\":5,\"1976\":2,\"1978\":3,\"1979\":1,\"1980\":4,\"1981\":1,\"1982\":3,\"1983\":1,\"1992\":6,\"1993\":11,\"1994\":5,\"1995\":5,\"1996\":4,\"1997\":4,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":2,\"2004\":2,\"2005\":1,\"2006\":1,\"2007\":2,\"2008\":2,\"2015\":2,\"2044\":1,\"2065\":2,\"2126\":5,\"2127\":8,\"2129\":2,\"2130\":1,\"2134\":2,\"2136\":1,\"2167\":1,\"2176\":2,\"2184\":2,\"2191\":8,\"2216\":2,\"2217\":2,\"2220\":4,\"2221\":6,\"2223\":3,\"2224\":2,\"2226\":3,\"2231\":2,\"2232\":1,\"2235\":10,\"2236\":13,\"2237\":2,\"2238\":1,\"2239\":33,\"2240\":22,\"2241\":4,\"2245\":22,\"2246\":6,\"2247\":6,\"2248\":6,\"2249\":12,\"2250\":6,\"2251\":6,\"2252\":6,\"2253\":6,\"2254\":6,\"2255\":6,\"2256\":6,\"2257\":6,\"2258\":6,\"2259\":6,\"2260\":6,\"2261\":6,\"2262\":6,\"2263\":6,\"2264\":6,\"2265\":5,\"2266\":6,\"2267\":6,\"2268\":6,\"2269\":6,\"2270\":6,\"2271\":6,\"2272\":6,\"2273\":6,\"2275\":1,\"2276\":1,\"2280\":2,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2292\":1,\"2293\":2,\"2298\":2,\"2313\":1,\"2314\":1,\"2318\":1,\"2320\":1,\"2324\":1,\"2327\":2,\"2328\":1,\"2331\":1,\"2334\":1,\"2336\":4,\"2337\":3,\"2339\":2,\"2340\":2,\"2341\":1,\"2342\":1,\"2343\":1,\"2344\":1,\"2346\":6,\"2347\":1,\"2348\":14,\"2350\":6,\"2352\":1,\"2353\":4,\"2354\":3,\"2356\":1,\"2359\":2,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2364\":2,\"2365\":1,\"2366\":1,\"2368\":8,\"2369\":2,\"2370\":24,\"2371\":1,\"2372\":14,\"2378\":1,\"2379\":2,\"2388\":1,\"2389\":1,\"2404\":3,\"2409\":4,\"2411\":24,\"2412\":26,\"2413\":4,\"2414\":3,\"2416\":4,\"2418\":3,\"2423\":24,\"2424\":4,\"2428\":1,\"2431\":20,\"2432\":23,\"2433\":1,\"2434\":1,\"2447\":25,\"2448\":4,\"2473\":1,\"2481\":1,\"2498\":1}}],[\"boeddeker\",{\"1\":{\"11\":1,\"1066\":1}}],[\"babel\",{\"1\":{\"2000\":1}}],[\"babble\",{\"1\":{\"746\":1}}],[\"balancing\",{\"1\":{\"2000\":4,\"2001\":1}}],[\"balanced\",{\"0\":{\"1999\":1,\"2009\":1},\"1\":{\"1644\":1,\"1645\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2009\":1}}],[\"balance\",{\"1\":{\"1396\":1,\"1408\":1,\"1466\":1,\"2000\":1}}],[\"balancer\",{\"0\":{\"1382\":2,\"1394\":1},\"1\":{\"1382\":2,\"1394\":1,\"1396\":2,\"1408\":1,\"1466\":1}}],[\"bais\",{\"1\":{\"911\":1}}],[\"ban\",{\"1\":{\"1293\":2}}],[\"banks\",{\"1\":{\"1533\":1,\"1558\":1,\"1608\":1}}],[\"bank\",{\"1\":{\"768\":2,\"1533\":1,\"1654\":1,\"1666\":1}}],[\"bandreject\",{\"0\":{\"1673\":1},\"1\":{\"1673\":1}}],[\"bandpassperturbation\",{\"0\":{\"1717\":1},\"1\":{\"1717\":2}}],[\"bandpass\",{\"0\":{\"1672\":1},\"1\":{\"1672\":2}}],[\"bands\",{\"1\":{\"1316\":2,\"1320\":1,\"1389\":1,\"1390\":1,\"1413\":3,\"1420\":4,\"1662\":1}}],[\"bandsplit\",{\"0\":{\"1063\":1},\"1\":{\"1061\":1,\"1062\":2,\"1063\":2}}],[\"bandlimit=none\",{\"1\":{\"823\":1,\"824\":1}}],[\"bandwidths\",{\"1\":{\"1654\":1}}],[\"bandwidth\",{\"0\":{\"1674\":1},\"1\":{\"368\":2,\"1155\":1,\"1157\":1,\"1316\":1,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1406\":2,\"1408\":1,\"1410\":1,\"1432\":2,\"1441\":11,\"1466\":1,\"1468\":1,\"1654\":1,\"1674\":3}}],[\"band\",{\"1\":{\"286\":5,\"1061\":4,\"1062\":4,\"1269\":1,\"1270\":1,\"1271\":1,\"1320\":1,\"1389\":1,\"1390\":1,\"1413\":1,\"1420\":1,\"1509\":1,\"1511\":1,\"1515\":1,\"1516\":1,\"1526\":1,\"1541\":1,\"1543\":1,\"1553\":1,\"1558\":1,\"1600\":1,\"1668\":1,\"1673\":1,\"1717\":2}}],[\"bayesriskctc\",{\"0\":{\"695\":1}}],[\"bayes\",{\"0\":{\"695\":1,\"917\":1}}],[\"bayashi\",{\"1\":{\"267\":1,\"276\":1,\"286\":2,\"290\":2,\"1608\":1,\"2045\":2}}],[\"bak\",{\"1\":{\"286\":1,\"1128\":1}}],[\"baevski\",{\"1\":{\"207\":1}}],[\"bar=\",{\"1\":{\"2334\":1}}],[\"bark\",{\"1\":{\"1654\":5}}],[\"barkscale\",{\"0\":{\"1654\":1},\"1\":{\"1654\":1}}],[\"barbara\",{\"1\":{\"202\":1}}],[\"bar\",{\"1\":{\"85\":2,\"119\":1,\"168\":1,\"821\":1,\"1311\":2,\"2355\":1}}],[\"barry\",{\"1\":{\"8\":1}}],[\"barriers\",{\"1\":{\"7\":1}}],[\"badim\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1126\":1,\"1217\":1,\"1539\":1,\"1766\":1}}],[\"bad\",{\"1\":{\"84\":1,\"290\":1}}],[\"batchpartialscorerinterface\",{\"0\":{\"1723\":1},\"1\":{\"1723\":1,\"1731\":1}}],[\"batchhypothesis\",{\"0\":{\"1722\":1},\"1\":{\"1719\":13,\"1720\":1,\"1722\":2,\"1806\":9}}],[\"batchbeamsearchonlinesim\",{\"0\":{\"1721\":1},\"1\":{\"1721\":1}}],[\"batchbeamsearchonline\",{\"0\":{\"1720\":1},\"1\":{\"1720\":1}}],[\"batchbeamsearch\",{\"0\":{\"1719\":1},\"1\":{\"1719\":1,\"1720\":1,\"1721\":1,\"1806\":1}}],[\"batch=1\",{\"1\":{\"958\":1}}],[\"batched\",{\"1\":{\"831\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1,\"1577\":2,\"1590\":2,\"1629\":3,\"1637\":2,\"1806\":1,\"2130\":5,\"2215\":1,\"2219\":1}}],[\"batches\",{\"0\":{\"2162\":1},\"1\":{\"78\":1,\"96\":1,\"97\":1,\"100\":1,\"223\":1,\"777\":1,\"961\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1643\":1,\"1644\":3,\"1645\":4,\"1646\":1,\"1647\":4,\"1649\":1,\"1650\":1,\"1794\":1,\"1940\":1,\"1942\":1,\"2000\":2,\"2001\":3,\"2008\":1,\"2130\":1,\"2131\":1,\"2134\":6,\"2145\":2,\"2146\":1,\"2147\":4,\"2162\":11,\"2249\":3,\"2253\":3,\"2258\":2,\"2355\":2}}],[\"batchify\",{\"1\":{\"777\":2,\"1156\":2,\"1940\":1,\"1941\":1,\"1942\":1,\"1943\":1}}],[\"batching\",{\"1\":{\"235\":1,\"2134\":3,\"2145\":2,\"2146\":1,\"2147\":1}}],[\"batchfied\",{\"1\":{\"692\":1,\"760\":1,\"790\":1,\"820\":1,\"850\":1,\"1722\":1,\"1724\":1,\"1787\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1992\":1}}],[\"batchfy\",{\"0\":{\"2145\":1,\"2146\":1,\"2147\":1},\"1\":{\"48\":1,\"1719\":1,\"2134\":2,\"2143\":1,\"2145\":1,\"2146\":1,\"2147\":1}}],[\"batchscorerinterface\",{\"0\":{\"1724\":1},\"1\":{\"692\":1,\"760\":1,\"790\":1,\"820\":1,\"1723\":1,\"1724\":1,\"1787\":1,\"1798\":1,\"1938\":1,\"1992\":1}}],[\"batchsampler\",{\"1\":{\"96\":1,\"2005\":1,\"2006\":1,\"2007\":1}}],[\"batchsize=1\",{\"1\":{\"1545\":3}}],[\"batchsize\",{\"1\":{\"41\":1,\"527\":1}}],[\"batch\",{\"0\":{\"94\":1,\"95\":1,\"123\":1,\"1292\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1868\":1,\"1871\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":2,\"2008\":2,\"2023\":1,\"2086\":1,\"2112\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2162\":1},\"1\":{\"43\":2,\"47\":4,\"48\":20,\"55\":3,\"78\":1,\"79\":6,\"82\":4,\"84\":2,\"94\":13,\"95\":13,\"96\":3,\"97\":6,\"98\":9,\"99\":8,\"100\":6,\"101\":9,\"102\":8,\"123\":3,\"136\":3,\"141\":2,\"173\":3,\"220\":1,\"223\":2,\"243\":4,\"290\":5,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"368\":2,\"377\":2,\"385\":2,\"389\":2,\"396\":2,\"404\":2,\"406\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":10,\"457\":2,\"463\":2,\"469\":2,\"475\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":2,\"505\":6,\"614\":7,\"616\":3,\"620\":3,\"634\":7,\"641\":5,\"643\":7,\"651\":5,\"692\":22,\"700\":4,\"701\":6,\"702\":5,\"703\":3,\"706\":8,\"709\":4,\"712\":4,\"716\":3,\"724\":2,\"725\":2,\"726\":4,\"727\":2,\"728\":2,\"730\":1,\"733\":4,\"734\":4,\"735\":6,\"736\":4,\"737\":6,\"744\":2,\"749\":5,\"750\":2,\"755\":1,\"759\":2,\"760\":11,\"768\":2,\"770\":1,\"774\":4,\"775\":7,\"777\":16,\"780\":4,\"784\":18,\"785\":1,\"786\":5,\"787\":2,\"790\":17,\"794\":4,\"800\":5,\"819\":1,\"820\":14,\"823\":1,\"824\":1,\"828\":8,\"829\":6,\"830\":6,\"831\":2,\"847\":5,\"850\":20,\"859\":4,\"867\":4,\"878\":1,\"879\":1,\"881\":2,\"882\":1,\"883\":1,\"884\":2,\"919\":1,\"921\":5,\"935\":5,\"939\":5,\"947\":3,\"948\":2,\"949\":3,\"954\":5,\"955\":2,\"958\":10,\"959\":1,\"960\":4,\"961\":4,\"962\":3,\"971\":1,\"974\":6,\"975\":1,\"976\":4,\"977\":2,\"978\":4,\"979\":5,\"980\":1,\"982\":1,\"984\":1,\"1029\":2,\"1053\":3,\"1062\":4,\"1066\":3,\"1070\":2,\"1071\":2,\"1072\":1,\"1073\":2,\"1074\":1,\"1107\":4,\"1112\":2,\"1113\":3,\"1117\":4,\"1118\":4,\"1119\":2,\"1125\":7,\"1126\":1,\"1127\":1,\"1130\":4,\"1131\":1,\"1132\":2,\"1133\":2,\"1134\":1,\"1136\":4,\"1137\":1,\"1141\":4,\"1145\":1,\"1155\":9,\"1156\":27,\"1157\":8,\"1158\":9,\"1162\":5,\"1163\":2,\"1164\":2,\"1167\":2,\"1168\":1,\"1170\":5,\"1171\":5,\"1172\":3,\"1173\":5,\"1175\":5,\"1179\":1,\"1204\":2,\"1208\":3,\"1209\":2,\"1210\":3,\"1215\":1,\"1217\":8,\"1222\":2,\"1223\":2,\"1228\":2,\"1232\":4,\"1235\":4,\"1245\":1,\"1250\":4,\"1251\":5,\"1252\":4,\"1253\":2,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":4,\"1264\":2,\"1267\":4,\"1268\":4,\"1269\":1,\"1270\":1,\"1271\":1,\"1273\":1,\"1274\":1,\"1275\":5,\"1277\":5,\"1278\":4,\"1279\":2,\"1280\":5,\"1281\":2,\"1282\":2,\"1283\":5,\"1292\":2,\"1296\":1,\"1306\":2,\"1333\":2,\"1334\":7,\"1371\":2,\"1382\":1,\"1400\":3,\"1441\":1,\"1469\":1,\"1526\":20,\"1529\":2,\"1552\":30,\"1553\":20,\"1589\":3,\"1598\":1,\"1599\":7,\"1600\":1,\"1627\":7,\"1640\":8,\"1641\":8,\"1643\":1,\"1644\":4,\"1645\":8,\"1646\":1,\"1647\":3,\"1650\":1,\"1660\":3,\"1664\":1,\"1665\":1,\"1667\":6,\"1668\":2,\"1669\":10,\"1670\":2,\"1691\":1,\"1699\":1,\"1702\":5,\"1719\":7,\"1720\":1,\"1721\":1,\"1722\":1,\"1723\":6,\"1724\":7,\"1730\":1,\"1731\":2,\"1735\":10,\"1736\":3,\"1737\":2,\"1738\":4,\"1739\":4,\"1740\":4,\"1741\":4,\"1742\":4,\"1743\":4,\"1744\":4,\"1745\":5,\"1746\":4,\"1747\":2,\"1748\":2,\"1749\":5,\"1750\":16,\"1751\":11,\"1753\":6,\"1754\":2,\"1756\":5,\"1757\":5,\"1758\":7,\"1759\":5,\"1760\":5,\"1764\":7,\"1770\":3,\"1771\":3,\"1782\":2,\"1784\":2,\"1785\":8,\"1786\":2,\"1787\":5,\"1788\":2,\"1789\":5,\"1790\":5,\"1794\":20,\"1795\":2,\"1798\":1,\"1799\":1,\"1800\":1,\"1806\":3,\"1808\":2,\"1810\":5,\"1812\":2,\"1814\":4,\"1815\":5,\"1816\":4,\"1817\":8,\"1818\":2,\"1820\":2,\"1837\":2,\"1839\":6,\"1843\":12,\"1845\":1,\"1848\":1,\"1856\":5,\"1858\":1,\"1868\":1,\"1871\":1,\"1901\":1,\"1903\":1,\"1905\":3,\"1907\":1,\"1940\":6,\"1941\":2,\"1942\":6,\"1943\":2,\"1944\":7,\"1945\":5,\"1947\":7,\"1950\":3,\"1959\":6,\"1960\":2,\"1961\":2,\"1965\":1,\"1974\":3,\"1975\":2,\"1977\":5,\"1984\":6,\"1991\":6,\"1992\":18,\"1993\":11,\"1995\":8,\"1996\":10,\"1997\":14,\"1999\":2,\"2000\":13,\"2001\":13,\"2002\":5,\"2003\":5,\"2004\":5,\"2005\":7,\"2006\":4,\"2007\":14,\"2008\":6,\"2127\":6,\"2130\":20,\"2131\":2,\"2133\":8,\"2134\":8,\"2136\":11,\"2142\":1,\"2143\":2,\"2145\":10,\"2146\":7,\"2147\":8,\"2155\":2,\"2162\":2,\"2167\":2,\"2176\":2,\"2183\":3,\"2184\":5,\"2187\":2,\"2190\":3,\"2191\":2,\"2207\":2,\"2208\":3,\"2218\":2,\"2221\":8,\"2223\":11,\"2226\":9,\"2227\":3,\"2231\":7,\"2232\":10,\"2235\":32,\"2236\":32,\"2237\":4,\"2238\":7,\"2239\":28,\"2240\":25,\"2241\":11,\"2245\":26,\"2249\":4,\"2253\":3,\"2258\":6,\"2325\":2,\"2327\":2,\"2355\":31,\"2359\":1,\"2377\":6,\"2411\":12,\"2412\":16,\"2413\":11,\"2423\":16,\"2424\":11,\"2425\":1,\"2427\":4,\"2428\":5,\"2429\":1,\"2431\":11,\"2432\":11,\"2433\":3,\"2435\":1,\"2439\":1,\"2447\":14,\"2448\":11,\"2460\":2,\"2471\":1,\"2473\":1}}],[\"batchnorm1d\",{\"1\":{\"141\":2}}],[\"batchnormalization\",{\"1\":{\"102\":1}}],[\"batchnorm\",{\"0\":{\"1065\":1},\"1\":{\"43\":2,\"1065\":2}}],[\"backbone\",{\"1\":{\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"819\":1,\"828\":1,\"829\":2,\"830\":1,\"859\":1}}],[\"backward\",{\"0\":{\"1918\":1},\"1\":{\"102\":2,\"113\":2,\"406\":2,\"484\":2,\"490\":2,\"654\":2,\"703\":1,\"706\":1,\"756\":7,\"773\":7,\"866\":3,\"867\":3,\"879\":6,\"881\":1,\"883\":6,\"884\":1,\"1502\":1,\"1708\":3,\"1709\":3,\"1710\":3,\"1750\":3,\"1768\":3,\"1918\":2,\"1976\":1,\"1993\":3,\"2224\":3,\"2245\":3,\"2355\":1,\"2369\":2,\"2431\":3}}],[\"backends\",{\"0\":{\"53\":1}}],[\"backend\",{\"0\":{\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1733\":1,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1764\":1,\"1766\":1,\"1768\":1,\"1769\":1,\"1770\":1,\"1771\":1,\"1779\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1801\":1,\"1803\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1820\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1842\":1,\"1847\":1,\"1849\":1,\"1851\":1,\"1854\":1,\"1855\":1,\"1858\":1,\"1860\":1,\"1861\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1885\":1,\"1886\":1,\"1888\":1,\"1891\":1,\"1892\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1907\":1,\"1908\":1,\"1910\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1926\":1,\"1927\":1,\"1928\":1,\"1931\":1,\"1932\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"1937\":1},\"1\":{\"32\":1,\"37\":1,\"38\":4,\"42\":1,\"46\":2,\"109\":1,\"110\":1,\"166\":1,\"246\":1,\"377\":2,\"449\":2,\"527\":2,\"541\":2,\"692\":1,\"710\":1,\"711\":1,\"731\":1,\"732\":1,\"766\":1,\"767\":1,\"775\":1,\"848\":1,\"849\":1,\"850\":1,\"1678\":7,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1731\":1,\"1733\":1,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1764\":1,\"1766\":1,\"1768\":1,\"1770\":1,\"1771\":1,\"1779\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1801\":1,\"1803\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1820\":1,\"1822\":5,\"1837\":1,\"1838\":1,\"1839\":1,\"1842\":1,\"1847\":1,\"1849\":1,\"1851\":1,\"1854\":1,\"1855\":1,\"1858\":1,\"1860\":1,\"1861\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1885\":1,\"1886\":1,\"1888\":1,\"1891\":1,\"1892\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1907\":1,\"1908\":1,\"1910\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1926\":1,\"1927\":1,\"1928\":1,\"1931\":1,\"1932\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"1937\":1,\"1957\":1,\"1960\":1,\"1961\":1,\"1992\":1,\"1995\":1,\"2129\":1,\"2286\":1,\"2340\":2,\"2439\":1}}],[\"back\",{\"1\":{\"26\":1,\"228\":1,\"247\":1,\"1155\":2,\"1157\":2,\"1250\":1,\"1387\":1,\"2130\":2,\"2136\":2,\"2287\":1,\"2355\":1}}],[\"basis\",{\"1\":{\"516\":1,\"1607\":1,\"2482\":1,\"2495\":1}}],[\"basiclayer\",{\"0\":{\"1064\":1},\"1\":{\"1029\":5,\"1064\":2,\"1235\":6,\"1280\":5,\"1281\":5,\"1282\":6}}],[\"basicblock\",{\"0\":{\"693\":1,\"2177\":1},\"1\":{\"693\":1,\"2177\":1,\"2198\":2}}],[\"basic\",{\"0\":{\"893\":1,\"894\":1},\"1\":{\"152\":1,\"216\":1,\"227\":1,\"243\":2,\"481\":1,\"615\":1,\"893\":1,\"894\":1,\"982\":1,\"1064\":1,\"1108\":1,\"1259\":1,\"1273\":1,\"1274\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1}}],[\"basicnorm\",{\"0\":{\"615\":1},\"1\":{\"141\":6,\"615\":3}}],[\"basically\",{\"1\":{\"48\":1,\"1334\":1}}],[\"basewriter\",{\"0\":{\"1718\":1},\"1\":{\"1718\":1,\"1773\":1,\"1781\":1,\"1826\":1,\"1829\":1}}],[\"basefrequencediscriminator\",{\"0\":{\"1514\":1},\"1\":{\"1514\":1}}],[\"basetransformerdecoder\",{\"0\":{\"692\":1},\"1\":{\"692\":1,\"731\":1,\"732\":1,\"766\":1,\"767\":1,\"848\":1,\"850\":1}}],[\"bases\",{\"1\":{\"614\":1,\"615\":1,\"616\":1,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"629\":1,\"630\":1,\"631\":1,\"632\":1,\"633\":1,\"634\":1,\"635\":1,\"636\":1,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"651\":1,\"652\":1,\"653\":1,\"654\":1,\"674\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"691\":1,\"692\":1,\"693\":1,\"696\":1,\"697\":1,\"698\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"703\":1,\"706\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"720\":1,\"722\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"738\":1,\"740\":1,\"741\":1,\"743\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"750\":1,\"751\":1,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"757\":1,\"759\":1,\"760\":1,\"761\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":1,\"767\":1,\"768\":1,\"770\":1,\"771\":1,\"772\":1,\"773\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"788\":1,\"790\":1,\"791\":1,\"793\":1,\"794\":1,\"795\":1,\"796\":1,\"798\":1,\"800\":1,\"801\":1,\"802\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"817\":1,\"820\":1,\"821\":1,\"823\":1,\"824\":1,\"825\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":1,\"832\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"859\":1,\"860\":1,\"862\":1,\"864\":1,\"866\":1,\"867\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":1,\"959\":1,\"960\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"978\":1,\"979\":1,\"980\":1,\"981\":1,\"982\":1,\"985\":1,\"987\":1,\"989\":1,\"991\":1,\"993\":1,\"995\":1,\"996\":1,\"998\":1,\"1000\":1,\"1001\":1,\"1003\":1,\"1005\":1,\"1007\":1,\"1009\":1,\"1011\":1,\"1013\":1,\"1015\":1,\"1017\":1,\"1029\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1050\":1,\"1051\":1,\"1053\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1107\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1118\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1129\":1,\"1130\":1,\"1131\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1137\":1,\"1139\":1,\"1141\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1147\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1159\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1176\":1,\"1177\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1189\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":1,\"1204\":1,\"1205\":1,\"1207\":1,\"1208\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":1,\"1218\":1,\"1219\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1228\":1,\"1229\":1,\"1230\":1,\"1232\":1,\"1233\":1,\"1235\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1244\":1,\"1245\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1267\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1333\":1,\"1334\":1,\"1381\":1,\"1382\":1,\"1383\":1,\"1385\":1,\"1387\":1,\"1389\":1,\"1390\":1,\"1391\":1,\"1392\":1,\"1394\":1,\"1395\":1,\"1396\":1,\"1397\":1,\"1398\":1,\"1400\":1,\"1401\":1,\"1402\":1,\"1403\":1,\"1404\":1,\"1406\":1,\"1408\":1,\"1409\":1,\"1410\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1432\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1466\":1,\"1467\":1,\"1468\":1,\"1469\":1,\"1508\":1,\"1509\":1,\"1511\":1,\"1513\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1519\":1,\"1520\":1,\"1521\":1,\"1522\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1529\":1,\"1530\":1,\"1532\":1,\"1533\":1,\"1534\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1548\":1,\"1549\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1556\":1,\"1576\":1,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1625\":1,\"1626\":1,\"1627\":1,\"1628\":1,\"1638\":1,\"1640\":1,\"1641\":1,\"1642\":1,\"1643\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1657\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1718\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1728\":1,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1733\":1,\"1734\":1,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1761\":1,\"1762\":1,\"1763\":1,\"1764\":1,\"1766\":1,\"1768\":1,\"1770\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1775\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1779\":1,\"1780\":1,\"1781\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1791\":1,\"1793\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1802\":1,\"1803\":1,\"1804\":1,\"1806\":1,\"1807\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1813\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":1,\"1821\":1,\"1823\":1,\"1824\":1,\"1825\":1,\"1826\":1,\"1828\":1,\"1829\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1948\":1,\"1957\":1,\"1959\":1,\"1960\":1,\"1961\":1,\"1962\":1,\"1965\":1,\"1966\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1992\":1,\"1993\":1,\"1994\":1,\"1995\":1,\"1996\":1,\"1997\":1,\"1998\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2039\":1,\"2040\":1,\"2043\":1,\"2044\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2061\":1,\"2065\":1,\"2066\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2128\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2132\":1,\"2133\":1,\"2134\":1,\"2136\":1,\"2137\":1,\"2138\":1,\"2139\":1,\"2141\":1,\"2142\":1,\"2143\":1,\"2144\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2218\":1,\"2219\":1,\"2220\":1,\"2221\":1,\"2222\":1,\"2223\":1,\"2226\":1,\"2227\":1,\"2228\":1,\"2229\":1,\"2231\":1,\"2232\":1,\"2235\":1,\"2236\":1,\"2237\":1,\"2238\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2245\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2274\":1,\"2275\":1,\"2276\":1,\"2277\":1,\"2278\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2282\":1,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2287\":1,\"2288\":1,\"2289\":1,\"2291\":1,\"2292\":1,\"2304\":1,\"2324\":1,\"2325\":1,\"2327\":1,\"2328\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":1,\"2335\":1,\"2336\":1,\"2337\":1,\"2338\":1,\"2339\":1,\"2340\":1,\"2341\":1,\"2342\":1,\"2344\":1,\"2345\":1,\"2346\":1,\"2347\":1,\"2348\":1,\"2349\":1,\"2350\":1,\"2351\":1,\"2353\":1,\"2354\":1,\"2355\":1,\"2356\":1,\"2357\":1,\"2358\":1,\"2359\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2364\":1,\"2365\":1,\"2366\":1,\"2367\":1,\"2368\":1,\"2369\":1,\"2370\":1,\"2371\":1,\"2372\":1,\"2373\":1,\"2401\":1,\"2403\":1,\"2404\":1,\"2407\":1,\"2408\":1,\"2409\":1,\"2411\":1,\"2412\":1,\"2413\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2421\":1,\"2422\":1,\"2423\":1,\"2424\":1,\"2425\":1,\"2426\":1,\"2427\":1,\"2428\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":1,\"2433\":1,\"2434\":1,\"2443\":1,\"2445\":1,\"2446\":1,\"2447\":1,\"2448\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1,\"2474\":1,\"2477\":1,\"2478\":1,\"2480\":1,\"2481\":1,\"2482\":1}}],[\"baseline\",{\"1\":{\"236\":1}}],[\"base64\",{\"1\":{\"60\":1,\"62\":1,\"63\":1,\"64\":1}}],[\"base\",{\"0\":{\"108\":1,\"827\":1,\"829\":1,\"858\":1,\"1164\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2085\":1,\"2086\":1,\"2087\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2118\":1,\"2119\":1},\"1\":{\"26\":1,\"28\":1,\"98\":1,\"161\":1,\"262\":1,\"267\":1,\"276\":3,\"286\":1,\"385\":2,\"692\":1,\"827\":1,\"829\":1,\"858\":1,\"950\":1,\"1036\":1,\"1042\":1,\"1163\":1,\"1164\":1,\"1174\":1,\"1276\":1,\"1333\":1,\"1389\":2,\"1391\":1,\"1396\":2,\"1401\":2,\"1403\":1,\"1408\":1,\"1415\":1,\"1419\":3,\"1450\":3,\"1452\":3,\"1454\":3,\"1456\":3,\"1466\":2,\"1468\":1,\"1514\":2,\"1526\":1,\"1552\":6,\"1553\":3,\"1598\":1,\"1600\":1,\"1607\":3,\"1611\":3,\"1612\":3,\"1613\":3,\"1618\":1,\"1625\":3,\"1626\":6,\"1628\":3,\"1651\":1,\"1662\":1,\"1681\":2,\"1683\":2,\"1718\":1,\"1800\":1,\"1967\":1,\"1969\":1,\"1980\":1,\"2018\":7,\"2130\":1,\"2131\":1,\"2137\":1,\"2148\":1,\"2416\":1,\"2456\":1}}],[\"based\",{\"0\":{\"24\":1,\"25\":1,\"411\":1,\"1253\":1},\"1\":{\"6\":1,\"22\":2,\"26\":4,\"38\":1,\"41\":1,\"43\":1,\"45\":1,\"56\":1,\"69\":1,\"84\":1,\"102\":1,\"133\":1,\"145\":1,\"146\":2,\"160\":1,\"198\":1,\"200\":11,\"205\":5,\"233\":1,\"240\":1,\"242\":4,\"267\":1,\"268\":1,\"273\":1,\"276\":1,\"277\":1,\"286\":2,\"287\":3,\"289\":1,\"290\":1,\"616\":3,\"634\":1,\"643\":1,\"691\":1,\"696\":6,\"697\":5,\"717\":1,\"776\":1,\"790\":1,\"791\":1,\"795\":1,\"1054\":1,\"1125\":1,\"1126\":1,\"1131\":1,\"1172\":1,\"1253\":1,\"1267\":1,\"1290\":1,\"1317\":1,\"1318\":1,\"1327\":1,\"1330\":1,\"1350\":1,\"1381\":1,\"1395\":1,\"1441\":1,\"1508\":1,\"1521\":1,\"1529\":1,\"1539\":1,\"1548\":1,\"1576\":1,\"1585\":1,\"1608\":1,\"1631\":1,\"1645\":1,\"1710\":1,\"1715\":1,\"1716\":1,\"1719\":1,\"1720\":1,\"1721\":2,\"1725\":1,\"1729\":1,\"1730\":2,\"1756\":1,\"1757\":1,\"1770\":1,\"1788\":2,\"1789\":1,\"1790\":1,\"1854\":1,\"1960\":1,\"1961\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"2000\":2,\"2001\":1,\"2044\":5,\"2049\":1,\"2130\":8,\"2131\":1,\"2133\":1,\"2148\":1,\"2183\":1,\"2187\":1,\"2239\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2262\":1,\"2298\":1,\"2327\":1,\"2347\":1,\"2354\":1,\"2371\":1,\"2378\":1,\"2404\":2,\"2405\":1,\"2428\":1,\"2434\":1,\"2445\":1}}],[\"bash\",{\"1\":{\"24\":1,\"31\":1,\"40\":1,\"109\":1,\"117\":1,\"126\":2,\"162\":1,\"163\":4,\"164\":1,\"168\":2,\"243\":2}}],[\"brnn\",{\"1\":{\"1918\":1}}],[\"brctc\",{\"1\":{\"706\":3}}],[\"break=none\",{\"1\":{\"2480\":1}}],[\"break\",{\"1\":{\"269\":1,\"278\":1}}],[\"breath\",{\"1\":{\"269\":3,\"278\":3}}],[\"br\",{\"1\":{\"269\":1,\"278\":1}}],[\"broadcasted\",{\"1\":{\"2136\":1}}],[\"broadcasts\",{\"1\":{\"1292\":1}}],[\"broadcast\",{\"0\":{\"1292\":1,\"1478\":1},\"1\":{\"1292\":1,\"1478\":2,\"1501\":1}}],[\"browsers\",{\"1\":{\"248\":1}}],[\"browser\",{\"1\":{\"248\":1}}],[\"brockman\",{\"1\":{\"202\":1}}],[\"brunskill\",{\"1\":{\"202\":1}}],[\"branches\",{\"1\":{\"939\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1}}],[\"branch\",{\"1\":{\"26\":1,\"34\":1,\"260\":2,\"689\":1,\"700\":1,\"701\":4,\"980\":1}}],[\"branchformerencoderlayer\",{\"0\":{\"701\":1},\"1\":{\"701\":1}}],[\"branchformerencoder\",{\"0\":{\"700\":1},\"1\":{\"700\":1}}],[\"branchformer\",{\"0\":{\"617\":2,\"656\":1,\"700\":1,\"701\":1,\"733\":1,\"734\":1,\"735\":1},\"1\":{\"6\":1,\"141\":6,\"243\":1,\"617\":8,\"624\":6,\"636\":2,\"656\":5,\"659\":3,\"700\":2,\"701\":2,\"733\":3,\"734\":2,\"735\":2}}],[\"bridgettesong\",{\"1\":{\"1977\":2}}],[\"bridge\",{\"1\":{\"67\":1}}],[\"bridging\",{\"1\":{\"16\":1}}],[\"brianyan918\",{\"1\":{\"260\":1}}],[\"brian\",{\"1\":{\"6\":2,\"10\":1,\"11\":1,\"12\":1,\"207\":1,\"244\":1}}],[\"being\",{\"1\":{\"929\":1,\"939\":1}}],[\"beyond\",{\"1\":{\"704\":1}}],[\"benchmark\",{\"1\":{\"377\":2,\"449\":2}}],[\"benchmarks\",{\"1\":{\"38\":1}}],[\"ben\",{\"1\":{\"275\":1}}],[\"behalf\",{\"1\":{\"2276\":1,\"2277\":1}}],[\"behaviour\",{\"1\":{\"2016\":1}}],[\"behavior\",{\"1\":{\"55\":1,\"82\":1,\"94\":1,\"123\":1,\"173\":1,\"223\":1}}],[\"behind\",{\"1\":{\"262\":1}}],[\"beatsencoder\",{\"0\":{\"699\":1},\"1\":{\"699\":1}}],[\"beatsconfig\",{\"0\":{\"698\":1},\"1\":{\"698\":1,\"699\":1}}],[\"beats\",{\"0\":{\"698\":1,\"699\":1,\"754\":1,\"787\":1,\"841\":1,\"851\":1,\"900\":1,\"901\":1,\"902\":1,\"911\":1,\"927\":1},\"1\":{\"214\":1,\"698\":1,\"699\":10,\"754\":1,\"787\":1,\"841\":1,\"851\":1,\"900\":1,\"901\":1,\"902\":1,\"911\":1,\"927\":1}}],[\"beans\",{\"1\":{\"212\":1}}],[\"beamsearchtimesyncstreaming\",{\"0\":{\"1727\":1},\"1\":{\"1727\":1}}],[\"beamsearchtimesync\",{\"0\":{\"1726\":1},\"1\":{\"1726\":1}}],[\"beamsearchtransducerstreaming\",{\"0\":{\"697\":1},\"1\":{\"697\":1}}],[\"beamsearchtransducer\",{\"0\":{\"616\":1,\"696\":1},\"1\":{\"616\":2,\"696\":1}}],[\"beamsearch\",{\"0\":{\"1725\":1},\"1\":{\"1719\":1,\"1725\":1}}],[\"beamformser\",{\"1\":{\"1354\":2}}],[\"beamformed\",{\"1\":{\"1334\":1}}],[\"beamformers\",{\"1\":{\"1354\":1}}],[\"beamformer\",{\"0\":{\"1054\":1,\"1126\":2,\"1217\":1,\"1291\":1,\"1293\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1315\":1,\"1318\":1,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1330\":1,\"1332\":1,\"1351\":1,\"1354\":2,\"1356\":1,\"1361\":1},\"1\":{\"223\":4,\"720\":2,\"738\":1,\"815\":1,\"1054\":1,\"1126\":5,\"1217\":3,\"1291\":1,\"1293\":3,\"1308\":1,\"1309\":3,\"1310\":1,\"1311\":3,\"1315\":1,\"1318\":2,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1326\":1,\"1327\":2,\"1328\":1,\"1329\":1,\"1330\":1,\"1332\":1,\"1351\":1,\"1354\":7,\"1356\":1,\"1361\":1,\"1539\":1,\"1766\":1}}],[\"beamform\",{\"1\":{\"1291\":1,\"1311\":1,\"1318\":1,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1330\":1}}],[\"beamforming\",{\"0\":{\"1291\":1},\"1\":{\"1126\":3,\"1291\":1,\"1308\":1,\"1318\":1,\"1332\":3,\"1334\":1}}],[\"beam\",{\"0\":{\"306\":1,\"319\":1,\"325\":1,\"394\":1,\"400\":1,\"412\":1,\"426\":1,\"434\":1,\"441\":1,\"447\":1,\"467\":1,\"473\":1,\"503\":1,\"509\":1,\"616\":1,\"628\":1,\"631\":1,\"696\":1,\"697\":1,\"743\":1,\"763\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1732\":1,\"1806\":1,\"1807\":1,\"1862\":2},\"1\":{\"45\":13,\"126\":1,\"129\":1,\"145\":3,\"175\":7,\"262\":1,\"267\":2,\"276\":2,\"286\":3,\"301\":2,\"315\":2,\"321\":4,\"389\":2,\"396\":2,\"406\":4,\"421\":2,\"429\":2,\"442\":2,\"463\":4,\"469\":2,\"498\":2,\"505\":2,\"616\":11,\"628\":2,\"631\":1,\"692\":3,\"696\":11,\"697\":11,\"743\":2,\"763\":1,\"795\":1,\"1719\":25,\"1720\":5,\"1721\":16,\"1722\":1,\"1723\":1,\"1725\":23,\"1726\":12,\"1727\":12,\"1730\":2,\"1731\":2,\"1732\":1,\"1762\":1,\"1787\":2,\"1794\":1,\"1804\":1,\"1806\":14,\"1807\":1,\"1821\":1,\"1822\":2,\"1862\":14,\"1920\":1,\"1966\":2}}],[\"bert\",{\"0\":{\"911\":1},\"1\":{\"232\":1,\"258\":1,\"911\":2,\"2137\":1,\"2216\":1}}],[\"bereket\",{\"1\":{\"160\":1}}],[\"berrebbi\",{\"1\":{\"6\":1,\"10\":1,\"244\":1}}],[\"below\",{\"1\":{\"133\":1,\"135\":1,\"141\":1,\"213\":1,\"243\":1,\"261\":1,\"269\":1,\"278\":1,\"285\":1,\"290\":1,\"733\":1,\"734\":1,\"1316\":1,\"2307\":1,\"2355\":1}}],[\"belonging\",{\"1\":{\"254\":1,\"269\":1,\"278\":1}}],[\"belong\",{\"1\":{\"101\":1}}],[\"become\",{\"1\":{\"1376\":1,\"1377\":1}}],[\"becomes\",{\"1\":{\"102\":1,\"211\":1,\"798\":1,\"862\":1,\"2000\":1}}],[\"because\",{\"1\":{\"62\":1,\"71\":1,\"79\":1,\"91\":1,\"96\":2,\"106\":1,\"128\":1,\"139\":1,\"152\":1,\"224\":1,\"242\":2,\"243\":1,\"254\":1,\"285\":1,\"290\":2,\"824\":1,\"974\":1,\"1008\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1228\":1,\"1647\":1,\"1804\":1,\"1833\":1,\"1883\":1,\"2480\":1}}],[\"best\",{\"1\":{\"46\":1,\"48\":2,\"50\":2,\"95\":1,\"134\":3,\"136\":1,\"175\":3,\"218\":2,\"224\":1,\"243\":1,\"248\":1,\"267\":3,\"276\":3,\"286\":2,\"526\":2,\"527\":2,\"543\":1,\"616\":5,\"696\":7,\"697\":6,\"1132\":1,\"1167\":1,\"1204\":1,\"1209\":2,\"1228\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1,\"1719\":2,\"1720\":1,\"1721\":2,\"1725\":3,\"1730\":3,\"1806\":2,\"1833\":1,\"1862\":1,\"1920\":1,\"1949\":5,\"2020\":1,\"2147\":1,\"2216\":1,\"2333\":1,\"2339\":2,\"2348\":1,\"2359\":2,\"2370\":2,\"2372\":1}}],[\"been\",{\"1\":{\"32\":1,\"133\":1,\"243\":1,\"267\":1,\"1246\":1,\"2130\":1,\"2355\":2}}],[\"before=true\",{\"1\":{\"1735\":1,\"1751\":1,\"1759\":1}}],[\"before=false\",{\"1\":{\"1548\":1}}],[\"beforehand\",{\"1\":{\"286\":1}}],[\"before\",{\"1\":{\"23\":1,\"24\":1,\"48\":1,\"56\":1,\"62\":1,\"69\":1,\"96\":1,\"106\":1,\"110\":1,\"127\":1,\"150\":1,\"153\":1,\"173\":1,\"201\":1,\"218\":2,\"223\":1,\"224\":1,\"267\":2,\"276\":2,\"286\":2,\"674\":2,\"692\":5,\"709\":3,\"710\":3,\"711\":3,\"731\":1,\"732\":1,\"745\":3,\"747\":4,\"748\":2,\"760\":1,\"766\":1,\"767\":1,\"771\":3,\"774\":3,\"775\":2,\"780\":3,\"787\":3,\"790\":1,\"820\":2,\"824\":1,\"828\":1,\"846\":1,\"848\":1,\"849\":3,\"850\":2,\"1070\":1,\"1071\":1,\"1107\":3,\"1145\":1,\"1155\":1,\"1157\":1,\"1267\":1,\"1268\":1,\"1273\":1,\"1274\":1,\"1278\":3,\"1387\":1,\"1493\":1,\"1494\":1,\"1519\":3,\"1526\":2,\"1535\":3,\"1536\":2,\"1546\":3,\"1548\":1,\"1552\":3,\"1553\":1,\"1598\":2,\"1599\":6,\"1600\":2,\"1604\":1,\"1605\":1,\"1606\":1,\"1615\":1,\"1622\":3,\"1625\":1,\"1626\":3,\"1645\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":2,\"1712\":1,\"1715\":1,\"1735\":2,\"1750\":1,\"1751\":2,\"1759\":2,\"1764\":3,\"1768\":1,\"1784\":1,\"1794\":1,\"1806\":1,\"1811\":1,\"1839\":3,\"1951\":1,\"1991\":3,\"1992\":3,\"1993\":1,\"1995\":3,\"2015\":1,\"2126\":1,\"2129\":3,\"2130\":3,\"2183\":1,\"2190\":1,\"2191\":3,\"2208\":1,\"2223\":1,\"2226\":3,\"2237\":3,\"2239\":7,\"2240\":6,\"2241\":3,\"2411\":6,\"2412\":6,\"2413\":3,\"2423\":5,\"2424\":3,\"2432\":6,\"2447\":6,\"2448\":3}}],[\"be\",{\"1\":{\"22\":2,\"24\":3,\"26\":1,\"27\":1,\"31\":1,\"36\":2,\"37\":2,\"39\":1,\"42\":1,\"43\":4,\"44\":4,\"46\":1,\"47\":2,\"49\":1,\"50\":3,\"51\":1,\"59\":1,\"60\":2,\"62\":1,\"67\":1,\"74\":1,\"78\":2,\"79\":2,\"81\":1,\"82\":4,\"87\":1,\"90\":1,\"91\":2,\"94\":2,\"96\":2,\"98\":2,\"101\":2,\"104\":1,\"106\":2,\"107\":1,\"108\":3,\"110\":1,\"118\":1,\"126\":1,\"127\":1,\"128\":4,\"130\":1,\"131\":1,\"133\":3,\"135\":1,\"136\":1,\"138\":3,\"139\":5,\"141\":2,\"143\":2,\"144\":1,\"147\":2,\"148\":2,\"150\":2,\"162\":1,\"166\":1,\"167\":1,\"168\":2,\"173\":1,\"175\":5,\"196\":2,\"197\":1,\"200\":3,\"205\":2,\"211\":2,\"213\":3,\"220\":1,\"223\":7,\"224\":4,\"225\":3,\"232\":1,\"233\":1,\"240\":1,\"242\":12,\"243\":8,\"247\":1,\"248\":1,\"258\":1,\"259\":1,\"260\":3,\"262\":8,\"263\":1,\"266\":7,\"267\":3,\"268\":2,\"269\":6,\"275\":8,\"276\":7,\"277\":2,\"278\":6,\"284\":1,\"285\":6,\"286\":3,\"290\":4,\"518\":1,\"663\":1,\"676\":1,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"689\":1,\"690\":1,\"691\":1,\"692\":2,\"693\":1,\"694\":1,\"699\":3,\"700\":2,\"703\":1,\"704\":2,\"706\":1,\"709\":6,\"710\":6,\"711\":6,\"713\":1,\"714\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"733\":2,\"734\":2,\"738\":1,\"739\":1,\"741\":1,\"742\":1,\"744\":1,\"745\":2,\"746\":1,\"747\":1,\"748\":1,\"752\":1,\"753\":1,\"755\":3,\"756\":11,\"757\":1,\"758\":1,\"760\":4,\"768\":3,\"770\":1,\"771\":1,\"773\":11,\"774\":6,\"778\":1,\"779\":1,\"780\":6,\"781\":1,\"782\":1,\"785\":3,\"786\":2,\"788\":1,\"789\":1,\"791\":1,\"792\":1,\"796\":1,\"797\":1,\"798\":1,\"799\":1,\"800\":2,\"804\":1,\"805\":1,\"806\":1,\"807\":1,\"808\":1,\"809\":1,\"810\":1,\"811\":1,\"812\":1,\"813\":1,\"814\":1,\"815\":1,\"816\":1,\"817\":2,\"819\":2,\"820\":1,\"821\":3,\"822\":1,\"825\":1,\"826\":1,\"828\":3,\"829\":2,\"830\":3,\"833\":1,\"834\":1,\"835\":1,\"836\":1,\"837\":1,\"838\":1,\"839\":1,\"840\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"846\":3,\"849\":3,\"852\":2,\"853\":1,\"854\":1,\"855\":1,\"856\":2,\"857\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":1,\"866\":4,\"867\":4,\"878\":2,\"879\":2,\"881\":1,\"882\":2,\"883\":2,\"884\":1,\"911\":4,\"912\":1,\"921\":2,\"922\":2,\"929\":1,\"932\":1,\"934\":1,\"935\":2,\"936\":2,\"937\":2,\"939\":1,\"950\":1,\"951\":1,\"952\":1,\"953\":1,\"956\":1,\"957\":1,\"959\":1,\"961\":1,\"962\":1,\"963\":1,\"964\":1,\"965\":1,\"966\":1,\"967\":1,\"968\":1,\"969\":1,\"970\":1,\"974\":2,\"984\":1,\"994\":1,\"1008\":1,\"1029\":1,\"1030\":1,\"1031\":2,\"1032\":1,\"1033\":1,\"1034\":1,\"1035\":1,\"1036\":1,\"1037\":1,\"1038\":1,\"1039\":1,\"1040\":1,\"1041\":1,\"1042\":1,\"1043\":1,\"1044\":1,\"1045\":1,\"1046\":1,\"1047\":1,\"1048\":1,\"1049\":1,\"1051\":2,\"1052\":1,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1059\":1,\"1060\":1,\"1061\":4,\"1062\":1,\"1063\":3,\"1066\":1,\"1067\":1,\"1068\":1,\"1069\":1,\"1076\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1083\":1,\"1084\":1,\"1085\":1,\"1086\":2,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":1,\"1094\":1,\"1095\":1,\"1096\":1,\"1097\":1,\"1098\":1,\"1099\":1,\"1100\":1,\"1101\":1,\"1102\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1106\":1,\"1107\":2,\"1108\":1,\"1109\":1,\"1110\":1,\"1111\":1,\"1112\":1,\"1114\":1,\"1115\":1,\"1119\":1,\"1120\":1,\"1121\":1,\"1122\":1,\"1123\":1,\"1124\":2,\"1125\":1,\"1126\":1,\"1134\":1,\"1135\":1,\"1137\":1,\"1138\":1,\"1139\":2,\"1140\":1,\"1142\":1,\"1143\":1,\"1145\":1,\"1146\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1152\":1,\"1153\":1,\"1154\":1,\"1155\":5,\"1157\":5,\"1159\":1,\"1160\":1,\"1165\":1,\"1166\":1,\"1168\":2,\"1169\":1,\"1177\":1,\"1178\":1,\"1185\":1,\"1186\":1,\"1187\":1,\"1188\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1193\":1,\"1194\":1,\"1195\":1,\"1196\":1,\"1197\":1,\"1200\":1,\"1201\":1,\"1202\":3,\"1203\":1,\"1204\":1,\"1205\":1,\"1206\":1,\"1207\":2,\"1209\":5,\"1211\":1,\"1212\":1,\"1213\":1,\"1214\":1,\"1215\":1,\"1216\":1,\"1219\":1,\"1220\":1,\"1222\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1227\":1,\"1228\":5,\"1230\":1,\"1231\":1,\"1233\":1,\"1234\":1,\"1235\":1,\"1236\":1,\"1237\":1,\"1238\":1,\"1239\":1,\"1240\":1,\"1241\":1,\"1242\":1,\"1243\":1,\"1246\":1,\"1248\":1,\"1249\":1,\"1250\":1,\"1253\":1,\"1254\":1,\"1255\":1,\"1256\":1,\"1257\":1,\"1258\":1,\"1259\":5,\"1260\":1,\"1261\":3,\"1262\":1,\"1263\":1,\"1265\":1,\"1266\":1,\"1273\":1,\"1274\":1,\"1278\":2,\"1279\":1,\"1280\":2,\"1281\":2,\"1282\":2,\"1283\":1,\"1284\":1,\"1285\":1,\"1286\":1,\"1287\":1,\"1288\":1,\"1289\":1,\"1296\":1,\"1301\":1,\"1306\":2,\"1308\":1,\"1316\":3,\"1320\":2,\"1328\":1,\"1350\":3,\"1354\":1,\"1368\":1,\"1371\":2,\"1372\":1,\"1383\":1,\"1384\":1,\"1387\":1,\"1388\":1,\"1389\":1,\"1392\":1,\"1393\":1,\"1395\":1,\"1398\":1,\"1399\":1,\"1401\":1,\"1404\":1,\"1405\":1,\"1406\":1,\"1407\":1,\"1408\":1,\"1409\":1,\"1411\":1,\"1412\":1,\"1413\":1,\"1414\":1,\"1415\":1,\"1416\":1,\"1417\":1,\"1418\":1,\"1420\":1,\"1421\":1,\"1422\":1,\"1423\":1,\"1424\":1,\"1425\":1,\"1426\":1,\"1427\":1,\"1428\":1,\"1429\":1,\"1430\":1,\"1431\":1,\"1433\":1,\"1434\":1,\"1435\":1,\"1436\":1,\"1437\":1,\"1438\":1,\"1439\":1,\"1440\":1,\"1442\":1,\"1443\":1,\"1444\":1,\"1445\":1,\"1446\":1,\"1447\":1,\"1448\":1,\"1449\":1,\"1450\":1,\"1451\":1,\"1452\":1,\"1453\":1,\"1454\":1,\"1455\":1,\"1456\":1,\"1457\":1,\"1458\":1,\"1459\":1,\"1460\":1,\"1461\":1,\"1462\":1,\"1463\":1,\"1464\":1,\"1465\":1,\"1466\":1,\"1469\":1,\"1470\":1,\"1478\":1,\"1502\":1,\"1509\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1513\":1,\"1517\":1,\"1518\":1,\"1521\":1,\"1522\":1,\"1523\":1,\"1526\":3,\"1527\":1,\"1528\":1,\"1530\":1,\"1531\":1,\"1537\":1,\"1538\":1,\"1539\":1,\"1540\":1,\"1541\":1,\"1542\":1,\"1543\":1,\"1544\":1,\"1545\":1,\"1548\":1,\"1549\":3,\"1550\":1,\"1551\":1,\"1552\":5,\"1553\":3,\"1554\":1,\"1555\":1,\"1558\":1,\"1585\":1,\"1590\":1,\"1592\":1,\"1593\":1,\"1595\":1,\"1596\":2,\"1597\":3,\"1598\":3,\"1599\":4,\"1600\":3,\"1604\":4,\"1605\":1,\"1606\":2,\"1609\":2,\"1610\":1,\"1618\":1,\"1619\":1,\"1625\":3,\"1626\":4,\"1628\":1,\"1629\":1,\"1638\":1,\"1639\":1,\"1647\":2,\"1652\":1,\"1653\":1,\"1655\":7,\"1657\":1,\"1658\":1,\"1662\":1,\"1663\":1,\"1668\":1,\"1676\":2,\"1678\":1,\"1681\":1,\"1683\":2,\"1697\":1,\"1698\":1,\"1719\":3,\"1720\":1,\"1721\":4,\"1725\":3,\"1726\":1,\"1727\":1,\"1735\":4,\"1748\":1,\"1750\":2,\"1751\":6,\"1758\":1,\"1759\":4,\"1770\":1,\"1782\":1,\"1783\":1,\"1785\":1,\"1786\":1,\"1794\":2,\"1799\":1,\"1800\":1,\"1817\":1,\"1818\":1,\"1854\":1,\"1860\":1,\"1862\":3,\"1881\":1,\"1883\":1,\"1901\":1,\"1903\":1,\"1932\":1,\"1938\":2,\"1939\":1,\"1940\":1,\"1941\":1,\"1942\":1,\"1943\":1,\"1945\":1,\"1946\":1,\"1949\":1,\"1957\":1,\"1958\":1,\"1967\":1,\"1968\":1,\"1969\":1,\"1970\":1,\"1972\":1,\"1973\":1,\"1975\":1,\"1976\":1,\"1978\":1,\"1979\":1,\"1980\":1,\"1981\":1,\"1982\":1,\"1983\":1,\"1985\":1,\"1986\":1,\"1988\":1,\"1989\":1,\"1992\":5,\"1993\":3,\"1995\":5,\"2000\":1,\"2001\":2,\"2026\":1,\"2027\":1,\"2028\":1,\"2029\":1,\"2030\":1,\"2031\":1,\"2032\":1,\"2033\":1,\"2034\":1,\"2035\":1,\"2039\":2,\"2040\":1,\"2043\":1,\"2045\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2124\":1,\"2125\":1,\"2129\":3,\"2130\":5,\"2131\":1,\"2134\":3,\"2136\":2,\"2155\":1,\"2168\":1,\"2169\":1,\"2170\":1,\"2171\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2175\":1,\"2177\":1,\"2178\":1,\"2179\":1,\"2180\":1,\"2181\":1,\"2182\":1,\"2183\":1,\"2184\":2,\"2185\":1,\"2186\":1,\"2188\":2,\"2189\":1,\"2191\":2,\"2192\":1,\"2193\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2197\":1,\"2198\":1,\"2199\":1,\"2200\":1,\"2201\":1,\"2203\":1,\"2204\":1,\"2205\":1,\"2206\":1,\"2209\":1,\"2210\":1,\"2211\":1,\"2212\":1,\"2215\":1,\"2216\":2,\"2217\":1,\"2220\":3,\"2224\":2,\"2228\":1,\"2229\":1,\"2231\":1,\"2235\":4,\"2236\":4,\"2239\":4,\"2240\":4,\"2245\":4,\"2246\":2,\"2248\":2,\"2249\":4,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":3,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2262\":1,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2276\":1,\"2277\":1,\"2304\":1,\"2305\":1,\"2306\":1,\"2309\":1,\"2312\":1,\"2325\":1,\"2326\":1,\"2327\":3,\"2345\":1,\"2353\":1,\"2354\":4,\"2355\":7,\"2364\":2,\"2365\":2,\"2401\":1,\"2402\":1,\"2405\":1,\"2406\":1,\"2408\":1,\"2409\":1,\"2410\":1,\"2411\":5,\"2412\":5,\"2414\":1,\"2415\":1,\"2416\":1,\"2417\":1,\"2418\":1,\"2419\":1,\"2423\":5,\"2431\":4,\"2432\":4,\"2434\":1,\"2435\":1,\"2443\":1,\"2444\":1,\"2446\":1,\"2447\":5,\"2449\":1,\"2450\":1,\"2451\":1,\"2452\":1,\"2453\":1,\"2454\":1,\"2456\":1,\"2457\":1,\"2458\":1,\"2459\":1,\"2460\":1,\"2461\":1,\"2465\":1,\"2466\":1,\"2467\":1,\"2468\":1}}],[\"beta=none\",{\"1\":{\"887\":1}}],[\"beta=1\",{\"1\":{\"809\":1}}],[\"betas\",{\"0\":{\"879\":1,\"883\":1},\"1\":{\"84\":1,\"243\":1,\"703\":3,\"879\":4,\"881\":2,\"883\":4,\"884\":2}}],[\"beta\",{\"1\":{\"45\":1,\"141\":6,\"145\":1,\"616\":3,\"635\":4,\"650\":7,\"652\":7,\"664\":9,\"696\":2,\"697\":2,\"703\":1,\"879\":1,\"881\":1,\"883\":1,\"884\":1,\"1225\":10,\"1526\":1,\"1577\":1,\"1600\":1,\"1608\":4,\"1631\":3,\"1920\":1,\"2423\":1,\"2428\":3,\"2442\":6}}],[\"between\",{\"0\":{\"94\":1,\"123\":1},\"1\":{\"16\":1,\"46\":1,\"50\":1,\"79\":1,\"138\":1,\"242\":5,\"246\":1,\"247\":1,\"267\":1,\"269\":1,\"276\":1,\"278\":1,\"286\":1,\"620\":1,\"701\":1,\"706\":1,\"710\":1,\"711\":1,\"818\":1,\"846\":3,\"1131\":1,\"1164\":1,\"1172\":1,\"1204\":1,\"1209\":1,\"1228\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1301\":1,\"1327\":1,\"1330\":1,\"1334\":1,\"1372\":1,\"1392\":1,\"1396\":1,\"1397\":1,\"1422\":1,\"1552\":1,\"1645\":1,\"1650\":1,\"1686\":1,\"1691\":1,\"1694\":1,\"1736\":1,\"1753\":1,\"1782\":1,\"1907\":1,\"2000\":1,\"2130\":1,\"2220\":3,\"2308\":1,\"2325\":1,\"2355\":2,\"2380\":1}}],[\"better\",{\"1\":{\"6\":1,\"71\":1,\"196\":1,\"242\":1,\"249\":1,\"262\":1,\"267\":1,\"268\":1,\"276\":1,\"277\":1,\"286\":3,\"1270\":1,\"2020\":1,\"2176\":1}}],[\"begins\",{\"1\":{\"961\":3}}],[\"beginner\",{\"1\":{\"536\":1}}],[\"beginning\",{\"1\":{\"242\":1,\"286\":1,\"290\":1,\"1301\":1,\"1356\":1,\"1372\":1,\"2134\":1}}],[\"begin\",{\"1\":{\"1\":1}}],[\"bypass\",{\"1\":{\"1156\":1}}],[\"bytetensor\",{\"1\":{\"787\":2,\"1753\":2,\"1901\":1,\"2433\":1}}],[\"bytes\",{\"0\":{\"2163\":1,\"2322\":1},\"1\":{\"703\":1,\"717\":3,\"1000\":6,\"2054\":3,\"2163\":1,\"2322\":1}}],[\"byte\",{\"1\":{\"224\":1,\"1000\":1,\"1883\":1,\"2054\":1}}],[\"byter\",{\"1\":{\"70\":1}}],[\"byan\",{\"1\":{\"136\":3}}],[\"by\",{\"0\":{\"3\":1,\"74\":1,\"1629\":1,\"1905\":1,\"1934\":1,\"2303\":1},\"1\":{\"0\":1,\"1\":2,\"3\":2,\"22\":3,\"26\":1,\"32\":2,\"39\":1,\"41\":1,\"43\":2,\"45\":3,\"47\":1,\"49\":1,\"50\":1,\"51\":1,\"57\":1,\"59\":1,\"67\":1,\"71\":2,\"78\":1,\"79\":3,\"81\":2,\"82\":1,\"86\":1,\"90\":1,\"91\":2,\"94\":1,\"96\":1,\"97\":1,\"99\":1,\"100\":1,\"102\":1,\"104\":2,\"106\":1,\"107\":3,\"109\":1,\"118\":1,\"124\":2,\"125\":1,\"127\":1,\"128\":2,\"139\":4,\"141\":4,\"142\":1,\"145\":2,\"146\":1,\"147\":1,\"148\":2,\"160\":1,\"161\":1,\"162\":2,\"163\":1,\"164\":1,\"165\":1,\"166\":1,\"168\":1,\"173\":2,\"175\":1,\"191\":1,\"192\":1,\"196\":4,\"197\":4,\"200\":3,\"201\":2,\"205\":2,\"212\":1,\"213\":3,\"217\":1,\"218\":1,\"219\":1,\"223\":3,\"224\":1,\"235\":1,\"242\":1,\"243\":3,\"246\":1,\"266\":1,\"267\":5,\"268\":4,\"271\":1,\"272\":1,\"275\":1,\"276\":6,\"277\":4,\"280\":1,\"282\":1,\"285\":1,\"286\":5,\"287\":15,\"289\":2,\"290\":3,\"517\":1,\"536\":1,\"614\":1,\"616\":5,\"629\":2,\"635\":1,\"636\":2,\"644\":1,\"650\":1,\"652\":1,\"675\":1,\"676\":2,\"678\":2,\"680\":2,\"682\":2,\"684\":2,\"686\":2,\"689\":2,\"691\":1,\"692\":1,\"693\":2,\"696\":4,\"697\":4,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"704\":1,\"706\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":2,\"715\":1,\"718\":2,\"720\":2,\"722\":1,\"724\":2,\"725\":2,\"726\":1,\"727\":1,\"728\":2,\"729\":2,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":2,\"738\":1,\"741\":2,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"750\":1,\"751\":1,\"752\":2,\"754\":1,\"756\":4,\"757\":2,\"759\":1,\"766\":1,\"767\":1,\"768\":1,\"771\":1,\"773\":4,\"774\":1,\"775\":1,\"777\":1,\"778\":2,\"780\":1,\"781\":2,\"783\":1,\"784\":1,\"786\":2,\"787\":2,\"788\":2,\"790\":1,\"791\":2,\"793\":1,\"794\":1,\"796\":2,\"798\":2,\"800\":2,\"805\":2,\"807\":2,\"809\":2,\"811\":2,\"813\":2,\"815\":2,\"817\":2,\"820\":1,\"821\":1,\"823\":1,\"824\":1,\"825\":2,\"828\":1,\"829\":3,\"830\":2,\"833\":2,\"835\":2,\"837\":2,\"839\":2,\"841\":1,\"842\":2,\"844\":2,\"846\":2,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":2,\"854\":2,\"856\":2,\"859\":2,\"860\":2,\"862\":2,\"864\":2,\"866\":2,\"867\":2,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"921\":1,\"924\":1,\"927\":1,\"935\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":2,\"952\":2,\"954\":1,\"955\":1,\"956\":2,\"958\":1,\"963\":2,\"965\":2,\"967\":2,\"969\":2,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"978\":1,\"979\":1,\"980\":1,\"981\":1,\"1008\":1,\"1030\":2,\"1031\":1,\"1032\":2,\"1034\":2,\"1036\":2,\"1038\":2,\"1040\":2,\"1042\":2,\"1044\":2,\"1046\":2,\"1048\":2,\"1051\":3,\"1054\":1,\"1055\":2,\"1057\":2,\"1059\":2,\"1061\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":2,\"1068\":2,\"1072\":1,\"1074\":1,\"1075\":1,\"1076\":2,\"1078\":2,\"1080\":1,\"1082\":1,\"1084\":2,\"1086\":1,\"1087\":2,\"1089\":2,\"1091\":2,\"1093\":2,\"1095\":2,\"1097\":2,\"1099\":2,\"1101\":2,\"1103\":2,\"1105\":2,\"1108\":2,\"1110\":2,\"1112\":2,\"1113\":1,\"1114\":2,\"1119\":3,\"1120\":2,\"1122\":2,\"1126\":1,\"1127\":1,\"1132\":1,\"1133\":1,\"1134\":2,\"1137\":2,\"1139\":2,\"1142\":2,\"1144\":1,\"1145\":2,\"1148\":1,\"1149\":2,\"1151\":2,\"1153\":3,\"1156\":3,\"1157\":2,\"1158\":1,\"1159\":2,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":2,\"1167\":1,\"1168\":2,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1177\":2,\"1179\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":2,\"1187\":2,\"1190\":2,\"1192\":2,\"1194\":2,\"1196\":2,\"1198\":1,\"1199\":1,\"1200\":2,\"1202\":3,\"1205\":2,\"1207\":1,\"1208\":1,\"1210\":1,\"1211\":3,\"1213\":2,\"1215\":2,\"1217\":1,\"1219\":2,\"1222\":1,\"1223\":1,\"1225\":1,\"1226\":2,\"1230\":2,\"1233\":2,\"1236\":2,\"1238\":2,\"1240\":2,\"1242\":2,\"1246\":1,\"1247\":1,\"1248\":2,\"1250\":2,\"1251\":1,\"1252\":1,\"1253\":2,\"1255\":2,\"1257\":2,\"1259\":3,\"1261\":2,\"1262\":2,\"1264\":1,\"1265\":2,\"1269\":2,\"1270\":3,\"1271\":3,\"1272\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1279\":2,\"1280\":1,\"1281\":2,\"1282\":1,\"1283\":1,\"1284\":2,\"1286\":2,\"1288\":2,\"1290\":1,\"1301\":2,\"1306\":1,\"1330\":1,\"1333\":1,\"1334\":3,\"1350\":1,\"1371\":1,\"1372\":2,\"1381\":1,\"1383\":2,\"1387\":2,\"1389\":4,\"1392\":3,\"1396\":4,\"1398\":2,\"1400\":1,\"1401\":4,\"1404\":2,\"1406\":2,\"1408\":4,\"1411\":2,\"1413\":2,\"1415\":1,\"1417\":2,\"1419\":1,\"1420\":6,\"1422\":1,\"1424\":2,\"1426\":2,\"1428\":2,\"1430\":2,\"1433\":2,\"1435\":2,\"1437\":2,\"1439\":2,\"1441\":1,\"1442\":2,\"1444\":2,\"1446\":2,\"1448\":2,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1458\":2,\"1460\":2,\"1462\":2,\"1464\":2,\"1466\":4,\"1469\":2,\"1508\":1,\"1509\":2,\"1511\":2,\"1515\":1,\"1516\":1,\"1517\":2,\"1522\":2,\"1526\":4,\"1527\":2,\"1530\":2,\"1533\":1,\"1537\":2,\"1539\":2,\"1541\":2,\"1543\":2,\"1545\":1,\"1547\":1,\"1549\":1,\"1553\":4,\"1554\":2,\"1558\":1,\"1576\":1,\"1584\":3,\"1587\":6,\"1588\":1,\"1590\":1,\"1591\":3,\"1598\":4,\"1600\":4,\"1601\":1,\"1602\":1,\"1603\":1,\"1625\":4,\"1629\":1,\"1638\":2,\"1640\":2,\"1641\":2,\"1652\":2,\"1656\":1,\"1657\":2,\"1660\":1,\"1662\":2,\"1664\":1,\"1665\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1692\":3,\"1702\":1,\"1719\":2,\"1720\":2,\"1721\":1,\"1725\":6,\"1730\":1,\"1750\":1,\"1758\":1,\"1782\":1,\"1788\":1,\"1794\":1,\"1806\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"1838\":1,\"1855\":1,\"1905\":1,\"1906\":1,\"1919\":1,\"1920\":2,\"1934\":1,\"1938\":2,\"1940\":2,\"1942\":2,\"1944\":1,\"1945\":2,\"1947\":1,\"1957\":1,\"1959\":2,\"1961\":1,\"1962\":1,\"1965\":2,\"1967\":2,\"1969\":2,\"1971\":1,\"1972\":2,\"1974\":1,\"1975\":3,\"1977\":1,\"1978\":2,\"1979\":1,\"1980\":2,\"1981\":1,\"1982\":2,\"1983\":1,\"1984\":1,\"1985\":2,\"1987\":1,\"1988\":2,\"1990\":1,\"1991\":1,\"1994\":1,\"1996\":1,\"1997\":2,\"2001\":1,\"2005\":1,\"2014\":1,\"2016\":1,\"2026\":2,\"2028\":2,\"2030\":2,\"2032\":2,\"2034\":2,\"2039\":1,\"2043\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2124\":2,\"2126\":1,\"2127\":2,\"2129\":1,\"2130\":1,\"2131\":1,\"2134\":1,\"2136\":2,\"2141\":1,\"2143\":2,\"2144\":1,\"2146\":1,\"2147\":2,\"2162\":1,\"2167\":1,\"2168\":2,\"2170\":2,\"2172\":2,\"2174\":2,\"2176\":2,\"2177\":2,\"2179\":2,\"2181\":2,\"2183\":1,\"2184\":1,\"2185\":2,\"2187\":1,\"2188\":2,\"2190\":1,\"2191\":1,\"2192\":2,\"2194\":2,\"2196\":2,\"2198\":2,\"2200\":2,\"2202\":1,\"2203\":2,\"2205\":2,\"2207\":1,\"2208\":1,\"2209\":2,\"2211\":2,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":3,\"2220\":2,\"2221\":2,\"2222\":1,\"2232\":1,\"2238\":1,\"2246\":4,\"2248\":4,\"2249\":7,\"2250\":4,\"2251\":4,\"2252\":4,\"2253\":5,\"2254\":4,\"2255\":4,\"2256\":4,\"2257\":4,\"2259\":4,\"2260\":4,\"2261\":4,\"2263\":4,\"2264\":4,\"2265\":4,\"2266\":4,\"2267\":4,\"2268\":4,\"2269\":4,\"2270\":4,\"2271\":4,\"2272\":4,\"2273\":4,\"2286\":2,\"2298\":1,\"2303\":1,\"2305\":2,\"2307\":1,\"2325\":2,\"2327\":1,\"2338\":1,\"2345\":1,\"2347\":1,\"2354\":3,\"2355\":2,\"2369\":1,\"2371\":1,\"2380\":1,\"2401\":2,\"2403\":1,\"2405\":2,\"2407\":1,\"2409\":2,\"2414\":2,\"2415\":1,\"2416\":2,\"2417\":1,\"2418\":2,\"2419\":1,\"2420\":1,\"2427\":1,\"2431\":1,\"2434\":2,\"2443\":2,\"2445\":1,\"2449\":2,\"2451\":2,\"2453\":2,\"2455\":1,\"2456\":2,\"2458\":2,\"2460\":2,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":2,\"2467\":2,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1}}],[\"iv\",{\"1\":{\"1319\":1}}],[\"i+1\",{\"1\":{\"1245\":1}}],[\"iii\",{\"1\":{\"1124\":1,\"1180\":1,\"1181\":1}}],[\"i1\",{\"1\":{\"287\":1}}],[\"irrelevant\",{\"1\":{\"246\":1}}],[\"ie\",{\"1\":{\"760\":1,\"770\":1,\"959\":1}}],[\"ieeexplore\",{\"1\":{\"616\":3,\"696\":3,\"697\":3,\"1061\":1,\"1062\":1,\"1131\":1,\"1172\":1,\"1309\":1,\"1311\":1,\"1321\":1,\"1322\":1,\"1327\":1,\"1330\":1}}],[\"ieee\",{\"1\":{\"207\":1,\"616\":3,\"696\":3,\"697\":3,\"1061\":1,\"1062\":1,\"1131\":1,\"1172\":1,\"1309\":2,\"1311\":2,\"1321\":1,\"1322\":1,\"1327\":1,\"1330\":1,\"2203\":1,\"2209\":1}}],[\"iemocap\",{\"1\":{\"201\":1}}],[\"ilya\",{\"1\":{\"202\":1}}],[\"ilens=none\",{\"1\":{\"1250\":1,\"1758\":1,\"2231\":1}}],[\"ilens\",{\"1\":{\"678\":1,\"699\":2,\"700\":2,\"709\":2,\"710\":6,\"711\":6,\"733\":2,\"734\":2,\"745\":2,\"746\":2,\"747\":2,\"748\":2,\"771\":2,\"774\":2,\"780\":2,\"791\":1,\"792\":2,\"798\":1,\"846\":2,\"849\":2,\"862\":1,\"865\":2,\"952\":1,\"955\":2,\"963\":1,\"965\":1,\"967\":1,\"968\":1,\"969\":1,\"976\":2,\"977\":2,\"978\":3,\"979\":2,\"980\":3,\"1030\":1,\"1031\":2,\"1032\":1,\"1034\":1,\"1038\":1,\"1039\":1,\"1040\":2,\"1044\":1,\"1053\":4,\"1054\":3,\"1062\":3,\"1107\":3,\"1112\":4,\"1113\":2,\"1117\":3,\"1118\":3,\"1125\":3,\"1126\":8,\"1127\":6,\"1130\":3,\"1131\":3,\"1136\":3,\"1141\":3,\"1162\":3,\"1199\":3,\"1217\":2,\"1222\":2,\"1223\":2,\"1232\":3,\"1250\":3,\"1251\":2,\"1252\":3,\"1261\":3,\"1267\":3,\"1268\":5,\"1269\":3,\"1270\":3,\"1271\":3,\"1278\":3,\"1280\":3,\"1283\":3,\"1334\":3,\"1589\":2,\"1627\":2,\"1656\":3,\"1660\":2,\"1662\":1,\"1669\":5,\"1671\":2,\"1700\":2,\"1758\":1,\"1764\":2,\"1766\":1,\"1770\":2,\"1771\":2,\"1814\":2,\"1816\":2,\"1990\":1,\"2129\":2,\"2226\":2,\"2231\":2,\"2241\":2,\"2413\":2,\"2424\":2,\"2448\":2,\"2453\":1,\"2455\":2}}],[\"ilen\",{\"1\":{\"48\":3,\"706\":1}}],[\"ice\",{\"1\":{\"2280\":1}}],[\"icefall\",{\"1\":{\"146\":1,\"615\":1,\"644\":1,\"669\":2,\"670\":2}}],[\"ic\",{\"1\":{\"2262\":1}}],[\"icassp40776\",{\"1\":{\"207\":1}}],[\"icassp\",{\"1\":{\"9\":1,\"12\":1,\"14\":1,\"207\":2,\"691\":1,\"1061\":1,\"1062\":1,\"1210\":1,\"1264\":1,\"1270\":1,\"1271\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1334\":1,\"2208\":1,\"2209\":1}}],[\"ipq\",{\"1\":{\"927\":1}}],[\"ipython\",{\"1\":{\"163\":2,\"286\":2}}],[\"ipynb\",{\"1\":{\"3\":1,\"178\":3,\"179\":2,\"180\":1,\"181\":1,\"182\":1,\"184\":2,\"185\":2,\"186\":1,\"187\":1,\"188\":1,\"190\":9,\"191\":2,\"192\":1,\"193\":6,\"536\":1}}],[\"ip\",{\"1\":{\"67\":1}}],[\"ibm\",{\"1\":{\"1172\":1,\"1173\":1,\"1175\":1}}],[\"ib\",{\"1\":{\"67\":2}}],[\"i\",{\"0\":{\"764\":1},\"1\":{\"46\":1,\"48\":1,\"52\":1,\"59\":1,\"60\":1,\"62\":1,\"70\":1,\"74\":2,\"98\":6,\"99\":1,\"100\":1,\"102\":2,\"104\":1,\"106\":1,\"132\":1,\"142\":1,\"147\":1,\"167\":1,\"201\":1,\"224\":1,\"242\":3,\"252\":1,\"254\":1,\"271\":2,\"280\":2,\"287\":9,\"290\":1,\"536\":1,\"538\":1,\"649\":1,\"667\":1,\"692\":2,\"709\":2,\"710\":2,\"711\":2,\"764\":1,\"774\":2,\"780\":2,\"803\":2,\"804\":2,\"809\":1,\"822\":1,\"830\":2,\"831\":1,\"846\":1,\"849\":2,\"924\":1,\"926\":2,\"939\":1,\"1008\":1,\"1051\":2,\"1107\":2,\"1176\":1,\"1245\":5,\"1268\":2,\"1274\":2,\"1278\":2,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"1328\":1,\"1610\":1,\"1626\":1,\"1628\":1,\"1655\":1,\"1686\":3,\"1694\":3,\"1719\":2,\"1725\":2,\"1731\":2,\"1735\":2,\"1751\":2,\"1759\":2,\"1799\":1,\"1806\":2,\"1822\":2,\"1833\":2,\"1880\":2,\"1958\":1,\"1960\":1,\"1961\":1,\"1962\":1,\"1966\":1,\"1992\":2,\"1995\":2,\"2129\":2,\"2130\":2,\"2133\":1,\"2136\":2,\"2137\":1,\"2299\":2}}],[\"id=none\",{\"1\":{\"1731\":1}}],[\"id=0nqwnnwaori\",{\"1\":{\"1589\":1}}],[\"id=\",{\"1\":{\"1171\":1}}],[\"id=1zf88brnbjhw9hnbq3nrdg8vnggibremg\",{\"1\":{\"518\":1}}],[\"idx2lang\",{\"1\":{\"2354\":1}}],[\"idxs\",{\"1\":{\"1633\":2,\"1920\":2}}],[\"idx=1\",{\"1\":{\"2327\":1}}],[\"idx=none\",{\"1\":{\"733\":1,\"734\":1,\"1279\":1,\"1281\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1768\":1,\"2467\":1}}],[\"idx=0\",{\"1\":{\"706\":1,\"796\":1,\"961\":1,\"1316\":1,\"1735\":3,\"1758\":1,\"2227\":1,\"2231\":1,\"2327\":1,\"2355\":2}}],[\"idx\",{\"1\":{\"614\":2,\"616\":2,\"634\":1,\"641\":2,\"643\":1,\"651\":2,\"700\":1,\"706\":1,\"709\":4,\"710\":3,\"711\":3,\"717\":3,\"733\":1,\"734\":1,\"771\":3,\"774\":4,\"780\":4,\"847\":2,\"849\":3,\"886\":1,\"960\":1,\"961\":1,\"1107\":3,\"1279\":1,\"1316\":3,\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1521\":1,\"1526\":1,\"1553\":1,\"1585\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1719\":2,\"1725\":6,\"1727\":1,\"1735\":2,\"1749\":3,\"1768\":1,\"1782\":2,\"1815\":2,\"1837\":1,\"1843\":3,\"1863\":2,\"1866\":2,\"1921\":3,\"1966\":1,\"1987\":1,\"2126\":1,\"2129\":3,\"2191\":3,\"2327\":7,\"2355\":16,\"2363\":2}}],[\"identifiers\",{\"1\":{\"2184\":1}}],[\"identifier\",{\"1\":{\"2133\":1,\"2140\":1,\"2148\":1}}],[\"identification\",{\"0\":{\"233\":1},\"1\":{\"175\":1,\"200\":1,\"233\":1,\"234\":1,\"235\":1,\"596\":1,\"1702\":1,\"2001\":1,\"2184\":1,\"2354\":1}}],[\"identically\",{\"1\":{\"1202\":1,\"1259\":1,\"1261\":1}}],[\"identityfeatureextract\",{\"0\":{\"2449\":1},\"1\":{\"2449\":1}}],[\"identityencoder\",{\"0\":{\"2188\":1},\"1\":{\"2188\":1}}],[\"identity\",{\"0\":{\"909\":1,\"1777\":1,\"2188\":1,\"2449\":1},\"1\":{\"243\":1,\"700\":1,\"733\":1,\"734\":1,\"764\":1,\"780\":1,\"909\":1,\"1668\":1,\"1777\":2,\"2188\":2,\"2449\":1}}],[\"ideally\",{\"1\":{\"197\":1}}],[\"idea\",{\"1\":{\"78\":1,\"1717\":1}}],[\"id3\",{\"1\":{\"97\":2}}],[\"id1\",{\"1\":{\"97\":2,\"1644\":1,\"1647\":1}}],[\"id2\",{\"1\":{\"82\":1,\"97\":2,\"1644\":1,\"1647\":1}}],[\"ida\",{\"1\":{\"76\":1}}],[\"id>\",{\"1\":{\"73\":2,\"114\":1}}],[\"id\",{\"0\":{\"596\":1,\"2278\":1,\"2283\":1,\"2291\":1},\"1\":{\"59\":1,\"69\":4,\"74\":4,\"75\":4,\"76\":3,\"80\":11,\"82\":3,\"97\":1,\"195\":1,\"196\":11,\"200\":1,\"213\":7,\"235\":1,\"240\":1,\"242\":2,\"265\":2,\"266\":2,\"267\":7,\"268\":12,\"274\":2,\"275\":2,\"276\":7,\"277\":12,\"284\":2,\"285\":2,\"286\":7,\"289\":2,\"377\":2,\"449\":2,\"513\":2,\"514\":2,\"596\":1,\"614\":5,\"616\":1,\"625\":9,\"626\":1,\"627\":3,\"630\":4,\"631\":1,\"634\":4,\"641\":6,\"642\":2,\"643\":4,\"649\":4,\"651\":6,\"667\":8,\"672\":3,\"696\":1,\"705\":1,\"706\":2,\"736\":3,\"737\":2,\"740\":3,\"761\":1,\"762\":1,\"777\":1,\"803\":1,\"804\":2,\"847\":9,\"954\":1,\"974\":1,\"1155\":1,\"1157\":1,\"1158\":1,\"1202\":2,\"1259\":2,\"1261\":2,\"1395\":1,\"1521\":7,\"1546\":2,\"1585\":4,\"1640\":2,\"1641\":2,\"1644\":1,\"1647\":1,\"1651\":1,\"1719\":3,\"1721\":2,\"1725\":5,\"1730\":2,\"1731\":3,\"1749\":13,\"1782\":2,\"1815\":11,\"1822\":2,\"1824\":2,\"1843\":6,\"1858\":2,\"1862\":2,\"1863\":1,\"1866\":1,\"1868\":3,\"1870\":3,\"1888\":4,\"1897\":2,\"1907\":2,\"1910\":6,\"1915\":1,\"1921\":3,\"1927\":1,\"1928\":1,\"1936\":2,\"1937\":1,\"1940\":1,\"1942\":1,\"1945\":1,\"1959\":2,\"1965\":1,\"1966\":1,\"1975\":1,\"1993\":2,\"1996\":2,\"1997\":2,\"2000\":3,\"2001\":1,\"2102\":1,\"2127\":2,\"2139\":1,\"2143\":1,\"2144\":2,\"2150\":3,\"2156\":1,\"2160\":1,\"2221\":4,\"2228\":7,\"2229\":7,\"2239\":2,\"2240\":2,\"2245\":2,\"2278\":1,\"2283\":1,\"2291\":1,\"2354\":7,\"2384\":1,\"2385\":1,\"2408\":7,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":2,\"2432\":2,\"2446\":7,\"2447\":3,\"2448\":1}}],[\"ids=\",{\"1\":{\"2305\":1}}],[\"ids2tokens\",{\"1\":{\"2278\":1,\"2283\":1,\"2291\":1}}],[\"ids2text\",{\"1\":{\"776\":1}}],[\"ids\",{\"0\":{\"2091\":1,\"2092\":1,\"2093\":1,\"2094\":1},\"1\":{\"44\":1,\"69\":1,\"82\":1,\"692\":3,\"775\":1,\"776\":1,\"790\":2,\"820\":1,\"850\":2,\"1526\":6,\"1552\":10,\"1553\":6,\"1644\":1,\"1647\":1,\"1719\":9,\"1725\":10,\"1730\":3,\"1731\":4,\"1758\":2,\"1806\":4,\"1822\":1,\"1936\":2,\"1944\":2,\"1947\":2,\"1965\":1,\"1966\":2,\"1992\":4,\"1993\":3,\"1995\":3,\"2001\":1,\"2045\":1,\"2124\":3,\"2128\":3,\"2132\":2,\"2137\":3,\"2139\":4,\"2141\":2,\"2144\":4,\"2156\":1,\"2231\":2,\"2235\":9,\"2236\":9,\"2239\":6,\"2240\":6,\"2245\":6,\"2411\":3,\"2412\":3,\"2423\":3,\"2431\":3,\"2432\":3,\"2447\":3}}],[\"idim=80\",{\"1\":{\"2425\":1}}],[\"idim\",{\"1\":{\"43\":6,\"48\":3,\"1199\":1,\"1526\":2,\"1553\":2,\"1598\":2,\"1599\":2,\"1600\":2,\"1625\":2,\"1668\":1,\"1736\":2,\"1738\":3,\"1739\":3,\"1740\":3,\"1741\":3,\"1742\":3,\"1743\":3,\"1744\":3,\"1745\":3,\"1746\":3,\"1750\":4,\"1752\":1,\"1753\":3,\"1758\":4,\"1766\":1,\"1809\":2,\"1810\":3,\"1811\":1,\"1812\":2,\"1814\":2,\"1816\":3,\"1851\":2,\"1863\":2,\"1878\":2,\"1885\":1,\"1893\":1,\"1985\":1,\"1992\":2,\"1993\":4,\"1994\":1,\"1995\":1,\"2223\":3,\"2227\":2,\"2231\":4,\"2235\":2,\"2236\":2,\"2239\":3,\"2240\":3,\"2245\":3,\"2411\":3,\"2412\":3,\"2423\":3,\"2425\":2,\"2428\":2,\"2429\":2,\"2431\":3,\"2432\":3,\"2433\":3,\"2447\":2}}],[\"io\",{\"0\":{\"667\":1,\"1824\":1,\"2130\":2,\"2133\":1,\"2136\":1,\"2137\":1,\"2138\":1},\"1\":{\"34\":1,\"71\":1,\"195\":1,\"667\":1,\"1678\":3,\"1824\":1,\"1881\":1,\"1937\":1,\"2130\":2,\"2133\":1,\"2136\":1,\"2137\":1,\"2138\":1,\"2143\":1,\"2249\":1}}],[\"ignores\",{\"1\":{\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"ignored\",{\"1\":{\"95\":1,\"98\":1,\"790\":1,\"1224\":1,\"1225\":1,\"1719\":2,\"1721\":2,\"1725\":2,\"1782\":1,\"1862\":2}}],[\"ignore=d\",{\"1\":{\"32\":1}}],[\"ignore\",{\"1\":{\"32\":1,\"81\":1,\"243\":1,\"625\":2,\"667\":2,\"706\":2,\"736\":1,\"737\":1,\"777\":1,\"1171\":1,\"1640\":1,\"1641\":1,\"1858\":2,\"1888\":1,\"1907\":2,\"1928\":3,\"1940\":1,\"1942\":1,\"1945\":1,\"1959\":1,\"1965\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2221\":2,\"2314\":1,\"2447\":1,\"2448\":1,\"2472\":1,\"2480\":2}}],[\"ifasnet\",{\"0\":{\"1333\":2,\"1360\":1},\"1\":{\"223\":1,\"1059\":1,\"1162\":2,\"1333\":2,\"1360\":1}}],[\"ifconfig\",{\"1\":{\"67\":1}}],[\"ifname=en\",{\"1\":{\"67\":1}}],[\"ifname=eth\",{\"1\":{\"67\":1}}],[\"ifname=^lo\",{\"1\":{\"67\":2}}],[\"ifname=<appropriate\",{\"1\":{\"66\":2}}],[\"ifname\",{\"1\":{\"66\":1}}],[\"if\",{\"1\":{\"22\":2,\"25\":1,\"31\":1,\"36\":1,\"40\":1,\"41\":3,\"42\":1,\"43\":2,\"44\":2,\"48\":2,\"49\":1,\"50\":1,\"55\":1,\"56\":1,\"60\":1,\"67\":1,\"69\":1,\"70\":1,\"71\":3,\"76\":2,\"78\":1,\"81\":2,\"82\":1,\"91\":1,\"95\":1,\"96\":1,\"102\":1,\"106\":1,\"110\":2,\"117\":1,\"123\":1,\"124\":1,\"128\":3,\"137\":1,\"138\":1,\"141\":9,\"144\":1,\"150\":3,\"152\":1,\"153\":2,\"159\":1,\"160\":1,\"161\":3,\"162\":6,\"163\":1,\"165\":2,\"167\":1,\"173\":4,\"174\":1,\"175\":3,\"195\":2,\"196\":3,\"197\":6,\"200\":5,\"205\":1,\"206\":2,\"211\":1,\"212\":1,\"213\":1,\"217\":3,\"218\":2,\"223\":1,\"224\":2,\"225\":1,\"236\":1,\"242\":4,\"243\":1,\"246\":1,\"249\":1,\"254\":4,\"255\":2,\"262\":1,\"263\":1,\"266\":5,\"267\":7,\"268\":3,\"269\":4,\"275\":5,\"276\":11,\"277\":3,\"278\":4,\"285\":7,\"286\":14,\"290\":10,\"536\":2,\"614\":1,\"619\":1,\"620\":1,\"622\":1,\"623\":1,\"629\":1,\"631\":1,\"635\":1,\"650\":2,\"652\":2,\"663\":1,\"692\":4,\"699\":8,\"701\":2,\"703\":1,\"709\":2,\"710\":2,\"711\":2,\"716\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"738\":1,\"744\":1,\"745\":1,\"756\":4,\"760\":6,\"768\":1,\"773\":4,\"774\":2,\"775\":2,\"780\":2,\"784\":1,\"790\":2,\"794\":1,\"798\":1,\"804\":1,\"817\":1,\"819\":3,\"820\":2,\"821\":2,\"827\":1,\"828\":2,\"829\":2,\"830\":2,\"831\":1,\"846\":6,\"849\":2,\"850\":2,\"862\":1,\"866\":2,\"867\":2,\"911\":3,\"932\":1,\"934\":1,\"939\":1,\"943\":2,\"974\":2,\"1008\":2,\"1022\":1,\"1029\":2,\"1061\":4,\"1062\":2,\"1063\":3,\"1064\":2,\"1080\":1,\"1084\":1,\"1107\":2,\"1118\":1,\"1126\":1,\"1209\":4,\"1228\":4,\"1235\":2,\"1245\":1,\"1246\":2,\"1250\":1,\"1251\":1,\"1262\":2,\"1273\":1,\"1274\":3,\"1278\":2,\"1279\":4,\"1280\":3,\"1281\":2,\"1282\":2,\"1283\":1,\"1290\":2,\"1306\":1,\"1334\":1,\"1356\":1,\"1371\":1,\"1400\":1,\"1420\":1,\"1450\":1,\"1452\":1,\"1469\":1,\"1485\":2,\"1493\":1,\"1494\":1,\"1501\":1,\"1513\":1,\"1525\":1,\"1533\":1,\"1548\":1,\"1551\":1,\"1552\":3,\"1558\":1,\"1581\":1,\"1586\":1,\"1588\":1,\"1592\":1,\"1596\":2,\"1597\":2,\"1599\":4,\"1603\":1,\"1604\":1,\"1605\":1,\"1607\":1,\"1609\":2,\"1610\":1,\"1613\":1,\"1616\":2,\"1619\":1,\"1626\":4,\"1628\":3,\"1662\":1,\"1672\":2,\"1675\":1,\"1680\":1,\"1692\":1,\"1698\":1,\"1702\":3,\"1719\":3,\"1720\":1,\"1721\":4,\"1725\":6,\"1731\":1,\"1735\":2,\"1748\":1,\"1750\":2,\"1751\":2,\"1759\":2,\"1760\":1,\"1782\":1,\"1784\":1,\"1806\":2,\"1814\":1,\"1816\":1,\"1822\":1,\"1854\":1,\"1859\":1,\"1862\":3,\"1866\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1897\":1,\"1898\":1,\"1901\":1,\"1903\":1,\"1919\":2,\"1927\":1,\"1963\":2,\"1966\":1,\"1992\":5,\"1993\":3,\"1995\":5,\"2015\":3,\"2016\":1,\"2020\":2,\"2021\":1,\"2039\":4,\"2044\":5,\"2045\":1,\"2049\":1,\"2054\":1,\"2065\":3,\"2129\":2,\"2130\":1,\"2134\":4,\"2136\":1,\"2139\":1,\"2144\":1,\"2145\":1,\"2155\":2,\"2162\":3,\"2184\":1,\"2220\":3,\"2224\":2,\"2235\":6,\"2236\":6,\"2239\":4,\"2240\":4,\"2245\":4,\"2246\":2,\"2248\":2,\"2249\":5,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":3,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2309\":1,\"2311\":1,\"2325\":1,\"2327\":3,\"2353\":1,\"2354\":4,\"2355\":17,\"2369\":2,\"2411\":5,\"2412\":5,\"2423\":5,\"2431\":4,\"2432\":4,\"2447\":5}}],[\"imitate\",{\"1\":{\"1881\":1}}],[\"imics\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1}}],[\"imics=1\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1}}],[\"imbalanced\",{\"1\":{\"2000\":1}}],[\"imbalance\",{\"1\":{\"1645\":1,\"2000\":2}}],[\"im\",{\"1\":{\"1366\":1}}],[\"imaginary\",{\"1\":{\"1051\":1,\"1124\":2,\"1280\":1,\"1283\":1}}],[\"imag=false\",{\"1\":{\"895\":1}}],[\"imag\",{\"1\":{\"895\":1,\"1118\":3,\"1348\":1,\"1933\":2}}],[\"images\",{\"1\":{\"22\":1,\"26\":1,\"114\":1,\"218\":1,\"267\":1,\"276\":1,\"286\":2,\"1306\":2,\"1371\":2,\"2355\":2,\"2359\":1}}],[\"image\",{\"0\":{\"114\":1},\"1\":{\"22\":1,\"26\":2,\"28\":3,\"114\":1,\"674\":4,\"1145\":4,\"1153\":1,\"1168\":2,\"1211\":1,\"1306\":2,\"1371\":2,\"2198\":1,\"2355\":1}}],[\"immediately\",{\"1\":{\"200\":1}}],[\"imgs\",{\"1\":{\"2355\":2}}],[\"img\",{\"1\":{\"114\":2}}],[\"impulse\",{\"0\":{\"1557\":1},\"1\":{\"1557\":1}}],[\"improving\",{\"1\":{\"1280\":1,\"1281\":1,\"1282\":1}}],[\"improvedtransformerlayer\",{\"0\":{\"1185\":1},\"1\":{\"1185\":1}}],[\"improved\",{\"1\":{\"1029\":2,\"1185\":1,\"1279\":2,\"1281\":2}}],[\"improvement\",{\"1\":{\"41\":1,\"173\":1,\"2020\":1}}],[\"improve\",{\"1\":{\"32\":2,\"45\":1,\"145\":1,\"262\":2}}],[\"implemenation\",{\"1\":{\"1977\":1}}],[\"implementaion\",{\"1\":{\"1402\":1,\"1467\":1,\"1594\":1,\"1595\":1}}],[\"implementation\",{\"1\":{\"36\":1,\"48\":1,\"96\":1,\"146\":1,\"232\":1,\"258\":1,\"287\":1,\"616\":4,\"696\":8,\"697\":7,\"760\":1,\"774\":1,\"778\":1,\"790\":1,\"833\":1,\"852\":1,\"922\":1,\"936\":1,\"937\":1,\"1202\":1,\"1255\":1,\"1259\":1,\"1310\":1,\"1385\":1,\"1415\":1,\"1439\":1,\"1469\":1,\"1513\":1,\"1548\":1,\"1551\":1,\"1592\":1,\"1605\":1,\"1606\":1,\"1668\":1,\"1691\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1725\":1,\"1731\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1,\"1794\":1,\"1806\":1,\"1817\":1,\"1818\":1,\"2016\":2,\"2130\":1,\"2133\":3,\"2235\":1,\"2236\":1,\"2405\":1,\"2427\":1,\"2474\":1}}],[\"implement\",{\"1\":{\"787\":1,\"821\":1,\"927\":1,\"960\":1,\"1064\":1,\"1078\":1,\"1153\":1,\"1202\":1,\"1262\":1,\"1290\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"2131\":3,\"2232\":1,\"2238\":1,\"2325\":1,\"2355\":4}}],[\"implements\",{\"1\":{\"225\":1,\"691\":1,\"828\":1,\"830\":1,\"939\":1,\"2142\":1}}],[\"implementing\",{\"1\":{\"150\":1}}],[\"implemented\",{\"0\":{\"19\":1},\"1\":{\"24\":1,\"223\":1,\"290\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":3,\"1647\":2,\"1800\":1,\"2130\":2,\"2312\":1,\"2438\":1,\"2440\":1}}],[\"impluse\",{\"1\":{\"1631\":1}}],[\"implants\",{\"1\":{\"1327\":1,\"1330\":1}}],[\"implicit\",{\"1\":{\"1162\":1}}],[\"implies\",{\"1\":{\"787\":1}}],[\"impose\",{\"1\":{\"269\":1,\"278\":1}}],[\"importance\",{\"1\":{\"2130\":1}}],[\"important\",{\"0\":{\"52\":1},\"1\":{\"42\":1,\"101\":1,\"138\":1,\"232\":1,\"242\":2,\"243\":1,\"258\":1}}],[\"importerror\",{\"1\":{\"760\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2065\":1}}],[\"imported\",{\"1\":{\"26\":1,\"1876\":1}}],[\"import\",{\"0\":{\"1876\":2},\"1\":{\"26\":1,\"71\":1,\"78\":2,\"82\":2,\"154\":1,\"220\":2,\"243\":1,\"286\":3,\"290\":4,\"1876\":5,\"1938\":1,\"2246\":3,\"2248\":3,\"2249\":3,\"2250\":3,\"2251\":3,\"2252\":3,\"2253\":3,\"2254\":3,\"2255\":3,\"2256\":3,\"2257\":3,\"2259\":3,\"2260\":3,\"2261\":3,\"2263\":3,\"2264\":3,\"2265\":2,\"2266\":3,\"2267\":3,\"2268\":3,\"2269\":3,\"2270\":3,\"2271\":3,\"2272\":3,\"2273\":3,\"2299\":1,\"2325\":1,\"2327\":1,\"2334\":1,\"2377\":3,\"2485\":1,\"2493\":1,\"2500\":1,\"2505\":1}}],[\"isg2p\",{\"0\":{\"2280\":1},\"1\":{\"2280\":1}}],[\"isvalid\",{\"1\":{\"2239\":2}}],[\"isvalid=false\",{\"1\":{\"2228\":1,\"2229\":1,\"2235\":1,\"2236\":1,\"2240\":1,\"2245\":1}}],[\"isn\",{\"1\":{\"2016\":1,\"2276\":1,\"2277\":1}}],[\"istft\",{\"0\":{\"1776\":1,\"1899\":1},\"1\":{\"1250\":1,\"1776\":2,\"1899\":1}}],[\"isik16\",{\"1\":{\"1130\":1}}],[\"isik\",{\"1\":{\"1130\":1}}],[\"isca\",{\"1\":{\"1061\":2,\"1062\":2,\"1130\":1,\"1131\":1,\"1172\":1,\"2168\":1}}],[\"isotropic\",{\"1\":{\"828\":1,\"1802\":1}}],[\"isort\",{\"1\":{\"225\":1}}],[\"issn\",{\"1\":{\"156\":1}}],[\"issuecomment\",{\"1\":{\"1411\":1}}],[\"issue\",{\"1\":{\"19\":1,\"42\":2,\"56\":1,\"132\":2,\"138\":2,\"153\":1,\"249\":1,\"290\":1}}],[\"issues\",{\"1\":{\"3\":1,\"269\":1,\"278\":1,\"831\":1,\"1411\":1,\"1608\":1,\"2039\":1,\"2240\":1}}],[\"is\",{\"0\":{\"70\":1,\"913\":1,\"914\":1,\"915\":1,\"1338\":1,\"1339\":1,\"1488\":1,\"1897\":1,\"1898\":1,\"2313\":1,\"2388\":1,\"2389\":1},\"1\":{\"2\":2,\"3\":2,\"22\":1,\"23\":2,\"24\":1,\"25\":1,\"26\":5,\"32\":1,\"34\":1,\"36\":1,\"37\":1,\"38\":1,\"39\":2,\"41\":1,\"43\":4,\"44\":1,\"46\":4,\"47\":1,\"48\":2,\"50\":1,\"55\":2,\"60\":1,\"66\":3,\"67\":3,\"68\":4,\"70\":2,\"71\":3,\"73\":1,\"74\":3,\"75\":1,\"78\":4,\"79\":6,\"80\":3,\"81\":4,\"82\":12,\"84\":1,\"87\":1,\"91\":3,\"94\":4,\"95\":1,\"96\":2,\"97\":5,\"98\":4,\"99\":1,\"100\":1,\"101\":4,\"102\":4,\"104\":1,\"106\":2,\"107\":2,\"108\":1,\"110\":3,\"111\":1,\"114\":1,\"119\":2,\"123\":2,\"124\":1,\"126\":2,\"127\":1,\"128\":6,\"135\":1,\"137\":1,\"138\":3,\"139\":1,\"140\":1,\"141\":4,\"142\":1,\"143\":1,\"144\":1,\"146\":1,\"147\":1,\"154\":1,\"159\":1,\"161\":2,\"162\":2,\"164\":2,\"166\":1,\"167\":2,\"168\":2,\"173\":5,\"175\":4,\"194\":2,\"196\":2,\"197\":4,\"198\":1,\"200\":7,\"203\":2,\"205\":6,\"208\":1,\"209\":1,\"211\":2,\"214\":1,\"215\":1,\"217\":1,\"220\":1,\"221\":1,\"222\":1,\"223\":15,\"224\":2,\"225\":2,\"226\":1,\"228\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":4,\"233\":2,\"235\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":3,\"242\":10,\"243\":11,\"245\":2,\"246\":1,\"248\":1,\"250\":1,\"251\":1,\"252\":3,\"254\":2,\"257\":1,\"258\":4,\"260\":2,\"262\":11,\"263\":1,\"264\":1,\"266\":3,\"267\":9,\"268\":3,\"269\":3,\"273\":1,\"275\":4,\"276\":14,\"277\":3,\"278\":3,\"283\":1,\"284\":3,\"285\":3,\"286\":11,\"289\":1,\"290\":13,\"291\":1,\"292\":1,\"356\":2,\"481\":1,\"505\":2,\"527\":1,\"536\":4,\"538\":1,\"616\":1,\"639\":6,\"653\":1,\"663\":1,\"665\":3,\"671\":1,\"675\":1,\"691\":1,\"692\":3,\"696\":1,\"699\":3,\"701\":2,\"703\":1,\"704\":3,\"705\":1,\"710\":3,\"711\":3,\"716\":1,\"722\":1,\"724\":2,\"725\":2,\"728\":2,\"729\":2,\"733\":2,\"734\":1,\"736\":1,\"737\":2,\"749\":2,\"756\":8,\"759\":2,\"760\":6,\"764\":1,\"768\":3,\"770\":1,\"773\":8,\"775\":1,\"777\":1,\"786\":1,\"787\":1,\"790\":3,\"800\":1,\"803\":1,\"804\":1,\"817\":1,\"820\":2,\"821\":6,\"824\":3,\"828\":1,\"829\":5,\"830\":2,\"831\":2,\"846\":5,\"850\":2,\"852\":1,\"859\":2,\"866\":5,\"867\":5,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"911\":3,\"913\":1,\"914\":1,\"915\":1,\"921\":1,\"924\":1,\"925\":1,\"928\":1,\"932\":1,\"934\":1,\"935\":1,\"939\":1,\"954\":1,\"959\":1,\"971\":3,\"972\":1,\"974\":3,\"975\":3,\"978\":2,\"980\":3,\"982\":1,\"984\":3,\"986\":1,\"993\":1,\"994\":1,\"1002\":1,\"1008\":6,\"1009\":1,\"1011\":1,\"1022\":1,\"1029\":4,\"1031\":1,\"1035\":1,\"1061\":1,\"1063\":1,\"1066\":2,\"1072\":2,\"1074\":2,\"1075\":1,\"1112\":1,\"1113\":1,\"1117\":2,\"1119\":2,\"1124\":1,\"1128\":1,\"1130\":2,\"1131\":2,\"1134\":3,\"1136\":2,\"1137\":3,\"1139\":3,\"1141\":2,\"1145\":1,\"1155\":2,\"1156\":3,\"1157\":2,\"1158\":1,\"1162\":1,\"1168\":1,\"1170\":2,\"1171\":2,\"1172\":2,\"1173\":2,\"1174\":4,\"1175\":2,\"1176\":1,\"1179\":3,\"1185\":2,\"1202\":3,\"1209\":1,\"1210\":3,\"1224\":1,\"1225\":1,\"1228\":2,\"1232\":2,\"1235\":3,\"1246\":6,\"1247\":3,\"1248\":2,\"1250\":1,\"1251\":1,\"1255\":3,\"1257\":2,\"1259\":4,\"1261\":5,\"1264\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":2,\"1273\":2,\"1274\":2,\"1275\":2,\"1276\":4,\"1277\":2,\"1279\":5,\"1280\":18,\"1281\":5,\"1282\":1,\"1283\":12,\"1296\":3,\"1301\":3,\"1306\":4,\"1308\":2,\"1309\":4,\"1310\":4,\"1311\":1,\"1316\":3,\"1320\":1,\"1328\":1,\"1330\":1,\"1334\":2,\"1338\":1,\"1339\":1,\"1354\":2,\"1371\":4,\"1372\":3,\"1395\":1,\"1420\":1,\"1450\":1,\"1452\":1,\"1484\":4,\"1485\":2,\"1488\":1,\"1493\":1,\"1494\":1,\"1501\":1,\"1521\":1,\"1526\":11,\"1533\":2,\"1534\":4,\"1545\":3,\"1546\":1,\"1552\":2,\"1553\":12,\"1558\":2,\"1585\":1,\"1597\":1,\"1598\":5,\"1600\":4,\"1608\":1,\"1611\":1,\"1612\":1,\"1616\":1,\"1622\":1,\"1625\":5,\"1626\":2,\"1631\":1,\"1640\":2,\"1641\":2,\"1644\":1,\"1645\":1,\"1647\":1,\"1650\":1,\"1655\":2,\"1662\":1,\"1668\":5,\"1680\":1,\"1692\":2,\"1698\":1,\"1706\":1,\"1711\":1,\"1712\":1,\"1713\":2,\"1714\":1,\"1715\":2,\"1716\":2,\"1719\":4,\"1720\":4,\"1721\":3,\"1725\":5,\"1726\":1,\"1727\":2,\"1729\":1,\"1730\":1,\"1731\":2,\"1735\":2,\"1748\":1,\"1749\":1,\"1750\":6,\"1752\":1,\"1753\":1,\"1754\":1,\"1755\":2,\"1756\":2,\"1757\":2,\"1758\":2,\"1759\":1,\"1779\":3,\"1788\":1,\"1789\":2,\"1790\":2,\"1794\":2,\"1795\":2,\"1804\":1,\"1806\":5,\"1810\":1,\"1811\":1,\"1833\":1,\"1842\":1,\"1855\":2,\"1859\":1,\"1862\":2,\"1869\":1,\"1871\":3,\"1881\":2,\"1883\":4,\"1897\":3,\"1898\":2,\"1907\":1,\"1920\":1,\"1921\":3,\"1933\":1,\"1936\":1,\"1938\":1,\"1941\":1,\"1943\":2,\"1951\":1,\"1959\":2,\"1971\":2,\"1975\":1,\"1976\":1,\"1977\":1,\"1992\":2,\"1993\":1,\"1994\":1,\"1995\":1,\"1996\":1,\"1997\":2,\"2000\":9,\"2001\":4,\"2006\":3,\"2016\":4,\"2017\":2,\"2018\":4,\"2019\":1,\"2020\":2,\"2021\":1,\"2039\":1,\"2040\":1,\"2044\":4,\"2045\":1,\"2049\":2,\"2065\":2,\"2127\":2,\"2130\":10,\"2131\":2,\"2136\":1,\"2137\":2,\"2139\":1,\"2144\":1,\"2145\":3,\"2146\":1,\"2147\":1,\"2155\":3,\"2162\":3,\"2184\":1,\"2220\":3,\"2221\":2,\"2222\":2,\"2223\":1,\"2224\":3,\"2225\":1,\"2227\":2,\"2228\":2,\"2229\":2,\"2231\":2,\"2235\":10,\"2236\":10,\"2239\":10,\"2240\":11,\"2245\":10,\"2246\":5,\"2248\":5,\"2249\":9,\"2250\":5,\"2251\":5,\"2252\":5,\"2253\":7,\"2254\":5,\"2255\":5,\"2256\":5,\"2257\":5,\"2259\":5,\"2260\":5,\"2261\":5,\"2262\":1,\"2263\":5,\"2264\":5,\"2265\":4,\"2266\":5,\"2267\":5,\"2268\":5,\"2269\":5,\"2270\":5,\"2271\":5,\"2272\":5,\"2273\":5,\"2276\":1,\"2277\":1,\"2286\":1,\"2287\":1,\"2298\":1,\"2307\":1,\"2309\":2,\"2313\":1,\"2325\":1,\"2327\":3,\"2344\":1,\"2345\":2,\"2353\":4,\"2354\":4,\"2355\":13,\"2364\":4,\"2366\":1,\"2367\":1,\"2380\":2,\"2384\":1,\"2385\":1,\"2388\":1,\"2389\":1,\"2403\":2,\"2404\":1,\"2405\":2,\"2408\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2425\":1,\"2427\":2,\"2428\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":1,\"2433\":1,\"2438\":1,\"2440\":1,\"2445\":2,\"2446\":1,\"2447\":1,\"2449\":1,\"2462\":2,\"2469\":1,\"2470\":6,\"2471\":3,\"2472\":1,\"2473\":3,\"2474\":2}}],[\"inria\",{\"1\":{\"1327\":1,\"1330\":1}}],[\"ineube\",{\"0\":{\"1334\":2},\"1\":{\"1145\":2,\"1168\":1,\"1264\":3,\"1265\":2,\"1334\":5}}],[\"in=400\",{\"1\":{\"768\":1}}],[\"inline\",{\"1\":{\"755\":1,\"785\":1}}],[\"inner\",{\"0\":{\"1299\":1},\"1\":{\"633\":1,\"1299\":1,\"1350\":1,\"1392\":3}}],[\"inyaml\",{\"1\":{\"545\":1}}],[\"inherit\",{\"1\":{\"2325\":1,\"2369\":1}}],[\"inheritance\",{\"1\":{\"1962\":1}}],[\"inherited\",{\"1\":{\"1209\":1,\"1228\":1}}],[\"inherite\",{\"1\":{\"1156\":3}}],[\"inherits\",{\"1\":{\"262\":1,\"2325\":1,\"2327\":1}}],[\"inheriting\",{\"1\":{\"78\":1,\"784\":1,\"2148\":1}}],[\"invariant\",{\"1\":{\"1066\":2,\"1156\":1,\"1204\":1,\"1209\":2,\"1228\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1280\":1,\"1281\":1,\"1282\":1}}],[\"invalid\",{\"0\":{\"2477\":1},\"1\":{\"801\":1,\"2145\":1,\"2477\":1}}],[\"inv\",{\"1\":{\"821\":2}}],[\"invert\",{\"1\":{\"1654\":1,\"1666\":1,\"1899\":1}}],[\"inverts\",{\"1\":{\"1335\":1}}],[\"invers=false\",{\"1\":{\"1566\":1}}],[\"inversible\",{\"0\":{\"1659\":1},\"1\":{\"1659\":1}}],[\"inversibleinterface\",{\"0\":{\"1659\":1},\"1\":{\"1521\":3,\"1585\":3,\"1656\":1,\"1659\":1,\"1669\":1,\"2228\":3,\"2229\":3,\"2408\":3,\"2446\":2}}],[\"inversion\",{\"1\":{\"1334\":1}}],[\"inverse=false\",{\"1\":{\"1634\":1,\"1635\":1,\"1636\":1}}],[\"inverse\",{\"0\":{\"1337\":1,\"1693\":1},\"1\":{\"1314\":2,\"1315\":2,\"1337\":1,\"1377\":2,\"1581\":4,\"1586\":4,\"1588\":4,\"1603\":4,\"1612\":3,\"1613\":4,\"1616\":5,\"1656\":1,\"1659\":1,\"1669\":2,\"1693\":1}}],[\"investigated\",{\"1\":{\"691\":1}}],[\"investing\",{\"1\":{\"269\":1,\"278\":1}}],[\"invent\",{\"1\":{\"242\":1}}],[\"involves\",{\"1\":{\"262\":1,\"269\":1,\"278\":1}}],[\"invoked\",{\"1\":{\"102\":1,\"109\":1,\"1962\":1,\"2249\":1,\"2253\":1}}],[\"invoking\",{\"1\":{\"3\":1}}],[\"inout\",{\"1\":{\"48\":1}}],[\"inoue\",{\"1\":{\"9\":1}}],[\"inc\",{\"1\":{\"1566\":1}}],[\"inchannels\",{\"1\":{\"1301\":2,\"1372\":2}}],[\"inclusive\",{\"1\":{\"703\":1,\"755\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"919\":1}}],[\"included\",{\"1\":{\"70\":1,\"167\":1,\"200\":1,\"223\":1,\"228\":1,\"232\":1,\"243\":1,\"259\":1,\"625\":1,\"1053\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1131\":1,\"1132\":1,\"1136\":1,\"1141\":1,\"1162\":1,\"1217\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1278\":1,\"1280\":1,\"1283\":1}}],[\"includes\",{\"1\":{\"28\":1,\"87\":1,\"196\":1,\"254\":2,\"268\":1,\"277\":1,\"536\":1,\"2130\":1}}],[\"include\",{\"1\":{\"3\":1,\"39\":1,\"196\":1,\"197\":2,\"200\":1,\"213\":1,\"223\":5,\"235\":1,\"242\":1,\"268\":1,\"277\":1,\"536\":1,\"692\":2,\"785\":1,\"790\":1,\"819\":1,\"846\":1,\"850\":1,\"922\":2,\"1228\":1,\"1368\":1,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1526\":1,\"1553\":1,\"1587\":3,\"1598\":1,\"1600\":1,\"1606\":1,\"1625\":1,\"1683\":1,\"1992\":1,\"2043\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2134\":1,\"2355\":4}}],[\"including\",{\"1\":{\"3\":2,\"38\":2,\"53\":1,\"119\":1,\"162\":1,\"174\":1,\"246\":1,\"276\":1,\"286\":2,\"716\":1,\"804\":1,\"932\":1,\"934\":1,\"1000\":3,\"1002\":1,\"1549\":1,\"1901\":1,\"1903\":1,\"1993\":1,\"2039\":1,\"2044\":1,\"2045\":1,\"2131\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"incrementalpytorchpickler\",{\"0\":{\"2050\":1}}],[\"incremental\",{\"0\":{\"2097\":1},\"1\":{\"469\":2,\"787\":1,\"1720\":1,\"1727\":1}}],[\"increased\",{\"1\":{\"821\":1}}],[\"increases\",{\"1\":{\"262\":1,\"1008\":1,\"2018\":1}}],[\"increase\",{\"1\":{\"45\":1,\"55\":2,\"123\":2,\"145\":1,\"173\":3,\"175\":1,\"777\":1,\"817\":1,\"1156\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1940\":1,\"1942\":1}}],[\"incurs\",{\"1\":{\"262\":1}}],[\"incoming\",{\"1\":{\"1662\":1}}],[\"inconsistent\",{\"1\":{\"2155\":1}}],[\"inconsistencies\",{\"1\":{\"1332\":1}}],[\"inconvenient\",{\"1\":{\"47\":1}}],[\"incorporates\",{\"1\":{\"262\":1}}],[\"incorporated\",{\"1\":{\"200\":1}}],[\"incorporate\",{\"1\":{\"200\":1}}],[\"incorrectly\",{\"1\":{\"831\":1}}],[\"incorrect\",{\"1\":{\"66\":1}}],[\"infs\",{\"1\":{\"918\":1,\"1209\":2}}],[\"infer\",{\"0\":{\"2122\":1},\"1\":{\"710\":4,\"711\":4,\"1556\":1,\"1735\":2,\"2239\":1}}],[\"inferencing\",{\"1\":{\"222\":1,\"223\":1}}],[\"inferences\",{\"1\":{\"1209\":1}}],[\"inference\",{\"0\":{\"45\":1,\"145\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"361\":1,\"368\":1,\"377\":1,\"384\":1,\"389\":1,\"396\":1,\"404\":1,\"406\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"456\":1,\"457\":1,\"463\":1,\"469\":1,\"475\":1,\"484\":1,\"490\":1,\"498\":1,\"505\":1,\"2022\":1,\"2023\":1,\"2024\":1,\"2025\":1,\"2057\":1,\"2062\":1,\"2063\":1,\"2075\":1,\"2091\":1,\"2092\":1,\"2093\":1,\"2094\":1,\"2103\":1,\"2105\":1,\"2122\":1},\"1\":{\"78\":2,\"81\":3,\"109\":1,\"120\":2,\"126\":2,\"132\":1,\"133\":1,\"134\":3,\"136\":1,\"137\":4,\"139\":1,\"142\":1,\"148\":2,\"173\":1,\"178\":2,\"179\":1,\"181\":1,\"197\":1,\"199\":1,\"200\":2,\"201\":3,\"204\":1,\"205\":2,\"210\":1,\"211\":1,\"217\":2,\"220\":1,\"223\":11,\"228\":5,\"233\":1,\"234\":1,\"235\":2,\"240\":1,\"242\":2,\"243\":6,\"252\":1,\"254\":1,\"262\":2,\"263\":5,\"265\":1,\"266\":3,\"267\":10,\"274\":1,\"275\":3,\"276\":3,\"285\":2,\"286\":19,\"290\":8,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":2,\"342\":4,\"349\":4,\"356\":1,\"361\":4,\"368\":2,\"377\":1,\"389\":1,\"396\":1,\"404\":2,\"406\":2,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"457\":1,\"463\":1,\"469\":1,\"475\":1,\"484\":2,\"490\":2,\"498\":1,\"505\":1,\"543\":1,\"616\":1,\"634\":1,\"643\":2,\"649\":1,\"696\":1,\"697\":1,\"710\":1,\"711\":1,\"737\":1,\"776\":3,\"784\":1,\"792\":1,\"794\":2,\"795\":3,\"849\":1,\"958\":1,\"978\":1,\"1156\":2,\"1209\":1,\"1389\":2,\"1401\":2,\"1408\":2,\"1466\":2,\"1521\":1,\"1526\":3,\"1548\":2,\"1552\":2,\"1553\":3,\"1592\":2,\"1598\":3,\"1599\":2,\"1600\":3,\"1605\":2,\"1610\":2,\"1619\":2,\"1625\":3,\"1626\":2,\"1640\":1,\"1641\":1,\"1702\":2,\"1750\":1,\"1753\":4,\"1758\":2,\"1794\":1,\"1959\":1,\"1965\":3,\"1971\":1,\"1975\":1,\"1976\":1,\"1992\":1,\"1993\":4,\"1997\":1,\"2043\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2127\":1,\"2131\":1,\"2184\":1,\"2217\":1,\"2221\":1,\"2222\":1,\"2224\":1,\"2227\":2,\"2228\":1,\"2229\":1,\"2231\":2,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":4,\"2246\":2,\"2247\":6,\"2248\":2,\"2249\":5,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2262\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2354\":3,\"2403\":1,\"2408\":1,\"2411\":2,\"2412\":1,\"2423\":1,\"2428\":3,\"2431\":4,\"2432\":4,\"2445\":1,\"2446\":1,\"2447\":1,\"2462\":1}}],[\"infinity\",{\"1\":{\"691\":1,\"706\":3,\"2065\":1}}],[\"infinite\",{\"1\":{\"100\":1,\"706\":1}}],[\"infile\",{\"1\":{\"572\":1}}],[\"inf\",{\"1\":{\"224\":1,\"356\":2,\"543\":2,\"736\":1,\"794\":7,\"849\":2,\"950\":1,\"1036\":1,\"1042\":1,\"1066\":2,\"1132\":2,\"1167\":2,\"1170\":2,\"1171\":2,\"1172\":2,\"1173\":2,\"1175\":2,\"1204\":2,\"1228\":3,\"1248\":1,\"1275\":2,\"1277\":2,\"1290\":1,\"2065\":1}}],[\"info=false\",{\"1\":{\"1529\":3}}],[\"info\",{\"1\":{\"66\":1,\"113\":3,\"218\":1,\"220\":1,\"223\":1,\"267\":1,\"276\":1,\"286\":1,\"290\":2,\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"377\":1,\"385\":1,\"389\":1,\"396\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":1,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"511\":1,\"1381\":1,\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1529\":3,\"1948\":6,\"2143\":1,\"2219\":2,\"2353\":2,\"2364\":2}}],[\"information\",{\"1\":{\"3\":1,\"18\":1,\"22\":1,\"36\":1,\"39\":2,\"55\":1,\"95\":4,\"96\":2,\"97\":3,\"98\":1,\"99\":1,\"123\":1,\"139\":1,\"141\":1,\"150\":1,\"173\":1,\"196\":2,\"200\":3,\"205\":3,\"211\":1,\"213\":2,\"217\":4,\"223\":1,\"235\":1,\"242\":3,\"243\":2,\"254\":1,\"262\":1,\"266\":1,\"268\":3,\"269\":1,\"275\":1,\"277\":3,\"278\":1,\"285\":2,\"535\":1,\"699\":3,\"705\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"744\":1,\"804\":2,\"828\":1,\"829\":1,\"830\":1,\"859\":1,\"930\":2,\"1021\":1,\"1064\":1,\"1076\":1,\"1078\":1,\"1153\":1,\"1202\":1,\"1262\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"1290\":1,\"1381\":1,\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1529\":3,\"1644\":1,\"1647\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1948\":1,\"1951\":1,\"2006\":1,\"2136\":2,\"2232\":1,\"2235\":1,\"2236\":1,\"2238\":1,\"2245\":1,\"2353\":1,\"2355\":2,\"2364\":1,\"2422\":1}}],[\"ind\",{\"1\":{\"1400\":3,\"1469\":1}}],[\"indent=none\",{\"1\":{\"2480\":1}}],[\"indexer\",{\"1\":{\"919\":4}}],[\"indexed\",{\"1\":{\"738\":1,\"2134\":1}}],[\"indexing\",{\"1\":{\"716\":1,\"717\":1,\"755\":1}}],[\"index\",{\"0\":{\"716\":1,\"910\":1},\"1\":{\"276\":3,\"630\":2,\"642\":1,\"649\":2,\"696\":2,\"703\":1,\"705\":1,\"706\":1,\"716\":5,\"717\":3,\"755\":1,\"785\":1,\"830\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"910\":1,\"922\":2,\"936\":1,\"937\":1,\"1029\":1,\"1070\":1,\"1071\":1,\"1073\":1,\"1125\":1,\"1126\":1,\"1140\":2,\"1155\":1,\"1157\":1,\"1235\":1,\"1268\":1,\"1274\":1,\"1279\":2,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1290\":1,\"1316\":1,\"1389\":1,\"1395\":1,\"1400\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1521\":4,\"1526\":4,\"1546\":1,\"1552\":1,\"1553\":4,\"1585\":4,\"1598\":5,\"1599\":7,\"1600\":3,\"1622\":1,\"1625\":7,\"1626\":7,\"1632\":1,\"1633\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1726\":2,\"1727\":2,\"1730\":2,\"1731\":1,\"1735\":2,\"1768\":1,\"1822\":2,\"1858\":3,\"1907\":3,\"1957\":1,\"1966\":2,\"2000\":2,\"2014\":1,\"2015\":1,\"2018\":1,\"2095\":1,\"2130\":1,\"2134\":1,\"2136\":3,\"2228\":3,\"2229\":3,\"2327\":1,\"2355\":4,\"2408\":3,\"2446\":3,\"2462\":1,\"2472\":1}}],[\"independent\",{\"1\":{\"223\":1,\"821\":3,\"938\":1,\"1170\":1,\"1209\":2,\"1228\":2,\"1271\":2,\"1280\":1,\"1283\":1}}],[\"independently\",{\"1\":{\"44\":1,\"138\":1,\"175\":1}}],[\"indim\",{\"1\":{\"1086\":1,\"1207\":1,\"1340\":1}}],[\"indicator\",{\"1\":{\"1901\":1,\"1902\":1,\"1903\":1,\"1904\":1}}],[\"indicating\",{\"1\":{\"912\":1,\"1019\":1,\"1024\":1,\"1753\":2,\"2044\":1,\"2433\":1}}],[\"indicated\",{\"1\":{\"787\":1}}],[\"indicates\",{\"1\":{\"79\":1,\"80\":1,\"91\":1,\"97\":1,\"167\":1,\"2327\":2}}],[\"indicate\",{\"1\":{\"59\":2,\"269\":1,\"278\":1,\"1526\":1,\"1553\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"2044\":3,\"2249\":1,\"2253\":1}}],[\"indices\",{\"1\":{\"616\":1,\"696\":4,\"910\":1,\"922\":1,\"1063\":2,\"1316\":1,\"1320\":1,\"1356\":1,\"1439\":1,\"1441\":1,\"1901\":2,\"1903\":2,\"1920\":1,\"2130\":2,\"2138\":1,\"2167\":1,\"2176\":1,\"2207\":1,\"2218\":2,\"2219\":1,\"2220\":1}}],[\"individual\",{\"1\":{\"156\":1,\"194\":1,\"286\":1,\"636\":1,\"2130\":4}}],[\"individually\",{\"1\":{\"43\":1}}],[\"inp\",{\"1\":{\"1108\":1,\"1168\":1,\"1265\":1}}],[\"inplace\",{\"0\":{\"1481\":1},\"1\":{\"703\":1,\"755\":2,\"785\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"1481\":1}}],[\"inplanes\",{\"1\":{\"693\":1,\"893\":1,\"894\":1,\"2179\":1,\"2185\":1,\"2196\":1}}],[\"input=\",{\"1\":{\"1211\":1}}],[\"inputs\",{\"0\":{\"76\":1,\"873\":1},\"1\":{\"134\":1,\"144\":1,\"228\":1,\"514\":6,\"616\":2,\"625\":1,\"667\":1,\"726\":1,\"756\":4,\"759\":2,\"773\":4,\"820\":1,\"827\":1,\"828\":3,\"829\":1,\"830\":2,\"859\":1,\"866\":1,\"867\":1,\"873\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1114\":1,\"1200\":1,\"1215\":1,\"1245\":1,\"1286\":1,\"1297\":1,\"1402\":1,\"1409\":1,\"1467\":1,\"1526\":1,\"1594\":1,\"1595\":1,\"1599\":1,\"1600\":1,\"1606\":1,\"1634\":1,\"1635\":1,\"1636\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1750\":1,\"1768\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"1814\":1,\"1816\":1,\"1856\":2,\"1878\":1,\"1918\":1,\"1932\":2,\"1960\":1,\"1961\":1,\"1993\":1,\"2215\":1,\"2216\":1,\"2218\":1,\"2223\":1,\"2235\":2,\"2236\":2,\"2239\":3,\"2240\":3,\"2245\":1,\"2287\":2,\"2411\":1,\"2412\":1,\"2423\":1,\"2428\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"input+output\",{\"1\":{\"48\":1}}],[\"input\",{\"0\":{\"77\":1,\"79\":1,\"300\":1,\"303\":1,\"311\":1,\"317\":1,\"323\":1,\"329\":1,\"333\":1,\"337\":1,\"344\":1,\"351\":1,\"358\":1,\"363\":1,\"370\":1,\"379\":1,\"387\":1,\"391\":1,\"398\":1,\"408\":1,\"420\":1,\"423\":1,\"431\":1,\"438\":1,\"444\":1,\"451\":1,\"459\":1,\"465\":1,\"471\":1,\"477\":1,\"486\":1,\"492\":1,\"500\":1,\"507\":1,\"621\":1,\"660\":1,\"673\":1,\"1866\":1,\"1888\":1,\"1914\":1,\"2091\":1,\"2092\":1,\"2093\":1,\"2094\":1},\"1\":{\"43\":6,\"47\":1,\"48\":3,\"68\":1,\"69\":1,\"71\":3,\"74\":1,\"76\":1,\"78\":1,\"79\":1,\"81\":4,\"82\":1,\"95\":1,\"97\":1,\"98\":1,\"106\":2,\"128\":6,\"130\":1,\"134\":7,\"135\":5,\"136\":1,\"141\":8,\"142\":2,\"148\":1,\"175\":2,\"199\":1,\"200\":3,\"203\":1,\"204\":1,\"205\":6,\"207\":1,\"211\":1,\"217\":1,\"223\":3,\"224\":2,\"235\":1,\"242\":1,\"243\":3,\"246\":1,\"247\":3,\"254\":1,\"262\":1,\"266\":3,\"269\":1,\"275\":3,\"276\":1,\"278\":1,\"285\":3,\"286\":1,\"287\":1,\"290\":1,\"293\":2,\"402\":1,\"449\":2,\"481\":2,\"513\":7,\"514\":1,\"515\":1,\"521\":1,\"527\":1,\"536\":3,\"538\":2,\"543\":3,\"615\":1,\"617\":5,\"618\":5,\"619\":2,\"620\":10,\"621\":7,\"622\":2,\"623\":2,\"624\":5,\"626\":12,\"630\":2,\"633\":3,\"634\":3,\"636\":2,\"637\":2,\"638\":2,\"639\":2,\"640\":2,\"641\":1,\"642\":2,\"643\":4,\"644\":1,\"645\":3,\"647\":2,\"648\":1,\"649\":2,\"660\":6,\"662\":1,\"663\":2,\"665\":3,\"666\":1,\"671\":8,\"672\":2,\"673\":8,\"674\":6,\"675\":5,\"680\":2,\"682\":2,\"684\":2,\"691\":7,\"692\":11,\"699\":3,\"700\":5,\"701\":5,\"702\":3,\"703\":2,\"705\":1,\"706\":1,\"709\":11,\"710\":14,\"711\":14,\"720\":2,\"722\":1,\"724\":2,\"725\":2,\"726\":1,\"727\":1,\"728\":2,\"729\":2,\"731\":1,\"732\":1,\"733\":6,\"734\":5,\"735\":5,\"736\":1,\"737\":3,\"738\":3,\"741\":1,\"744\":2,\"745\":5,\"746\":8,\"747\":5,\"748\":5,\"750\":1,\"752\":2,\"754\":1,\"755\":6,\"756\":7,\"759\":5,\"760\":4,\"761\":3,\"762\":3,\"764\":1,\"765\":4,\"766\":1,\"767\":1,\"768\":7,\"769\":1,\"771\":9,\"772\":3,\"773\":7,\"774\":11,\"775\":4,\"777\":1,\"778\":2,\"780\":11,\"784\":1,\"785\":4,\"787\":1,\"790\":5,\"791\":1,\"792\":1,\"798\":3,\"802\":1,\"804\":2,\"809\":1,\"813\":1,\"815\":2,\"820\":7,\"827\":1,\"828\":4,\"829\":3,\"830\":6,\"831\":9,\"832\":2,\"837\":1,\"839\":1,\"846\":4,\"847\":1,\"848\":1,\"849\":10,\"850\":6,\"854\":1,\"856\":1,\"859\":1,\"860\":1,\"862\":3,\"864\":2,\"865\":1,\"866\":7,\"867\":7,\"922\":2,\"932\":1,\"934\":1,\"936\":2,\"937\":2,\"939\":5,\"952\":1,\"954\":1,\"955\":3,\"958\":2,\"963\":2,\"965\":1,\"967\":1,\"968\":1,\"969\":1,\"972\":1,\"974\":1,\"976\":2,\"977\":3,\"978\":4,\"979\":6,\"980\":7,\"984\":1,\"1029\":13,\"1030\":1,\"1031\":1,\"1032\":2,\"1034\":1,\"1035\":3,\"1038\":1,\"1039\":1,\"1040\":2,\"1044\":1,\"1045\":1,\"1046\":1,\"1048\":1,\"1053\":5,\"1059\":2,\"1061\":6,\"1062\":6,\"1063\":7,\"1064\":6,\"1070\":4,\"1071\":4,\"1073\":4,\"1075\":1,\"1080\":2,\"1084\":1,\"1103\":1,\"1107\":11,\"1112\":4,\"1113\":6,\"1114\":1,\"1117\":6,\"1118\":6,\"1119\":2,\"1124\":7,\"1125\":9,\"1130\":6,\"1131\":6,\"1133\":11,\"1134\":6,\"1136\":6,\"1137\":6,\"1139\":5,\"1141\":6,\"1145\":4,\"1147\":3,\"1155\":2,\"1156\":1,\"1157\":3,\"1158\":1,\"1162\":6,\"1163\":3,\"1164\":8,\"1165\":1,\"1168\":1,\"1180\":2,\"1181\":2,\"1185\":3,\"1190\":1,\"1192\":1,\"1194\":1,\"1198\":1,\"1200\":1,\"1208\":8,\"1211\":1,\"1215\":1,\"1217\":4,\"1222\":4,\"1223\":3,\"1232\":7,\"1235\":13,\"1236\":1,\"1250\":5,\"1251\":7,\"1252\":7,\"1255\":6,\"1257\":6,\"1259\":6,\"1260\":1,\"1261\":7,\"1262\":4,\"1265\":1,\"1267\":7,\"1268\":11,\"1269\":7,\"1270\":7,\"1271\":7,\"1278\":7,\"1279\":8,\"1280\":13,\"1281\":12,\"1282\":12,\"1283\":9,\"1286\":1,\"1290\":2,\"1296\":1,\"1301\":1,\"1306\":2,\"1316\":3,\"1320\":2,\"1327\":1,\"1330\":1,\"1333\":3,\"1334\":7,\"1343\":1,\"1356\":1,\"1358\":1,\"1361\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":2,\"1372\":1,\"1374\":2,\"1385\":2,\"1386\":1,\"1389\":3,\"1390\":1,\"1391\":2,\"1392\":1,\"1395\":1,\"1397\":2,\"1401\":3,\"1402\":1,\"1403\":2,\"1406\":1,\"1408\":3,\"1409\":1,\"1410\":2,\"1422\":1,\"1441\":5,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1466\":3,\"1467\":1,\"1468\":2,\"1493\":1,\"1494\":1,\"1513\":2,\"1514\":2,\"1516\":1,\"1519\":1,\"1520\":2,\"1521\":1,\"1522\":3,\"1524\":4,\"1525\":2,\"1526\":3,\"1529\":5,\"1533\":1,\"1534\":2,\"1535\":2,\"1536\":1,\"1539\":4,\"1545\":1,\"1546\":3,\"1548\":5,\"1551\":3,\"1552\":6,\"1553\":3,\"1555\":2,\"1556\":6,\"1559\":1,\"1564\":1,\"1581\":2,\"1582\":1,\"1583\":1,\"1585\":1,\"1586\":1,\"1588\":1,\"1589\":1,\"1592\":3,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":2,\"1597\":2,\"1598\":2,\"1599\":6,\"1600\":2,\"1603\":1,\"1604\":2,\"1605\":3,\"1606\":2,\"1608\":2,\"1609\":2,\"1610\":3,\"1611\":3,\"1612\":2,\"1613\":2,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":5,\"1620\":3,\"1621\":3,\"1622\":2,\"1624\":1,\"1625\":2,\"1626\":5,\"1627\":1,\"1628\":2,\"1632\":1,\"1633\":1,\"1640\":1,\"1641\":1,\"1652\":2,\"1657\":1,\"1659\":2,\"1660\":2,\"1665\":1,\"1668\":4,\"1669\":4,\"1672\":1,\"1673\":1,\"1674\":2,\"1676\":1,\"1677\":1,\"1679\":1,\"1680\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1697\":1,\"1702\":2,\"1719\":2,\"1720\":3,\"1721\":2,\"1725\":4,\"1730\":3,\"1733\":1,\"1735\":10,\"1736\":6,\"1737\":2,\"1738\":3,\"1739\":3,\"1740\":3,\"1741\":3,\"1742\":3,\"1743\":3,\"1744\":3,\"1745\":3,\"1746\":3,\"1747\":1,\"1748\":1,\"1749\":8,\"1750\":5,\"1751\":4,\"1753\":3,\"1756\":1,\"1757\":1,\"1758\":4,\"1759\":7,\"1764\":1,\"1770\":1,\"1771\":1,\"1783\":1,\"1784\":3,\"1785\":1,\"1786\":2,\"1788\":1,\"1789\":1,\"1790\":1,\"1795\":2,\"1796\":1,\"1806\":3,\"1808\":3,\"1809\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"1814\":2,\"1815\":1,\"1816\":2,\"1817\":1,\"1818\":2,\"1820\":2,\"1833\":1,\"1837\":2,\"1843\":1,\"1847\":1,\"1849\":1,\"1851\":1,\"1854\":1,\"1856\":2,\"1857\":1,\"1862\":2,\"1863\":11,\"1866\":4,\"1878\":1,\"1888\":3,\"1893\":1,\"1905\":2,\"1914\":10,\"1919\":3,\"1937\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":3,\"1945\":1,\"1947\":3,\"1955\":1,\"1957\":4,\"1959\":3,\"1960\":10,\"1961\":11,\"1965\":2,\"1966\":2,\"1971\":3,\"1972\":2,\"1974\":2,\"1977\":2,\"1978\":2,\"1980\":2,\"1982\":2,\"1992\":10,\"1993\":5,\"1995\":8,\"1996\":2,\"1997\":1,\"2007\":1,\"2040\":2,\"2043\":3,\"2044\":3,\"2045\":2,\"2049\":1,\"2054\":4,\"2055\":3,\"2056\":3,\"2065\":2,\"2066\":3,\"2101\":1,\"2102\":1,\"2124\":1,\"2126\":4,\"2127\":2,\"2128\":1,\"2129\":10,\"2130\":2,\"2139\":1,\"2143\":1,\"2167\":4,\"2172\":1,\"2176\":4,\"2183\":5,\"2184\":1,\"2187\":5,\"2188\":3,\"2190\":4,\"2191\":9,\"2192\":3,\"2194\":1,\"2198\":3,\"2200\":1,\"2202\":1,\"2203\":3,\"2205\":1,\"2207\":4,\"2208\":4,\"2209\":3,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2219\":1,\"2221\":1,\"2223\":1,\"2224\":3,\"2226\":1,\"2228\":1,\"2229\":1,\"2231\":4,\"2232\":4,\"2235\":4,\"2236\":4,\"2239\":7,\"2240\":7,\"2241\":1,\"2245\":5,\"2246\":3,\"2248\":3,\"2249\":3,\"2250\":3,\"2251\":3,\"2252\":3,\"2253\":3,\"2254\":3,\"2255\":3,\"2256\":3,\"2257\":3,\"2259\":3,\"2260\":3,\"2261\":3,\"2263\":3,\"2264\":3,\"2265\":3,\"2266\":3,\"2267\":3,\"2268\":3,\"2269\":3,\"2270\":3,\"2271\":3,\"2272\":3,\"2273\":3,\"2287\":2,\"2298\":3,\"2325\":2,\"2327\":2,\"2343\":2,\"2352\":2,\"2392\":1,\"2401\":2,\"2405\":2,\"2407\":1,\"2408\":1,\"2409\":2,\"2411\":7,\"2412\":7,\"2413\":1,\"2414\":2,\"2416\":2,\"2418\":2,\"2420\":1,\"2421\":1,\"2423\":7,\"2424\":1,\"2425\":1,\"2426\":1,\"2428\":2,\"2429\":1,\"2430\":1,\"2431\":5,\"2432\":7,\"2433\":2,\"2434\":2,\"2439\":1,\"2443\":2,\"2446\":1,\"2447\":7,\"2448\":1,\"2449\":3,\"2458\":1,\"2460\":1,\"2472\":1,\"2487\":1}}],[\"inproceedings\",{\"1\":{\"5\":1,\"6\":3,\"7\":1,\"8\":1,\"9\":1,\"10\":2,\"11\":2,\"12\":1,\"13\":1,\"14\":1,\"15\":1,\"16\":1,\"156\":1,\"202\":1,\"207\":1,\"256\":1}}],[\"initliaze\",{\"1\":{\"1763\":1,\"1778\":1,\"1834\":1}}],[\"initilialize\",{\"1\":{\"1611\":1}}],[\"initilize\",{\"1\":{\"1529\":1,\"1594\":1,\"1595\":1,\"1597\":1,\"1604\":1,\"1606\":1,\"1608\":1,\"1612\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1753\":1,\"1754\":1,\"1788\":1,\"1896\":1,\"2425\":1,\"2429\":1,\"2430\":1,\"2433\":1}}],[\"initiate\",{\"1\":{\"787\":1}}],[\"initialzie\",{\"1\":{\"1613\":1}}],[\"initial\",{\"0\":{\"1895\":1},\"1\":{\"110\":1,\"130\":1,\"223\":1,\"262\":1,\"622\":1,\"623\":1,\"625\":1,\"641\":1,\"651\":1,\"710\":1,\"711\":1,\"726\":1,\"797\":2,\"823\":2,\"824\":2,\"828\":1,\"829\":1,\"830\":1,\"847\":1,\"859\":1,\"1404\":1,\"1408\":1,\"1410\":1,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1513\":1,\"1548\":1,\"1551\":1,\"1552\":1,\"1592\":1,\"1596\":2,\"1597\":1,\"1599\":3,\"1604\":1,\"1605\":2,\"1606\":1,\"1626\":1,\"1719\":2,\"1724\":2,\"1725\":2,\"1729\":2,\"1731\":4,\"1749\":1,\"1806\":2,\"1815\":1,\"1822\":2,\"1843\":1,\"1848\":4,\"1854\":1,\"1856\":2,\"1894\":1,\"1895\":1,\"1966\":2,\"2015\":6,\"2018\":1,\"2039\":5,\"2239\":2,\"2240\":2,\"2384\":1,\"2411\":2,\"2412\":2,\"2423\":2,\"2432\":3,\"2447\":2}}],[\"initializing\",{\"1\":{\"262\":2,\"2049\":1}}],[\"initializer\",{\"0\":{\"906\":1},\"1\":{\"906\":1,\"1779\":1,\"1815\":1,\"2043\":1,\"2044\":1,\"2055\":1,\"2056\":1,\"2066\":1}}],[\"initializer=none\",{\"1\":{\"726\":1,\"744\":1,\"769\":1,\"859\":1}}],[\"initializes\",{\"1\":{\"699\":2,\"760\":1,\"2039\":1,\"2040\":2}}],[\"initialized\",{\"1\":{\"699\":1,\"911\":3,\"2162\":1,\"2312\":1}}],[\"initialize\",{\"0\":{\"1896\":1,\"2312\":2},\"1\":{\"60\":1,\"614\":2,\"617\":1,\"618\":1,\"620\":1,\"624\":1,\"626\":1,\"629\":1,\"634\":1,\"635\":1,\"636\":1,\"641\":1,\"643\":1,\"650\":1,\"651\":1,\"652\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":2,\"691\":1,\"692\":1,\"693\":1,\"696\":1,\"697\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"706\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"715\":1,\"718\":1,\"720\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"741\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"751\":1,\"752\":1,\"754\":1,\"757\":1,\"759\":1,\"761\":1,\"762\":1,\"765\":1,\"766\":1,\"767\":1,\"768\":2,\"770\":1,\"771\":1,\"772\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"786\":1,\"787\":1,\"788\":1,\"790\":1,\"791\":1,\"793\":1,\"794\":1,\"795\":1,\"796\":1,\"798\":1,\"800\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"817\":1,\"820\":2,\"823\":1,\"824\":1,\"825\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":1,\"832\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":1,\"847\":2,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"859\":1,\"860\":1,\"862\":1,\"864\":1,\"907\":1,\"911\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":1,\"959\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"979\":1,\"981\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1072\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1126\":1,\"1127\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1156\":1,\"1158\":1,\"1159\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1177\":1,\"1179\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":1,\"1205\":1,\"1207\":1,\"1208\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":1,\"1219\":1,\"1222\":1,\"1223\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1279\":1,\"1281\":1,\"1282\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1333\":1,\"1334\":1,\"1381\":1,\"1383\":1,\"1385\":1,\"1387\":1,\"1390\":1,\"1391\":1,\"1392\":1,\"1395\":1,\"1397\":1,\"1398\":1,\"1400\":2,\"1402\":1,\"1403\":1,\"1404\":1,\"1406\":1,\"1409\":1,\"1410\":1,\"1411\":1,\"1417\":1,\"1419\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":2,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1467\":1,\"1468\":1,\"1469\":2,\"1508\":1,\"1509\":1,\"1511\":1,\"1513\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1519\":1,\"1520\":1,\"1521\":1,\"1522\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1530\":1,\"1533\":1,\"1534\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1548\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1556\":1,\"1576\":1,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1596\":1,\"1598\":1,\"1599\":2,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1610\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1621\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1625\":1,\"1626\":1,\"1627\":1,\"1628\":1,\"1638\":1,\"1640\":1,\"1641\":1,\"1652\":1,\"1656\":1,\"1657\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1667\":1,\"1668\":2,\"1669\":1,\"1670\":1,\"1671\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1728\":1,\"1729\":1,\"1731\":1,\"1733\":1,\"1734\":1,\"1737\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1758\":1,\"1764\":1,\"1766\":1,\"1768\":1,\"1770\":1,\"1771\":1,\"1772\":1,\"1774\":1,\"1776\":1,\"1780\":1,\"1781\":1,\"1784\":1,\"1786\":1,\"1787\":1,\"1791\":1,\"1793\":1,\"1795\":1,\"1796\":1,\"1798\":1,\"1799\":1,\"1800\":2,\"1801\":1,\"1802\":1,\"1803\":1,\"1806\":2,\"1810\":1,\"1812\":1,\"1813\":1,\"1814\":1,\"1815\":1,\"1816\":1,\"1820\":1,\"1824\":1,\"1825\":1,\"1827\":1,\"1828\":1,\"1830\":1,\"1832\":1,\"1833\":1,\"1835\":1,\"1836\":1,\"1838\":1,\"1839\":1,\"1843\":1,\"1846\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1852\":1,\"1853\":1,\"1854\":1,\"1856\":1,\"1874\":1,\"1879\":1,\"1894\":1,\"1896\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":1,\"1945\":1,\"1946\":1,\"1947\":1,\"1957\":1,\"1959\":1,\"1960\":1,\"1961\":1,\"1965\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1993\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2045\":1,\"2054\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2128\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2133\":1,\"2136\":1,\"2137\":1,\"2138\":1,\"2142\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2221\":1,\"2222\":1,\"2223\":1,\"2226\":1,\"2227\":1,\"2228\":1,\"2229\":1,\"2231\":1,\"2232\":1,\"2235\":2,\"2236\":2,\"2237\":1,\"2238\":1,\"2239\":2,\"2240\":2,\"2241\":1,\"2242\":1,\"2243\":1,\"2245\":1,\"2305\":1,\"2312\":3,\"2325\":1,\"2327\":1,\"2357\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2407\":1,\"2408\":1,\"2409\":1,\"2411\":2,\"2412\":2,\"2413\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2421\":1,\"2422\":1,\"2423\":2,\"2424\":1,\"2431\":1,\"2432\":2,\"2434\":1,\"2443\":1,\"2445\":1,\"2446\":1,\"2447\":2,\"2448\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1,\"2482\":1}}],[\"initializations\",{\"1\":{\"911\":1}}],[\"initialization\",{\"0\":{\"382\":1,\"454\":1},\"1\":{\"46\":2,\"50\":2,\"52\":1,\"60\":4,\"62\":1,\"104\":1,\"262\":2,\"263\":1,\"633\":1,\"637\":1,\"638\":1,\"646\":1,\"647\":1,\"699\":3,\"715\":1,\"749\":1,\"768\":3,\"769\":1,\"783\":1,\"821\":1,\"938\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1347\":1,\"1400\":2,\"1441\":1,\"1469\":1,\"1513\":1,\"1548\":1,\"1551\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1592\":1,\"1605\":1,\"1606\":1,\"1654\":1,\"1666\":1,\"2044\":8,\"2131\":1,\"2312\":3,\"2426\":1,\"2427\":1,\"2428\":1}}],[\"init=none\",{\"1\":{\"1110\":1,\"1331\":1}}],[\"init=false\",{\"1\":{\"769\":1,\"787\":1}}],[\"init\",{\"0\":{\"60\":1,\"911\":1,\"1305\":1,\"1487\":1,\"1504\":1,\"1566\":1,\"1651\":1,\"1874\":1,\"1879\":1,\"1894\":1,\"2098\":1,\"2242\":1,\"2243\":1},\"1\":{\"32\":1,\"50\":7,\"60\":4,\"62\":2,\"63\":2,\"64\":2,\"88\":7,\"130\":1,\"150\":1,\"223\":1,\"243\":4,\"286\":1,\"290\":8,\"374\":2,\"377\":4,\"449\":2,\"614\":1,\"634\":1,\"641\":1,\"643\":1,\"651\":1,\"710\":2,\"711\":2,\"749\":1,\"797\":1,\"820\":1,\"829\":2,\"847\":1,\"851\":1,\"911\":4,\"1053\":1,\"1057\":1,\"1144\":1,\"1153\":1,\"1211\":1,\"1213\":1,\"1238\":1,\"1242\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1346\":1,\"1347\":1,\"1389\":1,\"1391\":1,\"1396\":1,\"1400\":3,\"1401\":1,\"1403\":1,\"1406\":1,\"1408\":1,\"1410\":1,\"1441\":2,\"1466\":1,\"1468\":1,\"1469\":2,\"1487\":1,\"1504\":1,\"1526\":3,\"1566\":1,\"1598\":3,\"1599\":6,\"1600\":3,\"1651\":1,\"1668\":1,\"1719\":1,\"1724\":1,\"1725\":1,\"1731\":2,\"1748\":1,\"1749\":1,\"1784\":1,\"1800\":1,\"1806\":2,\"1815\":1,\"1822\":1,\"1843\":1,\"1848\":2,\"1874\":1,\"1879\":1,\"1894\":1,\"1966\":1,\"1994\":3,\"2014\":1,\"2015\":1,\"2018\":4,\"2039\":4,\"2235\":2,\"2236\":2,\"2239\":6,\"2240\":6,\"2242\":1,\"2243\":1,\"2311\":1,\"2312\":2,\"2314\":3,\"2334\":2,\"2340\":5,\"2355\":2,\"2360\":1,\"2361\":1,\"2411\":6,\"2412\":6,\"2423\":6,\"2432\":6,\"2447\":6}}],[\"insensitive\",{\"1\":{\"2311\":1}}],[\"insert\",{\"1\":{\"1493\":1,\"1494\":1}}],[\"inserted\",{\"1\":{\"242\":1,\"768\":1}}],[\"insertion\",{\"1\":{\"175\":1}}],[\"ins\",{\"1\":{\"286\":9,\"1869\":1}}],[\"inspired\",{\"1\":{\"78\":1,\"1668\":1}}],[\"inside\",{\"1\":{\"18\":1,\"22\":2,\"878\":1,\"879\":1,\"882\":1,\"883\":1,\"1545\":1}}],[\"instruct\",{\"1\":{\"1965\":1,\"2049\":2,\"2287\":1}}],[\"instruction\",{\"0\":{\"116\":1},\"1\":{\"2262\":1}}],[\"instructions\",{\"0\":{\"224\":1,\"225\":1},\"1\":{\"0\":1,\"198\":1,\"200\":3,\"222\":2,\"245\":1}}],[\"instabilities\",{\"1\":{\"1225\":1}}],[\"instantiated\",{\"1\":{\"2140\":1,\"2276\":1,\"2277\":1}}],[\"instantiate\",{\"0\":{\"912\":1,\"2099\":1,\"2100\":1},\"1\":{\"912\":2,\"1860\":1,\"1878\":1,\"1885\":1,\"1895\":1,\"2007\":1,\"2293\":1}}],[\"instantiations\",{\"1\":{\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"821\":1,\"829\":2,\"830\":1,\"859\":1}}],[\"instancenorm2d\",{\"1\":{\"1236\":1}}],[\"instancenorm2dplus\",{\"0\":{\"1187\":1},\"1\":{\"1187\":1}}],[\"instancenorm\",{\"1\":{\"1236\":1}}],[\"instances\",{\"1\":{\"1051\":1,\"2134\":1,\"2308\":1}}],[\"instance\",{\"1\":{\"617\":2,\"618\":4,\"624\":5,\"675\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1108\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1204\":1,\"1206\":1,\"1209\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1228\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1484\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1722\":1,\"1735\":4,\"1750\":1,\"1751\":6,\"1759\":4,\"1807\":1,\"1917\":1,\"1938\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1949\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2131\":1,\"2133\":1,\"2136\":1,\"2137\":1,\"2142\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2223\":1,\"2306\":1,\"2326\":1,\"2355\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"installing\",{\"1\":{\"159\":1,\"161\":1,\"162\":1}}],[\"installer\",{\"1\":{\"153\":1}}],[\"installers\",{\"1\":{\"128\":2,\"153\":1,\"163\":2}}],[\"installed\",{\"1\":{\"19\":3,\"22\":1,\"110\":1,\"128\":1,\"152\":1,\"159\":1,\"162\":1,\"163\":1,\"197\":1,\"2040\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2065\":1}}],[\"installation\",{\"0\":{\"154\":2,\"158\":1,\"162\":1,\"163\":1,\"164\":1},\"1\":{\"26\":1,\"28\":1,\"39\":2,\"92\":1,\"139\":1,\"152\":1,\"153\":3,\"154\":1,\"159\":1,\"160\":2,\"161\":1,\"162\":2,\"163\":1,\"164\":2,\"173\":1,\"260\":1}}],[\"install\",{\"0\":{\"31\":1,\"161\":1},\"1\":{\"1\":2,\"3\":1,\"31\":5,\"39\":4,\"127\":1,\"128\":3,\"152\":2,\"153\":4,\"159\":5,\"161\":7,\"162\":12,\"163\":10,\"164\":1,\"165\":1,\"213\":1,\"286\":2,\"2345\":1}}],[\"insteadof\",{\"1\":{\"828\":1,\"829\":1,\"830\":1}}],[\"instead\",{\"1\":{\"47\":2,\"84\":1,\"97\":1,\"99\":1,\"100\":1,\"101\":2,\"135\":1,\"138\":1,\"141\":1,\"168\":1,\"173\":1,\"196\":1,\"203\":1,\"242\":1,\"243\":1,\"268\":1,\"277\":1,\"287\":1,\"290\":2,\"536\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"738\":1,\"739\":1,\"742\":1,\"753\":1,\"756\":2,\"758\":1,\"768\":2,\"773\":2,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"817\":1,\"821\":1,\"822\":1,\"826\":1,\"827\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1000\":1,\"1002\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1061\":1,\"1062\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1126\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1155\":1,\"1157\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1246\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1273\":1,\"1274\":1,\"1279\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1454\":1,\"1455\":1,\"1456\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1501\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1546\":1,\"1550\":1,\"1555\":1,\"1607\":2,\"1622\":1,\"1626\":1,\"1639\":1,\"1644\":1,\"1647\":1,\"1653\":1,\"1658\":1,\"1662\":1,\"1663\":1,\"1794\":1,\"1833\":2,\"1881\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2065\":1,\"2125\":1,\"2133\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2412\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"inaguma\",{\"1\":{\"10\":2}}],[\"inaguma2020espnet\",{\"1\":{\"10\":1}}],[\"int32\",{\"1\":{\"2363\":2}}],[\"int2float\",{\"0\":{\"2101\":1},\"1\":{\"2101\":1}}],[\"intput\",{\"1\":{\"1847\":1}}],[\"intialize\",{\"1\":{\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1466\":1}}],[\"int16\",{\"1\":{\"991\":1,\"1003\":1,\"1015\":1,\"1824\":2,\"2040\":1,\"2045\":1,\"2101\":1}}],[\"intra\",{\"1\":{\"1139\":1,\"1140\":1,\"1141\":1,\"1185\":1,\"1270\":1,\"1280\":1,\"1283\":1,\"2176\":1}}],[\"intrandomgeneratedataset\",{\"0\":{\"989\":1},\"1\":{\"989\":1,\"990\":1}}],[\"introduces\",{\"1\":{\"262\":1}}],[\"introduce\",{\"1\":{\"200\":1}}],[\"introduced\",{\"1\":{\"143\":1,\"147\":1,\"1615\":1,\"1750\":1,\"1795\":1,\"2224\":2,\"2404\":1,\"2412\":1,\"2420\":1,\"2425\":1,\"2429\":1,\"2430\":1}}],[\"introduction\",{\"0\":{\"223\":1,\"228\":1},\"1\":{\"138\":1,\"222\":1}}],[\"introducing\",{\"1\":{\"15\":1}}],[\"ints\",{\"1\":{\"692\":1,\"850\":2,\"922\":2,\"936\":2,\"937\":2}}],[\"int64\",{\"1\":{\"692\":4,\"760\":2,\"775\":1,\"790\":3,\"797\":1,\"820\":3,\"850\":3,\"989\":1,\"1723\":2,\"1724\":1,\"1731\":2,\"1787\":2,\"1805\":1,\"1822\":1,\"1944\":2,\"1945\":1,\"1946\":1,\"1947\":2,\"1992\":2}}],[\"int=\",{\"1\":{\"674\":21,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2340\":1}}],[\"int\",{\"0\":{\"2394\":1,\"2492\":1,\"2503\":1},\"1\":{\"43\":19,\"44\":1,\"45\":7,\"79\":4,\"80\":1,\"82\":3,\"139\":1,\"141\":23,\"142\":15,\"143\":1,\"145\":4,\"147\":3,\"148\":2,\"614\":3,\"615\":1,\"616\":7,\"617\":4,\"618\":3,\"619\":2,\"620\":8,\"621\":4,\"622\":2,\"623\":2,\"624\":4,\"625\":4,\"626\":3,\"627\":2,\"628\":1,\"630\":7,\"631\":2,\"632\":4,\"633\":9,\"634\":15,\"635\":1,\"636\":4,\"637\":4,\"638\":2,\"639\":3,\"640\":1,\"641\":8,\"642\":6,\"643\":11,\"644\":6,\"645\":4,\"646\":2,\"647\":5,\"648\":1,\"649\":9,\"651\":6,\"653\":2,\"655\":1,\"660\":2,\"661\":3,\"662\":1,\"663\":3,\"664\":3,\"665\":6,\"667\":2,\"668\":1,\"669\":3,\"671\":3,\"672\":4,\"673\":2,\"674\":21,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"691\":4,\"692\":3,\"696\":10,\"697\":9,\"699\":7,\"700\":8,\"701\":2,\"702\":5,\"703\":14,\"704\":3,\"706\":2,\"709\":19,\"710\":12,\"711\":11,\"712\":4,\"713\":3,\"715\":2,\"716\":4,\"717\":7,\"720\":7,\"721\":1,\"731\":8,\"732\":8,\"733\":13,\"734\":14,\"735\":4,\"736\":5,\"737\":4,\"738\":6,\"739\":1,\"740\":1,\"743\":1,\"745\":4,\"746\":8,\"747\":8,\"748\":8,\"750\":1,\"752\":2,\"753\":1,\"755\":7,\"759\":2,\"760\":4,\"761\":3,\"762\":4,\"763\":2,\"765\":5,\"766\":8,\"767\":8,\"768\":8,\"770\":3,\"771\":4,\"772\":3,\"774\":18,\"775\":5,\"776\":2,\"777\":4,\"778\":6,\"779\":1,\"780\":24,\"781\":3,\"783\":2,\"784\":2,\"785\":8,\"786\":1,\"787\":3,\"790\":2,\"791\":1,\"792\":3,\"794\":1,\"795\":8,\"796\":5,\"798\":5,\"799\":1,\"800\":1,\"803\":4,\"815\":2,\"816\":1,\"820\":3,\"831\":5,\"833\":7,\"846\":16,\"847\":6,\"848\":6,\"849\":10,\"850\":5,\"851\":3,\"854\":2,\"862\":5,\"863\":1,\"864\":1,\"865\":1,\"869\":14,\"878\":6,\"879\":6,\"881\":6,\"882\":7,\"883\":7,\"884\":7,\"886\":1,\"888\":2,\"908\":4,\"919\":7,\"921\":1,\"922\":2,\"931\":2,\"933\":2,\"935\":1,\"936\":2,\"937\":2,\"947\":1,\"949\":1,\"955\":1,\"958\":1,\"959\":3,\"962\":2,\"970\":1,\"976\":2,\"977\":2,\"978\":5,\"979\":3,\"980\":10,\"987\":1,\"989\":3,\"994\":1,\"1000\":1,\"1019\":2,\"1022\":1,\"1024\":1,\"1028\":4,\"1029\":6,\"1030\":1,\"1034\":1,\"1035\":1,\"1053\":2,\"1061\":6,\"1062\":14,\"1063\":1,\"1064\":6,\"1065\":1,\"1066\":1,\"1070\":3,\"1071\":3,\"1073\":1,\"1107\":16,\"1112\":5,\"1113\":6,\"1117\":8,\"1118\":11,\"1119\":3,\"1124\":6,\"1125\":13,\"1126\":10,\"1127\":8,\"1130\":10,\"1131\":8,\"1133\":10,\"1134\":4,\"1136\":7,\"1137\":4,\"1139\":5,\"1141\":8,\"1147\":4,\"1155\":1,\"1156\":4,\"1157\":3,\"1158\":1,\"1159\":2,\"1162\":10,\"1164\":3,\"1176\":3,\"1180\":5,\"1181\":6,\"1185\":3,\"1202\":1,\"1208\":4,\"1217\":13,\"1222\":2,\"1223\":3,\"1232\":6,\"1235\":5,\"1246\":1,\"1250\":6,\"1251\":7,\"1252\":12,\"1253\":2,\"1255\":2,\"1257\":2,\"1259\":3,\"1261\":7,\"1262\":5,\"1267\":10,\"1268\":17,\"1274\":4,\"1278\":12,\"1279\":13,\"1280\":36,\"1281\":12,\"1282\":9,\"1283\":27,\"1290\":3,\"1311\":3,\"1314\":2,\"1315\":2,\"1316\":4,\"1318\":3,\"1319\":1,\"1320\":3,\"1322\":3,\"1323\":1,\"1327\":3,\"1328\":3,\"1329\":2,\"1330\":3,\"1334\":2,\"1351\":2,\"1354\":2,\"1356\":4,\"1374\":2,\"1375\":4,\"1377\":2,\"1381\":1,\"1385\":13,\"1386\":3,\"1387\":2,\"1389\":2,\"1391\":17,\"1392\":19,\"1395\":1,\"1396\":1,\"1397\":6,\"1400\":9,\"1401\":2,\"1402\":3,\"1403\":18,\"1406\":2,\"1408\":2,\"1409\":4,\"1410\":11,\"1413\":5,\"1416\":1,\"1417\":2,\"1419\":12,\"1420\":3,\"1422\":12,\"1439\":3,\"1441\":18,\"1442\":6,\"1444\":12,\"1446\":4,\"1448\":12,\"1450\":22,\"1452\":26,\"1454\":22,\"1456\":24,\"1458\":6,\"1460\":9,\"1462\":2,\"1466\":2,\"1467\":2,\"1468\":16,\"1469\":9,\"1478\":1,\"1482\":4,\"1484\":4,\"1491\":1,\"1493\":2,\"1494\":4,\"1498\":1,\"1504\":1,\"1505\":2,\"1506\":4,\"1508\":1,\"1509\":1,\"1511\":2,\"1513\":20,\"1514\":4,\"1516\":9,\"1519\":16,\"1520\":4,\"1521\":1,\"1523\":1,\"1524\":12,\"1525\":12,\"1526\":11,\"1533\":6,\"1534\":1,\"1535\":16,\"1536\":16,\"1539\":3,\"1540\":1,\"1546\":14,\"1548\":8,\"1549\":4,\"1551\":20,\"1552\":76,\"1553\":9,\"1556\":9,\"1558\":6,\"1571\":1,\"1576\":1,\"1577\":2,\"1579\":2,\"1581\":10,\"1582\":7,\"1583\":6,\"1585\":1,\"1586\":2,\"1592\":18,\"1593\":2,\"1594\":2,\"1595\":3,\"1596\":13,\"1597\":14,\"1598\":7,\"1599\":79,\"1600\":9,\"1604\":12,\"1605\":14,\"1606\":14,\"1607\":14,\"1608\":4,\"1609\":12,\"1610\":20,\"1611\":16,\"1612\":15,\"1613\":14,\"1614\":6,\"1615\":6,\"1616\":10,\"1617\":4,\"1618\":6,\"1619\":16,\"1620\":8,\"1621\":9,\"1622\":14,\"1623\":2,\"1624\":4,\"1625\":9,\"1626\":62,\"1628\":23,\"1631\":2,\"1632\":2,\"1633\":2,\"1640\":2,\"1641\":2,\"1642\":1,\"1643\":8,\"1644\":1,\"1645\":4,\"1646\":8,\"1647\":1,\"1648\":2,\"1650\":4,\"1654\":1,\"1655\":2,\"1657\":2,\"1660\":2,\"1662\":5,\"1664\":4,\"1665\":2,\"1666\":1,\"1668\":10,\"1669\":3,\"1670\":1,\"1672\":4,\"1673\":4,\"1674\":3,\"1676\":2,\"1677\":2,\"1678\":1,\"1679\":2,\"1680\":1,\"1682\":2,\"1683\":4,\"1684\":1,\"1685\":2,\"1686\":2,\"1687\":4,\"1689\":4,\"1690\":4,\"1691\":3,\"1692\":6,\"1694\":2,\"1697\":2,\"1698\":2,\"1699\":1,\"1704\":3,\"1705\":3,\"1706\":5,\"1707\":3,\"1708\":8,\"1709\":9,\"1710\":8,\"1711\":6,\"1712\":5,\"1713\":5,\"1714\":5,\"1715\":7,\"1716\":7,\"1719\":18,\"1721\":16,\"1725\":25,\"1726\":4,\"1727\":6,\"1730\":3,\"1731\":4,\"1735\":4,\"1736\":6,\"1737\":3,\"1738\":2,\"1739\":2,\"1740\":2,\"1741\":2,\"1742\":2,\"1743\":2,\"1744\":2,\"1745\":2,\"1746\":2,\"1747\":2,\"1748\":10,\"1749\":6,\"1750\":12,\"1751\":1,\"1753\":4,\"1756\":3,\"1757\":3,\"1758\":7,\"1759\":1,\"1762\":1,\"1766\":13,\"1768\":8,\"1775\":2,\"1779\":5,\"1782\":2,\"1783\":2,\"1784\":3,\"1785\":2,\"1786\":2,\"1787\":2,\"1789\":3,\"1790\":3,\"1794\":2,\"1795\":3,\"1803\":1,\"1806\":8,\"1808\":2,\"1809\":2,\"1810\":5,\"1812\":4,\"1814\":4,\"1815\":7,\"1816\":4,\"1817\":2,\"1818\":2,\"1820\":2,\"1822\":4,\"1837\":3,\"1842\":2,\"1843\":3,\"1847\":1,\"1848\":1,\"1849\":1,\"1851\":2,\"1854\":10,\"1858\":3,\"1860\":1,\"1862\":9,\"1863\":5,\"1866\":2,\"1870\":2,\"1873\":1,\"1877\":1,\"1883\":1,\"1888\":2,\"1893\":1,\"1895\":7,\"1897\":2,\"1901\":1,\"1903\":1,\"1907\":3,\"1910\":3,\"1913\":1,\"1914\":1,\"1917\":1,\"1919\":1,\"1921\":2,\"1926\":1,\"1928\":1,\"1936\":4,\"1937\":3,\"1940\":4,\"1941\":2,\"1942\":4,\"1943\":2,\"1944\":1,\"1945\":5,\"1947\":6,\"1949\":2,\"1951\":2,\"1957\":2,\"1958\":1,\"1959\":4,\"1960\":3,\"1961\":4,\"1965\":2,\"1966\":2,\"1973\":1,\"1975\":4,\"1976\":2,\"1978\":3,\"1979\":1,\"1980\":7,\"1981\":1,\"1982\":3,\"1983\":1,\"1984\":2,\"1987\":2,\"1992\":11,\"1993\":39,\"1994\":17,\"1995\":11,\"1996\":2,\"1997\":2,\"1999\":3,\"2000\":4,\"2001\":4,\"2002\":3,\"2003\":2,\"2004\":2,\"2005\":1,\"2006\":1,\"2007\":4,\"2008\":6,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":6,\"2015\":7,\"2016\":3,\"2017\":2,\"2018\":2,\"2019\":2,\"2020\":1,\"2021\":4,\"2039\":3,\"2040\":3,\"2044\":5,\"2045\":3,\"2054\":2,\"2065\":10,\"2124\":2,\"2125\":1,\"2126\":9,\"2127\":4,\"2128\":1,\"2129\":8,\"2130\":6,\"2131\":1,\"2132\":3,\"2133\":5,\"2134\":7,\"2136\":8,\"2137\":4,\"2141\":2,\"2145\":2,\"2146\":2,\"2147\":2,\"2149\":2,\"2151\":1,\"2157\":1,\"2159\":1,\"2161\":1,\"2163\":1,\"2166\":2,\"2167\":3,\"2168\":1,\"2170\":1,\"2173\":1,\"2175\":1,\"2176\":4,\"2183\":1,\"2187\":5,\"2188\":1,\"2189\":1,\"2190\":1,\"2191\":18,\"2192\":4,\"2193\":1,\"2196\":6,\"2198\":2,\"2199\":1,\"2200\":2,\"2203\":5,\"2204\":1,\"2207\":2,\"2208\":1,\"2209\":3,\"2210\":1,\"2218\":6,\"2219\":6,\"2220\":10,\"2221\":6,\"2223\":10,\"2224\":2,\"2227\":2,\"2228\":1,\"2231\":7,\"2232\":5,\"2235\":36,\"2236\":43,\"2238\":5,\"2239\":49,\"2240\":41,\"2245\":66,\"2249\":4,\"2258\":10,\"2278\":3,\"2283\":3,\"2287\":1,\"2291\":3,\"2307\":1,\"2309\":1,\"2310\":1,\"2321\":1,\"2322\":1,\"2327\":2,\"2329\":1,\"2332\":2,\"2335\":3,\"2336\":4,\"2337\":2,\"2339\":10,\"2340\":9,\"2341\":1,\"2342\":4,\"2343\":2,\"2346\":5,\"2348\":10,\"2350\":4,\"2351\":1,\"2352\":2,\"2353\":8,\"2354\":5,\"2356\":2,\"2357\":1,\"2359\":19,\"2360\":1,\"2361\":1,\"2362\":2,\"2364\":10,\"2365\":1,\"2367\":11,\"2368\":6,\"2369\":1,\"2370\":20,\"2372\":12,\"2373\":6,\"2374\":1,\"2376\":3,\"2378\":2,\"2379\":2,\"2381\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2390\":1,\"2394\":1,\"2398\":2,\"2402\":1,\"2404\":6,\"2406\":1,\"2409\":5,\"2410\":1,\"2411\":53,\"2412\":67,\"2414\":3,\"2415\":1,\"2416\":7,\"2417\":1,\"2418\":3,\"2419\":1,\"2423\":69,\"2425\":13,\"2426\":6,\"2427\":4,\"2428\":14,\"2429\":20,\"2430\":8,\"2431\":64,\"2432\":65,\"2433\":8,\"2434\":7,\"2435\":3,\"2441\":2,\"2442\":2,\"2447\":53,\"2448\":1,\"2454\":1,\"2458\":5,\"2460\":6,\"2462\":3,\"2471\":1,\"2472\":4,\"2473\":1,\"2482\":8,\"2486\":1,\"2490\":4,\"2492\":3,\"2495\":5,\"2503\":2}}],[\"intelligibility\",{\"1\":{\"246\":2,\"285\":1,\"286\":1}}],[\"integer\",{\"1\":{\"631\":1,\"1301\":1,\"1306\":1,\"1350\":1,\"1371\":1,\"1372\":1,\"1678\":2,\"2101\":3,\"2151\":2,\"2310\":2,\"2355\":2}}],[\"integers\",{\"1\":{\"205\":1,\"2101\":1,\"2278\":1,\"2283\":1,\"2291\":1}}],[\"integrating\",{\"1\":{\"185\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1965\":1}}],[\"integration\",{\"0\":{\"92\":1},\"1\":{\"5\":1,\"11\":1,\"53\":1,\"223\":1,\"246\":1,\"286\":1,\"1526\":1,\"1598\":1,\"1599\":2,\"1600\":1,\"1992\":2,\"1993\":2,\"1994\":1,\"1995\":2,\"2235\":4,\"2236\":4,\"2239\":2,\"2240\":2,\"2245\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":2,\"2432\":2,\"2438\":1,\"2440\":1,\"2447\":2}}],[\"integrated\",{\"1\":{\"246\":1,\"247\":2,\"2184\":1,\"2240\":1}}],[\"integrate\",{\"1\":{\"185\":1,\"286\":1,\"1599\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"2044\":1,\"2235\":2,\"2236\":2,\"2239\":1,\"2240\":1,\"2245\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"integratable\",{\"1\":{\"9\":1}}],[\"intensive\",{\"1\":{\"262\":1}}],[\"intended\",{\"1\":{\"756\":2,\"773\":2,\"819\":1}}],[\"intend\",{\"1\":{\"96\":1,\"2325\":1}}],[\"intention\",{\"1\":{\"81\":1}}],[\"intersection\",{\"1\":{\"2442\":1}}],[\"interspeech\",{\"1\":{\"6\":2,\"7\":1,\"8\":1,\"11\":1,\"13\":1,\"156\":3,\"256\":1,\"1061\":3,\"1062\":3,\"1130\":2,\"1131\":2,\"1172\":2,\"1185\":1,\"1730\":1,\"2168\":1,\"2187\":1,\"2191\":1,\"2192\":1}}],[\"interest\",{\"1\":{\"2355\":1}}],[\"intertopk\",{\"0\":{\"2176\":2},\"1\":{\"2176\":2}}],[\"interleaving\",{\"1\":{\"2136\":1}}],[\"interleave\",{\"1\":{\"2136\":2}}],[\"inter\",{\"1\":{\"1117\":1,\"1130\":1,\"1131\":1,\"1136\":1,\"1139\":1,\"1140\":1,\"1141\":2,\"1185\":1,\"1232\":1,\"1270\":1,\"1280\":2,\"1283\":2,\"2176\":4,\"2472\":1}}],[\"interator\",{\"1\":{\"974\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1}}],[\"interact\",{\"1\":{\"247\":1}}],[\"interactions\",{\"1\":{\"2044\":1}}],[\"interaction\",{\"1\":{\"246\":1,\"2044\":1}}],[\"interm\",{\"1\":{\"846\":4}}],[\"intermediates\",{\"1\":{\"692\":1,\"1719\":1,\"1721\":1,\"1725\":1}}],[\"intermediate\",{\"1\":{\"44\":1,\"141\":3,\"235\":1,\"262\":1,\"273\":1,\"709\":1,\"733\":1,\"734\":1,\"774\":1,\"980\":1,\"1124\":1,\"1125\":1,\"1147\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1}}],[\"interpolation\",{\"1\":{\"1552\":1,\"1559\":2,\"1582\":1,\"1617\":1,\"1624\":1}}],[\"interpolated\",{\"1\":{\"1617\":1}}],[\"interpolate\",{\"1\":{\"833\":1,\"1582\":1,\"1624\":2,\"1670\":2,\"1699\":2}}],[\"interpreted\",{\"1\":{\"817\":1,\"1725\":2,\"1806\":1}}],[\"interpreter\",{\"1\":{\"110\":1,\"162\":1}}],[\"interctc\",{\"1\":{\"429\":2,\"709\":2,\"733\":3,\"734\":3,\"736\":1,\"737\":1,\"774\":2,\"777\":1,\"780\":2,\"1996\":1,\"1997\":1,\"2127\":1}}],[\"interchanged\",{\"1\":{\"60\":1}}],[\"internet\",{\"1\":{\"248\":2}}],[\"international\",{\"1\":{\"202\":1,\"207\":1}}],[\"internally\",{\"1\":{\"2355\":1}}],[\"internal\",{\"1\":{\"140\":1,\"142\":1,\"150\":1,\"614\":1,\"629\":1,\"634\":1,\"635\":1,\"650\":1,\"652\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"692\":1,\"693\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"706\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"715\":1,\"718\":1,\"720\":1,\"724\":2,\"725\":2,\"726\":1,\"727\":1,\"728\":2,\"729\":2,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"741\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"751\":1,\"752\":1,\"754\":1,\"757\":1,\"759\":1,\"766\":1,\"767\":1,\"771\":1,\"774\":1,\"775\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"786\":1,\"787\":1,\"788\":1,\"790\":1,\"791\":1,\"793\":1,\"794\":1,\"796\":1,\"798\":1,\"800\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"820\":1,\"823\":1,\"825\":1,\"828\":1,\"829\":3,\"830\":2,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"859\":2,\"860\":1,\"862\":1,\"864\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"979\":1,\"981\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1072\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1126\":1,\"1127\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1156\":1,\"1158\":1,\"1159\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1177\":1,\"1179\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":1,\"1205\":1,\"1207\":1,\"1208\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":1,\"1219\":1,\"1222\":1,\"1223\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1269\":1,\"1270\":1,\"1271\":2,\"1272\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1279\":1,\"1281\":1,\"1282\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1333\":1,\"1334\":1,\"1381\":1,\"1383\":1,\"1387\":1,\"1392\":1,\"1398\":1,\"1400\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1417\":1,\"1419\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1508\":1,\"1509\":1,\"1511\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1533\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1547\":1,\"1554\":1,\"1576\":1,\"1588\":1,\"1590\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1638\":1,\"1640\":1,\"1641\":1,\"1652\":1,\"1656\":1,\"1657\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1667\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1702\":1,\"1838\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1959\":1,\"1965\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2129\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2221\":1,\"2222\":1,\"2232\":1,\"2238\":1,\"2305\":1,\"2325\":1,\"2327\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2407\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2434\":1,\"2443\":1,\"2445\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1}}],[\"interference\",{\"1\":{\"223\":1}}],[\"interfaces\",{\"1\":{\"67\":2,\"245\":1}}],[\"interface\",{\"0\":{\"1659\":1,\"1723\":1,\"1724\":1,\"1762\":1,\"1775\":1,\"1777\":1,\"1793\":1,\"1804\":1,\"1821\":1,\"1843\":1,\"1844\":1},\"1\":{\"66\":1,\"67\":1,\"78\":1,\"79\":2,\"84\":1,\"167\":1,\"245\":1,\"247\":1,\"248\":1,\"699\":1,\"793\":1,\"821\":1,\"829\":2,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1659\":1,\"1723\":2,\"1724\":2,\"1726\":1,\"1727\":1,\"1731\":1,\"1762\":1,\"1775\":1,\"1777\":1,\"1793\":2,\"1798\":1,\"1799\":2,\"1800\":1,\"1804\":2,\"1821\":2,\"1843\":2,\"1844\":2,\"1848\":1,\"1965\":1,\"2044\":4,\"2130\":1}}],[\"interface>\",{\"1\":{\"66\":2}}],[\"intervals\",{\"1\":{\"1919\":2,\"2143\":1}}],[\"interval=none\",{\"1\":{\"1854\":1}}],[\"interval\",{\"0\":{\"90\":1},\"1\":{\"90\":1,\"377\":4,\"449\":2,\"699\":4,\"1376\":1,\"1377\":1,\"1645\":1,\"1650\":1,\"1854\":2,\"1951\":1,\"2130\":2,\"2136\":1,\"2137\":1,\"2307\":1,\"2339\":2,\"2348\":3,\"2354\":3,\"2355\":2,\"2370\":6,\"2372\":3,\"2380\":1}}],[\"intorfloat\",{\"1\":{\"1905\":1}}],[\"intorlistofinteger\",{\"1\":{\"1878\":1}}],[\"intornone\",{\"1\":{\"1678\":1}}],[\"into\",{\"0\":{\"1335\":1,\"1336\":1},\"1\":{\"2\":1,\"3\":2,\"18\":1,\"34\":2,\"74\":1,\"102\":1,\"118\":1,\"127\":1,\"182\":1,\"200\":2,\"223\":1,\"235\":1,\"242\":1,\"262\":1,\"269\":6,\"276\":1,\"278\":6,\"287\":1,\"293\":1,\"536\":1,\"592\":1,\"703\":1,\"706\":1,\"755\":1,\"777\":1,\"785\":2,\"830\":1,\"912\":2,\"1000\":1,\"1035\":1,\"1061\":1,\"1062\":1,\"1113\":1,\"1156\":1,\"1164\":2,\"1251\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"1335\":2,\"1336\":2,\"1356\":1,\"1374\":1,\"1375\":1,\"1533\":1,\"1629\":1,\"1711\":1,\"1712\":1,\"1758\":1,\"1931\":1,\"1940\":1,\"1942\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"2040\":2,\"2045\":2,\"2130\":1,\"2131\":2,\"2133\":1,\"2136\":1,\"2138\":2,\"2143\":2,\"2146\":1,\"2147\":1,\"2184\":2,\"2227\":1,\"2231\":1,\"2312\":1,\"2431\":1,\"2432\":1,\"2443\":1,\"2490\":1}}],[\"in\",{\"0\":{\"22\":1,\"40\":1,\"71\":1,\"91\":1,\"114\":1,\"117\":1,\"259\":1,\"2388\":1,\"2389\":1},\"1\":{\"1\":1,\"3\":9,\"10\":1,\"19\":2,\"22\":3,\"24\":2,\"25\":1,\"26\":4,\"27\":1,\"28\":1,\"32\":2,\"36\":1,\"39\":3,\"40\":2,\"41\":4,\"42\":2,\"43\":8,\"44\":3,\"45\":4,\"47\":2,\"48\":5,\"49\":1,\"51\":1,\"55\":3,\"59\":1,\"60\":1,\"62\":1,\"67\":4,\"68\":3,\"69\":1,\"70\":5,\"71\":2,\"78\":2,\"79\":4,\"82\":3,\"84\":2,\"85\":2,\"86\":2,\"90\":1,\"91\":3,\"96\":2,\"97\":1,\"98\":3,\"99\":4,\"100\":3,\"102\":1,\"104\":1,\"106\":1,\"107\":1,\"108\":1,\"110\":2,\"117\":1,\"118\":1,\"119\":1,\"121\":1,\"123\":3,\"126\":3,\"128\":9,\"130\":1,\"133\":2,\"134\":3,\"135\":3,\"136\":1,\"137\":1,\"138\":6,\"139\":6,\"140\":1,\"141\":14,\"142\":3,\"143\":2,\"144\":2,\"145\":6,\"146\":2,\"147\":5,\"148\":3,\"150\":8,\"152\":1,\"153\":2,\"159\":1,\"162\":1,\"163\":2,\"164\":1,\"165\":2,\"167\":2,\"168\":3,\"173\":5,\"175\":1,\"190\":3,\"196\":1,\"197\":4,\"200\":18,\"201\":2,\"205\":5,\"206\":2,\"211\":9,\"212\":2,\"213\":1,\"217\":2,\"218\":3,\"219\":1,\"220\":2,\"223\":34,\"224\":17,\"225\":8,\"228\":3,\"232\":1,\"235\":1,\"240\":2,\"242\":10,\"243\":15,\"245\":1,\"246\":1,\"247\":2,\"249\":1,\"252\":1,\"254\":4,\"255\":2,\"259\":3,\"262\":6,\"263\":1,\"265\":1,\"266\":7,\"267\":26,\"268\":3,\"269\":20,\"270\":1,\"271\":1,\"272\":2,\"273\":1,\"274\":1,\"275\":10,\"276\":32,\"277\":3,\"278\":20,\"279\":1,\"280\":1,\"282\":2,\"284\":3,\"285\":9,\"286\":36,\"287\":3,\"288\":1,\"289\":3,\"290\":23,\"377\":2,\"449\":3,\"516\":2,\"523\":1,\"527\":2,\"535\":2,\"536\":1,\"537\":4,\"561\":1,\"572\":1,\"616\":1,\"617\":2,\"618\":2,\"620\":8,\"624\":2,\"625\":4,\"626\":4,\"630\":2,\"633\":2,\"634\":2,\"636\":2,\"639\":1,\"642\":1,\"645\":2,\"649\":2,\"664\":2,\"667\":1,\"669\":1,\"676\":2,\"691\":3,\"692\":13,\"696\":4,\"697\":1,\"699\":4,\"702\":1,\"703\":4,\"705\":1,\"706\":1,\"709\":3,\"710\":4,\"711\":5,\"712\":3,\"716\":2,\"717\":1,\"733\":1,\"735\":1,\"736\":1,\"737\":1,\"738\":1,\"745\":1,\"747\":2,\"748\":1,\"749\":1,\"750\":1,\"754\":1,\"755\":5,\"756\":3,\"760\":8,\"768\":16,\"770\":2,\"771\":1,\"773\":3,\"774\":4,\"775\":6,\"777\":2,\"780\":7,\"784\":1,\"785\":5,\"787\":2,\"790\":9,\"792\":1,\"796\":2,\"798\":1,\"803\":1,\"818\":1,\"820\":6,\"821\":4,\"828\":1,\"846\":11,\"849\":2,\"850\":12,\"862\":2,\"866\":1,\"867\":1,\"878\":6,\"879\":6,\"881\":6,\"882\":5,\"883\":5,\"884\":6,\"885\":1,\"911\":1,\"919\":2,\"922\":4,\"927\":3,\"936\":2,\"937\":2,\"943\":1,\"960\":1,\"962\":1,\"973\":1,\"974\":1,\"978\":4,\"980\":2,\"981\":1,\"982\":5,\"994\":2,\"1000\":1,\"1008\":2,\"1011\":1,\"1022\":1,\"1029\":10,\"1031\":2,\"1035\":2,\"1050\":2,\"1051\":1,\"1053\":2,\"1054\":2,\"1061\":3,\"1062\":7,\"1064\":1,\"1070\":1,\"1071\":1,\"1078\":1,\"1080\":4,\"1082\":2,\"1086\":1,\"1089\":1,\"1093\":1,\"1107\":7,\"1108\":1,\"1110\":1,\"1112\":3,\"1113\":3,\"1116\":1,\"1117\":1,\"1118\":3,\"1120\":1,\"1122\":1,\"1124\":9,\"1125\":8,\"1131\":2,\"1132\":1,\"1133\":5,\"1136\":2,\"1141\":2,\"1145\":7,\"1147\":7,\"1148\":1,\"1149\":1,\"1153\":2,\"1155\":10,\"1156\":2,\"1157\":12,\"1158\":2,\"1161\":1,\"1162\":7,\"1168\":1,\"1176\":2,\"1180\":9,\"1181\":10,\"1185\":1,\"1189\":1,\"1196\":1,\"1202\":2,\"1204\":1,\"1205\":1,\"1207\":1,\"1208\":1,\"1209\":4,\"1213\":1,\"1217\":2,\"1218\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1228\":4,\"1229\":1,\"1232\":2,\"1233\":1,\"1235\":8,\"1238\":1,\"1240\":2,\"1242\":1,\"1244\":1,\"1245\":1,\"1246\":4,\"1247\":1,\"1250\":3,\"1251\":3,\"1252\":5,\"1253\":3,\"1259\":1,\"1261\":3,\"1262\":1,\"1264\":10,\"1265\":6,\"1267\":3,\"1268\":3,\"1269\":9,\"1270\":11,\"1271\":9,\"1272\":1,\"1273\":6,\"1274\":6,\"1278\":3,\"1279\":7,\"1280\":11,\"1281\":14,\"1282\":11,\"1283\":4,\"1290\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1309\":1,\"1311\":2,\"1316\":2,\"1318\":1,\"1320\":1,\"1322\":1,\"1327\":4,\"1328\":2,\"1330\":4,\"1334\":17,\"1346\":1,\"1347\":1,\"1356\":1,\"1368\":3,\"1373\":1,\"1385\":7,\"1386\":2,\"1389\":1,\"1390\":1,\"1391\":2,\"1392\":3,\"1396\":1,\"1397\":3,\"1401\":3,\"1402\":3,\"1403\":2,\"1406\":2,\"1408\":3,\"1409\":3,\"1410\":2,\"1413\":1,\"1420\":3,\"1422\":3,\"1439\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":2,\"1452\":2,\"1454\":3,\"1456\":3,\"1458\":1,\"1460\":1,\"1466\":2,\"1467\":2,\"1468\":2,\"1469\":1,\"1513\":5,\"1514\":5,\"1516\":2,\"1517\":1,\"1519\":3,\"1520\":1,\"1521\":3,\"1524\":2,\"1526\":5,\"1530\":1,\"1533\":2,\"1534\":4,\"1535\":1,\"1536\":2,\"1545\":1,\"1546\":2,\"1548\":6,\"1549\":4,\"1551\":5,\"1552\":48,\"1553\":11,\"1568\":1,\"1577\":1,\"1579\":1,\"1581\":2,\"1582\":1,\"1583\":1,\"1590\":1,\"1592\":6,\"1593\":1,\"1594\":1,\"1595\":2,\"1596\":5,\"1597\":3,\"1598\":6,\"1599\":34,\"1600\":5,\"1604\":3,\"1605\":6,\"1606\":3,\"1608\":1,\"1609\":3,\"1610\":6,\"1611\":7,\"1612\":7,\"1613\":6,\"1614\":2,\"1615\":2,\"1616\":2,\"1617\":2,\"1618\":1,\"1619\":7,\"1620\":7,\"1621\":8,\"1622\":2,\"1624\":1,\"1625\":10,\"1626\":46,\"1627\":2,\"1628\":6,\"1644\":1,\"1645\":1,\"1647\":3,\"1651\":1,\"1655\":3,\"1656\":1,\"1660\":1,\"1662\":3,\"1664\":1,\"1665\":1,\"1668\":10,\"1669\":1,\"1670\":1,\"1671\":1,\"1672\":1,\"1673\":1,\"1674\":2,\"1676\":1,\"1678\":1,\"1679\":1,\"1680\":4,\"1683\":1,\"1686\":1,\"1687\":2,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":4,\"1694\":1,\"1697\":1,\"1698\":5,\"1702\":2,\"1708\":3,\"1709\":3,\"1710\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1717\":1,\"1719\":7,\"1720\":1,\"1721\":2,\"1725\":8,\"1729\":1,\"1730\":2,\"1731\":6,\"1733\":2,\"1736\":2,\"1737\":3,\"1747\":1,\"1748\":2,\"1750\":8,\"1751\":8,\"1752\":2,\"1753\":9,\"1754\":5,\"1755\":2,\"1758\":2,\"1760\":1,\"1764\":2,\"1768\":2,\"1770\":2,\"1771\":1,\"1779\":1,\"1785\":1,\"1786\":2,\"1787\":2,\"1788\":1,\"1794\":1,\"1795\":5,\"1806\":4,\"1808\":1,\"1810\":2,\"1811\":2,\"1812\":2,\"1814\":1,\"1816\":1,\"1817\":1,\"1818\":2,\"1821\":1,\"1822\":2,\"1833\":2,\"1839\":2,\"1846\":1,\"1855\":1,\"1859\":1,\"1860\":1,\"1862\":1,\"1863\":2,\"1880\":1,\"1881\":1,\"1882\":2,\"1883\":3,\"1893\":2,\"1901\":2,\"1903\":2,\"1916\":1,\"1918\":1,\"1927\":1,\"1931\":1,\"1934\":1,\"1941\":1,\"1943\":2,\"1955\":1,\"1959\":1,\"1961\":1,\"1963\":1,\"1966\":2,\"1975\":1,\"1990\":1,\"1992\":5,\"1993\":8,\"1994\":4,\"1995\":2,\"1996\":1,\"1997\":2,\"2000\":10,\"2001\":7,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":2,\"2006\":1,\"2007\":2,\"2016\":2,\"2018\":2,\"2019\":1,\"2020\":2,\"2021\":1,\"2039\":2,\"2065\":4,\"2101\":1,\"2127\":1,\"2129\":2,\"2130\":9,\"2131\":1,\"2132\":1,\"2133\":1,\"2134\":3,\"2136\":1,\"2143\":2,\"2150\":2,\"2162\":1,\"2177\":1,\"2181\":1,\"2183\":1,\"2184\":4,\"2187\":2,\"2191\":4,\"2192\":1,\"2198\":2,\"2203\":1,\"2209\":1,\"2216\":4,\"2218\":1,\"2221\":1,\"2223\":2,\"2224\":5,\"2225\":1,\"2226\":2,\"2227\":1,\"2228\":4,\"2229\":4,\"2231\":2,\"2232\":1,\"2235\":4,\"2236\":5,\"2237\":2,\"2238\":1,\"2239\":18,\"2240\":17,\"2241\":2,\"2245\":19,\"2246\":1,\"2248\":1,\"2249\":3,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":3,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":2,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2309\":2,\"2311\":1,\"2325\":2,\"2344\":2,\"2348\":1,\"2352\":1,\"2353\":3,\"2354\":4,\"2355\":19,\"2359\":1,\"2364\":3,\"2367\":1,\"2370\":2,\"2372\":1,\"2377\":1,\"2384\":2,\"2385\":2,\"2388\":1,\"2389\":1,\"2404\":1,\"2411\":32,\"2412\":34,\"2413\":2,\"2420\":1,\"2423\":33,\"2424\":2,\"2425\":8,\"2429\":9,\"2430\":4,\"2431\":19,\"2432\":30,\"2433\":1,\"2436\":1,\"2438\":1,\"2440\":1,\"2447\":25,\"2448\":3,\"2480\":2,\"2482\":2,\"2490\":2,\"2507\":1}}],[\"item\",{\"1\":{\"1382\":1,\"2039\":2,\"2130\":3,\"2139\":1,\"2355\":2}}],[\"items\",{\"1\":{\"290\":1,\"1618\":1,\"1824\":1,\"1993\":1,\"2130\":6,\"2139\":2,\"2144\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1,\"2478\":1}}],[\"iterfactory\",{\"1\":{\"1647\":1}}],[\"iter=none\",{\"1\":{\"1246\":1}}],[\"iterables\",{\"1\":{\"2355\":2}}],[\"iterabledataset\",{\"1\":{\"2351\":1,\"2366\":1}}],[\"iterableespnetdataset\",{\"0\":{\"2351\":1},\"1\":{\"2249\":1,\"2351\":1,\"2352\":1,\"2366\":2}}],[\"iterable\",{\"0\":{\"2351\":1,\"2366\":1,\"2392\":1},\"1\":{\"794\":4,\"1478\":1,\"1951\":2,\"1954\":1,\"2249\":1,\"2274\":1,\"2275\":3,\"2278\":2,\"2279\":1,\"2283\":2,\"2284\":1,\"2285\":2,\"2287\":1,\"2288\":1,\"2291\":3,\"2292\":2,\"2293\":3,\"2336\":4,\"2337\":4,\"2338\":2,\"2347\":2,\"2351\":1,\"2354\":1,\"2355\":4,\"2356\":3,\"2360\":3,\"2361\":3,\"2362\":4,\"2363\":3,\"2365\":2,\"2366\":1,\"2367\":1,\"2369\":3,\"2371\":2,\"2392\":1}}],[\"iterates\",{\"1\":{\"2134\":1}}],[\"iterate\",{\"1\":{\"1645\":2}}],[\"iterative\",{\"1\":{\"927\":1,\"1246\":1,\"1334\":1}}],[\"iterations=3\",{\"1\":{\"1376\":1,\"1853\":1}}],[\"iterations\",{\"0\":{\"91\":1},\"1\":{\"262\":1,\"309\":2,\"516\":1,\"776\":1,\"1126\":1,\"1127\":1,\"1130\":1,\"1217\":1,\"1311\":3,\"1318\":3,\"1322\":3,\"1327\":3,\"1328\":4,\"1329\":1,\"1330\":3,\"1376\":1,\"1400\":1,\"1441\":1,\"1469\":1,\"1645\":1,\"1719\":1,\"1725\":1,\"1806\":1,\"2249\":1,\"2253\":1,\"2307\":2,\"2367\":1,\"2482\":1,\"2490\":1}}],[\"iteration\",{\"0\":{\"1377\":1},\"1\":{\"39\":1,\"48\":2,\"79\":1,\"91\":1,\"102\":1,\"259\":1,\"1130\":2,\"1377\":2,\"2307\":2}}],[\"iteratoroptions\",{\"0\":{\"2258\":1},\"1\":{\"2249\":7,\"2258\":1}}],[\"iterators\",{\"0\":{\"1642\":1,\"1643\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1,\"2528\":1},\"1\":{\"91\":1,\"974\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1642\":1,\"1643\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1,\"2134\":1}}],[\"iterator\",{\"0\":{\"1778\":1,\"2134\":1},\"1\":{\"79\":1,\"91\":2,\"101\":4,\"102\":1,\"974\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1642\":1,\"1644\":1,\"1645\":1,\"1647\":2,\"1648\":1,\"1650\":1,\"1778\":3,\"2134\":8,\"2139\":3,\"2144\":3,\"2249\":5,\"2253\":2,\"2338\":2,\"2347\":2,\"2354\":1,\"2359\":1,\"2365\":2,\"2366\":2,\"2369\":3,\"2371\":2}}],[\"iter1\",{\"1\":{\"259\":1}}],[\"iters=10\",{\"1\":{\"1489\":1}}],[\"iters\",{\"1\":{\"39\":1,\"91\":2,\"516\":2,\"558\":2,\"1389\":1,\"1391\":1,\"1396\":1,\"1400\":2,\"1401\":1,\"1403\":1,\"1406\":1,\"1408\":1,\"1410\":1,\"1441\":2,\"1466\":1,\"1468\":1,\"1469\":2,\"1645\":2,\"1647\":1,\"1650\":1,\"2015\":1,\"2249\":3,\"2253\":3,\"2258\":2,\"2482\":2}}],[\"iter\",{\"0\":{\"1642\":1,\"1643\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1},\"1\":{\"39\":1,\"91\":4,\"113\":2,\"173\":1,\"377\":2,\"449\":2,\"974\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1246\":1,\"1642\":2,\"1643\":1,\"1644\":4,\"1645\":2,\"1646\":1,\"1647\":4,\"1648\":2,\"1649\":1,\"1650\":2,\"1651\":1,\"1951\":2,\"2134\":1,\"2249\":21,\"2253\":5,\"2338\":2,\"2367\":1,\"2369\":3,\"2377\":1,\"2490\":2}}],[\"italian\",{\"1\":{\"287\":1,\"481\":1}}],[\"ith\",{\"1\":{\"98\":1}}],[\"itself\",{\"1\":{\"146\":1,\"760\":1,\"2133\":1}}],[\"its\",{\"1\":{\"44\":1,\"46\":1,\"49\":1,\"141\":1,\"175\":1,\"262\":4,\"269\":1,\"278\":1,\"567\":1,\"919\":1,\"939\":1,\"960\":1,\"1124\":1,\"1125\":1,\"1155\":1,\"1157\":1,\"1306\":1,\"1327\":1,\"1330\":1,\"1371\":1,\"1375\":1,\"1719\":5,\"1721\":1,\"1725\":8,\"1806\":2,\"1862\":1,\"2000\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2325\":1,\"2327\":1,\"2334\":1,\"2355\":1}}],[\"it\",{\"0\":{\"1503\":1,\"1952\":1},\"1\":{\"0\":1,\"1\":1,\"3\":1,\"18\":1,\"22\":1,\"26\":2,\"28\":1,\"37\":1,\"39\":2,\"40\":1,\"43\":1,\"44\":1,\"47\":3,\"60\":1,\"67\":2,\"68\":1,\"71\":2,\"80\":1,\"81\":1,\"82\":4,\"84\":2,\"88\":1,\"91\":1,\"96\":1,\"98\":1,\"101\":4,\"104\":1,\"106\":4,\"108\":1,\"110\":2,\"119\":1,\"124\":3,\"126\":1,\"128\":1,\"138\":1,\"139\":3,\"141\":1,\"152\":1,\"154\":1,\"162\":3,\"165\":1,\"166\":1,\"167\":1,\"168\":2,\"194\":1,\"195\":1,\"196\":3,\"197\":1,\"200\":6,\"205\":5,\"211\":6,\"213\":3,\"214\":1,\"217\":3,\"223\":2,\"224\":4,\"225\":5,\"232\":2,\"233\":1,\"235\":1,\"240\":3,\"242\":7,\"243\":5,\"252\":2,\"254\":5,\"258\":1,\"259\":2,\"262\":1,\"266\":5,\"267\":5,\"268\":3,\"269\":2,\"275\":5,\"276\":5,\"277\":3,\"278\":2,\"285\":6,\"286\":6,\"290\":6,\"526\":2,\"536\":1,\"691\":1,\"696\":1,\"697\":1,\"699\":1,\"704\":1,\"722\":1,\"724\":2,\"725\":2,\"726\":1,\"727\":1,\"728\":2,\"729\":1,\"733\":1,\"744\":1,\"756\":4,\"760\":2,\"773\":4,\"817\":1,\"821\":3,\"828\":1,\"829\":3,\"830\":2,\"859\":2,\"866\":3,\"867\":3,\"922\":1,\"943\":1,\"1000\":1,\"1008\":1,\"1022\":1,\"1031\":2,\"1035\":2,\"1112\":2,\"1113\":2,\"1124\":1,\"1155\":1,\"1157\":1,\"1202\":1,\"1209\":1,\"1224\":1,\"1225\":1,\"1250\":2,\"1251\":2,\"1259\":1,\"1261\":1,\"1269\":3,\"1270\":3,\"1271\":3,\"1273\":1,\"1274\":1,\"1290\":1,\"1301\":1,\"1308\":1,\"1310\":1,\"1327\":1,\"1334\":2,\"1372\":1,\"1397\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1502\":1,\"1503\":1,\"1513\":1,\"1526\":1,\"1548\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1592\":1,\"1596\":2,\"1597\":2,\"1598\":1,\"1599\":1,\"1600\":1,\"1605\":1,\"1607\":1,\"1609\":1,\"1610\":1,\"1619\":1,\"1625\":1,\"1626\":1,\"1628\":1,\"1644\":2,\"1647\":3,\"1679\":1,\"1680\":1,\"1692\":1,\"1697\":1,\"1698\":2,\"1711\":1,\"1712\":1,\"1716\":1,\"1719\":1,\"1720\":4,\"1721\":5,\"1725\":2,\"1754\":1,\"1804\":1,\"1806\":1,\"1808\":1,\"1860\":1,\"1862\":2,\"1883\":1,\"1932\":1,\"1952\":1,\"2000\":1,\"2001\":1,\"2016\":1,\"2039\":1,\"2044\":5,\"2130\":2,\"2133\":1,\"2134\":1,\"2138\":1,\"2184\":1,\"2239\":1,\"2240\":1,\"2276\":1,\"2277\":1,\"2314\":1,\"2345\":2,\"2354\":1,\"2355\":6,\"2380\":2,\"2447\":1,\"2474\":1}}],[\"ghi\",{\"1\":{\"2502\":2}}],[\"gdca\",{\"1\":{\"2245\":1}}],[\"gdcattloc\",{\"0\":{\"1768\":1},\"1\":{\"1768\":2}}],[\"gz\",{\"1\":{\"2139\":2}}],[\"gpt2\",{\"1\":{\"2137\":1}}],[\"gptneoxmlp\",{\"0\":{\"2048\":1}}],[\"gpt\",{\"0\":{\"2046\":1}}],[\"gpurnnt\",{\"0\":{\"755\":1},\"1\":{\"755\":1,\"785\":1}}],[\"gpu=false\",{\"1\":{\"1128\":1}}],[\"gpu=2\",{\"1\":{\"168\":1}}],[\"gpu=$0\",{\"1\":{\"168\":3}}],[\"gpu=\",{\"1\":{\"168\":3}}],[\"gpu=0\",{\"1\":{\"168\":2}}],[\"gpus\",{\"0\":{\"93\":1,\"94\":1,\"121\":1,\"123\":1},\"1\":{\"24\":1,\"41\":2,\"55\":2,\"56\":1,\"58\":1,\"63\":1,\"94\":3,\"102\":1,\"104\":1,\"123\":2,\"173\":5,\"243\":1,\"522\":1,\"527\":1,\"2134\":2}}],[\"gpu\",{\"0\":{\"24\":1,\"41\":1,\"173\":1,\"522\":1,\"755\":1,\"785\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"919\":1,\"922\":1,\"937\":1},\"1\":{\"22\":4,\"23\":1,\"24\":3,\"25\":1,\"26\":1,\"28\":2,\"29\":2,\"41\":6,\"53\":1,\"54\":9,\"55\":1,\"56\":1,\"62\":1,\"94\":3,\"95\":1,\"102\":1,\"106\":1,\"123\":1,\"126\":1,\"168\":2,\"173\":5,\"195\":1,\"223\":2,\"243\":1,\"356\":2,\"522\":2,\"527\":1,\"614\":1,\"634\":1,\"641\":1,\"643\":1,\"651\":1,\"755\":1,\"777\":1,\"785\":1,\"847\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"907\":1,\"908\":1,\"919\":1,\"922\":2,\"937\":2,\"1156\":1,\"1749\":1,\"1815\":1,\"2130\":8,\"2131\":1,\"2134\":1,\"2136\":2,\"2162\":2,\"2355\":1}}],[\"g=none\",{\"1\":{\"1519\":1,\"1520\":1,\"1536\":1,\"1554\":1,\"1556\":2}}],[\"gfp\",{\"1\":{\"1119\":1}}],[\"gn\",{\"1\":{\"1061\":2,\"1062\":2,\"1063\":1,\"1198\":1}}],[\"gnome\",{\"1\":{\"114\":1}}],[\"gs\",{\"1\":{\"2235\":1,\"2236\":1}}],[\"gsm\",{\"1\":{\"1678\":2}}],[\"gss\",{\"1\":{\"819\":2}}],[\"gst\",{\"0\":{\"2421\":1,\"2425\":1,\"2429\":1,\"2430\":1},\"1\":{\"289\":1,\"1598\":9,\"1599\":26,\"1600\":9,\"2245\":26,\"2411\":26,\"2412\":26,\"2421\":1,\"2423\":26,\"2425\":1,\"2429\":10,\"2430\":11,\"2431\":26,\"2432\":26}}],[\"glstm\",{\"0\":{\"1176\":1},\"1\":{\"1124\":9,\"1125\":8,\"1176\":1}}],[\"gln\",{\"1\":{\"973\":1,\"975\":2,\"980\":2,\"981\":1,\"982\":2,\"1072\":1,\"1139\":1,\"1141\":1,\"1148\":1,\"1179\":2,\"1185\":1,\"1202\":1,\"1255\":1,\"1259\":2,\"1267\":2,\"1268\":2,\"1272\":1,\"1273\":2,\"1274\":2}}],[\"gluconv2d\",{\"0\":{\"1180\":1},\"1\":{\"1147\":1,\"1180\":1}}],[\"gluconvtranspose2d\",{\"0\":{\"1181\":1},\"1\":{\"1147\":2,\"1181\":1}}],[\"gluconv\",{\"1\":{\"1147\":3}}],[\"glu=true\",{\"1\":{\"754\":1}}],[\"glu\",{\"0\":{\"754\":1},\"1\":{\"674\":2,\"754\":3,\"817\":1,\"1180\":1,\"1181\":1}}],[\"globalmvn\",{\"0\":{\"1656\":1},\"1\":{\"1656\":1}}],[\"globallayernorm\",{\"0\":{\"975\":1,\"1179\":1},\"1\":{\"975\":1,\"1179\":1}}],[\"global\",{\"0\":{\"801\":1,\"896\":1,\"940\":1,\"946\":1,\"1656\":1},\"1\":{\"243\":1,\"801\":1,\"817\":1,\"896\":1,\"929\":1,\"940\":1,\"946\":1,\"975\":1,\"1119\":1,\"1179\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"1513\":5,\"1519\":5,\"1520\":5,\"1526\":1,\"1536\":4,\"1548\":3,\"1551\":5,\"1552\":3,\"1553\":1,\"1556\":3,\"1581\":1,\"1583\":2,\"1592\":7,\"1598\":1,\"1599\":4,\"1600\":1,\"1611\":5,\"1612\":5,\"1613\":5,\"1616\":4,\"1625\":1,\"1626\":3,\"1628\":6,\"1656\":2,\"1768\":3,\"2134\":3,\"2223\":1,\"2224\":1,\"2231\":1,\"2239\":1,\"2245\":2,\"2411\":1,\"2412\":1,\"2423\":1,\"2429\":1,\"2431\":1,\"2432\":1}}],[\"g2pk\",{\"0\":{\"2277\":1},\"1\":{\"287\":6,\"481\":3,\"2277\":4}}],[\"g2p\",{\"0\":{\"2276\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":1,\"2300\":1,\"2301\":1,\"2302\":1},\"1\":{\"218\":2,\"265\":1,\"266\":2,\"269\":4,\"271\":1,\"274\":1,\"275\":2,\"278\":4,\"280\":1,\"284\":1,\"285\":6,\"286\":13,\"287\":8,\"290\":6,\"481\":7,\"2276\":7,\"2277\":3,\"2280\":2,\"2285\":1,\"2286\":1,\"2293\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":1,\"2299\":2,\"2300\":1,\"2301\":1,\"2302\":1,\"2336\":1,\"2337\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1}}],[\"gcc\",{\"1\":{\"152\":1,\"159\":1,\"161\":1}}],[\"gcp\",{\"0\":{\"152\":1}}],[\"gtn\",{\"0\":{\"1769\":1}}],[\"gtnctclossfunction\",{\"0\":{\"1769\":1}}],[\"gtnctc\",{\"1\":{\"706\":1}}],[\"gtcrn\",{\"1\":{\"1316\":2}}],[\"gta\",{\"1\":{\"284\":1,\"290\":4}}],[\"gtxt\",{\"1\":{\"212\":1}}],[\"gt\",{\"1\":{\"126\":3,\"196\":7,\"200\":2,\"201\":6,\"211\":1,\"212\":3,\"213\":4,\"224\":5,\"225\":6,\"242\":41,\"243\":5,\"259\":7,\"267\":22,\"268\":7,\"271\":2,\"276\":22,\"277\":7,\"280\":2,\"285\":2,\"286\":45,\"287\":7,\"290\":4,\"515\":2,\"516\":17,\"517\":2,\"518\":5,\"519\":4,\"520\":3,\"521\":8,\"522\":1,\"523\":13,\"524\":10,\"525\":10,\"526\":12,\"527\":10,\"531\":2,\"533\":1,\"535\":5,\"536\":1,\"537\":10,\"1552\":4,\"1556\":2}}],[\"gemmamlp\",{\"0\":{\"2047\":1}}],[\"gev\",{\"0\":{\"1318\":1,\"1332\":1},\"1\":{\"1318\":2,\"1332\":2}}],[\"geometry\",{\"1\":{\"1163\":1,\"1164\":1,\"1264\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1333\":1,\"1334\":1}}],[\"gelu\",{\"0\":{\"900\":1,\"901\":1},\"1\":{\"674\":2,\"744\":1,\"817\":1,\"900\":1,\"901\":1,\"1205\":1,\"1262\":1}}],[\"german\",{\"1\":{\"261\":1,\"287\":1,\"481\":1}}],[\"getting\",{\"1\":{\"205\":1,\"262\":1,\"1271\":1}}],[\"get\",{\"0\":{\"664\":1,\"665\":1,\"666\":1,\"667\":1,\"902\":1,\"903\":1,\"904\":1,\"905\":1,\"906\":1,\"907\":1,\"908\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1312\":1,\"1313\":1,\"1314\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1330\":1,\"1331\":1,\"1482\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1486\":1,\"1563\":1,\"1632\":2,\"1633\":2,\"1688\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1889\":1,\"1891\":1,\"1892\":1,\"1893\":1,\"1953\":1,\"2089\":1,\"2090\":1,\"2091\":1,\"2092\":1,\"2093\":1,\"2094\":1,\"2095\":1,\"2096\":1,\"2151\":1,\"2152\":1,\"2310\":1,\"2311\":2,\"2313\":1,\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2400\":1,\"2477\":1,\"2487\":2,\"2489\":1},\"1\":{\"76\":1,\"159\":3,\"161\":1,\"173\":1,\"201\":1,\"218\":1,\"267\":1,\"269\":1,\"276\":4,\"278\":1,\"284\":1,\"286\":3,\"290\":1,\"614\":1,\"637\":2,\"639\":4,\"641\":1,\"647\":1,\"651\":1,\"664\":1,\"665\":1,\"666\":2,\"667\":2,\"761\":1,\"762\":1,\"765\":1,\"768\":1,\"770\":1,\"772\":1,\"797\":1,\"847\":1,\"902\":1,\"903\":1,\"904\":1,\"905\":1,\"906\":1,\"907\":1,\"908\":1,\"942\":1,\"958\":1,\"959\":1,\"960\":1,\"992\":1,\"994\":1,\"997\":1,\"999\":1,\"1004\":1,\"1006\":1,\"1008\":1,\"1010\":1,\"1016\":1,\"1018\":1,\"1128\":1,\"1210\":1,\"1254\":2,\"1290\":1,\"1309\":1,\"1310\":2,\"1311\":1,\"1312\":2,\"1313\":1,\"1314\":1,\"1315\":2,\"1316\":2,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":2,\"1321\":1,\"1322\":1,\"1323\":1,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1329\":1,\"1330\":1,\"1331\":2,\"1368\":1,\"1441\":2,\"1482\":1,\"1483\":1,\"1484\":3,\"1485\":1,\"1486\":1,\"1563\":1,\"1632\":3,\"1633\":3,\"1644\":1,\"1647\":1,\"1668\":1,\"1688\":1,\"1705\":1,\"1719\":1,\"1724\":1,\"1725\":1,\"1731\":2,\"1749\":1,\"1806\":1,\"1815\":1,\"1822\":1,\"1843\":1,\"1848\":2,\"1886\":1,\"1887\":2,\"1888\":1,\"1889\":2,\"1890\":1,\"1891\":2,\"1892\":1,\"1893\":1,\"1921\":1,\"1948\":1,\"1953\":1,\"1966\":1,\"1973\":1,\"1977\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2021\":1,\"2095\":1,\"2096\":1,\"2126\":1,\"2128\":1,\"2130\":11,\"2132\":1,\"2133\":1,\"2134\":2,\"2136\":8,\"2137\":7,\"2151\":7,\"2152\":1,\"2232\":1,\"2238\":2,\"2249\":2,\"2278\":1,\"2283\":1,\"2291\":1,\"2310\":7,\"2311\":2,\"2313\":1,\"2334\":2,\"2359\":6,\"2367\":2,\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":2,\"2385\":2,\"2386\":1,\"2387\":1,\"2400\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2462\":1,\"2477\":1,\"2487\":3,\"2488\":1,\"2489\":1}}],[\"genration\",{\"1\":{\"1854\":1}}],[\"generic\",{\"1\":{\"828\":1,\"830\":1}}],[\"generalized\",{\"0\":{\"1308\":1},\"1\":{\"1308\":4,\"1318\":2}}],[\"generalise\",{\"1\":{\"286\":1}}],[\"generally\",{\"1\":{\"703\":1,\"717\":1,\"724\":2,\"725\":2,\"726\":1,\"727\":1,\"728\":2,\"729\":1,\"744\":2,\"755\":1,\"784\":1,\"785\":1,\"821\":2,\"823\":1,\"824\":1,\"828\":2,\"829\":4,\"830\":2,\"859\":1,\"878\":1,\"879\":1,\"881\":1,\"884\":1}}],[\"general\",{\"0\":{\"139\":1},\"1\":{\"79\":1,\"175\":1,\"191\":1,\"192\":2,\"257\":1,\"1526\":1,\"1600\":1}}],[\"generator=true\",{\"1\":{\"2327\":1}}],[\"generatoraversarialloss\",{\"1\":{\"1591\":1}}],[\"generatoradversarialloss\",{\"0\":{\"1591\":1},\"1\":{\"1591\":1}}],[\"generator\",{\"0\":{\"1404\":1,\"1524\":1,\"1525\":1,\"1545\":1,\"1552\":1,\"1599\":1,\"1626\":1,\"2453\":2,\"2460\":2,\"2465\":2,\"2467\":2},\"1\":{\"2\":1,\"286\":3,\"1381\":2,\"1389\":7,\"1391\":2,\"1395\":4,\"1396\":3,\"1401\":7,\"1403\":2,\"1404\":1,\"1408\":7,\"1410\":2,\"1466\":7,\"1468\":2,\"1508\":2,\"1513\":1,\"1521\":4,\"1524\":2,\"1525\":3,\"1526\":11,\"1545\":2,\"1548\":1,\"1552\":10,\"1553\":20,\"1576\":2,\"1584\":1,\"1585\":4,\"1587\":1,\"1591\":3,\"1592\":1,\"1598\":31,\"1599\":29,\"1600\":11,\"1605\":1,\"1610\":1,\"1619\":1,\"1625\":18,\"1626\":3,\"1705\":1,\"1881\":1,\"2049\":1,\"2327\":14,\"2348\":2,\"2372\":2,\"2453\":2,\"2460\":3,\"2462\":2,\"2465\":2,\"2467\":2,\"2470\":1,\"2473\":1}}],[\"generated\",{\"1\":{\"3\":2,\"106\":1,\"196\":2,\"200\":1,\"205\":2,\"213\":2,\"218\":2,\"223\":1,\"242\":2,\"247\":1,\"267\":6,\"268\":3,\"269\":1,\"276\":6,\"277\":3,\"278\":1,\"284\":1,\"286\":7,\"290\":1,\"846\":1,\"1009\":2,\"1061\":1,\"1389\":4,\"1395\":4,\"1401\":4,\"1408\":4,\"1419\":1,\"1466\":4,\"1526\":3,\"1552\":2,\"1553\":2,\"1598\":1,\"1599\":1,\"1600\":2,\"1607\":1,\"1625\":2,\"1626\":2,\"1854\":2,\"1919\":1,\"1966\":1,\"2040\":1,\"2044\":4,\"2045\":1,\"2049\":1,\"2054\":1,\"2220\":1,\"2249\":1,\"2253\":1,\"2355\":1,\"2422\":1,\"2427\":2,\"2469\":1,\"2470\":1,\"2471\":1}}],[\"generates\",{\"1\":{\"3\":4,\"47\":1,\"197\":1,\"200\":1,\"211\":1,\"223\":1,\"266\":1,\"275\":1,\"285\":1,\"692\":1,\"760\":2,\"790\":1,\"797\":1,\"820\":2,\"850\":1,\"1723\":1,\"1724\":1,\"1731\":2,\"1750\":1,\"1787\":2,\"1805\":1,\"1822\":1,\"1944\":2,\"1945\":1,\"1946\":1,\"1947\":2,\"1992\":1,\"2049\":1,\"2223\":1}}],[\"generate\",{\"0\":{\"33\":1,\"523\":1,\"575\":1,\"2081\":2,\"2082\":2,\"2083\":2,\"2084\":2,\"2085\":2,\"2086\":2,\"2087\":2,\"2088\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2118\":1,\"2119\":1},\"1\":{\"1\":3,\"2\":1,\"3\":1,\"31\":1,\"33\":1,\"47\":1,\"76\":1,\"162\":1,\"199\":1,\"200\":1,\"201\":1,\"204\":2,\"205\":4,\"217\":1,\"224\":1,\"228\":1,\"232\":1,\"240\":1,\"242\":2,\"243\":2,\"254\":1,\"259\":2,\"267\":2,\"276\":3,\"284\":1,\"285\":5,\"286\":6,\"290\":1,\"429\":2,\"523\":2,\"536\":1,\"575\":2,\"768\":1,\"978\":1,\"987\":1,\"989\":1,\"1224\":1,\"1225\":1,\"1245\":1,\"1273\":1,\"1274\":1,\"1524\":1,\"1556\":1,\"1649\":1,\"1662\":1,\"1750\":1,\"1854\":2,\"1917\":1,\"1948\":2,\"1949\":1,\"1993\":1,\"1998\":1,\"2044\":1,\"2049\":1,\"2054\":1,\"2088\":1,\"2095\":1,\"2218\":1,\"2220\":1,\"2224\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2280\":1,\"2355\":1,\"2411\":1,\"2412\":1,\"2422\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"generation\",{\"0\":{\"30\":1,\"411\":1},\"1\":{\"0\":1,\"68\":1,\"126\":1,\"162\":1,\"199\":1,\"200\":2,\"202\":1,\"204\":1,\"205\":2,\"210\":1,\"211\":2,\"240\":1,\"265\":1,\"266\":2,\"274\":1,\"275\":2,\"284\":2,\"285\":3,\"286\":1,\"389\":1,\"536\":2,\"1750\":1,\"1854\":2,\"2143\":1,\"2224\":1,\"2262\":1}}],[\"generating\",{\"1\":{\"0\":1,\"247\":1,\"2044\":1,\"2354\":1}}],[\"gengeration\",{\"1\":{\"71\":1}}],[\"gen\",{\"0\":{\"987\":1,\"989\":1},\"1\":{\"3\":4,\"267\":1,\"276\":1,\"768\":1,\"987\":1,\"989\":1,\"1993\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"growth\",{\"1\":{\"1392\":2}}],[\"ground\",{\"1\":{\"267\":1,\"276\":1,\"286\":1,\"703\":1,\"717\":1,\"922\":1,\"936\":1,\"937\":1,\"947\":1,\"948\":1,\"949\":1,\"1515\":1,\"1552\":2,\"1556\":3,\"1599\":3,\"1625\":1,\"1626\":1,\"1702\":1,\"2167\":1,\"2176\":1,\"2207\":1}}],[\"groundtruth\",{\"1\":{\"126\":1,\"286\":2,\"290\":1,\"1419\":1,\"1584\":1,\"1587\":1,\"1607\":3,\"1754\":1,\"2239\":1,\"2240\":1,\"2411\":2,\"2412\":4,\"2423\":4,\"2427\":1,\"2447\":4}}],[\"groupresidualvectorquantization\",{\"0\":{\"1406\":1},\"1\":{\"1406\":1}}],[\"groupnorm\",{\"1\":{\"1133\":1,\"1252\":1}}],[\"grouped\",{\"1\":{\"1124\":4,\"1125\":4,\"1176\":6,\"1301\":1,\"1372\":1,\"2146\":1}}],[\"group\",{\"1\":{\"141\":1,\"162\":1,\"706\":1,\"846\":2,\"1176\":1,\"1279\":7,\"1280\":6,\"1281\":4,\"1283\":6,\"1452\":2,\"1456\":1,\"1460\":1,\"2277\":1}}],[\"groups=2\",{\"1\":{\"1124\":1,\"1176\":1}}],[\"groups=1\",{\"1\":{\"1080\":1,\"1082\":1,\"2213\":1,\"2214\":1}}],[\"groups=0\",{\"1\":{\"768\":1}}],[\"groups=none\",{\"1\":{\"768\":1}}],[\"groups\",{\"0\":{\"1963\":1,\"1964\":1},\"1\":{\"43\":2,\"141\":1,\"162\":3,\"620\":2,\"674\":2,\"768\":4,\"846\":3,\"1124\":2,\"1125\":3,\"1176\":2,\"1279\":2,\"1280\":2,\"1281\":1,\"1283\":2,\"1401\":1,\"1402\":1,\"1408\":1,\"1409\":1,\"1442\":1,\"1444\":1,\"1448\":1,\"1466\":1,\"1467\":1,\"1526\":1,\"1549\":1,\"1553\":1,\"1594\":1,\"1595\":1,\"1597\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1736\":2,\"1748\":3,\"1833\":1,\"1963\":1,\"1964\":1}}],[\"gru\",{\"0\":{\"1564\":1},\"1\":{\"787\":1,\"851\":1,\"1134\":1,\"1136\":1,\"1137\":1,\"1139\":1,\"1141\":1,\"1185\":1,\"1257\":1,\"1280\":1,\"1283\":1,\"1564\":1,\"1598\":2,\"1599\":6,\"1600\":2,\"2245\":6,\"2411\":6,\"2412\":6,\"2423\":6,\"2425\":7,\"2429\":6,\"2431\":6,\"2432\":6}}],[\"griffin\",{\"0\":{\"2482\":1,\"2490\":2,\"2495\":1},\"1\":{\"218\":1,\"267\":1,\"276\":1,\"286\":2,\"516\":2,\"558\":1,\"2482\":3,\"2490\":3,\"2495\":1}}],[\"gridnet\",{\"1\":{\"1269\":2,\"1270\":2,\"1271\":2}}],[\"gridnetv3block\",{\"0\":{\"1184\":1},\"1\":{\"1184\":1}}],[\"gridnetv2block\",{\"0\":{\"1183\":1},\"1\":{\"1183\":2,\"1184\":1}}],[\"gridnetblock\",{\"0\":{\"1182\":1},\"1\":{\"1182\":2}}],[\"grid\",{\"1\":{\"166\":2,\"804\":1,\"932\":1,\"934\":1,\"2355\":3}}],[\"grab\",{\"1\":{\"912\":1}}],[\"grabo\",{\"1\":{\"201\":1}}],[\"graph\",{\"1\":{\"377\":2,\"449\":2,\"505\":2,\"2348\":1,\"2370\":2,\"2372\":1,\"2405\":1}}],[\"gratis\",{\"1\":{\"295\":2,\"415\":2}}],[\"grammatek\",{\"1\":{\"2280\":1}}],[\"gram\",{\"1\":{\"200\":2,\"205\":2,\"242\":1}}],[\"graves\",{\"1\":{\"145\":1}}],[\"gradmultiply\",{\"0\":{\"756\":1},\"1\":{\"756\":1}}],[\"gradio\",{\"1\":{\"249\":2,\"2044\":4}}],[\"gradients\",{\"1\":{\"102\":2,\"703\":1,\"706\":1,\"755\":2,\"785\":2,\"846\":1,\"881\":3,\"884\":3,\"1301\":1,\"1372\":1,\"2307\":1,\"2355\":2}}],[\"gradient\",{\"0\":{\"102\":1,\"2307\":2,\"2470\":1},\"1\":{\"102\":2,\"654\":5,\"692\":1,\"703\":1,\"734\":1,\"755\":1,\"756\":5,\"773\":5,\"785\":1,\"786\":1,\"800\":1,\"848\":1,\"866\":5,\"867\":5,\"881\":1,\"884\":1,\"922\":2,\"936\":2,\"937\":2,\"1598\":2,\"1599\":6,\"1600\":2,\"2307\":2,\"2348\":1,\"2370\":2,\"2372\":1,\"2412\":6,\"2423\":6,\"2447\":6,\"2470\":2}}],[\"grad3\",{\"1\":{\"136\":3}}],[\"gradscaler\",{\"1\":{\"2347\":1,\"2369\":2,\"2371\":1}}],[\"grads\",{\"1\":{\"102\":1,\"703\":2,\"755\":3,\"756\":1,\"773\":1,\"785\":3,\"866\":1,\"867\":1,\"881\":3,\"884\":3,\"922\":2,\"936\":2,\"937\":2}}],[\"grad\",{\"0\":{\"881\":1,\"884\":1,\"1502\":1},\"1\":{\"102\":6,\"243\":1,\"654\":6,\"674\":2,\"703\":6,\"706\":2,\"746\":1,\"747\":1,\"755\":2,\"756\":3,\"773\":3,\"785\":2,\"846\":2,\"866\":3,\"867\":3,\"881\":1,\"884\":1,\"961\":1,\"979\":1,\"1501\":1,\"1502\":1,\"2348\":4,\"2355\":2,\"2370\":8,\"2372\":4,\"2470\":1}}],[\"greek\",{\"1\":{\"287\":1,\"481\":1}}],[\"greedy\",{\"1\":{\"45\":1,\"463\":2,\"696\":5,\"697\":2}}],[\"great\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1}}],[\"greatly\",{\"1\":{\"262\":1}}],[\"greater\",{\"1\":{\"48\":1,\"262\":2,\"276\":2}}],[\"grep\",{\"1\":{\"243\":2}}],[\"greg\",{\"1\":{\"202\":1}}],[\"gres\",{\"1\":{\"62\":1}}],[\"gin\",{\"1\":{\"1554\":1,\"1556\":3}}],[\"giving\",{\"1\":{\"47\":1,\"101\":1}}],[\"give\",{\"1\":{\"47\":2,\"86\":1,\"98\":1,\"101\":1,\"102\":1,\"119\":2,\"124\":1,\"162\":2,\"196\":2,\"213\":2,\"268\":2,\"277\":2,\"1881\":1,\"1949\":1}}],[\"given\",{\"1\":{\"39\":1,\"52\":1,\"79\":1,\"81\":2,\"84\":1,\"124\":1,\"200\":1,\"259\":1,\"265\":1,\"269\":1,\"274\":1,\"278\":1,\"284\":1,\"285\":1,\"290\":1,\"614\":2,\"616\":1,\"628\":1,\"634\":2,\"641\":1,\"643\":2,\"651\":1,\"666\":1,\"670\":1,\"696\":1,\"697\":1,\"704\":1,\"756\":1,\"768\":1,\"773\":1,\"847\":1,\"866\":1,\"867\":1,\"978\":1,\"1008\":4,\"1009\":1,\"1209\":1,\"1224\":1,\"1225\":1,\"1306\":2,\"1368\":1,\"1371\":2,\"1441\":5,\"1478\":1,\"1533\":1,\"1647\":1,\"1655\":2,\"1675\":1,\"1684\":1,\"1685\":1,\"1688\":1,\"1749\":1,\"1750\":1,\"1759\":1,\"1815\":1,\"1843\":1,\"1860\":1,\"1861\":1,\"1878\":1,\"1885\":1,\"1921\":1,\"1993\":1,\"2001\":1,\"2043\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2224\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2312\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"gives\",{\"1\":{\"39\":1,\"81\":1,\"1679\":1}}],[\"git\",{\"1\":{\"26\":2,\"127\":1,\"161\":2,\"162\":2,\"260\":3,\"1385\":2}}],[\"github\",{\"1\":{\"1\":1,\"26\":1,\"34\":1,\"69\":1,\"125\":1,\"161\":2,\"162\":1,\"195\":4,\"201\":1,\"213\":1,\"242\":1,\"260\":1,\"268\":1,\"277\":1,\"287\":1,\"289\":1,\"290\":2,\"536\":2,\"615\":1,\"644\":1,\"669\":1,\"670\":1,\"674\":1,\"709\":1,\"747\":1,\"750\":1,\"774\":2,\"780\":1,\"790\":1,\"791\":1,\"864\":1,\"922\":1,\"936\":1,\"937\":1,\"1053\":2,\"1153\":1,\"1211\":2,\"1308\":1,\"1316\":1,\"1320\":1,\"1332\":1,\"1350\":1,\"1385\":2,\"1411\":1,\"1513\":1,\"1548\":1,\"1551\":1,\"1592\":1,\"1605\":1,\"1606\":1,\"1608\":1,\"1668\":1,\"1756\":1,\"1757\":1,\"1785\":1,\"1786\":1,\"1789\":1,\"1790\":1,\"1817\":1,\"1818\":1,\"1945\":1,\"1977\":1,\"2018\":1,\"2101\":1,\"2151\":1,\"2191\":1,\"2198\":1,\"2280\":1,\"2286\":2,\"2310\":1,\"2427\":1,\"2462\":1,\"2474\":1,\"2489\":1}}],[\"gov\",{\"1\":{\"1117\":1}}],[\"goes\",{\"1\":{\"927\":1,\"2307\":1,\"2355\":1}}],[\"google\",{\"0\":{\"518\":1},\"1\":{\"162\":1,\"248\":1,\"290\":1,\"518\":3,\"536\":1,\"1833\":1}}],[\"good\",{\"1\":{\"84\":1,\"213\":1,\"1246\":1,\"2188\":1}}],[\"going\",{\"1\":{\"76\":2,\"242\":1}}],[\"go\",{\"1\":{\"26\":1,\"92\":1,\"101\":1,\"107\":1,\"236\":1,\"243\":1,\"290\":1}}],[\"gain\",{\"1\":{\"1301\":1,\"1306\":2,\"1371\":2,\"1372\":1,\"1655\":1,\"1672\":5,\"1687\":3,\"2341\":1,\"2350\":1}}],[\"gain=1\",{\"1\":{\"1301\":1,\"1306\":1,\"1371\":1,\"1372\":1}}],[\"gaussianupsampling\",{\"0\":{\"1590\":1,\"1977\":1},\"1\":{\"1590\":1,\"1977\":1}}],[\"gaussian\",{\"0\":{\"2441\":1},\"1\":{\"1177\":1,\"1246\":1,\"1545\":1,\"1590\":1,\"1754\":1,\"1977\":2,\"2441\":2}}],[\"gaussianfourierprojection\",{\"0\":{\"1177\":1},\"1\":{\"1177\":1}}],[\"gatherable\",{\"0\":{\"2309\":1},\"1\":{\"2309\":2}}],[\"gathering\",{\"1\":{\"1951\":1}}],[\"gather\",{\"0\":{\"1473\":1,\"1474\":1},\"1\":{\"1473\":1,\"1474\":1}}],[\"gating\",{\"1\":{\"622\":1,\"713\":1,\"715\":1,\"780\":1,\"781\":1,\"783\":1}}],[\"gate=none\",{\"1\":{\"817\":1}}],[\"gates\",{\"1\":{\"809\":1}}],[\"gated\",{\"1\":{\"633\":1,\"819\":1,\"1180\":1,\"1181\":1,\"1610\":1,\"1619\":3,\"1621\":3,\"1628\":1}}],[\"gate\",{\"1\":{\"243\":1,\"700\":1,\"713\":1,\"715\":3,\"733\":1,\"734\":1,\"780\":2,\"781\":1,\"783\":3,\"819\":1,\"1610\":2,\"1628\":2}}],[\"gans\",{\"1\":{\"2355\":2}}],[\"gansvstask\",{\"0\":{\"2255\":1},\"1\":{\"2255\":1}}],[\"ganttstask\",{\"0\":{\"2256\":1},\"1\":{\"2256\":1}}],[\"gantraineroptions\",{\"0\":{\"2348\":1},\"1\":{\"2347\":2,\"2348\":1}}],[\"gantrainer\",{\"0\":{\"2347\":1},\"1\":{\"2254\":1,\"2255\":1,\"2256\":1,\"2347\":1,\"2348\":1}}],[\"gancodectask\",{\"0\":{\"2254\":1},\"1\":{\"2254\":1}}],[\"gan\",{\"0\":{\"368\":1,\"1381\":2,\"1382\":1,\"1383\":1,\"1385\":1,\"1386\":1,\"1387\":1,\"1389\":1,\"1390\":1,\"1391\":1,\"1392\":1,\"1394\":1,\"1395\":1,\"1396\":1,\"1397\":1,\"1398\":1,\"1400\":1,\"1401\":1,\"1402\":1,\"1403\":1,\"1404\":1,\"1406\":1,\"1408\":1,\"1409\":1,\"1410\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1432\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1466\":1,\"1467\":1,\"1468\":1,\"1469\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1486\":1,\"1487\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1505\":1,\"1506\":1,\"1507\":1,\"1508\":2,\"1509\":1,\"1511\":1,\"1513\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1519\":1,\"1520\":1,\"1521\":1,\"1522\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1529\":1,\"1530\":1,\"1532\":1,\"1533\":1,\"1534\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1548\":1,\"1549\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1556\":1,\"1557\":1,\"1558\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1562\":1,\"1563\":1,\"1564\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1574\":1,\"1575\":1,\"1576\":2,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1625\":1,\"1626\":1,\"1627\":1,\"1628\":1,\"1629\":1,\"1630\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1634\":1,\"1635\":1,\"1636\":1,\"1637\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2327\":1,\"2347\":1,\"2348\":1,\"2524\":1,\"2525\":1,\"2526\":1},\"1\":{\"220\":1,\"267\":3,\"286\":8,\"290\":1,\"368\":1,\"1381\":3,\"1382\":1,\"1383\":1,\"1385\":1,\"1386\":1,\"1387\":1,\"1389\":1,\"1390\":1,\"1391\":1,\"1392\":1,\"1394\":1,\"1395\":2,\"1396\":1,\"1397\":1,\"1398\":1,\"1400\":1,\"1401\":1,\"1402\":2,\"1403\":1,\"1404\":1,\"1406\":1,\"1408\":1,\"1409\":3,\"1410\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1432\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1466\":1,\"1467\":2,\"1468\":1,\"1469\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1486\":1,\"1487\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1505\":1,\"1506\":1,\"1507\":1,\"1508\":3,\"1509\":1,\"1511\":1,\"1513\":2,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1519\":1,\"1520\":1,\"1521\":2,\"1522\":1,\"1524\":1,\"1525\":1,\"1526\":2,\"1527\":1,\"1529\":1,\"1530\":1,\"1532\":1,\"1533\":1,\"1534\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1548\":2,\"1549\":1,\"1551\":2,\"1552\":1,\"1553\":1,\"1554\":1,\"1556\":1,\"1557\":1,\"1558\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1562\":1,\"1563\":1,\"1564\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1574\":1,\"1575\":1,\"1576\":3,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":2,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":2,\"1593\":2,\"1594\":3,\"1595\":4,\"1596\":1,\"1597\":2,\"1598\":2,\"1599\":1,\"1600\":2,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1625\":1,\"1626\":1,\"1627\":1,\"1628\":1,\"1629\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1634\":1,\"1635\":1,\"1636\":1,\"1637\":1,\"2254\":2,\"2255\":2,\"2256\":2,\"2327\":2,\"2347\":3,\"2348\":1,\"2371\":2}}],[\"ganesan\",{\"1\":{\"12\":1}}],[\"ga642381\",{\"1\":{\"201\":1}}],[\"gamma=1\",{\"1\":{\"1784\":1}}],[\"gamma=none\",{\"1\":{\"887\":1}}],[\"gamma=0\",{\"1\":{\"689\":1}}],[\"gamma\",{\"1\":{\"45\":1,\"145\":1,\"616\":2,\"696\":2,\"697\":2,\"807\":1,\"1784\":1,\"1920\":2,\"2014\":2,\"2021\":2}}],[\"gap\",{\"1\":{\"16\":1}}],[\"garcia\",{\"1\":{\"14\":1}}],[\"gao\",{\"1\":{\"14\":1,\"1176\":1}}],[\"gao2022euro\",{\"1\":{\"14\":1}}],[\"gumbel\",{\"1\":{\"2462\":2}}],[\"guaranteed\",{\"1\":{\"2016\":1,\"2380\":1}}],[\"guarantees\",{\"1\":{\"101\":1,\"1645\":1,\"1650\":1}}],[\"guard\",{\"1\":{\"1376\":1,\"1377\":1}}],[\"gu\",{\"1\":{\"1061\":1,\"1062\":1}}],[\"gut\",{\"1\":{\"287\":1}}],[\"guo\",{\"1\":{\"13\":1}}],[\"guidedmultiheadattentionloss\",{\"0\":{\"1771\":1},\"1\":{\"1771\":1}}],[\"guided\",{\"0\":{\"1990\":1},\"1\":{\"1770\":5,\"1771\":3,\"1990\":2,\"2245\":9,\"2431\":9,\"2432\":17}}],[\"guidedattentionloss\",{\"0\":{\"1770\":1},\"1\":{\"1770\":1,\"1771\":1}}],[\"guide\",{\"0\":{\"0\":1,\"3\":1,\"2516\":1},\"1\":{\"3\":1,\"67\":1,\"1011\":1}}],[\"g\",{\"1\":{\"3\":1,\"22\":2,\"24\":1,\"26\":1,\"40\":1,\"41\":2,\"43\":5,\"47\":7,\"49\":1,\"50\":2,\"60\":1,\"67\":3,\"71\":1,\"79\":2,\"80\":1,\"81\":1,\"82\":2,\"85\":1,\"86\":3,\"97\":2,\"102\":1,\"108\":2,\"117\":1,\"118\":1,\"119\":2,\"126\":1,\"127\":1,\"128\":1,\"134\":3,\"138\":1,\"153\":1,\"162\":3,\"163\":5,\"168\":7,\"175\":1,\"195\":3,\"197\":5,\"201\":1,\"220\":1,\"223\":1,\"224\":1,\"225\":2,\"233\":1,\"235\":1,\"240\":2,\"242\":4,\"243\":3,\"246\":2,\"263\":1,\"267\":3,\"269\":2,\"271\":2,\"273\":1,\"276\":3,\"278\":2,\"280\":2,\"286\":10,\"287\":28,\"288\":2,\"290\":2,\"515\":1,\"516\":1,\"517\":1,\"518\":1,\"520\":2,\"521\":1,\"522\":1,\"524\":1,\"525\":1,\"527\":1,\"535\":3,\"537\":1,\"691\":1,\"724\":2,\"725\":2,\"726\":1,\"727\":1,\"728\":2,\"729\":1,\"744\":1,\"756\":1,\"773\":1,\"786\":1,\"820\":1,\"821\":2,\"828\":2,\"829\":3,\"830\":2,\"859\":2,\"866\":2,\"867\":1,\"912\":2,\"921\":1,\"922\":1,\"943\":1,\"978\":2,\"1053\":2,\"1062\":1,\"1107\":1,\"1117\":2,\"1118\":1,\"1125\":1,\"1130\":2,\"1131\":1,\"1132\":1,\"1136\":1,\"1141\":1,\"1145\":1,\"1155\":1,\"1157\":1,\"1162\":1,\"1224\":1,\"1225\":1,\"1232\":1,\"1245\":2,\"1252\":1,\"1261\":1,\"1264\":1,\"1265\":1,\"1267\":1,\"1268\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1278\":1,\"1280\":1,\"1283\":1,\"1334\":2,\"1354\":2,\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1509\":2,\"1511\":2,\"1513\":2,\"1516\":2,\"1519\":1,\"1520\":1,\"1521\":1,\"1526\":1,\"1533\":1,\"1536\":1,\"1548\":1,\"1551\":2,\"1553\":3,\"1556\":2,\"1581\":2,\"1583\":2,\"1585\":1,\"1592\":4,\"1598\":1,\"1600\":1,\"1611\":2,\"1612\":2,\"1613\":2,\"1616\":2,\"1625\":1,\"1628\":2,\"1681\":1,\"1683\":1,\"1697\":1,\"1698\":1,\"1716\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"1854\":1,\"1856\":1,\"1862\":1,\"1876\":1,\"1878\":4,\"1883\":3,\"1949\":1,\"2000\":3,\"2001\":3,\"2006\":1,\"2007\":1,\"2039\":1,\"2044\":1,\"2049\":1,\"2130\":7,\"2133\":1,\"2136\":1,\"2137\":1,\"2184\":2,\"2188\":1,\"2240\":1,\"2249\":1,\"2253\":1,\"2311\":5,\"2327\":1,\"2345\":1,\"2354\":1,\"2355\":2,\"2359\":2,\"2443\":1}}],[\"odd\",{\"1\":{\"1668\":1}}],[\"ode\",{\"1\":{\"1224\":1,\"1225\":1,\"1245\":3,\"1253\":2,\"1254\":1}}],[\"odim\",{\"0\":{\"1893\":1},\"1\":{\"43\":6,\"48\":3,\"706\":4,\"1526\":4,\"1552\":2,\"1553\":4,\"1577\":3,\"1598\":3,\"1599\":2,\"1600\":3,\"1625\":3,\"1668\":1,\"1709\":3,\"1736\":2,\"1738\":3,\"1739\":3,\"1740\":3,\"1741\":3,\"1742\":3,\"1743\":3,\"1744\":3,\"1745\":4,\"1746\":3,\"1749\":2,\"1750\":7,\"1764\":3,\"1810\":3,\"1812\":2,\"1815\":2,\"1839\":3,\"1851\":2,\"1893\":1,\"1991\":3,\"1992\":2,\"1993\":5,\"1994\":1,\"1995\":1,\"2223\":5,\"2224\":1,\"2226\":3,\"2235\":5,\"2236\":5,\"2237\":3,\"2239\":4,\"2240\":4,\"2241\":3,\"2245\":4,\"2411\":4,\"2412\":4,\"2413\":3,\"2423\":4,\"2424\":3,\"2429\":1,\"2431\":4,\"2432\":4,\"2447\":3,\"2448\":2}}],[\"ovr\",{\"1\":{\"1128\":1}}],[\"overtones\",{\"1\":{\"1545\":1}}],[\"overriding\",{\"1\":{\"699\":2,\"760\":2}}],[\"overrides\",{\"1\":{\"760\":1,\"911\":1,\"2480\":1}}],[\"override\",{\"1\":{\"699\":1,\"756\":1,\"760\":1,\"773\":1,\"912\":1,\"1029\":1,\"1061\":1,\"1062\":1,\"1064\":1,\"1235\":1,\"1262\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1290\":1,\"2130\":1,\"2355\":1,\"2369\":1}}],[\"overridden\",{\"1\":{\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"693\":1,\"713\":1,\"718\":1,\"720\":1,\"722\":1,\"738\":1,\"741\":1,\"752\":1,\"756\":2,\"757\":1,\"773\":2,\"778\":1,\"781\":1,\"788\":1,\"791\":1,\"796\":1,\"798\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"821\":1,\"825\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"842\":1,\"844\":1,\"852\":1,\"854\":1,\"856\":1,\"860\":1,\"862\":1,\"864\":1,\"866\":1,\"867\":1,\"950\":1,\"952\":1,\"956\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1066\":1,\"1068\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1114\":1,\"1120\":1,\"1122\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1145\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1159\":1,\"1165\":1,\"1168\":1,\"1177\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1200\":1,\"1202\":1,\"1205\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1219\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1248\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1262\":1,\"1265\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1383\":1,\"1387\":1,\"1392\":1,\"1398\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1509\":1,\"1511\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1549\":1,\"1554\":1,\"1638\":1,\"1652\":1,\"1657\":1,\"1662\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1945\":1,\"1957\":1,\"1967\":1,\"1969\":1,\"1972\":1,\"1975\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2185\":1,\"2188\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2203\":1,\"2205\":1,\"2209\":1,\"2211\":1,\"2216\":1,\"2305\":1,\"2325\":1,\"2401\":1,\"2405\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2434\":1,\"2443\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2465\":1,\"2467\":1}}],[\"overlaps\",{\"1\":{\"2220\":1}}],[\"overlapping\",{\"1\":{\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"1316\":1,\"1320\":1,\"1350\":1,\"1374\":1}}],[\"overlap=false\",{\"1\":{\"1259\":1}}],[\"overlap\",{\"0\":{\"1350\":1},\"1\":{\"674\":4,\"846\":9,\"1259\":2,\"1261\":3,\"1350\":3,\"1569\":1,\"2220\":10}}],[\"overfit\",{\"1\":{\"243\":1,\"747\":1}}],[\"overal\",{\"1\":{\"144\":1}}],[\"overall\",{\"1\":{\"44\":1,\"285\":1}}],[\"overwrite\",{\"1\":{\"47\":1,\"124\":1}}],[\"overwritten\",{\"1\":{\"47\":1,\"804\":1,\"932\":1,\"934\":1,\"1409\":1,\"1593\":1,\"1595\":1}}],[\"overview\",{\"1\":{\"46\":1}}],[\"over\",{\"1\":{\"26\":1,\"67\":1,\"133\":1,\"211\":1,\"217\":1,\"233\":1,\"262\":3,\"266\":1,\"275\":1,\"285\":1,\"770\":1,\"786\":1,\"787\":2,\"800\":1,\"831\":1,\"846\":2,\"852\":1,\"878\":1,\"879\":1,\"881\":1,\"921\":1,\"935\":1,\"959\":1,\"1145\":2,\"1168\":1,\"1292\":1,\"1400\":1,\"1413\":1,\"1441\":1,\"1469\":1,\"1645\":2,\"1918\":1,\"2001\":1,\"2018\":2,\"2134\":1,\"2139\":3,\"2144\":3,\"2184\":1,\"2235\":1,\"2236\":1,\"2367\":1}}],[\"ohio\",{\"1\":{\"1125\":1}}],[\"oom\",{\"1\":{\"777\":2,\"1156\":2,\"1940\":2,\"1942\":2,\"2039\":1}}],[\"omniinference\",{\"0\":{\"2057\":1},\"1\":{\"2054\":2}}],[\"omni\",{\"0\":{\"2022\":1,\"2023\":1,\"2024\":1,\"2025\":1,\"2036\":1,\"2037\":1,\"2038\":1,\"2041\":1,\"2042\":1,\"2046\":1,\"2047\":1,\"2048\":1,\"2050\":1,\"2051\":1,\"2052\":1,\"2053\":1,\"2054\":1,\"2057\":1,\"2058\":1,\"2059\":1,\"2060\":1,\"2061\":1,\"2062\":1,\"2063\":1,\"2064\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2070\":1,\"2071\":1,\"2072\":1,\"2073\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2078\":1,\"2079\":1,\"2080\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2085\":1,\"2086\":1,\"2087\":1,\"2088\":1,\"2089\":1,\"2090\":1,\"2091\":1,\"2092\":1,\"2093\":1,\"2094\":1,\"2095\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2119\":1,\"2120\":1,\"2121\":1,\"2122\":1,\"2123\":1},\"1\":{\"2044\":1,\"2054\":2,\"2061\":1,\"2088\":1,\"2095\":1,\"2096\":1,\"2102\":1,\"2115\":1,\"2116\":1,\"2133\":2}}],[\"omegaconf\",{\"0\":{\"925\":1},\"1\":{\"925\":1}}],[\"omp\",{\"1\":{\"703\":1,\"755\":1,\"785\":1}}],[\"omitted\",{\"1\":{\"44\":1,\"175\":1}}],[\"oː\",{\"1\":{\"287\":2}}],[\"octaves\",{\"1\":{\"1692\":2}}],[\"octave\",{\"1\":{\"1692\":5,\"2434\":1,\"2435\":2}}],[\"oc\",{\"0\":{\"949\":1},\"1\":{\"949\":1}}],[\"occurs\",{\"1\":{\"269\":1,\"278\":1,\"290\":1,\"699\":1,\"705\":1,\"804\":1,\"932\":1,\"934\":1}}],[\"ochiai17a\",{\"1\":{\"1126\":2}}],[\"ochiai\",{\"1\":{\"156\":1,\"1126\":1}}],[\"othger\",{\"1\":{\"141\":1}}],[\"other\",{\"0\":{\"160\":1,\"182\":1,\"232\":1,\"258\":1,\"819\":1},\"1\":{\"57\":1,\"71\":1,\"81\":1,\"84\":1,\"96\":2,\"108\":1,\"119\":1,\"128\":1,\"135\":1,\"137\":1,\"138\":2,\"144\":1,\"145\":1,\"150\":1,\"162\":1,\"197\":1,\"220\":1,\"223\":5,\"224\":3,\"232\":2,\"252\":1,\"258\":2,\"269\":2,\"278\":2,\"289\":1,\"290\":1,\"536\":1,\"674\":6,\"691\":1,\"692\":1,\"696\":1,\"704\":1,\"747\":2,\"756\":1,\"773\":1,\"775\":1,\"790\":1,\"819\":1,\"820\":1,\"846\":5,\"850\":1,\"866\":1,\"978\":1,\"1053\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1131\":1,\"1132\":1,\"1136\":1,\"1141\":1,\"1156\":1,\"1162\":1,\"1217\":2,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1278\":1,\"1280\":1,\"1283\":1,\"1334\":2,\"1402\":1,\"1467\":1,\"1594\":1,\"1595\":1,\"1678\":1,\"2000\":1,\"2001\":1,\"2143\":1,\"2220\":4,\"2276\":1,\"2277\":1,\"2304\":1,\"2325\":1,\"2380\":1,\"2474\":1}}],[\"otherwise\",{\"1\":{\"43\":1,\"148\":1,\"196\":1,\"213\":1,\"224\":1,\"225\":1,\"268\":1,\"277\":1,\"699\":1,\"710\":1,\"711\":1,\"1080\":1,\"1484\":1,\"2144\":1}}],[\"others=\",{\"1\":{\"1132\":1,\"1167\":1,\"1209\":1,\"1228\":1}}],[\"others\",{\"1\":{\"6\":2,\"244\":1,\"794\":1,\"978\":1,\"1042\":1,\"1053\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1125\":1,\"1130\":2,\"1131\":1,\"1132\":2,\"1136\":1,\"1141\":1,\"1157\":1,\"1158\":1,\"1162\":1,\"1167\":1,\"1204\":2,\"1209\":1,\"1228\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1268\":2,\"1278\":1,\"1280\":1,\"1283\":1,\"2143\":1}}],[\"otpions\",{\"1\":{\"118\":1}}],[\"observe\",{\"1\":{\"2359\":2}}],[\"observed\",{\"1\":{\"1225\":1,\"1311\":2}}],[\"observation\",{\"1\":{\"1126\":2,\"1311\":1,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1}}],[\"obj\",{\"0\":{\"898\":1},\"1\":{\"898\":2,\"1006\":2,\"1018\":2,\"2318\":1,\"2320\":1,\"2334\":2,\"2489\":1}}],[\"object>\",{\"1\":{\"2246\":9,\"2247\":4,\"2248\":6,\"2250\":7,\"2251\":7,\"2252\":24,\"2253\":6,\"2254\":1,\"2255\":10,\"2256\":7,\"2257\":6,\"2259\":12,\"2260\":2,\"2261\":7,\"2262\":1,\"2263\":14,\"2264\":7,\"2265\":10,\"2266\":6,\"2267\":13,\"2268\":10,\"2269\":8,\"2270\":6,\"2271\":7,\"2272\":4,\"2273\":5}}],[\"objects\",{\"1\":{\"1938\":1,\"2156\":1,\"2325\":1,\"2334\":1,\"2344\":1,\"2489\":1}}],[\"objectives\",{\"1\":{\"2216\":2}}],[\"objective\",{\"1\":{\"262\":2,\"265\":1,\"266\":2,\"267\":3,\"274\":1,\"275\":2,\"276\":3,\"285\":1,\"286\":3,\"2215\":1}}],[\"object\",{\"1\":{\"78\":1,\"79\":2,\"86\":1,\"106\":1,\"132\":1,\"615\":1,\"616\":2,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":1,\"627\":2,\"630\":1,\"631\":1,\"632\":1,\"633\":1,\"634\":1,\"636\":1,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"651\":1,\"674\":1,\"696\":1,\"697\":1,\"698\":1,\"699\":1,\"703\":1,\"716\":1,\"717\":1,\"740\":1,\"755\":1,\"756\":1,\"763\":1,\"773\":1,\"784\":1,\"942\":1,\"943\":1,\"960\":1,\"985\":1,\"992\":1,\"994\":1,\"995\":1,\"997\":1,\"998\":1,\"1000\":1,\"1002\":1,\"1004\":1,\"1005\":1,\"1008\":2,\"1009\":1,\"1012\":1,\"1013\":1,\"1016\":1,\"1017\":1,\"1022\":1,\"1026\":1,\"1128\":1,\"1129\":1,\"1382\":1,\"1394\":1,\"1432\":1,\"1532\":1,\"1654\":1,\"1655\":1,\"1666\":1,\"1703\":1,\"1717\":1,\"1718\":1,\"1726\":1,\"1727\":1,\"1728\":1,\"1729\":1,\"1730\":1,\"1732\":1,\"1734\":1,\"1735\":1,\"1736\":2,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1749\":1,\"1751\":1,\"1759\":1,\"1760\":2,\"1761\":1,\"1763\":1,\"1772\":1,\"1775\":1,\"1776\":1,\"1778\":1,\"1780\":1,\"1782\":1,\"1783\":1,\"1785\":1,\"1791\":1,\"1794\":1,\"1797\":1,\"1802\":1,\"1808\":1,\"1809\":1,\"1813\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1821\":1,\"1823\":1,\"1824\":1,\"1825\":1,\"1828\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1847\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1853\":1,\"1948\":1,\"2039\":2,\"2061\":1,\"2130\":2,\"2131\":4,\"2134\":1,\"2139\":1,\"2143\":1,\"2144\":1,\"2150\":1,\"2164\":1,\"2246\":1,\"2247\":6,\"2248\":1,\"2249\":4,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":3,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2276\":1,\"2277\":1,\"2278\":1,\"2280\":1,\"2281\":1,\"2282\":1,\"2283\":1,\"2286\":1,\"2289\":1,\"2291\":1,\"2309\":2,\"2311\":3,\"2323\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2334\":2,\"2335\":1,\"2339\":1,\"2340\":1,\"2344\":2,\"2345\":1,\"2349\":1,\"2358\":1,\"2359\":1,\"2367\":1,\"2369\":1,\"2370\":1,\"2477\":3,\"2478\":1,\"2482\":1}}],[\"obtain\",{\"1\":{\"232\":1,\"259\":1,\"1324\":1,\"1654\":1,\"1666\":1,\"1668\":1,\"1720\":2,\"1721\":2,\"1725\":2,\"1729\":1,\"1806\":2,\"1862\":2,\"1900\":1,\"1923\":1}}],[\"obtained\",{\"1\":{\"37\":1,\"70\":1,\"108\":1,\"110\":1,\"1330\":1,\"1599\":1,\"1668\":1}}],[\"ogg\",{\"1\":{\"71\":1,\"1678\":1}}],[\"ogawa\",{\"1\":{\"15\":1}}],[\"olens\",{\"1\":{\"692\":1,\"760\":1,\"775\":1,\"790\":1,\"820\":1,\"850\":1,\"976\":1,\"1589\":2,\"1764\":2,\"1770\":2,\"1771\":2,\"1839\":2,\"1990\":1,\"1991\":2,\"1993\":1,\"2226\":2,\"2237\":2,\"2241\":2,\"2413\":2,\"2424\":2,\"2448\":2}}],[\"olen\",{\"1\":{\"48\":3,\"706\":1}}],[\"oldest\",{\"1\":{\"2039\":1}}],[\"older\",{\"1\":{\"26\":1,\"36\":1}}],[\"olddatadir\",{\"1\":{\"529\":1}}],[\"old\",{\"0\":{\"2106\":1},\"1\":{\"48\":1,\"1695\":1,\"1785\":1,\"1786\":1,\"1916\":2}}],[\"o\",{\"1\":{\"47\":2,\"126\":2,\"243\":2,\"271\":2,\"280\":2,\"287\":12,\"295\":1,\"415\":1,\"545\":1,\"667\":1,\"701\":2,\"735\":2,\"1730\":2,\"2130\":2,\"2133\":1,\"2136\":2,\"2137\":1,\"2299\":1}}],[\"ouptut\",{\"1\":{\"2192\":1,\"2203\":1,\"2209\":1}}],[\"ouvpsde\",{\"0\":{\"1225\":1},\"1\":{\"1225\":2}}],[\"ouvesde\",{\"0\":{\"1224\":1},\"1\":{\"1224\":1}}],[\"ou\",{\"1\":{\"1050\":1,\"1116\":1,\"1161\":1,\"1189\":1,\"1218\":1,\"1221\":1,\"1229\":1,\"1244\":1}}],[\"ourselves\",{\"1\":{\"243\":1}}],[\"our\",{\"1\":{\"34\":1,\"39\":1,\"42\":1,\"69\":1,\"70\":3,\"79\":2,\"82\":1,\"91\":1,\"97\":1,\"104\":1,\"125\":1,\"138\":1,\"146\":1,\"152\":2,\"160\":1,\"165\":1,\"168\":1,\"195\":1,\"223\":1,\"224\":1,\"240\":1,\"243\":1,\"247\":1,\"261\":1,\"262\":1,\"536\":1,\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2325\":1}}],[\"outs\",{\"1\":{\"1627\":6,\"1764\":6,\"1839\":4,\"1991\":4,\"1993\":2,\"2226\":8,\"2237\":4,\"2241\":10,\"2413\":10,\"2424\":10,\"2448\":10}}],[\"outside\",{\"1\":{\"22\":1,\"696\":1,\"697\":1,\"1306\":1,\"1371\":1,\"1833\":1}}],[\"outchannels\",{\"1\":{\"1301\":1,\"1372\":1}}],[\"outlined\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1}}],[\"outlines\",{\"1\":{\"0\":1}}],[\"out=1\",{\"1\":{\"768\":1}}],[\"outyaml\",{\"1\":{\"545\":1}}],[\"outplanes\",{\"1\":{\"893\":1,\"894\":1}}],[\"outpath\",{\"1\":{\"403\":28,\"1954\":1,\"1955\":1}}],[\"output1\",{\"1\":{\"2044\":2}}],[\"outputdim\",{\"1\":{\"786\":1,\"800\":1,\"867\":1,\"921\":1,\"935\":1}}],[\"output=none\",{\"1\":{\"726\":1,\"744\":1}}],[\"outputs\",{\"1\":{\"126\":1,\"141\":2,\"211\":1,\"235\":1,\"246\":1,\"247\":1,\"284\":2,\"290\":5,\"429\":2,\"514\":7,\"625\":2,\"626\":3,\"706\":1,\"750\":1,\"756\":2,\"773\":2,\"864\":1,\"866\":2,\"867\":2,\"960\":1,\"961\":2,\"980\":1,\"1061\":1,\"1165\":1,\"1389\":2,\"1390\":1,\"1396\":2,\"1397\":1,\"1401\":2,\"1402\":1,\"1408\":2,\"1409\":1,\"1466\":2,\"1467\":1,\"1521\":1,\"1526\":4,\"1553\":4,\"1584\":10,\"1587\":7,\"1591\":5,\"1593\":1,\"1594\":1,\"1595\":1,\"1598\":4,\"1599\":1,\"1600\":4,\"1606\":1,\"1618\":1,\"1625\":4,\"1627\":3,\"1628\":2,\"1638\":1,\"1709\":1,\"1750\":5,\"1753\":2,\"1754\":2,\"1755\":1,\"1764\":3,\"1810\":1,\"1812\":1,\"1839\":2,\"1928\":2,\"1950\":1,\"1971\":1,\"1974\":2,\"1977\":4,\"1991\":2,\"1992\":4,\"1993\":10,\"1995\":4,\"2044\":4,\"2222\":1,\"2223\":3,\"2224\":2,\"2226\":4,\"2228\":2,\"2229\":2,\"2235\":2,\"2236\":2,\"2237\":2,\"2239\":2,\"2240\":2,\"2241\":5,\"2245\":5,\"2355\":2,\"2403\":1,\"2408\":2,\"2411\":2,\"2412\":2,\"2413\":5,\"2423\":2,\"2424\":5,\"2427\":2,\"2431\":5,\"2432\":2,\"2445\":1,\"2446\":2,\"2447\":2,\"2448\":5}}],[\"output\",{\"0\":{\"300\":1,\"345\":1,\"352\":1,\"364\":1,\"420\":1,\"1300\":1,\"1302\":1,\"1936\":1},\"1\":{\"3\":1,\"43\":8,\"44\":2,\"45\":1,\"46\":1,\"47\":1,\"48\":3,\"69\":4,\"71\":3,\"73\":1,\"74\":2,\"76\":5,\"126\":1,\"128\":2,\"141\":10,\"142\":1,\"162\":2,\"175\":2,\"199\":1,\"200\":2,\"204\":1,\"205\":2,\"211\":3,\"217\":1,\"242\":1,\"243\":1,\"247\":3,\"254\":1,\"262\":2,\"266\":1,\"269\":1,\"275\":1,\"278\":1,\"284\":1,\"285\":1,\"290\":1,\"293\":2,\"295\":1,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":4,\"335\":4,\"342\":6,\"349\":4,\"356\":2,\"361\":6,\"368\":2,\"372\":2,\"377\":2,\"385\":2,\"389\":2,\"396\":2,\"404\":2,\"406\":2,\"415\":1,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":2,\"457\":2,\"461\":2,\"463\":2,\"469\":2,\"475\":2,\"481\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":2,\"505\":2,\"511\":2,\"514\":2,\"521\":1,\"538\":1,\"614\":3,\"615\":1,\"616\":4,\"617\":3,\"618\":3,\"619\":2,\"620\":10,\"621\":5,\"622\":1,\"623\":1,\"624\":3,\"628\":1,\"630\":2,\"632\":8,\"633\":2,\"634\":4,\"636\":5,\"637\":2,\"638\":2,\"640\":1,\"641\":4,\"642\":2,\"643\":5,\"644\":3,\"648\":1,\"649\":3,\"651\":4,\"654\":3,\"655\":3,\"662\":1,\"665\":3,\"667\":1,\"671\":2,\"672\":5,\"673\":2,\"674\":2,\"675\":2,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"691\":3,\"692\":14,\"696\":8,\"697\":7,\"699\":2,\"700\":5,\"701\":1,\"702\":3,\"706\":2,\"709\":7,\"710\":4,\"711\":4,\"721\":1,\"724\":3,\"725\":3,\"726\":1,\"727\":2,\"728\":3,\"729\":2,\"731\":2,\"732\":2,\"733\":5,\"734\":5,\"735\":1,\"737\":1,\"738\":4,\"739\":1,\"744\":1,\"745\":3,\"746\":1,\"747\":4,\"748\":3,\"753\":1,\"754\":1,\"756\":5,\"759\":3,\"760\":5,\"761\":2,\"762\":2,\"765\":3,\"766\":2,\"767\":2,\"768\":6,\"769\":1,\"770\":4,\"771\":3,\"772\":3,\"773\":6,\"774\":7,\"775\":3,\"779\":2,\"780\":7,\"784\":1,\"786\":4,\"790\":4,\"792\":1,\"796\":1,\"798\":3,\"799\":1,\"800\":4,\"803\":2,\"804\":2,\"809\":1,\"816\":1,\"819\":2,\"820\":5,\"823\":1,\"824\":1,\"828\":5,\"829\":7,\"830\":5,\"831\":5,\"846\":2,\"847\":5,\"848\":2,\"849\":5,\"850\":5,\"856\":1,\"859\":4,\"860\":1,\"862\":3,\"863\":1,\"865\":1,\"866\":3,\"867\":5,\"921\":4,\"932\":2,\"934\":2,\"935\":4,\"947\":2,\"949\":2,\"955\":1,\"959\":4,\"972\":1,\"974\":1,\"976\":1,\"977\":1,\"979\":1,\"980\":1,\"986\":2,\"1008\":1,\"1009\":5,\"1029\":3,\"1031\":2,\"1035\":1,\"1053\":3,\"1059\":1,\"1061\":1,\"1063\":1,\"1070\":2,\"1071\":2,\"1073\":2,\"1075\":1,\"1082\":1,\"1084\":1,\"1093\":1,\"1103\":1,\"1107\":2,\"1112\":2,\"1113\":1,\"1114\":1,\"1118\":2,\"1119\":1,\"1120\":2,\"1124\":4,\"1125\":3,\"1130\":1,\"1133\":7,\"1134\":3,\"1136\":1,\"1137\":3,\"1139\":4,\"1141\":2,\"1145\":1,\"1147\":5,\"1155\":1,\"1157\":5,\"1162\":1,\"1165\":1,\"1180\":2,\"1181\":5,\"1184\":1,\"1185\":1,\"1198\":2,\"1200\":1,\"1208\":2,\"1209\":1,\"1211\":1,\"1217\":1,\"1223\":1,\"1228\":1,\"1232\":1,\"1233\":1,\"1235\":3,\"1236\":1,\"1246\":1,\"1247\":1,\"1250\":3,\"1251\":1,\"1252\":2,\"1259\":3,\"1264\":1,\"1265\":1,\"1267\":1,\"1269\":2,\"1270\":2,\"1271\":5,\"1273\":1,\"1274\":1,\"1278\":2,\"1279\":7,\"1280\":1,\"1281\":7,\"1282\":6,\"1283\":1,\"1286\":1,\"1290\":1,\"1300\":1,\"1302\":1,\"1334\":5,\"1350\":4,\"1385\":4,\"1386\":2,\"1390\":1,\"1392\":1,\"1397\":2,\"1400\":1,\"1402\":1,\"1409\":1,\"1422\":1,\"1453\":1,\"1458\":1,\"1460\":1,\"1467\":1,\"1484\":1,\"1513\":2,\"1514\":2,\"1515\":1,\"1516\":1,\"1519\":3,\"1523\":1,\"1525\":1,\"1526\":1,\"1529\":5,\"1536\":3,\"1540\":1,\"1545\":2,\"1548\":3,\"1551\":2,\"1553\":1,\"1559\":2,\"1581\":1,\"1583\":1,\"1586\":1,\"1587\":1,\"1592\":3,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":3,\"1598\":1,\"1599\":3,\"1600\":1,\"1603\":1,\"1604\":2,\"1605\":3,\"1606\":2,\"1608\":2,\"1609\":3,\"1610\":3,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1619\":3,\"1620\":1,\"1621\":1,\"1625\":1,\"1628\":2,\"1660\":1,\"1668\":2,\"1669\":1,\"1709\":1,\"1719\":1,\"1720\":3,\"1721\":3,\"1725\":5,\"1726\":2,\"1727\":2,\"1735\":3,\"1736\":7,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1749\":7,\"1750\":6,\"1751\":3,\"1756\":1,\"1757\":1,\"1759\":2,\"1770\":1,\"1771\":1,\"1779\":12,\"1783\":1,\"1785\":2,\"1789\":1,\"1790\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1806\":4,\"1810\":1,\"1812\":1,\"1815\":5,\"1817\":2,\"1843\":2,\"1847\":4,\"1851\":5,\"1862\":2,\"1863\":1,\"1893\":3,\"1913\":1,\"1934\":1,\"1936\":1,\"1937\":1,\"1949\":2,\"1951\":1,\"1958\":2,\"1959\":1,\"1960\":4,\"1961\":4,\"1971\":1,\"1973\":1,\"1977\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1984\":1,\"1992\":9,\"1993\":5,\"1995\":7,\"2044\":10,\"2054\":1,\"2065\":3,\"2095\":1,\"2115\":1,\"2116\":2,\"2124\":1,\"2125\":1,\"2126\":3,\"2128\":3,\"2129\":4,\"2130\":1,\"2133\":1,\"2139\":1,\"2143\":1,\"2157\":2,\"2161\":1,\"2167\":1,\"2168\":1,\"2173\":1,\"2175\":1,\"2176\":1,\"2183\":2,\"2187\":6,\"2189\":1,\"2190\":1,\"2191\":5,\"2192\":2,\"2193\":1,\"2194\":1,\"2195\":1,\"2199\":1,\"2202\":1,\"2203\":2,\"2204\":1,\"2205\":1,\"2206\":1,\"2207\":1,\"2208\":1,\"2209\":2,\"2210\":1,\"2211\":1,\"2212\":1,\"2213\":1,\"2214\":1,\"2215\":4,\"2216\":1,\"2217\":1,\"2218\":1,\"2219\":7,\"2220\":1,\"2222\":1,\"2223\":4,\"2224\":2,\"2232\":3,\"2235\":2,\"2236\":2,\"2238\":2,\"2239\":4,\"2240\":4,\"2245\":5,\"2246\":3,\"2248\":3,\"2249\":3,\"2250\":3,\"2251\":3,\"2252\":3,\"2253\":3,\"2254\":3,\"2255\":3,\"2256\":3,\"2257\":3,\"2259\":3,\"2260\":3,\"2261\":3,\"2262\":1,\"2263\":3,\"2264\":3,\"2265\":3,\"2266\":3,\"2267\":3,\"2268\":3,\"2269\":3,\"2270\":3,\"2271\":3,\"2272\":3,\"2273\":3,\"2333\":1,\"2338\":1,\"2339\":2,\"2343\":2,\"2348\":1,\"2352\":2,\"2354\":2,\"2355\":2,\"2359\":1,\"2365\":1,\"2369\":1,\"2370\":2,\"2372\":1,\"2402\":1,\"2403\":1,\"2406\":1,\"2410\":1,\"2411\":4,\"2412\":4,\"2415\":1,\"2417\":1,\"2419\":1,\"2420\":1,\"2423\":4,\"2426\":1,\"2427\":3,\"2428\":3,\"2431\":5,\"2432\":5,\"2435\":1,\"2445\":1,\"2447\":7,\"2454\":1,\"2460\":1,\"2461\":1,\"2472\":1,\"2473\":1,\"2480\":2}}],[\"outdir\",{\"1\":{\"126\":1,\"520\":2,\"545\":2,\"558\":1,\"575\":1,\"998\":1,\"1005\":1,\"1009\":2,\"1017\":1,\"1830\":1}}],[\"outdir=whisper\",{\"1\":{\"126\":1}}],[\"outdated\",{\"1\":{\"36\":1}}],[\"out\",{\"0\":{\"2098\":1},\"1\":{\"3\":2,\"36\":1,\"48\":3,\"141\":1,\"195\":1,\"213\":1,\"269\":1,\"278\":1,\"541\":2,\"561\":1,\"567\":1,\"570\":1,\"592\":1,\"596\":2,\"614\":2,\"616\":7,\"620\":4,\"621\":1,\"625\":2,\"626\":2,\"628\":3,\"632\":7,\"634\":3,\"636\":1,\"637\":1,\"641\":3,\"643\":3,\"651\":1,\"654\":1,\"661\":1,\"667\":2,\"692\":11,\"696\":14,\"697\":12,\"712\":3,\"715\":1,\"743\":2,\"760\":3,\"768\":10,\"775\":3,\"776\":1,\"777\":8,\"783\":1,\"790\":7,\"795\":1,\"820\":3,\"847\":5,\"850\":7,\"885\":1,\"973\":1,\"981\":1,\"1061\":1,\"1080\":2,\"1082\":2,\"1108\":1,\"1110\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1145\":2,\"1147\":5,\"1148\":3,\"1156\":8,\"1168\":1,\"1176\":1,\"1180\":6,\"1181\":6,\"1182\":1,\"1183\":1,\"1184\":1,\"1205\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1264\":1,\"1265\":2,\"1272\":1,\"1273\":2,\"1274\":2,\"1302\":1,\"1303\":1,\"1304\":1,\"1334\":1,\"1346\":1,\"1347\":1,\"1368\":2,\"1373\":1,\"1383\":1,\"1386\":2,\"1389\":1,\"1390\":1,\"1391\":1,\"1392\":2,\"1396\":1,\"1397\":2,\"1401\":2,\"1402\":2,\"1403\":1,\"1404\":1,\"1408\":3,\"1409\":3,\"1410\":1,\"1420\":1,\"1422\":2,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":2,\"1452\":1,\"1466\":1,\"1467\":1,\"1468\":1,\"1484\":2,\"1513\":3,\"1515\":2,\"1516\":2,\"1517\":1,\"1519\":2,\"1526\":3,\"1530\":1,\"1536\":3,\"1537\":1,\"1548\":4,\"1549\":2,\"1551\":3,\"1553\":2,\"1579\":1,\"1592\":4,\"1593\":1,\"1594\":1,\"1595\":2,\"1596\":2,\"1597\":2,\"1598\":3,\"1599\":2,\"1600\":3,\"1604\":2,\"1605\":3,\"1606\":2,\"1609\":2,\"1610\":4,\"1611\":5,\"1618\":1,\"1619\":4,\"1625\":2,\"1628\":3,\"1668\":4,\"1709\":2,\"1733\":2,\"1736\":2,\"1749\":3,\"1751\":5,\"1762\":2,\"1770\":1,\"1771\":1,\"1774\":1,\"1779\":6,\"1796\":1,\"1815\":5,\"1827\":1,\"1830\":1,\"1843\":4,\"1851\":2,\"1856\":1,\"1863\":1,\"1883\":10,\"1893\":2,\"1912\":4,\"1917\":1,\"1956\":2,\"1992\":4,\"2239\":2,\"2249\":1,\"2253\":1,\"2355\":9}}],[\"osize\",{\"1\":{\"671\":1}}],[\"os\",{\"1\":{\"27\":1,\"161\":1}}],[\"ow1\",{\"1\":{\"271\":2,\"280\":2,\"287\":4}}],[\"owsmmodel\",{\"0\":{\"2056\":1},\"1\":{\"2056\":1}}],[\"owsmctcmodel\",{\"0\":{\"2055\":1},\"1\":{\"2055\":1}}],[\"owsm\",{\"0\":{\"2055\":1,\"2056\":1},\"1\":{\"6\":4,\"162\":1,\"184\":2,\"185\":3,\"186\":2,\"240\":4,\"242\":1,\"243\":15,\"1996\":1,\"2055\":5,\"2056\":5,\"2360\":1}}],[\"own\",{\"0\":{\"195\":1},\"1\":{\"1\":1,\"26\":1,\"31\":1,\"141\":1,\"160\":1,\"194\":1,\"197\":1,\"243\":1,\"261\":1,\"267\":4,\"276\":3,\"284\":2,\"290\":2,\"1064\":1,\"1078\":1,\"1153\":1,\"1202\":1,\"1262\":1,\"1290\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"2232\":1,\"2238\":1,\"2355\":2}}],[\"op=<redoptype\",{\"1\":{\"1475\":1}}],[\"ops\",{\"1\":{\"1301\":1,\"1350\":2,\"1372\":1}}],[\"op\",{\"0\":{\"764\":1,\"802\":1,\"2318\":1,\"2319\":1,\"2320\":1},\"1\":{\"705\":1,\"764\":1,\"787\":1,\"802\":1,\"803\":2,\"804\":2,\"1301\":1,\"1372\":1,\"1509\":3,\"1511\":3,\"1516\":3,\"1553\":3,\"2318\":1,\"2319\":1,\"2320\":1}}],[\"opid\",{\"1\":{\"704\":1,\"705\":1,\"803\":2,\"804\":2}}],[\"opt2\",{\"1\":{\"2355\":2}}],[\"opt1\",{\"1\":{\"2355\":2}}],[\"opt=none\",{\"1\":{\"2246\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2}}],[\"opt\",{\"0\":{\"1944\":1},\"1\":{\"1701\":1,\"1944\":2,\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1}}],[\"opts\",{\"1\":{\"168\":2,\"516\":1,\"520\":1,\"521\":1,\"523\":1,\"524\":1,\"525\":1,\"535\":1,\"2249\":1}}],[\"optimmodule\",{\"0\":{\"793\":1},\"1\":{\"793\":1,\"823\":1,\"824\":1}}],[\"optimization\",{\"1\":{\"2355\":11}}],[\"optimizations\",{\"1\":{\"145\":1}}],[\"optimized\",{\"1\":{\"1608\":1}}],[\"optimizers\",{\"0\":{\"1962\":1,\"1963\":1,\"1964\":1,\"2535\":1},\"1\":{\"1962\":1,\"1963\":1,\"1964\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":2,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":2,\"2347\":1,\"2355\":10,\"2369\":7,\"2371\":1}}],[\"optimizer\",{\"0\":{\"1963\":1,\"1964\":1,\"2099\":1,\"2100\":1},\"1\":{\"84\":1,\"87\":1,\"102\":1,\"218\":1,\"267\":1,\"276\":1,\"286\":1,\"793\":1,\"821\":2,\"878\":1,\"879\":1,\"882\":1,\"883\":1,\"912\":1,\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1521\":1,\"1526\":1,\"1553\":1,\"1585\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1962\":1,\"1963\":1,\"1964\":1,\"2014\":5,\"2015\":5,\"2016\":2,\"2017\":2,\"2018\":4,\"2019\":5,\"2020\":7,\"2021\":6,\"2249\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2273\":1,\"2327\":3,\"2347\":1,\"2355\":7,\"2369\":4,\"2371\":1,\"2432\":1}}],[\"optimize\",{\"1\":{\"44\":1}}],[\"optim\",{\"0\":{\"1963\":1,\"1964\":1},\"1\":{\"84\":3,\"86\":4,\"102\":2,\"113\":2,\"119\":2,\"136\":3,\"243\":2,\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1521\":1,\"1526\":1,\"1553\":1,\"1585\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1962\":1,\"1963\":1,\"1964\":3,\"2018\":1,\"2327\":9,\"2355\":2,\"2462\":1}}],[\"optimum\",{\"1\":{\"46\":1,\"175\":2,\"1319\":1}}],[\"optimal\",{\"1\":{\"45\":1,\"145\":1,\"794\":1,\"1110\":1,\"1321\":1,\"1322\":1}}],[\"optionnaly\",{\"1\":{\"139\":1}}],[\"optional\",{\"0\":{\"161\":1,\"163\":1},\"1\":{\"31\":1,\"43\":19,\"44\":6,\"78\":1,\"81\":4,\"134\":1,\"140\":1,\"141\":30,\"143\":1,\"162\":3,\"168\":1,\"199\":1,\"200\":5,\"204\":1,\"205\":5,\"216\":1,\"217\":2,\"222\":1,\"223\":3,\"225\":1,\"227\":1,\"234\":2,\"235\":4,\"240\":1,\"242\":3,\"246\":1,\"253\":1,\"254\":1,\"267\":1,\"276\":1,\"284\":3,\"285\":2,\"286\":2,\"701\":2,\"702\":2,\"712\":2,\"724\":2,\"725\":2,\"726\":1,\"727\":1,\"728\":2,\"729\":1,\"735\":2,\"744\":2,\"760\":4,\"768\":1,\"784\":1,\"786\":1,\"787\":5,\"797\":1,\"800\":2,\"828\":2,\"829\":3,\"830\":2,\"859\":1,\"921\":1,\"935\":2,\"1061\":2,\"1062\":1,\"1063\":1,\"1064\":6,\"1118\":11,\"1185\":2,\"1209\":1,\"1210\":1,\"1262\":7,\"1280\":3,\"1283\":3,\"1290\":4,\"1413\":3,\"1419\":3,\"1420\":5,\"1477\":1,\"1513\":1,\"1514\":3,\"1519\":1,\"1520\":2,\"1521\":41,\"1525\":1,\"1526\":12,\"1529\":4,\"1533\":4,\"1551\":1,\"1552\":12,\"1553\":13,\"1556\":2,\"1558\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1585\":18,\"1592\":2,\"1598\":3,\"1599\":9,\"1607\":5,\"1610\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1616\":2,\"1624\":1,\"1625\":5,\"1626\":11,\"1628\":3,\"1678\":3,\"1724\":1,\"1750\":16,\"1753\":7,\"1754\":1,\"1758\":9,\"1770\":3,\"1771\":6,\"1788\":2,\"1810\":5,\"1812\":2,\"1822\":2,\"1854\":2,\"1856\":1,\"1901\":2,\"1903\":2,\"1966\":2,\"1992\":6,\"1993\":10,\"1995\":6,\"2043\":3,\"2044\":2,\"2045\":2,\"2049\":3,\"2054\":1,\"2055\":3,\"2056\":3,\"2065\":9,\"2066\":3,\"2130\":2,\"2134\":2,\"2139\":1,\"2144\":1,\"2157\":1,\"2183\":1,\"2184\":1,\"2190\":1,\"2208\":1,\"2223\":14,\"2224\":2,\"2227\":2,\"2228\":51,\"2229\":46,\"2231\":9,\"2235\":18,\"2236\":18,\"2239\":21,\"2240\":20,\"2245\":19,\"2246\":4,\"2247\":3,\"2248\":4,\"2249\":4,\"2250\":4,\"2251\":4,\"2252\":4,\"2253\":4,\"2254\":4,\"2255\":4,\"2256\":4,\"2257\":4,\"2258\":2,\"2259\":4,\"2260\":4,\"2261\":4,\"2262\":2,\"2263\":4,\"2264\":4,\"2265\":4,\"2266\":4,\"2267\":4,\"2268\":4,\"2269\":4,\"2270\":4,\"2271\":4,\"2272\":4,\"2273\":4,\"2334\":1,\"2354\":2,\"2408\":24,\"2411\":11,\"2412\":13,\"2423\":13,\"2425\":7,\"2427\":4,\"2428\":15,\"2429\":10,\"2430\":5,\"2431\":10,\"2432\":10,\"2442\":3,\"2446\":24,\"2447\":12}}],[\"optionally\",{\"1\":{\"22\":1,\"106\":1,\"163\":1,\"224\":1,\"233\":1,\"235\":2,\"247\":1,\"286\":1,\"699\":1,\"831\":1,\"1919\":1,\"2139\":1,\"2355\":1}}],[\"options=\",{\"1\":{\"126\":1}}],[\"options\",{\"0\":{\"40\":1},\"1\":{\"24\":1,\"40\":2,\"44\":1,\"46\":1,\"48\":1,\"49\":2,\"50\":2,\"84\":1,\"117\":2,\"124\":1,\"126\":3,\"143\":1,\"144\":1,\"168\":3,\"197\":1,\"200\":2,\"205\":2,\"211\":1,\"212\":1,\"217\":1,\"218\":1,\"242\":2,\"246\":1,\"254\":1,\"266\":2,\"267\":1,\"275\":2,\"276\":1,\"285\":2,\"286\":1,\"289\":1,\"374\":1,\"513\":1,\"514\":1,\"515\":2,\"516\":2,\"518\":1,\"520\":1,\"521\":2,\"523\":2,\"524\":2,\"525\":2,\"527\":2,\"535\":2,\"537\":2,\"819\":2,\"821\":1,\"846\":2,\"1268\":1,\"1274\":1,\"1892\":1,\"2043\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2220\":2,\"2249\":11,\"2286\":2,\"2334\":1,\"2338\":5,\"2340\":1,\"2347\":4,\"2354\":1,\"2355\":1,\"2365\":2,\"2369\":6,\"2371\":4}}],[\"option\",{\"1\":{\"3\":1,\"40\":1,\"41\":2,\"47\":2,\"48\":3,\"50\":2,\"51\":2,\"54\":1,\"71\":1,\"74\":1,\"80\":1,\"84\":2,\"92\":1,\"95\":1,\"102\":1,\"117\":1,\"139\":1,\"144\":1,\"162\":8,\"168\":11,\"196\":2,\"200\":2,\"211\":4,\"219\":1,\"266\":8,\"268\":2,\"269\":1,\"270\":1,\"271\":1,\"272\":1,\"275\":8,\"277\":2,\"278\":1,\"279\":1,\"280\":1,\"282\":1,\"285\":4,\"286\":4,\"287\":1,\"288\":1,\"289\":1,\"290\":2,\"403\":28,\"821\":1,\"1008\":1,\"1521\":6,\"1833\":1,\"1954\":1,\"2044\":23,\"2228\":6,\"2229\":6,\"2249\":5,\"2253\":2,\"2347\":2,\"2348\":1,\"2354\":1,\"2365\":2,\"2369\":3,\"2371\":2,\"2372\":1,\"2474\":1,\"2478\":1}}],[\"operands\",{\"1\":{\"1307\":1}}],[\"operates\",{\"1\":{\"2355\":1}}],[\"operator\",{\"1\":{\"705\":1,\"804\":2}}],[\"operation\",{\"0\":{\"1352\":1},\"1\":{\"704\":1,\"755\":1,\"756\":1,\"764\":1,\"773\":1,\"785\":1,\"802\":1,\"846\":2,\"866\":1,\"867\":1,\"1124\":1,\"1125\":1,\"1176\":1,\"1352\":1,\"2432\":1}}],[\"operations\",{\"1\":{\"0\":1,\"49\":1,\"104\":1,\"224\":1,\"1301\":1,\"1372\":1,\"2130\":2,\"2131\":1,\"2345\":1}}],[\"openreview\",{\"1\":{\"1589\":1}}],[\"openmp\",{\"1\":{\"922\":1,\"936\":1,\"937\":1}}],[\"opencpop\",{\"1\":{\"267\":3,\"269\":2,\"276\":3,\"277\":2,\"278\":2,\"282\":1}}],[\"openblas\",{\"1\":{\"161\":6}}],[\"openaiwhispertokenizer\",{\"0\":{\"2284\":1},\"1\":{\"2284\":1}}],[\"openaiwhispertokenidconverter\",{\"0\":{\"2283\":1},\"1\":{\"2283\":1}}],[\"openaiwhisperencoder\",{\"0\":{\"791\":1},\"1\":{\"791\":1}}],[\"openaiwhisperdecoder\",{\"0\":{\"790\":1},\"1\":{\"790\":1}}],[\"openai\",{\"0\":{\"126\":1},\"1\":{\"126\":1,\"240\":1,\"267\":1,\"276\":1,\"286\":1,\"790\":2,\"791\":2,\"864\":2,\"1725\":1}}],[\"openssl\",{\"1\":{\"60\":1,\"62\":1,\"63\":1,\"64\":1}}],[\"open\",{\"1\":{\"6\":2,\"9\":1,\"14\":1,\"18\":1,\"42\":1,\"138\":1,\"243\":2,\"244\":1,\"245\":1,\"252\":1,\"269\":2,\"278\":2,\"518\":1,\"2184\":1,\"2365\":1}}],[\"onreim\",{\"0\":{\"1226\":1},\"1\":{\"1226\":1}}],[\"onto\",{\"1\":{\"704\":1}}],[\"onlineprocessor\",{\"1\":{\"639\":1}}],[\"onlineaudioprocessor\",{\"0\":{\"639\":1},\"1\":{\"639\":2}}],[\"online\",{\"0\":{\"639\":1,\"1720\":1,\"1721\":1},\"1\":{\"131\":1,\"150\":2,\"639\":1,\"1720\":2,\"1721\":2}}],[\"only=0\",{\"1\":{\"162\":1}}],[\"only\",{\"0\":{\"929\":1},\"1\":{\"5\":1,\"28\":1,\"44\":1,\"50\":1,\"67\":1,\"69\":1,\"78\":1,\"79\":1,\"88\":2,\"96\":1,\"97\":2,\"126\":1,\"137\":1,\"141\":2,\"142\":22,\"143\":1,\"160\":2,\"161\":1,\"162\":1,\"163\":1,\"173\":2,\"197\":2,\"211\":1,\"223\":1,\"242\":1,\"243\":1,\"261\":1,\"265\":1,\"268\":2,\"269\":1,\"274\":1,\"275\":1,\"277\":2,\"278\":1,\"286\":1,\"290\":3,\"368\":2,\"377\":2,\"449\":2,\"527\":2,\"629\":1,\"643\":1,\"674\":2,\"746\":1,\"748\":1,\"768\":1,\"828\":1,\"829\":1,\"830\":2,\"922\":1,\"925\":1,\"927\":1,\"929\":2,\"962\":1,\"1021\":1,\"1022\":1,\"1029\":1,\"1037\":1,\"1061\":2,\"1063\":2,\"1066\":1,\"1133\":1,\"1147\":1,\"1155\":1,\"1157\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":2,\"1175\":1,\"1210\":1,\"1235\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1264\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1275\":1,\"1276\":2,\"1277\":1,\"1279\":1,\"1280\":7,\"1281\":1,\"1282\":1,\"1283\":2,\"1301\":1,\"1318\":1,\"1327\":1,\"1330\":1,\"1334\":1,\"1361\":1,\"1372\":1,\"1397\":2,\"1444\":1,\"1448\":1,\"1469\":1,\"1505\":1,\"1506\":1,\"1552\":5,\"1553\":1,\"1612\":3,\"1613\":3,\"1625\":1,\"1626\":5,\"1655\":2,\"1678\":1,\"1683\":3,\"1684\":1,\"1711\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1,\"1794\":3,\"1808\":1,\"1872\":1,\"1902\":1,\"1904\":1,\"1919\":1,\"1943\":1,\"1962\":1,\"1963\":1,\"1965\":1,\"1996\":1,\"2000\":1,\"2001\":1,\"2127\":1,\"2130\":4,\"2136\":1,\"2143\":1,\"2215\":1,\"2216\":1,\"2245\":1,\"2304\":1,\"2325\":1,\"2355\":5,\"2431\":1,\"2432\":1}}],[\"ony\",{\"1\":{\"102\":1}}],[\"once\",{\"1\":{\"38\":1,\"196\":1,\"213\":1,\"214\":1,\"242\":2,\"263\":1,\"268\":1,\"277\":1,\"286\":1,\"1086\":2,\"1207\":2,\"1301\":1,\"1372\":1,\"1484\":1}}],[\"onnx\",{\"0\":{\"16\":1,\"1935\":1},\"1\":{\"16\":1,\"182\":2,\"1935\":2}}],[\"onehot\",{\"0\":{\"962\":1,\"1803\":1},\"1\":{\"962\":3,\"1667\":3,\"1803\":2}}],[\"one\",{\"0\":{\"1377\":1},\"1\":{\"10\":1,\"43\":1,\"45\":1,\"48\":1,\"71\":1,\"78\":1,\"86\":2,\"101\":1,\"102\":1,\"119\":1,\"127\":1,\"138\":2,\"140\":1,\"142\":1,\"145\":1,\"146\":1,\"147\":1,\"166\":1,\"167\":1,\"196\":1,\"200\":1,\"211\":1,\"213\":3,\"223\":2,\"232\":1,\"254\":1,\"258\":1,\"262\":2,\"268\":1,\"269\":3,\"277\":1,\"278\":3,\"289\":2,\"293\":1,\"614\":2,\"634\":2,\"637\":1,\"641\":2,\"643\":2,\"651\":2,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"689\":1,\"690\":1,\"692\":3,\"694\":1,\"709\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"724\":2,\"725\":2,\"726\":1,\"728\":2,\"729\":2,\"739\":1,\"742\":1,\"744\":2,\"753\":1,\"758\":1,\"774\":1,\"779\":1,\"780\":1,\"782\":1,\"784\":2,\"789\":1,\"790\":2,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"819\":1,\"822\":1,\"824\":1,\"826\":1,\"828\":3,\"829\":3,\"830\":3,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"847\":2,\"850\":2,\"853\":1,\"855\":1,\"857\":1,\"859\":1,\"861\":1,\"863\":1,\"865\":1,\"939\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"978\":1,\"1008\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1050\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1064\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1116\":1,\"1117\":1,\"1121\":1,\"1123\":1,\"1125\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1161\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1181\":1,\"1186\":1,\"1188\":1,\"1189\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":2,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1218\":1,\"1220\":1,\"1221\":1,\"1224\":1,\"1225\":1,\"1227\":1,\"1229\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1244\":1,\"1245\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1280\":1,\"1283\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1317\":1,\"1318\":1,\"1328\":1,\"1354\":1,\"1377\":2,\"1384\":1,\"1388\":1,\"1393\":1,\"1397\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1419\":2,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1484\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1607\":2,\"1638\":1,\"1639\":1,\"1645\":1,\"1650\":1,\"1653\":1,\"1655\":1,\"1658\":1,\"1663\":1,\"1668\":1,\"1683\":1,\"1711\":1,\"1720\":4,\"1749\":2,\"1803\":2,\"1815\":2,\"1843\":2,\"1892\":2,\"1907\":1,\"1932\":1,\"1938\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"1992\":2,\"2000\":1,\"2001\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2133\":1,\"2136\":1,\"2151\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2184\":1,\"2186\":1,\"2189\":1,\"2191\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2310\":1,\"2326\":1,\"2338\":2,\"2347\":4,\"2355\":2,\"2365\":1,\"2369\":4,\"2371\":4,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1,\"2469\":1}}],[\"oneseded\",{\"1\":{\"1419\":1,\"1607\":1}}],[\"onesided\",{\"1\":{\"720\":1,\"1250\":1,\"1251\":1,\"1419\":2,\"1607\":2,\"1669\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1}}],[\"ones\",{\"1\":{\"3\":1,\"1390\":1,\"1402\":1,\"1409\":1,\"1454\":1,\"1456\":1,\"1467\":1,\"1595\":1,\"1909\":3,\"1933\":2}}],[\"on\",{\"0\":{\"152\":1,\"224\":1,\"225\":1},\"1\":{\"3\":2,\"6\":3,\"22\":1,\"26\":6,\"27\":1,\"38\":1,\"40\":1,\"44\":1,\"45\":2,\"50\":1,\"66\":1,\"71\":2,\"78\":1,\"79\":1,\"84\":1,\"101\":1,\"102\":1,\"106\":2,\"107\":1,\"117\":1,\"125\":1,\"126\":1,\"128\":1,\"133\":1,\"136\":1,\"138\":2,\"139\":1,\"141\":2,\"145\":2,\"146\":3,\"150\":1,\"160\":1,\"162\":2,\"168\":1,\"175\":1,\"178\":1,\"184\":2,\"185\":2,\"186\":1,\"187\":1,\"188\":1,\"190\":2,\"191\":1,\"193\":1,\"197\":2,\"198\":1,\"200\":2,\"202\":1,\"205\":1,\"206\":1,\"207\":1,\"212\":1,\"218\":1,\"222\":2,\"223\":5,\"224\":1,\"225\":1,\"235\":1,\"240\":1,\"242\":1,\"243\":2,\"245\":1,\"246\":1,\"247\":1,\"249\":1,\"259\":2,\"260\":1,\"261\":1,\"262\":3,\"263\":1,\"267\":2,\"269\":2,\"276\":1,\"278\":2,\"284\":1,\"285\":2,\"286\":3,\"287\":3,\"522\":1,\"616\":3,\"634\":1,\"639\":1,\"643\":1,\"689\":1,\"696\":7,\"697\":5,\"703\":1,\"716\":1,\"718\":1,\"722\":1,\"756\":1,\"764\":1,\"773\":1,\"802\":1,\"820\":1,\"828\":1,\"831\":1,\"832\":1,\"856\":1,\"911\":1,\"927\":1,\"929\":1,\"960\":1,\"961\":5,\"992\":1,\"994\":1,\"997\":1,\"1000\":1,\"1002\":1,\"1004\":1,\"1008\":1,\"1011\":2,\"1012\":1,\"1016\":1,\"1131\":1,\"1133\":2,\"1168\":1,\"1170\":2,\"1171\":2,\"1172\":3,\"1173\":2,\"1174\":1,\"1175\":2,\"1252\":1,\"1271\":1,\"1308\":1,\"1317\":1,\"1318\":1,\"1321\":1,\"1322\":1,\"1334\":1,\"1350\":1,\"1392\":1,\"1400\":1,\"1441\":2,\"1444\":1,\"1448\":1,\"1493\":1,\"1494\":1,\"1502\":2,\"1514\":1,\"1529\":1,\"1539\":1,\"1608\":1,\"1609\":1,\"1631\":1,\"1645\":2,\"1661\":1,\"1668\":3,\"1676\":2,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1719\":6,\"1720\":1,\"1721\":2,\"1725\":8,\"1729\":1,\"1730\":1,\"1750\":1,\"1751\":2,\"1756\":1,\"1757\":1,\"1758\":1,\"1768\":1,\"1770\":1,\"1788\":2,\"1789\":1,\"1790\":1,\"1806\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"1854\":1,\"1860\":1,\"1895\":1,\"1951\":1,\"1960\":1,\"1961\":1,\"2000\":2,\"2001\":1,\"2043\":2,\"2044\":5,\"2049\":2,\"2054\":1,\"2055\":2,\"2056\":2,\"2066\":1,\"2130\":5,\"2131\":2,\"2133\":1,\"2136\":3,\"2138\":1,\"2148\":1,\"2162\":1,\"2176\":1,\"2184\":1,\"2215\":1,\"2239\":1,\"2247\":3,\"2262\":1,\"2276\":1,\"2277\":1,\"2298\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2333\":1,\"2345\":1,\"2354\":1,\"2355\":4,\"2404\":1,\"2405\":1,\"2431\":1}}],[\"ornstein\",{\"1\":{\"1224\":1,\"1225\":1}}],[\"ornone\",{\"1\":{\"1126\":2}}],[\"oracle\",{\"1\":{\"978\":1,\"1126\":4}}],[\"ori\",{\"1\":{\"741\":1}}],[\"originated\",{\"1\":{\"2151\":1,\"2310\":1,\"2474\":1}}],[\"originally\",{\"1\":{\"71\":1,\"196\":1,\"268\":1,\"277\":1,\"1883\":1,\"2001\":1,\"2249\":1,\"2253\":1}}],[\"original\",{\"1\":{\"68\":1,\"70\":1,\"146\":1,\"197\":1,\"200\":1,\"232\":1,\"243\":1,\"258\":1,\"286\":1,\"674\":1,\"696\":1,\"733\":1,\"750\":1,\"755\":2,\"760\":2,\"785\":2,\"924\":1,\"1050\":1,\"1155\":1,\"1157\":1,\"1327\":1,\"1330\":1,\"1375\":1,\"1387\":1,\"1717\":1,\"2016\":1,\"2130\":1,\"2148\":2,\"2155\":1,\"2287\":1}}],[\"origin\",{\"1\":{\"790\":1,\"1162\":1}}],[\"orig\",{\"0\":{\"1335\":1},\"1\":{\"578\":2,\"611\":2,\"1335\":2,\"2054\":2}}],[\"organizes\",{\"1\":{\"2143\":1}}],[\"organized\",{\"1\":{\"162\":1,\"2130\":1}}],[\"organization\",{\"1\":{\"2130\":1}}],[\"org\",{\"1\":{\"60\":1,\"104\":1,\"106\":1,\"196\":1,\"218\":1,\"267\":1,\"276\":1,\"286\":1,\"290\":1,\"616\":5,\"617\":1,\"624\":1,\"629\":1,\"634\":1,\"635\":1,\"640\":1,\"643\":1,\"648\":1,\"652\":3,\"696\":7,\"697\":6,\"699\":1,\"711\":1,\"768\":1,\"780\":1,\"785\":1,\"786\":1,\"846\":1,\"866\":2,\"882\":1,\"883\":1,\"884\":1,\"921\":2,\"922\":2,\"974\":3,\"1061\":2,\"1062\":2,\"1066\":1,\"1130\":1,\"1131\":2,\"1170\":1,\"1172\":2,\"1252\":1,\"1308\":1,\"1309\":1,\"1311\":1,\"1321\":1,\"1322\":1,\"1327\":1,\"1330\":2,\"1385\":1,\"1396\":1,\"1411\":1,\"1439\":1,\"1515\":1,\"1530\":1,\"1541\":1,\"1577\":1,\"1590\":1,\"1668\":1,\"1672\":1,\"1673\":1,\"1683\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1705\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1720\":1,\"1721\":1,\"1731\":2,\"1748\":1,\"1768\":1,\"1784\":2,\"1785\":1,\"1786\":1,\"1817\":1,\"1818\":1,\"1820\":1,\"1963\":1,\"1967\":1,\"1977\":1,\"1994\":1,\"2000\":1,\"2001\":1,\"2016\":1,\"2167\":1,\"2168\":1,\"2176\":1,\"2183\":1,\"2223\":2,\"2227\":1,\"2231\":1,\"2239\":1,\"2245\":1,\"2428\":1}}],[\"order=2\",{\"1\":{\"1703\":1,\"1857\":1}}],[\"ordereddict\",{\"1\":{\"967\":1,\"968\":1,\"969\":1,\"978\":2,\"1038\":1,\"1039\":1,\"1040\":1,\"1044\":1,\"1053\":1,\"1062\":2,\"1107\":2,\"1117\":2,\"1118\":2,\"1125\":2,\"1130\":3,\"1131\":2,\"1136\":2,\"1141\":2,\"1157\":1,\"1158\":1,\"1162\":2,\"1217\":2,\"1232\":2,\"1252\":2,\"1261\":2,\"1267\":2,\"1268\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":2,\"1280\":2,\"1283\":2,\"1334\":1}}],[\"ordering\",{\"1\":{\"262\":2,\"819\":1,\"827\":1,\"1209\":1,\"1228\":1}}],[\"order\",{\"0\":{\"1167\":1},\"1\":{\"44\":1,\"70\":1,\"96\":1,\"97\":1,\"104\":1,\"133\":1,\"140\":1,\"144\":1,\"147\":1,\"150\":1,\"211\":1,\"223\":1,\"637\":4,\"846\":1,\"1155\":2,\"1156\":1,\"1157\":2,\"1167\":2,\"1204\":1,\"1209\":1,\"1228\":1,\"1301\":1,\"1308\":1,\"1372\":1,\"1454\":2,\"1456\":2,\"1846\":1,\"2136\":2}}],[\"or\",{\"0\":{\"153\":1,\"174\":1,\"1110\":1,\"1301\":1,\"1306\":1,\"1331\":1,\"1344\":1,\"1345\":1,\"1371\":1,\"1372\":1,\"2484\":1,\"2491\":1,\"2492\":1,\"2503\":1,\"2504\":1},\"1\":{\"1\":1,\"3\":1,\"18\":1,\"38\":2,\"41\":1,\"43\":10,\"44\":1,\"48\":2,\"50\":1,\"52\":1,\"56\":1,\"67\":2,\"70\":4,\"79\":2,\"82\":1,\"95\":1,\"97\":1,\"104\":2,\"106\":1,\"124\":1,\"127\":1,\"128\":1,\"130\":1,\"138\":3,\"139\":4,\"140\":1,\"141\":11,\"142\":2,\"150\":2,\"156\":1,\"161\":1,\"162\":1,\"166\":1,\"167\":1,\"195\":2,\"196\":1,\"197\":2,\"201\":1,\"211\":1,\"213\":1,\"223\":3,\"224\":2,\"232\":3,\"235\":2,\"240\":2,\"242\":4,\"252\":1,\"254\":2,\"258\":2,\"259\":1,\"261\":1,\"262\":1,\"263\":1,\"266\":2,\"267\":6,\"268\":2,\"269\":7,\"273\":2,\"275\":4,\"276\":6,\"277\":2,\"278\":7,\"284\":1,\"285\":4,\"286\":4,\"290\":7,\"372\":2,\"527\":2,\"538\":1,\"616\":1,\"620\":2,\"625\":1,\"631\":1,\"632\":3,\"634\":2,\"641\":10,\"661\":1,\"665\":1,\"669\":1,\"689\":1,\"692\":1,\"696\":1,\"697\":1,\"701\":2,\"703\":2,\"706\":1,\"709\":2,\"710\":1,\"711\":1,\"716\":1,\"735\":2,\"738\":1,\"755\":2,\"756\":3,\"760\":6,\"761\":1,\"762\":1,\"768\":1,\"773\":3,\"774\":3,\"777\":1,\"780\":3,\"784\":2,\"785\":2,\"792\":2,\"798\":1,\"804\":1,\"819\":1,\"821\":1,\"824\":5,\"831\":1,\"846\":2,\"849\":1,\"852\":1,\"856\":1,\"862\":1,\"866\":1,\"867\":1,\"878\":1,\"879\":1,\"881\":1,\"884\":1,\"912\":2,\"932\":1,\"934\":1,\"939\":2,\"942\":1,\"962\":1,\"982\":1,\"1026\":1,\"1051\":1,\"1061\":1,\"1062\":2,\"1086\":4,\"1107\":1,\"1110\":1,\"1119\":2,\"1124\":1,\"1125\":2,\"1126\":3,\"1127\":5,\"1133\":1,\"1139\":1,\"1141\":1,\"1145\":1,\"1155\":7,\"1156\":1,\"1157\":6,\"1158\":5,\"1162\":2,\"1170\":2,\"1171\":2,\"1173\":2,\"1175\":2,\"1176\":1,\"1202\":2,\"1207\":4,\"1210\":1,\"1224\":1,\"1225\":1,\"1247\":1,\"1253\":2,\"1259\":2,\"1261\":2,\"1264\":1,\"1265\":1,\"1267\":1,\"1268\":1,\"1269\":4,\"1270\":4,\"1271\":4,\"1273\":1,\"1274\":1,\"1275\":2,\"1277\":2,\"1278\":2,\"1280\":1,\"1283\":2,\"1290\":1,\"1301\":4,\"1306\":4,\"1308\":2,\"1311\":1,\"1318\":1,\"1319\":1,\"1322\":1,\"1323\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1330\":1,\"1331\":2,\"1334\":5,\"1344\":1,\"1345\":1,\"1350\":1,\"1354\":2,\"1356\":1,\"1371\":4,\"1372\":4,\"1381\":1,\"1395\":5,\"1419\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1485\":1,\"1508\":1,\"1519\":4,\"1521\":1,\"1526\":9,\"1529\":3,\"1535\":3,\"1545\":1,\"1548\":1,\"1549\":1,\"1552\":2,\"1553\":9,\"1559\":1,\"1576\":1,\"1584\":3,\"1585\":1,\"1587\":2,\"1591\":2,\"1598\":4,\"1599\":6,\"1600\":2,\"1619\":1,\"1625\":4,\"1626\":5,\"1655\":1,\"1669\":3,\"1687\":1,\"1735\":2,\"1736\":2,\"1751\":1,\"1756\":2,\"1757\":2,\"1758\":3,\"1759\":2,\"1785\":1,\"1788\":2,\"1789\":2,\"1790\":2,\"1794\":2,\"1798\":1,\"1799\":1,\"1800\":1,\"1817\":1,\"1851\":1,\"1854\":1,\"1860\":1,\"1863\":1,\"1865\":2,\"1867\":2,\"1868\":1,\"1870\":1,\"1878\":2,\"1881\":1,\"1883\":1,\"1891\":1,\"1898\":1,\"1913\":1,\"1926\":2,\"1932\":2,\"1937\":1,\"1940\":1,\"1942\":1,\"1957\":1,\"1960\":2,\"1961\":1,\"1971\":2,\"1976\":1,\"1984\":1,\"1992\":1,\"1995\":1,\"2000\":1,\"2001\":1,\"2005\":1,\"2006\":1,\"2007\":3,\"2043\":1,\"2044\":3,\"2054\":1,\"2055\":1,\"2056\":1,\"2065\":2,\"2066\":1,\"2101\":1,\"2128\":1,\"2129\":1,\"2130\":1,\"2134\":4,\"2136\":2,\"2137\":1,\"2144\":1,\"2145\":1,\"2149\":1,\"2150\":2,\"2155\":1,\"2168\":1,\"2184\":3,\"2187\":1,\"2191\":2,\"2198\":2,\"2222\":2,\"2231\":3,\"2235\":9,\"2236\":10,\"2239\":14,\"2240\":13,\"2241\":1,\"2245\":12,\"2249\":2,\"2262\":2,\"2278\":1,\"2309\":3,\"2312\":1,\"2327\":2,\"2354\":2,\"2355\":12,\"2403\":2,\"2404\":1,\"2407\":1,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":1,\"2435\":1,\"2445\":2,\"2447\":2,\"2480\":2,\"2484\":2,\"2485\":1,\"2491\":1,\"2492\":2,\"2493\":1,\"2503\":1,\"2504\":2,\"2505\":1}}],[\"ofuton\",{\"1\":{\"267\":3,\"268\":6,\"269\":2,\"272\":1,\"277\":4,\"278\":2}}],[\"often\",{\"1\":{\"39\":1,\"111\":1,\"194\":1,\"1000\":1,\"2355\":1}}],[\"offset=1\",{\"1\":{\"1752\":1,\"1754\":1}}],[\"offsets\",{\"1\":{\"1350\":1}}],[\"offsetting\",{\"1\":{\"1350\":1}}],[\"offset\",{\"1\":{\"704\":2,\"1000\":1,\"1753\":2,\"1754\":2}}],[\"offer\",{\"1\":{\"269\":1,\"278\":1}}],[\"official\",{\"1\":{\"269\":1,\"278\":1,\"287\":1,\"1389\":1,\"1390\":1,\"1401\":1,\"1402\":3,\"1408\":1,\"1409\":1,\"1466\":1,\"1467\":3,\"1513\":1,\"1526\":1,\"1548\":1,\"1549\":3,\"1551\":1,\"1553\":1,\"1592\":1,\"1594\":3,\"1595\":3,\"1598\":1,\"1600\":1,\"1605\":1,\"1606\":1,\"1625\":1,\"1883\":1}}],[\"officially\",{\"1\":{\"197\":1}}],[\"offline\",{\"0\":{\"262\":1},\"1\":{\"148\":1,\"150\":1,\"232\":1,\"235\":1,\"254\":1,\"258\":1,\"260\":2,\"261\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1}}],[\"off\",{\"1\":{\"8\":1,\"81\":1,\"104\":1,\"703\":1,\"755\":1,\"785\":2,\"1327\":1,\"1330\":1,\"1608\":1,\"1631\":1,\"1717\":1,\"1860\":1,\"2240\":1}}],[\"of\",{\"0\":{\"38\":1,\"41\":1,\"67\":1,\"91\":1,\"94\":1,\"109\":1,\"120\":1,\"123\":1,\"128\":1,\"167\":1,\"199\":1,\"204\":1,\"210\":1,\"216\":1,\"227\":1,\"234\":1,\"241\":1,\"253\":1,\"265\":1,\"274\":1,\"284\":1},\"1\":{\"0\":1,\"2\":1,\"3\":2,\"6\":1,\"9\":1,\"15\":1,\"18\":1,\"23\":1,\"26\":1,\"32\":1,\"36\":1,\"37\":1,\"38\":2,\"39\":1,\"42\":1,\"43\":20,\"44\":6,\"45\":5,\"47\":2,\"48\":5,\"50\":2,\"51\":1,\"53\":1,\"54\":1,\"55\":4,\"57\":1,\"58\":1,\"59\":3,\"67\":2,\"68\":5,\"69\":3,\"70\":4,\"71\":10,\"73\":1,\"74\":3,\"78\":5,\"79\":5,\"80\":1,\"81\":2,\"82\":16,\"84\":4,\"87\":3,\"90\":1,\"91\":5,\"94\":7,\"95\":6,\"96\":5,\"97\":5,\"98\":7,\"99\":3,\"100\":5,\"101\":3,\"102\":4,\"104\":1,\"106\":2,\"107\":5,\"108\":1,\"109\":4,\"110\":4,\"114\":1,\"119\":4,\"120\":2,\"123\":4,\"127\":1,\"128\":3,\"134\":4,\"135\":1,\"138\":7,\"139\":4,\"140\":1,\"141\":22,\"142\":12,\"143\":3,\"144\":3,\"145\":4,\"147\":2,\"148\":1,\"152\":1,\"154\":1,\"161\":1,\"162\":1,\"164\":1,\"165\":1,\"166\":1,\"167\":4,\"168\":1,\"173\":7,\"175\":3,\"179\":1,\"190\":10,\"191\":2,\"192\":2,\"194\":1,\"195\":1,\"196\":6,\"197\":11,\"198\":1,\"199\":1,\"200\":5,\"201\":3,\"202\":2,\"203\":2,\"204\":1,\"205\":6,\"206\":1,\"207\":2,\"208\":1,\"209\":1,\"211\":5,\"212\":1,\"213\":2,\"215\":1,\"216\":1,\"217\":3,\"218\":3,\"220\":1,\"221\":1,\"222\":1,\"223\":10,\"224\":1,\"226\":1,\"227\":1,\"228\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":2,\"233\":2,\"234\":1,\"235\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":3,\"242\":9,\"243\":1,\"245\":2,\"246\":3,\"247\":4,\"250\":1,\"251\":1,\"252\":1,\"254\":4,\"255\":1,\"256\":1,\"257\":1,\"258\":2,\"260\":1,\"261\":1,\"262\":23,\"263\":1,\"264\":1,\"266\":3,\"267\":12,\"268\":7,\"269\":5,\"272\":1,\"273\":1,\"274\":1,\"275\":6,\"276\":20,\"277\":7,\"278\":5,\"282\":1,\"283\":1,\"284\":3,\"285\":7,\"286\":22,\"287\":2,\"289\":2,\"290\":18,\"291\":1,\"292\":1,\"515\":2,\"516\":4,\"518\":1,\"520\":1,\"521\":2,\"522\":1,\"523\":2,\"524\":2,\"525\":2,\"527\":6,\"535\":1,\"536\":4,\"537\":1,\"545\":1,\"614\":3,\"616\":11,\"617\":3,\"618\":3,\"619\":2,\"620\":8,\"621\":2,\"622\":2,\"623\":2,\"624\":3,\"625\":8,\"626\":3,\"627\":2,\"628\":1,\"630\":2,\"632\":2,\"633\":3,\"634\":8,\"636\":6,\"637\":1,\"639\":4,\"640\":1,\"641\":1,\"642\":1,\"643\":4,\"644\":5,\"645\":2,\"646\":1,\"647\":2,\"649\":2,\"661\":4,\"664\":2,\"666\":1,\"669\":3,\"677\":2,\"679\":2,\"681\":2,\"683\":2,\"685\":2,\"686\":1,\"687\":2,\"690\":2,\"691\":2,\"692\":11,\"694\":2,\"696\":8,\"697\":4,\"699\":1,\"701\":4,\"702\":3,\"703\":21,\"705\":2,\"706\":19,\"709\":9,\"710\":8,\"711\":8,\"712\":1,\"714\":2,\"716\":3,\"717\":6,\"719\":2,\"721\":2,\"723\":2,\"724\":6,\"725\":6,\"726\":3,\"727\":2,\"728\":6,\"729\":4,\"735\":3,\"738\":1,\"739\":2,\"740\":1,\"742\":2,\"744\":3,\"745\":1,\"747\":1,\"748\":5,\"750\":1,\"753\":2,\"755\":19,\"756\":6,\"758\":2,\"760\":9,\"768\":19,\"771\":3,\"773\":6,\"774\":9,\"779\":3,\"780\":12,\"782\":2,\"784\":3,\"785\":16,\"786\":7,\"787\":1,\"789\":2,\"790\":3,\"792\":2,\"797\":4,\"798\":4,\"799\":2,\"800\":6,\"804\":6,\"806\":2,\"808\":2,\"810\":2,\"812\":2,\"814\":2,\"816\":2,\"817\":2,\"819\":1,\"820\":7,\"821\":13,\"822\":2,\"824\":9,\"826\":2,\"827\":1,\"828\":9,\"829\":12,\"830\":9,\"831\":4,\"832\":3,\"833\":1,\"834\":2,\"836\":2,\"838\":2,\"840\":2,\"843\":2,\"845\":2,\"846\":17,\"847\":4,\"849\":10,\"850\":3,\"853\":2,\"855\":2,\"857\":2,\"859\":6,\"861\":2,\"862\":4,\"863\":2,\"865\":2,\"866\":3,\"867\":8,\"878\":12,\"879\":12,\"881\":16,\"882\":16,\"883\":16,\"884\":20,\"887\":1,\"911\":3,\"912\":1,\"919\":6,\"921\":7,\"922\":11,\"927\":4,\"932\":6,\"934\":6,\"935\":6,\"936\":10,\"937\":10,\"938\":1,\"939\":2,\"941\":1,\"951\":2,\"953\":2,\"957\":2,\"960\":2,\"961\":1,\"964\":2,\"966\":2,\"968\":2,\"970\":2,\"978\":6,\"980\":3,\"982\":6,\"984\":1,\"993\":1,\"994\":2,\"996\":1,\"998\":1,\"1000\":3,\"1002\":3,\"1008\":1,\"1009\":2,\"1014\":2,\"1019\":1,\"1024\":1,\"1029\":15,\"1031\":2,\"1033\":2,\"1035\":2,\"1037\":2,\"1039\":2,\"1041\":2,\"1043\":2,\"1045\":2,\"1047\":2,\"1049\":2,\"1050\":2,\"1051\":2,\"1052\":2,\"1053\":4,\"1056\":2,\"1058\":2,\"1060\":2,\"1061\":10,\"1062\":6,\"1063\":3,\"1064\":6,\"1067\":2,\"1069\":2,\"1070\":3,\"1071\":3,\"1073\":2,\"1077\":2,\"1078\":1,\"1079\":2,\"1081\":2,\"1083\":2,\"1085\":2,\"1088\":2,\"1090\":2,\"1092\":2,\"1094\":2,\"1096\":2,\"1098\":2,\"1100\":2,\"1102\":2,\"1104\":2,\"1106\":2,\"1107\":9,\"1109\":2,\"1111\":2,\"1115\":2,\"1116\":2,\"1117\":5,\"1118\":9,\"1119\":3,\"1121\":2,\"1123\":2,\"1124\":18,\"1125\":20,\"1126\":1,\"1127\":1,\"1130\":5,\"1131\":5,\"1132\":1,\"1133\":7,\"1134\":4,\"1135\":2,\"1136\":3,\"1137\":4,\"1138\":2,\"1139\":8,\"1140\":2,\"1141\":6,\"1143\":2,\"1145\":5,\"1146\":2,\"1147\":4,\"1150\":2,\"1152\":2,\"1153\":2,\"1154\":2,\"1155\":11,\"1157\":12,\"1160\":2,\"1161\":2,\"1162\":3,\"1163\":1,\"1164\":1,\"1166\":2,\"1169\":2,\"1176\":4,\"1178\":2,\"1180\":2,\"1181\":3,\"1185\":6,\"1186\":2,\"1188\":2,\"1189\":2,\"1191\":2,\"1193\":2,\"1195\":2,\"1197\":2,\"1198\":3,\"1199\":1,\"1201\":2,\"1202\":4,\"1203\":2,\"1204\":2,\"1206\":2,\"1208\":4,\"1209\":5,\"1210\":2,\"1212\":2,\"1214\":2,\"1216\":2,\"1218\":2,\"1220\":2,\"1221\":2,\"1224\":4,\"1225\":5,\"1227\":2,\"1228\":4,\"1229\":2,\"1231\":2,\"1232\":3,\"1234\":2,\"1235\":13,\"1237\":2,\"1239\":2,\"1241\":2,\"1243\":2,\"1244\":2,\"1245\":5,\"1246\":7,\"1247\":1,\"1249\":2,\"1252\":6,\"1253\":4,\"1254\":2,\"1255\":3,\"1256\":2,\"1257\":2,\"1258\":2,\"1259\":5,\"1260\":2,\"1261\":4,\"1262\":4,\"1263\":2,\"1264\":12,\"1265\":2,\"1266\":2,\"1267\":4,\"1268\":9,\"1269\":9,\"1270\":9,\"1271\":11,\"1273\":9,\"1274\":11,\"1278\":8,\"1279\":21,\"1280\":30,\"1281\":24,\"1282\":17,\"1283\":22,\"1285\":2,\"1287\":2,\"1289\":2,\"1290\":9,\"1292\":1,\"1296\":1,\"1301\":5,\"1306\":7,\"1309\":2,\"1310\":1,\"1311\":2,\"1314\":5,\"1315\":1,\"1316\":5,\"1318\":3,\"1319\":2,\"1320\":4,\"1321\":1,\"1322\":2,\"1323\":1,\"1327\":3,\"1328\":3,\"1329\":1,\"1330\":3,\"1333\":1,\"1334\":19,\"1350\":2,\"1352\":1,\"1354\":2,\"1356\":3,\"1368\":10,\"1371\":7,\"1372\":5,\"1374\":1,\"1375\":2,\"1376\":1,\"1377\":1,\"1381\":1,\"1384\":2,\"1385\":3,\"1386\":1,\"1388\":2,\"1389\":1,\"1390\":3,\"1391\":2,\"1392\":5,\"1393\":2,\"1395\":2,\"1397\":11,\"1399\":2,\"1400\":1,\"1401\":1,\"1402\":6,\"1403\":2,\"1405\":2,\"1406\":1,\"1407\":2,\"1408\":1,\"1409\":6,\"1410\":2,\"1412\":2,\"1413\":3,\"1414\":2,\"1415\":1,\"1416\":3,\"1418\":2,\"1419\":3,\"1420\":3,\"1421\":2,\"1422\":5,\"1423\":3,\"1425\":2,\"1427\":2,\"1429\":2,\"1431\":2,\"1434\":2,\"1436\":2,\"1438\":2,\"1440\":2,\"1441\":5,\"1442\":1,\"1443\":2,\"1444\":1,\"1445\":2,\"1446\":1,\"1447\":2,\"1448\":1,\"1449\":2,\"1450\":4,\"1451\":2,\"1452\":4,\"1453\":2,\"1454\":4,\"1455\":2,\"1456\":4,\"1457\":2,\"1458\":3,\"1459\":2,\"1460\":3,\"1461\":2,\"1462\":1,\"1463\":2,\"1465\":2,\"1466\":1,\"1467\":6,\"1468\":2,\"1469\":1,\"1470\":2,\"1477\":1,\"1484\":2,\"1501\":1,\"1510\":2,\"1512\":2,\"1513\":14,\"1514\":13,\"1515\":8,\"1516\":4,\"1518\":2,\"1519\":13,\"1520\":4,\"1521\":5,\"1523\":2,\"1524\":6,\"1525\":3,\"1526\":22,\"1528\":2,\"1531\":2,\"1533\":5,\"1534\":8,\"1535\":15,\"1536\":12,\"1538\":2,\"1539\":1,\"1540\":2,\"1542\":2,\"1544\":2,\"1545\":5,\"1546\":6,\"1548\":13,\"1549\":3,\"1550\":2,\"1551\":14,\"1552\":82,\"1553\":25,\"1555\":2,\"1556\":25,\"1558\":7,\"1577\":3,\"1581\":4,\"1582\":5,\"1583\":2,\"1584\":7,\"1585\":1,\"1586\":1,\"1587\":9,\"1589\":6,\"1591\":4,\"1592\":13,\"1593\":4,\"1594\":6,\"1595\":7,\"1596\":11,\"1597\":14,\"1598\":1,\"1599\":49,\"1604\":8,\"1605\":10,\"1606\":13,\"1607\":3,\"1608\":2,\"1609\":8,\"1610\":12,\"1611\":9,\"1612\":9,\"1613\":7,\"1614\":3,\"1615\":3,\"1616\":5,\"1618\":6,\"1619\":10,\"1620\":2,\"1621\":3,\"1622\":6,\"1624\":2,\"1625\":2,\"1626\":41,\"1627\":11,\"1628\":13,\"1631\":4,\"1637\":1,\"1639\":2,\"1644\":1,\"1645\":7,\"1647\":2,\"1650\":3,\"1653\":2,\"1654\":1,\"1655\":11,\"1656\":1,\"1658\":2,\"1660\":1,\"1662\":5,\"1663\":2,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":8,\"1669\":1,\"1670\":1,\"1671\":1,\"1676\":2,\"1679\":2,\"1680\":2,\"1681\":1,\"1683\":3,\"1688\":1,\"1692\":3,\"1697\":1,\"1698\":1,\"1702\":2,\"1704\":3,\"1705\":5,\"1706\":8,\"1707\":3,\"1708\":7,\"1709\":8,\"1710\":7,\"1711\":6,\"1712\":9,\"1713\":5,\"1714\":5,\"1715\":8,\"1716\":8,\"1717\":2,\"1719\":26,\"1720\":7,\"1721\":13,\"1722\":1,\"1723\":1,\"1724\":2,\"1725\":28,\"1729\":1,\"1730\":2,\"1731\":4,\"1735\":3,\"1736\":7,\"1737\":6,\"1747\":3,\"1748\":4,\"1749\":5,\"1750\":43,\"1751\":1,\"1752\":3,\"1753\":11,\"1754\":2,\"1756\":4,\"1757\":4,\"1758\":14,\"1759\":1,\"1764\":10,\"1768\":7,\"1770\":3,\"1771\":3,\"1779\":3,\"1782\":1,\"1785\":3,\"1787\":4,\"1788\":6,\"1789\":4,\"1790\":4,\"1794\":4,\"1795\":6,\"1796\":1,\"1798\":2,\"1799\":2,\"1800\":2,\"1803\":1,\"1805\":1,\"1806\":10,\"1807\":1,\"1809\":1,\"1810\":11,\"1811\":4,\"1812\":6,\"1814\":9,\"1815\":4,\"1816\":10,\"1817\":3,\"1821\":1,\"1822\":3,\"1824\":3,\"1833\":3,\"1839\":10,\"1843\":6,\"1851\":5,\"1854\":8,\"1855\":1,\"1856\":6,\"1858\":4,\"1860\":2,\"1861\":1,\"1862\":8,\"1863\":1,\"1866\":1,\"1871\":1,\"1875\":1,\"1878\":5,\"1880\":1,\"1881\":2,\"1883\":2,\"1889\":1,\"1892\":2,\"1893\":1,\"1895\":6,\"1897\":2,\"1901\":4,\"1903\":4,\"1905\":3,\"1907\":5,\"1908\":2,\"1914\":1,\"1916\":1,\"1917\":2,\"1918\":1,\"1919\":2,\"1920\":2,\"1921\":1,\"1926\":1,\"1927\":4,\"1931\":1,\"1932\":1,\"1936\":2,\"1937\":1,\"1938\":2,\"1939\":2,\"1941\":2,\"1943\":2,\"1944\":3,\"1945\":2,\"1946\":3,\"1947\":3,\"1949\":1,\"1950\":2,\"1955\":1,\"1957\":1,\"1958\":3,\"1960\":2,\"1961\":3,\"1962\":2,\"1963\":1,\"1965\":1,\"1966\":1,\"1968\":2,\"1970\":2,\"1973\":2,\"1976\":2,\"1977\":1,\"1979\":2,\"1981\":2,\"1983\":2,\"1986\":2,\"1989\":2,\"1991\":8,\"1992\":23,\"1993\":35,\"1994\":1,\"1995\":20,\"1997\":1,\"2000\":15,\"2001\":12,\"2007\":1,\"2014\":3,\"2015\":4,\"2016\":1,\"2017\":1,\"2018\":6,\"2019\":1,\"2020\":2,\"2021\":2,\"2027\":2,\"2029\":2,\"2031\":2,\"2033\":2,\"2035\":2,\"2039\":5,\"2040\":2,\"2044\":14,\"2045\":2,\"2049\":2,\"2054\":1,\"2061\":1,\"2065\":3,\"2101\":1,\"2116\":1,\"2125\":2,\"2129\":8,\"2130\":20,\"2132\":6,\"2133\":10,\"2134\":5,\"2136\":18,\"2137\":5,\"2139\":2,\"2141\":3,\"2143\":1,\"2144\":1,\"2145\":4,\"2146\":4,\"2147\":4,\"2148\":1,\"2149\":1,\"2155\":7,\"2156\":1,\"2157\":1,\"2162\":5,\"2164\":1,\"2167\":3,\"2168\":1,\"2169\":2,\"2171\":2,\"2173\":2,\"2175\":2,\"2176\":5,\"2178\":2,\"2180\":2,\"2182\":2,\"2183\":6,\"2184\":4,\"2186\":2,\"2187\":3,\"2189\":2,\"2190\":6,\"2191\":9,\"2192\":4,\"2193\":2,\"2195\":2,\"2197\":2,\"2198\":3,\"2199\":2,\"2201\":2,\"2203\":3,\"2204\":2,\"2206\":2,\"2207\":3,\"2208\":6,\"2209\":1,\"2210\":2,\"2212\":2,\"2215\":2,\"2217\":2,\"2218\":8,\"2219\":4,\"2220\":7,\"2223\":26,\"2224\":9,\"2226\":13,\"2227\":9,\"2228\":5,\"2229\":5,\"2231\":14,\"2232\":1,\"2235\":52,\"2236\":55,\"2237\":5,\"2238\":1,\"2239\":58,\"2240\":57,\"2241\":16,\"2245\":69,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":8,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":7,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2276\":1,\"2277\":1,\"2280\":1,\"2286\":1,\"2298\":2,\"2306\":2,\"2307\":5,\"2311\":1,\"2312\":2,\"2323\":1,\"2326\":2,\"2327\":1,\"2335\":1,\"2338\":1,\"2344\":1,\"2350\":1,\"2353\":11,\"2354\":4,\"2355\":19,\"2364\":11,\"2367\":2,\"2369\":2,\"2377\":2,\"2385\":1,\"2402\":2,\"2406\":2,\"2407\":1,\"2408\":2,\"2410\":2,\"2411\":48,\"2412\":63,\"2413\":16,\"2415\":2,\"2417\":2,\"2419\":2,\"2423\":62,\"2424\":16,\"2425\":10,\"2426\":4,\"2427\":6,\"2428\":12,\"2429\":13,\"2430\":4,\"2431\":51,\"2432\":54,\"2433\":8,\"2435\":5,\"2436\":1,\"2438\":2,\"2439\":1,\"2440\":2,\"2442\":2,\"2444\":2,\"2446\":2,\"2447\":49,\"2448\":16,\"2450\":2,\"2452\":2,\"2454\":2,\"2457\":2,\"2459\":2,\"2461\":2,\"2466\":2,\"2468\":2,\"2469\":2,\"2471\":1,\"2473\":2,\"2474\":2,\"2480\":1,\"2482\":3,\"2487\":1,\"2489\":1,\"2490\":2,\"2495\":2}}],[\"dfs\",{\"1\":{\"2437\":1}}],[\"df\",{\"1\":{\"2436\":2}}],[\"df053b8c13c26fe289fc882751801fd781e9d43e\",{\"1\":{\"289\":1}}],[\"d2\",{\"1\":{\"1950\":1}}],[\"ddropout\",{\"1\":{\"2235\":2,\"2236\":2}}],[\"dds\",{\"1\":{\"1616\":3,\"1625\":1,\"1626\":3}}],[\"ddsp\",{\"0\":{\"1557\":1,\"1560\":1,\"1561\":1,\"1562\":1,\"1564\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1575\":1},\"1\":{\"1551\":2,\"1557\":1,\"1560\":1,\"1561\":1,\"1562\":1,\"1564\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1575\":1}}],[\"ddpm\",{\"0\":{\"1303\":1,\"1304\":1},\"1\":{\"1057\":1,\"1240\":1,\"1242\":1,\"1303\":2,\"1304\":2,\"1305\":1}}],[\"ddp\",{\"1\":{\"54\":1,\"57\":1,\"377\":2,\"449\":2,\"2348\":2,\"2370\":4,\"2372\":2}}],[\"dl\",{\"1\":{\"1330\":1}}],[\"dlayers=2\",{\"1\":{\"1750\":1,\"2223\":1}}],[\"dlayers\",{\"1\":{\"22\":1,\"1526\":1,\"1598\":1,\"1599\":2,\"1600\":1,\"1750\":1,\"1815\":2,\"1993\":2,\"2223\":1,\"2235\":2,\"2236\":2,\"2239\":2,\"2240\":2,\"2245\":2,\"2411\":2,\"2412\":2,\"2423\":1,\"2431\":2,\"2432\":2,\"2447\":2}}],[\"dw\",{\"1\":{\"1224\":1,\"1225\":1}}],[\"dx\",{\"1\":{\"1224\":1,\"1225\":1}}],[\"d1\",{\"1\":{\"994\":1,\"1950\":1}}],[\"d12\",{\"1\":{\"243\":1}}],[\"d=4\",{\"1\":{\"2479\":2}}],[\"d=win\",{\"1\":{\"831\":1}}],[\"d=c=1\",{\"1\":{\"831\":1}}],[\"ds\",{\"1\":{\"1590\":2,\"1627\":2,\"1629\":2,\"1764\":2,\"1788\":2,\"2226\":2,\"2241\":2,\"2413\":2,\"2424\":2,\"2448\":2}}],[\"dsp\",{\"1\":{\"1308\":1}}],[\"dst\",{\"1\":{\"290\":1,\"2308\":2}}],[\"dset\",{\"1\":{\"286\":4,\"496\":2}}],[\"dsd\",{\"1\":{\"70\":1}}],[\"dnsmos\",{\"0\":{\"359\":1,\"1128\":2,\"1129\":2,\"1353\":1},\"1\":{\"285\":4,\"356\":13,\"1128\":2,\"1129\":2,\"1353\":1}}],[\"dnn2\",{\"1\":{\"1334\":2}}],[\"dnn1=false\",{\"1\":{\"1334\":1}}],[\"dnn1\",{\"1\":{\"1334\":4}}],[\"dnn\",{\"0\":{\"1054\":1,\"1126\":2,\"1127\":2},\"1\":{\"68\":1,\"70\":1,\"85\":1,\"111\":1,\"119\":1,\"223\":2,\"252\":1,\"720\":1,\"738\":1,\"815\":1,\"1054\":1,\"1126\":4,\"1127\":4,\"1217\":1,\"1539\":1,\"1766\":1,\"2208\":1,\"2209\":1,\"2325\":1,\"2327\":1}}],[\"dt\",{\"1\":{\"819\":2,\"821\":6,\"823\":1,\"824\":4,\"1224\":1,\"1225\":1}}],[\"dtw\",{\"1\":{\"267\":1,\"276\":1,\"286\":1}}],[\"dtype=long\",{\"1\":{\"2155\":1}}],[\"dtype=\",{\"1\":{\"1824\":1}}],[\"dtype=<class\",{\"1\":{\"1011\":1}}],[\"dtype=none\",{\"1\":{\"854\":1,\"993\":1,\"1007\":1,\"1013\":1,\"1028\":1,\"1065\":1,\"2164\":1,\"2323\":1,\"2390\":1,\"2393\":1,\"2397\":1,\"2399\":1}}],[\"dtype=np\",{\"1\":{\"220\":1,\"1824\":1,\"1933\":1}}],[\"dtype=torch\",{\"1\":{\"692\":4,\"790\":2,\"850\":2,\"895\":1,\"924\":1,\"928\":1,\"1373\":1,\"1901\":2,\"1902\":4,\"1903\":2,\"1904\":4,\"1926\":1,\"1992\":2}}],[\"dtype\",{\"0\":{\"896\":1},\"1\":{\"43\":5,\"295\":1,\"301\":2,\"309\":1,\"315\":1,\"321\":2,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"377\":2,\"385\":1,\"389\":2,\"396\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":2,\"457\":1,\"463\":1,\"469\":1,\"475\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":2,\"505\":1,\"896\":1,\"987\":2,\"989\":2,\"991\":1,\"1003\":1,\"1009\":1,\"1015\":1,\"1725\":1,\"1815\":2,\"1824\":1,\"1837\":1,\"1926\":3,\"2043\":2,\"2049\":2,\"2054\":1,\"2055\":2,\"2056\":2,\"2066\":2,\"2133\":3,\"2163\":1,\"2249\":1,\"2322\":1,\"2331\":1,\"2338\":1,\"2339\":4,\"2342\":2,\"2348\":1,\"2351\":2,\"2370\":2,\"2372\":1,\"2378\":1}}],[\"drift\",{\"1\":{\"1225\":1}}],[\"drive\",{\"0\":{\"518\":1},\"1\":{\"518\":3}}],[\"dr\",{\"1\":{\"288\":1,\"2290\":1}}],[\"drastically\",{\"1\":{\"262\":1}}],[\"droppath\",{\"0\":{\"1153\":1},\"1\":{\"1153\":1}}],[\"dropping\",{\"1\":{\"141\":1,\"636\":1,\"661\":1,\"701\":1,\"927\":1,\"939\":1,\"1796\":1,\"1917\":1}}],[\"drop=0\",{\"1\":{\"1064\":2,\"1205\":1,\"1262\":2,\"1290\":2}}],[\"dropinp=0\",{\"1\":{\"828\":1}}],[\"dropinp\",{\"1\":{\"820\":2,\"828\":1}}],[\"drop\",{\"1\":{\"243\":1,\"449\":2,\"700\":1,\"701\":2,\"709\":1,\"733\":1,\"734\":1,\"780\":1,\"820\":3,\"828\":2,\"830\":3,\"846\":3,\"848\":1,\"1029\":3,\"1064\":4,\"1153\":2,\"1235\":3,\"1262\":4,\"1280\":3,\"1281\":3,\"1282\":3,\"1290\":2,\"1796\":3,\"1917\":2,\"1992\":1,\"1995\":1,\"1999\":1,\"2000\":3,\"2001\":3,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":2,\"2008\":1,\"2191\":1,\"2298\":3}}],[\"dropout2d\",{\"1\":{\"820\":1,\"828\":1,\"830\":1}}],[\"dropout=true\",{\"1\":{\"818\":1}}],[\"dropout=false\",{\"1\":{\"744\":1,\"817\":1,\"828\":1,\"830\":1}}],[\"dropout=0\",{\"1\":{\"744\":1,\"784\":1,\"787\":1,\"817\":1,\"828\":1,\"830\":1,\"1029\":2,\"1059\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1164\":1,\"1185\":1,\"1202\":1,\"1211\":1,\"1235\":2,\"1238\":1,\"1240\":1,\"1242\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1279\":1,\"1281\":2,\"1282\":2,\"1548\":1,\"1554\":1,\"1985\":1}}],[\"dropout1d\",{\"1\":{\"730\":1,\"818\":1,\"820\":1,\"828\":1,\"830\":1}}],[\"dropoutnd\",{\"0\":{\"730\":1},\"1\":{\"730\":1}}],[\"dropouts\",{\"1\":{\"699\":1}}],[\"dropout\",{\"1\":{\"43\":18,\"44\":4,\"141\":22,\"142\":16,\"144\":2,\"243\":7,\"290\":1,\"617\":3,\"618\":3,\"620\":3,\"622\":3,\"624\":3,\"625\":3,\"633\":9,\"634\":15,\"638\":3,\"641\":6,\"642\":6,\"643\":9,\"644\":3,\"645\":3,\"651\":3,\"661\":3,\"674\":20,\"692\":6,\"700\":3,\"701\":3,\"706\":3,\"709\":9,\"710\":9,\"711\":9,\"713\":1,\"715\":1,\"730\":2,\"731\":4,\"732\":4,\"733\":3,\"734\":3,\"735\":3,\"746\":7,\"747\":9,\"748\":7,\"749\":1,\"765\":1,\"766\":4,\"767\":4,\"768\":8,\"770\":1,\"771\":3,\"772\":1,\"774\":9,\"775\":4,\"780\":9,\"781\":1,\"783\":1,\"784\":2,\"790\":1,\"791\":1,\"796\":1,\"798\":3,\"818\":3,\"820\":7,\"828\":5,\"830\":4,\"832\":6,\"846\":12,\"847\":6,\"848\":4,\"849\":9,\"850\":4,\"851\":3,\"862\":3,\"959\":1,\"979\":1,\"1029\":6,\"1064\":2,\"1107\":9,\"1117\":3,\"1126\":1,\"1127\":1,\"1130\":3,\"1131\":3,\"1133\":3,\"1134\":2,\"1136\":3,\"1137\":2,\"1139\":2,\"1141\":3,\"1162\":3,\"1185\":2,\"1199\":1,\"1202\":2,\"1208\":3,\"1232\":3,\"1235\":4,\"1255\":2,\"1257\":2,\"1259\":2,\"1261\":3,\"1262\":2,\"1278\":9,\"1279\":2,\"1280\":6,\"1281\":4,\"1282\":4,\"1283\":3,\"1290\":2,\"1389\":1,\"1391\":1,\"1401\":1,\"1403\":1,\"1441\":1,\"1469\":1,\"1517\":1,\"1519\":9,\"1520\":3,\"1524\":3,\"1525\":3,\"1526\":8,\"1535\":9,\"1536\":9,\"1546\":9,\"1552\":15,\"1553\":5,\"1583\":3,\"1598\":11,\"1599\":33,\"1600\":12,\"1610\":4,\"1611\":3,\"1612\":3,\"1613\":3,\"1616\":3,\"1622\":9,\"1625\":6,\"1626\":18,\"1628\":4,\"1683\":3,\"1685\":1,\"1717\":1,\"1735\":3,\"1736\":3,\"1737\":3,\"1738\":3,\"1739\":3,\"1740\":3,\"1741\":3,\"1742\":3,\"1743\":3,\"1744\":3,\"1745\":3,\"1746\":3,\"1748\":2,\"1749\":3,\"1750\":3,\"1751\":3,\"1752\":1,\"1753\":2,\"1756\":3,\"1757\":3,\"1758\":2,\"1759\":3,\"1784\":3,\"1785\":3,\"1786\":3,\"1789\":3,\"1790\":3,\"1794\":3,\"1795\":3,\"1808\":3,\"1809\":3,\"1810\":3,\"1811\":1,\"1812\":1,\"1814\":3,\"1815\":6,\"1816\":3,\"1817\":3,\"1818\":3,\"1820\":3,\"1837\":3,\"1847\":3,\"1863\":6,\"1914\":6,\"1945\":1,\"1947\":3,\"1957\":3,\"1960\":3,\"1961\":3,\"1984\":1,\"1992\":8,\"1993\":3,\"1994\":2,\"1995\":8,\"2126\":3,\"2129\":9,\"2191\":9,\"2223\":3,\"2227\":2,\"2231\":2,\"2235\":8,\"2236\":11,\"2239\":27,\"2240\":24,\"2245\":3,\"2411\":24,\"2412\":36,\"2421\":1,\"2423\":33,\"2428\":3,\"2430\":3,\"2431\":3,\"2432\":30,\"2433\":3,\"2447\":36,\"2458\":1,\"2460\":1}}],[\"dcs\",{\"1\":{\"1717\":1}}],[\"dcunetcomplexencoderblock\",{\"0\":{\"1122\":1},\"1\":{\"1122\":1}}],[\"dcunetcomplexdecoderblock\",{\"0\":{\"1120\":1},\"1\":{\"1120\":1}}],[\"dcunet\",{\"0\":{\"1051\":1,\"1065\":1,\"1084\":1,\"1119\":2,\"1120\":1,\"1122\":1,\"1149\":1,\"1165\":1,\"1226\":1,\"1313\":1,\"1341\":1,\"1366\":1,\"1368\":1},\"1\":{\"1051\":1,\"1065\":1,\"1084\":1,\"1119\":12,\"1120\":1,\"1122\":1,\"1149\":1,\"1165\":1,\"1226\":1,\"1313\":1,\"1341\":1,\"1366\":1,\"1368\":1}}],[\"dccrnseparator\",{\"0\":{\"1118\":1},\"1\":{\"1118\":1}}],[\"dccrn\",{\"0\":{\"1118\":1},\"1\":{\"223\":1,\"1118\":2}}],[\"dc\",{\"0\":{\"1124\":2,\"1125\":2,\"1147\":1,\"1176\":1,\"1180\":1,\"1181\":1},\"1\":{\"223\":2,\"1124\":4,\"1125\":4,\"1147\":1,\"1176\":1,\"1180\":1,\"1181\":1}}],[\"dynamicmixingpreprocessor\",{\"0\":{\"2341\":1},\"1\":{\"2341\":1}}],[\"dynamics\",{\"1\":{\"1050\":1}}],[\"dynamicconvolution2d\",{\"0\":{\"1757\":1},\"1\":{\"1757\":1}}],[\"dynamicconvolution2dtransformerdecoder\",{\"0\":{\"731\":1},\"1\":{\"731\":1}}],[\"dynamicconvolution\",{\"0\":{\"1756\":1},\"1\":{\"1756\":1}}],[\"dynamicconvolutiontransformerdecoder\",{\"0\":{\"732\":1},\"1\":{\"732\":1}}],[\"dynamic\",{\"0\":{\"1756\":1,\"1757\":1,\"1876\":2},\"1\":{\"146\":1,\"147\":6,\"267\":1,\"276\":1,\"286\":1,\"290\":1,\"661\":5,\"1756\":4,\"1757\":4,\"1768\":1,\"1779\":1,\"1876\":3,\"2224\":3,\"2231\":1,\"2245\":6,\"2341\":1,\"2350\":2}}],[\"dynamically\",{\"1\":{\"84\":1,\"2044\":5,\"2148\":1}}],[\"dbidirectional\",{\"1\":{\"2235\":2,\"2236\":2}}],[\"dbunit=true\",{\"1\":{\"1802\":1,\"1852\":1}}],[\"db=\",{\"1\":{\"1547\":1}}],[\"db=20\",{\"1\":{\"1547\":1}}],[\"db=none\",{\"1\":{\"1246\":1,\"1247\":1}}],[\"db=true\",{\"1\":{\"1128\":1}}],[\"db\",{\"1\":{\"109\":1,\"110\":2,\"197\":1,\"206\":2,\"212\":2,\"218\":2,\"236\":1,\"255\":2,\"267\":4,\"268\":2,\"272\":1,\"276\":2,\"286\":2,\"537\":1,\"1246\":3,\"1247\":3,\"1687\":1,\"2336\":1,\"2337\":1,\"2341\":1,\"2346\":1,\"2350\":2,\"2353\":3,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2364\":3,\"2368\":1}}],[\"dprenet\",{\"1\":{\"2432\":6}}],[\"dprnnseparator\",{\"0\":{\"1136\":1},\"1\":{\"1136\":1}}],[\"dprnn\",{\"0\":{\"1134\":2,\"1136\":1,\"1137\":2,\"1257\":1,\"1343\":1,\"1358\":1},\"1\":{\"223\":2,\"225\":1,\"1134\":2,\"1136\":2,\"1137\":2,\"1162\":2,\"1257\":1,\"1343\":1,\"1358\":1}}],[\"dplr\",{\"0\":{\"895\":1},\"1\":{\"895\":1,\"938\":1}}],[\"dpclsolver\",{\"0\":{\"1132\":1},\"1\":{\"1132\":1}}],[\"dpclseparator\",{\"0\":{\"1131\":1},\"1\":{\"1131\":1}}],[\"dpcle2eseparator\",{\"0\":{\"1130\":1},\"1\":{\"1130\":1}}],[\"dpcl\",{\"0\":{\"1130\":1,\"1131\":1,\"1132\":1},\"1\":{\"223\":3,\"1130\":1,\"1131\":1,\"1132\":2,\"1172\":1}}],[\"dptnetseparator\",{\"0\":{\"1141\":1},\"1\":{\"1141\":1}}],[\"dptnet\",{\"0\":{\"1139\":2,\"1141\":1,\"1185\":1},\"1\":{\"223\":2,\"1053\":1,\"1139\":2,\"1141\":2,\"1185\":1}}],[\"dpmulcat\",{\"0\":{\"1133\":2,\"1208\":1},\"1\":{\"223\":1,\"1133\":4,\"1208\":1}}],[\"dp\",{\"0\":{\"2236\":1},\"1\":{\"54\":1,\"265\":1,\"267\":2,\"274\":1,\"276\":3,\"1726\":2,\"1727\":2,\"2236\":1}}],[\"d\",{\"1\":{\"31\":1,\"43\":8,\"162\":4,\"252\":1,\"271\":2,\"280\":2,\"286\":12,\"287\":10,\"545\":1,\"615\":2,\"616\":4,\"617\":8,\"618\":8,\"619\":4,\"620\":11,\"621\":2,\"622\":3,\"623\":3,\"624\":8,\"625\":2,\"626\":2,\"628\":1,\"631\":2,\"632\":6,\"633\":2,\"634\":3,\"636\":5,\"637\":8,\"639\":3,\"640\":2,\"641\":25,\"642\":2,\"643\":24,\"644\":10,\"648\":2,\"649\":8,\"651\":3,\"654\":10,\"675\":1,\"691\":2,\"696\":7,\"697\":6,\"699\":1,\"706\":2,\"710\":3,\"711\":3,\"715\":3,\"722\":1,\"724\":4,\"725\":4,\"726\":4,\"727\":4,\"728\":4,\"729\":3,\"744\":4,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"759\":2,\"768\":6,\"769\":2,\"770\":2,\"771\":1,\"779\":1,\"783\":3,\"784\":6,\"788\":1,\"809\":3,\"811\":1,\"817\":4,\"819\":1,\"821\":2,\"824\":2,\"827\":1,\"828\":7,\"829\":11,\"830\":8,\"831\":4,\"832\":1,\"846\":1,\"847\":27,\"849\":1,\"852\":2,\"856\":3,\"859\":5,\"860\":2,\"925\":1,\"959\":2,\"992\":2,\"994\":1,\"997\":2,\"999\":1,\"1000\":1,\"1002\":1,\"1004\":1,\"1008\":2,\"1012\":1,\"1016\":2,\"1018\":1,\"1020\":2,\"1025\":2,\"1080\":4,\"1117\":2,\"1130\":2,\"1131\":3,\"1132\":1,\"1172\":1,\"1176\":2,\"1389\":1,\"1395\":3,\"1400\":2,\"1401\":1,\"1408\":1,\"1466\":1,\"1480\":1,\"1509\":6,\"1511\":6,\"1516\":6,\"1521\":4,\"1526\":1,\"1529\":3,\"1553\":7,\"1585\":3,\"1590\":2,\"1598\":1,\"1600\":1,\"1625\":1,\"1627\":2,\"1668\":2,\"1700\":1,\"1704\":3,\"1705\":3,\"1706\":3,\"1707\":2,\"1708\":3,\"1710\":3,\"1711\":3,\"1712\":3,\"1713\":3,\"1714\":3,\"1715\":3,\"1716\":3,\"1719\":4,\"1720\":2,\"1721\":1,\"1725\":5,\"1736\":8,\"1749\":11,\"1756\":4,\"1757\":4,\"1764\":2,\"1768\":3,\"1779\":3,\"1784\":2,\"1785\":1,\"1786\":2,\"1788\":2,\"1789\":4,\"1790\":4,\"1794\":6,\"1801\":2,\"1806\":2,\"1808\":2,\"1814\":1,\"1815\":27,\"1817\":1,\"1818\":2,\"1820\":2,\"1837\":2,\"1847\":3,\"1851\":4,\"1862\":1,\"1868\":1,\"1870\":2,\"1880\":2,\"1919\":2,\"1928\":2,\"1958\":1,\"1960\":4,\"1961\":2,\"1984\":1,\"2000\":37,\"2129\":1,\"2209\":1,\"2215\":1,\"2219\":1,\"2226\":2,\"2228\":3,\"2229\":3,\"2241\":2,\"2327\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2355\":3,\"2369\":1,\"2408\":3,\"2413\":2,\"2424\":2,\"2446\":3,\"2448\":2,\"2479\":3}}],[\"dual\",{\"1\":{\"1125\":1,\"1133\":1,\"1134\":1,\"1136\":2,\"1137\":1,\"1139\":1,\"1141\":2,\"1162\":1,\"1185\":1,\"1252\":1,\"1389\":1,\"1391\":3,\"1396\":1,\"1401\":1,\"1403\":3,\"1408\":1,\"1410\":3,\"1466\":1,\"1468\":3}}],[\"dunits=1024\",{\"1\":{\"1750\":1,\"2223\":1}}],[\"dunits\",{\"1\":{\"869\":1,\"1526\":1,\"1598\":1,\"1599\":2,\"1600\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":4,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1750\":1,\"1768\":2,\"1815\":2,\"1895\":2,\"1993\":2,\"2223\":1,\"2235\":2,\"2236\":2,\"2239\":2,\"2240\":2,\"2245\":2,\"2411\":2,\"2412\":2,\"2423\":1,\"2431\":2,\"2432\":2,\"2447\":2}}],[\"dutch\",{\"1\":{\"287\":1,\"481\":1}}],[\"dummy3\",{\"1\":{\"243\":1}}],[\"dummy2\",{\"1\":{\"243\":1}}],[\"dummy\",{\"0\":{\"1761\":1,\"1763\":1,\"1778\":1,\"1797\":1,\"1819\":1,\"1823\":1,\"1834\":1},\"1\":{\"196\":2,\"213\":2,\"243\":2,\"254\":1,\"268\":2,\"277\":2,\"784\":1,\"1228\":1,\"1704\":1,\"1707\":2,\"1713\":1,\"1714\":1,\"1761\":2,\"1763\":3,\"1778\":3,\"1797\":2,\"1801\":2,\"1819\":2,\"1823\":2,\"1834\":3,\"1965\":1,\"2040\":1,\"2043\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2066\":1}}],[\"dumper=noaliassafedumper\",{\"1\":{\"2480\":1}}],[\"dumper\",{\"1\":{\"2480\":1}}],[\"dumped\",{\"1\":{\"286\":1}}],[\"dumpdir\",{\"1\":{\"267\":1,\"285\":1,\"286\":9,\"519\":1}}],[\"dumping\",{\"1\":{\"200\":1,\"211\":1,\"217\":1,\"266\":1,\"275\":1,\"285\":1}}],[\"dump\",{\"0\":{\"519\":1,\"2480\":1,\"2507\":2},\"1\":{\"187\":1,\"199\":1,\"200\":3,\"201\":6,\"204\":1,\"205\":3,\"210\":1,\"211\":1,\"216\":1,\"217\":1,\"218\":2,\"227\":1,\"243\":2,\"259\":3,\"265\":1,\"266\":1,\"267\":11,\"274\":1,\"275\":1,\"276\":10,\"284\":1,\"285\":1,\"286\":28,\"519\":1,\"2480\":2,\"2507\":3}}],[\"dumps\",{\"1\":{\"68\":1}}],[\"dur\",{\"1\":{\"449\":2,\"475\":2,\"484\":2,\"490\":2,\"1526\":3,\"1546\":2,\"1552\":10,\"1553\":4,\"1625\":4,\"1626\":4,\"2239\":2,\"2240\":2}}],[\"durationcalculator\",{\"0\":{\"2407\":1},\"1\":{\"2407\":1}}],[\"durationpredictorloss\",{\"0\":{\"1754\":1},\"1\":{\"1754\":1}}],[\"durationpredictor\",{\"0\":{\"1520\":1,\"1752\":1,\"1974\":1},\"1\":{\"1520\":1,\"1752\":1,\"1974\":1}}],[\"durations=\",{\"1\":{\"921\":1}}],[\"durations\",{\"1\":{\"286\":5,\"696\":2,\"786\":3,\"866\":3,\"882\":2,\"883\":2,\"884\":2,\"921\":2,\"922\":4,\"1585\":6,\"1590\":1,\"1625\":2,\"1626\":1,\"1627\":1,\"1629\":1,\"1753\":2,\"1754\":2,\"1764\":1,\"1788\":3,\"1977\":3,\"2226\":1,\"2227\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2307\":1,\"2405\":2,\"2407\":1,\"2408\":8,\"2409\":2,\"2411\":7,\"2412\":7,\"2413\":1,\"2423\":7,\"2424\":1,\"2434\":2,\"2446\":8,\"2447\":7,\"2448\":1}}],[\"duration\",{\"0\":{\"1520\":1,\"1616\":1,\"1629\":1,\"1752\":1,\"1754\":1,\"2227\":1,\"2407\":1},\"1\":{\"101\":1,\"136\":1,\"211\":2,\"217\":2,\"254\":1,\"266\":2,\"269\":4,\"275\":2,\"278\":4,\"284\":1,\"285\":2,\"286\":1,\"290\":2,\"377\":4,\"449\":2,\"696\":1,\"882\":1,\"883\":1,\"884\":1,\"1002\":2,\"1250\":1,\"1251\":1,\"1520\":4,\"1521\":40,\"1526\":11,\"1529\":9,\"1546\":2,\"1552\":6,\"1553\":8,\"1585\":6,\"1590\":1,\"1598\":6,\"1599\":15,\"1600\":4,\"1616\":5,\"1625\":9,\"1626\":16,\"1627\":2,\"1629\":3,\"1637\":2,\"1697\":1,\"1698\":1,\"1752\":5,\"1753\":2,\"1754\":3,\"1764\":2,\"1768\":3,\"1974\":2,\"1994\":3,\"2000\":3,\"2001\":2,\"2065\":2,\"2150\":3,\"2156\":1,\"2223\":1,\"2224\":1,\"2226\":2,\"2227\":3,\"2228\":48,\"2229\":48,\"2231\":1,\"2232\":4,\"2235\":9,\"2236\":23,\"2238\":6,\"2239\":28,\"2240\":31,\"2241\":2,\"2244\":1,\"2245\":11,\"2307\":2,\"2353\":8,\"2364\":3,\"2407\":3,\"2408\":7,\"2411\":18,\"2412\":17,\"2413\":2,\"2423\":17,\"2424\":2,\"2433\":1,\"2446\":7,\"2447\":17,\"2448\":2}}],[\"during\",{\"1\":{\"0\":1,\"26\":1,\"55\":1,\"94\":1,\"102\":1,\"104\":1,\"106\":1,\"123\":1,\"132\":3,\"139\":4,\"142\":1,\"147\":2,\"148\":1,\"173\":1,\"175\":1,\"197\":2,\"242\":2,\"269\":2,\"278\":2,\"286\":2,\"290\":1,\"616\":1,\"625\":2,\"661\":1,\"696\":4,\"697\":4,\"756\":2,\"773\":2,\"819\":1,\"846\":1,\"866\":1,\"867\":1,\"1334\":1,\"1526\":1,\"1553\":1,\"1598\":2,\"1600\":1,\"1625\":2,\"1719\":1,\"1721\":1,\"1725\":1,\"1759\":1,\"1760\":1,\"1794\":1,\"1862\":1,\"2044\":3,\"2130\":5,\"2131\":2,\"2176\":1,\"2184\":1,\"2411\":1,\"2427\":1,\"2428\":2}}],[\"duplicating\",{\"1\":{\"2162\":1}}],[\"duplication\",{\"1\":{\"71\":1}}],[\"duplicates\",{\"1\":{\"1936\":1,\"2162\":2}}],[\"duplicated\",{\"1\":{\"78\":1}}],[\"due\",{\"1\":{\"36\":1,\"145\":1,\"232\":1,\"258\":1,\"262\":2,\"833\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"1332\":1,\"2162\":1,\"2220\":1}}],[\"duh\",{\"1\":{\"10\":1}}],[\"damo\",{\"1\":{\"1385\":2}}],[\"dampening\",{\"1\":{\"1962\":1}}],[\"damped\",{\"0\":{\"637\":1},\"1\":{\"637\":2}}],[\"damping\",{\"1\":{\"637\":3}}],[\"das\",{\"1\":{\"287\":1}}],[\"dacgenerator\",{\"0\":{\"1391\":1},\"1\":{\"1391\":1}}],[\"dacdiscriminator\",{\"0\":{\"1390\":1},\"1\":{\"1390\":1}}],[\"dac\",{\"0\":{\"1389\":3,\"1390\":2,\"1391\":2},\"1\":{\"219\":1,\"1389\":5,\"1390\":4,\"1391\":7}}],[\"days\",{\"1\":{\"39\":1}}],[\"dalmia\",{\"1\":{\"10\":1,\"12\":1}}],[\"danseparator\",{\"0\":{\"1117\":1},\"1\":{\"1117\":1}}],[\"danielpovey\",{\"1\":{\"2208\":1}}],[\"daniel\",{\"1\":{\"833\":1}}],[\"dan\",{\"0\":{\"1117\":1},\"1\":{\"6\":1,\"10\":1,\"223\":1,\"244\":1,\"1117\":1}}],[\"data2vec\",{\"1\":{\"2216\":1}}],[\"dataiteratorfactory\",{\"0\":{\"2134\":1},\"1\":{\"2134\":2}}],[\"data=none\",{\"1\":{\"1824\":1}}],[\"dataaugmentation\",{\"0\":{\"1655\":1},\"1\":{\"1655\":1}}],[\"datatype\",{\"1\":{\"1301\":1,\"1372\":1}}],[\"datadirwriter\",{\"0\":{\"985\":1},\"1\":{\"985\":1,\"986\":1}}],[\"datadir\",{\"0\":{\"985\":1},\"1\":{\"985\":1}}],[\"dataclass\",{\"0\":{\"2483\":2},\"1\":{\"628\":1,\"2348\":1,\"2372\":1,\"2483\":4}}],[\"dataloading\",{\"1\":{\"242\":1}}],[\"dataloaders\",{\"1\":{\"2355\":10}}],[\"dataloader\",{\"0\":{\"2132\":1,\"2134\":1,\"2139\":1,\"2141\":1,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2162\":1},\"1\":{\"82\":3,\"961\":1,\"1645\":2,\"1650\":2,\"1651\":1,\"2131\":2,\"2132\":1,\"2134\":7,\"2139\":1,\"2141\":1,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2162\":1,\"2246\":3,\"2248\":3,\"2249\":5,\"2250\":3,\"2251\":3,\"2252\":3,\"2253\":3,\"2254\":3,\"2255\":3,\"2256\":3,\"2257\":3,\"2259\":3,\"2260\":3,\"2261\":3,\"2263\":3,\"2264\":3,\"2266\":3,\"2267\":3,\"2268\":3,\"2269\":3,\"2270\":3,\"2271\":3,\"2272\":3,\"2273\":3,\"2355\":18}}],[\"dataparallel\",{\"1\":{\"54\":1,\"2304\":1,\"2305\":1,\"2309\":2}}],[\"databases\",{\"1\":{\"268\":1,\"277\":1}}],[\"database\",{\"1\":{\"38\":1,\"210\":1,\"211\":1,\"246\":1,\"265\":1,\"266\":1,\"274\":1,\"275\":1}}],[\"dataset2utt\",{\"1\":{\"2000\":2,\"2008\":1}}],[\"datasets\",{\"1\":{\"200\":1,\"201\":2,\"2000\":4,\"2132\":7,\"2134\":1}}],[\"dataset\",{\"0\":{\"987\":1,\"989\":1,\"2132\":1,\"2141\":1,\"2158\":2,\"2165\":1,\"2324\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2342\":1,\"2344\":1,\"2345\":1,\"2349\":1,\"2351\":1,\"2366\":1,\"2390\":1,\"2391\":1,\"2392\":1,\"2393\":1,\"2394\":1,\"2396\":1,\"2397\":1,\"2399\":1},\"1\":{\"15\":1,\"79\":1,\"82\":6,\"96\":1,\"107\":1,\"184\":1,\"187\":1,\"200\":3,\"201\":5,\"212\":1,\"236\":1,\"243\":2,\"246\":2,\"267\":4,\"268\":1,\"269\":4,\"276\":1,\"277\":1,\"278\":4,\"284\":2,\"286\":2,\"290\":5,\"978\":1,\"987\":1,\"988\":3,\"989\":1,\"990\":3,\"1567\":1,\"1643\":1,\"1644\":1,\"1645\":7,\"1646\":1,\"1647\":1,\"1650\":1,\"1824\":2,\"2000\":25,\"2001\":4,\"2008\":2,\"2130\":1,\"2132\":10,\"2141\":5,\"2249\":5,\"2324\":2,\"2329\":1,\"2330\":1,\"2331\":1,\"2342\":2,\"2343\":2,\"2344\":5,\"2345\":4,\"2349\":1,\"2351\":2,\"2352\":2,\"2354\":1,\"2355\":3,\"2366\":1,\"2377\":4,\"2390\":1,\"2391\":1,\"2392\":1,\"2393\":1,\"2394\":1,\"2396\":1,\"2397\":1,\"2399\":1}}],[\"data\",{\"0\":{\"74\":1,\"77\":1,\"79\":1,\"82\":1,\"196\":1,\"213\":1,\"268\":1,\"277\":1,\"303\":1,\"311\":1,\"317\":1,\"323\":1,\"329\":1,\"333\":1,\"337\":1,\"339\":1,\"344\":1,\"345\":1,\"347\":1,\"351\":1,\"352\":1,\"354\":1,\"358\":1,\"363\":1,\"364\":1,\"366\":1,\"370\":1,\"379\":1,\"387\":1,\"391\":1,\"398\":1,\"408\":1,\"423\":1,\"431\":1,\"438\":1,\"444\":1,\"451\":1,\"459\":1,\"465\":1,\"471\":1,\"477\":1,\"486\":1,\"492\":1,\"500\":1,\"507\":1,\"528\":1,\"880\":1,\"886\":1,\"2088\":1,\"2155\":1,\"2164\":1},\"1\":{\"6\":2,\"38\":3,\"39\":1,\"54\":1,\"56\":1,\"69\":2,\"70\":3,\"71\":2,\"78\":2,\"79\":10,\"81\":18,\"82\":17,\"91\":2,\"96\":5,\"97\":4,\"98\":9,\"99\":4,\"100\":4,\"101\":2,\"106\":2,\"107\":1,\"108\":1,\"111\":1,\"118\":3,\"120\":1,\"124\":2,\"136\":1,\"173\":2,\"174\":1,\"190\":2,\"194\":1,\"195\":3,\"196\":17,\"197\":28,\"199\":2,\"200\":10,\"201\":1,\"204\":2,\"205\":11,\"210\":2,\"211\":11,\"212\":3,\"213\":15,\"216\":2,\"217\":11,\"218\":2,\"222\":3,\"223\":7,\"224\":9,\"227\":3,\"228\":3,\"234\":1,\"235\":3,\"236\":1,\"240\":3,\"242\":16,\"243\":12,\"244\":1,\"246\":1,\"253\":1,\"254\":7,\"259\":2,\"261\":2,\"263\":1,\"265\":2,\"266\":10,\"267\":9,\"268\":17,\"269\":9,\"274\":2,\"275\":11,\"276\":10,\"277\":17,\"278\":9,\"284\":3,\"285\":14,\"286\":7,\"290\":5,\"301\":4,\"309\":4,\"315\":4,\"321\":4,\"327\":4,\"331\":4,\"335\":4,\"342\":4,\"349\":4,\"361\":4,\"368\":4,\"377\":2,\"385\":4,\"389\":4,\"396\":4,\"404\":2,\"406\":4,\"421\":4,\"429\":4,\"436\":4,\"442\":4,\"449\":4,\"457\":2,\"463\":4,\"469\":4,\"475\":4,\"484\":4,\"490\":4,\"496\":4,\"498\":4,\"505\":4,\"515\":2,\"516\":5,\"521\":3,\"523\":5,\"524\":5,\"525\":5,\"526\":2,\"527\":1,\"528\":1,\"531\":1,\"533\":1,\"535\":2,\"537\":2,\"554\":1,\"639\":6,\"704\":1,\"746\":1,\"756\":1,\"768\":2,\"773\":1,\"804\":1,\"831\":1,\"833\":1,\"880\":1,\"886\":1,\"932\":1,\"934\":1,\"978\":1,\"985\":1,\"999\":2,\"1000\":1,\"1006\":2,\"1009\":1,\"1010\":8,\"1014\":1,\"1018\":2,\"1053\":2,\"1062\":2,\"1107\":2,\"1117\":2,\"1118\":2,\"1125\":1,\"1126\":6,\"1127\":4,\"1130\":2,\"1131\":2,\"1132\":1,\"1136\":2,\"1141\":2,\"1162\":2,\"1217\":2,\"1228\":1,\"1232\":2,\"1252\":2,\"1261\":2,\"1267\":2,\"1268\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1278\":2,\"1280\":2,\"1283\":2,\"1334\":2,\"1400\":1,\"1462\":1,\"1519\":1,\"1524\":3,\"1655\":4,\"1668\":1,\"1719\":1,\"1722\":1,\"1725\":1,\"1806\":1,\"1807\":2,\"1846\":1,\"1951\":1,\"1998\":1,\"2043\":2,\"2049\":1,\"2055\":2,\"2056\":2,\"2066\":2,\"2088\":1,\"2124\":1,\"2128\":1,\"2130\":24,\"2131\":8,\"2132\":1,\"2133\":6,\"2134\":6,\"2136\":7,\"2137\":5,\"2139\":1,\"2141\":1,\"2142\":3,\"2143\":8,\"2149\":2,\"2155\":1,\"2159\":1,\"2162\":1,\"2164\":2,\"2166\":1,\"2215\":1,\"2246\":7,\"2247\":7,\"2248\":7,\"2249\":10,\"2250\":7,\"2251\":7,\"2252\":7,\"2253\":7,\"2254\":7,\"2255\":7,\"2256\":7,\"2257\":7,\"2258\":2,\"2259\":7,\"2260\":7,\"2261\":7,\"2262\":4,\"2263\":7,\"2264\":7,\"2265\":6,\"2266\":7,\"2267\":7,\"2268\":7,\"2269\":7,\"2270\":7,\"2271\":7,\"2272\":7,\"2273\":7,\"2309\":1,\"2323\":1,\"2336\":3,\"2337\":3,\"2338\":1,\"2343\":1,\"2344\":1,\"2345\":1,\"2346\":3,\"2350\":2,\"2352\":2,\"2355\":10,\"2356\":3,\"2362\":3,\"2366\":2,\"2368\":3,\"2376\":1,\"2436\":1,\"2438\":3,\"2440\":3,\"2443\":1,\"2460\":1,\"2480\":2,\"2481\":1,\"2507\":1}}],[\"dilateddepthseparableconv\",{\"0\":{\"1583\":1},\"1\":{\"1583\":2}}],[\"dilated\",{\"1\":{\"1530\":1,\"1583\":1,\"1604\":1,\"1605\":2,\"1606\":1,\"1610\":1,\"1615\":1,\"1628\":1,\"1733\":1,\"1854\":1}}],[\"dilations=\",{\"1\":{\"1548\":1,\"2213\":1,\"2214\":1}}],[\"dilations\",{\"1\":{\"1392\":2,\"1458\":3,\"1460\":3,\"1509\":1,\"1511\":1,\"1513\":3,\"1526\":1,\"1530\":1,\"1543\":1,\"1548\":1,\"1551\":3,\"1552\":3,\"1553\":2,\"1592\":3,\"1598\":1,\"1599\":3,\"1600\":1,\"1614\":2,\"1625\":1,\"1626\":3,\"2209\":1,\"2426\":1}}],[\"dilation=\",{\"1\":{\"1433\":1,\"1435\":1}}],[\"dilation=none\",{\"1\":{\"1103\":1,\"2179\":1,\"2185\":1}}],[\"dilation=1\",{\"1\":{\"1080\":1,\"1236\":1,\"1265\":1,\"1300\":1,\"1302\":1,\"1304\":1,\"1346\":1,\"1347\":1,\"1486\":1,\"1563\":1,\"1733\":1}}],[\"dilation\",{\"1\":{\"43\":2,\"141\":1,\"620\":2,\"774\":3,\"973\":1,\"981\":1,\"1120\":1,\"1122\":1,\"1148\":1,\"1265\":2,\"1272\":1,\"1389\":1,\"1391\":1,\"1392\":1,\"1396\":1,\"1398\":1,\"1401\":1,\"1403\":1,\"1404\":1,\"1408\":1,\"1410\":1,\"1442\":1,\"1444\":1,\"1450\":3,\"1452\":3,\"1454\":3,\"1456\":3,\"1466\":1,\"1468\":1,\"1482\":1,\"1548\":1,\"1552\":6,\"1553\":2,\"1554\":1,\"1556\":3,\"1609\":5,\"1610\":1,\"1611\":3,\"1612\":3,\"1613\":3,\"1614\":2,\"1615\":4,\"1619\":3,\"1621\":1,\"1625\":2,\"1626\":6,\"1628\":4,\"1668\":2,\"1736\":2,\"1854\":7,\"2426\":2,\"2458\":1,\"2460\":1}}],[\"dildcunet\",{\"1\":{\"1119\":1}}],[\"digits\",{\"1\":{\"1000\":1}}],[\"did\",{\"1\":{\"290\":1}}],[\"dicriminative\",{\"1\":{\"2471\":1}}],[\"dicussion\",{\"1\":{\"1608\":1}}],[\"dic\",{\"1\":{\"269\":2,\"278\":2,\"2354\":7}}],[\"dicts\",{\"1\":{\"2308\":1}}],[\"dictionaries\",{\"1\":{\"2039\":2,\"2131\":1}}],[\"dictionary\",{\"1\":{\"38\":1,\"81\":1,\"211\":1,\"266\":1,\"275\":1,\"285\":1,\"286\":1,\"583\":1,\"589\":1,\"699\":1,\"748\":1,\"760\":1,\"912\":2,\"1354\":1,\"1477\":1,\"1965\":1,\"2049\":2,\"2130\":2,\"2131\":2,\"2132\":2,\"2142\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2149\":1,\"2156\":1,\"2161\":1,\"2280\":1,\"2355\":4}}],[\"dictornone\",{\"1\":{\"1053\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1162\":1,\"1217\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":1,\"1280\":1,\"1283\":1,\"1334\":1}}],[\"dictconfig\",{\"1\":{\"925\":1}}],[\"dict\",{\"0\":{\"86\":1,\"533\":1,\"589\":1,\"913\":1,\"942\":1,\"1916\":1,\"1953\":1,\"2106\":1,\"2308\":1,\"2478\":1,\"2481\":1,\"2489\":1},\"1\":{\"46\":2,\"78\":1,\"79\":2,\"82\":3,\"86\":2,\"106\":1,\"141\":3,\"142\":2,\"269\":2,\"278\":2,\"526\":3,\"531\":1,\"533\":1,\"578\":1,\"581\":1,\"583\":1,\"586\":3,\"589\":1,\"614\":8,\"617\":1,\"618\":1,\"619\":1,\"622\":1,\"624\":1,\"625\":3,\"626\":3,\"628\":1,\"631\":2,\"633\":2,\"634\":19,\"636\":1,\"637\":1,\"639\":1,\"642\":1,\"643\":2,\"655\":2,\"656\":2,\"657\":2,\"658\":1,\"659\":2,\"660\":1,\"661\":1,\"662\":1,\"666\":1,\"671\":2,\"672\":1,\"673\":2,\"698\":1,\"699\":2,\"720\":1,\"736\":1,\"737\":1,\"738\":3,\"743\":1,\"746\":2,\"748\":3,\"752\":1,\"760\":1,\"763\":2,\"777\":1,\"787\":2,\"791\":1,\"794\":1,\"795\":2,\"796\":1,\"815\":1,\"824\":1,\"847\":2,\"913\":1,\"930\":1,\"942\":2,\"954\":3,\"958\":3,\"974\":2,\"1008\":1,\"1019\":1,\"1021\":1,\"1022\":2,\"1024\":1,\"1026\":3,\"1040\":1,\"1042\":3,\"1044\":1,\"1053\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1125\":1,\"1130\":1,\"1131\":1,\"1132\":1,\"1136\":1,\"1141\":1,\"1155\":3,\"1156\":2,\"1157\":4,\"1158\":4,\"1162\":1,\"1167\":1,\"1204\":3,\"1209\":2,\"1217\":1,\"1228\":2,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1268\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1278\":1,\"1280\":2,\"1283\":2,\"1334\":2,\"1354\":1,\"1381\":3,\"1389\":12,\"1390\":1,\"1391\":3,\"1392\":2,\"1395\":9,\"1396\":6,\"1397\":3,\"1401\":12,\"1402\":7,\"1403\":4,\"1408\":12,\"1409\":8,\"1420\":2,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":6,\"1452\":6,\"1454\":4,\"1456\":4,\"1458\":4,\"1460\":4,\"1466\":12,\"1467\":6,\"1468\":3,\"1477\":1,\"1508\":2,\"1509\":3,\"1511\":4,\"1513\":2,\"1521\":12,\"1526\":44,\"1539\":1,\"1548\":1,\"1549\":8,\"1551\":2,\"1553\":38,\"1576\":2,\"1582\":2,\"1585\":8,\"1592\":2,\"1593\":2,\"1594\":4,\"1595\":6,\"1596\":2,\"1597\":2,\"1598\":23,\"1599\":2,\"1600\":29,\"1604\":4,\"1605\":4,\"1606\":6,\"1609\":2,\"1610\":2,\"1614\":2,\"1615\":4,\"1618\":2,\"1619\":2,\"1624\":2,\"1625\":23,\"1640\":2,\"1641\":2,\"1644\":1,\"1645\":1,\"1647\":1,\"1655\":3,\"1681\":2,\"1702\":3,\"1719\":20,\"1720\":6,\"1721\":6,\"1722\":6,\"1725\":29,\"1726\":4,\"1727\":4,\"1749\":2,\"1762\":1,\"1775\":2,\"1798\":1,\"1799\":1,\"1800\":1,\"1806\":6,\"1807\":4,\"1815\":2,\"1843\":2,\"1862\":6,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1871\":2,\"1876\":1,\"1911\":2,\"1913\":1,\"1914\":2,\"1916\":4,\"1921\":2,\"1932\":1,\"1937\":1,\"1940\":2,\"1942\":2,\"1950\":4,\"1951\":2,\"1953\":2,\"1954\":2,\"1955\":2,\"1959\":2,\"1965\":2,\"1966\":1,\"1971\":3,\"1973\":1,\"1975\":3,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1992\":1,\"1993\":3,\"1996\":2,\"1997\":2,\"2008\":1,\"2010\":2,\"2011\":2,\"2012\":2,\"2013\":2,\"2020\":3,\"2039\":8,\"2049\":2,\"2127\":2,\"2130\":2,\"2131\":8,\"2132\":1,\"2134\":1,\"2139\":1,\"2142\":1,\"2143\":5,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2149\":1,\"2156\":3,\"2161\":1,\"2184\":2,\"2215\":1,\"2216\":3,\"2219\":3,\"2221\":2,\"2222\":3,\"2228\":10,\"2229\":10,\"2232\":1,\"2235\":24,\"2236\":24,\"2238\":1,\"2239\":24,\"2240\":24,\"2245\":24,\"2246\":4,\"2247\":4,\"2248\":4,\"2249\":7,\"2250\":4,\"2251\":4,\"2252\":4,\"2253\":5,\"2254\":4,\"2255\":4,\"2256\":4,\"2257\":4,\"2259\":4,\"2260\":4,\"2261\":4,\"2263\":4,\"2264\":4,\"2265\":2,\"2266\":4,\"2267\":4,\"2268\":4,\"2269\":4,\"2270\":4,\"2271\":4,\"2272\":4,\"2273\":4,\"2280\":1,\"2287\":1,\"2288\":1,\"2293\":1,\"2308\":5,\"2309\":1,\"2325\":2,\"2327\":8,\"2329\":1,\"2334\":1,\"2338\":3,\"2342\":2,\"2345\":1,\"2347\":2,\"2351\":2,\"2354\":4,\"2355\":4,\"2356\":1,\"2359\":5,\"2363\":1,\"2365\":2,\"2366\":3,\"2367\":2,\"2369\":3,\"2371\":2,\"2376\":2,\"2377\":1,\"2402\":1,\"2403\":3,\"2406\":1,\"2408\":10,\"2410\":1,\"2411\":5,\"2412\":5,\"2415\":1,\"2417\":1,\"2419\":1,\"2423\":5,\"2431\":5,\"2432\":5,\"2435\":1,\"2443\":1,\"2445\":3,\"2446\":10,\"2447\":5,\"2449\":1,\"2458\":1,\"2460\":1,\"2462\":4,\"2463\":1,\"2478\":2,\"2481\":2,\"2489\":1}}],[\"dio\",{\"0\":{\"2404\":2},\"1\":{\"267\":5,\"276\":3,\"2404\":4}}],[\"dialect\",{\"1\":{\"2280\":1}}],[\"dialog\",{\"1\":{\"2044\":4}}],[\"dialogue\",{\"1\":{\"245\":1,\"246\":2,\"2044\":2,\"2054\":1}}],[\"diagnose\",{\"1\":{\"2143\":1}}],[\"diag=none\",{\"1\":{\"1246\":1}}],[\"diagonalize\",{\"1\":{\"924\":1}}],[\"diagonal=true\",{\"1\":{\"895\":1}}],[\"diagonal\",{\"1\":{\"823\":1,\"824\":1,\"1126\":1,\"1127\":1,\"1217\":1,\"1246\":1,\"1309\":3,\"1310\":3,\"1311\":3,\"1318\":3,\"1319\":3,\"1321\":3,\"1322\":3,\"1323\":3,\"1327\":3,\"1329\":1,\"1330\":3,\"1334\":2,\"1770\":2,\"1771\":2,\"1811\":1}}],[\"diag\",{\"1\":{\"821\":8,\"824\":1,\"1126\":1,\"1127\":1,\"1217\":2,\"1246\":1,\"1309\":2,\"1310\":2,\"1311\":2,\"1318\":2,\"1319\":2,\"1321\":2,\"1322\":2,\"1323\":2,\"1327\":2,\"1329\":1,\"1330\":2}}],[\"diarize\",{\"0\":{\"340\":1}}],[\"diarizationtask\",{\"0\":{\"2251\":1},\"1\":{\"2251\":1}}],[\"diarization\",{\"1\":{\"335\":1,\"955\":1,\"974\":2,\"976\":1,\"977\":1,\"979\":1,\"980\":1,\"1155\":1,\"1157\":1,\"2184\":1,\"2456\":1}}],[\"diarisation\",{\"0\":{\"221\":1,\"229\":1}}],[\"diar\",{\"0\":{\"335\":1,\"341\":1,\"963\":1,\"965\":1,\"967\":2,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"978\":1,\"979\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"984\":1,\"2251\":1,\"2520\":1},\"1\":{\"227\":1,\"228\":1,\"335\":3,\"402\":1,\"403\":2,\"954\":1,\"963\":1,\"965\":1,\"967\":2,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":3,\"975\":1,\"976\":1,\"977\":1,\"978\":1,\"979\":1,\"980\":2,\"981\":1,\"982\":1,\"983\":1,\"984\":1,\"1156\":2,\"2251\":1}}],[\"diar1\",{\"1\":{\"221\":1,\"227\":1,\"229\":1}}],[\"dinosr\",{\"1\":{\"232\":1,\"258\":1,\"2216\":1}}],[\"diffwave\",{\"1\":{\"1149\":1}}],[\"diffusionstepembedding\",{\"0\":{\"1149\":1},\"1\":{\"1149\":1}}],[\"diffusion\",{\"0\":{\"1032\":2,\"1050\":1,\"1116\":1,\"1155\":1,\"1161\":1,\"1189\":1,\"1218\":1,\"1221\":1,\"1224\":1,\"1225\":1,\"1229\":1,\"1244\":1,\"1245\":1,\"1253\":2,\"1292\":1},\"1\":{\"1032\":2,\"1050\":1,\"1116\":1,\"1149\":1,\"1155\":2,\"1161\":1,\"1189\":1,\"1218\":1,\"1221\":1,\"1224\":1,\"1225\":1,\"1229\":1,\"1244\":1,\"1245\":2,\"1253\":4,\"1292\":1,\"2131\":1,\"2423\":7,\"2426\":2,\"2428\":5}}],[\"diffsinger\",{\"1\":{\"265\":1,\"267\":6}}],[\"difficult\",{\"1\":{\"232\":1,\"258\":1,\"262\":1,\"267\":1,\"276\":1,\"286\":1}}],[\"differ\",{\"1\":{\"138\":1}}],[\"differs\",{\"1\":{\"127\":1}}],[\"differencefunctiontorch\",{\"0\":{\"2439\":1},\"1\":{\"2439\":1}}],[\"differencefunction\",{\"0\":{\"2438\":1,\"2440\":1},\"1\":{\"2438\":1,\"2439\":1,\"2440\":1}}],[\"differences\",{\"0\":{\"232\":1,\"258\":1},\"1\":{\"46\":1,\"722\":1,\"1396\":1}}],[\"difference\",{\"1\":{\"45\":1,\"104\":2,\"145\":1,\"197\":1,\"203\":1,\"267\":1,\"276\":1,\"286\":1,\"616\":1,\"696\":1,\"697\":1,\"1002\":1,\"1920\":1,\"2019\":1,\"2309\":1,\"2435\":2,\"2436\":3,\"2438\":2,\"2440\":2}}],[\"differentiation\",{\"1\":{\"756\":1,\"773\":1,\"866\":1,\"867\":1}}],[\"differentiating\",{\"1\":{\"756\":1,\"773\":1,\"866\":1,\"867\":1}}],[\"differentiable\",{\"1\":{\"262\":1}}],[\"different\",{\"1\":{\"25\":1,\"43\":3,\"52\":1,\"55\":1,\"68\":1,\"94\":1,\"101\":1,\"123\":1,\"128\":2,\"135\":2,\"146\":1,\"173\":1,\"193\":1,\"197\":1,\"223\":1,\"232\":1,\"242\":3,\"254\":1,\"258\":1,\"261\":1,\"265\":1,\"267\":9,\"269\":2,\"274\":1,\"275\":1,\"278\":2,\"286\":1,\"634\":1,\"821\":1,\"944\":1,\"993\":1,\"994\":1,\"1011\":1,\"1155\":3,\"1157\":3,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"1415\":1,\"1668\":1,\"1716\":1,\"1753\":1,\"1938\":1,\"2130\":4,\"2162\":1,\"2184\":1,\"2344\":1,\"2421\":1}}],[\"dirs\",{\"0\":{\"293\":1},\"1\":{\"293\":1}}],[\"direcotry\",{\"1\":{\"276\":1}}],[\"direct\",{\"1\":{\"1185\":1,\"1246\":1,\"1992\":1,\"1993\":1,\"1994\":2,\"2132\":2}}],[\"direction\",{\"1\":{\"1008\":1,\"1582\":1,\"1624\":1,\"1691\":1}}],[\"directories\",{\"1\":{\"24\":2,\"197\":2,\"200\":2,\"201\":2,\"205\":1,\"211\":2,\"217\":2,\"218\":1,\"224\":1,\"235\":1,\"242\":1,\"254\":1,\"266\":2,\"267\":1,\"275\":2,\"276\":1,\"285\":2,\"286\":1,\"293\":1}}],[\"directory>\",{\"1\":{\"124\":3}}],[\"directory\",{\"0\":{\"37\":1,\"108\":2,\"109\":1,\"124\":1,\"196\":1,\"213\":1,\"268\":1,\"277\":1},\"1\":{\"3\":9,\"22\":2,\"38\":4,\"69\":2,\"79\":1,\"109\":1,\"124\":1,\"134\":1,\"150\":1,\"162\":1,\"194\":1,\"195\":3,\"196\":5,\"197\":2,\"200\":1,\"201\":3,\"205\":1,\"206\":2,\"210\":1,\"211\":1,\"212\":2,\"213\":7,\"217\":1,\"218\":7,\"222\":1,\"224\":4,\"226\":1,\"235\":1,\"236\":1,\"243\":4,\"254\":1,\"255\":2,\"265\":1,\"266\":1,\"267\":10,\"268\":8,\"274\":1,\"275\":1,\"276\":10,\"277\":8,\"285\":2,\"286\":12,\"293\":1,\"518\":1,\"527\":2,\"745\":1,\"747\":1,\"985\":1,\"1949\":1,\"2134\":1,\"2139\":1,\"2157\":1}}],[\"directly\",{\"1\":{\"3\":1,\"47\":1,\"49\":1,\"106\":1,\"119\":1,\"168\":1,\"225\":1,\"259\":1,\"263\":1,\"269\":2,\"278\":2,\"286\":1,\"290\":1,\"756\":1,\"768\":1,\"773\":1,\"2136\":1,\"2235\":1,\"2236\":1,\"2438\":1,\"2440\":1}}],[\"dir=\",{\"1\":{\"161\":1,\"2134\":1}}],[\"dir\",{\"0\":{\"528\":1,\"2071\":1,\"2077\":1,\"2098\":1},\"1\":{\"69\":2,\"73\":1,\"76\":4,\"128\":1,\"134\":4,\"136\":1,\"162\":1,\"196\":4,\"213\":4,\"223\":2,\"224\":2,\"242\":1,\"243\":2,\"267\":14,\"268\":4,\"276\":14,\"277\":4,\"284\":1,\"286\":38,\"290\":4,\"293\":4,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"356\":2,\"361\":2,\"368\":2,\"377\":2,\"385\":2,\"389\":2,\"396\":2,\"404\":2,\"406\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":2,\"457\":2,\"461\":2,\"463\":2,\"469\":2,\"475\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":2,\"505\":2,\"515\":1,\"516\":7,\"518\":2,\"520\":1,\"521\":1,\"523\":7,\"524\":7,\"525\":7,\"527\":1,\"528\":1,\"531\":1,\"533\":1,\"535\":2,\"537\":2,\"543\":2,\"570\":1,\"745\":2,\"746\":3,\"747\":2,\"759\":1,\"790\":1,\"791\":1,\"815\":1,\"864\":1,\"889\":1,\"890\":1,\"891\":1,\"1539\":1,\"1949\":2,\"1951\":1,\"2008\":1,\"2134\":2,\"2139\":2,\"2157\":2,\"2333\":1,\"2338\":1,\"2339\":2,\"2348\":1,\"2354\":1,\"2359\":1,\"2365\":1,\"2369\":1,\"2370\":2,\"2372\":1}}],[\"dispatcher\",{\"1\":{\"938\":1}}],[\"display\",{\"1\":{\"321\":2,\"653\":1}}],[\"displays\",{\"1\":{\"247\":1}}],[\"displayed\",{\"1\":{\"3\":1,\"19\":1,\"247\":1}}],[\"disallowed\",{\"1\":{\"290\":2}}],[\"disables\",{\"1\":{\"2480\":1}}],[\"disabled\",{\"1\":{\"82\":1,\"246\":1,\"629\":1,\"2355\":1}}],[\"disable\",{\"1\":{\"56\":1,\"92\":1,\"161\":1,\"315\":2,\"469\":2,\"1008\":1,\"1720\":1,\"2136\":2,\"2480\":1}}],[\"discuss\",{\"1\":{\"1963\":1}}],[\"disciminator\",{\"1\":{\"1618\":1}}],[\"disc\",{\"1\":{\"1511\":1,\"1549\":2}}],[\"disc=\",{\"1\":{\"823\":1}}],[\"discrimininative\",{\"1\":{\"2470\":1}}],[\"discrimination\",{\"1\":{\"2176\":1}}],[\"discriminative\",{\"1\":{\"1131\":1,\"1172\":1,\"2462\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":2}}],[\"discriminatoraversarialloss\",{\"1\":{\"1584\":1}}],[\"discriminatoradversarialloss\",{\"0\":{\"1584\":1},\"1\":{\"1584\":1}}],[\"discriminatorstft\",{\"0\":{\"1392\":1},\"1\":{\"1392\":1}}],[\"discriminators\",{\"1\":{\"1389\":3,\"1396\":3,\"1401\":3,\"1402\":1,\"1408\":3,\"1415\":2,\"1416\":2,\"1420\":1,\"1423\":2,\"1466\":3,\"1467\":1,\"1526\":3,\"1553\":3,\"1584\":3,\"1587\":3,\"1591\":3,\"1594\":1,\"1595\":1,\"1598\":3,\"1600\":3,\"1618\":1,\"1625\":3}}],[\"discriminator\",{\"0\":{\"1383\":2,\"1385\":2,\"1386\":2,\"1392\":2,\"1411\":2,\"1413\":2,\"1415\":2,\"1417\":2,\"1420\":2,\"1422\":2,\"1426\":1,\"1471\":2,\"1472\":2,\"1482\":2,\"2451\":2,\"2458\":2,\"2469\":1},\"1\":{\"286\":3,\"1381\":1,\"1383\":2,\"1385\":5,\"1386\":2,\"1389\":5,\"1390\":6,\"1392\":3,\"1395\":1,\"1396\":3,\"1397\":7,\"1401\":5,\"1402\":11,\"1408\":5,\"1409\":12,\"1411\":2,\"1413\":4,\"1415\":2,\"1417\":2,\"1420\":5,\"1422\":3,\"1426\":1,\"1466\":4,\"1467\":10,\"1471\":2,\"1472\":2,\"1482\":2,\"1508\":1,\"1509\":1,\"1511\":1,\"1514\":3,\"1515\":1,\"1516\":1,\"1521\":1,\"1526\":12,\"1534\":5,\"1541\":1,\"1543\":1,\"1549\":10,\"1553\":14,\"1576\":1,\"1584\":10,\"1585\":1,\"1587\":5,\"1591\":3,\"1593\":5,\"1594\":7,\"1595\":10,\"1596\":1,\"1597\":2,\"1598\":13,\"1599\":1,\"1600\":12,\"1604\":1,\"1606\":2,\"1609\":1,\"1618\":4,\"1625\":13,\"2327\":3,\"2348\":2,\"2451\":2,\"2458\":3,\"2462\":2,\"2469\":2,\"2470\":2,\"2471\":1,\"2473\":1}}],[\"discrimiantor\",{\"1\":{\"2327\":4}}],[\"discretize\",{\"1\":{\"1245\":2}}],[\"discretized\",{\"1\":{\"207\":1,\"824\":1}}],[\"discretization\",{\"1\":{\"1224\":1,\"1225\":1,\"1245\":2}}],[\"discret\",{\"1\":{\"276\":1}}],[\"discreteloss\",{\"0\":{\"2226\":1},\"1\":{\"2226\":1}}],[\"discreteaudioio\",{\"0\":{\"2136\":1},\"1\":{\"2136\":2}}],[\"discrete\",{\"0\":{\"203\":1,\"291\":1,\"412\":1,\"737\":1,\"1992\":1,\"2226\":1,\"2228\":1,\"2230\":1,\"2234\":1,\"2239\":1,\"2447\":1},\"1\":{\"203\":1,\"204\":1,\"205\":6,\"232\":1,\"258\":1,\"273\":1,\"274\":1,\"275\":6,\"276\":11,\"286\":2,\"475\":2,\"737\":1,\"1992\":7,\"1995\":5,\"2130\":12,\"2133\":2,\"2136\":4,\"2137\":2,\"2138\":2,\"2143\":1,\"2226\":1,\"2228\":17,\"2239\":18,\"2363\":2,\"2443\":1,\"2445\":1,\"2446\":13,\"2447\":11,\"2448\":1,\"2449\":1}}],[\"discarded\",{\"1\":{\"831\":1,\"2136\":1,\"2145\":1}}],[\"discard\",{\"1\":{\"106\":1,\"1643\":1,\"1646\":1}}],[\"discarding\",{\"1\":{\"106\":2}}],[\"disk\",{\"1\":{\"173\":1,\"2354\":3}}],[\"disributed\",{\"1\":{\"71\":1}}],[\"distrib\",{\"0\":{\"1475\":1,\"1477\":1,\"1478\":1,\"1488\":1,\"1496\":1,\"1501\":1,\"1502\":1,\"1507\":1},\"1\":{\"1475\":1,\"1477\":1,\"1478\":1,\"1488\":1,\"1496\":1,\"1501\":1,\"1502\":1,\"1507\":1}}],[\"distribution\",{\"1\":{\"911\":3,\"1224\":3,\"1225\":3,\"1245\":3,\"1373\":1,\"1907\":1,\"2001\":1,\"2307\":1,\"2312\":1}}],[\"distributions\",{\"0\":{\"160\":1},\"1\":{\"160\":1,\"846\":2,\"2000\":2,\"2220\":2}}],[\"distribute\",{\"1\":{\"173\":1}}],[\"distributedoption\",{\"0\":{\"2340\":1},\"1\":{\"2249\":4,\"2253\":1,\"2340\":1,\"2347\":2,\"2354\":1,\"2365\":2,\"2369\":3,\"2371\":2}}],[\"distributeddataparallel\",{\"1\":{\"54\":4,\"106\":1,\"1502\":1}}],[\"distributed\",{\"0\":{\"54\":1,\"56\":1,\"58\":1,\"62\":1,\"64\":1,\"121\":1,\"381\":1,\"453\":1,\"1488\":1,\"1499\":1,\"2340\":1,\"2380\":1,\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2388\":1,\"2389\":1,\"2395\":2},\"1\":{\"54\":5,\"56\":5,\"57\":1,\"58\":3,\"60\":1,\"61\":3,\"62\":1,\"63\":1,\"64\":1,\"70\":1,\"93\":2,\"106\":1,\"121\":2,\"153\":1,\"173\":1,\"200\":2,\"205\":2,\"211\":1,\"217\":1,\"242\":2,\"254\":1,\"263\":1,\"266\":1,\"275\":1,\"285\":1,\"374\":3,\"377\":2,\"449\":2,\"1488\":1,\"1499\":1,\"2008\":1,\"2130\":1,\"2132\":1,\"2134\":3,\"2141\":1,\"2162\":3,\"2249\":4,\"2253\":1,\"2258\":2,\"2318\":1,\"2320\":1,\"2340\":6,\"2347\":2,\"2354\":3,\"2355\":3,\"2365\":2,\"2369\":3,\"2371\":2,\"2380\":1,\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":3,\"2385\":3,\"2386\":1,\"2387\":1,\"2388\":1,\"2389\":1,\"2395\":2}}],[\"distance=128\",{\"1\":{\"787\":1}}],[\"distance\",{\"1\":{\"286\":1,\"851\":1,\"1400\":1,\"1469\":1}}],[\"distillation\",{\"1\":{\"286\":2}}],[\"distinct\",{\"1\":{\"1422\":1}}],[\"distinction\",{\"1\":{\"79\":1}}],[\"distinguish\",{\"1\":{\"242\":1,\"710\":1,\"711\":1}}],[\"distortions\",{\"1\":{\"1332\":1}}],[\"distortionless\",{\"1\":{\"1309\":1,\"1311\":1,\"1321\":1,\"1322\":1}}],[\"distortion=none\",{\"1\":{\"1126\":1}}],[\"distortion\",{\"1\":{\"267\":2,\"276\":2,\"285\":1,\"286\":1,\"1066\":1,\"1126\":1,\"1210\":1,\"1246\":2,\"1264\":1,\"1327\":2,\"1330\":4,\"1334\":1,\"1354\":2,\"1674\":1,\"1676\":1}}],[\"dist\",{\"1\":{\"54\":3,\"58\":8,\"59\":2,\"60\":6,\"61\":12,\"62\":3,\"63\":3,\"64\":3,\"377\":13,\"449\":13,\"2340\":14,\"2354\":1}}],[\"divisor\",{\"1\":{\"2009\":1}}],[\"divisors=\",{\"1\":{\"1514\":1,\"1534\":1}}],[\"divisors\",{\"1\":{\"1511\":1,\"1514\":2,\"1534\":2,\"1549\":1}}],[\"division\",{\"1\":{\"1316\":1,\"1320\":1}}],[\"divisible\",{\"1\":{\"1119\":2}}],[\"dividing\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1}}],[\"divide\",{\"0\":{\"517\":1,\"2319\":1},\"1\":{\"517\":2,\"821\":1,\"824\":1,\"1558\":1,\"2319\":1}}],[\"divided\",{\"1\":{\"118\":1,\"517\":1,\"786\":1,\"800\":1,\"921\":1,\"935\":1,\"2220\":1}}],[\"diversity\",{\"0\":{\"2471\":1},\"1\":{\"246\":1,\"2471\":2}}],[\"diverse\",{\"1\":{\"202\":1,\"252\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"2131\":1,\"2184\":1}}],[\"divergence\",{\"1\":{\"44\":4,\"1553\":1,\"1601\":3,\"1602\":2,\"1625\":1,\"1936\":1}}],[\"div\",{\"0\":{\"888\":1},\"1\":{\"44\":2,\"888\":1,\"1936\":2}}],[\"dimenstion\",{\"1\":{\"1809\":1}}],[\"dimensions\",{\"1\":{\"759\":1,\"768\":1,\"824\":1,\"831\":1,\"939\":1,\"1271\":1,\"1292\":1,\"1350\":2,\"1387\":1,\"1748\":1,\"1878\":1,\"1960\":1,\"1961\":1,\"2426\":1,\"2432\":1}}],[\"dimensionality\",{\"1\":{\"821\":1,\"1268\":1,\"1274\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"2183\":1,\"2187\":1,\"2192\":1,\"2203\":1,\"2209\":1}}],[\"dimensional\",{\"1\":{\"637\":1,\"786\":1,\"800\":1,\"867\":1,\"921\":1,\"935\":1,\"1757\":3,\"1790\":3}}],[\"dimension\",{\"1\":{\"43\":8,\"44\":1,\"95\":1,\"96\":2,\"97\":1,\"128\":1,\"141\":6,\"286\":1,\"620\":2,\"621\":1,\"689\":1,\"691\":1,\"692\":1,\"701\":1,\"703\":2,\"706\":1,\"709\":2,\"710\":1,\"711\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"735\":1,\"745\":1,\"747\":1,\"748\":2,\"755\":1,\"759\":1,\"760\":1,\"768\":2,\"771\":1,\"774\":2,\"779\":1,\"780\":2,\"785\":1,\"804\":1,\"817\":1,\"819\":2,\"820\":1,\"827\":2,\"828\":1,\"829\":4,\"830\":3,\"831\":1,\"846\":2,\"847\":1,\"849\":1,\"852\":1,\"856\":1,\"859\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"919\":1,\"932\":1,\"934\":1,\"980\":2,\"1029\":4,\"1046\":1,\"1048\":1,\"1053\":1,\"1061\":1,\"1062\":1,\"1070\":2,\"1071\":2,\"1073\":1,\"1080\":1,\"1107\":2,\"1117\":3,\"1118\":2,\"1124\":1,\"1125\":1,\"1127\":1,\"1130\":3,\"1131\":3,\"1133\":3,\"1134\":3,\"1136\":2,\"1137\":3,\"1139\":3,\"1141\":2,\"1162\":3,\"1181\":1,\"1185\":2,\"1192\":1,\"1194\":1,\"1202\":1,\"1208\":2,\"1232\":2,\"1235\":3,\"1252\":2,\"1255\":2,\"1257\":2,\"1259\":3,\"1261\":2,\"1267\":2,\"1268\":3,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":2,\"1279\":5,\"1280\":6,\"1281\":6,\"1282\":5,\"1283\":5,\"1292\":1,\"1356\":1,\"1392\":1,\"1400\":1,\"1441\":3,\"1450\":3,\"1452\":3,\"1454\":3,\"1456\":3,\"1458\":1,\"1460\":1,\"1462\":1,\"1469\":3,\"1519\":2,\"1526\":2,\"1533\":1,\"1535\":1,\"1536\":1,\"1546\":1,\"1552\":1,\"1553\":2,\"1577\":2,\"1598\":2,\"1599\":3,\"1600\":2,\"1622\":1,\"1625\":2,\"1626\":1,\"1668\":1,\"1686\":1,\"1694\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":2,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1735\":1,\"1736\":2,\"1738\":2,\"1739\":2,\"1740\":2,\"1741\":2,\"1742\":2,\"1743\":2,\"1744\":2,\"1745\":2,\"1746\":2,\"1748\":1,\"1749\":1,\"1750\":2,\"1751\":1,\"1753\":1,\"1759\":1,\"1768\":1,\"1779\":4,\"1783\":1,\"1784\":1,\"1786\":1,\"1803\":1,\"1808\":1,\"1810\":2,\"1812\":2,\"1814\":1,\"1815\":2,\"1816\":1,\"1818\":1,\"1820\":1,\"1837\":1,\"1847\":1,\"1851\":2,\"1854\":1,\"1863\":2,\"1878\":1,\"1895\":1,\"1901\":1,\"1902\":1,\"1903\":1,\"1904\":1,\"1913\":1,\"1914\":1,\"1937\":1,\"1958\":1,\"1960\":1,\"1961\":1,\"1992\":2,\"1993\":5,\"1995\":2,\"2007\":1,\"2129\":1,\"2130\":4,\"2133\":2,\"2155\":1,\"2167\":1,\"2176\":1,\"2183\":1,\"2187\":2,\"2188\":1,\"2190\":1,\"2191\":2,\"2192\":2,\"2198\":1,\"2203\":2,\"2207\":1,\"2208\":1,\"2209\":2,\"2218\":3,\"2219\":1,\"2220\":1,\"2223\":2,\"2235\":5,\"2236\":5,\"2239\":5,\"2240\":5,\"2245\":5,\"2309\":2,\"2411\":3,\"2412\":3,\"2421\":1,\"2423\":3,\"2425\":1,\"2428\":2,\"2429\":2,\"2430\":2,\"2431\":5,\"2432\":4,\"2433\":1,\"2447\":3}}],[\"dimention\",{\"1\":{\"1269\":1,\"1270\":1}}],[\"dims\",{\"1\":{\"1119\":2,\"2428\":3}}],[\"dim=2\",{\"1\":{\"1437\":1}}],[\"dim=256\",{\"1\":{\"1029\":1,\"1070\":1,\"1071\":1,\"1235\":1,\"1279\":1,\"1281\":1,\"1282\":1}}],[\"dim=0\",{\"1\":{\"1355\":1,\"1474\":1,\"1495\":1}}],[\"dim=48\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1}}],[\"dim=481\",{\"1\":{\"1061\":1}}],[\"dim=4\",{\"1\":{\"1190\":1}}],[\"dim=100\",{\"1\":{\"1522\":1}}],[\"dim=1\",{\"1\":{\"1190\":1,\"1545\":1,\"2355\":2}}],[\"dim=512\",{\"1\":{\"1182\":1,\"1183\":1,\"1269\":1,\"1270\":1,\"1758\":1,\"2227\":1,\"2231\":1}}],[\"dim=none\",{\"1\":{\"839\":1,\"1120\":1,\"1122\":1,\"1215\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1784\":1}}],[\"dim=\",{\"1\":{\"688\":1,\"1298\":1,\"1325\":1,\"1490\":1,\"1783\":1,\"1901\":1,\"1903\":1}}],[\"dim1layernorm\",{\"0\":{\"722\":1},\"1\":{\"722\":1,\"887\":1}}],[\"dim1\",{\"1\":{\"97\":1,\"702\":1,\"1076\":1,\"1133\":2,\"1623\":1}}],[\"dim2\",{\"1\":{\"82\":1,\"97\":1,\"1076\":1,\"1133\":2,\"1623\":1}}],[\"dim\",{\"0\":{\"875\":1,\"887\":1},\"1\":{\"44\":1,\"82\":2,\"286\":2,\"634\":1,\"674\":12,\"692\":1,\"710\":1,\"711\":1,\"722\":1,\"730\":1,\"745\":1,\"746\":4,\"747\":1,\"748\":1,\"749\":3,\"752\":1,\"754\":2,\"771\":1,\"777\":2,\"787\":1,\"796\":1,\"817\":2,\"820\":2,\"821\":2,\"824\":2,\"828\":1,\"830\":1,\"846\":6,\"849\":1,\"851\":2,\"869\":1,\"875\":2,\"887\":1,\"947\":3,\"949\":3,\"976\":2,\"978\":4,\"980\":8,\"1029\":2,\"1035\":1,\"1053\":2,\"1054\":1,\"1059\":4,\"1061\":1,\"1062\":2,\"1063\":3,\"1064\":4,\"1070\":3,\"1071\":3,\"1073\":2,\"1084\":2,\"1103\":2,\"1107\":2,\"1113\":1,\"1114\":2,\"1117\":2,\"1118\":2,\"1119\":1,\"1124\":2,\"1125\":2,\"1130\":2,\"1131\":2,\"1136\":2,\"1141\":2,\"1149\":1,\"1156\":2,\"1159\":1,\"1162\":8,\"1164\":4,\"1165\":2,\"1182\":1,\"1183\":1,\"1184\":1,\"1190\":1,\"1198\":1,\"1200\":2,\"1208\":2,\"1213\":1,\"1217\":1,\"1223\":1,\"1232\":2,\"1235\":2,\"1236\":2,\"1251\":1,\"1252\":3,\"1261\":2,\"1262\":4,\"1267\":6,\"1268\":14,\"1269\":4,\"1270\":4,\"1271\":3,\"1274\":4,\"1278\":2,\"1279\":1,\"1280\":5,\"1281\":2,\"1282\":2,\"1283\":4,\"1286\":2,\"1290\":2,\"1316\":3,\"1320\":2,\"1383\":2,\"1389\":1,\"1391\":2,\"1396\":1,\"1400\":2,\"1401\":1,\"1403\":2,\"1404\":1,\"1406\":1,\"1408\":1,\"1410\":1,\"1441\":1,\"1458\":2,\"1460\":2,\"1466\":1,\"1468\":1,\"1469\":6,\"1473\":1,\"1519\":3,\"1526\":5,\"1529\":6,\"1535\":3,\"1536\":3,\"1543\":1,\"1545\":1,\"1546\":5,\"1552\":4,\"1553\":3,\"1598\":2,\"1599\":4,\"1600\":1,\"1622\":5,\"1625\":3,\"1626\":4,\"1660\":2,\"1664\":1,\"1665\":1,\"1691\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":6,\"1713\":4,\"1714\":4,\"1715\":4,\"1716\":4,\"1748\":2,\"1758\":1,\"1768\":2,\"1783\":2,\"1784\":2,\"1815\":2,\"1863\":1,\"1868\":1,\"1901\":1,\"1903\":1,\"1914\":2,\"1958\":1,\"1960\":3,\"1961\":3,\"1974\":1,\"1977\":2,\"1992\":4,\"1993\":6,\"1994\":1,\"1995\":4,\"2129\":1,\"2130\":2,\"2133\":1,\"2167\":1,\"2168\":1,\"2176\":1,\"2183\":2,\"2190\":2,\"2203\":1,\"2207\":1,\"2208\":2,\"2218\":7,\"2219\":3,\"2220\":2,\"2227\":1,\"2231\":1,\"2232\":2,\"2235\":8,\"2236\":9,\"2239\":8,\"2240\":8,\"2245\":8,\"2411\":4,\"2412\":4,\"2421\":3,\"2423\":4,\"2429\":3,\"2430\":6,\"2431\":6,\"2432\":6,\"2447\":4,\"2458\":1,\"2460\":2,\"2472\":2}}],[\"double\",{\"0\":{\"1364\":1},\"1\":{\"1126\":1,\"1127\":1,\"1364\":1,\"1511\":1,\"1534\":2,\"1549\":1}}],[\"doens\",{\"1\":{\"821\":1}}],[\"does\",{\"1\":{\"46\":1,\"101\":1,\"175\":1,\"242\":1,\"267\":1,\"276\":1,\"286\":1,\"536\":1,\"699\":1,\"974\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1218\":1,\"1221\":1,\"1376\":1,\"1377\":1,\"1704\":1,\"1707\":2,\"1713\":1,\"1714\":1,\"1801\":2,\"2146\":1,\"2188\":1,\"2280\":1,\"2411\":1,\"2480\":1}}],[\"doesn\",{\"1\":{\"3\":1,\"79\":1,\"96\":1,\"108\":2,\"162\":1,\"196\":1,\"197\":1,\"213\":1,\"249\":1,\"268\":1,\"269\":1,\"277\":1,\"278\":1,\"821\":1,\"833\":1,\"1485\":1,\"1502\":1,\"1647\":1,\"1962\":1,\"2016\":1,\"2133\":1,\"2134\":1,\"2249\":1,\"2253\":1}}],[\"domain=\",{\"1\":{\"1534\":1,\"1547\":1}}],[\"domain\",{\"0\":{\"1066\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1210\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1275\":1,\"1276\":1,\"1277\":1},\"1\":{\"223\":2,\"225\":1,\"269\":2,\"278\":2,\"768\":2,\"1066\":1,\"1155\":3,\"1157\":3,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":2,\"1175\":1,\"1210\":5,\"1246\":1,\"1247\":1,\"1248\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1275\":2,\"1276\":2,\"1277\":2,\"1321\":1,\"1322\":1,\"1334\":1,\"1401\":2,\"1403\":2,\"1511\":1,\"1534\":2,\"1549\":1,\"1668\":4,\"1752\":1,\"1753\":6,\"1754\":4,\"1755\":2,\"2235\":1,\"2236\":1}}],[\"doing\",{\"1\":{\"819\":1}}],[\"doi=\",{\"1\":{\"207\":1}}],[\"doi\",{\"1\":{\"156\":1,\"1309\":1,\"1311\":1,\"1330\":1}}],[\"down=1\",{\"1\":{\"1369\":1}}],[\"down=false\",{\"1\":{\"1110\":1,\"1238\":1}}],[\"downpool2d\",{\"0\":{\"727\":1},\"1\":{\"727\":1}}],[\"downpool\",{\"0\":{\"726\":1},\"1\":{\"726\":1}}],[\"downlinearpool\",{\"0\":{\"725\":1},\"1\":{\"725\":1}}],[\"download=1\",{\"1\":{\"290\":1}}],[\"downloader\",{\"1\":{\"286\":1}}],[\"downloaded>\",{\"1\":{\"24\":1}}],[\"downloaded\",{\"1\":{\"24\":1,\"26\":1,\"110\":1,\"243\":1,\"518\":2}}],[\"downloading\",{\"1\":{\"161\":1,\"261\":1,\"267\":1,\"746\":1,\"2049\":1}}],[\"downloads\",{\"1\":{\"110\":1,\"212\":1,\"286\":3,\"518\":1,\"2045\":1,\"2355\":1}}],[\"download\",{\"0\":{\"518\":1,\"889\":1,\"890\":1,\"891\":1,\"2075\":1},\"1\":{\"22\":1,\"24\":1,\"38\":1,\"125\":1,\"128\":1,\"136\":1,\"201\":2,\"206\":1,\"218\":1,\"243\":4,\"255\":1,\"267\":2,\"276\":1,\"285\":1,\"286\":7,\"290\":1,\"518\":4,\"745\":1,\"746\":1,\"747\":1,\"759\":1,\"790\":1,\"791\":1,\"815\":1,\"864\":1,\"889\":1,\"890\":1,\"891\":1,\"1539\":1,\"2355\":1}}],[\"downavgpool\",{\"0\":{\"724\":1},\"1\":{\"724\":1}}],[\"down\",{\"0\":{\"1110\":1,\"1301\":1,\"1306\":1,\"1331\":1,\"1344\":1,\"1345\":1,\"1371\":1,\"1372\":1,\"2009\":1},\"1\":{\"704\":1,\"927\":1,\"1110\":1,\"1301\":1,\"1306\":1,\"1331\":1,\"1344\":1,\"1345\":1,\"1370\":2,\"1371\":1,\"1372\":1,\"1692\":1,\"2009\":1}}],[\"downspectralpool\",{\"0\":{\"729\":1},\"1\":{\"729\":1}}],[\"downsamples\",{\"1\":{\"1306\":1}}],[\"downsample=false\",{\"1\":{\"807\":1}}],[\"downsample=none\",{\"1\":{\"693\":1}}],[\"downsample\",{\"0\":{\"728\":1,\"892\":1,\"893\":1,\"894\":1,\"1151\":1,\"1301\":1,\"1306\":1,\"1344\":1},\"1\":{\"728\":1,\"892\":1,\"893\":1,\"894\":1,\"1151\":1,\"1155\":1,\"1157\":1,\"1301\":2,\"1306\":2,\"1344\":1,\"1389\":2,\"1390\":2,\"1401\":6,\"1402\":8,\"1408\":6,\"1409\":8,\"1420\":2,\"1466\":4,\"1467\":6,\"1526\":6,\"1548\":2,\"1549\":8,\"1552\":4,\"1553\":6,\"1593\":2,\"1594\":6,\"1595\":8,\"1596\":4,\"1597\":4,\"1598\":6,\"1600\":6,\"1604\":4,\"1606\":8,\"1618\":2,\"1625\":6,\"2472\":1}}],[\"downsampling\",{\"1\":{\"128\":1,\"699\":3,\"1110\":1,\"1301\":1,\"1306\":2,\"1402\":1,\"1409\":1,\"1454\":1,\"1456\":1,\"1467\":1,\"1549\":2,\"1552\":2,\"1594\":1,\"1595\":1,\"1596\":2,\"1597\":3,\"1604\":2,\"1606\":3,\"2350\":1}}],[\"downstream\",{\"1\":{\"223\":1,\"228\":1,\"233\":1,\"2354\":1}}],[\"done\",{\"1\":{\"126\":1,\"175\":1,\"224\":1,\"286\":1,\"768\":1,\"1450\":1,\"1452\":1,\"1668\":1,\"1720\":1,\"1721\":1,\"2006\":1,\"2365\":1}}],[\"don\",{\"1\":{\"67\":1,\"69\":1,\"79\":1,\"91\":1,\"98\":1,\"104\":1,\"106\":1,\"110\":2,\"141\":1,\"152\":1,\"162\":1,\"173\":1,\"196\":1,\"197\":1,\"213\":1,\"268\":1,\"277\":1,\"2131\":1,\"2133\":1,\"2355\":4}}],[\"dongji\",{\"1\":{\"14\":1}}],[\"dot\",{\"1\":{\"53\":1,\"644\":1,\"784\":1,\"1707\":2,\"1714\":1,\"1785\":1,\"1794\":3,\"1817\":1}}],[\"do\",{\"1\":{\"26\":1,\"32\":1,\"51\":1,\"78\":1,\"106\":1,\"119\":1,\"126\":1,\"137\":1,\"144\":1,\"168\":2,\"197\":2,\"200\":1,\"201\":1,\"223\":1,\"243\":4,\"286\":1,\"287\":2,\"791\":1,\"1051\":1,\"1207\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1334\":1,\"1356\":3,\"2049\":1,\"2176\":2,\"2262\":1,\"2355\":5,\"2462\":1}}],[\"doctor\",{\"1\":{\"288\":1,\"2290\":1}}],[\"doctoc\",{\"1\":{\"194\":1}}],[\"dockerfile\",{\"1\":{\"26\":1,\"29\":3}}],[\"docker\",{\"0\":{\"21\":1,\"22\":1},\"1\":{\"22\":18,\"23\":3,\"24\":15,\"25\":4,\"26\":8,\"27\":2,\"29\":3,\"67\":4,\"249\":1}}],[\"docstring\",{\"1\":{\"32\":1,\"2367\":1}}],[\"docstrings\",{\"0\":{\"32\":1},\"1\":{\"3\":2,\"31\":2,\"32\":4}}],[\"docs\",{\"1\":{\"3\":2,\"18\":1,\"31\":1,\"33\":1,\"60\":1,\"67\":2,\"92\":2,\"104\":1,\"200\":1,\"201\":1,\"205\":1,\"217\":1,\"223\":1,\"242\":1,\"285\":1,\"1002\":1,\"1053\":1}}],[\"doc\",{\"1\":{\"1\":2,\"2\":1,\"3\":6,\"31\":1,\"32\":1,\"33\":1,\"55\":1,\"123\":1,\"162\":2,\"173\":1,\"196\":1}}],[\"documents\",{\"1\":{\"217\":1}}],[\"documentation\",{\"1\":{\"2\":1,\"3\":5,\"31\":1,\"152\":1,\"161\":1,\"162\":2,\"195\":1,\"260\":1,\"1883\":1}}],[\"document\",{\"0\":{\"30\":1},\"1\":{\"0\":1,\"3\":1,\"36\":1,\"616\":3,\"696\":3,\"697\":3,\"1061\":1,\"1062\":1,\"1131\":1,\"1172\":1,\"1309\":1,\"1311\":1,\"1321\":1,\"1322\":1,\"1327\":2,\"1330\":2}}],[\"deemphasis\",{\"0\":{\"1686\":1},\"1\":{\"1686\":1}}],[\"deepspeedtraineroptions\",{\"0\":{\"2339\":1},\"1\":{\"2338\":4,\"2339\":1}}],[\"deepspeedtrainer\",{\"0\":{\"2135\":1,\"2338\":1},\"1\":{\"2338\":1}}],[\"deepspeed\",{\"0\":{\"2135\":1,\"2338\":1,\"2339\":1},\"1\":{\"1949\":1,\"2131\":1,\"2338\":2,\"2339\":3,\"2340\":1,\"2355\":1}}],[\"deep\",{\"1\":{\"285\":2,\"828\":2,\"851\":1,\"939\":1,\"1117\":2,\"1125\":1,\"1130\":2,\"1131\":3,\"1134\":1,\"1137\":1,\"1170\":1,\"1172\":3,\"1750\":1,\"1770\":1,\"2130\":1,\"2167\":1,\"2198\":1,\"2224\":1}}],[\"deeper\",{\"1\":{\"262\":1}}],[\"deeplearning\",{\"1\":{\"67\":1}}],[\"demucs\",{\"1\":{\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1}}],[\"demonstration\",{\"1\":{\"190\":1,\"193\":2,\"536\":2}}],[\"demos\",{\"1\":{\"10\":2}}],[\"demo\",{\"0\":{\"177\":1},\"1\":{\"3\":1,\"178\":4,\"179\":3,\"180\":1,\"181\":1,\"182\":1,\"190\":1,\"193\":2,\"245\":1,\"247\":2,\"248\":1,\"290\":1,\"536\":2}}],[\"dequantization\",{\"1\":{\"1957\":1}}],[\"dequantize\",{\"1\":{\"1400\":1}}],[\"dequntization\",{\"1\":{\"1395\":1}}],[\"dead\",{\"1\":{\"1389\":1,\"1391\":1,\"1396\":1,\"1400\":3,\"1401\":1,\"1403\":1,\"1406\":1,\"1408\":1,\"1410\":1,\"1441\":3,\"1466\":1,\"1468\":1,\"1469\":3}}],[\"dealing\",{\"1\":{\"1270\":1}}],[\"dealt\",{\"1\":{\"106\":1}}],[\"deal\",{\"1\":{\"102\":1}}],[\"de\",{\"1\":{\"515\":1,\"535\":1,\"1548\":1,\"1686\":3}}],[\"density\",{\"0\":{\"1326\":1},\"1\":{\"1224\":2,\"1225\":2,\"1245\":2,\"1326\":2}}],[\"dense=\",{\"1\":{\"1264\":1,\"1334\":1}}],[\"dense=32\",{\"1\":{\"1264\":1,\"1334\":1}}],[\"densenet\",{\"1\":{\"1145\":3,\"1264\":2,\"1334\":2}}],[\"denseblock\",{\"0\":{\"1145\":1},\"1\":{\"1145\":1}}],[\"denselyconnectedblock\",{\"0\":{\"1147\":1},\"1\":{\"1124\":12,\"1125\":12,\"1147\":2}}],[\"densely\",{\"1\":{\"1124\":1,\"1125\":1,\"1147\":1}}],[\"dense\",{\"0\":{\"1144\":1},\"1\":{\"821\":1,\"1144\":1,\"1145\":2,\"1264\":2,\"1334\":2,\"1987\":1,\"2133\":2,\"2469\":2,\"2471\":2,\"2473\":4}}],[\"denoised\",{\"1\":{\"2428\":1}}],[\"denoise\",{\"1\":{\"2428\":1}}],[\"denoiser\",{\"0\":{\"2420\":1,\"2426\":1,\"2428\":1,\"2442\":1},\"1\":{\"2420\":1,\"2423\":2,\"2426\":2,\"2428\":2,\"2442\":1}}],[\"denoising\",{\"1\":{\"1050\":1,\"1116\":1,\"1161\":1,\"1189\":1,\"1218\":1,\"1221\":1,\"1229\":1,\"1244\":1,\"1279\":2,\"1280\":5,\"1281\":2,\"1283\":5,\"1309\":1,\"1311\":1,\"1327\":3,\"1330\":3,\"2428\":1}}],[\"denom\",{\"1\":{\"755\":4,\"878\":2,\"879\":2,\"881\":2,\"882\":2,\"883\":2,\"884\":2,\"919\":3,\"931\":1,\"933\":1}}],[\"denominator\",{\"1\":{\"615\":1,\"640\":1,\"648\":1,\"666\":1,\"755\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"919\":2}}],[\"denormalized\",{\"1\":{\"267\":1,\"276\":1,\"286\":1}}],[\"denorm\",{\"1\":{\"267\":1,\"276\":1,\"286\":1}}],[\"denote\",{\"1\":{\"2000\":2}}],[\"denoted\",{\"1\":{\"817\":2,\"827\":1}}],[\"denotes\",{\"1\":{\"246\":1,\"824\":1}}],[\"denoting\",{\"1\":{\"242\":1,\"1350\":1}}],[\"denisov\",{\"1\":{\"12\":1}}],[\"deriving\",{\"1\":{\"1951\":1}}],[\"derive\",{\"1\":{\"1950\":1,\"2184\":1}}],[\"derived\",{\"1\":{\"162\":1,\"242\":1,\"286\":1,\"2016\":1}}],[\"derivative=0\",{\"1\":{\"1634\":1,\"1635\":1,\"1636\":1}}],[\"derivatives\",{\"1\":{\"1634\":1,\"1635\":1,\"1636\":1}}],[\"dereverb1\",{\"1\":{\"1217\":2}}],[\"dereverberated\",{\"1\":{\"224\":2}}],[\"dereverberation\",{\"1\":{\"223\":2,\"224\":1,\"1127\":1,\"1279\":2,\"1280\":5,\"1281\":2,\"1283\":5,\"1309\":1,\"1311\":1}}],[\"dereverb\",{\"1\":{\"223\":4,\"224\":2,\"1066\":1,\"1157\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":2,\"1175\":1,\"1210\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1275\":1,\"1276\":2,\"1277\":1,\"1280\":4,\"1283\":4,\"2346\":2,\"2368\":2}}],[\"debian11\",{\"1\":{\"160\":1}}],[\"debugger\",{\"1\":{\"263\":1}}],[\"debugging\",{\"1\":{\"39\":1,\"263\":1,\"2143\":1}}],[\"debug\",{\"1\":{\"126\":1,\"243\":1,\"249\":1,\"263\":1,\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"377\":1,\"385\":1,\"389\":1,\"396\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":1,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"511\":1,\"1229\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"1862\":1}}],[\"dedicated\",{\"1\":{\"138\":1,\"852\":1}}],[\"deltas\",{\"0\":{\"1703\":1,\"1857\":2,\"1875\":1},\"1\":{\"1703\":2,\"1857\":3,\"1875\":1,\"1876\":1}}],[\"delta=0\",{\"1\":{\"1590\":1}}],[\"delta\",{\"0\":{\"1875\":1},\"1\":{\"821\":1,\"1846\":1,\"1875\":1}}],[\"delay=3\",{\"1\":{\"1376\":1,\"1853\":1}}],[\"delay\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1127\":1,\"1217\":1,\"1314\":3,\"1352\":1,\"1354\":1,\"1356\":1,\"1376\":2,\"1377\":3,\"1539\":1,\"1766\":1,\"2136\":3}}],[\"delegate\",{\"1\":{\"1938\":1,\"2325\":1,\"2327\":1}}],[\"deleforge\",{\"1\":{\"691\":1}}],[\"delete\",{\"1\":{\"545\":1}}],[\"deletion\",{\"1\":{\"175\":1}}],[\"deliberationencoder\",{\"1\":{\"2127\":1}}],[\"delimiter\",{\"1\":{\"481\":2,\"1008\":1,\"2292\":1,\"2293\":1,\"2336\":1,\"2337\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1}}],[\"deliver\",{\"1\":{\"242\":1}}],[\"dell\",{\"1\":{\"67\":1}}],[\"delcroix\",{\"1\":{\"15\":1}}],[\"deta\",{\"1\":{\"1875\":1}}],[\"detail\",{\"1\":{\"119\":1,\"195\":1,\"206\":1,\"212\":1,\"218\":1,\"255\":1,\"267\":2,\"276\":2,\"285\":1,\"286\":2,\"927\":1,\"1810\":1,\"1812\":1,\"1883\":1,\"2130\":1}}],[\"detailed\",{\"1\":{\"18\":1,\"786\":1,\"866\":1,\"921\":1,\"922\":1,\"1385\":1}}],[\"details\",{\"1\":{\"0\":1,\"3\":1,\"129\":1,\"132\":1,\"150\":1,\"160\":1,\"208\":1,\"214\":1,\"223\":2,\"240\":1,\"260\":1,\"262\":2,\"267\":1,\"275\":1,\"536\":2,\"709\":1,\"711\":1,\"756\":2,\"773\":2,\"774\":2,\"780\":1,\"787\":1,\"974\":1,\"1155\":1,\"1157\":1,\"1396\":1,\"1678\":3,\"1785\":1,\"1786\":1,\"1817\":1,\"1818\":1,\"2191\":1,\"2344\":1,\"2435\":1}}],[\"detected\",{\"1\":{\"2065\":3}}],[\"detecting\",{\"1\":{\"2065\":1}}],[\"detection=false\",{\"1\":{\"1720\":1}}],[\"detection\",{\"1\":{\"200\":1,\"254\":1,\"262\":2,\"315\":2,\"469\":2,\"1319\":1,\"1880\":1,\"2044\":1,\"2065\":1,\"2378\":1}}],[\"detect\",{\"0\":{\"154\":1,\"1880\":1,\"2378\":1},\"1\":{\"377\":2,\"449\":2,\"1720\":1,\"1721\":1,\"1725\":1,\"1806\":1,\"1862\":1,\"1880\":1,\"2065\":2,\"2378\":5}}],[\"determinant\",{\"1\":{\"1581\":1,\"1586\":1,\"1588\":1,\"1603\":1,\"1613\":1}}],[\"determining\",{\"1\":{\"716\":1,\"1155\":1,\"1157\":1}}],[\"deterministic=false\",{\"1\":{\"821\":1}}],[\"deterministic\",{\"1\":{\"104\":4,\"284\":1,\"290\":1,\"377\":2,\"449\":2,\"2049\":1,\"2134\":1}}],[\"determinization\",{\"0\":{\"104\":1}}],[\"determine\",{\"1\":{\"99\":1,\"100\":1,\"101\":1,\"267\":1,\"276\":1,\"286\":1,\"622\":1,\"623\":1,\"1224\":1,\"1225\":1,\"1245\":1,\"2148\":1}}],[\"determines\",{\"1\":{\"81\":1,\"1514\":2,\"2000\":1,\"2001\":1}}],[\"determined\",{\"1\":{\"62\":1,\"82\":1,\"124\":1,\"2144\":1}}],[\"desplanques\",{\"1\":{\"2187\":1}}],[\"desperate\",{\"1\":{\"817\":1}}],[\"dest\",{\"1\":{\"880\":1,\"886\":1,\"2478\":1}}],[\"destdir\",{\"1\":{\"528\":1}}],[\"destination\",{\"1\":{\"167\":1,\"535\":1}}],[\"descending\",{\"1\":{\"449\":2,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":2,\"2007\":1,\"2147\":1}}],[\"descritive=false\",{\"1\":{\"2277\":1}}],[\"descriptinc\",{\"1\":{\"1605\":1,\"1606\":1}}],[\"description\",{\"1\":{\"143\":1,\"1655\":1}}],[\"describing\",{\"1\":{\"106\":1,\"2007\":1}}],[\"describes\",{\"1\":{\"97\":1,\"245\":1}}],[\"describe\",{\"1\":{\"79\":1,\"140\":1}}],[\"described\",{\"1\":{\"45\":1,\"96\":1,\"138\":1,\"139\":1,\"145\":1,\"200\":1,\"242\":1,\"262\":1,\"768\":1,\"927\":1,\"1546\":1,\"1553\":1,\"1589\":1,\"1598\":1,\"1611\":1,\"1616\":1,\"1622\":1,\"1625\":1,\"1626\":1,\"1750\":1,\"1752\":1,\"1758\":1,\"1770\":1,\"1788\":1,\"1810\":1,\"1811\":1,\"1855\":1,\"1880\":1,\"1992\":1,\"1993\":1,\"1994\":1,\"1995\":1,\"2151\":1,\"2223\":1,\"2231\":1,\"2245\":1,\"2310\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2433\":1}}],[\"desbele\",{\"1\":{\"160\":1}}],[\"design\",{\"0\":{\"1631\":1},\"1\":{\"145\":1,\"691\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1631\":3,\"2240\":1}}],[\"designed\",{\"1\":{\"11\":1,\"101\":1,\"138\":2,\"194\":1,\"197\":1,\"231\":1,\"232\":1,\"257\":1,\"258\":1,\"1245\":1,\"1795\":1,\"2000\":1,\"2345\":1,\"2354\":1,\"2365\":1}}],[\"desired\",{\"1\":{\"24\":1,\"1687\":1}}],[\"decreasing\",{\"1\":{\"2147\":1}}],[\"decrease\",{\"1\":{\"175\":1,\"2014\":1}}],[\"declare\",{\"1\":{\"1718\":1}}],[\"deconstruct\",{\"1\":{\"2467\":1}}],[\"deconvolution\",{\"1\":{\"1849\":1}}],[\"deconv1d\",{\"1\":{\"1269\":2,\"1270\":3,\"1271\":2}}],[\"deconvglu\",{\"1\":{\"1181\":1}}],[\"decomposition\",{\"0\":{\"1308\":1},\"1\":{\"1308\":4,\"1318\":2,\"1328\":1}}],[\"decompose\",{\"1\":{\"924\":1}}],[\"decorator\",{\"1\":{\"929\":2}}],[\"decode=false\",{\"1\":{\"1720\":1}}],[\"decoded\",{\"1\":{\"133\":1,\"218\":1,\"267\":1,\"276\":1,\"286\":1,\"692\":1,\"760\":1,\"775\":1,\"790\":1,\"820\":1,\"850\":1,\"1381\":1,\"1751\":1,\"2130\":2,\"2137\":1}}],[\"decode\",{\"0\":{\"513\":1,\"1637\":1,\"1873\":1},\"1\":{\"45\":1,\"47\":3,\"126\":4,\"134\":3,\"135\":1,\"136\":2,\"145\":1,\"201\":3,\"218\":1,\"222\":1,\"223\":4,\"243\":1,\"267\":13,\"276\":9,\"286\":33,\"290\":1,\"404\":2,\"469\":2,\"475\":1,\"513\":2,\"526\":1,\"527\":8,\"543\":1,\"710\":1,\"711\":1,\"1381\":1,\"1389\":1,\"1391\":1,\"1395\":2,\"1400\":1,\"1401\":1,\"1403\":1,\"1406\":1,\"1408\":1,\"1410\":1,\"1439\":1,\"1441\":2,\"1466\":1,\"1468\":1,\"1469\":1,\"1521\":1,\"1637\":1,\"1727\":1,\"1873\":1,\"1965\":1,\"2130\":3,\"2136\":2,\"2137\":2,\"2228\":1,\"2229\":1,\"2408\":1,\"2446\":1}}],[\"decoderlayer\",{\"0\":{\"1751\":1},\"1\":{\"1751\":2,\"1847\":1}}],[\"decoders\",{\"1\":{\"223\":1,\"225\":1,\"262\":1,\"692\":2,\"790\":1,\"850\":1,\"1368\":1,\"1822\":1,\"1992\":1}}],[\"decoder\",{\"0\":{\"142\":1,\"614\":2,\"630\":1,\"633\":1,\"634\":2,\"637\":1,\"638\":1,\"641\":2,\"642\":1,\"643\":2,\"646\":1,\"647\":1,\"649\":1,\"651\":2,\"654\":1,\"668\":1,\"676\":2,\"692\":2,\"731\":2,\"732\":2,\"741\":2,\"760\":2,\"766\":2,\"767\":2,\"770\":2,\"775\":2,\"790\":2,\"796\":2,\"820\":2,\"847\":2,\"848\":2,\"850\":2,\"869\":2,\"904\":2,\"905\":2,\"930\":2,\"952\":2,\"955\":2,\"956\":2,\"959\":2,\"965\":2,\"977\":2,\"1030\":2,\"1112\":2,\"1142\":1,\"1222\":2,\"1250\":2,\"1341\":1,\"1368\":1,\"1428\":1,\"1430\":1,\"1437\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1460\":1,\"1505\":1,\"1506\":1,\"1519\":1,\"1536\":1,\"1556\":1,\"1749\":1,\"1750\":2,\"1751\":1,\"1762\":1,\"1775\":1,\"1810\":1,\"1811\":1,\"1815\":1,\"1843\":1,\"1847\":1,\"1855\":1,\"1874\":2,\"1888\":1,\"2223\":2,\"2242\":2},\"1\":{\"43\":4,\"46\":3,\"50\":2,\"51\":1,\"52\":5,\"86\":1,\"88\":13,\"89\":1,\"140\":1,\"142\":11,\"200\":3,\"223\":6,\"225\":9,\"240\":1,\"243\":2,\"260\":3,\"261\":3,\"262\":18,\"290\":5,\"301\":4,\"315\":2,\"463\":4,\"469\":4,\"614\":16,\"616\":3,\"625\":3,\"627\":3,\"628\":1,\"630\":3,\"631\":1,\"632\":5,\"633\":3,\"634\":19,\"637\":1,\"638\":1,\"641\":22,\"642\":3,\"643\":21,\"646\":1,\"647\":1,\"649\":4,\"651\":18,\"654\":1,\"667\":2,\"668\":1,\"674\":22,\"676\":2,\"686\":1,\"692\":5,\"696\":3,\"697\":3,\"709\":1,\"710\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"731\":2,\"732\":2,\"736\":3,\"737\":5,\"740\":3,\"741\":2,\"760\":5,\"766\":2,\"767\":2,\"768\":1,\"770\":2,\"774\":1,\"775\":3,\"777\":5,\"780\":1,\"787\":1,\"790\":4,\"795\":1,\"796\":2,\"820\":4,\"829\":1,\"830\":1,\"847\":24,\"848\":2,\"850\":3,\"859\":1,\"869\":2,\"904\":2,\"905\":2,\"930\":2,\"952\":2,\"954\":2,\"955\":3,\"956\":2,\"958\":1,\"959\":2,\"965\":2,\"974\":2,\"977\":3,\"979\":2,\"1030\":2,\"1112\":3,\"1118\":1,\"1142\":1,\"1155\":4,\"1156\":3,\"1157\":4,\"1158\":2,\"1222\":3,\"1250\":3,\"1252\":1,\"1341\":2,\"1368\":3,\"1389\":4,\"1391\":6,\"1396\":4,\"1401\":4,\"1403\":6,\"1408\":1,\"1410\":3,\"1428\":1,\"1430\":1,\"1437\":1,\"1446\":1,\"1448\":1,\"1450\":2,\"1452\":2,\"1454\":1,\"1456\":1,\"1460\":1,\"1466\":4,\"1468\":6,\"1505\":1,\"1506\":1,\"1519\":4,\"1526\":3,\"1536\":3,\"1552\":28,\"1553\":7,\"1556\":3,\"1598\":3,\"1599\":16,\"1600\":3,\"1625\":7,\"1626\":22,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":1,\"1708\":2,\"1709\":3,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1719\":2,\"1720\":1,\"1721\":2,\"1725\":2,\"1726\":3,\"1727\":3,\"1731\":4,\"1732\":1,\"1749\":19,\"1750\":9,\"1751\":2,\"1762\":1,\"1768\":2,\"1775\":1,\"1779\":5,\"1805\":1,\"1810\":2,\"1811\":3,\"1815\":23,\"1822\":5,\"1843\":16,\"1847\":4,\"1848\":1,\"1855\":1,\"1862\":2,\"1863\":2,\"1865\":1,\"1867\":2,\"1868\":3,\"1870\":3,\"1874\":3,\"1888\":3,\"1891\":1,\"1895\":1,\"1913\":1,\"1937\":1,\"1959\":5,\"1966\":1,\"1975\":2,\"1992\":2,\"1993\":2,\"1995\":2,\"1997\":3,\"2127\":3,\"2129\":1,\"2184\":1,\"2221\":5,\"2223\":9,\"2235\":4,\"2236\":4,\"2239\":15,\"2240\":15,\"2242\":3,\"2245\":2,\"2315\":9,\"2355\":1,\"2411\":16,\"2412\":16,\"2423\":14,\"2431\":2,\"2432\":16,\"2447\":16}}],[\"decoding\",{\"0\":{\"131\":1,\"148\":1,\"313\":1,\"410\":1,\"479\":1,\"488\":1,\"494\":1},\"1\":{\"41\":5,\"45\":8,\"46\":1,\"47\":1,\"118\":1,\"131\":1,\"132\":1,\"133\":2,\"134\":2,\"136\":2,\"137\":1,\"139\":1,\"145\":8,\"146\":1,\"148\":5,\"150\":1,\"175\":6,\"201\":1,\"211\":1,\"216\":1,\"217\":3,\"223\":2,\"227\":1,\"228\":2,\"261\":2,\"262\":2,\"266\":2,\"267\":1,\"275\":2,\"276\":1,\"284\":1,\"285\":3,\"286\":6,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":3,\"327\":1,\"331\":1,\"389\":1,\"396\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"463\":1,\"469\":1,\"496\":1,\"498\":1,\"505\":5,\"513\":1,\"527\":1,\"616\":3,\"639\":7,\"696\":2,\"697\":2,\"797\":1,\"829\":1,\"1391\":1,\"1395\":2,\"1403\":1,\"1406\":1,\"1410\":1,\"1468\":1,\"1720\":2,\"1721\":6,\"1724\":1,\"1725\":2,\"1731\":6,\"1794\":3,\"1806\":1,\"1807\":1,\"1822\":1,\"1848\":2,\"1862\":1,\"1873\":1,\"1966\":1,\"2006\":1,\"2130\":2,\"2136\":1,\"2411\":1}}],[\"decided\",{\"1\":{\"1645\":1,\"1650\":1,\"2249\":1,\"2253\":1}}],[\"decides\",{\"1\":{\"691\":1,\"703\":1}}],[\"decide\",{\"1\":{\"269\":1,\"278\":1,\"1949\":1}}],[\"decision\",{\"1\":{\"102\":1,\"217\":1,\"285\":1}}],[\"decays\",{\"1\":{\"2018\":1}}],[\"decayfalse\",{\"1\":{\"1963\":1}}],[\"decaytrue\",{\"1\":{\"1963\":1}}],[\"decay==false\",{\"1\":{\"1963\":1}}],[\"decay=0\",{\"1\":{\"1963\":1}}],[\"decay=false\",{\"1\":{\"1963\":2}}],[\"decayresidual\",{\"0\":{\"718\":1},\"1\":{\"718\":1}}],[\"decay\",{\"0\":{\"2015\":1},\"1\":{\"84\":1,\"243\":1,\"649\":3,\"654\":5,\"718\":1,\"793\":1,\"830\":1,\"1382\":1,\"1389\":1,\"1391\":1,\"1394\":1,\"1396\":2,\"1400\":3,\"1401\":1,\"1403\":1,\"1406\":1,\"1408\":2,\"1410\":1,\"1441\":3,\"1466\":2,\"1468\":1,\"1469\":3,\"1481\":1,\"1962\":1,\"1963\":3,\"1964\":1,\"2015\":6,\"2018\":6,\"2462\":1}}],[\"dec\",{\"1\":{\"43\":4,\"46\":1,\"50\":2,\"51\":2,\"52\":1,\"261\":2,\"290\":7,\"526\":2,\"628\":5,\"631\":3,\"632\":5,\"634\":3,\"641\":24,\"643\":14,\"651\":1,\"692\":1,\"743\":3,\"763\":2,\"847\":34,\"850\":2,\"963\":1,\"979\":2,\"1526\":5,\"1598\":5,\"1599\":10,\"1600\":5,\"1704\":3,\"1705\":3,\"1706\":3,\"1707\":2,\"1708\":3,\"1709\":2,\"1710\":3,\"1711\":3,\"1712\":3,\"1713\":3,\"1714\":3,\"1715\":3,\"1716\":3,\"1749\":30,\"1762\":3,\"1768\":3,\"1775\":2,\"1779\":3,\"1801\":2,\"1815\":34,\"1843\":9,\"1847\":3,\"1868\":2,\"1870\":2,\"1993\":1,\"1994\":2,\"2239\":9,\"2240\":9,\"2245\":1,\"2411\":10,\"2412\":10,\"2423\":6,\"2431\":1,\"2432\":10,\"2447\":10}}],[\"depth=10\",{\"1\":{\"1854\":1}}],[\"depth=\",{\"1\":{\"1235\":1,\"1282\":1}}],[\"depthwiseseparableconv\",{\"0\":{\"973\":1,\"1148\":1},\"1\":{\"973\":1,\"1148\":1}}],[\"depthwiseconvolution\",{\"0\":{\"623\":1},\"1\":{\"141\":1,\"623\":5,\"624\":1}}],[\"depthwise\",{\"1\":{\"141\":1,\"624\":2,\"768\":11,\"780\":1}}],[\"depth\",{\"0\":{\"939\":1},\"1\":{\"70\":1,\"623\":1,\"700\":1,\"701\":3,\"709\":1,\"718\":1,\"733\":1,\"734\":1,\"735\":1,\"780\":1,\"820\":1,\"828\":1,\"830\":1,\"837\":2,\"939\":5,\"1064\":3,\"1153\":1,\"1235\":2,\"1262\":1,\"1265\":3,\"1280\":3,\"1282\":2,\"1583\":1,\"1678\":1,\"1759\":2,\"1803\":3,\"1854\":2,\"2191\":1,\"2458\":1}}],[\"depends\",{\"1\":{\"128\":1,\"197\":1}}],[\"depend\",{\"1\":{\"71\":1,\"117\":1,\"161\":1,\"175\":1,\"259\":2}}],[\"dependency\",{\"1\":{\"162\":2}}],[\"dependencies\",{\"1\":{\"1\":2,\"3\":1,\"162\":7,\"2054\":1}}],[\"dependent\",{\"1\":{\"45\":1,\"145\":1,\"210\":1,\"211\":1,\"265\":1,\"266\":1,\"274\":1,\"275\":1,\"849\":1}}],[\"depending\",{\"1\":{\"44\":1,\"71\":1,\"106\":1,\"138\":1,\"141\":1,\"162\":1,\"168\":1,\"223\":2,\"224\":1,\"269\":2,\"278\":2,\"718\":1,\"911\":1,\"2184\":1,\"2247\":3,\"2262\":1}}],[\"deploy\",{\"0\":{\"34\":1},\"1\":{\"34\":1}}],[\"deprecated\",{\"0\":{\"27\":1},\"1\":{\"27\":1,\"74\":1,\"222\":1,\"223\":1,\"709\":1,\"774\":1,\"780\":1,\"1155\":3,\"1157\":3,\"1247\":1,\"2191\":1}}],[\"defalut\",{\"1\":{\"980\":1,\"1267\":1,\"1268\":1}}],[\"defaultly\",{\"1\":{\"2354\":1}}],[\"defaultrnnlm\",{\"1\":{\"1822\":1}}],[\"defaultfrontend\",{\"0\":{\"720\":1},\"1\":{\"720\":1}}],[\"default=none\",{\"1\":{\"2478\":1}}],[\"default=5\",{\"1\":{\"1711\":1}}],[\"default=32\",{\"1\":{\"537\":1}}],[\"default=run\",{\"1\":{\"537\":1}}],[\"default=0\",{\"1\":{\"537\":1}}],[\"default=60\",{\"1\":{\"537\":1}}],[\"default=64\",{\"1\":{\"516\":1}}],[\"default=1\",{\"1\":{\"1053\":1}}],[\"default=16\",{\"1\":{\"537\":1}}],[\"default=16000\",{\"1\":{\"537\":1}}],[\"default=1024\",{\"1\":{\"516\":1,\"523\":1,\"537\":1}}],[\"default=22050\",{\"1\":{\"523\":1}}],[\"default=256\",{\"1\":{\"516\":1,\"523\":1,\"537\":1}}],[\"default=zip\",{\"1\":{\"518\":1}}],[\"default=downloads\",{\"1\":{\"518\":1}}],[\"default=80\",{\"1\":{\"516\":1}}],[\"default=\",{\"1\":{\"516\":1,\"774\":1,\"2334\":1,\"2479\":1}}],[\"defaults\",{\"1\":{\"516\":2,\"523\":2,\"524\":2,\"525\":2,\"760\":6,\"1118\":8,\"1245\":1,\"1413\":1,\"1514\":3,\"1536\":18,\"1556\":2,\"1683\":3,\"2043\":3,\"2045\":2,\"2049\":3,\"2054\":1,\"2055\":3,\"2056\":3,\"2065\":4,\"2066\":2,\"2249\":1,\"2427\":4,\"2428\":12,\"2442\":3}}],[\"default\",{\"0\":{\"720\":1,\"1305\":1,\"1480\":1,\"2090\":1,\"2477\":1,\"2487\":2},\"1\":{\"22\":2,\"24\":1,\"32\":1,\"39\":3,\"41\":1,\"43\":21,\"44\":8,\"45\":5,\"46\":1,\"47\":1,\"66\":1,\"67\":1,\"71\":1,\"91\":1,\"101\":1,\"104\":1,\"118\":1,\"124\":1,\"135\":1,\"139\":7,\"141\":51,\"142\":22,\"143\":2,\"144\":5,\"145\":7,\"147\":4,\"148\":2,\"161\":1,\"162\":1,\"163\":1,\"165\":1,\"166\":1,\"168\":2,\"175\":5,\"200\":1,\"218\":1,\"236\":1,\"267\":8,\"276\":4,\"285\":3,\"286\":3,\"290\":1,\"521\":1,\"527\":2,\"536\":2,\"616\":2,\"629\":1,\"631\":1,\"696\":4,\"697\":4,\"720\":1,\"726\":1,\"760\":1,\"763\":1,\"768\":2,\"784\":1,\"786\":1,\"787\":3,\"800\":2,\"819\":1,\"821\":2,\"823\":1,\"824\":2,\"828\":1,\"829\":2,\"830\":1,\"859\":1,\"911\":1,\"921\":1,\"935\":2,\"939\":1,\"974\":1,\"1008\":1,\"1029\":5,\"1064\":1,\"1117\":2,\"1130\":2,\"1131\":2,\"1133\":4,\"1134\":3,\"1136\":2,\"1137\":3,\"1139\":3,\"1141\":2,\"1144\":1,\"1155\":1,\"1156\":3,\"1157\":1,\"1158\":1,\"1162\":1,\"1185\":1,\"1202\":2,\"1208\":2,\"1210\":1,\"1225\":1,\"1232\":2,\"1235\":4,\"1246\":1,\"1250\":1,\"1251\":1,\"1252\":8,\"1255\":2,\"1257\":2,\"1259\":2,\"1261\":3,\"1262\":1,\"1279\":1,\"1280\":2,\"1281\":2,\"1282\":2,\"1283\":1,\"1290\":1,\"1301\":3,\"1305\":1,\"1306\":3,\"1330\":1,\"1371\":3,\"1372\":3,\"1413\":1,\"1420\":5,\"1480\":1,\"1533\":5,\"1534\":4,\"1545\":5,\"1558\":1,\"1643\":1,\"1646\":1,\"1720\":1,\"1721\":1,\"1725\":1,\"1775\":1,\"1806\":1,\"1822\":1,\"1862\":1,\"1962\":2,\"2014\":6,\"2015\":2,\"2018\":1,\"2130\":1,\"2132\":4,\"2136\":3,\"2138\":1,\"2141\":2,\"2155\":1,\"2249\":1,\"2327\":1,\"2334\":1,\"2354\":1,\"2355\":3,\"2357\":1,\"2477\":1,\"2480\":2,\"2487\":3,\"2488\":1}}],[\"def\",{\"1\":{\"78\":7,\"81\":2,\"82\":4,\"756\":3,\"773\":3,\"829\":1,\"960\":1,\"2246\":2,\"2248\":2,\"2249\":5,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2305\":1,\"2325\":2,\"2327\":2,\"2334\":2,\"2355\":7,\"2369\":2,\"2488\":1,\"2500\":4,\"2502\":2}}],[\"definite\",{\"1\":{\"1308\":1}}],[\"definitely\",{\"1\":{\"153\":1}}],[\"definition\",{\"1\":{\"43\":2,\"107\":1,\"143\":1,\"150\":2,\"615\":1,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":1,\"626\":1,\"628\":1,\"629\":1,\"630\":1,\"631\":1,\"635\":1,\"636\":1,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"644\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"652\":1,\"654\":1,\"743\":1,\"763\":1,\"1061\":1,\"1062\":1,\"1545\":1,\"1762\":1,\"1775\":1,\"1866\":1,\"2247\":1,\"2249\":1,\"2253\":1}}],[\"defining\",{\"1\":{\"141\":1,\"640\":1,\"666\":1,\"756\":2,\"773\":2,\"866\":2,\"867\":2}}],[\"defines\",{\"1\":{\"162\":1,\"263\":2,\"652\":1,\"1655\":1,\"2131\":1,\"2325\":1,\"2357\":1}}],[\"defined\",{\"1\":{\"43\":2,\"44\":2,\"141\":3,\"142\":2,\"144\":1,\"147\":1,\"223\":3,\"228\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1209\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1228\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1334\":1,\"1354\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1469\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1655\":2,\"1658\":1,\"1663\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2246\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"define\",{\"1\":{\"3\":1,\"44\":1,\"49\":1,\"139\":1,\"142\":1,\"144\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"693\":1,\"713\":1,\"718\":1,\"720\":1,\"722\":1,\"738\":1,\"741\":1,\"752\":1,\"756\":3,\"757\":1,\"773\":3,\"778\":1,\"781\":1,\"788\":1,\"791\":1,\"796\":1,\"798\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"821\":1,\"825\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"842\":1,\"844\":1,\"852\":1,\"854\":1,\"856\":1,\"860\":1,\"862\":1,\"864\":1,\"866\":1,\"867\":1,\"950\":1,\"952\":1,\"956\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1066\":1,\"1068\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1114\":1,\"1120\":1,\"1122\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1145\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1159\":1,\"1165\":1,\"1168\":1,\"1177\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1200\":1,\"1202\":1,\"1205\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1219\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1248\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1262\":1,\"1265\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1383\":1,\"1387\":1,\"1392\":1,\"1398\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1509\":1,\"1511\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1549\":1,\"1554\":1,\"1638\":1,\"1652\":1,\"1657\":1,\"1662\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1945\":1,\"1957\":1,\"1967\":1,\"1969\":1,\"1972\":1,\"1975\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"2001\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2185\":1,\"2188\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2203\":1,\"2205\":1,\"2209\":1,\"2211\":1,\"2216\":1,\"2246\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2262\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2286\":1,\"2305\":1,\"2325\":1,\"2401\":1,\"2405\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2434\":1,\"2443\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2465\":1,\"2467\":1}}],[\"devided\",{\"1\":{\"1961\":1}}],[\"deviation\",{\"1\":{\"633\":1,\"637\":2,\"638\":1,\"646\":1,\"647\":1,\"1770\":1,\"1771\":2,\"2307\":1}}],[\"device=\",{\"1\":{\"1373\":1,\"1926\":1}}],[\"device=none\",{\"1\":{\"726\":1,\"819\":1,\"828\":1,\"829\":1,\"854\":1,\"859\":1,\"1065\":1,\"2088\":1,\"2116\":1,\"2164\":1,\"2323\":1}}],[\"device\",{\"0\":{\"1931\":1,\"2164\":1,\"2309\":1,\"2323\":2},\"1\":{\"93\":1,\"102\":1,\"195\":1,\"614\":5,\"617\":4,\"618\":4,\"620\":4,\"624\":4,\"626\":4,\"634\":6,\"636\":4,\"639\":4,\"641\":6,\"643\":6,\"651\":6,\"669\":4,\"704\":1,\"847\":6,\"1533\":2,\"1725\":1,\"1749\":6,\"1815\":6,\"1837\":1,\"1926\":2,\"1931\":2,\"2043\":3,\"2045\":3,\"2049\":3,\"2054\":3,\"2055\":3,\"2056\":3,\"2066\":3,\"2133\":3,\"2136\":3,\"2138\":3,\"2164\":2,\"2249\":3,\"2263\":1,\"2268\":1,\"2270\":1,\"2271\":1,\"2305\":1,\"2309\":3,\"2323\":3}}],[\"devices=2\",{\"1\":{\"93\":1}}],[\"devices=0\",{\"1\":{\"41\":1}}],[\"devices\",{\"0\":{\"2114\":1},\"1\":{\"41\":1,\"93\":1,\"94\":2,\"195\":1}}],[\"devtalk\",{\"1\":{\"66\":1}}],[\"devel\",{\"1\":{\"29\":1}}],[\"developed\",{\"1\":{\"2001\":1}}],[\"developers\",{\"0\":{\"197\":1,\"224\":1},\"1\":{\"194\":1,\"222\":1}}],[\"developer\",{\"0\":{\"0\":1},\"1\":{\"67\":1,\"705\":1,\"804\":1,\"932\":1,\"934\":1}}],[\"developing\",{\"1\":{\"263\":1,\"287\":2}}],[\"develop\",{\"1\":{\"70\":1}}],[\"development\",{\"1\":{\"18\":1,\"36\":1,\"106\":1,\"162\":1,\"196\":1,\"197\":1,\"213\":1,\"260\":1,\"268\":1,\"277\":1,\"2216\":2,\"2369\":1}}],[\"devcontainers\",{\"1\":{\"18\":1}}],[\"devcontainer\",{\"0\":{\"18\":1},\"1\":{\"19\":1}}],[\"dev\",{\"0\":{\"19\":1},\"1\":{\"3\":1,\"18\":2,\"19\":4,\"33\":1,\"79\":2,\"161\":1,\"162\":3,\"196\":2,\"197\":4,\"213\":2,\"218\":7,\"243\":2,\"259\":2,\"267\":7,\"268\":8,\"276\":10,\"277\":8,\"285\":4,\"286\":14,\"517\":1}}],[\"tpu\",{\"1\":{\"2355\":1}}],[\"tweaks\",{\"1\":{\"2298\":1}}],[\"twooptimizertrainer\",{\"1\":{\"2369\":1}}],[\"two\",{\"1\":{\"45\":1,\"49\":1,\"60\":1,\"67\":1,\"80\":1,\"84\":1,\"85\":1,\"119\":1,\"121\":1,\"138\":2,\"141\":1,\"144\":1,\"145\":1,\"147\":1,\"148\":1,\"180\":1,\"201\":1,\"211\":1,\"217\":1,\"232\":1,\"242\":1,\"243\":1,\"258\":1,\"262\":1,\"269\":1,\"275\":1,\"278\":1,\"285\":1,\"286\":1,\"704\":1,\"756\":1,\"773\":1,\"817\":1,\"921\":1,\"1350\":1,\"1597\":2,\"1604\":3,\"1606\":2,\"1667\":1,\"1673\":1,\"1719\":2,\"1725\":3,\"1751\":1,\"2127\":1,\"2131\":1,\"2151\":1,\"2249\":1,\"2253\":1,\"2310\":1,\"2355\":1}}],[\"t2\",{\"0\":{\"2025\":1,\"2063\":1}}],[\"t1\",{\"0\":{\"2024\":1,\"2062\":1,\"2063\":1}}],[\"tt\",{\"0\":{\"2087\":1,\"2092\":1}}],[\"ttranslatotron\",{\"1\":{\"1993\":1}}],[\"ttstask\",{\"0\":{\"2271\":1},\"1\":{\"2271\":1}}],[\"tts2task\",{\"0\":{\"2270\":1},\"1\":{\"2270\":1}}],[\"tts2\",{\"0\":{\"484\":1,\"2270\":1,\"2443\":1,\"2445\":2,\"2446\":1,\"2447\":1,\"2448\":1,\"2449\":1,\"2553\":1},\"1\":{\"291\":1,\"484\":1,\"2270\":1,\"2443\":1,\"2445\":3,\"2446\":1,\"2447\":1,\"2448\":1,\"2449\":1}}],[\"tts1\",{\"1\":{\"37\":1,\"195\":1,\"197\":1,\"285\":1,\"286\":12,\"289\":4,\"290\":2,\"2447\":1}}],[\"tts\",{\"0\":{\"9\":1,\"181\":1,\"187\":1,\"490\":1,\"1576\":2,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1625\":1,\"1626\":1,\"1627\":1,\"1628\":1,\"1629\":1,\"1630\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1634\":1,\"1635\":1,\"1636\":1,\"1637\":1,\"1764\":1,\"1770\":1,\"1771\":1,\"1839\":1,\"2032\":2,\"2040\":2,\"2045\":2,\"2256\":1,\"2271\":1,\"2401\":1,\"2403\":2,\"2404\":1,\"2407\":1,\"2408\":1,\"2409\":1,\"2411\":1,\"2412\":1,\"2413\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2421\":1,\"2422\":1,\"2423\":1,\"2424\":1,\"2425\":1,\"2426\":1,\"2427\":1,\"2428\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":1,\"2433\":1,\"2434\":1,\"2436\":1,\"2437\":1,\"2438\":1,\"2439\":1,\"2440\":1,\"2441\":1,\"2442\":1,\"2526\":1,\"2552\":1},\"1\":{\"9\":3,\"37\":1,\"53\":1,\"78\":1,\"119\":2,\"124\":3,\"162\":6,\"163\":1,\"181\":2,\"187\":1,\"190\":2,\"193\":6,\"197\":1,\"246\":4,\"247\":2,\"283\":1,\"284\":4,\"285\":8,\"286\":43,\"287\":2,\"288\":1,\"289\":3,\"290\":46,\"402\":1,\"403\":2,\"484\":1,\"490\":2,\"536\":2,\"1576\":3,\"1577\":2,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":2,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1625\":1,\"1626\":1,\"1627\":1,\"1628\":1,\"1629\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1634\":1,\"1635\":1,\"1636\":1,\"1637\":1,\"1750\":1,\"1758\":1,\"1764\":1,\"1770\":1,\"1771\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"1839\":1,\"1971\":1,\"1980\":1,\"2032\":2,\"2040\":3,\"2044\":19,\"2045\":9,\"2134\":2,\"2256\":1,\"2271\":1,\"2298\":1,\"2401\":1,\"2403\":3,\"2404\":1,\"2407\":1,\"2408\":2,\"2409\":1,\"2411\":1,\"2412\":1,\"2413\":1,\"2414\":1,\"2416\":2,\"2418\":1,\"2420\":1,\"2421\":1,\"2422\":1,\"2423\":1,\"2424\":1,\"2425\":1,\"2426\":1,\"2427\":1,\"2428\":1,\"2429\":1,\"2430\":1,\"2431\":2,\"2432\":2,\"2433\":1,\"2434\":1,\"2436\":1,\"2437\":1,\"2438\":1,\"2439\":1,\"2440\":1,\"2441\":1,\"2442\":1,\"2445\":1,\"2446\":1}}],[\"tmp\",{\"1\":{\"1800\":1}}],[\"tmax\",{\"1\":{\"706\":8,\"770\":1,\"959\":1,\"1526\":11,\"1552\":14,\"1709\":2,\"1750\":4,\"1753\":6,\"1758\":3,\"1764\":2,\"1788\":1,\"1810\":2,\"1814\":2,\"1816\":2,\"1908\":1,\"2223\":3,\"2226\":2,\"2227\":2,\"2231\":3,\"2235\":12,\"2236\":12,\"2239\":6,\"2240\":10,\"2245\":10,\"2432\":1,\"2433\":3}}],[\"tdspeakerbeamextractor\",{\"0\":{\"1268\":1},\"1\":{\"1268\":1}}],[\"td\",{\"0\":{\"1268\":1},\"1\":{\"1268\":2}}],[\"tdnn\",{\"0\":{\"2187\":1,\"2196\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2213\":1,\"2214\":1},\"1\":{\"43\":4,\"2183\":2,\"2187\":5,\"2196\":1,\"2202\":1,\"2203\":3,\"2205\":1,\"2213\":1,\"2214\":1}}],[\"tzw\",{\"1\":{\"1125\":1}}],[\"t=0\",{\"1\":{\"1225\":1}}],[\"t=20\",{\"1\":{\"941\":1}}],[\"t=5\",{\"1\":{\"941\":1}}],[\"tvm\",{\"1\":{\"774\":1}}],[\"tying\",{\"1\":{\"760\":1}}],[\"typ\",{\"1\":{\"1814\":1,\"1816\":1}}],[\"typ=\",{\"1\":{\"1814\":1,\"1816\":1}}],[\"typical\",{\"1\":{\"68\":1,\"70\":1}}],[\"typically\",{\"1\":{\"45\":1,\"70\":1,\"145\":1,\"262\":4,\"269\":1,\"278\":1,\"702\":1,\"787\":1,\"1686\":1,\"1694\":1,\"1794\":1,\"2039\":2,\"2043\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2101\":2,\"2130\":1,\"2131\":1,\"2344\":1}}],[\"typeerror\",{\"1\":{\"2155\":1}}],[\"type2\",{\"1\":{\"1655\":2}}],[\"type2=tuple\",{\"1\":{\"1655\":1}}],[\"type1\",{\"1\":{\"1655\":3}}],[\"type1=tuple\",{\"1\":{\"1655\":1}}],[\"type=str\",{\"1\":{\"2505\":1}}],[\"type=str2pair\",{\"1\":{\"2500\":1}}],[\"type=int\",{\"1\":{\"2493\":1}}],[\"type=float\",{\"1\":{\"2485\":1}}],[\"type=filepath\",{\"1\":{\"81\":5,\"82\":1}}],[\"type=catpow\",{\"1\":{\"2000\":1,\"2001\":1}}],[\"type=char\",{\"1\":{\"285\":1}}],[\"type=none\",{\"1\":{\"869\":1,\"1170\":1,\"1171\":1,\"1566\":1}}],[\"type=numel\",{\"1\":{\"95\":1,\"290\":1}}],[\"type=phn\",{\"1\":{\"218\":1,\"266\":2,\"267\":1,\"275\":2,\"276\":1,\"285\":2,\"286\":1}}],[\"type=raw\",{\"1\":{\"218\":1,\"267\":1,\"276\":1,\"286\":1}}],[\"type=word\",{\"1\":{\"211\":1}}],[\"type=length\",{\"1\":{\"95\":1}}],[\"type=\",{\"1\":{\"79\":4,\"276\":2,\"693\":1,\"754\":1,\"807\":1,\"823\":1,\"824\":1,\"958\":1,\"973\":1,\"981\":1,\"982\":1,\"1029\":2,\"1059\":1,\"1061\":1,\"1063\":1,\"1120\":1,\"1122\":1,\"1139\":1,\"1148\":1,\"1172\":2,\"1173\":1,\"1175\":1,\"1198\":1,\"1202\":2,\"1211\":2,\"1253\":1,\"1255\":1,\"1259\":2,\"1272\":1,\"1273\":1,\"1274\":1,\"1279\":2,\"1281\":2,\"1354\":1,\"1674\":1,\"1833\":1,\"2249\":1}}],[\"type\",{\"0\":{\"86\":1,\"95\":1,\"150\":1,\"876\":1,\"1335\":1},\"1\":{\"43\":10,\"45\":11,\"71\":2,\"79\":4,\"82\":4,\"86\":1,\"95\":1,\"96\":6,\"97\":9,\"98\":12,\"99\":5,\"100\":5,\"101\":11,\"139\":2,\"141\":30,\"142\":11,\"143\":2,\"145\":8,\"150\":2,\"211\":1,\"218\":2,\"223\":2,\"243\":7,\"247\":1,\"259\":2,\"266\":5,\"267\":2,\"268\":1,\"269\":1,\"275\":5,\"276\":7,\"277\":1,\"278\":1,\"285\":3,\"286\":3,\"290\":1,\"295\":1,\"301\":3,\"309\":3,\"315\":3,\"321\":3,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"368\":2,\"377\":2,\"385\":2,\"389\":3,\"396\":3,\"404\":2,\"406\":3,\"415\":1,\"421\":3,\"429\":3,\"436\":2,\"442\":3,\"449\":4,\"457\":2,\"463\":4,\"469\":3,\"475\":4,\"481\":1,\"484\":2,\"490\":2,\"496\":2,\"498\":3,\"505\":3,\"536\":2,\"603\":1,\"614\":2,\"616\":9,\"617\":2,\"618\":2,\"619\":1,\"620\":4,\"621\":1,\"622\":1,\"623\":1,\"624\":2,\"625\":3,\"626\":2,\"627\":1,\"630\":1,\"632\":4,\"633\":6,\"634\":13,\"636\":2,\"637\":4,\"638\":1,\"639\":3,\"640\":1,\"641\":8,\"642\":1,\"643\":7,\"644\":3,\"645\":1,\"646\":1,\"647\":2,\"649\":2,\"651\":3,\"654\":2,\"661\":12,\"664\":3,\"665\":1,\"666\":3,\"667\":1,\"669\":1,\"671\":1,\"672\":1,\"673\":1,\"674\":8,\"691\":1,\"692\":5,\"696\":10,\"697\":9,\"699\":1,\"700\":4,\"701\":1,\"702\":1,\"706\":6,\"709\":15,\"710\":5,\"711\":3,\"712\":1,\"715\":1,\"733\":6,\"734\":6,\"735\":1,\"740\":1,\"749\":1,\"759\":1,\"760\":3,\"768\":9,\"770\":2,\"771\":1,\"774\":15,\"775\":1,\"780\":17,\"781\":1,\"783\":2,\"784\":3,\"790\":3,\"796\":2,\"797\":1,\"798\":1,\"805\":1,\"820\":4,\"831\":1,\"847\":8,\"849\":3,\"850\":3,\"862\":1,\"876\":1,\"930\":1,\"939\":1,\"958\":3,\"959\":2,\"962\":2,\"971\":1,\"973\":1,\"975\":1,\"976\":1,\"978\":1,\"979\":1,\"980\":3,\"982\":2,\"983\":1,\"984\":1,\"987\":1,\"989\":1,\"991\":1,\"994\":1,\"1003\":1,\"1015\":1,\"1019\":1,\"1029\":6,\"1031\":1,\"1035\":1,\"1050\":1,\"1053\":4,\"1054\":1,\"1061\":3,\"1062\":4,\"1063\":1,\"1066\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1107\":13,\"1112\":1,\"1113\":2,\"1116\":1,\"1117\":3,\"1118\":2,\"1119\":2,\"1124\":1,\"1125\":1,\"1126\":4,\"1127\":2,\"1130\":3,\"1131\":3,\"1132\":1,\"1134\":2,\"1136\":3,\"1137\":2,\"1139\":4,\"1141\":6,\"1147\":1,\"1148\":1,\"1155\":4,\"1156\":1,\"1157\":7,\"1161\":1,\"1162\":3,\"1167\":1,\"1170\":2,\"1171\":2,\"1172\":2,\"1173\":2,\"1174\":1,\"1175\":2,\"1176\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1185\":3,\"1189\":1,\"1198\":1,\"1199\":2,\"1202\":2,\"1204\":1,\"1208\":1,\"1209\":1,\"1210\":3,\"1217\":5,\"1218\":1,\"1221\":1,\"1228\":1,\"1229\":1,\"1232\":3,\"1235\":2,\"1244\":1,\"1246\":1,\"1247\":1,\"1250\":2,\"1251\":3,\"1252\":1,\"1253\":2,\"1255\":1,\"1257\":2,\"1259\":3,\"1261\":4,\"1264\":1,\"1267\":3,\"1268\":6,\"1269\":3,\"1270\":3,\"1271\":3,\"1273\":2,\"1274\":5,\"1275\":1,\"1277\":1,\"1278\":3,\"1279\":6,\"1280\":7,\"1281\":6,\"1282\":1,\"1283\":7,\"1293\":1,\"1295\":1,\"1296\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1328\":1,\"1330\":1,\"1332\":1,\"1334\":4,\"1335\":2,\"1340\":1,\"1351\":1,\"1354\":2,\"1356\":1,\"1361\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1385\":1,\"1386\":1,\"1389\":6,\"1390\":1,\"1391\":3,\"1395\":6,\"1396\":2,\"1397\":1,\"1400\":1,\"1401\":6,\"1402\":1,\"1403\":3,\"1406\":2,\"1408\":6,\"1409\":1,\"1410\":3,\"1419\":2,\"1441\":1,\"1466\":6,\"1467\":1,\"1468\":3,\"1469\":2,\"1513\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1519\":13,\"1520\":1,\"1521\":3,\"1524\":1,\"1525\":1,\"1526\":23,\"1529\":3,\"1533\":4,\"1534\":1,\"1535\":13,\"1536\":13,\"1546\":13,\"1548\":2,\"1549\":1,\"1551\":1,\"1552\":21,\"1553\":17,\"1556\":3,\"1558\":1,\"1559\":1,\"1577\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":4,\"1585\":2,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":4,\"1592\":2,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":19,\"1599\":25,\"1600\":22,\"1601\":1,\"1603\":1,\"1604\":1,\"1605\":2,\"1606\":1,\"1607\":2,\"1608\":2,\"1609\":1,\"1610\":2,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":2,\"1620\":1,\"1621\":2,\"1622\":13,\"1624\":1,\"1625\":14,\"1626\":15,\"1627\":1,\"1628\":1,\"1629\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1637\":1,\"1645\":3,\"1654\":1,\"1660\":1,\"1666\":1,\"1667\":1,\"1668\":2,\"1669\":2,\"1672\":1,\"1673\":1,\"1674\":2,\"1676\":1,\"1678\":1,\"1679\":1,\"1680\":1,\"1681\":1,\"1683\":3,\"1686\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1692\":1,\"1694\":1,\"1697\":1,\"1698\":1,\"1702\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1713\":2,\"1714\":2,\"1715\":2,\"1716\":2,\"1719\":7,\"1720\":3,\"1721\":2,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":10,\"1731\":3,\"1733\":1,\"1735\":2,\"1736\":3,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":20,\"1750\":3,\"1751\":1,\"1753\":2,\"1754\":2,\"1756\":1,\"1757\":1,\"1758\":3,\"1759\":1,\"1764\":1,\"1768\":2,\"1770\":1,\"1771\":1,\"1779\":4,\"1783\":1,\"1784\":1,\"1785\":2,\"1786\":1,\"1787\":2,\"1788\":1,\"1789\":1,\"1790\":1,\"1794\":3,\"1795\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":2,\"1803\":1,\"1805\":1,\"1806\":6,\"1807\":1,\"1808\":1,\"1810\":1,\"1812\":1,\"1814\":2,\"1815\":6,\"1816\":2,\"1817\":2,\"1818\":1,\"1820\":1,\"1822\":3,\"1824\":1,\"1837\":1,\"1839\":1,\"1843\":5,\"1846\":3,\"1847\":1,\"1849\":1,\"1851\":2,\"1854\":2,\"1856\":1,\"1858\":2,\"1861\":1,\"1862\":1,\"1863\":16,\"1864\":11,\"1865\":1,\"1867\":6,\"1868\":1,\"1870\":1,\"1871\":1,\"1873\":1,\"1877\":1,\"1881\":1,\"1888\":1,\"1891\":7,\"1892\":1,\"1894\":1,\"1895\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1908\":1,\"1910\":1,\"1914\":4,\"1915\":1,\"1917\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1926\":1,\"1927\":1,\"1928\":1,\"1931\":1,\"1932\":2,\"1936\":1,\"1937\":1,\"1944\":2,\"1945\":2,\"1947\":2,\"1950\":1,\"1960\":1,\"1961\":1,\"1965\":1,\"1966\":2,\"1975\":1,\"1977\":1,\"1984\":1,\"1990\":1,\"1991\":2,\"1992\":5,\"1993\":4,\"1994\":8,\"1995\":3,\"1997\":1,\"2007\":3,\"2008\":1,\"2039\":4,\"2040\":2,\"2043\":1,\"2044\":5,\"2045\":2,\"2049\":2,\"2054\":1,\"2055\":1,\"2056\":1,\"2065\":3,\"2066\":1,\"2101\":1,\"2124\":1,\"2126\":5,\"2128\":1,\"2129\":3,\"2130\":3,\"2133\":3,\"2136\":5,\"2137\":1,\"2139\":1,\"2149\":2,\"2155\":1,\"2167\":1,\"2176\":1,\"2183\":1,\"2187\":2,\"2190\":1,\"2191\":15,\"2192\":1,\"2198\":3,\"2203\":1,\"2207\":1,\"2208\":1,\"2218\":1,\"2219\":2,\"2220\":1,\"2223\":1,\"2224\":1,\"2226\":1,\"2227\":1,\"2228\":3,\"2229\":3,\"2231\":3,\"2232\":2,\"2235\":11,\"2236\":8,\"2237\":1,\"2238\":1,\"2239\":24,\"2240\":20,\"2241\":4,\"2245\":7,\"2247\":3,\"2249\":2,\"2258\":4,\"2283\":1,\"2284\":1,\"2285\":1,\"2293\":2,\"2298\":1,\"2311\":1,\"2327\":1,\"2334\":4,\"2336\":2,\"2337\":2,\"2342\":1,\"2344\":1,\"2346\":1,\"2348\":1,\"2351\":1,\"2353\":2,\"2356\":2,\"2360\":2,\"2361\":2,\"2362\":2,\"2363\":2,\"2364\":2,\"2366\":1,\"2368\":1,\"2370\":2,\"2372\":1,\"2394\":1,\"2407\":1,\"2408\":3,\"2411\":25,\"2412\":25,\"2413\":1,\"2420\":1,\"2422\":1,\"2423\":25,\"2424\":1,\"2425\":1,\"2426\":1,\"2427\":3,\"2428\":5,\"2429\":1,\"2430\":1,\"2431\":7,\"2432\":11,\"2433\":1,\"2435\":3,\"2436\":1,\"2438\":1,\"2440\":1,\"2441\":1,\"2442\":4,\"2446\":3,\"2447\":25,\"2448\":1,\"2474\":1,\"2482\":1,\"2490\":1}}],[\"types=1\",{\"1\":{\"1279\":1,\"1281\":1}}],[\"types\",{\"0\":{\"2484\":1,\"2491\":1,\"2492\":1,\"2496\":1,\"2497\":1,\"2498\":1,\"2499\":1,\"2501\":1,\"2503\":1,\"2504\":1},\"1\":{\"43\":2,\"53\":1,\"95\":1,\"142\":1,\"197\":1,\"223\":1,\"224\":1,\"275\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"744\":1,\"756\":1,\"773\":1,\"828\":1,\"829\":1,\"830\":1,\"859\":1,\"1279\":1,\"1280\":2,\"1281\":1,\"1283\":2,\"1354\":1,\"2155\":1,\"2289\":1,\"2484\":1,\"2491\":1,\"2492\":1,\"2496\":1,\"2497\":1,\"2498\":1,\"2499\":1,\"2501\":1,\"2503\":1,\"2504\":1}}],[\"tgt\",{\"0\":{\"1972\":2,\"1978\":1,\"1980\":1,\"1982\":1},\"1\":{\"692\":10,\"787\":1,\"790\":4,\"850\":4,\"1270\":2,\"1271\":2,\"1751\":7,\"1952\":1,\"1972\":2,\"1975\":10,\"1976\":2,\"1978\":1,\"1980\":1,\"1982\":1,\"1992\":4,\"2221\":5}}],[\"tɕ\",{\"1\":{\"287\":1}}],[\"tʃ\",{\"1\":{\"287\":1}}],[\"tfgridnet\",{\"0\":{\"1182\":1,\"1192\":1,\"1194\":1,\"1269\":2},\"1\":{\"1182\":1,\"1192\":1,\"1194\":1,\"1269\":5,\"1270\":1}}],[\"tfgridnetv2\",{\"0\":{\"1048\":1,\"1183\":1,\"1270\":2},\"1\":{\"1048\":1,\"1183\":1,\"1270\":6,\"1271\":1}}],[\"tfgridnetv3\",{\"0\":{\"1046\":1,\"1184\":1,\"1190\":1,\"1271\":2},\"1\":{\"1046\":1,\"1184\":1,\"1190\":1,\"1271\":6}}],[\"tf\",{\"0\":{\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1},\"1\":{\"223\":1,\"225\":1,\"1117\":1,\"1130\":1,\"1131\":3,\"1132\":1,\"1155\":2,\"1157\":2,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1264\":2,\"1269\":2,\"1270\":2,\"1271\":2,\"1280\":6,\"1301\":1,\"1334\":1,\"1372\":1}}],[\"tcnresblock\",{\"0\":{\"1265\":1},\"1\":{\"1265\":1}}],[\"tcndensenet\",{\"1\":{\"1264\":1}}],[\"tcndenseunet\",{\"0\":{\"1108\":1,\"1145\":1,\"1168\":1,\"1264\":2,\"1265\":1},\"1\":{\"1108\":1,\"1145\":1,\"1168\":1,\"1264\":2,\"1265\":1}}],[\"tcn=3\",{\"1\":{\"1264\":1,\"1334\":1}}],[\"tcnseparatornomask\",{\"0\":{\"980\":1},\"1\":{\"980\":1}}],[\"tcnseparator\",{\"0\":{\"1267\":1},\"1\":{\"978\":1,\"980\":1,\"1267\":1}}],[\"tcn\",{\"0\":{\"971\":1,\"972\":1,\"973\":1,\"975\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"984\":1,\"1075\":1,\"1148\":1,\"1179\":1,\"1267\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1295\":1},\"1\":{\"223\":2,\"225\":3,\"971\":1,\"972\":1,\"973\":1,\"975\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"984\":1,\"1075\":1,\"1148\":1,\"1179\":1,\"1264\":11,\"1265\":2,\"1267\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1295\":1,\"1334\":11}}],[\"tc\",{\"1\":{\"168\":1,\"535\":1}}],[\"tcp\",{\"1\":{\"60\":3}}],[\"txt\",{\"1\":{\"97\":4,\"98\":2,\"99\":4,\"100\":4,\"101\":2,\"200\":1,\"223\":1,\"242\":1,\"243\":2,\"259\":4,\"536\":7,\"594\":1,\"748\":1,\"986\":2,\"987\":1,\"988\":2,\"989\":1,\"990\":2,\"1883\":1,\"1890\":1,\"2283\":1,\"2284\":1}}],[\"tsnormalization\",{\"0\":{\"844\":1},\"1\":{\"844\":1}}],[\"tsne\",{\"1\":{\"377\":4,\"2354\":1}}],[\"tsinversenormalization\",{\"0\":{\"842\":1},\"1\":{\"842\":1}}],[\"tsepreprocessor\",{\"0\":{\"2368\":1},\"1\":{\"2368\":1}}],[\"tse\",{\"0\":{\"361\":1,\"1158\":1,\"2272\":1},\"1\":{\"356\":2,\"361\":1,\"1158\":1,\"2272\":1}}],[\"tsunoo\",{\"1\":{\"202\":1,\"711\":1,\"1720\":1,\"1721\":1}}],[\"tsubasa\",{\"1\":{\"156\":1}}],[\"tsd\",{\"1\":{\"45\":2,\"145\":2,\"616\":1,\"696\":2,\"697\":2}}],[\"tsao\",{\"1\":{\"11\":1}}],[\"trn2stm\",{\"0\":{\"611\":1},\"1\":{\"611\":1}}],[\"trn2ctm\",{\"0\":{\"609\":1},\"1\":{\"609\":1}}],[\"trn\",{\"1\":{\"609\":2,\"611\":2}}],[\"treated\",{\"1\":{\"242\":1,\"1680\":1,\"1692\":1,\"1698\":1}}],[\"trees\",{\"1\":{\"1319\":1}}],[\"tree\",{\"1\":{\"195\":1,\"213\":1,\"242\":1,\"268\":1,\"277\":1,\"286\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1,\"2462\":1}}],[\"trigonometric\",{\"1\":{\"2176\":1}}],[\"triplet\",{\"0\":{\"2165\":1}}],[\"trillion\",{\"1\":{\"2151\":2,\"2310\":2}}],[\"trillions\",{\"1\":{\"2151\":1,\"2310\":1}}],[\"tri\",{\"1\":{\"2018\":3}}],[\"tristagelr\",{\"0\":{\"2018\":1},\"1\":{\"2018\":1}}],[\"tristage\",{\"0\":{\"2018\":1},\"1\":{\"2018\":1}}],[\"triton\",{\"1\":{\"852\":1}}],[\"triangular\",{\"1\":{\"709\":1,\"774\":1,\"780\":1,\"1308\":1,\"1533\":1,\"1558\":2,\"1785\":1,\"1817\":1,\"2191\":1}}],[\"trial\",{\"1\":{\"254\":1,\"449\":2}}],[\"trials\",{\"1\":{\"254\":1}}],[\"triu=false\",{\"1\":{\"1785\":1,\"1817\":1}}],[\"triu\",{\"0\":{\"1935\":1},\"1\":{\"700\":1,\"709\":2,\"733\":1,\"734\":1,\"774\":2,\"780\":2,\"1526\":1,\"1598\":1,\"1599\":3,\"1600\":1,\"1785\":1,\"1817\":1,\"1935\":2,\"1994\":1,\"2126\":1,\"2191\":2,\"2239\":1,\"2240\":1,\"2411\":3,\"2412\":3,\"2423\":3,\"2447\":3}}],[\"trimmed\",{\"1\":{\"1934\":1}}],[\"trimming\",{\"1\":{\"290\":1,\"1450\":2,\"1452\":2}}],[\"trim\",{\"0\":{\"537\":1,\"606\":1,\"1934\":1},\"1\":{\"224\":1,\"527\":1,\"537\":3,\"606\":2,\"709\":1,\"791\":1,\"792\":2,\"1011\":1,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1446\":1,\"1448\":1,\"1450\":2,\"1452\":2,\"1466\":1,\"1468\":1,\"1868\":1,\"1870\":1,\"1934\":2}}],[\"tries\",{\"1\":{\"162\":2,\"2355\":2}}],[\"truncated\",{\"1\":{\"1061\":2,\"1063\":2}}],[\"truncation\",{\"1\":{\"142\":2,\"633\":3,\"634\":3,\"637\":3}}],[\"truth\",{\"1\":{\"267\":1,\"276\":1,\"286\":1,\"703\":1,\"717\":1,\"922\":1,\"936\":1,\"937\":1,\"947\":1,\"948\":1,\"949\":1,\"1515\":1,\"1552\":2,\"1556\":3,\"1599\":3,\"1625\":1,\"1626\":1,\"1702\":1,\"2167\":1,\"2176\":1,\"2207\":1}}],[\"true\",{\"1\":{\"43\":5,\"44\":5,\"54\":2,\"56\":1,\"57\":2,\"58\":2,\"62\":1,\"64\":1,\"76\":2,\"81\":2,\"87\":1,\"92\":1,\"103\":1,\"118\":4,\"124\":4,\"125\":1,\"126\":1,\"128\":1,\"130\":2,\"131\":2,\"132\":1,\"136\":2,\"139\":2,\"141\":2,\"147\":1,\"148\":1,\"200\":4,\"223\":2,\"224\":2,\"243\":6,\"266\":3,\"267\":12,\"275\":3,\"276\":10,\"285\":5,\"286\":20,\"515\":1,\"619\":1,\"620\":3,\"621\":1,\"622\":1,\"623\":1,\"625\":1,\"632\":1,\"665\":1,\"674\":2,\"692\":4,\"696\":1,\"697\":1,\"699\":2,\"700\":3,\"706\":2,\"709\":4,\"710\":6,\"711\":5,\"720\":4,\"731\":2,\"732\":2,\"734\":1,\"736\":3,\"737\":2,\"738\":3,\"746\":2,\"747\":1,\"756\":1,\"759\":1,\"760\":4,\"766\":2,\"767\":2,\"768\":1,\"771\":1,\"773\":1,\"774\":3,\"775\":3,\"777\":3,\"778\":1,\"780\":4,\"787\":1,\"790\":1,\"794\":1,\"798\":3,\"815\":1,\"817\":1,\"819\":1,\"820\":2,\"824\":1,\"827\":1,\"833\":4,\"846\":3,\"848\":3,\"849\":2,\"850\":4,\"854\":2,\"862\":3,\"864\":1,\"866\":1,\"867\":1,\"925\":1,\"939\":3,\"976\":1,\"979\":1,\"991\":1,\"1029\":1,\"1061\":1,\"1062\":1,\"1064\":1,\"1065\":2,\"1107\":3,\"1118\":3,\"1126\":3,\"1127\":3,\"1133\":2,\"1134\":1,\"1136\":1,\"1139\":1,\"1141\":2,\"1147\":1,\"1153\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1185\":1,\"1208\":2,\"1209\":1,\"1217\":7,\"1228\":2,\"1235\":1,\"1245\":1,\"1246\":1,\"1247\":1,\"1250\":2,\"1251\":3,\"1252\":2,\"1261\":1,\"1262\":1,\"1267\":1,\"1268\":1,\"1278\":2,\"1280\":3,\"1281\":1,\"1282\":1,\"1283\":1,\"1290\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1318\":1,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1327\":2,\"1329\":1,\"1330\":2,\"1377\":1,\"1382\":1,\"1385\":1,\"1389\":10,\"1390\":2,\"1391\":3,\"1392\":1,\"1395\":1,\"1396\":6,\"1400\":1,\"1401\":11,\"1402\":3,\"1403\":3,\"1408\":8,\"1409\":2,\"1410\":1,\"1419\":3,\"1420\":2,\"1441\":1,\"1442\":1,\"1444\":1,\"1450\":3,\"1452\":3,\"1454\":3,\"1456\":3,\"1458\":4,\"1460\":4,\"1462\":1,\"1466\":9,\"1467\":1,\"1468\":2,\"1469\":1,\"1485\":1,\"1509\":2,\"1511\":3,\"1513\":4,\"1519\":1,\"1521\":1,\"1526\":17,\"1534\":1,\"1535\":1,\"1536\":2,\"1539\":1,\"1545\":1,\"1546\":1,\"1548\":1,\"1549\":5,\"1551\":4,\"1552\":8,\"1553\":16,\"1584\":1,\"1585\":1,\"1587\":2,\"1591\":1,\"1592\":4,\"1593\":2,\"1594\":1,\"1595\":4,\"1596\":4,\"1597\":4,\"1598\":17,\"1599\":10,\"1600\":18,\"1604\":1,\"1605\":4,\"1606\":2,\"1607\":2,\"1609\":3,\"1610\":4,\"1611\":2,\"1612\":3,\"1613\":3,\"1614\":2,\"1615\":1,\"1618\":2,\"1619\":3,\"1620\":1,\"1621\":1,\"1622\":1,\"1625\":14,\"1626\":8,\"1627\":1,\"1628\":3,\"1643\":1,\"1646\":1,\"1656\":2,\"1660\":1,\"1664\":1,\"1665\":1,\"1669\":2,\"1671\":1,\"1672\":1,\"1691\":1,\"1700\":1,\"1702\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1725\":1,\"1735\":1,\"1736\":2,\"1751\":1,\"1759\":1,\"1760\":3,\"1766\":1,\"1782\":1,\"1794\":1,\"1846\":1,\"1872\":1,\"1948\":1,\"1955\":1,\"1959\":2,\"1975\":4,\"1978\":2,\"1980\":2,\"1982\":2,\"1991\":1,\"1992\":3,\"1993\":3,\"1994\":2,\"1995\":3,\"1996\":3,\"1997\":3,\"2003\":1,\"2004\":1,\"2007\":1,\"2015\":1,\"2065\":1,\"2126\":2,\"2127\":3,\"2129\":2,\"2130\":1,\"2184\":1,\"2191\":2,\"2216\":2,\"2217\":1,\"2220\":1,\"2221\":4,\"2232\":1,\"2235\":3,\"2236\":3,\"2238\":1,\"2239\":9,\"2240\":6,\"2241\":1,\"2245\":5,\"2246\":2,\"2247\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2262\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2280\":2,\"2298\":1,\"2327\":2,\"2340\":2,\"2346\":1,\"2350\":3,\"2353\":1,\"2354\":2,\"2355\":1,\"2368\":1,\"2369\":1,\"2379\":2,\"2404\":3,\"2409\":3,\"2411\":7,\"2412\":7,\"2413\":1,\"2414\":2,\"2416\":2,\"2418\":2,\"2423\":6,\"2424\":1,\"2427\":1,\"2431\":5,\"2432\":5,\"2433\":1,\"2447\":7,\"2448\":1,\"2458\":1,\"2460\":2,\"2462\":1,\"2463\":1,\"2464\":1,\"2480\":1}}],[\"tr\",{\"1\":{\"79\":2,\"218\":3,\"267\":3,\"268\":6,\"276\":5,\"277\":6,\"286\":6,\"526\":4,\"1309\":1,\"1321\":1,\"1327\":1,\"1452\":1,\"1484\":1}}],[\"troubleshooting\",{\"0\":{\"66\":1,\"249\":1}}],[\"trying\",{\"1\":{\"536\":1}}],[\"try\",{\"1\":{\"48\":2,\"56\":1,\"70\":2,\"153\":2,\"290\":2,\"699\":1}}],[\"trace\",{\"0\":{\"1367\":1},\"1\":{\"1367\":1}}],[\"track\",{\"1\":{\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"820\":2,\"828\":2,\"829\":2,\"830\":1,\"859\":1,\"1065\":1,\"1078\":1,\"2355\":1}}],[\"trade\",{\"1\":{\"1327\":1,\"1330\":1}}],[\"traditional\",{\"1\":{\"128\":1,\"1668\":1}}],[\"trailing\",{\"1\":{\"224\":1,\"831\":1}}],[\"train=true\",{\"1\":{\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1}}],[\"trainable\",{\"1\":{\"262\":1,\"821\":1,\"824\":1,\"1599\":1,\"1770\":1,\"2239\":1,\"2240\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2432\":1,\"2447\":1}}],[\"trains\",{\"1\":{\"235\":1,\"252\":1,\"285\":1}}],[\"traininig\",{\"1\":{\"132\":1}}],[\"training\",{\"0\":{\"47\":1,\"54\":1,\"57\":1,\"77\":1,\"83\":1,\"87\":1,\"103\":1,\"112\":1,\"114\":1,\"119\":1,\"121\":2,\"125\":1,\"130\":1,\"147\":1,\"259\":1,\"381\":1,\"453\":1},\"1\":{\"6\":1,\"7\":1,\"38\":1,\"39\":2,\"41\":1,\"42\":1,\"43\":1,\"44\":2,\"46\":3,\"47\":1,\"48\":2,\"49\":1,\"54\":1,\"55\":1,\"57\":1,\"71\":1,\"78\":2,\"79\":3,\"84\":1,\"85\":1,\"87\":2,\"90\":1,\"91\":5,\"93\":2,\"94\":2,\"96\":3,\"102\":1,\"104\":3,\"106\":6,\"107\":1,\"109\":1,\"111\":1,\"118\":2,\"119\":6,\"121\":2,\"123\":1,\"132\":1,\"139\":4,\"146\":1,\"147\":4,\"150\":1,\"173\":2,\"174\":1,\"175\":2,\"184\":1,\"190\":1,\"193\":1,\"196\":1,\"197\":5,\"199\":4,\"200\":19,\"201\":2,\"204\":4,\"205\":19,\"206\":1,\"207\":1,\"210\":1,\"211\":10,\"213\":2,\"216\":2,\"217\":10,\"218\":6,\"222\":2,\"223\":3,\"224\":4,\"227\":2,\"228\":5,\"232\":6,\"233\":1,\"234\":2,\"235\":3,\"236\":3,\"240\":4,\"242\":20,\"243\":7,\"244\":1,\"252\":2,\"253\":2,\"254\":8,\"255\":1,\"258\":5,\"259\":3,\"260\":1,\"261\":3,\"262\":7,\"263\":3,\"265\":11,\"266\":12,\"267\":23,\"268\":2,\"274\":7,\"275\":12,\"276\":18,\"277\":2,\"284\":9,\"285\":10,\"286\":37,\"290\":7,\"661\":4,\"699\":1,\"785\":1,\"786\":1,\"792\":1,\"846\":1,\"866\":1,\"882\":1,\"883\":1,\"884\":1,\"921\":1,\"927\":1,\"939\":2,\"961\":1,\"978\":1,\"1000\":1,\"1029\":2,\"1066\":1,\"1132\":1,\"1133\":1,\"1155\":3,\"1156\":1,\"1157\":3,\"1167\":1,\"1204\":2,\"1209\":3,\"1228\":2,\"1235\":2,\"1280\":2,\"1281\":2,\"1282\":2,\"1334\":2,\"1400\":1,\"1526\":1,\"1553\":1,\"1598\":3,\"1600\":1,\"1625\":2,\"1645\":2,\"1650\":2,\"1683\":4,\"1702\":1,\"1721\":1,\"1759\":1,\"1760\":1,\"1833\":1,\"1872\":2,\"2000\":1,\"2130\":7,\"2131\":10,\"2132\":1,\"2134\":7,\"2141\":1,\"2142\":1,\"2143\":2,\"2162\":2,\"2176\":1,\"2184\":2,\"2216\":1,\"2235\":1,\"2236\":5,\"2239\":5,\"2240\":5,\"2245\":4,\"2247\":4,\"2249\":3,\"2253\":1,\"2325\":1,\"2338\":2,\"2344\":1,\"2347\":1,\"2353\":1,\"2355\":9,\"2364\":1,\"2365\":1,\"2369\":2,\"2371\":1,\"2411\":4,\"2412\":4,\"2423\":4,\"2428\":1,\"2431\":4,\"2432\":4,\"2447\":4,\"2462\":1,\"2470\":4,\"2471\":1,\"2473\":1}}],[\"traineroptions\",{\"0\":{\"2370\":1},\"1\":{\"2347\":1,\"2348\":1,\"2354\":1,\"2365\":2,\"2369\":4,\"2370\":1,\"2371\":1,\"2372\":1}}],[\"trainer\",{\"0\":{\"382\":1,\"454\":1,\"2135\":2,\"2338\":1,\"2339\":1,\"2347\":1,\"2348\":1,\"2354\":1,\"2365\":1,\"2369\":2,\"2370\":1,\"2371\":1,\"2372\":1},\"1\":{\"113\":3,\"960\":2,\"961\":5,\"2246\":2,\"2247\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":2,\"2259\":1,\"2260\":2,\"2261\":2,\"2262\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":1,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":1,\"2333\":1,\"2338\":3,\"2339\":1,\"2347\":4,\"2348\":2,\"2354\":4,\"2355\":6,\"2365\":3,\"2369\":6,\"2370\":1,\"2371\":4,\"2372\":2}}],[\"trained\",{\"0\":{\"127\":1},\"1\":{\"42\":1,\"44\":1,\"50\":1,\"52\":2,\"124\":1,\"132\":1,\"138\":1,\"175\":1,\"178\":3,\"179\":1,\"180\":1,\"181\":1,\"185\":1,\"187\":1,\"188\":1,\"193\":2,\"200\":3,\"205\":3,\"211\":2,\"214\":1,\"217\":2,\"220\":3,\"223\":5,\"233\":1,\"235\":2,\"240\":1,\"242\":3,\"243\":8,\"254\":1,\"261\":1,\"262\":4,\"266\":2,\"267\":2,\"275\":2,\"276\":1,\"284\":2,\"285\":2,\"286\":3,\"290\":5,\"738\":1,\"746\":2,\"760\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1,\"2043\":1,\"2045\":4,\"2049\":1,\"2055\":1,\"2056\":1,\"2138\":1,\"2216\":1,\"2422\":1}}],[\"train2\",{\"1\":{\"47\":1,\"97\":1,\"98\":2,\"99\":1,\"100\":1}}],[\"train\",{\"0\":{\"2324\":1,\"2325\":1,\"2327\":1,\"2328\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":1,\"2335\":1,\"2336\":1,\"2337\":1,\"2338\":1,\"2339\":1,\"2340\":1,\"2341\":1,\"2342\":1,\"2344\":1,\"2345\":1,\"2346\":1,\"2347\":1,\"2348\":1,\"2349\":1,\"2350\":1,\"2351\":1,\"2353\":1,\"2354\":1,\"2355\":1,\"2356\":1,\"2357\":1,\"2358\":1,\"2359\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2364\":1,\"2365\":1,\"2366\":1,\"2367\":1,\"2368\":1,\"2369\":1,\"2370\":1,\"2371\":1,\"2372\":1,\"2373\":1,\"2374\":1,\"2375\":1,\"2376\":1,\"2378\":1,\"2379\":1,\"2380\":1,\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2388\":1,\"2389\":1,\"2390\":1,\"2391\":1,\"2392\":1,\"2393\":1,\"2394\":1,\"2395\":1,\"2396\":1,\"2397\":1,\"2398\":1,\"2399\":1,\"2400\":1,\"2551\":1},\"1\":{\"3\":2,\"25\":1,\"39\":1,\"42\":1,\"47\":14,\"49\":2,\"50\":2,\"56\":2,\"57\":1,\"58\":2,\"61\":3,\"62\":1,\"63\":1,\"64\":1,\"78\":2,\"79\":4,\"80\":1,\"81\":5,\"82\":3,\"84\":5,\"85\":3,\"86\":2,\"87\":1,\"88\":6,\"89\":1,\"90\":1,\"91\":1,\"92\":1,\"93\":2,\"94\":4,\"96\":5,\"97\":6,\"98\":14,\"99\":8,\"100\":8,\"101\":6,\"102\":1,\"103\":1,\"104\":1,\"113\":6,\"114\":2,\"115\":1,\"118\":1,\"119\":2,\"124\":2,\"125\":1,\"132\":1,\"136\":6,\"147\":2,\"184\":1,\"195\":4,\"196\":6,\"197\":6,\"200\":1,\"201\":2,\"205\":4,\"211\":2,\"213\":6,\"217\":4,\"218\":5,\"219\":2,\"223\":4,\"224\":2,\"228\":3,\"240\":1,\"242\":1,\"243\":13,\"254\":1,\"259\":3,\"262\":1,\"266\":3,\"267\":43,\"272\":2,\"275\":3,\"276\":24,\"282\":2,\"284\":3,\"285\":9,\"286\":65,\"289\":2,\"290\":13,\"295\":2,\"301\":6,\"309\":2,\"315\":6,\"321\":4,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"368\":2,\"377\":3,\"385\":2,\"389\":4,\"396\":6,\"403\":34,\"404\":2,\"406\":2,\"415\":2,\"421\":6,\"429\":6,\"436\":2,\"442\":6,\"449\":3,\"457\":2,\"463\":10,\"469\":6,\"475\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":6,\"505\":6,\"515\":1,\"516\":2,\"521\":3,\"523\":2,\"524\":2,\"525\":2,\"526\":3,\"535\":1,\"537\":2,\"710\":3,\"711\":3,\"960\":2,\"961\":2,\"1155\":1,\"1157\":1,\"1246\":1,\"1526\":1,\"1600\":1,\"1734\":1,\"1735\":1,\"1846\":1,\"1892\":2,\"1949\":1,\"1951\":2,\"2043\":2,\"2134\":2,\"2246\":17,\"2247\":14,\"2248\":14,\"2249\":9,\"2250\":15,\"2251\":15,\"2252\":32,\"2253\":14,\"2254\":9,\"2255\":18,\"2256\":15,\"2257\":14,\"2258\":2,\"2259\":20,\"2260\":10,\"2261\":15,\"2262\":7,\"2263\":22,\"2264\":15,\"2265\":17,\"2266\":14,\"2267\":21,\"2268\":18,\"2269\":16,\"2270\":14,\"2271\":15,\"2272\":12,\"2273\":13,\"2324\":1,\"2325\":1,\"2327\":1,\"2328\":2,\"2329\":1,\"2330\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":1,\"2335\":1,\"2336\":2,\"2337\":2,\"2338\":4,\"2339\":3,\"2340\":1,\"2341\":2,\"2342\":1,\"2344\":1,\"2345\":1,\"2346\":2,\"2347\":4,\"2348\":2,\"2349\":1,\"2350\":2,\"2351\":1,\"2353\":3,\"2354\":1,\"2355\":2,\"2356\":2,\"2357\":1,\"2358\":1,\"2359\":3,\"2360\":2,\"2361\":2,\"2362\":2,\"2363\":2,\"2364\":3,\"2365\":1,\"2366\":1,\"2367\":1,\"2368\":3,\"2369\":6,\"2370\":3,\"2371\":4,\"2372\":2,\"2373\":1,\"2374\":1,\"2375\":1,\"2376\":1,\"2377\":1,\"2378\":1,\"2379\":1,\"2380\":1,\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2388\":1,\"2389\":1,\"2390\":1,\"2391\":1,\"2392\":1,\"2393\":1,\"2394\":1,\"2395\":1,\"2396\":1,\"2397\":1,\"2398\":1,\"2399\":1,\"2400\":1}}],[\"tranducer\",{\"1\":{\"138\":1}}],[\"transition\",{\"0\":{\"944\":1},\"1\":{\"944\":2,\"1709\":1,\"1768\":1,\"2223\":1,\"2224\":1,\"2227\":2}}],[\"transducuer\",{\"1\":{\"882\":1}}],[\"transduction\",{\"1\":{\"878\":1,\"879\":1,\"881\":1}}],[\"transducerdecoderinterface\",{\"0\":{\"1843\":1},\"1\":{\"1749\":1,\"1815\":1,\"1843\":1}}],[\"transducerdecoder\",{\"0\":{\"847\":1},\"1\":{\"847\":1}}],[\"transducers\",{\"1\":{\"262\":1,\"785\":1,\"786\":1,\"866\":1,\"882\":1,\"883\":1,\"884\":1,\"921\":1}}],[\"transducer\",{\"0\":{\"42\":1,\"138\":1,\"321\":1,\"614\":1,\"615\":1,\"616\":2,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":2,\"626\":1,\"627\":1,\"628\":2,\"629\":1,\"630\":1,\"631\":2,\"632\":1,\"633\":1,\"634\":1,\"635\":1,\"636\":1,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"651\":1,\"652\":1,\"653\":1,\"654\":1,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"663\":1,\"664\":1,\"665\":1,\"666\":1,\"667\":2,\"668\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":1,\"673\":1,\"696\":2,\"697\":2,\"703\":1,\"704\":1,\"716\":1,\"717\":1,\"740\":1,\"743\":2,\"755\":1,\"763\":2,\"764\":1,\"773\":1,\"785\":1,\"786\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"847\":1,\"866\":1,\"867\":1,\"868\":1,\"873\":1,\"874\":1,\"875\":1,\"876\":1,\"878\":1,\"879\":1,\"880\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"886\":1,\"888\":1,\"896\":1,\"897\":1,\"899\":1,\"908\":1,\"909\":1,\"916\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"931\":1,\"933\":1,\"935\":1,\"936\":1,\"937\":1,\"940\":1,\"946\":1,\"1736\":1,\"1749\":1,\"1762\":1,\"1775\":1,\"1779\":1,\"1815\":1,\"1843\":1,\"1847\":1,\"1851\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1888\":1,\"1891\":1,\"1894\":1,\"1897\":1,\"1910\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1920\":1,\"1921\":1,\"1927\":1,\"1936\":1,\"1937\":1,\"2247\":1,\"2514\":1},\"1\":{\"42\":5,\"43\":6,\"44\":18,\"45\":2,\"46\":4,\"52\":1,\"138\":5,\"139\":4,\"141\":1,\"144\":3,\"145\":1,\"146\":1,\"150\":1,\"153\":1,\"163\":3,\"260\":1,\"261\":2,\"262\":6,\"301\":2,\"321\":2,\"442\":2,\"463\":2,\"469\":2,\"614\":1,\"615\":1,\"616\":3,\"617\":1,\"618\":1,\"619\":1,\"620\":1,\"621\":1,\"622\":1,\"623\":1,\"624\":1,\"625\":7,\"626\":1,\"627\":2,\"628\":2,\"629\":1,\"630\":1,\"631\":3,\"632\":2,\"633\":1,\"634\":1,\"635\":1,\"636\":1,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"641\":1,\"642\":1,\"643\":1,\"644\":1,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"651\":2,\"652\":1,\"653\":1,\"654\":1,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":2,\"662\":1,\"663\":1,\"664\":1,\"665\":1,\"666\":1,\"667\":3,\"668\":1,\"669\":1,\"670\":1,\"671\":1,\"672\":1,\"673\":1,\"696\":6,\"697\":4,\"703\":2,\"704\":1,\"716\":1,\"717\":1,\"740\":2,\"743\":2,\"755\":2,\"763\":3,\"764\":1,\"773\":1,\"785\":2,\"786\":2,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"847\":2,\"866\":3,\"867\":1,\"868\":1,\"873\":1,\"874\":1,\"875\":1,\"876\":1,\"878\":1,\"879\":1,\"880\":1,\"881\":1,\"882\":1,\"883\":2,\"884\":2,\"886\":1,\"888\":1,\"896\":1,\"897\":1,\"899\":1,\"908\":1,\"909\":1,\"916\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":3,\"922\":3,\"923\":1,\"931\":1,\"933\":1,\"935\":2,\"936\":3,\"937\":3,\"940\":1,\"946\":1,\"1720\":1,\"1736\":1,\"1749\":2,\"1762\":1,\"1775\":2,\"1779\":2,\"1815\":3,\"1843\":2,\"1847\":2,\"1851\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1870\":1,\"1871\":1,\"1872\":3,\"1888\":1,\"1891\":1,\"1894\":1,\"1897\":1,\"1910\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1920\":1,\"1921\":1,\"1927\":1,\"1936\":1,\"1937\":1,\"2247\":5}}],[\"transposelast\",{\"0\":{\"2467\":1},\"1\":{\"2467\":1}}],[\"transpose\",{\"0\":{\"1623\":1},\"1\":{\"749\":2,\"820\":1,\"828\":1,\"830\":1,\"854\":1,\"858\":1,\"1509\":1,\"1511\":1,\"1553\":1,\"1623\":4}}],[\"transposedmodule\",{\"0\":{\"858\":1},\"1\":{\"858\":1}}],[\"transposedlinear\",{\"0\":{\"856\":1},\"1\":{\"856\":1}}],[\"transposedln\",{\"0\":{\"852\":1},\"1\":{\"852\":1}}],[\"transposedlayernorm\",{\"0\":{\"854\":1},\"1\":{\"722\":1,\"854\":1}}],[\"transposed=false\",{\"1\":{\"744\":1,\"757\":1,\"769\":1,\"784\":1,\"788\":1,\"811\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"892\":1,\"945\":1,\"1147\":1}}],[\"transposed=true\",{\"1\":{\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"817\":1,\"859\":1,\"860\":1,\"1147\":1}}],[\"transposed\",{\"1\":{\"689\":1,\"702\":2,\"718\":1,\"809\":1,\"819\":2,\"820\":2,\"827\":1,\"828\":1,\"830\":1,\"852\":1,\"856\":1,\"858\":1,\"1112\":1,\"1147\":1,\"1450\":1,\"1452\":1,\"2136\":1}}],[\"transposing\",{\"1\":{\"722\":1}}],[\"translatotron\",{\"0\":{\"1993\":2},\"1\":{\"1993\":3}}],[\"translatotron2\",{\"0\":{\"1974\":1,\"1977\":1,\"1985\":1,\"1994\":2},\"1\":{\"1974\":1,\"1977\":1,\"1985\":1,\"1994\":4}}],[\"translate\",{\"1\":{\"240\":1}}],[\"translation\",{\"0\":{\"10\":1,\"185\":1,\"230\":1,\"238\":1,\"239\":1,\"260\":1},\"1\":{\"10\":2,\"11\":1,\"162\":1,\"190\":2,\"193\":2,\"242\":3,\"262\":4,\"586\":1,\"1975\":1,\"1992\":2,\"1993\":2,\"1994\":2,\"1995\":2}}],[\"trans\",{\"1\":{\"144\":2,\"603\":1,\"1768\":2,\"2223\":2,\"2224\":2}}],[\"transcripts\",{\"1\":{\"246\":1}}],[\"transcript\",{\"1\":{\"200\":1,\"242\":2,\"2040\":3,\"2045\":3,\"2124\":4,\"2127\":7,\"2128\":4,\"2362\":1}}],[\"transcriptions\",{\"1\":{\"205\":1,\"2262\":1}}],[\"transcription\",{\"1\":{\"80\":1,\"196\":3,\"213\":1,\"217\":1,\"242\":1,\"246\":1,\"247\":1,\"268\":5,\"277\":5,\"583\":1,\"586\":1,\"589\":1,\"878\":1,\"879\":1,\"881\":1,\"882\":1,\"883\":1,\"884\":1}}],[\"transcribed\",{\"1\":{\"2043\":3,\"2055\":3,\"2056\":3,\"2066\":3}}],[\"transcribe\",{\"1\":{\"126\":2,\"240\":1,\"242\":1,\"2283\":1,\"2284\":1}}],[\"transforner\",{\"1\":{\"1795\":1}}],[\"transforminterface\",{\"0\":{\"1844\":1},\"1\":{\"1777\":1,\"1844\":1}}],[\"transforms\",{\"1\":{\"829\":1,\"1336\":1}}],[\"transformations\",{\"1\":{\"2131\":1}}],[\"transformation\",{\"0\":{\"1845\":2},\"1\":{\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"744\":1,\"828\":2,\"829\":2,\"830\":2,\"859\":1,\"1668\":1,\"1845\":2,\"1846\":2,\"2432\":1}}],[\"transformed\",{\"1\":{\"644\":9,\"784\":5,\"1794\":5}}],[\"transformerpostencoder\",{\"0\":{\"2129\":1},\"1\":{\"2129\":1}}],[\"transformerdiscretesynthesizer\",{\"0\":{\"1992\":1},\"1\":{\"1992\":1}}],[\"transformerdecoderlayer\",{\"0\":{\"1847\":1},\"1\":{\"1847\":1,\"1867\":1}}],[\"transformerdecoder\",{\"0\":{\"848\":1},\"1\":{\"848\":1}}],[\"transformermddecoder\",{\"0\":{\"850\":1},\"1\":{\"850\":1}}],[\"transformerencoder\",{\"0\":{\"849\":1},\"1\":{\"849\":1}}],[\"transformerseparator\",{\"0\":{\"1278\":1},\"1\":{\"1278\":1}}],[\"transformersentenceencoderlayer\",{\"0\":{\"851\":1},\"1\":{\"851\":1,\"1684\":1}}],[\"transformers\",{\"0\":{\"760\":1,\"761\":1,\"762\":1,\"904\":1,\"905\":1,\"930\":1,\"2128\":1},\"1\":{\"759\":3,\"760\":6,\"761\":2,\"762\":2,\"828\":1,\"904\":1,\"905\":1,\"930\":1,\"1965\":1,\"2049\":2,\"2126\":1,\"2128\":2}}],[\"transformerlm\",{\"0\":{\"1947\":1},\"1\":{\"527\":1,\"1822\":1,\"1947\":1}}],[\"transformer2\",{\"1\":{\"136\":2}}],[\"transformer\",{\"0\":{\"692\":1,\"711\":1,\"731\":1,\"732\":1,\"766\":1,\"767\":1,\"848\":1,\"849\":1,\"850\":1,\"1064\":1,\"1153\":1,\"1205\":1,\"1262\":1,\"1278\":1,\"1290\":1,\"1362\":1,\"1374\":1,\"1375\":1,\"1735\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1748\":1,\"1751\":1,\"1756\":1,\"1757\":1,\"1759\":1,\"1771\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1789\":1,\"1790\":1,\"1792\":1,\"1794\":1,\"1795\":1,\"1796\":1,\"1808\":1,\"1809\":1,\"1817\":1,\"1818\":1,\"1820\":1,\"1837\":1,\"1842\":1,\"1847\":1,\"1858\":1,\"1867\":1,\"1869\":1,\"1917\":1,\"1926\":1,\"1947\":1,\"2129\":1,\"2432\":3},\"1\":{\"43\":12,\"46\":2,\"52\":2,\"53\":1,\"129\":1,\"130\":4,\"223\":1,\"225\":1,\"243\":1,\"284\":1,\"286\":2,\"289\":1,\"290\":1,\"527\":8,\"536\":13,\"675\":1,\"692\":2,\"710\":1,\"711\":4,\"724\":2,\"725\":2,\"726\":2,\"727\":2,\"728\":2,\"731\":2,\"732\":2,\"744\":2,\"766\":2,\"767\":2,\"775\":1,\"777\":2,\"787\":1,\"790\":1,\"791\":1,\"828\":2,\"829\":2,\"830\":2,\"846\":3,\"848\":2,\"849\":3,\"850\":2,\"851\":1,\"859\":2,\"1029\":4,\"1064\":2,\"1107\":1,\"1139\":2,\"1141\":2,\"1153\":1,\"1156\":2,\"1185\":2,\"1205\":1,\"1235\":2,\"1262\":2,\"1278\":3,\"1279\":3,\"1280\":2,\"1281\":4,\"1282\":2,\"1290\":1,\"1362\":1,\"1374\":1,\"1375\":1,\"1526\":8,\"1546\":1,\"1598\":8,\"1599\":17,\"1600\":6,\"1622\":1,\"1626\":1,\"1720\":1,\"1721\":1,\"1735\":1,\"1737\":2,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1748\":1,\"1751\":1,\"1756\":1,\"1757\":1,\"1759\":1,\"1764\":2,\"1771\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1794\":1,\"1795\":2,\"1796\":1,\"1808\":1,\"1809\":1,\"1817\":1,\"1818\":1,\"1820\":1,\"1822\":2,\"1837\":1,\"1842\":1,\"1847\":6,\"1858\":1,\"1867\":4,\"1869\":1,\"1892\":1,\"1917\":1,\"1926\":1,\"1940\":1,\"1942\":1,\"1947\":1,\"1957\":1,\"1960\":1,\"1961\":1,\"1992\":1,\"1995\":1,\"2129\":3,\"2218\":2,\"2220\":1,\"2226\":2,\"2235\":1,\"2236\":1,\"2239\":17,\"2240\":17,\"2241\":1,\"2411\":19,\"2412\":17,\"2413\":1,\"2423\":13,\"2424\":1,\"2432\":24,\"2447\":17,\"2448\":1}}],[\"transform\",{\"0\":{\"1634\":2,\"1635\":1,\"1636\":1,\"1703\":1,\"1717\":1,\"1728\":1,\"1734\":1,\"1765\":1,\"1767\":1,\"1776\":1,\"1777\":2,\"1791\":1,\"1802\":1,\"1813\":1,\"1831\":1,\"1832\":1,\"1833\":1,\"1835\":1,\"1836\":1,\"1840\":1,\"1841\":1,\"1844\":2,\"1845\":1,\"1850\":1,\"1852\":1,\"1853\":1,\"1857\":1,\"1875\":1,\"1884\":1,\"1899\":1,\"1900\":1,\"1922\":1,\"1923\":1,\"1924\":1,\"1925\":1,\"1929\":1,\"1930\":1},\"1\":{\"82\":1,\"644\":1,\"784\":1,\"1071\":1,\"1073\":1,\"1250\":1,\"1251\":2,\"1547\":1,\"1634\":2,\"1635\":1,\"1636\":1,\"1703\":1,\"1717\":1,\"1728\":1,\"1734\":1,\"1776\":1,\"1777\":2,\"1791\":1,\"1794\":1,\"1802\":1,\"1813\":1,\"1832\":1,\"1833\":1,\"1835\":1,\"1836\":1,\"1844\":3,\"1845\":1,\"1846\":2,\"1850\":1,\"1852\":1,\"1853\":1,\"1857\":1,\"1875\":1,\"1876\":1,\"1899\":1,\"1900\":1,\"1923\":1,\"1924\":1,\"1925\":1}}],[\"transfomer\",{\"1\":{\"692\":1,\"1992\":1,\"1995\":1}}],[\"transfer\",{\"0\":{\"50\":1,\"88\":1},\"1\":{\"49\":1,\"50\":5,\"52\":1,\"178\":1,\"269\":1,\"278\":1,\"1066\":1,\"1328\":1,\"1329\":1,\"1403\":2,\"2425\":1,\"2429\":1,\"2430\":1}}],[\"tunethresholdfromscore\",{\"0\":{\"2506\":1},\"1\":{\"2506\":1}}],[\"tuned\",{\"1\":{\"243\":1}}],[\"tune\",{\"1\":{\"240\":2,\"243\":6,\"817\":1}}],[\"tuning\",{\"0\":{\"88\":1},\"1\":{\"5\":1,\"178\":1,\"184\":1,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"224\":2,\"243\":3,\"262\":1,\"267\":12,\"272\":1,\"276\":3,\"282\":1,\"286\":28,\"289\":3,\"290\":1,\"699\":1,\"747\":1,\"819\":1,\"2176\":1,\"2249\":1}}],[\"turns\",{\"1\":{\"821\":1}}],[\"turnlist\",{\"1\":{\"528\":1}}],[\"turned\",{\"1\":{\"104\":1}}],[\"turn\",{\"1\":{\"81\":1,\"162\":1,\"1533\":1}}],[\"tupleofint\",{\"1\":{\"1392\":2}}],[\"tuples\",{\"1\":{\"1014\":2,\"1368\":2,\"2130\":1,\"2132\":1,\"2136\":1,\"2156\":1,\"2353\":1,\"2364\":1}}],[\"tuple\",{\"0\":{\"1336\":1,\"1503\":1},\"1\":{\"43\":3,\"82\":2,\"141\":4,\"614\":10,\"617\":2,\"618\":2,\"619\":1,\"620\":6,\"621\":2,\"622\":1,\"623\":1,\"624\":2,\"625\":3,\"626\":1,\"627\":1,\"628\":1,\"630\":1,\"631\":2,\"633\":1,\"634\":3,\"637\":3,\"639\":1,\"641\":13,\"642\":1,\"643\":3,\"644\":1,\"647\":1,\"649\":4,\"651\":2,\"654\":1,\"663\":1,\"665\":2,\"666\":1,\"667\":1,\"671\":1,\"672\":1,\"674\":2,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"691\":1,\"692\":9,\"699\":1,\"700\":1,\"701\":2,\"702\":2,\"706\":1,\"709\":1,\"710\":3,\"711\":3,\"712\":1,\"720\":1,\"733\":1,\"734\":1,\"735\":2,\"736\":2,\"737\":4,\"738\":1,\"740\":1,\"743\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"752\":1,\"756\":3,\"759\":1,\"760\":8,\"761\":1,\"762\":1,\"763\":2,\"765\":1,\"768\":1,\"770\":1,\"771\":1,\"772\":1,\"773\":3,\"774\":1,\"775\":3,\"777\":2,\"778\":1,\"780\":1,\"787\":1,\"790\":7,\"791\":1,\"797\":2,\"798\":1,\"815\":1,\"820\":8,\"831\":1,\"832\":2,\"846\":1,\"847\":14,\"849\":1,\"850\":7,\"862\":1,\"864\":1,\"866\":1,\"867\":1,\"908\":1,\"943\":1,\"952\":1,\"954\":2,\"956\":1,\"958\":3,\"959\":1,\"963\":1,\"965\":1,\"967\":1,\"968\":1,\"969\":2,\"974\":2,\"978\":1,\"980\":1,\"1021\":1,\"1026\":1,\"1028\":1,\"1029\":3,\"1030\":1,\"1034\":1,\"1038\":1,\"1039\":1,\"1040\":2,\"1042\":1,\"1044\":2,\"1053\":1,\"1054\":1,\"1062\":2,\"1064\":2,\"1086\":4,\"1107\":1,\"1117\":1,\"1118\":1,\"1124\":8,\"1125\":17,\"1126\":3,\"1127\":2,\"1130\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1147\":6,\"1155\":2,\"1156\":3,\"1157\":3,\"1158\":3,\"1162\":1,\"1180\":3,\"1181\":4,\"1199\":3,\"1207\":4,\"1217\":1,\"1232\":1,\"1235\":4,\"1252\":1,\"1261\":1,\"1262\":1,\"1267\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":1,\"1280\":7,\"1281\":2,\"1282\":3,\"1283\":1,\"1290\":1,\"1314\":1,\"1316\":2,\"1320\":2,\"1334\":1,\"1335\":1,\"1336\":3,\"1348\":1,\"1368\":2,\"1374\":1,\"1375\":1,\"1392\":2,\"1403\":1,\"1444\":3,\"1448\":3,\"1452\":1,\"1456\":1,\"1460\":2,\"1482\":2,\"1493\":1,\"1494\":3,\"1503\":1,\"1505\":1,\"1506\":3,\"1515\":2,\"1516\":2,\"1522\":1,\"1539\":1,\"1546\":1,\"1552\":4,\"1581\":1,\"1584\":1,\"1586\":1,\"1588\":1,\"1599\":2,\"1603\":1,\"1611\":1,\"1613\":1,\"1622\":1,\"1626\":4,\"1627\":1,\"1632\":1,\"1640\":3,\"1641\":3,\"1644\":1,\"1647\":1,\"1652\":1,\"1655\":4,\"1656\":2,\"1659\":1,\"1660\":1,\"1662\":1,\"1669\":2,\"1671\":1,\"1700\":1,\"1702\":5,\"1712\":2,\"1719\":8,\"1720\":3,\"1723\":3,\"1724\":3,\"1725\":8,\"1726\":1,\"1727\":1,\"1731\":4,\"1736\":7,\"1749\":3,\"1762\":1,\"1766\":1,\"1775\":2,\"1787\":5,\"1798\":2,\"1799\":2,\"1800\":2,\"1805\":3,\"1806\":5,\"1815\":14,\"1822\":3,\"1843\":11,\"1851\":3,\"1856\":2,\"1863\":1,\"1866\":1,\"1881\":1,\"1891\":1,\"1898\":1,\"1913\":1,\"1937\":1,\"1938\":1,\"1940\":2,\"1941\":1,\"1942\":3,\"1943\":1,\"1944\":7,\"1945\":4,\"1946\":4,\"1947\":7,\"1951\":2,\"1959\":4,\"1960\":1,\"1961\":1,\"1965\":2,\"1971\":1,\"1972\":1,\"1975\":5,\"1978\":1,\"1980\":1,\"1982\":1,\"1992\":5,\"1993\":1,\"1995\":1,\"1996\":2,\"1997\":4,\"2000\":2,\"2001\":2,\"2002\":1,\"2003\":1,\"2004\":1,\"2007\":1,\"2008\":2,\"2040\":3,\"2044\":5,\"2045\":3,\"2054\":3,\"2126\":1,\"2127\":4,\"2129\":1,\"2130\":5,\"2132\":1,\"2133\":8,\"2136\":12,\"2137\":3,\"2149\":1,\"2155\":2,\"2156\":1,\"2161\":1,\"2167\":1,\"2176\":1,\"2184\":2,\"2191\":1,\"2198\":1,\"2207\":1,\"2215\":1,\"2216\":2,\"2217\":1,\"2218\":1,\"2219\":1,\"2221\":4,\"2222\":1,\"2228\":1,\"2229\":1,\"2232\":2,\"2235\":2,\"2236\":2,\"2238\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2245\":1,\"2246\":4,\"2247\":4,\"2248\":4,\"2249\":4,\"2250\":4,\"2251\":4,\"2252\":4,\"2253\":4,\"2254\":4,\"2255\":4,\"2256\":4,\"2257\":4,\"2259\":4,\"2260\":4,\"2261\":4,\"2262\":2,\"2263\":4,\"2264\":4,\"2265\":2,\"2266\":4,\"2267\":4,\"2268\":4,\"2269\":4,\"2270\":4,\"2271\":4,\"2272\":4,\"2273\":4,\"2287\":1,\"2309\":1,\"2324\":1,\"2325\":1,\"2334\":1,\"2338\":2,\"2342\":1,\"2343\":1,\"2344\":2,\"2347\":1,\"2351\":1,\"2352\":1,\"2353\":10,\"2355\":1,\"2359\":5,\"2364\":10,\"2366\":2,\"2369\":2,\"2371\":1,\"2373\":2,\"2376\":2,\"2401\":1,\"2403\":1,\"2405\":1,\"2407\":1,\"2408\":1,\"2409\":1,\"2411\":1,\"2412\":1,\"2413\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2423\":1,\"2424\":1,\"2431\":1,\"2432\":1,\"2434\":1,\"2443\":1,\"2445\":1,\"2446\":1,\"2447\":1,\"2448\":1,\"2449\":1,\"2453\":1,\"2462\":2,\"2499\":1,\"2501\":1}}],[\"tutorial\",{\"1\":{\"36\":1,\"37\":1,\"108\":1,\"138\":1,\"171\":1,\"172\":1,\"190\":2,\"191\":4,\"192\":2,\"193\":1,\"208\":2}}],[\"tau\",{\"1\":{\"2434\":1,\"2435\":2,\"2438\":2,\"2439\":2,\"2440\":2}}],[\"ta\",{\"0\":{\"2085\":1,\"2086\":1,\"2091\":1}}],[\"tarfile\",{\"1\":{\"1956\":2}}],[\"tarinfo\",{\"1\":{\"1948\":2}}],[\"targetspeakerextractiontask\",{\"0\":{\"2272\":1},\"1\":{\"2272\":1}}],[\"targets\",{\"1\":{\"200\":1,\"205\":1,\"240\":1,\"786\":1,\"800\":1,\"846\":1,\"867\":1,\"921\":1,\"935\":1,\"960\":2,\"1264\":1,\"1754\":2,\"1755\":1,\"1928\":2,\"2218\":1,\"2219\":2,\"2239\":1}}],[\"target\",{\"0\":{\"1675\":1},\"1\":{\"50\":1,\"52\":1,\"78\":1,\"79\":1,\"81\":3,\"135\":1,\"145\":1,\"205\":3,\"242\":3,\"286\":2,\"368\":2,\"377\":2,\"449\":2,\"515\":1,\"535\":2,\"616\":1,\"627\":11,\"667\":2,\"674\":4,\"703\":3,\"706\":2,\"716\":2,\"717\":1,\"740\":11,\"748\":2,\"755\":2,\"785\":2,\"786\":1,\"800\":1,\"823\":1,\"824\":1,\"878\":3,\"879\":3,\"881\":3,\"882\":3,\"883\":3,\"884\":3,\"912\":4,\"919\":2,\"921\":1,\"922\":1,\"935\":1,\"936\":1,\"937\":1,\"1061\":3,\"1062\":3,\"1063\":3,\"1155\":1,\"1158\":1,\"1164\":3,\"1210\":2,\"1261\":1,\"1268\":2,\"1269\":3,\"1270\":3,\"1271\":2,\"1316\":5,\"1320\":4,\"1334\":6,\"1389\":1,\"1391\":2,\"1396\":1,\"1401\":1,\"1403\":2,\"1406\":2,\"1408\":1,\"1410\":2,\"1441\":2,\"1466\":1,\"1468\":2,\"1526\":2,\"1552\":4,\"1553\":2,\"1556\":5,\"1557\":1,\"1589\":1,\"1627\":2,\"1675\":3,\"1682\":1,\"1683\":2,\"1684\":2,\"1685\":2,\"1695\":1,\"1750\":2,\"1760\":1,\"1764\":2,\"1782\":4,\"1839\":2,\"1858\":1,\"1907\":2,\"1928\":1,\"1944\":1,\"1947\":1,\"1991\":2,\"1992\":2,\"1993\":2,\"1995\":2,\"2065\":3,\"2184\":1,\"2223\":1,\"2226\":3,\"2235\":3,\"2236\":3,\"2237\":2,\"2239\":2,\"2240\":2,\"2241\":4,\"2245\":2,\"2308\":1,\"2312\":1,\"2353\":3,\"2364\":3,\"2368\":1,\"2411\":2,\"2412\":2,\"2413\":4,\"2423\":2,\"2424\":4,\"2425\":1,\"2427\":2,\"2429\":1,\"2431\":2,\"2432\":2,\"2448\":4,\"2476\":1,\"2506\":2}}],[\"tadelayer\",{\"0\":{\"1620\":1},\"1\":{\"1620\":2}}],[\"taderesblock\",{\"0\":{\"1621\":1},\"1\":{\"1619\":1,\"1621\":3}}],[\"tade\",{\"0\":{\"1620\":1,\"1621\":1},\"1\":{\"1619\":1,\"1620\":2,\"1621\":1}}],[\"tattc\",{\"1\":{\"1071\":2}}],[\"tactoron2\",{\"1\":{\"1839\":1,\"2237\":1}}],[\"tac\",{\"0\":{\"1137\":1,\"1163\":1},\"1\":{\"1029\":2,\"1073\":1,\"1137\":2,\"1163\":1,\"1235\":3,\"1279\":1,\"1280\":3,\"1281\":3,\"1282\":3,\"1283\":1}}],[\"tacotron`\",{\"1\":{\"2224\":1}}],[\"tacotron\",{\"0\":{\"1991\":1,\"2223\":1,\"2227\":1,\"2231\":1,\"2242\":1,\"2243\":1,\"2245\":3},\"1\":{\"218\":2,\"265\":1,\"267\":4,\"272\":1,\"286\":13,\"288\":2,\"289\":1,\"290\":1,\"481\":1,\"1768\":1,\"1974\":1,\"1977\":1,\"1985\":1,\"1990\":1,\"1991\":2,\"2223\":3,\"2227\":3,\"2231\":4,\"2242\":1,\"2243\":1,\"2245\":7,\"2290\":1}}],[\"tacotron2loss\",{\"0\":{\"1839\":1},\"1\":{\"1839\":1}}],[\"tacotron2\",{\"0\":{\"1750\":1,\"1758\":1,\"1770\":1,\"1810\":1,\"1811\":1,\"1839\":1,\"1855\":1,\"1874\":1,\"1879\":1,\"2431\":3},\"1\":{\"218\":1,\"284\":1,\"286\":3,\"290\":3,\"536\":10,\"1750\":3,\"1758\":3,\"1770\":1,\"1810\":1,\"1811\":1,\"1839\":2,\"1855\":1,\"1874\":1,\"1879\":1,\"1993\":1,\"2237\":1,\"2431\":6}}],[\"taps=10\",{\"1\":{\"1376\":1,\"1853\":1}}],[\"taps\",{\"1\":{\"720\":1,\"738\":1,\"815\":1,\"1127\":1,\"1217\":1,\"1314\":6,\"1317\":4,\"1352\":1,\"1354\":1,\"1376\":2,\"1377\":3,\"1526\":1,\"1539\":1,\"1600\":1,\"1608\":3,\"1631\":4,\"1766\":1}}],[\"talk\",{\"1\":{\"240\":1,\"242\":2}}],[\"taslp\",{\"1\":{\"1270\":1,\"1271\":1}}],[\"taslp21\",{\"1\":{\"1125\":1}}],[\"tasnet\",{\"1\":{\"225\":1,\"978\":1,\"982\":2,\"1273\":2,\"1274\":2}}],[\"tasking\",{\"0\":{\"198\":1},\"1\":{\"198\":1,\"200\":5,\"201\":1,\"262\":1}}],[\"task=asr1\",{\"1\":{\"195\":1,\"197\":1}}],[\"task\",{\"0\":{\"44\":1,\"77\":1,\"78\":1,\"144\":1,\"667\":1,\"2249\":1,\"2258\":1},\"1\":{\"44\":2,\"45\":1,\"63\":1,\"69\":1,\"71\":1,\"78\":8,\"79\":3,\"81\":6,\"82\":4,\"84\":1,\"91\":1,\"96\":1,\"107\":1,\"119\":1,\"126\":1,\"127\":3,\"128\":2,\"138\":2,\"139\":1,\"144\":3,\"145\":1,\"162\":2,\"173\":1,\"190\":1,\"191\":1,\"195\":4,\"197\":6,\"198\":1,\"199\":1,\"200\":13,\"201\":4,\"205\":2,\"222\":2,\"223\":7,\"226\":2,\"228\":3,\"240\":1,\"242\":6,\"263\":1,\"267\":3,\"286\":7,\"301\":2,\"335\":2,\"342\":2,\"349\":2,\"421\":2,\"429\":2,\"463\":2,\"475\":2,\"511\":2,\"625\":2,\"667\":1,\"733\":1,\"980\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1204\":1,\"1209\":1,\"1228\":1,\"1395\":1,\"1521\":1,\"1585\":1,\"1938\":1,\"2132\":2,\"2134\":3,\"2184\":4,\"2228\":1,\"2229\":1,\"2246\":5,\"2247\":11,\"2248\":5,\"2249\":14,\"2250\":5,\"2251\":5,\"2252\":5,\"2253\":5,\"2254\":6,\"2255\":6,\"2256\":6,\"2257\":5,\"2258\":1,\"2259\":5,\"2260\":5,\"2261\":5,\"2262\":5,\"2263\":5,\"2264\":5,\"2265\":5,\"2266\":5,\"2267\":5,\"2268\":5,\"2269\":5,\"2270\":5,\"2271\":5,\"2272\":5,\"2273\":5,\"2283\":1,\"2284\":1,\"2293\":1,\"2325\":4,\"2327\":2,\"2336\":2,\"2337\":2,\"2344\":3,\"2345\":2,\"2346\":1,\"2356\":1,\"2363\":1,\"2377\":1,\"2408\":1,\"2446\":1}}],[\"tasks\",{\"0\":{\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2548\":1},\"1\":{\"19\":1,\"78\":1,\"101\":1,\"128\":1,\"144\":3,\"162\":2,\"163\":1,\"175\":2,\"191\":1,\"194\":1,\"200\":4,\"201\":3,\"202\":1,\"205\":1,\"208\":1,\"220\":1,\"223\":8,\"225\":2,\"228\":3,\"240\":2,\"242\":2,\"247\":1,\"254\":1,\"263\":2,\"286\":1,\"290\":1,\"691\":1,\"846\":1,\"912\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1779\":1,\"2134\":1,\"2142\":2,\"2184\":2,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":2,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2325\":2,\"2327\":1,\"2344\":1,\"2353\":1,\"2364\":1,\"2377\":1}}],[\"tables\",{\"1\":{\"261\":1,\"536\":1}}],[\"table\",{\"0\":{\"199\":1,\"204\":1,\"210\":1,\"216\":1,\"227\":1,\"234\":1,\"241\":1,\"253\":1,\"265\":1,\"274\":1,\"284\":1},\"1\":{\"194\":1,\"222\":1,\"234\":1,\"240\":1,\"260\":1,\"274\":1,\"284\":1}}],[\"tag=\",{\"1\":{\"220\":1,\"290\":4}}],[\"tag=medium\",{\"1\":{\"126\":1}}],[\"tag\",{\"1\":{\"125\":1,\"126\":4,\"200\":1,\"220\":1,\"267\":6,\"276\":8,\"285\":1,\"286\":17,\"290\":1,\"301\":2,\"309\":2,\"321\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"368\":2,\"389\":2,\"396\":2,\"406\":2,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":2,\"457\":2,\"463\":2,\"484\":4,\"490\":4,\"498\":2,\"505\":2,\"1040\":1,\"1268\":4,\"1957\":3,\"2043\":3,\"2045\":3,\"2049\":3,\"2055\":3,\"2056\":3,\"2066\":3,\"2133\":2,\"2136\":6,\"2140\":2,\"2148\":2}}],[\"tags=none\",{\"1\":{\"2480\":1}}],[\"tags\",{\"0\":{\"28\":1},\"1\":{\"200\":1}}],[\"tails\",{\"1\":{\"2281\":1}}],[\"tails=\",{\"1\":{\"1636\":1}}],[\"tails=none\",{\"1\":{\"1634\":1}}],[\"tail\",{\"1\":{\"39\":1,\"113\":1,\"1581\":3,\"1634\":1,\"1636\":1,\"1934\":1}}],[\"tao\",{\"1\":{\"13\":1,\"202\":1}}],[\"tanaka21\",{\"1\":{\"1131\":1,\"1172\":1}}],[\"tanaka\",{\"1\":{\"1131\":1,\"1172\":1}}],[\"tanh\",{\"1\":{\"143\":1,\"632\":1,\"635\":1,\"650\":1,\"1107\":1,\"1117\":2,\"1130\":2,\"1131\":2,\"1136\":1,\"1141\":1,\"1232\":1,\"1261\":1,\"1267\":1,\"1268\":2,\"1278\":1,\"1555\":1,\"1749\":1}}],[\"tang\",{\"1\":{\"10\":1}}],[\"tan\",{\"1\":{\"9\":1,\"1125\":1}}],[\"taking\",{\"1\":{\"128\":1,\"130\":1}}],[\"takashi\",{\"1\":{\"207\":1}}],[\"takatomo\",{\"1\":{\"15\":1}}],[\"takamichi\",{\"1\":{\"9\":1}}],[\"takaaki\",{\"1\":{\"9\":1,\"156\":1}}],[\"taken\",{\"1\":{\"786\":1,\"800\":1,\"921\":1,\"935\":1,\"1228\":1,\"2101\":1,\"2162\":1,\"2489\":1}}],[\"takenori\",{\"1\":{\"9\":2}}],[\"takes\",{\"1\":{\"50\":1,\"106\":1,\"228\":1,\"232\":1,\"258\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":2,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1113\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1245\":1,\"1249\":1,\"1251\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"take\",{\"1\":{\"18\":1,\"51\":1,\"139\":1,\"168\":1,\"224\":1,\"242\":2,\"262\":2,\"269\":1,\"278\":1,\"286\":1,\"1279\":1,\"1711\":1,\"1712\":1}}],[\"takeda\",{\"1\":{\"9\":1}}],[\"takuya\",{\"1\":{\"8\":1}}],[\"tesnor\",{\"1\":{\"2407\":1}}],[\"test=false\",{\"1\":{\"1066\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1210\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1275\":1,\"1276\":1,\"1277\":1}}],[\"testing\",{\"1\":{\"821\":1,\"827\":1,\"1020\":1,\"1025\":1}}],[\"tests\",{\"1\":{\"222\":1,\"224\":1,\"225\":3}}],[\"test2\",{\"1\":{\"197\":2}}],[\"test1\",{\"1\":{\"197\":2}}],[\"tested\",{\"1\":{\"63\":1,\"106\":1,\"286\":1}}],[\"test\",{\"0\":{\"1360\":1,\"2122\":1},\"1\":{\"26\":2,\"32\":3,\"33\":1,\"37\":4,\"126\":4,\"136\":3,\"162\":5,\"195\":2,\"196\":2,\"197\":4,\"201\":5,\"205\":2,\"213\":1,\"224\":1,\"225\":6,\"242\":1,\"243\":4,\"268\":1,\"277\":1,\"284\":1,\"285\":3,\"286\":5,\"290\":1,\"1037\":1,\"1155\":2,\"1157\":2,\"1174\":1,\"1276\":1,\"1360\":1}}],[\"temb=none\",{\"1\":{\"1238\":1,\"1240\":1,\"1242\":1}}],[\"temb\",{\"1\":{\"1119\":3,\"1120\":2,\"1122\":2,\"1238\":1,\"1240\":1,\"1242\":1}}],[\"tempo\",{\"1\":{\"992\":1,\"1016\":1,\"1526\":1,\"1833\":3,\"2227\":1}}],[\"temporarily\",{\"1\":{\"2044\":3}}],[\"temporary\",{\"1\":{\"3\":2,\"527\":1,\"2354\":2}}],[\"temporally\",{\"1\":{\"2136\":1}}],[\"temporalconvnetinformed\",{\"0\":{\"1274\":1},\"1\":{\"1274\":1}}],[\"temporalconvnet\",{\"0\":{\"982\":1,\"1273\":1},\"1\":{\"982\":1,\"1273\":1,\"1274\":1}}],[\"temporalblock\",{\"0\":{\"981\":1,\"1272\":1},\"1\":{\"981\":1,\"1272\":1}}],[\"temporal\",{\"1\":{\"980\":1,\"1267\":1,\"1309\":1,\"1310\":1,\"1315\":1,\"1805\":1}}],[\"temp\",{\"1\":{\"674\":4,\"1726\":1}}],[\"temperature\",{\"1\":{\"126\":1,\"1590\":1,\"2049\":1,\"2462\":3}}],[\"templates\",{\"1\":{\"2131\":1}}],[\"template\",{\"0\":{\"194\":1,\"273\":1},\"1\":{\"68\":1,\"194\":2,\"195\":3,\"197\":1,\"198\":1,\"203\":1,\"209\":1,\"213\":1,\"215\":1,\"220\":1,\"221\":1,\"223\":4,\"224\":3,\"227\":3,\"228\":1,\"229\":1,\"230\":1,\"231\":1,\"233\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":1,\"243\":2,\"245\":1,\"250\":1,\"251\":1,\"252\":1,\"257\":1,\"264\":1,\"268\":1,\"273\":1,\"274\":1,\"277\":1,\"283\":1,\"289\":1,\"291\":1,\"292\":1,\"2131\":1,\"2142\":2,\"2143\":1,\"2344\":1}}],[\"team\",{\"1\":{\"1053\":2,\"1308\":1}}],[\"teacher\",{\"1\":{\"285\":1,\"286\":16,\"290\":1,\"406\":2,\"475\":2,\"484\":2,\"490\":2,\"1526\":3,\"1552\":3,\"1553\":3,\"1598\":3,\"1599\":3,\"1625\":3,\"1626\":4,\"1750\":2,\"1976\":1,\"1993\":3,\"2224\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":3,\"2411\":3,\"2412\":3,\"2423\":3,\"2431\":3,\"2432\":3,\"2447\":3}}],[\"technology\",{\"1\":{\"2000\":1,\"2001\":1}}],[\"techniques\",{\"1\":{\"232\":1,\"258\":1,\"260\":1,\"262\":2,\"2176\":1}}],[\"technique\",{\"1\":{\"98\":1,\"102\":1,\"882\":1,\"883\":1,\"884\":1}}],[\"techqnique\",{\"1\":{\"536\":1}}],[\"terhardt\",{\"1\":{\"1654\":1}}],[\"ter\",{\"1\":{\"223\":1}}],[\"terminal\",{\"1\":{\"110\":1}}],[\"term\",{\"1\":{\"96\":1,\"691\":3,\"846\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1318\":1,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1330\":1,\"1441\":1,\"1756\":1,\"1757\":1,\"1789\":1,\"1790\":1}}],[\"terms\",{\"1\":{\"59\":1,\"1638\":1}}],[\"tells\",{\"1\":{\"2355\":1}}],[\"tell\",{\"1\":{\"62\":1}}],[\"tensor2\",{\"1\":{\"2427\":2}}],[\"tensor1\",{\"1\":{\"2427\":2}}],[\"tensordot\",{\"1\":{\"1299\":1}}],[\"tensororint\",{\"1\":{\"1311\":1,\"1318\":1,\"1319\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1328\":1,\"1330\":1}}],[\"tensororcomplextensor\",{\"1\":{\"980\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1125\":1,\"1130\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1268\":2,\"1278\":1,\"1280\":1,\"1283\":1}}],[\"tensorornone\",{\"1\":{\"702\":1,\"712\":1,\"1745\":1,\"2220\":1}}],[\"tensor=\",{\"1\":{\"628\":2,\"743\":1,\"1762\":1}}],[\"tensors\",{\"0\":{\"1478\":1,\"2116\":1},\"1\":{\"82\":1,\"639\":1,\"701\":1,\"735\":1,\"756\":5,\"773\":5,\"832\":2,\"866\":2,\"867\":2,\"1157\":1,\"1269\":2,\"1270\":2,\"1271\":1,\"1318\":1,\"1334\":1,\"1354\":1,\"1390\":1,\"1397\":1,\"1402\":1,\"1409\":1,\"1467\":1,\"1478\":3,\"1513\":1,\"1514\":1,\"1515\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1604\":1,\"1606\":1,\"1725\":1,\"1737\":2,\"1750\":2,\"1751\":1,\"1795\":2,\"1810\":1,\"1812\":2,\"1908\":2,\"1928\":2,\"2065\":1,\"2116\":2,\"2130\":4,\"2133\":1,\"2138\":1,\"2223\":2}}],[\"tensor\",{\"0\":{\"899\":1,\"915\":1,\"1339\":1,\"1919\":1,\"1932\":1},\"1\":{\"48\":1,\"82\":12,\"102\":1,\"614\":37,\"615\":2,\"616\":7,\"617\":13,\"618\":13,\"619\":5,\"620\":17,\"621\":4,\"622\":5,\"623\":5,\"624\":13,\"625\":16,\"626\":8,\"627\":6,\"628\":4,\"629\":2,\"630\":4,\"631\":4,\"632\":3,\"633\":13,\"634\":18,\"635\":2,\"636\":10,\"637\":13,\"638\":2,\"639\":7,\"640\":2,\"641\":26,\"642\":4,\"643\":16,\"644\":45,\"645\":3,\"646\":1,\"647\":5,\"648\":2,\"649\":17,\"650\":2,\"651\":6,\"652\":2,\"654\":15,\"667\":6,\"669\":2,\"670\":2,\"675\":12,\"676\":6,\"678\":6,\"680\":4,\"682\":4,\"684\":4,\"686\":4,\"691\":5,\"692\":31,\"696\":8,\"697\":7,\"699\":13,\"700\":14,\"701\":11,\"702\":8,\"703\":28,\"706\":17,\"709\":15,\"710\":27,\"711\":27,\"712\":7,\"715\":3,\"716\":1,\"717\":5,\"720\":4,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"733\":15,\"734\":15,\"735\":11,\"736\":7,\"737\":11,\"738\":4,\"740\":6,\"743\":7,\"744\":1,\"745\":8,\"746\":11,\"747\":8,\"748\":10,\"749\":1,\"752\":4,\"755\":28,\"756\":3,\"759\":6,\"760\":17,\"761\":4,\"762\":4,\"763\":8,\"764\":1,\"765\":4,\"768\":4,\"770\":6,\"771\":9,\"772\":4,\"773\":3,\"774\":14,\"775\":7,\"776\":1,\"777\":16,\"778\":4,\"780\":14,\"783\":3,\"784\":26,\"785\":21,\"786\":4,\"787\":9,\"790\":19,\"791\":6,\"792\":8,\"793\":2,\"794\":8,\"795\":1,\"797\":5,\"798\":6,\"800\":4,\"802\":1,\"803\":2,\"806\":1,\"815\":4,\"820\":17,\"824\":2,\"828\":4,\"829\":5,\"830\":4,\"831\":7,\"832\":2,\"846\":10,\"847\":30,\"849\":9,\"850\":25,\"851\":3,\"854\":2,\"859\":1,\"862\":6,\"864\":4,\"865\":6,\"866\":3,\"867\":7,\"878\":17,\"879\":17,\"880\":2,\"881\":20,\"882\":18,\"883\":18,\"884\":21,\"886\":2,\"899\":2,\"900\":2,\"910\":1,\"915\":2,\"918\":2,\"919\":9,\"921\":4,\"922\":8,\"931\":1,\"933\":1,\"935\":4,\"936\":8,\"937\":8,\"939\":5,\"947\":6,\"948\":5,\"949\":6,\"950\":1,\"951\":1,\"952\":4,\"954\":13,\"955\":4,\"956\":4,\"958\":17,\"959\":4,\"962\":3,\"963\":5,\"965\":4,\"967\":4,\"968\":4,\"969\":2,\"974\":18,\"976\":2,\"977\":4,\"978\":11,\"979\":6,\"980\":7,\"1029\":4,\"1030\":4,\"1031\":3,\"1032\":3,\"1034\":4,\"1035\":2,\"1036\":1,\"1038\":4,\"1039\":4,\"1040\":6,\"1042\":1,\"1044\":4,\"1045\":1,\"1050\":4,\"1053\":13,\"1054\":5,\"1061\":4,\"1062\":10,\"1063\":4,\"1064\":1,\"1066\":3,\"1070\":2,\"1071\":2,\"1073\":2,\"1086\":2,\"1107\":10,\"1112\":7,\"1113\":7,\"1116\":4,\"1117\":10,\"1118\":20,\"1119\":5,\"1124\":2,\"1125\":10,\"1126\":16,\"1127\":14,\"1130\":10,\"1131\":7,\"1132\":3,\"1133\":5,\"1136\":10,\"1141\":10,\"1145\":1,\"1147\":2,\"1155\":11,\"1156\":28,\"1157\":27,\"1158\":23,\"1161\":4,\"1162\":11,\"1167\":3,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1175\":1,\"1176\":2,\"1180\":2,\"1181\":2,\"1189\":4,\"1198\":4,\"1199\":3,\"1204\":5,\"1207\":2,\"1208\":4,\"1209\":4,\"1210\":2,\"1217\":10,\"1218\":4,\"1221\":4,\"1222\":4,\"1223\":4,\"1228\":3,\"1229\":4,\"1232\":11,\"1235\":4,\"1244\":4,\"1245\":1,\"1246\":5,\"1247\":5,\"1248\":3,\"1250\":3,\"1251\":7,\"1252\":11,\"1253\":2,\"1261\":11,\"1264\":4,\"1267\":11,\"1268\":12,\"1269\":10,\"1270\":10,\"1271\":9,\"1275\":1,\"1277\":1,\"1278\":10,\"1279\":2,\"1280\":11,\"1281\":2,\"1282\":2,\"1283\":11,\"1291\":3,\"1294\":1,\"1298\":2,\"1301\":3,\"1306\":2,\"1308\":2,\"1309\":5,\"1310\":5,\"1311\":5,\"1314\":4,\"1315\":3,\"1317\":3,\"1318\":4,\"1319\":4,\"1321\":2,\"1322\":5,\"1323\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1330\":1,\"1331\":1,\"1334\":18,\"1336\":1,\"1337\":2,\"1339\":1,\"1342\":3,\"1348\":3,\"1350\":3,\"1351\":3,\"1352\":3,\"1354\":5,\"1355\":1,\"1356\":3,\"1357\":2,\"1359\":1,\"1367\":1,\"1371\":2,\"1372\":3,\"1376\":2,\"1377\":3,\"1381\":4,\"1385\":3,\"1389\":22,\"1390\":4,\"1391\":14,\"1392\":1,\"1395\":25,\"1397\":4,\"1400\":3,\"1401\":22,\"1402\":4,\"1403\":17,\"1406\":8,\"1408\":22,\"1409\":4,\"1410\":14,\"1415\":1,\"1419\":8,\"1422\":1,\"1432\":8,\"1439\":4,\"1441\":10,\"1466\":22,\"1467\":4,\"1468\":14,\"1475\":2,\"1478\":1,\"1484\":1,\"1493\":1,\"1494\":1,\"1505\":1,\"1506\":1,\"1508\":2,\"1509\":3,\"1511\":3,\"1513\":8,\"1514\":4,\"1515\":6,\"1516\":5,\"1519\":9,\"1520\":8,\"1521\":185,\"1522\":4,\"1524\":8,\"1525\":6,\"1526\":51,\"1529\":20,\"1533\":4,\"1534\":3,\"1535\":7,\"1536\":10,\"1539\":4,\"1545\":5,\"1546\":22,\"1548\":13,\"1551\":10,\"1552\":80,\"1553\":60,\"1556\":27,\"1558\":2,\"1559\":5,\"1576\":2,\"1577\":9,\"1581\":16,\"1582\":6,\"1583\":12,\"1584\":16,\"1585\":84,\"1586\":13,\"1587\":10,\"1588\":10,\"1589\":8,\"1590\":7,\"1591\":8,\"1592\":18,\"1593\":3,\"1594\":4,\"1595\":4,\"1596\":4,\"1597\":4,\"1598\":49,\"1599\":83,\"1600\":31,\"1601\":13,\"1602\":9,\"1603\":13,\"1604\":4,\"1605\":12,\"1606\":4,\"1607\":11,\"1608\":12,\"1609\":5,\"1610\":14,\"1611\":22,\"1612\":12,\"1613\":16,\"1614\":6,\"1615\":6,\"1616\":16,\"1617\":6,\"1618\":4,\"1619\":15,\"1620\":11,\"1621\":11,\"1622\":18,\"1623\":2,\"1624\":5,\"1625\":57,\"1626\":89,\"1627\":18,\"1628\":12,\"1629\":7,\"1632\":12,\"1633\":9,\"1637\":8,\"1640\":18,\"1641\":18,\"1644\":1,\"1647\":1,\"1652\":4,\"1654\":3,\"1656\":8,\"1659\":4,\"1660\":4,\"1661\":3,\"1662\":4,\"1664\":2,\"1665\":2,\"1666\":3,\"1667\":3,\"1668\":10,\"1669\":9,\"1670\":2,\"1671\":4,\"1672\":3,\"1673\":3,\"1676\":2,\"1678\":2,\"1679\":2,\"1680\":2,\"1686\":2,\"1687\":4,\"1689\":3,\"1690\":3,\"1691\":2,\"1692\":2,\"1694\":2,\"1697\":3,\"1698\":2,\"1699\":1,\"1700\":4,\"1702\":22,\"1704\":5,\"1705\":3,\"1706\":3,\"1707\":5,\"1708\":5,\"1709\":6,\"1710\":5,\"1711\":5,\"1712\":3,\"1713\":4,\"1714\":4,\"1715\":4,\"1716\":4,\"1719\":36,\"1720\":11,\"1721\":4,\"1722\":13,\"1723\":9,\"1724\":9,\"1725\":45,\"1726\":3,\"1727\":3,\"1730\":3,\"1731\":19,\"1733\":4,\"1735\":28,\"1736\":12,\"1737\":2,\"1738\":6,\"1739\":6,\"1740\":6,\"1741\":6,\"1742\":6,\"1743\":6,\"1744\":6,\"1745\":6,\"1746\":6,\"1747\":4,\"1748\":3,\"1749\":17,\"1750\":12,\"1751\":16,\"1753\":3,\"1754\":2,\"1756\":6,\"1757\":6,\"1758\":4,\"1759\":10,\"1760\":4,\"1762\":7,\"1764\":7,\"1768\":6,\"1770\":2,\"1771\":2,\"1775\":8,\"1779\":3,\"1782\":3,\"1783\":4,\"1784\":5,\"1785\":16,\"1786\":6,\"1787\":9,\"1788\":3,\"1789\":6,\"1790\":6,\"1794\":26,\"1795\":2,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":5,\"1803\":3,\"1805\":9,\"1806\":20,\"1807\":10,\"1808\":5,\"1810\":3,\"1812\":2,\"1814\":4,\"1815\":30,\"1816\":4,\"1817\":16,\"1818\":5,\"1820\":4,\"1822\":9,\"1837\":5,\"1839\":7,\"1843\":31,\"1847\":3,\"1848\":6,\"1849\":4,\"1851\":9,\"1854\":7,\"1856\":6,\"1858\":5,\"1862\":2,\"1870\":2,\"1888\":2,\"1901\":6,\"1902\":6,\"1903\":7,\"1904\":6,\"1905\":5,\"1906\":2,\"1907\":5,\"1908\":2,\"1909\":4,\"1916\":1,\"1919\":8,\"1920\":2,\"1926\":2,\"1928\":1,\"1931\":5,\"1932\":4,\"1933\":6,\"1934\":4,\"1938\":4,\"1940\":12,\"1941\":4,\"1942\":12,\"1943\":4,\"1944\":16,\"1945\":12,\"1946\":9,\"1947\":16,\"1950\":3,\"1951\":2,\"1957\":2,\"1959\":16,\"1960\":6,\"1961\":6,\"1965\":17,\"1966\":2,\"1967\":1,\"1969\":1,\"1971\":10,\"1972\":4,\"1975\":23,\"1976\":8,\"1978\":4,\"1980\":4,\"1982\":4,\"1984\":14,\"1987\":2,\"1990\":4,\"1991\":13,\"1992\":25,\"1993\":27,\"1995\":13,\"1996\":28,\"1997\":26,\"2007\":1,\"2124\":1,\"2126\":4,\"2127\":22,\"2128\":1,\"2129\":9,\"2130\":1,\"2131\":1,\"2133\":4,\"2136\":8,\"2137\":1,\"2155\":6,\"2167\":5,\"2170\":2,\"2172\":2,\"2174\":2,\"2176\":5,\"2183\":5,\"2184\":23,\"2187\":5,\"2188\":1,\"2190\":5,\"2191\":8,\"2192\":1,\"2198\":2,\"2207\":5,\"2208\":5,\"2215\":3,\"2216\":16,\"2217\":4,\"2218\":10,\"2219\":7,\"2220\":6,\"2221\":20,\"2222\":10,\"2223\":7,\"2224\":5,\"2226\":20,\"2227\":2,\"2228\":215,\"2229\":201,\"2231\":4,\"2232\":16,\"2235\":45,\"2236\":45,\"2237\":5,\"2238\":18,\"2239\":49,\"2240\":44,\"2241\":26,\"2245\":45,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2308\":2,\"2309\":3,\"2318\":1,\"2319\":1,\"2320\":1,\"2325\":6,\"2327\":9,\"2332\":2,\"2338\":2,\"2347\":2,\"2354\":1,\"2355\":4,\"2365\":2,\"2367\":3,\"2369\":3,\"2371\":2,\"2373\":6,\"2374\":1,\"2376\":2,\"2398\":2,\"2401\":4,\"2403\":10,\"2405\":7,\"2407\":5,\"2408\":114,\"2409\":7,\"2411\":32,\"2412\":45,\"2413\":26,\"2414\":4,\"2416\":4,\"2418\":4,\"2420\":6,\"2422\":8,\"2423\":45,\"2424\":26,\"2425\":4,\"2426\":12,\"2427\":10,\"2428\":40,\"2429\":4,\"2430\":4,\"2431\":30,\"2432\":30,\"2433\":5,\"2434\":7,\"2435\":6,\"2437\":2,\"2438\":1,\"2439\":2,\"2441\":2,\"2442\":2,\"2443\":2,\"2445\":10,\"2446\":126,\"2447\":45,\"2448\":26,\"2449\":2,\"2451\":3,\"2453\":5,\"2455\":6,\"2456\":1,\"2458\":2,\"2460\":5,\"2462\":20,\"2463\":6,\"2464\":6,\"2469\":2,\"2470\":2,\"2471\":1,\"2472\":2,\"2473\":2}}],[\"tensorflow\",{\"1\":{\"39\":1,\"1301\":1,\"1350\":3,\"1372\":1}}],[\"tensorboard\",{\"0\":{\"115\":1},\"1\":{\"39\":8,\"115\":2,\"218\":2,\"267\":2,\"276\":2,\"286\":2,\"377\":4,\"449\":4,\"2348\":2,\"2359\":1,\"2367\":1,\"2370\":4,\"2372\":2}}],[\"tedlium3\",{\"1\":{\"527\":1}}],[\"tedlium2\",{\"1\":{\"527\":6}}],[\"ted\",{\"1\":{\"38\":1}}],[\"textcleaner\",{\"0\":{\"2289\":1},\"1\":{\"2289\":1,\"2290\":1}}],[\"textual\",{\"1\":{\"2262\":1}}],[\"textiowrapper\",{\"1\":{\"2249\":1}}],[\"textreader\",{\"0\":{\"2144\":1},\"1\":{\"2144\":1}}],[\"textencoder\",{\"0\":{\"1546\":1,\"1622\":1},\"1\":{\"1546\":2,\"1622\":2}}],[\"text3line\",{\"1\":{\"1000\":1}}],[\"text1line\",{\"1\":{\"1000\":1}}],[\"text2line\",{\"1\":{\"1000\":1}}],[\"text2tokens\",{\"1\":{\"2274\":1,\"2275\":1,\"2279\":1,\"2284\":1,\"2285\":2,\"2287\":1,\"2288\":1,\"2292\":1}}],[\"text2token\",{\"0\":{\"603\":1},\"1\":{\"603\":1}}],[\"text2speech\",{\"1\":{\"290\":13}}],[\"text2mel\",{\"1\":{\"284\":1,\"286\":7,\"290\":5,\"1526\":6,\"1600\":10}}],[\"text2wav\",{\"0\":{\"1600\":1},\"1\":{\"284\":1,\"286\":1,\"1600\":1}}],[\"texts\",{\"1\":{\"200\":2,\"205\":2,\"242\":2,\"254\":1,\"481\":1,\"2144\":1}}],[\"text=dump\",{\"1\":{\"126\":1}}],[\"text\",{\"0\":{\"181\":1,\"185\":1,\"187\":1,\"230\":1,\"240\":1,\"260\":1,\"270\":1,\"271\":1,\"279\":1,\"280\":1,\"283\":1,\"287\":1,\"288\":1,\"291\":1,\"298\":1,\"307\":1,\"314\":1,\"320\":1,\"326\":1,\"395\":1,\"401\":1,\"414\":1,\"418\":1,\"427\":1,\"435\":1,\"448\":1,\"468\":1,\"474\":1,\"481\":1,\"504\":1,\"510\":1,\"1000\":1,\"1019\":2,\"1021\":1,\"1022\":2,\"1024\":1,\"1026\":2,\"1546\":1,\"1622\":1,\"2137\":1,\"2144\":1,\"2274\":1,\"2275\":1,\"2276\":1,\"2277\":1,\"2278\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2282\":1,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2287\":1,\"2288\":1,\"2289\":1,\"2291\":1,\"2292\":1,\"2293\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":1,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2549\":1},\"1\":{\"6\":1,\"9\":1,\"69\":1,\"79\":6,\"80\":1,\"81\":1,\"82\":5,\"106\":2,\"126\":4,\"162\":1,\"168\":1,\"195\":2,\"196\":2,\"197\":2,\"201\":1,\"205\":1,\"211\":6,\"212\":2,\"213\":2,\"242\":24,\"243\":10,\"246\":2,\"247\":5,\"259\":3,\"265\":2,\"266\":5,\"267\":9,\"268\":2,\"270\":1,\"274\":2,\"275\":5,\"276\":9,\"277\":2,\"279\":1,\"284\":4,\"285\":4,\"286\":12,\"288\":1,\"290\":13,\"295\":6,\"315\":2,\"415\":6,\"469\":2,\"481\":1,\"515\":1,\"536\":10,\"572\":1,\"581\":1,\"603\":3,\"625\":8,\"736\":4,\"737\":12,\"777\":4,\"790\":1,\"994\":1,\"1000\":13,\"1008\":1,\"1019\":3,\"1020\":2,\"1021\":1,\"1022\":3,\"1023\":1,\"1024\":2,\"1025\":1,\"1026\":3,\"1027\":1,\"1156\":9,\"1521\":19,\"1526\":10,\"1546\":11,\"1552\":58,\"1553\":34,\"1577\":9,\"1585\":15,\"1589\":1,\"1590\":3,\"1598\":14,\"1599\":26,\"1600\":13,\"1601\":2,\"1602\":2,\"1611\":1,\"1612\":1,\"1616\":5,\"1622\":9,\"1625\":31,\"1626\":69,\"1627\":6,\"1629\":5,\"1637\":5,\"1640\":6,\"1641\":6,\"1720\":1,\"1752\":1,\"1760\":2,\"1770\":1,\"1788\":1,\"1795\":1,\"1940\":8,\"1941\":4,\"1942\":8,\"1943\":4,\"1959\":16,\"1960\":1,\"1961\":1,\"1965\":4,\"1975\":4,\"1996\":20,\"1997\":22,\"2007\":1,\"2030\":1,\"2032\":1,\"2040\":2,\"2043\":3,\"2044\":8,\"2045\":3,\"2049\":3,\"2054\":3,\"2055\":2,\"2056\":2,\"2066\":2,\"2127\":6,\"2130\":2,\"2137\":14,\"2144\":7,\"2216\":6,\"2219\":6,\"2221\":12,\"2222\":3,\"2226\":2,\"2228\":19,\"2229\":19,\"2235\":6,\"2236\":6,\"2239\":25,\"2240\":10,\"2241\":6,\"2245\":8,\"2256\":1,\"2262\":3,\"2274\":1,\"2275\":1,\"2276\":1,\"2277\":1,\"2278\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2282\":3,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2287\":6,\"2288\":1,\"2289\":2,\"2291\":1,\"2292\":1,\"2293\":1,\"2294\":2,\"2295\":2,\"2296\":2,\"2297\":2,\"2298\":4,\"2299\":1,\"2300\":2,\"2301\":2,\"2302\":2,\"2303\":2,\"2336\":3,\"2337\":3,\"2343\":1,\"2352\":1,\"2355\":1,\"2356\":3,\"2360\":8,\"2361\":8,\"2362\":3,\"2363\":3,\"2403\":3,\"2407\":3,\"2408\":19,\"2411\":14,\"2412\":22,\"2413\":6,\"2423\":21,\"2424\":6,\"2431\":9,\"2432\":9,\"2433\":1,\"2445\":3,\"2446\":19,\"2447\":20,\"2448\":6,\"2460\":1,\"2462\":4}}],[\"tikhonov\",{\"1\":{\"1361\":1}}],[\"tik\",{\"0\":{\"1361\":1},\"1\":{\"1334\":4,\"1361\":1}}],[\"timm\",{\"1\":{\"1153\":1}}],[\"timemask\",{\"0\":{\"1840\":1}}],[\"timewarp\",{\"0\":{\"1670\":1,\"1841\":1},\"1\":{\"1670\":1}}],[\"timedomainmse\",{\"0\":{\"1277\":1},\"1\":{\"1277\":1}}],[\"timedomainl1\",{\"0\":{\"1275\":1},\"1\":{\"1275\":1}}],[\"timedomainloss\",{\"0\":{\"1276\":1},\"1\":{\"1066\":1,\"1210\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1275\":1,\"1276\":1,\"1277\":1}}],[\"time2\",{\"1\":{\"784\":13,\"1756\":3,\"1757\":3,\"1785\":5,\"1789\":3,\"1790\":3,\"1794\":13,\"1817\":4,\"1984\":4}}],[\"time1\",{\"1\":{\"784\":9,\"1756\":3,\"1757\":3,\"1785\":5,\"1789\":3,\"1790\":3,\"1794\":9,\"1817\":7,\"1984\":3}}],[\"timesync\",{\"0\":{\"1726\":1,\"1727\":1,\"1732\":1},\"1\":{\"1726\":1,\"1727\":1,\"1732\":1}}],[\"timescale\",{\"1\":{\"824\":1,\"2423\":1,\"2428\":3}}],[\"timestamp\",{\"1\":{\"243\":1}}],[\"timestamps\",{\"1\":{\"242\":6}}],[\"timesteps\",{\"1\":{\"2220\":1,\"2428\":3,\"2442\":2}}],[\"timestep\",{\"1\":{\"45\":1,\"139\":1,\"637\":3,\"919\":2}}],[\"times\",{\"1\":{\"133\":1,\"134\":4,\"135\":2,\"136\":2,\"141\":1,\"242\":1,\"246\":1,\"286\":1,\"543\":2,\"820\":1,\"828\":1,\"1749\":1,\"1863\":1,\"1917\":1}}],[\"time=4\",{\"1\":{\"113\":1}}],[\"time=0\",{\"1\":{\"113\":9}}],[\"time>\",{\"1\":{\"73\":2}}],[\"time\",{\"0\":{\"133\":1,\"941\":1,\"1066\":1,\"1210\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1670\":1,\"1698\":1,\"1699\":2,\"1929\":1,\"1930\":1,\"2096\":1},\"1\":{\"39\":2,\"45\":7,\"106\":1,\"113\":1,\"128\":1,\"133\":3,\"134\":1,\"136\":1,\"139\":1,\"145\":7,\"196\":5,\"212\":1,\"218\":1,\"223\":1,\"225\":1,\"242\":1,\"243\":9,\"245\":1,\"261\":1,\"267\":2,\"268\":6,\"269\":3,\"276\":2,\"277\":6,\"278\":3,\"285\":1,\"286\":3,\"295\":1,\"301\":2,\"415\":1,\"421\":2,\"469\":2,\"543\":1,\"616\":4,\"625\":1,\"627\":1,\"649\":7,\"654\":10,\"667\":1,\"692\":2,\"696\":4,\"697\":4,\"701\":7,\"702\":1,\"706\":2,\"726\":1,\"735\":7,\"749\":5,\"768\":2,\"787\":2,\"790\":1,\"819\":1,\"824\":1,\"833\":8,\"850\":1,\"859\":1,\"941\":1,\"1002\":2,\"1025\":6,\"1029\":8,\"1050\":1,\"1061\":3,\"1062\":1,\"1066\":2,\"1070\":2,\"1071\":2,\"1073\":2,\"1080\":1,\"1116\":1,\"1119\":7,\"1120\":1,\"1122\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1155\":1,\"1157\":1,\"1161\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1175\":1,\"1189\":1,\"1208\":2,\"1210\":5,\"1211\":1,\"1218\":1,\"1221\":1,\"1224\":1,\"1225\":1,\"1229\":1,\"1235\":5,\"1244\":1,\"1245\":5,\"1246\":1,\"1247\":1,\"1248\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":2,\"1275\":2,\"1276\":2,\"1277\":2,\"1279\":2,\"1280\":4,\"1281\":4,\"1282\":4,\"1283\":1,\"1334\":1,\"1356\":1,\"1392\":1,\"1401\":2,\"1403\":4,\"1444\":1,\"1448\":1,\"1484\":2,\"1514\":1,\"1533\":2,\"1545\":1,\"1617\":1,\"1655\":2,\"1664\":1,\"1665\":1,\"1668\":2,\"1670\":4,\"1672\":2,\"1673\":2,\"1674\":2,\"1676\":2,\"1678\":2,\"1679\":2,\"1680\":2,\"1686\":3,\"1687\":2,\"1689\":2,\"1690\":2,\"1692\":2,\"1694\":3,\"1697\":2,\"1698\":5,\"1699\":5,\"1720\":3,\"1726\":3,\"1727\":3,\"1735\":10,\"1738\":8,\"1739\":8,\"1740\":8,\"1741\":8,\"1742\":8,\"1743\":8,\"1744\":8,\"1745\":8,\"1746\":8,\"1747\":2,\"1759\":5,\"1784\":2,\"1786\":3,\"1808\":2,\"1818\":2,\"1820\":2,\"1837\":2,\"1917\":1,\"1992\":1,\"1997\":2,\"2044\":1,\"2096\":1,\"2130\":1,\"2136\":2,\"2150\":1,\"2155\":1,\"2228\":1,\"2229\":1,\"2235\":1,\"2236\":1,\"2361\":3,\"2367\":2,\"2378\":1,\"2380\":1,\"2404\":1,\"2427\":2,\"2428\":5,\"2435\":1,\"2442\":1}}],[\"tied\",{\"1\":{\"821\":1}}],[\"ties\",{\"1\":{\"818\":1}}],[\"tie\",{\"1\":{\"730\":2,\"744\":1,\"817\":1,\"818\":1,\"820\":3,\"828\":3,\"830\":3,\"1945\":1}}],[\"tie=true\",{\"1\":{\"730\":1}}],[\"tid\",{\"1\":{\"704\":1,\"705\":1}}],[\"tile\",{\"1\":{\"646\":1}}],[\"till25epoch\",{\"1\":{\"243\":2}}],[\"ti1\",{\"1\":{\"287\":1}}],[\"tip\",{\"1\":{\"260\":1,\"263\":1}}],[\"tips\",{\"0\":{\"122\":1,\"173\":1},\"1\":{\"128\":1}}],[\"title\",{\"1\":{\"156\":1,\"202\":1}}],[\"title=\",{\"1\":{\"5\":1,\"6\":3,\"7\":1,\"8\":1,\"9\":2,\"10\":2,\"11\":2,\"12\":1,\"13\":1,\"14\":1,\"15\":1,\"16\":1,\"202\":1,\"207\":2,\"244\":1,\"256\":1}}],[\"tiny\",{\"1\":{\"37\":1,\"108\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1318\":1,\"1319\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1330\":1,\"1354\":1,\"1493\":1,\"1494\":1}}],[\"tian2024effects\",{\"1\":{\"6\":1}}],[\"tian\",{\"1\":{\"6\":3,\"244\":1}}],[\"t\",{\"1\":{\"3\":1,\"62\":1,\"67\":1,\"69\":1,\"79\":2,\"91\":2,\"96\":1,\"98\":1,\"104\":1,\"106\":1,\"108\":2,\"110\":2,\"139\":1,\"141\":1,\"152\":1,\"162\":2,\"168\":2,\"173\":1,\"196\":2,\"197\":2,\"213\":2,\"233\":1,\"234\":1,\"235\":2,\"249\":1,\"268\":2,\"269\":1,\"277\":2,\"278\":1,\"286\":1,\"287\":7,\"295\":1,\"415\":1,\"615\":2,\"616\":4,\"617\":13,\"618\":13,\"619\":2,\"620\":19,\"621\":4,\"622\":2,\"623\":2,\"624\":13,\"625\":2,\"626\":4,\"629\":1,\"632\":5,\"636\":9,\"640\":2,\"644\":35,\"645\":3,\"648\":2,\"667\":1,\"691\":2,\"696\":8,\"697\":7,\"699\":2,\"703\":14,\"715\":3,\"717\":4,\"722\":1,\"755\":4,\"756\":3,\"759\":2,\"768\":3,\"770\":1,\"773\":3,\"783\":3,\"785\":2,\"804\":3,\"821\":3,\"831\":3,\"833\":1,\"847\":1,\"866\":3,\"867\":3,\"876\":1,\"878\":4,\"879\":4,\"881\":6,\"882\":4,\"883\":4,\"884\":6,\"919\":7,\"922\":2,\"932\":3,\"934\":3,\"936\":2,\"937\":2,\"941\":2,\"947\":2,\"949\":2,\"955\":1,\"959\":1,\"961\":1,\"977\":1,\"979\":1,\"980\":2,\"1029\":1,\"1031\":1,\"1035\":1,\"1050\":2,\"1053\":5,\"1061\":2,\"1062\":2,\"1063\":2,\"1072\":3,\"1074\":3,\"1080\":4,\"1107\":2,\"1112\":2,\"1113\":1,\"1116\":2,\"1117\":3,\"1118\":7,\"1119\":3,\"1120\":1,\"1122\":1,\"1124\":2,\"1125\":3,\"1126\":11,\"1127\":7,\"1130\":2,\"1131\":4,\"1132\":2,\"1136\":2,\"1141\":2,\"1147\":2,\"1149\":1,\"1161\":2,\"1162\":1,\"1163\":1,\"1164\":2,\"1170\":4,\"1171\":4,\"1172\":2,\"1173\":4,\"1175\":4,\"1176\":2,\"1182\":2,\"1183\":2,\"1184\":2,\"1189\":2,\"1198\":3,\"1199\":3,\"1210\":2,\"1218\":2,\"1221\":2,\"1224\":7,\"1225\":9,\"1229\":3,\"1232\":2,\"1235\":1,\"1244\":2,\"1245\":9,\"1250\":2,\"1251\":2,\"1252\":2,\"1253\":2,\"1254\":1,\"1261\":2,\"1264\":2,\"1267\":2,\"1268\":3,\"1269\":2,\"1270\":2,\"1271\":3,\"1275\":4,\"1277\":4,\"1278\":2,\"1280\":4,\"1281\":1,\"1282\":1,\"1283\":3,\"1309\":2,\"1311\":1,\"1314\":2,\"1315\":2,\"1325\":2,\"1326\":2,\"1333\":1,\"1334\":9,\"1351\":2,\"1352\":1,\"1354\":4,\"1356\":3,\"1376\":2,\"1377\":3,\"1385\":1,\"1389\":8,\"1390\":1,\"1391\":2,\"1395\":12,\"1397\":1,\"1400\":3,\"1401\":8,\"1402\":1,\"1403\":2,\"1406\":1,\"1408\":8,\"1409\":1,\"1410\":2,\"1419\":2,\"1466\":8,\"1467\":1,\"1468\":2,\"1485\":1,\"1490\":1,\"1502\":1,\"1513\":2,\"1515\":4,\"1516\":3,\"1519\":3,\"1520\":3,\"1521\":30,\"1524\":4,\"1525\":3,\"1526\":5,\"1529\":6,\"1534\":1,\"1536\":3,\"1546\":7,\"1548\":6,\"1551\":3,\"1552\":11,\"1553\":19,\"1556\":8,\"1577\":5,\"1581\":2,\"1582\":4,\"1583\":3,\"1585\":4,\"1586\":2,\"1588\":2,\"1589\":2,\"1590\":5,\"1592\":4,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":9,\"1599\":20,\"1600\":6,\"1601\":5,\"1602\":4,\"1603\":3,\"1604\":1,\"1605\":4,\"1606\":1,\"1607\":3,\"1608\":4,\"1609\":2,\"1610\":6,\"1611\":5,\"1612\":2,\"1613\":2,\"1614\":2,\"1615\":2,\"1616\":4,\"1617\":2,\"1618\":1,\"1619\":4,\"1620\":4,\"1621\":4,\"1622\":5,\"1624\":2,\"1625\":10,\"1626\":19,\"1627\":6,\"1628\":6,\"1629\":3,\"1632\":1,\"1633\":1,\"1637\":3,\"1647\":3,\"1669\":2,\"1700\":1,\"1704\":2,\"1705\":1,\"1706\":1,\"1707\":2,\"1708\":2,\"1710\":3,\"1711\":3,\"1712\":3,\"1713\":2,\"1714\":2,\"1715\":3,\"1716\":3,\"1719\":4,\"1720\":2,\"1721\":1,\"1725\":5,\"1730\":2,\"1733\":2,\"1736\":12,\"1737\":2,\"1750\":2,\"1754\":2,\"1758\":3,\"1768\":4,\"1770\":2,\"1771\":2,\"1779\":2,\"1788\":2,\"1795\":2,\"1801\":1,\"1803\":2,\"1806\":2,\"1815\":1,\"1849\":4,\"1854\":5,\"1862\":1,\"1883\":1,\"1890\":1,\"1892\":1,\"1908\":3,\"1919\":2,\"1960\":3,\"1961\":2,\"1962\":2,\"1963\":1,\"1992\":2,\"1993\":6,\"1995\":2,\"2016\":2,\"2130\":3,\"2131\":1,\"2133\":2,\"2134\":1,\"2145\":3,\"2146\":3,\"2147\":3,\"2151\":3,\"2162\":2,\"2202\":4,\"2213\":4,\"2214\":4,\"2215\":1,\"2219\":2,\"2224\":2,\"2226\":2,\"2228\":33,\"2229\":30,\"2231\":3,\"2235\":1,\"2236\":1,\"2239\":13,\"2240\":6,\"2241\":9,\"2245\":7,\"2249\":1,\"2253\":1,\"2276\":2,\"2277\":2,\"2304\":1,\"2310\":3,\"2354\":1,\"2355\":4,\"2407\":5,\"2408\":6,\"2411\":8,\"2412\":16,\"2413\":9,\"2422\":3,\"2423\":16,\"2424\":9,\"2431\":7,\"2432\":6,\"2435\":2,\"2446\":8,\"2447\":16,\"2448\":9,\"2490\":1,\"2495\":2}}],[\"th\",{\"0\":{\"1928\":1},\"1\":{\"162\":2,\"637\":4,\"706\":4,\"1928\":1}}],[\"third\",{\"1\":{\"703\":1,\"716\":1}}],[\"think\",{\"1\":{\"821\":1}}],[\"thin\",{\"1\":{\"197\":1,\"1962\":1}}],[\"things\",{\"1\":{\"138\":1,\"242\":1,\"817\":1,\"2355\":2}}],[\"this\",{\"1\":{\"0\":1,\"1\":2,\"3\":6,\"19\":1,\"22\":1,\"26\":2,\"32\":2,\"36\":2,\"38\":1,\"39\":3,\"41\":1,\"47\":2,\"48\":6,\"55\":1,\"60\":1,\"62\":1,\"78\":1,\"79\":2,\"81\":3,\"82\":3,\"84\":1,\"85\":1,\"96\":4,\"97\":3,\"98\":5,\"99\":2,\"100\":1,\"101\":2,\"102\":1,\"108\":1,\"110\":2,\"119\":1,\"123\":1,\"126\":1,\"127\":1,\"128\":1,\"134\":1,\"138\":1,\"139\":1,\"141\":2,\"159\":1,\"162\":2,\"164\":1,\"168\":3,\"173\":2,\"175\":1,\"195\":1,\"197\":1,\"198\":2,\"200\":8,\"201\":1,\"203\":1,\"205\":2,\"208\":1,\"209\":1,\"211\":7,\"212\":2,\"215\":1,\"217\":1,\"218\":1,\"221\":1,\"222\":1,\"223\":7,\"226\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":3,\"233\":1,\"235\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":1,\"242\":7,\"243\":7,\"245\":2,\"246\":1,\"250\":1,\"251\":1,\"252\":1,\"254\":3,\"257\":1,\"258\":3,\"259\":2,\"260\":4,\"262\":16,\"264\":1,\"266\":5,\"267\":3,\"273\":1,\"275\":5,\"276\":7,\"283\":1,\"285\":6,\"286\":5,\"287\":13,\"290\":9,\"291\":1,\"292\":1,\"536\":4,\"635\":1,\"664\":1,\"677\":2,\"679\":2,\"681\":2,\"683\":2,\"685\":2,\"687\":2,\"690\":2,\"691\":2,\"694\":2,\"696\":1,\"699\":4,\"704\":1,\"710\":1,\"711\":1,\"714\":2,\"719\":2,\"721\":2,\"723\":2,\"724\":4,\"725\":4,\"726\":1,\"727\":1,\"728\":4,\"729\":3,\"733\":2,\"737\":1,\"739\":2,\"742\":2,\"744\":3,\"753\":2,\"755\":2,\"756\":4,\"758\":2,\"760\":2,\"768\":2,\"770\":1,\"773\":4,\"777\":3,\"779\":2,\"782\":2,\"784\":3,\"785\":2,\"789\":2,\"792\":2,\"797\":2,\"799\":2,\"806\":2,\"808\":2,\"810\":2,\"812\":2,\"814\":2,\"816\":2,\"817\":1,\"821\":3,\"822\":2,\"824\":3,\"826\":2,\"828\":4,\"829\":7,\"830\":5,\"831\":1,\"834\":2,\"836\":2,\"838\":2,\"840\":2,\"843\":2,\"845\":2,\"852\":1,\"853\":2,\"855\":2,\"857\":2,\"859\":2,\"861\":2,\"863\":2,\"865\":2,\"866\":3,\"867\":2,\"881\":2,\"884\":2,\"911\":1,\"951\":2,\"953\":2,\"957\":2,\"959\":1,\"960\":1,\"964\":2,\"966\":2,\"968\":2,\"970\":2,\"978\":3,\"980\":3,\"982\":1,\"993\":1,\"1002\":1,\"1008\":1,\"1031\":3,\"1033\":2,\"1035\":4,\"1037\":2,\"1039\":2,\"1041\":2,\"1043\":2,\"1045\":2,\"1047\":2,\"1049\":2,\"1052\":2,\"1056\":2,\"1058\":2,\"1060\":2,\"1061\":1,\"1062\":2,\"1064\":1,\"1067\":2,\"1069\":2,\"1077\":2,\"1078\":1,\"1079\":2,\"1081\":2,\"1083\":2,\"1085\":2,\"1088\":2,\"1090\":2,\"1092\":2,\"1094\":2,\"1096\":2,\"1098\":2,\"1100\":2,\"1102\":2,\"1104\":2,\"1106\":2,\"1107\":1,\"1109\":2,\"1111\":2,\"1112\":1,\"1113\":2,\"1115\":2,\"1118\":1,\"1121\":2,\"1123\":2,\"1131\":1,\"1132\":1,\"1135\":2,\"1136\":1,\"1138\":2,\"1140\":2,\"1141\":1,\"1143\":2,\"1146\":2,\"1150\":2,\"1152\":2,\"1153\":1,\"1154\":2,\"1155\":3,\"1156\":5,\"1157\":3,\"1160\":2,\"1162\":2,\"1166\":2,\"1169\":2,\"1178\":2,\"1186\":2,\"1188\":2,\"1191\":2,\"1193\":2,\"1195\":2,\"1197\":2,\"1201\":2,\"1202\":1,\"1203\":2,\"1204\":1,\"1206\":2,\"1209\":2,\"1212\":2,\"1214\":2,\"1216\":2,\"1217\":1,\"1220\":2,\"1227\":2,\"1228\":4,\"1231\":2,\"1232\":1,\"1234\":2,\"1237\":2,\"1239\":2,\"1241\":2,\"1243\":2,\"1246\":4,\"1249\":2,\"1250\":1,\"1251\":2,\"1252\":1,\"1254\":2,\"1256\":2,\"1258\":2,\"1260\":2,\"1261\":1,\"1262\":1,\"1263\":2,\"1264\":1,\"1266\":2,\"1267\":1,\"1268\":1,\"1269\":3,\"1270\":3,\"1271\":4,\"1273\":1,\"1278\":1,\"1279\":1,\"1280\":3,\"1281\":1,\"1283\":1,\"1285\":2,\"1287\":2,\"1289\":2,\"1290\":1,\"1310\":1,\"1316\":1,\"1328\":1,\"1334\":4,\"1384\":2,\"1388\":2,\"1393\":2,\"1396\":1,\"1399\":2,\"1405\":2,\"1407\":2,\"1412\":2,\"1414\":2,\"1416\":2,\"1418\":2,\"1421\":2,\"1423\":2,\"1424\":1,\"1425\":2,\"1426\":1,\"1427\":2,\"1428\":1,\"1429\":2,\"1430\":1,\"1431\":2,\"1434\":2,\"1436\":2,\"1438\":2,\"1440\":2,\"1443\":2,\"1445\":2,\"1447\":2,\"1449\":2,\"1451\":2,\"1453\":2,\"1455\":2,\"1457\":2,\"1459\":2,\"1461\":2,\"1463\":2,\"1465\":2,\"1470\":2,\"1478\":1,\"1484\":1,\"1485\":1,\"1493\":1,\"1494\":1,\"1502\":1,\"1510\":2,\"1512\":2,\"1513\":1,\"1518\":2,\"1523\":2,\"1528\":2,\"1531\":2,\"1533\":1,\"1538\":2,\"1540\":2,\"1542\":2,\"1544\":2,\"1545\":1,\"1546\":1,\"1548\":1,\"1550\":2,\"1551\":1,\"1552\":1,\"1553\":1,\"1555\":2,\"1592\":1,\"1598\":1,\"1605\":1,\"1606\":1,\"1608\":1,\"1611\":1,\"1612\":1,\"1616\":1,\"1622\":1,\"1625\":1,\"1626\":1,\"1631\":1,\"1639\":2,\"1640\":1,\"1641\":1,\"1644\":1,\"1645\":3,\"1647\":1,\"1650\":3,\"1653\":2,\"1655\":3,\"1656\":2,\"1658\":2,\"1660\":1,\"1662\":1,\"1663\":2,\"1664\":1,\"1665\":1,\"1668\":2,\"1669\":1,\"1670\":1,\"1671\":1,\"1677\":1,\"1678\":1,\"1691\":2,\"1692\":1,\"1697\":1,\"1698\":1,\"1706\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1715\":1,\"1716\":1,\"1720\":2,\"1721\":2,\"1731\":2,\"1750\":4,\"1752\":1,\"1756\":3,\"1757\":3,\"1758\":2,\"1759\":1,\"1770\":1,\"1788\":1,\"1789\":3,\"1790\":3,\"1794\":1,\"1795\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"1833\":1,\"1854\":1,\"1855\":2,\"1881\":2,\"1883\":2,\"1901\":1,\"1903\":1,\"1919\":1,\"1938\":2,\"1939\":2,\"1940\":2,\"1941\":3,\"1942\":2,\"1943\":3,\"1946\":2,\"1951\":1,\"1958\":2,\"1959\":1,\"1968\":2,\"1970\":2,\"1973\":2,\"1975\":1,\"1976\":2,\"1977\":1,\"1979\":2,\"1981\":2,\"1983\":2,\"1986\":2,\"1989\":2,\"1992\":1,\"1993\":1,\"1994\":1,\"1995\":1,\"1997\":1,\"2000\":3,\"2001\":2,\"2006\":2,\"2016\":2,\"2017\":1,\"2018\":1,\"2019\":2,\"2020\":2,\"2021\":2,\"2027\":2,\"2029\":2,\"2031\":2,\"2033\":2,\"2035\":2,\"2044\":7,\"2045\":1,\"2065\":1,\"2101\":1,\"2125\":2,\"2127\":1,\"2130\":11,\"2131\":3,\"2133\":1,\"2134\":1,\"2136\":2,\"2137\":1,\"2138\":1,\"2139\":1,\"2142\":1,\"2162\":1,\"2169\":2,\"2171\":2,\"2173\":2,\"2175\":2,\"2176\":1,\"2178\":2,\"2180\":2,\"2182\":2,\"2186\":2,\"2189\":2,\"2193\":2,\"2195\":2,\"2197\":2,\"2199\":2,\"2201\":2,\"2204\":2,\"2206\":2,\"2210\":2,\"2212\":2,\"2217\":2,\"2220\":1,\"2221\":1,\"2223\":1,\"2224\":1,\"2225\":1,\"2227\":2,\"2231\":2,\"2232\":1,\"2235\":1,\"2236\":1,\"2238\":1,\"2240\":1,\"2245\":1,\"2246\":2,\"2248\":2,\"2249\":6,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":4,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2276\":1,\"2277\":1,\"2286\":1,\"2287\":1,\"2304\":1,\"2306\":2,\"2325\":1,\"2326\":2,\"2327\":2,\"2338\":1,\"2344\":2,\"2353\":1,\"2354\":6,\"2355\":14,\"2357\":1,\"2367\":2,\"2369\":2,\"2380\":1,\"2384\":1,\"2385\":1,\"2402\":2,\"2404\":1,\"2405\":1,\"2406\":2,\"2410\":2,\"2411\":1,\"2412\":1,\"2415\":2,\"2417\":2,\"2419\":2,\"2423\":1,\"2425\":1,\"2427\":2,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":1,\"2433\":1,\"2435\":2,\"2436\":1,\"2438\":2,\"2440\":2,\"2444\":2,\"2447\":1,\"2450\":2,\"2452\":2,\"2454\":2,\"2457\":2,\"2459\":2,\"2461\":2,\"2466\":2,\"2468\":2,\"2474\":2,\"2480\":1}}],[\"thousand\",{\"1\":{\"2151\":1,\"2310\":1}}],[\"thousands\",{\"1\":{\"2151\":1,\"2310\":1}}],[\"though\",{\"1\":{\"81\":1,\"756\":1,\"773\":1}}],[\"thoreshold\",{\"1\":{\"1545\":1}}],[\"thorough\",{\"1\":{\"1264\":1,\"1334\":1}}],[\"those\",{\"1\":{\"3\":2,\"200\":1,\"243\":2,\"249\":1,\"276\":1,\"922\":1,\"1753\":1}}],[\"throw\",{\"1\":{\"699\":1}}],[\"through\",{\"1\":{\"12\":1,\"26\":1,\"43\":3,\"46\":1,\"139\":1,\"142\":1,\"143\":2,\"147\":1,\"236\":1,\"269\":3,\"278\":3,\"699\":1,\"822\":2,\"958\":1,\"1051\":1,\"1308\":1,\"1514\":1,\"1516\":1,\"1520\":1,\"1529\":1,\"1702\":1,\"1742\":1,\"1800\":1,\"2133\":1}}],[\"thres\",{\"1\":{\"1126\":1,\"1127\":1,\"1217\":2,\"2336\":1,\"2337\":1,\"2346\":1,\"2353\":2,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2364\":2,\"2368\":1}}],[\"thresh\",{\"1\":{\"449\":2}}],[\"thresholds\",{\"1\":{\"2476\":1}}],[\"threshold=0\",{\"1\":{\"1545\":1,\"1750\":1,\"2020\":1,\"2224\":1}}],[\"thresholding\",{\"1\":{\"606\":1}}],[\"threshold`\",{\"1\":{\"147\":1}}],[\"threshold\",{\"1\":{\"84\":2,\"141\":4,\"147\":3,\"211\":2,\"217\":1,\"266\":1,\"275\":1,\"285\":1,\"301\":2,\"309\":2,\"406\":2,\"421\":2,\"484\":2,\"490\":2,\"537\":3,\"606\":2,\"629\":6,\"635\":3,\"636\":1,\"661\":4,\"664\":5,\"776\":1,\"795\":1,\"1130\":3,\"1389\":1,\"1391\":1,\"1396\":1,\"1400\":4,\"1401\":1,\"1403\":1,\"1406\":1,\"1408\":1,\"1410\":1,\"1441\":4,\"1466\":1,\"1468\":1,\"1469\":4,\"1545\":2,\"1750\":2,\"1976\":1,\"1993\":3,\"2020\":1,\"2065\":6,\"2224\":2,\"2245\":3,\"2353\":1,\"2364\":1,\"2378\":1,\"2431\":3,\"2432\":3}}],[\"thread\",{\"1\":{\"705\":1,\"2280\":1}}],[\"threads\",{\"0\":{\"940\":1},\"1\":{\"168\":1,\"703\":3,\"705\":1,\"755\":3,\"785\":3,\"804\":1,\"922\":3,\"932\":1,\"934\":1,\"936\":3,\"937\":3,\"940\":1}}],[\"threads=1\",{\"1\":{\"168\":1}}],[\"threads=\",{\"1\":{\"168\":1}}],[\"threading\",{\"1\":{\"54\":2,\"56\":1}}],[\"threed\",{\"1\":{\"806\":1}}],[\"three\",{\"1\":{\"48\":1,\"60\":1,\"79\":1,\"127\":1,\"140\":2,\"141\":1,\"143\":1,\"145\":1,\"150\":1,\"242\":1,\"263\":1,\"699\":1,\"1655\":1,\"2018\":1,\"2131\":1,\"2151\":1,\"2176\":1,\"2184\":2,\"2307\":1,\"2310\":1,\"2325\":1}}],[\"thus\",{\"1\":{\"46\":1,\"50\":1,\"71\":1,\"96\":1,\"110\":1,\"138\":1,\"197\":1,\"242\":1,\"243\":1,\"2016\":1,\"2355\":1,\"2380\":1}}],[\"thanks\",{\"1\":{\"160\":1}}],[\"than\",{\"1\":{\"48\":3,\"66\":1,\"70\":1,\"94\":1,\"102\":2,\"135\":1,\"223\":1,\"242\":1,\"262\":3,\"269\":2,\"276\":2,\"278\":2,\"286\":1,\"514\":2,\"696\":1,\"704\":1,\"722\":1,\"831\":1,\"1063\":1,\"1301\":1,\"1310\":1,\"1350\":1,\"1372\":1,\"1400\":1,\"1441\":1,\"1469\":1,\"1678\":1,\"1717\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"2130\":1,\"2151\":1,\"2310\":1,\"2474\":1}}],[\"thang\",{\"1\":{\"12\":1}}],[\"that\",{\"1\":{\"2\":1,\"3\":1,\"24\":2,\"26\":3,\"32\":4,\"36\":1,\"39\":2,\"41\":1,\"46\":1,\"48\":2,\"55\":1,\"57\":1,\"58\":1,\"67\":1,\"68\":2,\"69\":1,\"70\":1,\"71\":3,\"78\":1,\"79\":1,\"81\":2,\"84\":2,\"91\":2,\"94\":2,\"95\":1,\"97\":1,\"98\":1,\"101\":2,\"102\":1,\"104\":1,\"107\":3,\"108\":1,\"110\":1,\"118\":1,\"119\":1,\"121\":1,\"123\":1,\"138\":2,\"152\":1,\"162\":1,\"164\":1,\"168\":2,\"173\":3,\"175\":1,\"194\":1,\"195\":1,\"196\":2,\"197\":3,\"200\":4,\"203\":1,\"211\":2,\"213\":2,\"217\":1,\"223\":5,\"224\":2,\"225\":1,\"232\":1,\"240\":1,\"242\":4,\"243\":2,\"258\":1,\"262\":4,\"266\":2,\"267\":7,\"268\":4,\"269\":4,\"275\":2,\"276\":3,\"277\":4,\"278\":4,\"285\":4,\"286\":4,\"290\":1,\"653\":1,\"692\":1,\"696\":1,\"703\":2,\"716\":1,\"718\":1,\"737\":1,\"747\":1,\"755\":5,\"756\":2,\"760\":4,\"764\":1,\"768\":3,\"770\":1,\"773\":2,\"785\":6,\"787\":1,\"790\":1,\"793\":1,\"797\":2,\"819\":1,\"820\":3,\"829\":1,\"850\":1,\"866\":1,\"867\":1,\"878\":2,\"879\":2,\"881\":1,\"882\":2,\"883\":2,\"884\":1,\"924\":1,\"928\":1,\"929\":1,\"959\":1,\"960\":2,\"980\":1,\"994\":1,\"1008\":2,\"1031\":1,\"1035\":1,\"1051\":2,\"1062\":1,\"1066\":1,\"1112\":1,\"1113\":1,\"1155\":4,\"1156\":2,\"1157\":4,\"1165\":1,\"1168\":1,\"1218\":1,\"1221\":1,\"1224\":1,\"1225\":1,\"1245\":1,\"1250\":1,\"1251\":1,\"1306\":2,\"1334\":1,\"1371\":2,\"1376\":1,\"1377\":1,\"1387\":1,\"1400\":1,\"1420\":1,\"1441\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1469\":1,\"1478\":1,\"1484\":2,\"1502\":1,\"1552\":3,\"1558\":1,\"1599\":3,\"1626\":3,\"1640\":1,\"1641\":1,\"1655\":3,\"1679\":1,\"1719\":4,\"1720\":2,\"1723\":2,\"1724\":1,\"1725\":4,\"1731\":4,\"1787\":2,\"1805\":2,\"1806\":2,\"1822\":2,\"1842\":1,\"1934\":1,\"1944\":2,\"1945\":1,\"1946\":1,\"1947\":2,\"1959\":1,\"1962\":1,\"1975\":1,\"1992\":4,\"1993\":3,\"1995\":3,\"1997\":1,\"2000\":1,\"2001\":2,\"2017\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2040\":1,\"2127\":1,\"2130\":1,\"2132\":1,\"2133\":1,\"2134\":1,\"2136\":2,\"2146\":1,\"2147\":1,\"2215\":1,\"2221\":1,\"2235\":3,\"2236\":3,\"2239\":3,\"2240\":3,\"2245\":3,\"2249\":2,\"2253\":2,\"2262\":1,\"2287\":1,\"2327\":1,\"2344\":1,\"2345\":1,\"2354\":1,\"2355\":7,\"2366\":1,\"2377\":2,\"2380\":2,\"2411\":3,\"2412\":3,\"2423\":3,\"2431\":3,\"2432\":3,\"2447\":3,\"2474\":1,\"2480\":2}}],[\"theory\",{\"1\":{\"1319\":1}}],[\"theobald\",{\"1\":{\"8\":1}}],[\"theta\",{\"1\":{\"1224\":2}}],[\"theta=1\",{\"1\":{\"1224\":1}}],[\"theenhancementtask\",{\"1\":{\"223\":1}}],[\"they\",{\"1\":{\"49\":1,\"50\":1,\"59\":2,\"70\":1,\"82\":1,\"86\":1,\"110\":1,\"150\":1,\"152\":1,\"197\":1,\"205\":1,\"211\":1,\"224\":1,\"225\":1,\"756\":2,\"773\":2,\"1306\":1,\"1371\":1,\"2130\":1,\"2136\":1,\"2377\":1}}],[\"thereby\",{\"1\":{\"1719\":2,\"1725\":3,\"1751\":1}}],[\"there\",{\"1\":{\"46\":1,\"60\":1,\"79\":1,\"84\":1,\"95\":1,\"102\":1,\"104\":1,\"118\":1,\"165\":1,\"267\":4,\"269\":2,\"278\":2,\"290\":1,\"756\":2,\"773\":2,\"831\":1,\"866\":1,\"867\":1,\"1558\":1,\"1943\":1,\"2355\":3,\"2380\":1}}],[\"therefore\",{\"1\":{\"39\":1,\"55\":1,\"62\":1,\"82\":1,\"91\":1,\"94\":1,\"101\":1,\"123\":1,\"162\":1,\"167\":1,\"173\":1,\"232\":1,\"258\":1,\"267\":1,\"276\":2,\"286\":2,\"2405\":1}}],[\"themselves\",{\"1\":{\"232\":1,\"259\":1,\"2000\":1}}],[\"them\",{\"1\":{\"24\":1,\"70\":1,\"96\":1,\"110\":1,\"118\":1,\"150\":1,\"152\":2,\"159\":1,\"163\":1,\"167\":1,\"196\":1,\"197\":2,\"213\":1,\"223\":1,\"224\":1,\"268\":1,\"269\":1,\"277\":1,\"278\":1,\"286\":1,\"536\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"978\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1387\":1,\"1388\":1,\"1393\":1,\"1395\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1521\":2,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1585\":1,\"1639\":1,\"1653\":1,\"1655\":1,\"1658\":1,\"1663\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2133\":1,\"2155\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2228\":2,\"2229\":2,\"2306\":1,\"2326\":1,\"2355\":1,\"2402\":1,\"2406\":1,\"2408\":2,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2446\":2,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"then\",{\"1\":{\"3\":1,\"26\":1,\"38\":1,\"40\":1,\"69\":1,\"73\":1,\"76\":1,\"117\":1,\"127\":1,\"136\":1,\"153\":1,\"162\":1,\"197\":3,\"211\":1,\"217\":1,\"218\":1,\"225\":1,\"236\":1,\"243\":3,\"246\":1,\"260\":1,\"267\":3,\"269\":1,\"276\":4,\"278\":1,\"285\":3,\"286\":7,\"290\":2,\"536\":1,\"756\":1,\"768\":1,\"773\":1,\"777\":1,\"786\":1,\"800\":1,\"911\":3,\"921\":1,\"935\":1,\"1133\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1334\":1,\"1719\":2,\"1725\":3,\"1751\":1,\"1806\":1,\"1940\":1,\"1942\":1,\"2000\":1,\"2131\":1,\"2136\":1,\"2246\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2369\":1}}],[\"their\",{\"1\":{\"3\":1,\"44\":1,\"144\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"744\":1,\"828\":1,\"829\":1,\"830\":1,\"859\":1,\"1250\":1,\"1251\":1,\"1719\":1,\"1725\":1,\"1806\":1,\"2130\":2,\"2355\":1}}],[\"these\",{\"1\":{\"3\":1,\"22\":1,\"24\":1,\"26\":1,\"27\":1,\"60\":2,\"138\":1,\"147\":1,\"163\":1,\"166\":1,\"173\":1,\"197\":2,\"200\":4,\"201\":1,\"242\":1,\"243\":1,\"249\":1,\"259\":1,\"261\":2,\"263\":1,\"267\":3,\"269\":2,\"276\":3,\"278\":2,\"285\":1,\"286\":3,\"821\":1,\"2325\":1,\"2344\":1,\"2355\":1}}],[\"the\",{\"0\":{\"0\":1,\"1\":1,\"47\":1,\"67\":1,\"71\":1,\"83\":1,\"86\":1,\"91\":1,\"94\":1,\"108\":1,\"110\":1,\"112\":1,\"113\":1,\"114\":1,\"119\":1,\"120\":1,\"150\":1,\"154\":1,\"174\":2,\"304\":1,\"312\":1,\"318\":1,\"324\":1,\"330\":1,\"334\":1,\"338\":1,\"346\":1,\"353\":1,\"365\":1,\"371\":1,\"380\":1,\"384\":1,\"388\":1,\"392\":1,\"399\":1,\"409\":1,\"432\":1,\"445\":1,\"452\":1,\"456\":1,\"460\":1,\"466\":1,\"472\":1,\"478\":1,\"487\":1,\"493\":1,\"501\":1,\"508\":1},\"1\":{\"0\":5,\"1\":11,\"2\":2,\"3\":32,\"6\":1,\"8\":1,\"9\":1,\"18\":1,\"19\":9,\"22\":13,\"23\":5,\"24\":11,\"25\":4,\"26\":18,\"27\":1,\"31\":3,\"32\":2,\"33\":1,\"36\":3,\"37\":4,\"38\":6,\"39\":8,\"40\":2,\"41\":4,\"42\":2,\"43\":30,\"44\":19,\"45\":7,\"46\":4,\"47\":6,\"48\":13,\"49\":4,\"50\":6,\"51\":2,\"52\":4,\"55\":5,\"56\":1,\"57\":1,\"58\":1,\"59\":3,\"60\":1,\"62\":3,\"67\":5,\"68\":12,\"69\":8,\"70\":14,\"71\":21,\"73\":3,\"76\":3,\"78\":5,\"79\":13,\"80\":4,\"81\":8,\"82\":15,\"84\":10,\"85\":4,\"86\":3,\"87\":7,\"88\":2,\"90\":4,\"91\":13,\"92\":1,\"93\":1,\"94\":8,\"95\":4,\"96\":14,\"97\":17,\"98\":12,\"99\":4,\"100\":4,\"101\":4,\"102\":8,\"104\":1,\"106\":8,\"107\":10,\"108\":2,\"109\":1,\"110\":10,\"117\":3,\"118\":3,\"119\":7,\"120\":2,\"121\":1,\"123\":5,\"124\":1,\"125\":2,\"127\":9,\"128\":20,\"129\":1,\"130\":3,\"131\":1,\"132\":2,\"133\":6,\"135\":3,\"136\":2,\"137\":2,\"138\":15,\"139\":19,\"140\":4,\"141\":56,\"142\":20,\"143\":10,\"144\":12,\"145\":14,\"146\":5,\"147\":10,\"148\":5,\"150\":11,\"152\":2,\"153\":1,\"154\":1,\"159\":2,\"160\":3,\"161\":2,\"162\":16,\"163\":2,\"164\":2,\"165\":2,\"167\":8,\"168\":9,\"173\":13,\"174\":4,\"175\":19,\"184\":1,\"185\":2,\"186\":1,\"187\":1,\"190\":3,\"194\":1,\"195\":2,\"196\":11,\"197\":23,\"200\":38,\"201\":11,\"202\":1,\"203\":1,\"205\":23,\"206\":6,\"208\":3,\"211\":15,\"212\":8,\"213\":9,\"217\":15,\"218\":11,\"219\":2,\"220\":1,\"222\":4,\"223\":44,\"224\":22,\"225\":13,\"226\":2,\"228\":4,\"231\":1,\"232\":12,\"233\":2,\"235\":9,\"236\":4,\"240\":5,\"242\":50,\"243\":34,\"245\":3,\"246\":3,\"247\":21,\"248\":2,\"249\":3,\"252\":2,\"254\":8,\"255\":7,\"256\":1,\"257\":1,\"258\":6,\"259\":12,\"260\":3,\"261\":3,\"262\":34,\"263\":8,\"265\":1,\"266\":13,\"267\":57,\"268\":13,\"269\":36,\"270\":1,\"271\":1,\"272\":2,\"274\":1,\"275\":16,\"276\":46,\"277\":13,\"278\":36,\"279\":1,\"280\":1,\"282\":2,\"284\":9,\"285\":30,\"286\":73,\"287\":14,\"288\":1,\"289\":5,\"290\":67,\"518\":1,\"521\":1,\"524\":1,\"525\":1,\"536\":9,\"615\":1,\"616\":2,\"617\":2,\"618\":2,\"619\":2,\"620\":6,\"622\":2,\"623\":2,\"624\":2,\"625\":2,\"626\":2,\"628\":1,\"630\":2,\"632\":1,\"633\":2,\"634\":3,\"636\":5,\"639\":4,\"640\":3,\"642\":3,\"643\":2,\"645\":2,\"648\":1,\"649\":2,\"653\":4,\"661\":4,\"663\":1,\"664\":2,\"665\":2,\"666\":3,\"669\":4,\"670\":1,\"675\":1,\"676\":1,\"677\":5,\"678\":1,\"679\":5,\"680\":1,\"681\":5,\"682\":1,\"683\":5,\"684\":1,\"685\":5,\"686\":3,\"687\":5,\"689\":2,\"690\":5,\"691\":11,\"692\":8,\"693\":1,\"694\":5,\"696\":5,\"697\":1,\"699\":13,\"701\":4,\"702\":1,\"703\":23,\"704\":5,\"706\":4,\"709\":9,\"710\":5,\"711\":5,\"713\":1,\"714\":5,\"716\":5,\"717\":7,\"718\":2,\"719\":5,\"720\":1,\"721\":5,\"722\":2,\"723\":5,\"724\":7,\"725\":7,\"728\":7,\"729\":7,\"733\":2,\"735\":3,\"736\":1,\"737\":1,\"738\":1,\"739\":5,\"741\":1,\"742\":5,\"744\":3,\"745\":2,\"747\":3,\"748\":3,\"750\":2,\"752\":1,\"753\":5,\"755\":30,\"756\":24,\"757\":1,\"758\":5,\"759\":4,\"760\":23,\"761\":2,\"762\":2,\"764\":1,\"765\":2,\"768\":18,\"770\":2,\"771\":2,\"772\":2,\"773\":24,\"774\":9,\"775\":1,\"777\":1,\"778\":1,\"779\":5,\"780\":13,\"781\":1,\"782\":5,\"784\":6,\"785\":28,\"786\":11,\"787\":8,\"788\":1,\"789\":5,\"790\":2,\"791\":1,\"792\":6,\"794\":2,\"796\":1,\"797\":7,\"798\":5,\"799\":5,\"800\":10,\"802\":1,\"803\":4,\"804\":1,\"805\":1,\"806\":5,\"807\":1,\"808\":5,\"809\":1,\"810\":5,\"811\":1,\"812\":5,\"813\":1,\"814\":5,\"815\":1,\"816\":5,\"817\":4,\"818\":2,\"819\":2,\"820\":3,\"821\":16,\"822\":7,\"824\":3,\"825\":1,\"826\":5,\"828\":8,\"829\":16,\"830\":11,\"831\":2,\"833\":1,\"834\":5,\"835\":1,\"836\":5,\"837\":1,\"838\":5,\"839\":1,\"840\":5,\"842\":1,\"843\":5,\"844\":1,\"845\":5,\"846\":20,\"849\":5,\"850\":2,\"852\":1,\"853\":5,\"854\":1,\"855\":5,\"856\":2,\"857\":5,\"859\":4,\"860\":1,\"861\":5,\"862\":5,\"863\":5,\"864\":1,\"865\":5,\"866\":14,\"867\":16,\"878\":28,\"879\":28,\"881\":28,\"882\":27,\"883\":27,\"884\":29,\"902\":1,\"911\":8,\"912\":5,\"919\":11,\"921\":10,\"922\":11,\"924\":2,\"926\":1,\"927\":9,\"931\":1,\"932\":1,\"933\":1,\"934\":1,\"935\":9,\"936\":6,\"937\":6,\"939\":7,\"950\":1,\"951\":5,\"952\":1,\"953\":5,\"954\":1,\"956\":1,\"957\":5,\"958\":4,\"959\":2,\"960\":6,\"961\":8,\"963\":1,\"964\":5,\"965\":1,\"966\":5,\"967\":1,\"968\":5,\"969\":1,\"970\":5,\"972\":3,\"974\":5,\"978\":3,\"980\":4,\"984\":1,\"994\":4,\"1000\":2,\"1002\":2,\"1008\":4,\"1009\":3,\"1011\":4,\"1022\":1,\"1029\":16,\"1030\":1,\"1031\":9,\"1032\":1,\"1033\":5,\"1034\":1,\"1035\":8,\"1036\":1,\"1037\":5,\"1038\":1,\"1039\":5,\"1040\":1,\"1041\":5,\"1042\":1,\"1043\":5,\"1044\":1,\"1045\":5,\"1046\":1,\"1047\":5,\"1048\":1,\"1049\":5,\"1050\":6,\"1051\":3,\"1052\":5,\"1053\":4,\"1055\":1,\"1056\":5,\"1057\":1,\"1058\":5,\"1059\":1,\"1060\":5,\"1061\":8,\"1062\":8,\"1063\":6,\"1064\":3,\"1066\":1,\"1067\":5,\"1068\":1,\"1069\":5,\"1070\":2,\"1071\":2,\"1073\":2,\"1075\":3,\"1076\":1,\"1077\":5,\"1078\":3,\"1079\":5,\"1080\":1,\"1081\":5,\"1082\":1,\"1083\":5,\"1084\":1,\"1085\":5,\"1086\":3,\"1087\":1,\"1088\":5,\"1089\":1,\"1090\":5,\"1091\":1,\"1092\":5,\"1093\":1,\"1094\":5,\"1095\":1,\"1096\":5,\"1097\":1,\"1098\":5,\"1099\":1,\"1100\":5,\"1101\":1,\"1102\":5,\"1103\":1,\"1104\":5,\"1105\":1,\"1106\":5,\"1107\":6,\"1108\":1,\"1109\":5,\"1110\":1,\"1111\":5,\"1112\":4,\"1113\":3,\"1114\":1,\"1115\":5,\"1116\":6,\"1117\":4,\"1118\":6,\"1119\":4,\"1120\":1,\"1121\":5,\"1122\":1,\"1123\":5,\"1124\":18,\"1125\":20,\"1126\":1,\"1127\":1,\"1130\":9,\"1131\":4,\"1132\":1,\"1133\":14,\"1134\":6,\"1135\":5,\"1136\":4,\"1137\":6,\"1138\":5,\"1139\":6,\"1140\":5,\"1141\":5,\"1142\":1,\"1143\":5,\"1145\":4,\"1146\":5,\"1147\":8,\"1149\":1,\"1150\":5,\"1151\":1,\"1152\":5,\"1153\":3,\"1154\":5,\"1155\":19,\"1156\":5,\"1157\":21,\"1158\":3,\"1159\":1,\"1160\":5,\"1161\":5,\"1162\":1,\"1163\":1,\"1164\":3,\"1165\":1,\"1166\":5,\"1167\":1,\"1168\":2,\"1169\":5,\"1176\":1,\"1177\":1,\"1178\":5,\"1180\":1,\"1181\":2,\"1185\":5,\"1186\":5,\"1187\":1,\"1188\":5,\"1189\":5,\"1190\":1,\"1191\":5,\"1192\":1,\"1193\":5,\"1194\":1,\"1195\":5,\"1196\":1,\"1197\":5,\"1199\":2,\"1200\":1,\"1201\":5,\"1202\":8,\"1203\":5,\"1204\":1,\"1205\":1,\"1206\":5,\"1207\":3,\"1208\":9,\"1209\":9,\"1210\":4,\"1211\":1,\"1212\":5,\"1213\":1,\"1214\":5,\"1215\":1,\"1216\":5,\"1218\":5,\"1219\":1,\"1220\":5,\"1221\":5,\"1222\":3,\"1224\":8,\"1225\":9,\"1226\":1,\"1227\":5,\"1228\":6,\"1229\":6,\"1230\":1,\"1231\":5,\"1232\":4,\"1233\":1,\"1234\":5,\"1235\":10,\"1236\":1,\"1237\":5,\"1238\":1,\"1239\":5,\"1240\":2,\"1241\":5,\"1242\":1,\"1243\":5,\"1244\":5,\"1245\":12,\"1246\":15,\"1247\":3,\"1248\":1,\"1249\":5,\"1250\":4,\"1251\":3,\"1252\":7,\"1253\":7,\"1254\":5,\"1255\":6,\"1256\":5,\"1257\":5,\"1258\":5,\"1259\":9,\"1260\":5,\"1261\":7,\"1262\":3,\"1263\":5,\"1264\":11,\"1265\":2,\"1266\":5,\"1267\":4,\"1268\":6,\"1269\":4,\"1270\":5,\"1271\":7,\"1273\":1,\"1274\":3,\"1278\":6,\"1279\":22,\"1280\":26,\"1281\":21,\"1282\":9,\"1283\":22,\"1284\":1,\"1285\":5,\"1286\":1,\"1287\":5,\"1288\":1,\"1289\":5,\"1290\":5,\"1292\":1,\"1296\":1,\"1301\":9,\"1305\":1,\"1306\":13,\"1308\":1,\"1309\":6,\"1310\":4,\"1311\":3,\"1312\":1,\"1314\":1,\"1315\":2,\"1316\":4,\"1318\":2,\"1319\":2,\"1320\":1,\"1321\":2,\"1322\":2,\"1323\":2,\"1324\":1,\"1327\":3,\"1328\":2,\"1329\":3,\"1330\":4,\"1333\":1,\"1334\":22,\"1350\":2,\"1354\":2,\"1356\":4,\"1357\":1,\"1368\":4,\"1371\":13,\"1372\":9,\"1374\":1,\"1375\":3,\"1381\":1,\"1383\":1,\"1384\":5,\"1385\":5,\"1386\":1,\"1387\":2,\"1388\":5,\"1389\":1,\"1392\":4,\"1393\":5,\"1395\":2,\"1396\":2,\"1398\":1,\"1399\":5,\"1400\":7,\"1401\":1,\"1402\":7,\"1404\":1,\"1405\":5,\"1406\":1,\"1407\":5,\"1408\":1,\"1409\":3,\"1411\":1,\"1412\":5,\"1413\":2,\"1414\":5,\"1415\":1,\"1416\":5,\"1417\":1,\"1418\":5,\"1419\":2,\"1420\":2,\"1421\":5,\"1422\":1,\"1423\":5,\"1424\":1,\"1425\":5,\"1426\":1,\"1427\":5,\"1428\":1,\"1429\":5,\"1430\":1,\"1431\":5,\"1433\":1,\"1434\":5,\"1435\":1,\"1436\":5,\"1437\":1,\"1438\":5,\"1439\":1,\"1440\":5,\"1441\":16,\"1442\":1,\"1443\":5,\"1444\":3,\"1445\":5,\"1446\":1,\"1447\":5,\"1448\":3,\"1449\":5,\"1450\":20,\"1451\":5,\"1452\":20,\"1453\":5,\"1454\":19,\"1455\":5,\"1456\":19,\"1457\":5,\"1458\":9,\"1459\":5,\"1460\":9,\"1461\":5,\"1462\":4,\"1463\":5,\"1464\":1,\"1465\":5,\"1466\":1,\"1467\":7,\"1469\":6,\"1470\":5,\"1477\":1,\"1478\":3,\"1484\":3,\"1485\":3,\"1493\":3,\"1494\":3,\"1502\":1,\"1509\":1,\"1510\":5,\"1511\":1,\"1512\":5,\"1513\":4,\"1514\":12,\"1515\":2,\"1516\":2,\"1517\":1,\"1518\":5,\"1519\":24,\"1520\":2,\"1521\":1,\"1522\":1,\"1523\":5,\"1524\":6,\"1525\":4,\"1526\":12,\"1527\":1,\"1528\":5,\"1529\":2,\"1530\":1,\"1531\":5,\"1533\":1,\"1534\":2,\"1535\":20,\"1536\":7,\"1537\":1,\"1538\":5,\"1539\":4,\"1540\":5,\"1541\":1,\"1542\":5,\"1543\":1,\"1544\":5,\"1545\":1,\"1546\":2,\"1548\":4,\"1549\":8,\"1550\":5,\"1551\":4,\"1552\":20,\"1553\":10,\"1554\":1,\"1555\":5,\"1556\":18,\"1558\":5,\"1581\":1,\"1582\":3,\"1584\":2,\"1585\":1,\"1586\":1,\"1587\":5,\"1588\":1,\"1589\":2,\"1591\":2,\"1592\":4,\"1593\":1,\"1594\":6,\"1595\":7,\"1596\":5,\"1597\":11,\"1598\":7,\"1599\":16,\"1600\":9,\"1603\":1,\"1604\":7,\"1605\":4,\"1606\":9,\"1608\":3,\"1609\":4,\"1610\":3,\"1612\":1,\"1613\":1,\"1616\":1,\"1618\":2,\"1619\":4,\"1622\":2,\"1624\":2,\"1625\":8,\"1626\":9,\"1627\":1,\"1628\":7,\"1629\":1,\"1631\":2,\"1638\":1,\"1639\":5,\"1640\":1,\"1641\":1,\"1645\":16,\"1647\":4,\"1650\":5,\"1652\":1,\"1653\":5,\"1654\":1,\"1655\":15,\"1656\":2,\"1657\":1,\"1658\":5,\"1660\":2,\"1661\":1,\"1662\":5,\"1663\":5,\"1664\":2,\"1665\":2,\"1666\":1,\"1667\":1,\"1668\":11,\"1669\":2,\"1670\":2,\"1671\":2,\"1672\":1,\"1673\":1,\"1674\":2,\"1675\":2,\"1676\":4,\"1677\":2,\"1678\":5,\"1679\":3,\"1680\":6,\"1681\":2,\"1683\":1,\"1684\":1,\"1685\":1,\"1686\":1,\"1687\":2,\"1688\":2,\"1689\":1,\"1690\":1,\"1691\":2,\"1692\":7,\"1694\":1,\"1695\":2,\"1697\":3,\"1698\":5,\"1702\":2,\"1703\":1,\"1705\":1,\"1708\":2,\"1709\":2,\"1710\":2,\"1712\":3,\"1717\":6,\"1719\":21,\"1720\":3,\"1721\":13,\"1723\":1,\"1724\":2,\"1725\":29,\"1729\":1,\"1730\":1,\"1731\":8,\"1733\":2,\"1734\":1,\"1735\":7,\"1736\":4,\"1747\":1,\"1748\":4,\"1750\":24,\"1751\":4,\"1752\":2,\"1753\":2,\"1754\":1,\"1756\":2,\"1757\":2,\"1758\":6,\"1759\":6,\"1764\":2,\"1768\":2,\"1770\":2,\"1782\":2,\"1784\":5,\"1785\":3,\"1787\":2,\"1788\":2,\"1789\":2,\"1790\":2,\"1794\":6,\"1803\":2,\"1804\":2,\"1805\":1,\"1806\":12,\"1808\":4,\"1809\":1,\"1810\":10,\"1811\":2,\"1812\":5,\"1814\":1,\"1816\":1,\"1817\":4,\"1818\":1,\"1821\":2,\"1822\":5,\"1824\":1,\"1833\":3,\"1837\":1,\"1839\":2,\"1842\":4,\"1845\":1,\"1848\":2,\"1849\":2,\"1854\":5,\"1857\":2,\"1860\":3,\"1861\":5,\"1862\":5,\"1869\":1,\"1873\":2,\"1875\":2,\"1877\":2,\"1878\":3,\"1881\":3,\"1883\":3,\"1885\":1,\"1892\":3,\"1893\":2,\"1895\":1,\"1901\":4,\"1902\":2,\"1903\":4,\"1904\":2,\"1907\":2,\"1908\":1,\"1918\":1,\"1919\":2,\"1920\":1,\"1931\":3,\"1934\":3,\"1936\":1,\"1937\":1,\"1938\":4,\"1939\":5,\"1940\":2,\"1941\":5,\"1942\":2,\"1943\":5,\"1944\":1,\"1945\":2,\"1946\":5,\"1947\":1,\"1949\":4,\"1950\":2,\"1951\":1,\"1955\":1,\"1957\":4,\"1958\":6,\"1959\":1,\"1960\":2,\"1961\":3,\"1962\":4,\"1966\":3,\"1967\":1,\"1968\":5,\"1969\":1,\"1970\":5,\"1971\":1,\"1972\":1,\"1973\":5,\"1975\":1,\"1976\":5,\"1977\":1,\"1978\":1,\"1979\":6,\"1980\":1,\"1981\":6,\"1982\":1,\"1983\":6,\"1985\":1,\"1986\":5,\"1988\":1,\"1989\":5,\"1991\":2,\"1992\":13,\"1993\":14,\"1994\":1,\"1995\":12,\"1996\":1,\"1997\":3,\"2000\":15,\"2001\":11,\"2007\":2,\"2014\":2,\"2015\":2,\"2016\":6,\"2017\":3,\"2018\":9,\"2019\":3,\"2020\":3,\"2021\":4,\"2026\":1,\"2027\":5,\"2028\":1,\"2029\":5,\"2030\":1,\"2031\":5,\"2032\":1,\"2033\":5,\"2034\":1,\"2035\":5,\"2039\":24,\"2040\":9,\"2043\":10,\"2044\":50,\"2045\":10,\"2049\":16,\"2054\":10,\"2055\":10,\"2056\":10,\"2065\":16,\"2066\":10,\"2101\":4,\"2116\":2,\"2124\":1,\"2125\":5,\"2126\":1,\"2127\":1,\"2128\":2,\"2129\":4,\"2130\":20,\"2131\":14,\"2133\":4,\"2134\":9,\"2136\":5,\"2139\":2,\"2142\":4,\"2143\":1,\"2145\":1,\"2146\":2,\"2147\":1,\"2148\":2,\"2149\":1,\"2150\":1,\"2151\":1,\"2155\":4,\"2159\":1,\"2162\":4,\"2164\":1,\"2168\":2,\"2169\":5,\"2170\":1,\"2171\":5,\"2172\":1,\"2173\":5,\"2174\":1,\"2175\":5,\"2177\":1,\"2178\":5,\"2179\":1,\"2180\":5,\"2181\":1,\"2182\":5,\"2183\":3,\"2184\":7,\"2185\":1,\"2186\":5,\"2187\":2,\"2188\":2,\"2189\":5,\"2190\":2,\"2191\":9,\"2192\":4,\"2193\":5,\"2194\":1,\"2195\":5,\"2196\":1,\"2197\":5,\"2198\":2,\"2199\":5,\"2200\":1,\"2201\":5,\"2203\":3,\"2204\":5,\"2205\":1,\"2206\":5,\"2208\":2,\"2209\":2,\"2210\":5,\"2211\":1,\"2212\":5,\"2216\":3,\"2217\":5,\"2218\":11,\"2220\":13,\"2221\":1,\"2222\":1,\"2223\":15,\"2224\":6,\"2226\":2,\"2227\":4,\"2228\":2,\"2229\":2,\"2231\":6,\"2232\":2,\"2235\":18,\"2236\":18,\"2237\":1,\"2238\":2,\"2239\":21,\"2240\":25,\"2241\":2,\"2245\":18,\"2246\":2,\"2248\":2,\"2249\":21,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":14,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2262\":6,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2276\":1,\"2277\":1,\"2280\":1,\"2287\":2,\"2298\":1,\"2304\":3,\"2305\":1,\"2306\":5,\"2307\":7,\"2309\":4,\"2310\":1,\"2311\":1,\"2312\":2,\"2314\":1,\"2323\":1,\"2325\":7,\"2326\":5,\"2327\":15,\"2334\":1,\"2338\":1,\"2344\":1,\"2345\":2,\"2353\":11,\"2354\":11,\"2355\":63,\"2357\":3,\"2359\":2,\"2364\":9,\"2367\":4,\"2369\":2,\"2377\":2,\"2380\":3,\"2384\":3,\"2385\":3,\"2401\":1,\"2402\":5,\"2403\":1,\"2405\":2,\"2406\":5,\"2408\":2,\"2409\":1,\"2410\":5,\"2411\":20,\"2412\":20,\"2413\":2,\"2414\":1,\"2415\":6,\"2416\":1,\"2417\":6,\"2418\":1,\"2419\":6,\"2422\":1,\"2423\":20,\"2424\":2,\"2425\":11,\"2427\":3,\"2428\":4,\"2429\":13,\"2430\":3,\"2431\":15,\"2432\":16,\"2434\":1,\"2435\":5,\"2443\":2,\"2444\":5,\"2445\":1,\"2446\":2,\"2447\":14,\"2448\":2,\"2449\":2,\"2450\":5,\"2451\":1,\"2452\":5,\"2453\":1,\"2454\":5,\"2456\":1,\"2457\":5,\"2458\":1,\"2459\":5,\"2460\":1,\"2461\":5,\"2462\":1,\"2465\":1,\"2466\":5,\"2467\":1,\"2468\":5,\"2480\":3,\"2482\":3,\"2487\":2,\"2490\":2,\"2495\":2}}],[\"toward\",{\"1\":{\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1}}],[\"towards\",{\"1\":{\"262\":2,\"1053\":1,\"1210\":1,\"1264\":1,\"1334\":1}}],[\"tolerance=0\",{\"1\":{\"824\":1}}],[\"together\",{\"1\":{\"275\":1,\"768\":1,\"2131\":1}}],[\"toksing\",{\"0\":{\"2239\":2},\"1\":{\"274\":1,\"276\":1,\"282\":1,\"2239\":4}}],[\"tok\",{\"1\":{\"212\":1}}],[\"tokenidconverter\",{\"0\":{\"2291\":1},\"1\":{\"2291\":1}}],[\"tokenizes\",{\"1\":{\"2143\":1}}],[\"tokenized\",{\"1\":{\"603\":1,\"2130\":1}}],[\"tokenize\",{\"0\":{\"481\":1},\"1\":{\"290\":1,\"295\":1,\"415\":1,\"481\":2,\"2137\":1}}],[\"tokenizers\",{\"1\":{\"699\":1,\"2130\":3,\"2136\":3,\"2137\":2}}],[\"tokenizer\",{\"0\":{\"2064\":2,\"2274\":1,\"2275\":1,\"2276\":1,\"2277\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2287\":1,\"2288\":1,\"2292\":1,\"2293\":2,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":1,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1},\"1\":{\"290\":3,\"2136\":1,\"2137\":5,\"2274\":1,\"2275\":1,\"2276\":1,\"2277\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2287\":2,\"2288\":1,\"2292\":1,\"2293\":3,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":1,\"2299\":1,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2356\":1,\"2357\":1}}],[\"tokenization\",{\"1\":{\"106\":1,\"200\":1,\"205\":1,\"220\":2,\"266\":1,\"275\":1,\"285\":1,\"2130\":1,\"2136\":2,\"2137\":2,\"2143\":1,\"2287\":1}}],[\"token=\",{\"1\":{\"276\":2}}],[\"tokens2ids\",{\"1\":{\"2278\":1,\"2283\":1,\"2291\":1}}],[\"tokens2text\",{\"1\":{\"2274\":1,\"2275\":1,\"2279\":1,\"2284\":1,\"2285\":1,\"2287\":1,\"2288\":1,\"2292\":1}}],[\"tokens\",{\"1\":{\"139\":2,\"200\":3,\"203\":1,\"204\":1,\"205\":6,\"211\":1,\"240\":1,\"242\":5,\"243\":2,\"259\":1,\"273\":1,\"275\":1,\"276\":10,\"286\":1,\"514\":2,\"625\":2,\"692\":2,\"733\":1,\"740\":1,\"760\":6,\"790\":2,\"797\":2,\"820\":4,\"850\":2,\"1029\":1,\"1279\":4,\"1280\":3,\"1281\":3,\"1283\":3,\"1395\":1,\"1598\":1,\"1599\":2,\"1600\":1,\"1719\":8,\"1720\":1,\"1721\":1,\"1723\":5,\"1724\":2,\"1725\":10,\"1731\":6,\"1787\":5,\"1804\":2,\"1805\":4,\"1806\":5,\"1821\":1,\"1822\":4,\"1843\":3,\"1862\":1,\"1907\":1,\"1944\":4,\"1945\":2,\"1946\":2,\"1947\":4,\"1960\":1,\"1961\":2,\"1966\":3,\"1992\":2,\"2049\":1,\"2088\":1,\"2130\":4,\"2133\":2,\"2134\":1,\"2136\":11,\"2137\":6,\"2138\":2,\"2143\":5,\"2145\":1,\"2146\":2,\"2147\":1,\"2184\":3,\"2228\":3,\"2239\":5,\"2245\":2,\"2274\":1,\"2275\":1,\"2278\":1,\"2279\":1,\"2283\":2,\"2284\":2,\"2285\":1,\"2287\":3,\"2288\":1,\"2291\":1,\"2292\":1,\"2411\":2,\"2412\":2,\"2423\":2,\"2425\":1,\"2429\":3,\"2430\":3,\"2431\":2,\"2432\":3}}],[\"token\",{\"0\":{\"1907\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2278\":1,\"2283\":1,\"2291\":1},\"1\":{\"79\":2,\"199\":1,\"200\":6,\"204\":1,\"205\":3,\"210\":1,\"211\":5,\"212\":1,\"213\":2,\"218\":1,\"228\":1,\"232\":1,\"240\":1,\"242\":7,\"243\":9,\"258\":1,\"259\":5,\"265\":1,\"266\":6,\"267\":5,\"268\":2,\"274\":2,\"275\":17,\"276\":27,\"277\":2,\"284\":1,\"285\":7,\"286\":4,\"290\":1,\"295\":1,\"301\":7,\"309\":1,\"315\":1,\"321\":1,\"389\":1,\"396\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"442\":1,\"463\":2,\"469\":1,\"475\":2,\"481\":1,\"498\":1,\"505\":3,\"511\":2,\"583\":1,\"589\":1,\"625\":2,\"627\":3,\"674\":2,\"692\":11,\"696\":4,\"697\":1,\"703\":2,\"717\":1,\"736\":2,\"737\":2,\"740\":2,\"755\":2,\"760\":7,\"761\":1,\"762\":1,\"775\":3,\"777\":1,\"785\":2,\"790\":9,\"795\":2,\"797\":2,\"804\":1,\"820\":7,\"846\":1,\"850\":8,\"878\":3,\"879\":3,\"881\":3,\"882\":2,\"883\":2,\"884\":3,\"919\":1,\"922\":1,\"932\":1,\"934\":1,\"936\":1,\"937\":1,\"958\":1,\"1279\":2,\"1280\":1,\"1281\":1,\"1283\":1,\"1590\":1,\"1599\":3,\"1627\":2,\"1629\":3,\"1637\":1,\"1640\":1,\"1641\":1,\"1719\":8,\"1721\":2,\"1723\":2,\"1724\":2,\"1725\":14,\"1726\":1,\"1727\":1,\"1731\":6,\"1745\":1,\"1768\":3,\"1787\":4,\"1798\":4,\"1799\":7,\"1800\":7,\"1805\":3,\"1806\":4,\"1822\":2,\"1839\":2,\"1843\":2,\"1862\":2,\"1907\":3,\"1942\":1,\"1944\":4,\"1945\":2,\"1946\":2,\"1947\":4,\"1957\":3,\"1959\":2,\"1961\":4,\"1965\":1,\"1975\":3,\"1987\":1,\"1991\":1,\"1992\":5,\"1993\":1,\"1996\":1,\"1997\":1,\"2044\":3,\"2049\":3,\"2124\":1,\"2127\":2,\"2128\":1,\"2130\":2,\"2136\":3,\"2137\":4,\"2143\":5,\"2145\":3,\"2146\":3,\"2147\":2,\"2216\":1,\"2220\":1,\"2221\":3,\"2223\":3,\"2224\":3,\"2227\":2,\"2228\":13,\"2235\":1,\"2236\":1,\"2239\":12,\"2245\":3,\"2278\":1,\"2283\":1,\"2286\":1,\"2291\":2,\"2293\":1,\"2336\":2,\"2337\":2,\"2343\":1,\"2352\":1,\"2356\":2,\"2360\":2,\"2361\":2,\"2362\":3,\"2363\":4,\"2404\":1,\"2409\":1,\"2411\":1,\"2412\":7,\"2413\":2,\"2423\":6,\"2424\":2,\"2429\":4,\"2430\":7,\"2431\":3,\"2432\":1,\"2434\":1,\"2443\":1,\"2446\":2,\"2447\":8,\"2448\":2,\"2462\":4,\"2469\":2}}],[\"todo\",{\"1\":{\"211\":1,\"213\":1,\"214\":1,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1410\":1,\"1466\":1,\"1468\":1,\"1644\":1,\"1656\":1,\"1993\":2,\"2216\":4,\"2219\":1}}],[\"today\",{\"1\":{\"80\":1,\"242\":1}}],[\"toda\",{\"1\":{\"9\":1}}],[\"torque\",{\"1\":{\"165\":1,\"166\":1}}],[\"torchvision\",{\"1\":{\"2355\":1}}],[\"torchstft\",{\"0\":{\"1547\":1},\"1\":{\"1547\":1}}],[\"torch=false\",{\"1\":{\"1128\":1}}],[\"torch=1\",{\"1\":{\"87\":1}}],[\"torchaudiohubertpretrainmodel\",{\"0\":{\"1641\":1},\"1\":{\"1641\":1,\"2257\":1}}],[\"torchaudiohubertpretrainencoder\",{\"0\":{\"846\":1},\"1\":{\"846\":1}}],[\"torchaudio\",{\"1\":{\"778\":1,\"846\":2,\"1126\":1,\"1217\":1,\"1641\":1,\"1677\":1,\"1678\":3}}],[\"torcheval\",{\"1\":{\"210\":1,\"213\":2}}],[\"torch\",{\"0\":{\"778\":1,\"872\":1,\"1339\":1,\"1366\":1,\"1872\":1,\"1932\":1,\"2100\":1,\"2304\":1,\"2307\":1,\"2308\":1,\"2309\":1,\"2310\":1,\"2311\":1,\"2312\":1,\"2313\":1,\"2314\":1,\"2316\":1,\"2317\":1,\"2318\":1,\"2319\":1,\"2320\":1,\"2321\":1,\"2322\":1,\"2323\":1,\"2550\":1},\"1\":{\"82\":2,\"87\":1,\"102\":1,\"286\":4,\"356\":2,\"617\":1,\"618\":1,\"622\":1,\"624\":1,\"633\":1,\"636\":1,\"638\":2,\"642\":1,\"692\":4,\"700\":6,\"701\":5,\"706\":4,\"709\":7,\"715\":3,\"733\":6,\"734\":6,\"735\":5,\"749\":1,\"750\":1,\"756\":1,\"760\":8,\"768\":1,\"773\":1,\"774\":7,\"778\":1,\"780\":7,\"783\":3,\"784\":15,\"790\":4,\"794\":4,\"797\":5,\"820\":8,\"833\":1,\"846\":1,\"850\":4,\"947\":3,\"948\":2,\"949\":3,\"955\":2,\"977\":2,\"978\":6,\"979\":3,\"980\":4,\"1029\":4,\"1051\":1,\"1053\":7,\"1054\":4,\"1061\":2,\"1062\":7,\"1063\":2,\"1064\":1,\"1070\":2,\"1071\":2,\"1073\":2,\"1107\":8,\"1108\":1,\"1112\":2,\"1113\":3,\"1117\":7,\"1118\":12,\"1124\":2,\"1125\":7,\"1126\":20,\"1127\":11,\"1130\":7,\"1131\":4,\"1132\":3,\"1133\":3,\"1136\":7,\"1141\":7,\"1145\":2,\"1147\":2,\"1156\":2,\"1157\":3,\"1162\":7,\"1167\":3,\"1168\":1,\"1176\":2,\"1180\":2,\"1181\":2,\"1198\":3,\"1199\":1,\"1204\":3,\"1205\":1,\"1208\":2,\"1209\":4,\"1217\":9,\"1222\":2,\"1223\":2,\"1228\":3,\"1232\":7,\"1235\":4,\"1236\":1,\"1245\":2,\"1250\":1,\"1251\":4,\"1252\":7,\"1253\":2,\"1261\":7,\"1262\":2,\"1264\":4,\"1265\":2,\"1267\":7,\"1268\":8,\"1269\":6,\"1270\":6,\"1271\":6,\"1278\":7,\"1279\":2,\"1280\":7,\"1281\":2,\"1282\":2,\"1283\":7,\"1293\":3,\"1309\":4,\"1310\":4,\"1311\":5,\"1317\":1,\"1318\":5,\"1319\":4,\"1321\":4,\"1322\":5,\"1323\":4,\"1326\":3,\"1327\":4,\"1328\":4,\"1330\":4,\"1334\":11,\"1339\":1,\"1351\":1,\"1354\":5,\"1356\":1,\"1361\":2,\"1366\":1,\"1391\":9,\"1403\":9,\"1406\":4,\"1410\":9,\"1432\":4,\"1441\":1,\"1468\":9,\"1514\":2,\"1556\":16,\"1592\":1,\"1594\":1,\"1605\":1,\"1623\":1,\"1654\":2,\"1666\":2,\"1668\":5,\"1670\":1,\"1672\":2,\"1673\":2,\"1676\":2,\"1678\":2,\"1679\":2,\"1680\":2,\"1681\":1,\"1683\":1,\"1686\":2,\"1687\":2,\"1689\":2,\"1690\":2,\"1692\":2,\"1694\":2,\"1697\":3,\"1698\":2,\"1699\":1,\"1704\":5,\"1705\":3,\"1706\":3,\"1707\":5,\"1708\":5,\"1709\":6,\"1710\":5,\"1711\":5,\"1712\":3,\"1713\":4,\"1714\":4,\"1715\":4,\"1716\":4,\"1719\":17,\"1720\":5,\"1721\":2,\"1723\":6,\"1724\":5,\"1725\":21,\"1726\":1,\"1727\":1,\"1730\":3,\"1731\":14,\"1735\":20,\"1737\":2,\"1738\":5,\"1739\":5,\"1740\":5,\"1741\":5,\"1742\":5,\"1743\":5,\"1744\":5,\"1745\":6,\"1746\":4,\"1747\":2,\"1750\":2,\"1751\":14,\"1756\":5,\"1757\":5,\"1759\":7,\"1760\":4,\"1768\":6,\"1782\":4,\"1783\":2,\"1784\":2,\"1785\":8,\"1786\":3,\"1787\":9,\"1789\":5,\"1790\":5,\"1794\":15,\"1795\":2,\"1796\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":5,\"1805\":5,\"1806\":11,\"1808\":2,\"1814\":4,\"1816\":4,\"1817\":8,\"1818\":2,\"1820\":2,\"1822\":5,\"1837\":2,\"1848\":2,\"1856\":3,\"1858\":3,\"1860\":1,\"1861\":1,\"1862\":1,\"1872\":1,\"1878\":1,\"1896\":2,\"1902\":3,\"1904\":3,\"1906\":1,\"1907\":3,\"1909\":3,\"1926\":3,\"1931\":5,\"1932\":2,\"1933\":4,\"1944\":11,\"1945\":4,\"1946\":2,\"1947\":11,\"1962\":1,\"1966\":1,\"1984\":5,\"1992\":4,\"2130\":2,\"2131\":1,\"2162\":1,\"2187\":2,\"2191\":3,\"2223\":2,\"2246\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2304\":2,\"2305\":3,\"2307\":1,\"2308\":1,\"2309\":4,\"2310\":1,\"2311\":7,\"2312\":1,\"2313\":1,\"2314\":1,\"2316\":1,\"2317\":1,\"2318\":1,\"2319\":1,\"2320\":1,\"2321\":1,\"2322\":1,\"2323\":1,\"2325\":1,\"2327\":1,\"2332\":1,\"2339\":1,\"2340\":1,\"2354\":1,\"2355\":6,\"2373\":3,\"2376\":1,\"2384\":1,\"2385\":1,\"2420\":2,\"2426\":5,\"2427\":4,\"2428\":14,\"2435\":3,\"2438\":1,\"2441\":1}}],[\"toml\",{\"1\":{\"162\":3}}],[\"tomoki\",{\"1\":{\"9\":3,\"10\":2,\"11\":1,\"13\":1,\"16\":1,\"156\":1}}],[\"tot\",{\"1\":{\"144\":1}}],[\"totally\",{\"1\":{\"96\":1}}],[\"total\",{\"1\":{\"39\":1,\"43\":1,\"44\":1,\"55\":1,\"99\":1,\"100\":1,\"123\":1,\"128\":1,\"136\":2,\"144\":1,\"173\":1,\"243\":3,\"290\":1,\"631\":1,\"665\":1,\"824\":1,\"1124\":1,\"1125\":1,\"1147\":1,\"1176\":1,\"1190\":1,\"1382\":1,\"1484\":2,\"1497\":1,\"1676\":2,\"1735\":3,\"2000\":5,\"2001\":3,\"2015\":4,\"2018\":1,\"2132\":1,\"2134\":1,\"2136\":1,\"2141\":1,\"2146\":1,\"2367\":2}}],[\"touch\",{\"1\":{\"82\":1,\"110\":1,\"162\":1}}],[\"tooshortutterror\",{\"0\":{\"653\":1,\"1842\":1},\"1\":{\"653\":2,\"1842\":2}}],[\"tood\",{\"1\":{\"212\":1}}],[\"too\",{\"1\":{\"78\":1,\"91\":1,\"205\":1,\"242\":2,\"653\":1,\"663\":1,\"696\":1,\"1804\":1,\"1842\":1,\"1869\":1}}],[\"tool=espnet\",{\"1\":{\"285\":1}}],[\"tool\",{\"0\":{\"163\":1},\"1\":{\"137\":1,\"166\":1,\"225\":1,\"285\":2,\"1598\":3,\"1625\":3}}],[\"toolkits\",{\"1\":{\"223\":2,\"269\":2,\"278\":2}}],[\"toolkit\",{\"1\":{\"6\":1,\"8\":1,\"9\":1,\"10\":2,\"11\":1,\"13\":1,\"14\":1,\"15\":1,\"139\":1,\"156\":1,\"244\":1,\"245\":2,\"285\":1,\"286\":1,\"289\":1,\"1668\":1}}],[\"tools>\",{\"1\":{\"153\":2}}],[\"tools\",{\"0\":{\"2510\":1},\"1\":{\"3\":1,\"31\":1,\"78\":1,\"84\":1,\"110\":1,\"119\":2,\"128\":2,\"137\":1,\"152\":1,\"153\":2,\"154\":2,\"161\":5,\"162\":17,\"163\":4,\"164\":1,\"197\":1,\"2311\":1}}],[\"top=1\",{\"1\":{\"1635\":1}}],[\"topk\",{\"1\":{\"616\":4,\"1719\":3,\"1725\":3,\"1806\":2,\"1920\":4,\"2176\":4}}],[\"topic\",{\"1\":{\"66\":1,\"1833\":1}}],[\"top\",{\"0\":{\"2119\":1},\"1\":{\"3\":1,\"263\":1,\"1271\":1,\"2176\":2,\"2344\":1}}],[\"to\",{\"0\":{\"0\":1,\"3\":1,\"16\":1,\"48\":1,\"49\":1,\"57\":1,\"75\":1,\"108\":1,\"117\":1,\"150\":2,\"152\":1,\"154\":1,\"156\":1,\"181\":1,\"185\":1,\"187\":1,\"197\":1,\"201\":1,\"206\":1,\"212\":1,\"218\":1,\"223\":1,\"228\":1,\"230\":1,\"236\":1,\"239\":1,\"240\":1,\"243\":1,\"247\":1,\"255\":1,\"260\":1,\"267\":1,\"276\":1,\"283\":1,\"286\":1,\"291\":1,\"521\":1,\"558\":1,\"567\":1,\"942\":1,\"943\":1,\"962\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1557\":1,\"1861\":1,\"1931\":1,\"1932\":1,\"2022\":1,\"2023\":1,\"2024\":1,\"2025\":1,\"2028\":1,\"2036\":1,\"2037\":1,\"2038\":1,\"2041\":1,\"2042\":1,\"2046\":1,\"2047\":1,\"2048\":1,\"2050\":1,\"2051\":1,\"2052\":1,\"2053\":1,\"2054\":1,\"2057\":1,\"2058\":1,\"2059\":1,\"2060\":1,\"2061\":1,\"2062\":1,\"2063\":1,\"2064\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2070\":1,\"2071\":1,\"2072\":1,\"2073\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2078\":1,\"2079\":1,\"2080\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2085\":1,\"2086\":1,\"2087\":1,\"2088\":1,\"2089\":1,\"2090\":1,\"2091\":1,\"2092\":1,\"2093\":1,\"2094\":1,\"2095\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2119\":1,\"2120\":1,\"2121\":1,\"2122\":1,\"2123\":1,\"2163\":1,\"2164\":1,\"2244\":1,\"2322\":1,\"2323\":1,\"2398\":1},\"1\":{\"1\":3,\"2\":1,\"3\":7,\"6\":1,\"7\":1,\"9\":2,\"11\":1,\"13\":1,\"18\":1,\"19\":2,\"22\":4,\"23\":6,\"24\":3,\"25\":2,\"26\":4,\"27\":1,\"28\":2,\"31\":3,\"32\":3,\"36\":2,\"37\":1,\"38\":2,\"39\":4,\"40\":1,\"41\":3,\"42\":2,\"43\":16,\"44\":4,\"45\":4,\"46\":3,\"47\":4,\"48\":2,\"49\":3,\"50\":2,\"52\":2,\"55\":2,\"56\":1,\"60\":4,\"66\":4,\"67\":3,\"68\":5,\"69\":3,\"70\":3,\"71\":5,\"76\":2,\"78\":6,\"79\":3,\"80\":1,\"81\":1,\"82\":5,\"84\":3,\"85\":1,\"86\":1,\"88\":2,\"92\":2,\"93\":1,\"94\":2,\"95\":2,\"96\":5,\"98\":3,\"99\":2,\"100\":2,\"101\":7,\"102\":3,\"104\":1,\"106\":4,\"107\":2,\"110\":7,\"111\":2,\"113\":1,\"117\":1,\"118\":3,\"119\":5,\"120\":2,\"121\":2,\"123\":2,\"124\":3,\"125\":1,\"126\":6,\"127\":8,\"128\":10,\"129\":1,\"130\":3,\"131\":2,\"132\":2,\"133\":1,\"134\":1,\"135\":2,\"137\":1,\"138\":2,\"139\":10,\"140\":1,\"141\":14,\"142\":3,\"143\":1,\"144\":3,\"145\":4,\"146\":2,\"147\":6,\"148\":4,\"150\":10,\"152\":1,\"153\":3,\"154\":1,\"156\":3,\"160\":2,\"161\":1,\"162\":10,\"163\":6,\"165\":2,\"166\":1,\"167\":3,\"168\":8,\"173\":5,\"174\":1,\"175\":4,\"178\":1,\"182\":1,\"190\":4,\"191\":2,\"193\":3,\"194\":3,\"195\":2,\"196\":13,\"197\":7,\"198\":1,\"199\":1,\"200\":28,\"201\":8,\"204\":1,\"205\":7,\"206\":3,\"207\":1,\"208\":2,\"210\":2,\"211\":6,\"212\":5,\"213\":10,\"216\":1,\"217\":10,\"218\":6,\"219\":1,\"220\":1,\"222\":4,\"223\":16,\"224\":9,\"225\":12,\"227\":1,\"228\":10,\"232\":11,\"234\":1,\"235\":9,\"236\":3,\"240\":3,\"242\":16,\"243\":23,\"245\":1,\"246\":4,\"247\":8,\"248\":1,\"249\":1,\"252\":1,\"253\":1,\"254\":8,\"255\":4,\"258\":8,\"259\":7,\"260\":3,\"261\":1,\"262\":15,\"263\":2,\"265\":1,\"266\":4,\"267\":39,\"268\":15,\"269\":16,\"274\":1,\"275\":4,\"276\":31,\"277\":15,\"278\":16,\"284\":16,\"285\":11,\"286\":66,\"287\":5,\"289\":1,\"290\":44,\"356\":2,\"402\":1,\"513\":2,\"514\":3,\"515\":1,\"516\":3,\"518\":2,\"520\":1,\"521\":4,\"523\":3,\"524\":3,\"525\":3,\"527\":1,\"535\":1,\"536\":7,\"537\":1,\"538\":1,\"558\":2,\"567\":2,\"570\":1,\"578\":1,\"581\":1,\"583\":1,\"586\":1,\"589\":1,\"594\":1,\"598\":1,\"603\":1,\"609\":1,\"611\":1,\"614\":2,\"615\":1,\"616\":4,\"617\":1,\"618\":1,\"619\":2,\"620\":8,\"621\":1,\"622\":3,\"623\":3,\"624\":1,\"625\":4,\"627\":4,\"634\":1,\"635\":2,\"636\":1,\"639\":4,\"640\":1,\"641\":2,\"643\":2,\"644\":4,\"648\":1,\"650\":2,\"651\":2,\"652\":1,\"653\":1,\"661\":2,\"664\":1,\"666\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"689\":1,\"690\":1,\"691\":3,\"692\":6,\"694\":1,\"696\":2,\"697\":1,\"699\":6,\"700\":2,\"703\":5,\"704\":1,\"705\":2,\"706\":1,\"709\":9,\"710\":10,\"711\":10,\"714\":1,\"719\":1,\"721\":1,\"722\":2,\"723\":1,\"724\":5,\"725\":5,\"726\":3,\"727\":3,\"728\":5,\"729\":2,\"733\":3,\"734\":2,\"738\":4,\"739\":1,\"740\":4,\"742\":1,\"744\":4,\"745\":6,\"746\":1,\"747\":6,\"748\":3,\"749\":1,\"753\":1,\"755\":7,\"756\":12,\"758\":1,\"759\":1,\"760\":14,\"768\":4,\"771\":2,\"773\":12,\"774\":9,\"777\":2,\"779\":1,\"780\":10,\"782\":1,\"784\":1,\"785\":7,\"786\":6,\"787\":2,\"789\":1,\"790\":1,\"792\":3,\"794\":2,\"797\":1,\"799\":1,\"800\":5,\"803\":2,\"804\":1,\"806\":2,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"817\":4,\"819\":2,\"821\":11,\"822\":1,\"824\":3,\"826\":1,\"828\":8,\"829\":14,\"830\":10,\"831\":1,\"832\":1,\"833\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"846\":19,\"847\":3,\"849\":3,\"850\":2,\"852\":1,\"853\":1,\"855\":1,\"856\":1,\"857\":1,\"858\":1,\"859\":4,\"861\":1,\"863\":1,\"865\":1,\"866\":7,\"867\":6,\"878\":1,\"879\":1,\"881\":6,\"882\":1,\"883\":1,\"884\":6,\"902\":1,\"911\":3,\"912\":5,\"921\":3,\"922\":4,\"924\":1,\"927\":2,\"929\":1,\"931\":2,\"932\":1,\"933\":2,\"934\":1,\"935\":2,\"936\":3,\"937\":3,\"938\":1,\"939\":1,\"942\":3,\"943\":2,\"951\":1,\"953\":1,\"957\":1,\"960\":1,\"962\":2,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"972\":1,\"974\":1,\"978\":4,\"980\":2,\"985\":1,\"994\":6,\"1000\":1,\"1002\":1,\"1008\":1,\"1011\":2,\"1022\":2,\"1029\":8,\"1031\":4,\"1033\":1,\"1035\":3,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1051\":2,\"1052\":1,\"1053\":4,\"1056\":1,\"1058\":1,\"1060\":1,\"1061\":7,\"1062\":4,\"1063\":5,\"1064\":5,\"1067\":1,\"1069\":1,\"1075\":1,\"1077\":1,\"1078\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1084\":1,\"1085\":1,\"1086\":4,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1107\":5,\"1109\":1,\"1111\":1,\"1112\":3,\"1113\":2,\"1115\":1,\"1118\":9,\"1119\":1,\"1121\":1,\"1123\":1,\"1124\":5,\"1125\":3,\"1126\":1,\"1128\":2,\"1130\":3,\"1133\":1,\"1135\":1,\"1136\":1,\"1137\":1,\"1138\":1,\"1139\":1,\"1140\":1,\"1141\":2,\"1143\":1,\"1145\":1,\"1146\":1,\"1147\":2,\"1150\":1,\"1152\":1,\"1153\":1,\"1154\":1,\"1155\":16,\"1156\":4,\"1157\":16,\"1160\":1,\"1162\":1,\"1164\":1,\"1165\":1,\"1166\":1,\"1168\":1,\"1169\":1,\"1176\":2,\"1178\":1,\"1181\":1,\"1185\":2,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1202\":1,\"1203\":1,\"1206\":1,\"1207\":4,\"1209\":1,\"1210\":2,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1224\":2,\"1225\":2,\"1227\":1,\"1228\":2,\"1231\":1,\"1232\":1,\"1234\":1,\"1235\":8,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1245\":3,\"1246\":5,\"1247\":1,\"1249\":1,\"1250\":3,\"1251\":2,\"1252\":1,\"1253\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1262\":3,\"1263\":1,\"1264\":3,\"1265\":1,\"1266\":1,\"1267\":2,\"1268\":3,\"1269\":3,\"1270\":3,\"1271\":4,\"1273\":1,\"1274\":1,\"1278\":3,\"1279\":3,\"1280\":9,\"1281\":8,\"1282\":5,\"1283\":4,\"1285\":1,\"1287\":1,\"1289\":1,\"1290\":2,\"1301\":1,\"1306\":2,\"1309\":2,\"1310\":2,\"1311\":2,\"1316\":4,\"1318\":2,\"1319\":2,\"1320\":3,\"1321\":2,\"1322\":2,\"1323\":2,\"1327\":5,\"1330\":4,\"1332\":2,\"1334\":6,\"1350\":1,\"1356\":2,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1368\":5,\"1371\":2,\"1372\":1,\"1384\":1,\"1385\":2,\"1387\":2,\"1388\":1,\"1389\":3,\"1391\":1,\"1392\":3,\"1393\":1,\"1395\":3,\"1399\":1,\"1400\":3,\"1401\":3,\"1402\":1,\"1403\":3,\"1405\":1,\"1407\":1,\"1408\":3,\"1410\":1,\"1412\":1,\"1413\":2,\"1414\":1,\"1416\":1,\"1418\":1,\"1419\":6,\"1420\":4,\"1421\":1,\"1422\":1,\"1423\":1,\"1424\":2,\"1425\":1,\"1426\":2,\"1427\":1,\"1428\":2,\"1429\":1,\"1430\":2,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1441\":4,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1450\":10,\"1451\":1,\"1452\":10,\"1453\":1,\"1454\":8,\"1455\":1,\"1456\":8,\"1457\":1,\"1458\":6,\"1459\":1,\"1460\":6,\"1461\":1,\"1463\":1,\"1465\":1,\"1466\":3,\"1467\":1,\"1468\":1,\"1469\":2,\"1470\":1,\"1478\":3,\"1484\":2,\"1493\":2,\"1494\":2,\"1502\":2,\"1510\":1,\"1512\":1,\"1513\":5,\"1514\":4,\"1518\":1,\"1519\":11,\"1521\":3,\"1523\":1,\"1524\":1,\"1525\":2,\"1526\":13,\"1528\":1,\"1529\":4,\"1531\":1,\"1533\":1,\"1534\":1,\"1535\":3,\"1536\":18,\"1538\":1,\"1539\":2,\"1540\":1,\"1542\":1,\"1544\":1,\"1546\":6,\"1548\":7,\"1549\":4,\"1550\":1,\"1551\":6,\"1552\":18,\"1553\":12,\"1555\":1,\"1556\":3,\"1557\":1,\"1558\":3,\"1559\":2,\"1577\":1,\"1581\":1,\"1584\":1,\"1585\":4,\"1586\":1,\"1587\":3,\"1588\":1,\"1590\":2,\"1591\":1,\"1592\":5,\"1594\":1,\"1595\":1,\"1596\":8,\"1597\":7,\"1598\":11,\"1599\":27,\"1600\":11,\"1603\":1,\"1604\":1,\"1605\":4,\"1606\":2,\"1607\":4,\"1609\":3,\"1610\":5,\"1611\":4,\"1612\":6,\"1613\":4,\"1614\":2,\"1615\":1,\"1616\":3,\"1618\":3,\"1619\":4,\"1620\":1,\"1621\":1,\"1622\":5,\"1625\":12,\"1626\":14,\"1627\":2,\"1628\":8,\"1629\":3,\"1639\":1,\"1644\":1,\"1645\":12,\"1647\":1,\"1650\":2,\"1653\":1,\"1654\":2,\"1655\":8,\"1656\":1,\"1658\":1,\"1660\":1,\"1662\":3,\"1663\":1,\"1664\":1,\"1665\":1,\"1666\":2,\"1668\":5,\"1669\":1,\"1670\":1,\"1671\":1,\"1674\":1,\"1676\":3,\"1677\":2,\"1678\":1,\"1679\":2,\"1680\":3,\"1681\":1,\"1683\":5,\"1692\":3,\"1698\":2,\"1704\":1,\"1705\":2,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1719\":10,\"1720\":4,\"1721\":5,\"1723\":1,\"1725\":14,\"1726\":2,\"1727\":2,\"1729\":2,\"1730\":3,\"1731\":4,\"1735\":2,\"1736\":5,\"1737\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1748\":2,\"1749\":3,\"1750\":7,\"1751\":3,\"1752\":1,\"1753\":1,\"1754\":2,\"1759\":3,\"1764\":2,\"1768\":2,\"1770\":5,\"1771\":6,\"1778\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1788\":3,\"1794\":2,\"1795\":2,\"1799\":1,\"1800\":1,\"1803\":1,\"1804\":2,\"1805\":1,\"1806\":8,\"1807\":1,\"1808\":1,\"1810\":2,\"1811\":2,\"1815\":3,\"1817\":1,\"1822\":3,\"1824\":1,\"1833\":2,\"1836\":1,\"1839\":2,\"1843\":2,\"1844\":1,\"1845\":1,\"1854\":1,\"1856\":1,\"1857\":1,\"1861\":2,\"1862\":4,\"1864\":1,\"1865\":1,\"1867\":1,\"1873\":2,\"1877\":2,\"1880\":1,\"1881\":1,\"1883\":1,\"1895\":1,\"1905\":2,\"1911\":1,\"1917\":1,\"1918\":1,\"1919\":3,\"1920\":2,\"1921\":1,\"1925\":1,\"1931\":1,\"1932\":2,\"1933\":3,\"1938\":2,\"1939\":1,\"1940\":2,\"1941\":1,\"1942\":2,\"1943\":2,\"1946\":1,\"1949\":3,\"1958\":1,\"1962\":1,\"1966\":2,\"1967\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1975\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"1992\":9,\"1993\":13,\"1994\":2,\"1995\":9,\"2000\":13,\"2001\":11,\"2007\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":3,\"2019\":1,\"2020\":1,\"2021\":1,\"2027\":1,\"2028\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2039\":10,\"2040\":2,\"2043\":7,\"2044\":20,\"2045\":6,\"2049\":7,\"2054\":8,\"2055\":6,\"2056\":6,\"2061\":1,\"2065\":5,\"2066\":5,\"2088\":1,\"2095\":1,\"2096\":1,\"2101\":4,\"2102\":1,\"2115\":1,\"2116\":1,\"2124\":1,\"2125\":1,\"2128\":1,\"2129\":3,\"2130\":6,\"2131\":3,\"2132\":3,\"2133\":2,\"2134\":7,\"2136\":12,\"2137\":2,\"2138\":4,\"2139\":1,\"2140\":2,\"2141\":1,\"2143\":2,\"2144\":2,\"2145\":5,\"2146\":4,\"2147\":5,\"2148\":1,\"2151\":1,\"2155\":3,\"2156\":2,\"2157\":4,\"2160\":1,\"2161\":1,\"2162\":2,\"2163\":1,\"2164\":1,\"2166\":1,\"2167\":1,\"2168\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2176\":4,\"2178\":1,\"2180\":1,\"2182\":1,\"2183\":1,\"2184\":2,\"2186\":1,\"2187\":1,\"2188\":2,\"2189\":1,\"2190\":1,\"2191\":5,\"2192\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2203\":1,\"2204\":1,\"2206\":1,\"2208\":1,\"2210\":1,\"2212\":1,\"2215\":2,\"2217\":1,\"2218\":1,\"2220\":9,\"2223\":4,\"2224\":5,\"2226\":3,\"2227\":1,\"2228\":2,\"2229\":2,\"2231\":2,\"2232\":1,\"2235\":10,\"2236\":11,\"2237\":2,\"2238\":1,\"2239\":19,\"2240\":19,\"2241\":2,\"2244\":1,\"2245\":20,\"2246\":1,\"2248\":1,\"2249\":7,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":4,\"2254\":1,\"2255\":1,\"2256\":2,\"2257\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":3,\"2263\":1,\"2264\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2276\":1,\"2277\":2,\"2280\":1,\"2287\":2,\"2293\":1,\"2298\":2,\"2304\":1,\"2306\":1,\"2307\":3,\"2309\":4,\"2310\":1,\"2311\":1,\"2314\":1,\"2322\":1,\"2323\":1,\"2325\":2,\"2326\":1,\"2327\":5,\"2334\":1,\"2342\":1,\"2353\":9,\"2354\":11,\"2355\":29,\"2364\":8,\"2369\":1,\"2376\":2,\"2384\":1,\"2385\":1,\"2398\":1,\"2402\":1,\"2406\":1,\"2407\":1,\"2408\":3,\"2410\":1,\"2411\":25,\"2412\":29,\"2413\":2,\"2415\":1,\"2417\":1,\"2419\":1,\"2422\":1,\"2423\":27,\"2424\":2,\"2425\":1,\"2427\":4,\"2428\":12,\"2429\":1,\"2430\":1,\"2431\":18,\"2432\":24,\"2433\":2,\"2435\":3,\"2436\":1,\"2438\":1,\"2440\":1,\"2442\":3,\"2444\":1,\"2446\":3,\"2447\":24,\"2448\":2,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1,\"2478\":2,\"2480\":1,\"2482\":3,\"2483\":1,\"2495\":3}}],[\"s>\",{\"1\":{\"2462\":1}}],[\"skcwse\",{\"1\":{\"2196\":1}}],[\"skfwse\",{\"1\":{\"2196\":1}}],[\"skablock\",{\"1\":{\"2203\":1}}],[\"skatdnnprojector\",{\"0\":{\"2205\":1},\"1\":{\"2205\":1}}],[\"skatdnnencoder\",{\"0\":{\"2203\":1},\"1\":{\"2203\":1}}],[\"skattentionmodule\",{\"0\":{\"2202\":1},\"1\":{\"2202\":1}}],[\"ska\",{\"0\":{\"2196\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2213\":1,\"2214\":1},\"1\":{\"2196\":1,\"2202\":1,\"2203\":4,\"2205\":1,\"2213\":1,\"2214\":1}}],[\"skirt\",{\"1\":{\"1672\":3}}],[\"skimseparator\",{\"0\":{\"1261\":1},\"1\":{\"1261\":1}}],[\"skim\",{\"0\":{\"1202\":1,\"1255\":1,\"1259\":2,\"1261\":1},\"1\":{\"223\":2,\"1202\":2,\"1255\":2,\"1259\":3,\"1261\":3}}],[\"skipch\",{\"1\":{\"1854\":1}}],[\"skipch=256\",{\"1\":{\"1854\":1}}],[\"skipped\",{\"1\":{\"1328\":1}}],[\"skipping\",{\"1\":{\"1259\":1,\"1261\":1,\"2354\":1}}],[\"skips\",{\"1\":{\"1310\":1}}],[\"skip=none\",{\"1\":{\"859\":1}}],[\"skip\",{\"1\":{\"118\":11,\"124\":4,\"125\":1,\"127\":2,\"136\":2,\"159\":1,\"161\":1,\"195\":1,\"200\":1,\"205\":1,\"211\":6,\"242\":1,\"243\":1,\"266\":4,\"275\":4,\"293\":1,\"603\":2,\"674\":4,\"846\":6,\"1057\":1,\"1076\":1,\"1086\":2,\"1124\":9,\"1125\":9,\"1148\":2,\"1155\":1,\"1157\":1,\"1207\":2,\"1211\":3,\"1238\":1,\"1242\":1,\"1268\":3,\"1272\":1,\"1273\":1,\"1274\":3,\"1368\":3,\"1389\":1,\"1391\":1,\"1396\":1,\"1401\":1,\"1403\":1,\"1450\":4,\"1452\":4,\"1454\":4,\"1456\":4,\"1458\":4,\"1460\":4,\"1462\":1,\"1466\":1,\"1468\":1,\"1610\":3,\"1626\":1,\"1628\":6,\"1759\":2,\"1854\":1,\"2348\":2,\"2355\":2,\"2462\":1}}],[\"sbdblock\",{\"0\":{\"1543\":1},\"1\":{\"1543\":1}}],[\"sbd\",{\"0\":{\"1541\":1},\"1\":{\"1509\":9,\"1511\":9,\"1541\":2,\"1543\":1,\"1553\":9}}],[\"sfi\",{\"1\":{\"1271\":1}}],[\"sqrt\",{\"1\":{\"1224\":1,\"1225\":1}}],[\"squared\",{\"1\":{\"1754\":1}}],[\"squaredrelu\",{\"0\":{\"835\":1},\"1\":{\"835\":1}}],[\"square\",{\"1\":{\"267\":1,\"276\":1,\"285\":1,\"286\":1,\"1839\":1,\"1991\":1,\"2237\":1}}],[\"sgd\",{\"0\":{\"1962\":2},\"1\":{\"1962\":5}}],[\"sgmse\",{\"1\":{\"1211\":1,\"1225\":1}}],[\"sge\",{\"1\":{\"165\":1}}],[\"s4d\",{\"1\":{\"821\":1,\"823\":1}}],[\"s4decoder\",{\"0\":{\"820\":1},\"1\":{\"820\":1}}],[\"s4\",{\"0\":{\"793\":1,\"817\":2,\"820\":1,\"821\":1,\"823\":1,\"824\":1,\"877\":1,\"895\":1,\"907\":1,\"924\":1,\"926\":1,\"928\":1,\"929\":1,\"938\":1,\"944\":1},\"1\":{\"793\":1,\"817\":3,\"820\":2,\"821\":2,\"823\":1,\"824\":1,\"877\":1,\"895\":1,\"907\":2,\"924\":1,\"926\":1,\"928\":1,\"929\":1,\"938\":1,\"944\":1}}],[\"sz=none\",{\"1\":{\"1210\":1}}],[\"sz=\",{\"1\":{\"1210\":1}}],[\"sz\",{\"1\":{\"639\":2,\"1210\":3}}],[\"s5\",{\"1\":{\"290\":1}}],[\"snyder\",{\"1\":{\"2209\":1}}],[\"snacmodel\",{\"1\":{\"2088\":1}}],[\"snacconfig\",{\"0\":{\"2061\":1},\"1\":{\"2061\":1}}],[\"snac\",{\"0\":{\"2061\":1,\"2088\":1,\"2095\":2,\"2096\":1,\"2102\":1,\"2115\":2,\"2116\":1},\"1\":{\"2061\":1,\"2088\":2,\"2095\":2,\"2096\":1,\"2102\":1,\"2115\":2,\"2116\":1}}],[\"snakers4\",{\"1\":{\"2101\":1}}],[\"snake1d\",{\"0\":{\"1464\":1},\"1\":{\"1464\":1}}],[\"snake\",{\"0\":{\"1464\":1,\"1500\":2},\"1\":{\"1389\":1,\"1391\":1,\"1464\":1,\"1500\":2}}],[\"snapshots\",{\"1\":{\"541\":3}}],[\"snapshot\",{\"1\":{\"50\":2,\"261\":1,\"541\":1}}],[\"snr=0\",{\"1\":{\"1253\":1}}],[\"snrloss\",{\"0\":{\"1248\":1},\"1\":{\"1248\":1}}],[\"snr\",{\"1\":{\"1050\":1,\"1053\":1,\"1116\":1,\"1189\":1,\"1247\":3,\"1253\":2}}],[\"sne\",{\"1\":{\"233\":1,\"234\":1,\"235\":2,\"2354\":1}}],[\"sdpa\",{\"1\":{\"1794\":1}}],[\"sdpa=false\",{\"1\":{\"1794\":1}}],[\"sdw\",{\"0\":{\"1330\":1},\"1\":{\"1330\":2}}],[\"sdrloss\",{\"0\":{\"1246\":1},\"1\":{\"1246\":1}}],[\"sdr\",{\"1\":{\"1066\":2,\"1246\":4,\"1247\":3,\"1269\":1,\"1270\":1,\"1271\":1}}],[\"sdes\",{\"0\":{\"1224\":1,\"1225\":1,\"1245\":1,\"1292\":1},\"1\":{\"1224\":1,\"1225\":1,\"1245\":1,\"1292\":1}}],[\"sde\",{\"0\":{\"1245\":1},\"1\":{\"1050\":1,\"1116\":1,\"1161\":1,\"1189\":1,\"1211\":1,\"1224\":6,\"1225\":6,\"1229\":1,\"1244\":1,\"1245\":8}}],[\"sds1\",{\"1\":{\"247\":1}}],[\"sds\",{\"0\":{\"245\":1,\"2022\":1,\"2023\":1,\"2024\":1,\"2025\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2037\":1,\"2038\":1,\"2039\":1,\"2040\":1,\"2041\":1,\"2042\":1,\"2043\":1,\"2044\":1,\"2045\":1,\"2046\":1,\"2047\":1,\"2048\":1,\"2049\":1,\"2050\":1,\"2051\":1,\"2052\":1,\"2053\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2057\":1,\"2058\":1,\"2059\":1,\"2060\":1,\"2061\":1,\"2062\":1,\"2063\":1,\"2064\":1,\"2065\":1,\"2066\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2070\":1,\"2071\":1,\"2072\":1,\"2073\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2078\":1,\"2079\":1,\"2080\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2085\":1,\"2086\":1,\"2087\":1,\"2088\":1,\"2089\":1,\"2090\":1,\"2091\":1,\"2092\":1,\"2093\":1,\"2094\":1,\"2095\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2119\":1,\"2120\":1,\"2121\":1,\"2122\":1,\"2123\":1,\"2541\":1},\"1\":{\"245\":3,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2039\":1,\"2040\":1,\"2043\":1,\"2044\":2,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2061\":1,\"2065\":1,\"2066\":1,\"2088\":1,\"2095\":1,\"2096\":1,\"2101\":1,\"2102\":1,\"2115\":1,\"2116\":1}}],[\"sd\",{\"1\":{\"228\":1,\"849\":2}}],[\"sdk\",{\"1\":{\"67\":1}}],[\"svoiceseparator\",{\"0\":{\"1252\":1},\"1\":{\"1252\":1}}],[\"svoice\",{\"0\":{\"1142\":1,\"1159\":1,\"1252\":1,\"1350\":1},\"1\":{\"223\":1,\"1142\":1,\"1159\":1,\"1252\":2,\"1350\":1}}],[\"svspreprocessor\",{\"0\":{\"2363\":1},\"1\":{\"2363\":1}}],[\"svstask\",{\"0\":{\"2268\":1},\"1\":{\"2268\":1}}],[\"svs2\",{\"0\":{\"273\":1},\"1\":{\"273\":2,\"274\":1,\"275\":1,\"276\":3,\"277\":1,\"282\":1}}],[\"svs1\",{\"1\":{\"267\":3,\"268\":1,\"269\":2,\"272\":1,\"273\":1,\"276\":1,\"278\":2}}],[\"svs\",{\"0\":{\"188\":1,\"475\":1,\"1508\":2,\"1509\":1,\"1511\":1,\"1513\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1519\":1,\"1520\":1,\"1521\":1,\"1522\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1529\":1,\"1530\":1,\"1532\":1,\"1533\":1,\"1534\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1548\":1,\"1549\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1556\":1,\"1557\":1,\"1558\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1562\":1,\"1563\":1,\"1564\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1574\":1,\"1575\":1,\"2222\":2,\"2223\":1,\"2226\":1,\"2227\":1,\"2228\":2,\"2229\":1,\"2230\":1,\"2231\":1,\"2232\":1,\"2233\":1,\"2234\":1,\"2235\":1,\"2236\":1,\"2237\":1,\"2238\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2242\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2255\":1,\"2268\":1,\"2525\":1,\"2547\":1},\"1\":{\"188\":1,\"264\":1,\"265\":3,\"266\":6,\"267\":15,\"270\":1,\"271\":1,\"274\":3,\"275\":6,\"276\":7,\"279\":1,\"280\":1,\"402\":1,\"403\":2,\"475\":4,\"1508\":3,\"1509\":1,\"1511\":1,\"1513\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1519\":1,\"1520\":1,\"1521\":2,\"1522\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1529\":1,\"1530\":1,\"1532\":1,\"1533\":1,\"1534\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1548\":1,\"1549\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1556\":1,\"1557\":1,\"1558\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1562\":1,\"1563\":1,\"1564\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1574\":1,\"1575\":1,\"2222\":3,\"2223\":1,\"2226\":1,\"2227\":1,\"2228\":3,\"2229\":2,\"2231\":1,\"2232\":1,\"2233\":1,\"2235\":2,\"2236\":2,\"2237\":1,\"2238\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2242\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2255\":1,\"2268\":1,\"2285\":1,\"2363\":1}}],[\"sw\",{\"1\":{\"1262\":1}}],[\"sw02001\",{\"1\":{\"196\":4}}],[\"swich\",{\"1\":{\"1838\":1}}],[\"swith\",{\"1\":{\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1768\":1,\"1895\":1}}],[\"switches\",{\"1\":{\"2044\":1}}],[\"switch\",{\"1\":{\"56\":1,\"175\":1,\"246\":1,\"267\":1,\"1860\":1,\"2327\":1,\"2355\":3}}],[\"swintransformerblock\",{\"0\":{\"1262\":1},\"1\":{\"1262\":1}}],[\"swin\",{\"0\":{\"1064\":1,\"1071\":1,\"1153\":1,\"1205\":1,\"1235\":1,\"1262\":1,\"1282\":2,\"1290\":1,\"1362\":1,\"1374\":1,\"1375\":1},\"1\":{\"1029\":1,\"1064\":2,\"1071\":1,\"1153\":1,\"1205\":1,\"1235\":4,\"1262\":2,\"1280\":5,\"1281\":1,\"1282\":7,\"1290\":1,\"1362\":1,\"1374\":1,\"1375\":1}}],[\"swish\",{\"0\":{\"652\":1,\"841\":1,\"1838\":2},\"1\":{\"141\":6,\"142\":1,\"629\":1,\"634\":1,\"652\":7,\"661\":2,\"664\":3,\"709\":1,\"710\":1,\"733\":1,\"734\":1,\"774\":1,\"780\":1,\"841\":2,\"1107\":1,\"1120\":1,\"1211\":1,\"1519\":1,\"1526\":1,\"1535\":1,\"1536\":2,\"1546\":1,\"1552\":1,\"1553\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1622\":1,\"1625\":1,\"1626\":1,\"1838\":3,\"1994\":1,\"2126\":1,\"2191\":1,\"2239\":2,\"2240\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2447\":1}}],[\"s2sttask\",{\"0\":{\"2263\":1},\"1\":{\"2263\":1}}],[\"s2sttacotron2loss\",{\"0\":{\"1991\":1},\"1\":{\"1991\":1}}],[\"s2stguidedattentionloss\",{\"0\":{\"1990\":1},\"1\":{\"1990\":1}}],[\"s2stctcloss\",{\"0\":{\"1988\":1},\"1\":{\"1988\":1}}],[\"s2stattentionloss\",{\"0\":{\"1987\":1},\"1\":{\"1987\":1}}],[\"s2st1\",{\"1\":{\"239\":1}}],[\"s2st\",{\"0\":{\"406\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1992\":1,\"1993\":1,\"1994\":1,\"1995\":1,\"2263\":1,\"2537\":1},\"1\":{\"190\":1,\"402\":1,\"403\":6,\"406\":2,\"1967\":2,\"1969\":2,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":2,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":2,\"1985\":1,\"1987\":2,\"1988\":2,\"1990\":2,\"1991\":2,\"1992\":1,\"1993\":1,\"1994\":1,\"1995\":1,\"2263\":1}}],[\"s2tpreprocessor\",{\"0\":{\"2361\":1},\"1\":{\"2361\":1}}],[\"s2tctcpreprocessor\",{\"0\":{\"2360\":1},\"1\":{\"2360\":1}}],[\"s2ttask\",{\"0\":{\"2264\":1},\"1\":{\"2264\":1}}],[\"s2t1\",{\"1\":{\"240\":2,\"242\":3,\"243\":7}}],[\"s2t\",{\"0\":{\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"1156\":1,\"1996\":1,\"1997\":1,\"2252\":1,\"2264\":1,\"2538\":1},\"1\":{\"162\":1,\"228\":6,\"240\":4,\"242\":9,\"243\":12,\"301\":2,\"335\":2,\"342\":2,\"349\":2,\"402\":2,\"403\":12,\"415\":5,\"421\":8,\"429\":8,\"436\":8,\"463\":2,\"1155\":1,\"1156\":3,\"1157\":1,\"1996\":1,\"1997\":2,\"2252\":1,\"2264\":1}}],[\"ssim\",{\"1\":{\"2427\":4}}],[\"ssimloss\",{\"0\":{\"2427\":1},\"1\":{\"2427\":2}}],[\"sskernelnplr\",{\"0\":{\"824\":1},\"1\":{\"824\":1}}],[\"sskerneldiag\",{\"0\":{\"823\":1},\"1\":{\"823\":1}}],[\"sskernel\",{\"0\":{\"821\":1},\"1\":{\"819\":1,\"821\":3,\"824\":1}}],[\"ss\",{\"1\":{\"818\":1}}],[\"ssm=1\",{\"1\":{\"821\":1}}],[\"ssm=none\",{\"1\":{\"821\":2}}],[\"ssms\",{\"1\":{\"821\":2}}],[\"ssm\",{\"0\":{\"938\":1},\"1\":{\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"744\":1,\"817\":1,\"819\":1,\"821\":5,\"822\":1,\"824\":3,\"828\":1,\"829\":1,\"830\":1,\"859\":1,\"877\":1,\"938\":4}}],[\"sse\",{\"1\":{\"228\":1}}],[\"ssh\",{\"1\":{\"166\":2,\"167\":1}}],[\"ssltask\",{\"0\":{\"2266\":1},\"1\":{\"2266\":1}}],[\"ssls\",{\"1\":{\"1992\":1,\"1995\":1}}],[\"ssl1\",{\"0\":{\"259\":1},\"1\":{\"232\":3,\"257\":1,\"258\":2,\"259\":3}}],[\"sslrs\",{\"1\":{\"128\":3}}],[\"sslr\",{\"1\":{\"128\":2}}],[\"ssl\",{\"0\":{\"738\":1,\"750\":1,\"2215\":1,\"2216\":1,\"2218\":1,\"2219\":1,\"2220\":1,\"2266\":1,\"2545\":1},\"1\":{\"92\":1,\"190\":1,\"205\":2,\"231\":1,\"232\":2,\"257\":1,\"258\":2,\"259\":6,\"260\":1,\"262\":4,\"267\":2,\"273\":1,\"276\":1,\"402\":1,\"403\":6,\"702\":1,\"738\":2,\"750\":1,\"1539\":1,\"1553\":9,\"1748\":1,\"2127\":1,\"2136\":15,\"2138\":2,\"2188\":1,\"2215\":3,\"2216\":3,\"2218\":1,\"2219\":1,\"2220\":1,\"2266\":1}}],[\"smi\",{\"1\":{\"173\":1}}],[\"smish\",{\"0\":{\"650\":1},\"1\":{\"141\":4,\"650\":5,\"664\":6}}],[\"smp\",{\"1\":{\"168\":1}}],[\"smoothness\",{\"0\":{\"2473\":1},\"1\":{\"2473\":2}}],[\"smoother\",{\"1\":{\"1668\":1}}],[\"smoothed\",{\"1\":{\"139\":1,\"1782\":1}}],[\"smooth\",{\"1\":{\"139\":2}}],[\"smoothing\",{\"0\":{\"1491\":1,\"1782\":1},\"1\":{\"44\":4,\"144\":4,\"625\":4,\"962\":1,\"1491\":1,\"1782\":5,\"1987\":2,\"2469\":2}}],[\"smallest\",{\"1\":{\"1224\":1,\"1225\":1}}],[\"smaller\",{\"1\":{\"243\":1,\"1063\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"2220\":1}}],[\"small\",{\"1\":{\"126\":1,\"243\":1,\"290\":2,\"790\":1,\"791\":1,\"864\":1,\"1246\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1493\":1,\"1494\":1}}],[\"s3prlpostfrontend\",{\"0\":{\"1539\":1},\"1\":{\"1539\":2}}],[\"s3prlfrontend\",{\"0\":{\"815\":1},\"1\":{\"815\":1,\"1539\":1}}],[\"s3prl\",{\"0\":{\"815\":1,\"1539\":1},\"1\":{\"106\":1,\"128\":4,\"750\":5,\"815\":1,\"1539\":1}}],[\"s16le\",{\"1\":{\"71\":1,\"74\":2}}],[\"sythesis\",{\"1\":{\"2363\":1}}],[\"syllabify\",{\"1\":{\"2280\":1}}],[\"syllablescorefeats\",{\"0\":{\"2238\":1},\"1\":{\"2238\":2}}],[\"syllable\",{\"1\":{\"267\":9,\"269\":1,\"276\":6,\"278\":1,\"1521\":4,\"2228\":4,\"2229\":4,\"2240\":2,\"2285\":1,\"2286\":1}}],[\"syl=false\",{\"1\":{\"2277\":1}}],[\"syb\",{\"1\":{\"1521\":12,\"1526\":2,\"1553\":2,\"2228\":14,\"2229\":14,\"2236\":3,\"2239\":3,\"2240\":3,\"2245\":3}}],[\"system\",{\"0\":{\"77\":1,\"79\":1,\"165\":1},\"1\":{\"60\":1,\"69\":1,\"78\":1,\"79\":2,\"81\":2,\"85\":1,\"96\":1,\"110\":2,\"120\":1,\"121\":1,\"162\":2,\"168\":2,\"206\":1,\"212\":1,\"218\":1,\"246\":3,\"247\":11,\"255\":1,\"267\":2,\"276\":2,\"285\":1,\"286\":2,\"824\":1,\"1246\":1,\"1261\":1,\"1717\":1,\"1770\":1,\"1948\":1,\"2040\":1,\"2044\":6,\"2240\":1,\"2325\":1,\"2404\":1}}],[\"systems\",{\"1\":{\"15\":1,\"165\":2,\"245\":1,\"246\":4,\"247\":1,\"2044\":3,\"2054\":1}}],[\"syms\",{\"1\":{\"603\":2,\"1942\":1}}],[\"symlink\",{\"1\":{\"218\":2,\"267\":2,\"276\":2,\"286\":2}}],[\"symbol=\",{\"1\":{\"2277\":1,\"2281\":1}}],[\"symbols\",{\"1\":{\"287\":2,\"481\":4,\"785\":1,\"922\":1,\"1934\":1,\"2130\":1,\"2136\":1,\"2275\":3,\"2285\":2,\"2292\":2,\"2293\":2,\"2298\":2,\"2336\":1,\"2337\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1}}],[\"symbolic\",{\"1\":{\"224\":1,\"243\":1}}],[\"symbol\",{\"1\":{\"45\":1,\"139\":1,\"145\":1,\"211\":1,\"213\":1,\"242\":1,\"243\":4,\"290\":2,\"372\":2,\"481\":6,\"511\":2,\"616\":1,\"625\":3,\"627\":3,\"634\":1,\"641\":1,\"643\":1,\"651\":1,\"667\":2,\"696\":1,\"697\":1,\"706\":1,\"740\":2,\"785\":1,\"847\":1,\"1589\":1,\"1749\":1,\"1760\":2,\"1815\":1,\"1863\":1,\"1866\":1,\"1868\":1,\"1870\":1,\"1910\":1,\"2275\":1,\"2283\":1,\"2284\":1,\"2285\":1,\"2291\":1,\"2293\":2,\"2298\":1,\"2336\":3,\"2337\":3,\"2356\":2,\"2360\":4,\"2361\":6,\"2362\":2,\"2363\":2}}],[\"sym\",{\"1\":{\"45\":2,\"145\":1,\"243\":1,\"421\":4,\"429\":4,\"436\":4,\"616\":2,\"625\":4,\"627\":4,\"696\":2,\"697\":2,\"736\":4,\"737\":2,\"740\":4,\"777\":3,\"1640\":2,\"1760\":4,\"1942\":1,\"1959\":2,\"1975\":2,\"1996\":6,\"1997\":6,\"2127\":2,\"2221\":6}}],[\"symmetry\",{\"1\":{\"824\":1}}],[\"symmetric\",{\"1\":{\"44\":4,\"1308\":2,\"1368\":1,\"1936\":1}}],[\"symm\",{\"1\":{\"44\":2,\"1936\":2}}],[\"synchronized\",{\"1\":{\"2162\":1}}],[\"synchronize\",{\"0\":{\"2162\":1},\"1\":{\"2162\":3}}],[\"synchronization\",{\"1\":{\"2134\":1}}],[\"synchronous\",{\"1\":{\"45\":3,\"129\":1,\"145\":4,\"616\":2,\"696\":2,\"697\":2,\"1720\":1,\"1721\":1,\"1726\":1,\"1727\":1}}],[\"sync=false\",{\"1\":{\"1720\":1}}],[\"sync\",{\"0\":{\"1501\":1,\"1502\":1},\"1\":{\"261\":2,\"301\":2,\"469\":2,\"616\":2,\"696\":2,\"697\":2,\"704\":1,\"1501\":2,\"1502\":1,\"1720\":2}}],[\"synth\",{\"0\":{\"536\":1,\"1565\":1},\"1\":{\"536\":5,\"1565\":1}}],[\"synthesize\",{\"1\":{\"1551\":1}}],[\"synthesizer\",{\"0\":{\"1971\":2,\"1974\":1,\"1977\":1,\"1985\":1,\"1992\":2,\"1993\":1,\"1994\":1,\"1995\":2},\"1\":{\"269\":1,\"278\":1,\"1971\":2,\"1974\":1,\"1975\":1,\"1977\":1,\"1985\":1,\"1992\":3,\"1993\":2,\"1994\":3,\"1995\":3}}],[\"synthesized\",{\"1\":{\"246\":2,\"247\":3,\"267\":1,\"276\":1,\"286\":1,\"2054\":2,\"2262\":1}}],[\"synthesis`\",{\"1\":{\"2223\":1,\"2227\":1,\"2231\":2,\"2245\":2}}],[\"synthesis\",{\"0\":{\"13\":1,\"188\":1,\"264\":1},\"1\":{\"13\":1,\"287\":2,\"1521\":1,\"1608\":2,\"1708\":1,\"1709\":1,\"1750\":1,\"1758\":1,\"1768\":1,\"1810\":1,\"1811\":1,\"1812\":1,\"2044\":1,\"2228\":1,\"2229\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":2,\"2245\":1,\"2255\":1,\"2404\":1,\"2425\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":1}}],[\"syntax\",{\"1\":{\"67\":1,\"168\":1,\"1876\":1}}],[\"slaney\",{\"1\":{\"1558\":1,\"1662\":1}}],[\"slstm\",{\"0\":{\"1462\":1},\"1\":{\"1462\":1}}],[\"slow\",{\"1\":{\"262\":1,\"821\":1,\"852\":1,\"1692\":1,\"1833\":1,\"2133\":1}}],[\"slower\",{\"1\":{\"262\":1}}],[\"slope\",{\"1\":{\"141\":2,\"664\":3,\"1389\":1,\"1390\":1,\"1392\":1,\"1396\":1,\"1397\":1,\"1401\":2,\"1402\":2,\"1408\":3,\"1409\":3,\"1420\":1,\"1466\":1,\"1467\":1,\"1513\":1,\"1526\":3,\"1548\":1,\"1549\":2,\"1551\":1,\"1553\":2,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":2,\"1596\":1,\"1597\":1,\"1598\":3,\"1599\":1,\"1600\":3,\"1604\":1,\"1605\":1,\"1606\":1,\"1609\":1,\"1614\":1,\"1615\":1,\"1618\":1,\"1619\":1,\"1625\":2}}],[\"slicing\",{\"1\":{\"2470\":1}}],[\"sliced\",{\"1\":{\"703\":1,\"755\":1,\"785\":2}}],[\"slight\",{\"1\":{\"1066\":1}}],[\"slightly\",{\"1\":{\"246\":1,\"1271\":1}}],[\"slience\",{\"1\":{\"606\":1}}],[\"slidingwindow\",{\"0\":{\"831\":1},\"1\":{\"768\":1,\"831\":1}}],[\"sliding\",{\"1\":{\"43\":2,\"141\":1,\"691\":1,\"768\":2,\"774\":3,\"831\":3,\"1918\":1,\"1960\":1,\"1961\":1}}],[\"slupreprocessor\",{\"0\":{\"2362\":1},\"1\":{\"2362\":1}}],[\"slutask\",{\"0\":{\"2265\":1},\"1\":{\"2265\":1}}],[\"slu1\",{\"1\":{\"250\":1}}],[\"slur=true\",{\"1\":{\"1546\":1}}],[\"slur\",{\"1\":{\"1521\":11,\"1526\":6,\"1546\":3,\"1552\":2,\"1553\":6,\"2228\":17,\"2229\":17,\"2235\":9,\"2236\":9,\"2239\":9,\"2240\":9,\"2245\":9}}],[\"slurp\",{\"1\":{\"186\":1,\"227\":2}}],[\"slurm\",{\"0\":{\"2388\":1,\"2389\":1},\"1\":{\"41\":1,\"62\":1,\"63\":1,\"165\":1,\"166\":3,\"167\":1,\"377\":1,\"449\":1,\"2388\":1,\"2389\":1}}],[\"slu\",{\"0\":{\"180\":1,\"186\":1,\"442\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2128\":1,\"2129\":1,\"2265\":1,\"2542\":1},\"1\":{\"12\":1,\"180\":1,\"186\":1,\"190\":1,\"199\":1,\"201\":1,\"228\":1,\"442\":5,\"2124\":1,\"2126\":1,\"2127\":1,\"2128\":1,\"2129\":1,\"2265\":1}}],[\"slt\",{\"1\":{\"5\":1,\"11\":1,\"267\":1,\"2203\":1}}],[\"scipy\",{\"0\":{\"1859\":1,\"1898\":1},\"1\":{\"1859\":2,\"1898\":1}}],[\"scope\",{\"1\":{\"1556\":11,\"2434\":2}}],[\"sconvtranspose2d\",{\"0\":{\"1448\":1},\"1\":{\"1448\":1}}],[\"sconvtranspose1d\",{\"0\":{\"1446\":1},\"1\":{\"1446\":1}}],[\"sconv2d\",{\"0\":{\"1444\":1},\"1\":{\"1444\":1}}],[\"sconv1d\",{\"0\":{\"1442\":1},\"1\":{\"1442\":1}}],[\"score2mel\",{\"1\":{\"1526\":4}}],[\"score2wav\",{\"0\":{\"1526\":1},\"1\":{\"1526\":1}}],[\"scoremodel\",{\"0\":{\"1253\":1},\"1\":{\"1253\":1}}],[\"scorers\",{\"0\":{\"1731\":1,\"1787\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1848\":1},\"1\":{\"795\":1,\"1719\":14,\"1720\":3,\"1721\":3,\"1725\":20,\"1726\":1,\"1727\":1,\"1731\":1,\"1787\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1805\":1,\"1806\":3,\"1822\":1,\"1848\":1,\"1862\":4}}],[\"scorer\",{\"0\":{\"1723\":1,\"1724\":1,\"1793\":1,\"1804\":1,\"1821\":1,\"1966\":1},\"1\":{\"692\":1,\"760\":2,\"790\":1,\"797\":1,\"820\":2,\"850\":1,\"1719\":2,\"1721\":2,\"1723\":3,\"1724\":3,\"1725\":2,\"1726\":2,\"1727\":2,\"1730\":1,\"1787\":2,\"1793\":2,\"1798\":1,\"1799\":2,\"1800\":1,\"1804\":4,\"1821\":3,\"1822\":2,\"1862\":2,\"1944\":2,\"1945\":1,\"1946\":1,\"1947\":2,\"1966\":2,\"1992\":1}}],[\"scorerinterface\",{\"0\":{\"1821\":1},\"1\":{\"676\":1,\"795\":1,\"1719\":2,\"1721\":2,\"1724\":1,\"1725\":2,\"1726\":3,\"1727\":3,\"1793\":1,\"1800\":1,\"1804\":1,\"1821\":1,\"1862\":2,\"1966\":1}}],[\"scores\",{\"0\":{\"1378\":2,\"1379\":1},\"1\":{\"254\":1,\"616\":1,\"644\":2,\"692\":1,\"696\":2,\"697\":2,\"706\":1,\"743\":2,\"749\":2,\"760\":2,\"784\":2,\"790\":1,\"797\":1,\"820\":2,\"850\":1,\"878\":1,\"879\":1,\"882\":1,\"883\":1,\"958\":2,\"1719\":7,\"1721\":3,\"1722\":3,\"1724\":1,\"1725\":20,\"1726\":1,\"1727\":1,\"1729\":1,\"1732\":2,\"1762\":2,\"1787\":2,\"1794\":2,\"1798\":1,\"1799\":1,\"1800\":1,\"1806\":3,\"1822\":1,\"1862\":1,\"1944\":2,\"1945\":1,\"1946\":1,\"1947\":2,\"1992\":1,\"1997\":1,\"2475\":1,\"2506\":1}}],[\"scored\",{\"1\":{\"223\":1}}],[\"score\",{\"0\":{\"269\":1,\"278\":1,\"530\":1,\"531\":1,\"532\":1,\"533\":1,\"596\":1,\"991\":1,\"995\":1,\"1003\":1,\"1005\":1,\"1015\":1,\"1017\":1,\"1253\":1,\"1729\":1,\"1730\":1,\"2232\":1,\"2233\":1,\"2238\":1,\"2244\":1,\"2396\":1},\"1\":{\"45\":1,\"141\":2,\"145\":1,\"211\":4,\"212\":2,\"223\":2,\"234\":1,\"235\":1,\"253\":1,\"254\":7,\"265\":2,\"266\":2,\"267\":17,\"268\":6,\"269\":4,\"274\":2,\"275\":2,\"276\":9,\"277\":6,\"278\":4,\"285\":3,\"295\":2,\"415\":2,\"531\":1,\"533\":1,\"596\":1,\"614\":2,\"616\":4,\"627\":4,\"628\":4,\"631\":3,\"634\":2,\"641\":2,\"643\":2,\"644\":9,\"651\":2,\"661\":3,\"692\":6,\"696\":6,\"697\":6,\"703\":1,\"740\":4,\"743\":1,\"755\":2,\"760\":5,\"763\":2,\"770\":1,\"775\":1,\"784\":2,\"785\":2,\"790\":5,\"797\":2,\"820\":5,\"847\":2,\"850\":5,\"947\":1,\"948\":1,\"949\":1,\"951\":1,\"958\":1,\"959\":1,\"991\":1,\"995\":1,\"1003\":2,\"1004\":6,\"1005\":2,\"1006\":7,\"1015\":1,\"1017\":1,\"1050\":1,\"1116\":1,\"1161\":1,\"1189\":1,\"1211\":1,\"1229\":1,\"1244\":1,\"1245\":3,\"1253\":1,\"1254\":1,\"1521\":1,\"1526\":9,\"1552\":6,\"1553\":9,\"1719\":14,\"1720\":5,\"1721\":2,\"1722\":3,\"1723\":4,\"1724\":2,\"1725\":15,\"1726\":5,\"1727\":5,\"1729\":1,\"1730\":1,\"1731\":8,\"1749\":2,\"1760\":5,\"1762\":1,\"1775\":2,\"1787\":4,\"1794\":2,\"1798\":2,\"1799\":3,\"1800\":3,\"1804\":2,\"1805\":4,\"1806\":5,\"1807\":3,\"1815\":2,\"1822\":5,\"1843\":2,\"1862\":2,\"1944\":4,\"1945\":2,\"1946\":2,\"1947\":4,\"1966\":4,\"1992\":4,\"2228\":1,\"2229\":1,\"2232\":1,\"2233\":1,\"2235\":10,\"2236\":13,\"2238\":1,\"2239\":12,\"2240\":13,\"2244\":1,\"2245\":12,\"2363\":1,\"2396\":1}}],[\"scoring\",{\"0\":{\"356\":1},\"1\":{\"38\":1,\"126\":2,\"199\":1,\"200\":2,\"204\":1,\"205\":2,\"210\":1,\"211\":1,\"216\":2,\"217\":6,\"222\":2,\"223\":7,\"227\":2,\"228\":3,\"236\":1,\"240\":1,\"242\":1,\"295\":2,\"356\":1,\"415\":2,\"596\":1,\"616\":1,\"958\":1,\"1804\":2,\"1821\":1,\"1966\":1}}],[\"sclae\",{\"1\":{\"1402\":1,\"1409\":1,\"1467\":1}}],[\"sclite\",{\"0\":{\"531\":1,\"532\":1,\"533\":1},\"1\":{\"531\":1,\"533\":1,\"594\":1}}],[\"sc=none\",{\"1\":{\"1273\":1,\"1274\":1}}],[\"sctm\",{\"1\":{\"578\":1}}],[\"scenarios\",{\"1\":{\"232\":1,\"258\":1,\"2184\":1}}],[\"scan\",{\"1\":{\"926\":1,\"1955\":1}}],[\"scarlett\",{\"1\":{\"202\":1}}],[\"scalable\",{\"1\":{\"232\":1,\"258\":1}}],[\"scalar=true\",{\"1\":{\"689\":1,\"852\":1}}],[\"scalar\",{\"1\":{\"78\":2,\"254\":1,\"689\":3,\"706\":1,\"1311\":1,\"1318\":1,\"1319\":1,\"1322\":1,\"1323\":1,\"1327\":1,\"1328\":1,\"1330\":1,\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1521\":1,\"1526\":1,\"1553\":1,\"1585\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1662\":5,\"1725\":1,\"1782\":1,\"1965\":1,\"2228\":1,\"2229\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2327\":1,\"2359\":1,\"2367\":1,\"2408\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2427\":2,\"2431\":1,\"2432\":1,\"2446\":1,\"2447\":1}}],[\"scaler\",{\"1\":{\"2347\":1,\"2369\":2,\"2371\":1}}],[\"scales=\",{\"1\":{\"1548\":2}}],[\"scales\",{\"1\":{\"1389\":1,\"1390\":1,\"1401\":3,\"1402\":5,\"1408\":3,\"1409\":6,\"1415\":1,\"1420\":1,\"1466\":2,\"1467\":4,\"1513\":3,\"1526\":4,\"1548\":3,\"1549\":5,\"1551\":3,\"1552\":6,\"1553\":4,\"1569\":1,\"1582\":4,\"1592\":3,\"1593\":1,\"1594\":4,\"1595\":5,\"1596\":3,\"1597\":3,\"1598\":4,\"1599\":3,\"1600\":4,\"1604\":3,\"1605\":5,\"1606\":6,\"1610\":1,\"1618\":1,\"1619\":8,\"1624\":3,\"1625\":4,\"1626\":3,\"2101\":1}}],[\"scale=8\",{\"1\":{\"2185\":1}}],[\"scale=4\",{\"1\":{\"2179\":1}}],[\"scale=false\",{\"1\":{\"1547\":1}}],[\"scale=true\",{\"1\":{\"1534\":1}}],[\"scale=0\",{\"1\":{\"1057\":1,\"1211\":1,\"1213\":1,\"1238\":1,\"1242\":1}}],[\"scale=none\",{\"1\":{\"1029\":1,\"1064\":1,\"1235\":1,\"1262\":1,\"1281\":1,\"1282\":1,\"1290\":1}}],[\"scale=16\",{\"1\":{\"1211\":1}}],[\"scale=1\",{\"1\":{\"895\":2,\"1177\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1346\":1,\"1347\":1}}],[\"scalenorm\",{\"0\":{\"648\":1},\"1\":{\"648\":3}}],[\"scaledpositionalencoding\",{\"0\":{\"1820\":1},\"1\":{\"692\":1,\"710\":1,\"711\":1,\"849\":1,\"1820\":1,\"1891\":1,\"1957\":1,\"1960\":1,\"1961\":1,\"1992\":1,\"1995\":1,\"2129\":1}}],[\"scaled\",{\"1\":{\"644\":1,\"784\":1,\"1278\":3,\"1306\":1,\"1371\":1,\"1526\":1,\"1598\":1,\"1599\":5,\"1600\":1,\"1785\":1,\"1794\":2,\"1817\":1,\"1820\":1,\"2239\":5,\"2240\":5,\"2411\":5,\"2412\":5,\"2423\":5,\"2432\":5,\"2447\":5}}],[\"scale\",{\"0\":{\"1573\":1},\"1\":{\"139\":5,\"202\":1,\"232\":1,\"258\":1,\"335\":2,\"342\":2,\"361\":2,\"475\":4,\"484\":4,\"490\":4,\"648\":1,\"674\":2,\"689\":2,\"756\":1,\"768\":3,\"778\":1,\"846\":2,\"1029\":2,\"1064\":2,\"1153\":1,\"1211\":1,\"1235\":2,\"1262\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1280\":3,\"1281\":2,\"1282\":2,\"1290\":2,\"1373\":1,\"1389\":1,\"1390\":2,\"1392\":4,\"1397\":4,\"1401\":4,\"1402\":9,\"1403\":1,\"1408\":4,\"1409\":8,\"1419\":3,\"1422\":4,\"1466\":4,\"1467\":9,\"1511\":1,\"1526\":10,\"1534\":2,\"1549\":9,\"1552\":8,\"1553\":11,\"1573\":1,\"1594\":3,\"1595\":10,\"1597\":2,\"1598\":4,\"1600\":4,\"1601\":2,\"1602\":2,\"1606\":1,\"1611\":1,\"1616\":3,\"1617\":6,\"1622\":1,\"1625\":10,\"1626\":8,\"1628\":6,\"1654\":2,\"1666\":2,\"1668\":1,\"1680\":3,\"1784\":1,\"2018\":8,\"2167\":2,\"2176\":3,\"2187\":3,\"2191\":1,\"2192\":3,\"2203\":4,\"2307\":3}}],[\"scaling=false\",{\"1\":{\"1784\":1}}],[\"scaling=1\",{\"1\":{\"1708\":1,\"1709\":1,\"1768\":1}}],[\"scaling=2\",{\"1\":{\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1715\":1}}],[\"scaling\",{\"0\":{\"1373\":1},\"1\":{\"139\":1,\"625\":1,\"703\":1,\"755\":1,\"757\":1,\"785\":1,\"786\":1,\"800\":1,\"867\":1,\"881\":1,\"884\":1,\"895\":1,\"922\":1,\"936\":1,\"937\":1,\"1054\":2,\"1301\":1,\"1306\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1526\":4,\"1553\":8,\"1598\":5,\"1600\":4,\"1617\":2,\"1625\":5,\"1683\":1,\"1698\":1,\"1704\":2,\"1705\":2,\"1706\":2,\"1707\":2,\"1708\":2,\"1709\":2,\"1710\":2,\"1711\":2,\"1712\":2,\"1715\":2,\"1768\":2,\"1770\":1,\"1771\":2,\"1784\":1,\"2000\":3,\"2001\":3,\"2008\":1,\"2167\":1,\"2168\":1,\"2176\":1,\"2239\":4,\"2240\":4}}],[\"sc\",{\"1\":{\"201\":1,\"1148\":1,\"1273\":1,\"1274\":1}}],[\"scratch\",{\"1\":{\"152\":1,\"184\":1,\"197\":1,\"261\":1,\"286\":2}}],[\"scriptmodule\",{\"1\":{\"614\":1,\"629\":1,\"635\":1,\"650\":1,\"652\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"692\":1,\"693\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"706\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"715\":1,\"718\":1,\"720\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"741\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"751\":1,\"752\":1,\"754\":1,\"757\":1,\"759\":1,\"766\":1,\"767\":1,\"771\":1,\"774\":1,\"775\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"786\":1,\"787\":1,\"788\":1,\"790\":1,\"791\":1,\"793\":1,\"794\":1,\"796\":1,\"798\":1,\"800\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"820\":1,\"823\":1,\"825\":1,\"828\":1,\"829\":1,\"830\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"859\":1,\"860\":1,\"862\":1,\"864\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"979\":1,\"981\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1072\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1126\":1,\"1127\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1156\":1,\"1158\":1,\"1159\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1177\":1,\"1179\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":1,\"1205\":1,\"1207\":1,\"1208\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":1,\"1219\":1,\"1222\":1,\"1223\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1279\":1,\"1281\":1,\"1282\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1333\":1,\"1334\":1,\"1381\":1,\"1383\":1,\"1387\":1,\"1392\":1,\"1398\":1,\"1400\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1417\":1,\"1419\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1508\":1,\"1509\":1,\"1511\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1533\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1547\":1,\"1554\":1,\"1576\":1,\"1588\":1,\"1590\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1638\":1,\"1640\":1,\"1641\":1,\"1652\":1,\"1656\":1,\"1657\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1667\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1702\":1,\"1838\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1959\":1,\"1965\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2129\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2221\":1,\"2222\":1,\"2232\":1,\"2238\":1,\"2305\":1,\"2325\":1,\"2327\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2407\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2434\":1,\"2443\":1,\"2445\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1}}],[\"script>\",{\"1\":{\"108\":2}}],[\"scripts\",{\"0\":{\"38\":1,\"117\":1},\"1\":{\"3\":3,\"37\":4,\"69\":2,\"73\":1,\"76\":2,\"107\":1,\"108\":3,\"109\":1,\"117\":1,\"126\":1,\"128\":1,\"153\":1,\"162\":1,\"166\":1,\"168\":1,\"195\":1,\"197\":1,\"220\":1,\"222\":3,\"223\":3,\"224\":2,\"225\":4,\"243\":2,\"263\":1,\"267\":4,\"269\":1,\"276\":4,\"278\":1,\"285\":4,\"286\":5,\"290\":1}}],[\"script\",{\"1\":{\"1\":3,\"3\":9,\"24\":3,\"25\":1,\"26\":1,\"32\":1,\"37\":2,\"38\":3,\"40\":2,\"41\":1,\"68\":1,\"109\":1,\"111\":1,\"117\":1,\"119\":1,\"126\":1,\"127\":1,\"133\":4,\"162\":1,\"167\":1,\"195\":1,\"224\":2,\"285\":5,\"536\":1}}],[\"scpfile\",{\"1\":{\"998\":1,\"1005\":1,\"1009\":2,\"1013\":1,\"1017\":1}}],[\"scp2json\",{\"0\":{\"598\":1},\"1\":{\"598\":1}}],[\"scps\",{\"0\":{\"461\":1},\"1\":{\"461\":4}}],[\"scp\",{\"0\":{\"68\":1,\"592\":1,\"991\":1,\"993\":1,\"995\":1,\"996\":1,\"998\":1,\"1003\":1,\"1005\":1,\"1007\":1,\"1009\":1,\"1011\":1,\"1013\":1,\"1015\":1,\"1017\":1,\"1028\":1},\"1\":{\"68\":17,\"69\":5,\"71\":5,\"73\":4,\"74\":1,\"75\":1,\"76\":6,\"79\":2,\"80\":2,\"96\":4,\"97\":4,\"98\":12,\"99\":4,\"100\":4,\"101\":2,\"126\":2,\"196\":3,\"201\":1,\"205\":1,\"211\":1,\"213\":2,\"217\":2,\"222\":1,\"223\":13,\"224\":6,\"228\":1,\"235\":1,\"242\":1,\"254\":1,\"266\":2,\"267\":5,\"268\":5,\"275\":2,\"276\":5,\"277\":5,\"285\":1,\"286\":27,\"356\":4,\"461\":1,\"519\":1,\"520\":6,\"521\":5,\"570\":1,\"592\":5,\"598\":1,\"991\":2,\"992\":1,\"993\":2,\"994\":2,\"995\":1,\"996\":2,\"997\":1,\"998\":2,\"999\":1,\"1000\":3,\"1001\":1,\"1003\":2,\"1004\":1,\"1005\":2,\"1006\":1,\"1007\":2,\"1008\":4,\"1009\":2,\"1010\":2,\"1011\":3,\"1012\":1,\"1013\":2,\"1014\":1,\"1015\":2,\"1016\":1,\"1017\":2,\"1018\":1,\"1023\":2,\"1027\":2,\"1028\":1,\"1830\":2,\"1881\":2,\"1883\":12,\"1912\":4,\"2157\":4,\"2336\":2,\"2337\":2,\"2341\":1,\"2343\":1,\"2345\":1,\"2346\":2,\"2350\":4,\"2352\":1,\"2353\":6,\"2356\":2,\"2360\":2,\"2361\":2,\"2362\":2,\"2364\":6,\"2368\":2}}],[\"sched\",{\"1\":{\"2442\":2}}],[\"schedueler\",{\"1\":{\"2428\":1}}],[\"scheduling\",{\"0\":{\"165\":1},\"1\":{\"69\":1,\"110\":1,\"120\":1,\"121\":1,\"165\":2,\"206\":1,\"212\":1,\"218\":1,\"236\":1,\"255\":1,\"267\":1,\"276\":1,\"286\":1}}],[\"schedulers\",{\"0\":{\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2540\":1},\"1\":{\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2347\":1,\"2355\":4,\"2369\":3,\"2371\":1}}],[\"scheduler\",{\"0\":{\"166\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2442\":1},\"1\":{\"37\":1,\"84\":4,\"87\":1,\"109\":1,\"110\":1,\"168\":1,\"206\":1,\"212\":1,\"218\":2,\"243\":2,\"255\":1,\"267\":2,\"276\":2,\"286\":2,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":1,\"2015\":1,\"2016\":2,\"2017\":4,\"2018\":6,\"2019\":5,\"2020\":3,\"2021\":4,\"2348\":1,\"2355\":20,\"2370\":2,\"2372\":1,\"2423\":1,\"2428\":3,\"2442\":4}}],[\"scheme\",{\"1\":{\"262\":1}}],[\"scheibler\",{\"1\":{\"11\":1}}],[\"su\",{\"1\":{\"2427\":1}}],[\"sufficient\",{\"1\":{\"1246\":1,\"2130\":1}}],[\"suffixes\",{\"1\":{\"1155\":1,\"1157\":1}}],[\"suffix\",{\"1\":{\"1040\":1,\"1268\":5,\"1949\":3,\"2144\":1}}],[\"surprising\",{\"1\":{\"242\":1}}],[\"sure\",{\"1\":{\"132\":1,\"200\":1,\"223\":1,\"224\":1,\"225\":1,\"242\":1,\"243\":1,\"286\":4,\"738\":1,\"1484\":1}}],[\"sutskever\",{\"1\":{\"202\":1}}],[\"sun1\",{\"1\":{\"287\":1}}],[\"sun\",{\"1\":{\"166\":1}}],[\"successive\",{\"1\":{\"1552\":1}}],[\"success\",{\"1\":{\"801\":1}}],[\"successful\",{\"1\":{\"755\":1,\"785\":1}}],[\"successfully\",{\"1\":{\"132\":1,\"164\":1}}],[\"such\",{\"0\":{\"152\":1},\"1\":{\"24\":1,\"42\":1,\"43\":1,\"44\":2,\"71\":1,\"79\":2,\"86\":1,\"106\":1,\"107\":1,\"110\":1,\"223\":3,\"228\":1,\"232\":2,\"242\":1,\"243\":2,\"246\":1,\"258\":2,\"269\":2,\"276\":1,\"278\":2,\"285\":1,\"286\":1,\"924\":1,\"928\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1376\":1,\"1377\":1,\"2146\":1,\"2262\":1,\"2355\":1}}],[\"sums\",{\"1\":{\"704\":1}}],[\"sum\",{\"0\":{\"918\":1,\"2320\":1},\"1\":{\"99\":1,\"100\":1,\"128\":1,\"293\":1,\"738\":2,\"750\":2,\"780\":2,\"786\":1,\"800\":1,\"918\":1,\"919\":2,\"921\":1,\"926\":1,\"935\":1,\"1162\":1,\"1210\":2,\"1211\":1,\"1475\":1,\"1606\":1,\"1719\":1,\"1725\":1,\"1732\":2,\"1806\":1,\"2000\":1,\"2320\":1,\"2355\":2,\"2469\":1,\"2470\":1}}],[\"summary\",{\"0\":{\"2151\":1,\"2154\":2,\"2163\":1,\"2310\":1,\"2316\":2,\"2322\":1},\"1\":{\"2151\":1,\"2154\":2,\"2159\":1,\"2163\":1,\"2310\":1,\"2316\":2,\"2322\":1,\"2347\":1,\"2359\":1,\"2367\":1,\"2369\":2,\"2371\":1}}],[\"summarize\",{\"1\":{\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1521\":1,\"1526\":1,\"1553\":1,\"1585\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"2228\":1,\"2229\":1,\"2327\":1,\"2408\":1,\"2446\":1}}],[\"summarized\",{\"1\":{\"223\":1}}],[\"summarization\",{\"0\":{\"15\":1},\"1\":{\"15\":1,\"1705\":1}}],[\"summed\",{\"1\":{\"1155\":1,\"1157\":1,\"1209\":1}}],[\"summ\",{\"1\":{\"15\":1}}],[\"subreporter\",{\"0\":{\"2367\":1},\"1\":{\"2338\":2,\"2347\":2,\"2354\":1,\"2359\":3,\"2365\":2,\"2367\":1,\"2369\":3,\"2371\":2}}],[\"subramanian\",{\"1\":{\"11\":1}}],[\"subcenter\",{\"0\":{\"2176\":2},\"1\":{\"2176\":2}}],[\"subclass\",{\"1\":{\"2148\":1}}],[\"subclasses\",{\"1\":{\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"693\":1,\"713\":1,\"718\":1,\"720\":1,\"722\":1,\"738\":1,\"741\":1,\"752\":1,\"756\":2,\"757\":1,\"773\":2,\"778\":1,\"781\":1,\"788\":1,\"791\":1,\"796\":1,\"798\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"821\":1,\"825\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"842\":1,\"844\":1,\"852\":1,\"854\":1,\"856\":1,\"860\":1,\"862\":1,\"864\":1,\"866\":1,\"867\":1,\"950\":1,\"952\":1,\"956\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1066\":1,\"1068\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1114\":1,\"1120\":1,\"1122\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1145\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1159\":1,\"1165\":1,\"1168\":1,\"1177\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1200\":1,\"1202\":1,\"1205\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1219\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1248\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1262\":1,\"1265\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1383\":1,\"1387\":1,\"1392\":1,\"1398\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1509\":1,\"1511\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1549\":1,\"1554\":1,\"1638\":1,\"1652\":1,\"1657\":1,\"1662\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1945\":1,\"1957\":1,\"1967\":1,\"1969\":1,\"1972\":1,\"1975\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1985\":1,\"1988\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2130\":1,\"2131\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2185\":1,\"2188\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2203\":1,\"2205\":1,\"2209\":1,\"2211\":1,\"2216\":1,\"2305\":1,\"2325\":1,\"2401\":1,\"2405\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2434\":1,\"2443\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2465\":1,\"2467\":1}}],[\"subprocesses\",{\"1\":{\"2131\":1}}],[\"subband\",{\"1\":{\"1061\":2,\"1062\":2,\"1063\":1}}],[\"subbands\",{\"0\":{\"1316\":1,\"1320\":1},\"1\":{\"1061\":3,\"1062\":2,\"1063\":7,\"1198\":1,\"1316\":5,\"1320\":4,\"1526\":1,\"1600\":1,\"1608\":8}}],[\"subbands=none\",{\"1\":{\"1061\":1,\"1063\":1}}],[\"subwriter\",{\"1\":{\"986\":3}}],[\"submodules\",{\"0\":{\"1688\":1},\"1\":{\"1688\":2,\"2312\":1}}],[\"submodule\",{\"1\":{\"1264\":4,\"1334\":4}}],[\"submodel\",{\"0\":{\"839\":1},\"1\":{\"839\":1}}],[\"submission\",{\"1\":{\"1210\":1,\"1264\":1,\"1334\":1}}],[\"submit\",{\"1\":{\"165\":1,\"168\":1}}],[\"submitting\",{\"1\":{\"62\":1,\"120\":1}}],[\"subtype\",{\"1\":{\"1009\":2,\"1028\":1}}],[\"subtracted\",{\"1\":{\"1246\":1,\"1247\":1}}],[\"subtract\",{\"0\":{\"1927\":1},\"1\":{\"804\":1,\"932\":1,\"934\":1,\"1927\":1}}],[\"subtask\",{\"1\":{\"406\":13}}],[\"sub\",{\"0\":{\"403\":1},\"1\":{\"620\":5,\"621\":2,\"639\":2,\"663\":2,\"674\":2,\"986\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1392\":1,\"1415\":1,\"1541\":1,\"1543\":1,\"1736\":6,\"1851\":5,\"2176\":5,\"2359\":3}}],[\"subjective\",{\"1\":{\"267\":2,\"276\":2,\"286\":3}}],[\"subnet\",{\"1\":{\"262\":4}}],[\"substraction\",{\"0\":{\"917\":1}}],[\"substantial\",{\"1\":{\"852\":1}}],[\"subsampled\",{\"1\":{\"1738\":2,\"1739\":2,\"1740\":2,\"1741\":2,\"1742\":2,\"1743\":2,\"1744\":2,\"1745\":2,\"1746\":2,\"1851\":1}}],[\"subsample\",{\"0\":{\"1892\":1},\"1\":{\"798\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1816\":2,\"1878\":4,\"1892\":1,\"1936\":2,\"2463\":1,\"2464\":1}}],[\"subsampling\",{\"0\":{\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1842\":1,\"1869\":1},\"1\":{\"141\":3,\"148\":1,\"621\":3,\"626\":2,\"639\":1,\"653\":3,\"663\":3,\"665\":3,\"1264\":1,\"1334\":1,\"1738\":2,\"1739\":2,\"1740\":2,\"1741\":2,\"1742\":3,\"1743\":2,\"1744\":2,\"1745\":2,\"1746\":2,\"1816\":1,\"1842\":4,\"1863\":2,\"1866\":2,\"1869\":2,\"1892\":2,\"1936\":1}}],[\"subsequent\",{\"0\":{\"1926\":1},\"1\":{\"669\":1,\"927\":2,\"1350\":1,\"1926\":3}}],[\"subset\",{\"1\":{\"136\":1,\"205\":1,\"224\":2,\"276\":2,\"1927\":4,\"2132\":1}}],[\"suitable\",{\"1\":{\"37\":1,\"96\":1,\"108\":1,\"162\":1,\"175\":1,\"2130\":1,\"2133\":1,\"2136\":1}}],[\"suggested\",{\"1\":{\"269\":1,\"278\":1}}],[\"suggestions\",{\"1\":{\"249\":1}}],[\"suggest\",{\"1\":{\"36\":1}}],[\"sujay\",{\"1\":{\"12\":1}}],[\"sudo\",{\"1\":{\"6\":1,\"159\":5,\"161\":4}}],[\"super\",{\"1\":{\"2355\":1}}],[\"superb\",{\"1\":{\"2127\":1}}],[\"superior\",{\"1\":{\"262\":1}}],[\"supervision\",{\"1\":{\"202\":1}}],[\"supervised\",{\"0\":{\"6\":1,\"7\":1,\"128\":1,\"231\":1,\"240\":1,\"257\":1},\"1\":{\"7\":1,\"8\":1,\"106\":1,\"128\":1,\"184\":1,\"185\":2,\"186\":1,\"190\":1,\"205\":1,\"207\":2,\"232\":1,\"258\":1,\"262\":1,\"273\":1,\"276\":1,\"281\":1,\"2131\":1,\"2136\":1}}],[\"suppression\",{\"1\":{\"285\":2}}],[\"supplied\",{\"1\":{\"139\":1}}],[\"supplement\",{\"1\":{\"138\":1}}],[\"supposed\",{\"1\":{\"82\":1}}],[\"supporting\",{\"1\":{\"245\":1,\"520\":1,\"2130\":1,\"2144\":1,\"2474\":3}}],[\"supported\",{\"0\":{\"71\":1,\"74\":1,\"160\":1,\"214\":1,\"219\":1,\"270\":1,\"271\":1,\"272\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"287\":1,\"288\":1,\"289\":1,\"2090\":1,\"2313\":1},\"1\":{\"46\":2,\"53\":9,\"57\":1,\"71\":3,\"80\":1,\"96\":1,\"106\":1,\"128\":1,\"137\":1,\"138\":1,\"173\":1,\"200\":2,\"205\":2,\"210\":1,\"211\":2,\"216\":1,\"217\":2,\"227\":1,\"242\":2,\"265\":3,\"266\":4,\"274\":4,\"275\":6,\"284\":3,\"285\":4,\"286\":1,\"527\":1,\"882\":1,\"883\":1,\"884\":1,\"922\":1,\"927\":1,\"1061\":1,\"1145\":1,\"1264\":2,\"1265\":1,\"1269\":2,\"1270\":2,\"1271\":2,\"1334\":2,\"1678\":2,\"2000\":1,\"2001\":1,\"2313\":1,\"2355\":1}}],[\"supports\",{\"1\":{\"26\":1,\"41\":1,\"42\":1,\"49\":1,\"57\":1,\"128\":1,\"129\":1,\"138\":1,\"173\":1,\"211\":1,\"217\":1,\"232\":1,\"235\":1,\"240\":1,\"259\":1,\"733\":2,\"1008\":1,\"1290\":1,\"1301\":1,\"1372\":1,\"1469\":1,\"1678\":1,\"2044\":2,\"2045\":1,\"2134\":2,\"2139\":1,\"2354\":2}}],[\"support\",{\"1\":{\"3\":2,\"27\":1,\"28\":1,\"32\":1,\"44\":1,\"52\":1,\"53\":2,\"67\":1,\"82\":1,\"106\":4,\"120\":1,\"139\":1,\"144\":2,\"146\":1,\"150\":1,\"160\":1,\"165\":1,\"173\":1,\"194\":1,\"211\":1,\"213\":1,\"219\":1,\"223\":4,\"225\":1,\"242\":1,\"285\":1,\"289\":1,\"290\":2,\"821\":1,\"925\":1,\"1021\":1,\"1209\":1,\"1422\":1,\"1485\":1,\"1655\":1,\"1684\":1,\"1702\":1,\"2131\":1,\"2134\":1,\"2148\":1,\"2216\":1,\"2280\":1}}],[\"sr\",{\"1\":{\"1162\":2,\"2044\":2,\"2054\":2,\"2065\":2}}],[\"sr=16000\",{\"1\":{\"1128\":1,\"1164\":1}}],[\"srun\",{\"1\":{\"62\":1,\"63\":1}}],[\"srivastav\",{\"1\":{\"5\":1}}],[\"srcs=2\",{\"1\":{\"1269\":1,\"1270\":1,\"1271\":1}}],[\"srcs\",{\"1\":{\"586\":3,\"1269\":2,\"1270\":2,\"1271\":2}}],[\"srcdir\",{\"1\":{\"528\":1}}],[\"srctexts\",{\"1\":{\"266\":1,\"267\":1,\"275\":1,\"276\":1,\"285\":1,\"286\":1}}],[\"src\",{\"1\":{\"3\":1,\"161\":1,\"243\":1,\"290\":1,\"463\":13,\"586\":2,\"731\":1,\"732\":1,\"737\":10,\"766\":1,\"767\":1,\"775\":1,\"787\":2,\"848\":1,\"850\":1,\"1478\":1,\"1751\":2,\"1952\":1,\"1959\":12,\"1975\":9,\"1976\":2,\"1992\":1,\"1995\":1,\"2221\":8,\"2308\":2}}],[\"saijo\",{\"1\":{\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1}}],[\"sa\",{\"1\":{\"774\":1,\"974\":3}}],[\"sarulab\",{\"1\":{\"285\":1}}],[\"safedumper\",{\"1\":{\"2480\":2}}],[\"safely\",{\"1\":{\"2130\":1}}],[\"safety\",{\"1\":{\"918\":1,\"2000\":1,\"2001\":1}}],[\"safe\",{\"0\":{\"1572\":1,\"2480\":1,\"2507\":2},\"1\":{\"243\":1,\"1572\":1,\"2480\":1,\"2507\":3}}],[\"sabato\",{\"1\":{\"202\":1}}],[\"satisfy\",{\"1\":{\"196\":1,\"213\":1,\"268\":1,\"277\":1,\"829\":1,\"2249\":1}}],[\"savingproxyfortensor\",{\"0\":{\"2060\":1}}],[\"savingproxyforstorage\",{\"0\":{\"2059\":1}}],[\"saving\",{\"1\":{\"91\":1,\"1526\":1,\"1553\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1824\":1,\"2134\":1,\"2354\":1}}],[\"save\",{\"0\":{\"2097\":1,\"2120\":1,\"2121\":1,\"2161\":1},\"1\":{\"126\":1,\"286\":2,\"377\":8,\"449\":2,\"514\":1,\"518\":1,\"756\":2,\"773\":2,\"821\":1,\"1009\":1,\"1029\":1,\"1064\":1,\"1235\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1644\":1,\"1678\":3,\"2044\":4,\"2134\":3,\"2157\":1,\"2161\":2,\"2348\":1,\"2354\":2,\"2359\":1,\"2370\":2,\"2372\":1}}],[\"saved\",{\"1\":{\"3\":1,\"87\":1,\"243\":1,\"756\":2,\"773\":2,\"866\":1,\"867\":1,\"2134\":1,\"2138\":2,\"2249\":2,\"2354\":2}}],[\"saves\",{\"1\":{\"3\":1,\"235\":1}}],[\"saon\",{\"1\":{\"45\":2,\"145\":2}}],[\"samp\",{\"1\":{\"1545\":1}}],[\"sampled\",{\"1\":{\"1655\":1,\"2000\":1,\"2001\":1}}],[\"samplers\",{\"0\":{\"1998\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2539\":1},\"1\":{\"1998\":1,\"1999\":1,\"2000\":2,\"2001\":2,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2377\":1}}],[\"sampler\",{\"0\":{\"1998\":1,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":2,\"2008\":2,\"2009\":1},\"1\":{\"223\":1,\"1253\":4,\"1254\":2,\"1645\":7,\"1998\":2,\"1999\":1,\"2000\":5,\"2001\":4,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":2,\"2008\":2,\"2009\":1,\"2355\":2,\"2377\":3}}],[\"samplerate\",{\"1\":{\"135\":1,\"285\":1,\"1162\":1}}],[\"samples\",{\"0\":{\"2107\":1},\"1\":{\"82\":1,\"101\":1,\"135\":1,\"201\":1,\"242\":1,\"286\":1,\"639\":9,\"716\":1,\"777\":1,\"792\":1,\"881\":1,\"884\":1,\"954\":1,\"958\":2,\"974\":1,\"993\":1,\"1029\":1,\"1066\":2,\"1155\":8,\"1156\":1,\"1157\":4,\"1158\":8,\"1162\":1,\"1235\":1,\"1246\":2,\"1247\":2,\"1269\":3,\"1270\":3,\"1271\":3,\"1280\":1,\"1281\":1,\"1282\":1,\"1334\":3,\"1400\":2,\"1420\":1,\"1422\":1,\"1489\":1,\"1498\":1,\"1499\":1,\"1552\":1,\"1643\":2,\"1645\":1,\"1646\":2,\"1647\":1,\"1650\":1,\"1669\":1,\"1676\":2,\"1702\":1,\"1854\":5,\"1940\":1,\"1942\":1,\"2000\":1,\"2001\":2,\"2005\":1,\"2131\":2,\"2133\":3,\"2136\":4,\"2139\":3,\"2143\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2167\":1,\"2176\":4,\"2184\":1,\"2249\":1,\"2253\":1,\"2355\":2,\"2469\":2,\"2471\":1}}],[\"sample\",{\"0\":{\"1497\":1,\"1498\":1,\"1499\":1,\"1701\":1,\"2118\":1,\"2119\":1},\"1\":{\"80\":11,\"82\":5,\"97\":7,\"101\":1,\"135\":2,\"211\":1,\"267\":9,\"674\":2,\"702\":2,\"716\":1,\"748\":3,\"768\":1,\"770\":1,\"959\":1,\"994\":2,\"1002\":1,\"1061\":1,\"1062\":1,\"1113\":1,\"1153\":1,\"1155\":1,\"1157\":1,\"1222\":1,\"1223\":1,\"1224\":1,\"1225\":1,\"1245\":1,\"1251\":1,\"1389\":2,\"1390\":2,\"1391\":1,\"1395\":2,\"1403\":1,\"1406\":1,\"1410\":1,\"1413\":2,\"1417\":1,\"1420\":3,\"1441\":8,\"1468\":1,\"1489\":2,\"1497\":1,\"1498\":1,\"1499\":1,\"1511\":1,\"1524\":2,\"1533\":5,\"1534\":1,\"1539\":1,\"1545\":2,\"1547\":1,\"1549\":1,\"1552\":1,\"1558\":3,\"1654\":1,\"1666\":1,\"1668\":1,\"1672\":2,\"1673\":2,\"1674\":4,\"1676\":2,\"1677\":2,\"1678\":2,\"1679\":2,\"1680\":2,\"1686\":2,\"1687\":2,\"1689\":2,\"1690\":2,\"1692\":2,\"1693\":1,\"1694\":2,\"1696\":1,\"1697\":2,\"1698\":2,\"1701\":1,\"1839\":1,\"1919\":1,\"2034\":1,\"2040\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2065\":2,\"2131\":4,\"2132\":2,\"2133\":4,\"2136\":4,\"2141\":2,\"2143\":1,\"2145\":1,\"2146\":1,\"2147\":2,\"2221\":1,\"2245\":1,\"2336\":1,\"2346\":1,\"2350\":1,\"2353\":2,\"2355\":2,\"2363\":2,\"2364\":2,\"2368\":1,\"2431\":1,\"2432\":1,\"2435\":1,\"2470\":6,\"2471\":2,\"2473\":2}}],[\"sampling\",{\"0\":{\"1050\":1,\"1110\":1,\"1116\":1,\"1161\":1,\"1189\":1,\"1218\":1,\"1221\":1,\"1229\":1,\"1244\":1,\"1301\":1,\"1306\":1,\"1331\":1,\"1344\":1,\"1345\":1,\"1371\":1,\"1372\":1},\"1\":{\"69\":1,\"70\":2,\"71\":1,\"211\":1,\"235\":1,\"267\":1,\"516\":1,\"523\":1,\"537\":1,\"639\":3,\"796\":1,\"823\":1,\"824\":1,\"831\":1,\"994\":1,\"1050\":1,\"1061\":2,\"1062\":1,\"1063\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1116\":1,\"1155\":1,\"1157\":1,\"1161\":1,\"1189\":1,\"1218\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1224\":1,\"1225\":1,\"1229\":1,\"1244\":1,\"1245\":4,\"1250\":2,\"1251\":2,\"1253\":1,\"1271\":1,\"1301\":1,\"1306\":1,\"1316\":1,\"1320\":1,\"1331\":1,\"1344\":1,\"1345\":1,\"1371\":1,\"1372\":1,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1413\":1,\"1419\":1,\"1420\":2,\"1466\":1,\"1524\":1,\"1526\":3,\"1545\":1,\"1553\":3,\"1560\":1,\"1561\":1,\"1565\":1,\"1570\":1,\"1598\":3,\"1600\":3,\"1607\":1,\"1625\":3,\"1645\":2,\"1655\":2,\"1662\":1,\"1672\":1,\"1673\":1,\"1674\":2,\"1676\":1,\"1678\":1,\"1679\":1,\"1680\":1,\"1686\":1,\"1687\":1,\"1689\":1,\"1690\":1,\"1692\":1,\"1694\":1,\"1697\":1,\"1698\":1,\"1854\":2,\"2000\":7,\"2001\":8,\"2044\":1,\"2049\":1,\"2065\":2,\"2345\":1,\"2353\":1,\"2357\":2,\"2364\":1,\"2435\":2,\"2482\":1,\"2495\":1}}],[\"samepad\",{\"0\":{\"825\":1,\"2465\":1},\"1\":{\"825\":1,\"2465\":1}}],[\"same\",{\"1\":{\"22\":1,\"43\":1,\"50\":2,\"52\":1,\"58\":1,\"68\":1,\"71\":1,\"79\":1,\"84\":1,\"98\":1,\"102\":1,\"107\":1,\"108\":1,\"141\":1,\"145\":2,\"162\":1,\"196\":3,\"200\":1,\"205\":1,\"211\":1,\"213\":3,\"223\":8,\"242\":2,\"254\":1,\"261\":1,\"268\":3,\"275\":1,\"277\":3,\"286\":3,\"287\":3,\"290\":1,\"616\":1,\"674\":2,\"691\":1,\"706\":1,\"755\":2,\"785\":1,\"819\":1,\"829\":1,\"972\":1,\"978\":1,\"982\":1,\"994\":4,\"1075\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1180\":1,\"1181\":1,\"1222\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1273\":1,\"1301\":2,\"1305\":1,\"1334\":2,\"1346\":1,\"1347\":1,\"1372\":2,\"1478\":1,\"1484\":1,\"1655\":1,\"1662\":1,\"1668\":1,\"1806\":1,\"1901\":1,\"1903\":1,\"1915\":1,\"1931\":1,\"1950\":1,\"2019\":1,\"2130\":1,\"2136\":1,\"2137\":1,\"2143\":1,\"2155\":1,\"2162\":1,\"2249\":1,\"2253\":1,\"2447\":1}}],[\"samuele\",{\"1\":{\"5\":1,\"11\":1}}],[\"sanjeev\",{\"1\":{\"14\":1}}],[\"saeki\",{\"1\":{\"9\":1}}],[\"sisnrloss\",{\"0\":{\"1247\":1},\"1\":{\"1247\":1}}],[\"sil\",{\"1\":{\"2462\":1}}],[\"silero\",{\"1\":{\"2101\":1}}],[\"silently\",{\"1\":{\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1555\":1,\"1639\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"silences\",{\"1\":{\"269\":1,\"278\":1}}],[\"silence\",{\"0\":{\"537\":1,\"606\":1,\"2378\":1},\"1\":{\"243\":1,\"269\":1,\"278\":1,\"286\":2,\"290\":4,\"537\":5,\"606\":3,\"1011\":1,\"2360\":1,\"2361\":1,\"2378\":2}}],[\"silu\",{\"1\":{\"1119\":1,\"1122\":1}}],[\"si\",{\"1\":{\"1053\":1,\"1247\":6,\"1269\":1,\"1270\":1,\"1271\":1}}],[\"sigpro\",{\"1\":{\"1330\":1}}],[\"sig\",{\"1\":{\"1128\":1}}],[\"sigma=0\",{\"1\":{\"1770\":1,\"1771\":1}}],[\"sigma=true\",{\"1\":{\"1211\":1}}],[\"sigma\",{\"1\":{\"785\":2,\"786\":2,\"866\":2,\"882\":2,\"883\":2,\"884\":2,\"921\":1,\"922\":2,\"1224\":13,\"1225\":2,\"1770\":1,\"1771\":2,\"1990\":1,\"2245\":3,\"2307\":5,\"2431\":3,\"2432\":2,\"2441\":3}}],[\"sigmoid\",{\"1\":{\"629\":1,\"650\":1,\"652\":1,\"754\":1,\"1107\":1,\"1117\":1,\"1126\":1,\"1127\":1,\"1130\":1,\"1131\":1,\"1136\":1,\"1141\":1,\"1199\":1,\"1217\":1,\"1232\":2,\"1261\":1,\"1267\":1,\"1268\":2,\"1278\":1,\"1555\":1,\"1619\":1,\"1621\":1}}],[\"signed\",{\"1\":{\"1678\":1}}],[\"signature\",{\"1\":{\"724\":2,\"725\":2,\"728\":2,\"729\":2,\"744\":2,\"784\":2,\"828\":2,\"829\":2,\"830\":2}}],[\"signal\",{\"0\":{\"1356\":1},\"1\":{\"207\":1,\"223\":2,\"224\":1,\"225\":1,\"1061\":3,\"1062\":1,\"1063\":3,\"1107\":1,\"1125\":1,\"1130\":1,\"1136\":1,\"1141\":1,\"1155\":3,\"1157\":3,\"1162\":1,\"1164\":5,\"1232\":1,\"1246\":2,\"1247\":2,\"1267\":1,\"1278\":1,\"1301\":1,\"1306\":1,\"1309\":1,\"1311\":1,\"1314\":1,\"1315\":2,\"1325\":4,\"1334\":1,\"1350\":6,\"1351\":1,\"1352\":1,\"1354\":2,\"1356\":5,\"1371\":1,\"1372\":1,\"1376\":1,\"1377\":1,\"1385\":1,\"1390\":1,\"1397\":1,\"1402\":1,\"1409\":1,\"1467\":1,\"1524\":1,\"1533\":1,\"1534\":1,\"1551\":1,\"1556\":4,\"1560\":1,\"1561\":1,\"1562\":1,\"1569\":1,\"1575\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1597\":1,\"1604\":1,\"1606\":1,\"1609\":1,\"1610\":2,\"1628\":1,\"1662\":1,\"1672\":3,\"1673\":3,\"1674\":1,\"1676\":3,\"1677\":1,\"1678\":2,\"1679\":3,\"1680\":3,\"1686\":2,\"1687\":3,\"1689\":3,\"1690\":3,\"1692\":3,\"1694\":2,\"1697\":3,\"1698\":4,\"1734\":1,\"1782\":1,\"1873\":2,\"1877\":2,\"1900\":1,\"1923\":1,\"1924\":1,\"2101\":3,\"2130\":1,\"2375\":1}}],[\"signals\",{\"1\":{\"205\":1,\"223\":4,\"224\":1,\"1246\":2,\"1247\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1515\":2,\"1697\":1}}],[\"significantly\",{\"1\":{\"269\":1,\"278\":1}}],[\"significant\",{\"1\":{\"41\":1,\"1679\":1}}],[\"sich\",{\"1\":{\"287\":1}}],[\"sivan\",{\"1\":{\"202\":1}}],[\"situations\",{\"1\":{\"269\":3,\"278\":3}}],[\"situation\",{\"1\":{\"71\":1}}],[\"site\",{\"1\":{\"2\":1,\"3\":1,\"2311\":1}}],[\"sim=false\",{\"1\":{\"1489\":1}}],[\"sim\",{\"0\":{\"1721\":1},\"1\":{\"315\":2,\"469\":2,\"674\":2,\"1164\":1,\"1721\":1}}],[\"simulates\",{\"1\":{\"1720\":1,\"1721\":1}}],[\"simulate\",{\"1\":{\"809\":1,\"2000\":1,\"2001\":1}}],[\"simulation\",{\"1\":{\"223\":1,\"1031\":1,\"1035\":2,\"1112\":1,\"1113\":2,\"1250\":1,\"1251\":2}}],[\"simultaneous\",{\"1\":{\"260\":1,\"261\":2,\"1309\":1,\"1311\":1}}],[\"simultaneously\",{\"1\":{\"44\":1,\"1729\":1,\"1730\":1}}],[\"similarities\",{\"1\":{\"286\":1}}],[\"similarity\",{\"1\":{\"254\":1,\"285\":2,\"286\":1,\"1164\":1,\"2427\":1}}],[\"similar\",{\"1\":{\"139\":1,\"141\":1,\"200\":2,\"223\":1,\"228\":1,\"262\":1,\"794\":1,\"1742\":1,\"2000\":1,\"2001\":1,\"2017\":1,\"2355\":1}}],[\"similarly\",{\"1\":{\"46\":1,\"1655\":1}}],[\"simply\",{\"1\":{\"1279\":1,\"1645\":1,\"1650\":1,\"2355\":1}}],[\"simplicity\",{\"1\":{\"790\":1}}],[\"simplified\",{\"1\":{\"141\":2,\"190\":1,\"644\":3,\"661\":3,\"750\":1}}],[\"simplify\",{\"1\":{\"39\":1,\"127\":1}}],[\"simpleoier\",{\"1\":{\"2043\":2}}],[\"simplex\",{\"1\":{\"1131\":1,\"1172\":1}}],[\"simplest\",{\"1\":{\"927\":1}}],[\"simpler\",{\"1\":{\"821\":1,\"1502\":1}}],[\"simple\",{\"1\":{\"139\":2,\"142\":1,\"162\":1,\"200\":1,\"262\":1,\"606\":1,\"633\":1,\"634\":1,\"827\":1,\"833\":1,\"954\":1,\"958\":1,\"1000\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1502\":1,\"2474\":1}}],[\"sizeddict\",{\"0\":{\"2481\":1},\"1\":{\"2481\":1}}],[\"sized\",{\"0\":{\"1474\":1,\"2481\":1,\"2489\":1},\"1\":{\"1474\":1,\"1998\":1,\"2481\":1,\"2489\":1}}],[\"size=40\",{\"1\":{\"1720\":1}}],[\"size=7\",{\"1\":{\"1548\":1}}],[\"size=64\",{\"1\":{\"1279\":2,\"1281\":2,\"1282\":1}}],[\"size=320\",{\"1\":{\"1128\":1}}],[\"size=3\",{\"1\":{\"1114\":1,\"1200\":1,\"1286\":1,\"1433\":1,\"1435\":1,\"1752\":1}}],[\"size=2\",{\"1\":{\"1854\":1}}],[\"size=256\",{\"1\":{\"1177\":1,\"1211\":1,\"2128\":1}}],[\"size=20\",{\"1\":{\"1029\":1,\"1259\":1,\"1279\":1,\"1281\":1}}],[\"size=24\",{\"1\":{\"47\":2,\"1164\":1}}],[\"size=\",{\"1\":{\"1029\":1,\"1080\":1,\"1082\":1,\"1124\":3,\"1147\":2,\"1235\":1,\"1262\":2,\"1281\":1,\"1282\":1}}],[\"size=8\",{\"1\":{\"787\":1}}],[\"size=192\",{\"1\":{\"2194\":1}}],[\"size=16\",{\"1\":{\"1720\":1}}],[\"size=1024\",{\"1\":{\"1176\":1}}],[\"size=10000\",{\"1\":{\"2134\":1}}],[\"size=100\",{\"1\":{\"1059\":1}}],[\"size=128\",{\"1\":{\"1029\":1,\"1279\":1,\"1281\":1}}],[\"size=1\",{\"1\":{\"770\":1,\"959\":1,\"1300\":1,\"1302\":1}}],[\"size=none\",{\"1\":{\"688\":1,\"1120\":1,\"2179\":1,\"2185\":1}}],[\"size1024\",{\"1\":{\"243\":1}}],[\"size`\",{\"1\":{\"147\":1,\"195\":1}}],[\"sizes=\",{\"1\":{\"1548\":3}}],[\"sizes\",{\"0\":{\"1473\":1},\"1\":{\"128\":1,\"633\":1,\"774\":2,\"780\":3,\"781\":1,\"783\":1,\"927\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1210\":2,\"1389\":2,\"1390\":2,\"1392\":1,\"1398\":3,\"1401\":2,\"1402\":2,\"1404\":3,\"1408\":5,\"1409\":2,\"1410\":3,\"1420\":4,\"1458\":3,\"1460\":3,\"1466\":1,\"1467\":1,\"1473\":1,\"1474\":1,\"1509\":1,\"1511\":1,\"1513\":6,\"1526\":4,\"1548\":4,\"1549\":2,\"1551\":6,\"1552\":11,\"1553\":5,\"1592\":6,\"1593\":1,\"1594\":1,\"1595\":2,\"1596\":3,\"1597\":3,\"1598\":4,\"1599\":6,\"1600\":4,\"1604\":5,\"1606\":4,\"1618\":4,\"1625\":4,\"1626\":4,\"1746\":2,\"2209\":1}}],[\"size24\",{\"1\":{\"47\":1}}],[\"size\",{\"0\":{\"59\":1,\"94\":1,\"123\":1,\"908\":1,\"946\":1,\"1507\":1,\"2387\":1,\"2489\":1,\"2491\":1},\"1\":{\"43\":8,\"45\":9,\"47\":1,\"48\":3,\"54\":3,\"55\":3,\"58\":2,\"59\":3,\"61\":3,\"78\":1,\"84\":2,\"94\":13,\"95\":8,\"96\":3,\"97\":2,\"98\":4,\"99\":2,\"100\":2,\"101\":2,\"102\":4,\"123\":3,\"126\":1,\"128\":5,\"130\":5,\"141\":29,\"142\":25,\"143\":2,\"145\":2,\"147\":3,\"173\":3,\"175\":6,\"243\":5,\"259\":1,\"262\":2,\"267\":2,\"276\":2,\"284\":1,\"286\":3,\"290\":3,\"295\":4,\"301\":4,\"309\":2,\"315\":4,\"321\":4,\"327\":2,\"331\":2,\"335\":6,\"342\":6,\"349\":2,\"361\":6,\"368\":2,\"377\":4,\"385\":2,\"389\":4,\"396\":4,\"404\":2,\"406\":6,\"415\":4,\"421\":4,\"429\":4,\"436\":2,\"442\":4,\"449\":12,\"457\":2,\"463\":6,\"469\":4,\"475\":2,\"481\":2,\"484\":2,\"490\":2,\"496\":2,\"498\":4,\"505\":8,\"516\":1,\"523\":1,\"614\":3,\"615\":1,\"616\":4,\"617\":6,\"618\":3,\"619\":3,\"620\":7,\"621\":8,\"622\":6,\"623\":6,\"624\":6,\"625\":3,\"626\":3,\"628\":1,\"630\":13,\"632\":12,\"633\":15,\"634\":21,\"636\":3,\"637\":8,\"638\":8,\"639\":1,\"640\":1,\"641\":12,\"642\":16,\"643\":18,\"644\":12,\"645\":3,\"647\":10,\"648\":1,\"649\":17,\"651\":9,\"653\":4,\"655\":3,\"660\":3,\"661\":2,\"662\":3,\"663\":4,\"665\":11,\"668\":3,\"669\":9,\"671\":5,\"672\":5,\"673\":5,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"691\":5,\"692\":6,\"696\":3,\"697\":3,\"699\":2,\"700\":5,\"701\":7,\"702\":1,\"703\":3,\"704\":1,\"705\":3,\"706\":2,\"709\":11,\"710\":15,\"711\":15,\"712\":1,\"713\":2,\"715\":2,\"716\":3,\"721\":1,\"731\":2,\"732\":2,\"733\":5,\"734\":5,\"735\":8,\"736\":1,\"737\":3,\"739\":1,\"741\":1,\"745\":5,\"746\":3,\"747\":5,\"748\":5,\"749\":4,\"750\":4,\"753\":1,\"755\":3,\"759\":1,\"760\":7,\"761\":3,\"762\":3,\"765\":4,\"766\":2,\"767\":2,\"768\":6,\"770\":4,\"771\":5,\"772\":4,\"774\":11,\"775\":2,\"777\":3,\"779\":1,\"780\":11,\"781\":1,\"783\":1,\"784\":6,\"785\":3,\"786\":2,\"790\":3,\"791\":1,\"792\":1,\"795\":1,\"796\":3,\"798\":6,\"799\":1,\"800\":2,\"804\":1,\"816\":1,\"819\":1,\"820\":4,\"821\":3,\"824\":2,\"825\":1,\"831\":1,\"846\":4,\"847\":7,\"848\":2,\"849\":8,\"850\":3,\"854\":1,\"862\":6,\"863\":1,\"865\":1,\"867\":2,\"878\":3,\"879\":3,\"881\":3,\"882\":3,\"883\":3,\"884\":3,\"908\":1,\"919\":2,\"921\":2,\"927\":4,\"932\":1,\"934\":1,\"935\":2,\"938\":1,\"946\":1,\"955\":1,\"958\":2,\"959\":4,\"960\":1,\"962\":3,\"971\":3,\"972\":1,\"973\":1,\"975\":3,\"977\":1,\"978\":1,\"979\":1,\"980\":1,\"981\":1,\"982\":2,\"984\":3,\"1029\":13,\"1031\":1,\"1035\":1,\"1063\":1,\"1064\":5,\"1072\":3,\"1074\":3,\"1075\":1,\"1080\":2,\"1107\":6,\"1112\":2,\"1113\":2,\"1118\":3,\"1120\":1,\"1122\":1,\"1124\":6,\"1125\":9,\"1133\":8,\"1134\":8,\"1136\":3,\"1137\":8,\"1139\":8,\"1141\":3,\"1142\":1,\"1145\":1,\"1147\":4,\"1148\":1,\"1156\":2,\"1159\":1,\"1162\":3,\"1164\":3,\"1176\":4,\"1179\":3,\"1180\":5,\"1181\":6,\"1185\":4,\"1202\":2,\"1208\":5,\"1215\":2,\"1235\":11,\"1250\":1,\"1251\":1,\"1252\":8,\"1255\":5,\"1257\":5,\"1259\":10,\"1261\":3,\"1262\":6,\"1264\":2,\"1265\":1,\"1267\":1,\"1268\":1,\"1269\":3,\"1270\":3,\"1271\":3,\"1272\":1,\"1273\":2,\"1274\":2,\"1278\":3,\"1279\":11,\"1280\":17,\"1281\":16,\"1282\":12,\"1283\":11,\"1290\":2,\"1296\":3,\"1316\":1,\"1320\":1,\"1334\":4,\"1350\":4,\"1358\":1,\"1368\":2,\"1374\":5,\"1375\":5,\"1383\":1,\"1387\":1,\"1389\":3,\"1391\":3,\"1392\":4,\"1396\":3,\"1397\":2,\"1400\":4,\"1401\":4,\"1402\":1,\"1403\":3,\"1408\":1,\"1409\":1,\"1422\":2,\"1441\":2,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":10,\"1452\":10,\"1453\":1,\"1454\":10,\"1456\":11,\"1466\":4,\"1467\":1,\"1468\":3,\"1469\":5,\"1482\":1,\"1484\":2,\"1486\":1,\"1495\":1,\"1507\":1,\"1509\":1,\"1511\":1,\"1513\":3,\"1517\":1,\"1519\":6,\"1520\":3,\"1523\":1,\"1524\":6,\"1525\":6,\"1526\":10,\"1530\":1,\"1533\":1,\"1534\":1,\"1535\":7,\"1536\":6,\"1540\":1,\"1543\":1,\"1546\":7,\"1547\":3,\"1548\":2,\"1549\":1,\"1551\":3,\"1552\":21,\"1553\":8,\"1554\":1,\"1556\":3,\"1557\":1,\"1558\":3,\"1560\":1,\"1561\":1,\"1563\":1,\"1564\":1,\"1568\":2,\"1581\":3,\"1582\":4,\"1583\":3,\"1592\":3,\"1594\":1,\"1595\":1,\"1598\":13,\"1599\":33,\"1600\":15,\"1604\":2,\"1605\":6,\"1606\":1,\"1609\":2,\"1610\":6,\"1611\":3,\"1612\":3,\"1613\":3,\"1614\":3,\"1615\":3,\"1616\":3,\"1619\":3,\"1620\":3,\"1621\":3,\"1622\":7,\"1624\":3,\"1625\":9,\"1626\":25,\"1628\":5,\"1632\":4,\"1633\":4,\"1640\":1,\"1641\":1,\"1643\":1,\"1644\":2,\"1646\":1,\"1647\":2,\"1657\":1,\"1668\":3,\"1680\":1,\"1692\":1,\"1698\":1,\"1702\":3,\"1706\":1,\"1708\":3,\"1709\":3,\"1710\":4,\"1711\":2,\"1712\":1,\"1715\":1,\"1716\":3,\"1719\":9,\"1721\":18,\"1725\":7,\"1726\":3,\"1727\":3,\"1733\":1,\"1735\":8,\"1736\":3,\"1737\":3,\"1747\":3,\"1748\":2,\"1749\":3,\"1750\":3,\"1751\":8,\"1753\":2,\"1756\":3,\"1757\":3,\"1759\":5,\"1768\":4,\"1779\":8,\"1782\":2,\"1783\":1,\"1785\":4,\"1789\":3,\"1790\":3,\"1794\":8,\"1795\":3,\"1806\":2,\"1810\":1,\"1815\":3,\"1817\":4,\"1842\":4,\"1843\":3,\"1854\":2,\"1856\":5,\"1862\":6,\"1869\":1,\"1893\":4,\"1895\":2,\"1920\":1,\"1926\":5,\"1940\":3,\"1942\":3,\"1944\":3,\"1945\":1,\"1947\":3,\"1948\":1,\"1957\":1,\"1958\":1,\"1959\":3,\"1960\":4,\"1961\":4,\"1965\":2,\"1973\":1,\"1974\":3,\"1975\":3,\"1977\":5,\"1979\":1,\"1981\":1,\"1983\":1,\"1984\":3,\"1987\":1,\"1992\":3,\"1993\":2,\"1994\":2,\"1995\":4,\"1996\":1,\"1997\":1,\"1999\":2,\"2000\":5,\"2001\":4,\"2002\":2,\"2003\":1,\"2004\":1,\"2005\":2,\"2006\":3,\"2007\":5,\"2008\":3,\"2014\":2,\"2016\":2,\"2019\":1,\"2021\":2,\"2039\":6,\"2061\":2,\"2124\":1,\"2125\":1,\"2126\":5,\"2127\":2,\"2128\":2,\"2129\":8,\"2132\":2,\"2134\":4,\"2137\":2,\"2141\":2,\"2146\":1,\"2166\":1,\"2167\":3,\"2173\":1,\"2175\":1,\"2176\":3,\"2183\":7,\"2187\":7,\"2188\":2,\"2189\":1,\"2190\":6,\"2191\":11,\"2192\":4,\"2193\":1,\"2194\":1,\"2195\":1,\"2198\":2,\"2199\":1,\"2203\":4,\"2204\":1,\"2205\":2,\"2206\":1,\"2207\":3,\"2208\":6,\"2209\":4,\"2210\":1,\"2211\":2,\"2212\":1,\"2219\":3,\"2221\":2,\"2223\":1,\"2224\":2,\"2232\":1,\"2235\":2,\"2236\":5,\"2238\":1,\"2239\":9,\"2240\":7,\"2244\":1,\"2245\":7,\"2249\":1,\"2258\":4,\"2278\":1,\"2283\":1,\"2291\":1,\"2308\":1,\"2336\":1,\"2340\":2,\"2342\":1,\"2350\":1,\"2355\":1,\"2387\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2411\":15,\"2412\":27,\"2415\":1,\"2417\":1,\"2419\":1,\"2423\":26,\"2425\":4,\"2426\":2,\"2427\":3,\"2429\":4,\"2431\":7,\"2432\":9,\"2433\":3,\"2435\":3,\"2438\":1,\"2440\":1,\"2441\":3,\"2447\":23,\"2454\":1,\"2461\":1,\"2462\":1,\"2465\":1,\"2471\":3,\"2473\":3,\"2482\":1,\"2489\":2,\"2490\":1,\"2491\":1}}],[\"sin\",{\"1\":{\"1545\":1}}],[\"sincnet\",{\"1\":{\"1668\":1}}],[\"sincconv\",{\"0\":{\"1668\":1},\"1\":{\"1668\":1}}],[\"sincstride\",{\"1\":{\"691\":1}}],[\"sinc\",{\"0\":{\"768\":1,\"832\":1,\"1654\":1,\"1661\":1,\"1666\":1,\"1668\":1},\"1\":{\"691\":11,\"768\":11,\"831\":1,\"832\":1,\"1654\":1,\"1661\":1,\"1666\":1,\"1668\":11}}],[\"since\",{\"1\":{\"39\":1,\"217\":1,\"243\":1,\"267\":1,\"276\":1,\"286\":1,\"677\":1,\"679\":1,\"681\":1,\"683\":1,\"685\":1,\"687\":1,\"690\":1,\"694\":1,\"714\":1,\"719\":1,\"721\":1,\"723\":1,\"739\":1,\"742\":1,\"753\":1,\"758\":1,\"779\":1,\"782\":1,\"789\":1,\"792\":1,\"797\":1,\"799\":1,\"806\":1,\"808\":1,\"810\":1,\"812\":1,\"814\":1,\"816\":1,\"822\":1,\"826\":1,\"834\":1,\"836\":1,\"838\":1,\"840\":1,\"843\":1,\"845\":1,\"853\":1,\"855\":1,\"857\":1,\"861\":1,\"863\":1,\"865\":1,\"951\":1,\"953\":1,\"957\":1,\"964\":1,\"966\":1,\"968\":1,\"970\":1,\"1031\":1,\"1033\":1,\"1035\":1,\"1037\":1,\"1039\":1,\"1041\":1,\"1043\":1,\"1045\":1,\"1047\":1,\"1049\":1,\"1052\":1,\"1056\":1,\"1058\":1,\"1060\":1,\"1067\":1,\"1069\":1,\"1077\":1,\"1079\":1,\"1081\":1,\"1083\":1,\"1085\":1,\"1088\":1,\"1090\":1,\"1092\":1,\"1094\":1,\"1096\":1,\"1098\":1,\"1100\":1,\"1102\":1,\"1104\":1,\"1106\":1,\"1109\":1,\"1111\":1,\"1115\":1,\"1121\":1,\"1123\":1,\"1135\":1,\"1138\":1,\"1140\":1,\"1143\":1,\"1146\":1,\"1150\":1,\"1152\":1,\"1154\":1,\"1160\":1,\"1166\":1,\"1169\":1,\"1178\":1,\"1186\":1,\"1188\":1,\"1191\":1,\"1193\":1,\"1195\":1,\"1197\":1,\"1201\":1,\"1203\":1,\"1206\":1,\"1212\":1,\"1214\":1,\"1216\":1,\"1220\":1,\"1227\":1,\"1231\":1,\"1234\":1,\"1237\":1,\"1239\":1,\"1241\":1,\"1243\":1,\"1249\":1,\"1254\":1,\"1256\":1,\"1258\":1,\"1260\":1,\"1263\":1,\"1266\":1,\"1285\":1,\"1287\":1,\"1289\":1,\"1384\":1,\"1388\":1,\"1393\":1,\"1399\":1,\"1405\":1,\"1407\":1,\"1412\":1,\"1414\":1,\"1416\":1,\"1418\":1,\"1421\":1,\"1423\":1,\"1425\":1,\"1427\":1,\"1429\":1,\"1431\":1,\"1434\":1,\"1436\":1,\"1438\":1,\"1440\":1,\"1443\":1,\"1445\":1,\"1447\":1,\"1449\":1,\"1451\":1,\"1453\":1,\"1455\":1,\"1457\":1,\"1459\":1,\"1461\":1,\"1463\":1,\"1465\":1,\"1470\":1,\"1510\":1,\"1512\":1,\"1518\":1,\"1523\":1,\"1526\":1,\"1528\":1,\"1531\":1,\"1538\":1,\"1540\":1,\"1542\":1,\"1544\":1,\"1550\":1,\"1553\":1,\"1555\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1639\":1,\"1647\":1,\"1653\":1,\"1658\":1,\"1663\":1,\"1939\":1,\"1941\":1,\"1943\":1,\"1946\":1,\"1958\":1,\"1968\":1,\"1970\":1,\"1973\":1,\"1976\":1,\"1979\":1,\"1981\":1,\"1983\":1,\"1986\":1,\"1989\":1,\"2000\":1,\"2027\":1,\"2029\":1,\"2031\":1,\"2033\":1,\"2035\":1,\"2125\":1,\"2133\":1,\"2136\":1,\"2169\":1,\"2171\":1,\"2173\":1,\"2175\":1,\"2178\":1,\"2180\":1,\"2182\":1,\"2186\":1,\"2189\":1,\"2193\":1,\"2195\":1,\"2197\":1,\"2199\":1,\"2201\":1,\"2204\":1,\"2206\":1,\"2210\":1,\"2212\":1,\"2217\":1,\"2306\":1,\"2326\":1,\"2402\":1,\"2406\":1,\"2410\":1,\"2415\":1,\"2417\":1,\"2419\":1,\"2435\":1,\"2444\":1,\"2450\":1,\"2452\":1,\"2454\":1,\"2457\":1,\"2459\":1,\"2461\":1,\"2466\":1,\"2468\":1}}],[\"sinegen\",{\"0\":{\"1545\":1},\"1\":{\"1545\":3}}],[\"sine\",{\"0\":{\"1545\":1},\"1\":{\"647\":1,\"1545\":8}}],[\"sinusoidal\",{\"1\":{\"647\":1}}],[\"sinusoid\",{\"1\":{\"647\":1}}],[\"sint32\",{\"1\":{\"70\":1}}],[\"sint16\",{\"1\":{\"68\":1,\"70\":2}}],[\"sing\",{\"1\":{\"2363\":1}}],[\"singen\",{\"1\":{\"1545\":1}}],[\"singer\",{\"1\":{\"267\":1,\"1536\":1}}],[\"singomd\",{\"1\":{\"475\":2}}],[\"singledataset\",{\"0\":{\"2141\":1},\"1\":{\"2141\":1}}],[\"singlernn\",{\"0\":{\"1257\":1},\"1\":{\"1257\":1}}],[\"single\",{\"0\":{\"56\":1},\"1\":{\"41\":2,\"54\":6,\"56\":1,\"69\":1,\"106\":1,\"121\":1,\"173\":1,\"197\":1,\"202\":1,\"205\":1,\"240\":2,\"242\":1,\"254\":1,\"259\":1,\"262\":2,\"267\":1,\"275\":4,\"276\":9,\"284\":1,\"286\":4,\"289\":1,\"637\":1,\"689\":1,\"705\":1,\"759\":1,\"770\":2,\"828\":1,\"829\":1,\"830\":1,\"938\":1,\"959\":2,\"974\":1,\"1029\":1,\"1064\":1,\"1078\":1,\"1117\":1,\"1130\":1,\"1145\":1,\"1153\":1,\"1202\":1,\"1209\":3,\"1217\":1,\"1235\":1,\"1257\":1,\"1262\":1,\"1265\":1,\"1290\":1,\"1325\":1,\"1354\":1,\"1605\":1,\"1655\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1674\":1,\"1726\":1,\"1727\":1,\"1751\":1,\"1794\":1,\"1895\":1,\"2040\":1,\"2043\":1,\"2045\":1,\"2049\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2066\":1,\"2130\":5,\"2131\":1,\"2137\":8,\"2139\":2,\"2143\":1,\"2150\":1,\"2160\":1,\"2183\":1,\"2184\":1,\"2190\":1,\"2208\":1,\"2232\":1,\"2238\":1,\"2286\":1,\"2336\":1,\"2346\":1,\"2355\":5,\"2368\":1}}],[\"singh\",{\"1\":{\"15\":1}}],[\"singingscorewriter\",{\"0\":{\"1005\":1},\"1\":{\"1005\":1,\"1006\":1}}],[\"singingscorereader\",{\"0\":{\"1003\":1},\"1\":{\"1003\":1}}],[\"singing\",{\"0\":{\"13\":1,\"188\":1,\"264\":1,\"2223\":1,\"2227\":1,\"2231\":1,\"2242\":1,\"2243\":1,\"2245\":3},\"1\":{\"13\":1,\"265\":1,\"267\":8,\"269\":1,\"272\":1,\"276\":4,\"278\":1,\"1521\":16,\"1526\":9,\"1553\":9,\"1768\":2,\"2222\":2,\"2223\":4,\"2227\":4,\"2228\":16,\"2229\":16,\"2231\":5,\"2235\":2,\"2236\":2,\"2239\":1,\"2240\":3,\"2242\":1,\"2243\":1,\"2245\":9,\"2255\":1,\"2363\":3}}],[\"sids\",{\"1\":{\"1521\":6,\"1526\":4,\"1552\":5,\"1553\":4,\"1585\":4,\"1598\":2,\"1599\":5,\"1625\":4,\"1626\":5,\"1975\":1,\"1976\":1,\"1992\":3,\"1993\":5,\"1995\":3,\"2228\":6,\"2229\":6,\"2235\":5,\"2236\":5,\"2239\":5,\"2240\":5,\"2245\":5,\"2408\":6,\"2411\":5,\"2412\":5,\"2423\":5,\"2431\":5,\"2432\":5,\"2446\":6,\"2447\":5}}],[\"sid\",{\"1\":{\"266\":1,\"267\":6,\"275\":1,\"276\":6,\"285\":1,\"286\":12,\"1552\":2,\"1599\":2,\"1626\":2,\"1992\":2,\"1993\":2,\"1995\":2,\"2235\":2,\"2236\":2,\"2239\":2,\"2240\":2,\"2245\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":2,\"2432\":2,\"2447\":2}}],[\"siddharth\",{\"1\":{\"10\":1,\"12\":1}}],[\"siddhant\",{\"1\":{\"5\":1,\"6\":2,\"12\":1,\"15\":1,\"202\":1,\"244\":1}}],[\"sided\",{\"1\":{\"817\":1}}],[\"side\",{\"1\":{\"3\":1,\"1080\":1,\"1181\":1,\"1368\":1,\"2469\":1}}],[\"sidebar\",{\"1\":{\"3\":2}}],[\"spc\",{\"1\":{\"2490\":2}}],[\"spliced\",{\"1\":{\"2366\":1}}],[\"splicediterableespnetdataset\",{\"0\":{\"2366\":1},\"1\":{\"2366\":1}}],[\"spline\",{\"0\":{\"1635\":1,\"1636\":1},\"1\":{\"1635\":1,\"1636\":1}}],[\"splitted\",{\"1\":{\"2184\":1}}],[\"splitting\",{\"1\":{\"1259\":1,\"1261\":1,\"1883\":1}}],[\"splitjson\",{\"0\":{\"600\":1},\"1\":{\"600\":1}}],[\"splits\",{\"1\":{\"243\":1,\"461\":2,\"1035\":1,\"1113\":1,\"1251\":1}}],[\"split\",{\"0\":{\"461\":1,\"1358\":1,\"2303\":1},\"1\":{\"50\":1,\"102\":1,\"173\":2,\"212\":2,\"242\":1,\"259\":1,\"269\":2,\"278\":2,\"285\":3,\"461\":2,\"600\":1,\"1061\":4,\"1062\":4,\"1141\":1,\"1358\":1,\"2202\":1,\"2213\":1,\"2214\":1,\"2286\":1,\"2303\":1,\"2355\":1}}],[\"spriet\",{\"1\":{\"1330\":1}}],[\"spring2023\",{\"0\":{\"190\":1}}],[\"spsd\",{\"1\":{\"1321\":2,\"1323\":1,\"1327\":2,\"1330\":2}}],[\"spm\",{\"0\":{\"513\":1,\"514\":1,\"2511\":1},\"1\":{\"513\":1,\"514\":1}}],[\"spatio\",{\"1\":{\"1309\":1,\"1310\":1,\"1315\":1}}],[\"spatially\",{\"1\":{\"1330\":1}}],[\"spatialdropout\",{\"0\":{\"832\":1},\"1\":{\"832\":1}}],[\"spatial\",{\"1\":{\"622\":1,\"715\":1,\"783\":1,\"832\":2,\"1279\":2,\"1280\":2,\"1281\":2,\"1282\":2,\"1283\":2}}],[\"spatialized\",{\"1\":{\"227\":2}}],[\"spandh\",{\"1\":{\"1717\":1}}],[\"spans\",{\"1\":{\"846\":3,\"2220\":3}}],[\"span\",{\"1\":{\"846\":1,\"2220\":2}}],[\"spanish\",{\"1\":{\"287\":1,\"481\":1}}],[\"sparse\",{\"1\":{\"787\":1}}],[\"spacing\",{\"1\":{\"620\":1,\"1736\":1}}],[\"space=false\",{\"1\":{\"2277\":2,\"2281\":1}}],[\"spaces\",{\"0\":{\"688\":1,\"689\":1,\"707\":1,\"708\":1,\"718\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"744\":1,\"751\":1,\"757\":1,\"769\":1,\"784\":1,\"788\":1,\"793\":1,\"809\":1,\"811\":1,\"813\":1,\"817\":1,\"821\":1,\"823\":1,\"824\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"835\":1,\"837\":1,\"842\":1,\"844\":1,\"852\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"870\":1,\"871\":1,\"872\":1,\"877\":1,\"892\":1,\"895\":1,\"898\":1,\"903\":1,\"906\":1,\"907\":1,\"912\":1,\"913\":1,\"914\":1,\"924\":1,\"925\":1,\"926\":1,\"928\":1,\"929\":1,\"938\":1,\"939\":1,\"942\":1,\"943\":1,\"944\":1,\"945\":1},\"1\":{\"24\":1,\"247\":1,\"295\":2,\"415\":2,\"688\":1,\"689\":1,\"718\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"744\":1,\"751\":1,\"757\":1,\"769\":1,\"784\":1,\"788\":1,\"793\":1,\"809\":1,\"811\":1,\"813\":1,\"817\":1,\"821\":1,\"823\":1,\"824\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"835\":1,\"837\":1,\"842\":1,\"844\":1,\"852\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"877\":1,\"892\":1,\"895\":1,\"898\":1,\"903\":1,\"906\":1,\"907\":2,\"912\":1,\"913\":1,\"914\":1,\"924\":1,\"925\":1,\"926\":1,\"928\":1,\"929\":1,\"938\":1,\"939\":1,\"942\":1,\"943\":1,\"944\":1,\"945\":1}}],[\"space\",{\"0\":{\"2303\":1},\"1\":{\"19\":1,\"143\":2,\"211\":1,\"218\":2,\"271\":3,\"276\":1,\"280\":3,\"286\":13,\"287\":12,\"481\":6,\"517\":1,\"603\":2,\"625\":3,\"627\":3,\"632\":3,\"674\":4,\"703\":4,\"704\":1,\"717\":3,\"736\":1,\"737\":1,\"740\":3,\"777\":1,\"821\":1,\"824\":1,\"846\":7,\"955\":1,\"977\":1,\"979\":1,\"1008\":1,\"1640\":1,\"1760\":3,\"1779\":3,\"1959\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2134\":2,\"2147\":1,\"2220\":7,\"2221\":2,\"2275\":1,\"2276\":1,\"2277\":1,\"2281\":2,\"2285\":1,\"2293\":1,\"2303\":1,\"2336\":1,\"2337\":1,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2448\":1}}],[\"spoofing\",{\"0\":{\"208\":1},\"1\":{\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"954\":2}}],[\"spoken\",{\"0\":{\"12\":1,\"180\":1,\"186\":1,\"250\":1},\"1\":{\"10\":1,\"12\":1,\"175\":1,\"180\":1,\"190\":1,\"202\":1,\"233\":1,\"245\":1,\"247\":1,\"2044\":6}}],[\"spxx\",{\"1\":{\"200\":1,\"205\":1,\"242\":1}}],[\"sp\",{\"1\":{\"136\":3,\"1211\":1,\"2043\":2}}],[\"sph\",{\"1\":{\"75\":2,\"1678\":1}}],[\"sph2pipe\",{\"1\":{\"75\":3}}],[\"sphere\",{\"0\":{\"75\":1}}],[\"sphinx\",{\"1\":{\"3\":3,\"31\":1,\"33\":1,\"34\":1}}],[\"spicify\",{\"1\":{\"69\":1}}],[\"spktrainer\",{\"0\":{\"2365\":1},\"1\":{\"2269\":1,\"2365\":1}}],[\"spk=0\",{\"1\":{\"1126\":1}}],[\"spk=1\",{\"1\":{\"1061\":1,\"1198\":1,\"1264\":1,\"1334\":1}}],[\"spk=2\",{\"1\":{\"1059\":1}}],[\"spkn\",{\"1\":{\"978\":1,\"1053\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1125\":1,\"1130\":1,\"1136\":1,\"1141\":1,\"1162\":1,\"1217\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1278\":1,\"1280\":1,\"1283\":1}}],[\"spk2enroll\",{\"1\":{\"2368\":1}}],[\"spk2\",{\"1\":{\"978\":1,\"1002\":1,\"1053\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1125\":1,\"1130\":1,\"1136\":1,\"1141\":1,\"1156\":2,\"1162\":1,\"1217\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1278\":1,\"1280\":1,\"1283\":1}}],[\"spk2utt=none\",{\"1\":{\"1728\":1}}],[\"spk2utt\",{\"1\":{\"196\":8,\"201\":1,\"213\":8,\"224\":1,\"268\":8,\"277\":8,\"2364\":3}}],[\"spkrs\",{\"1\":{\"564\":2,\"578\":2,\"583\":2,\"589\":2}}],[\"spks\",{\"1\":{\"267\":4,\"276\":4,\"286\":4,\"1172\":1,\"1526\":1,\"1552\":2,\"1553\":1,\"1598\":1,\"1599\":2,\"1600\":1,\"1625\":1,\"1626\":2,\"1992\":2,\"1993\":2,\"1994\":1,\"1995\":2,\"2235\":2,\"2236\":2,\"2239\":2,\"2240\":2,\"2245\":2,\"2411\":2,\"2412\":2,\"2423\":2,\"2431\":2,\"2432\":2,\"2447\":2}}],[\"spk1\",{\"1\":{\"224\":1,\"252\":1,\"254\":1,\"255\":2,\"978\":1,\"1002\":2,\"1053\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1125\":1,\"1130\":1,\"1136\":1,\"1141\":1,\"1156\":2,\"1162\":1,\"1217\":1,\"1232\":1,\"1252\":1,\"1261\":1,\"1267\":1,\"1278\":1,\"1280\":1,\"1283\":1}}],[\"spkpreprocessor\",{\"0\":{\"2364\":1},\"1\":{\"101\":1,\"2364\":1}}],[\"spk\",{\"0\":{\"449\":1,\"457\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2269\":1,\"2365\":1,\"2544\":1},\"1\":{\"8\":1,\"101\":1,\"162\":1,\"190\":1,\"223\":1,\"224\":2,\"225\":1,\"253\":2,\"254\":6,\"267\":2,\"276\":2,\"285\":6,\"286\":22,\"335\":2,\"402\":1,\"403\":2,\"449\":11,\"457\":1,\"691\":1,\"954\":1,\"966\":1,\"969\":1,\"970\":1,\"974\":5,\"977\":2,\"978\":7,\"979\":3,\"980\":1,\"1002\":1,\"1045\":1,\"1053\":3,\"1061\":2,\"1062\":3,\"1107\":3,\"1117\":3,\"1118\":4,\"1125\":3,\"1126\":3,\"1130\":3,\"1131\":3,\"1132\":1,\"1133\":2,\"1136\":3,\"1141\":3,\"1155\":1,\"1156\":3,\"1157\":4,\"1158\":1,\"1162\":3,\"1167\":1,\"1198\":2,\"1204\":1,\"1209\":1,\"1217\":2,\"1228\":2,\"1232\":3,\"1252\":3,\"1261\":3,\"1264\":1,\"1267\":3,\"1268\":5,\"1269\":1,\"1270\":1,\"1271\":1,\"1278\":3,\"1280\":3,\"1283\":3,\"1319\":2,\"1334\":3,\"1526\":4,\"1552\":4,\"1553\":3,\"1598\":3,\"1599\":6,\"1600\":2,\"1625\":3,\"1626\":4,\"1992\":5,\"1993\":6,\"1994\":2,\"1995\":5,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":5,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":2,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2235\":6,\"2236\":6,\"2239\":6,\"2240\":6,\"2245\":6,\"2269\":1,\"2346\":1,\"2354\":1,\"2365\":1,\"2368\":2,\"2411\":6,\"2412\":6,\"2423\":6,\"2431\":6,\"2432\":6,\"2447\":6}}],[\"spembs\",{\"1\":{\"1521\":6,\"1526\":4,\"1552\":5,\"1553\":4,\"1585\":4,\"1598\":2,\"1599\":5,\"1625\":4,\"1626\":5,\"1975\":1,\"1976\":1,\"1992\":3,\"1993\":5,\"1995\":3,\"2228\":6,\"2229\":6,\"2235\":5,\"2236\":5,\"2239\":5,\"2240\":5,\"2245\":5,\"2408\":6,\"2411\":5,\"2412\":5,\"2423\":5,\"2431\":5,\"2432\":5,\"2446\":6,\"2447\":5}}],[\"speakup\",{\"1\":{\"2065\":2}}],[\"speaking\",{\"1\":{\"247\":1}}],[\"speakertask\",{\"0\":{\"2269\":1},\"1\":{\"2269\":1}}],[\"speakers\",{\"1\":{\"223\":1,\"252\":1,\"267\":2,\"276\":2,\"286\":2,\"978\":2,\"1053\":1,\"1062\":1,\"1107\":1,\"1117\":1,\"1118\":1,\"1125\":1,\"1130\":1,\"1131\":1,\"1133\":1,\"1136\":1,\"1141\":1,\"1155\":2,\"1157\":2,\"1162\":1,\"1228\":1,\"1232\":1,\"1252\":2,\"1261\":1,\"1264\":1,\"1267\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1273\":1,\"1278\":1,\"1280\":1,\"1283\":1,\"1334\":1,\"1552\":1,\"1599\":1,\"1626\":1,\"1992\":1,\"1993\":1,\"1995\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2368\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2447\":1}}],[\"speakerbeam\",{\"0\":{\"1268\":1},\"1\":{\"1268\":3}}],[\"speakerb\",{\"1\":{\"196\":3,\"213\":3,\"268\":3,\"277\":3}}],[\"speakera\",{\"1\":{\"196\":3,\"213\":3,\"268\":3,\"277\":3}}],[\"speaker\",{\"0\":{\"8\":1,\"208\":1,\"221\":1,\"229\":1,\"252\":1},\"1\":{\"8\":1,\"162\":2,\"175\":1,\"190\":2,\"196\":6,\"213\":6,\"217\":1,\"252\":4,\"253\":1,\"254\":4,\"256\":1,\"265\":2,\"266\":1,\"267\":8,\"268\":6,\"274\":2,\"275\":1,\"276\":7,\"277\":6,\"284\":9,\"285\":7,\"286\":36,\"289\":5,\"335\":1,\"377\":1,\"449\":1,\"457\":1,\"511\":2,\"849\":1,\"955\":1,\"974\":1,\"976\":1,\"977\":1,\"979\":1,\"1002\":3,\"1021\":1,\"1117\":1,\"1118\":1,\"1126\":1,\"1130\":1,\"1155\":4,\"1157\":3,\"1158\":3,\"1268\":5,\"1269\":1,\"1270\":1,\"1271\":1,\"1274\":1,\"1521\":6,\"1526\":4,\"1552\":5,\"1553\":4,\"1585\":4,\"1598\":2,\"1599\":6,\"1625\":4,\"1626\":5,\"1860\":1,\"1992\":4,\"1993\":6,\"1995\":4,\"2045\":3,\"2176\":1,\"2183\":1,\"2184\":12,\"2187\":1,\"2191\":1,\"2192\":1,\"2198\":1,\"2203\":1,\"2208\":1,\"2209\":1,\"2228\":6,\"2229\":6,\"2235\":6,\"2236\":6,\"2239\":6,\"2240\":6,\"2245\":6,\"2283\":1,\"2284\":1,\"2337\":1,\"2345\":2,\"2364\":1,\"2365\":1,\"2368\":1,\"2408\":6,\"2411\":6,\"2412\":6,\"2423\":6,\"2431\":6,\"2432\":6,\"2446\":6,\"2447\":6}}],[\"specs\",{\"1\":{\"2134\":2}}],[\"specgram\",{\"1\":{\"1533\":2}}],[\"spectogram\",{\"1\":{\"2428\":1}}],[\"spectogramdenoiser\",{\"0\":{\"2428\":1},\"1\":{\"2428\":1}}],[\"specturm\",{\"1\":{\"1253\":2}}],[\"spectra\",{\"1\":{\"1117\":1}}],[\"spectral\",{\"0\":{\"1326\":1},\"1\":{\"768\":1,\"1125\":1,\"1326\":2,\"1389\":1,\"1390\":1,\"1401\":1,\"1402\":2,\"1408\":2,\"1409\":2,\"1420\":1,\"1467\":1,\"1515\":1,\"1516\":1,\"1526\":2,\"1530\":1,\"1541\":1,\"1543\":1,\"1549\":3,\"1553\":2,\"1593\":1,\"1594\":1,\"1595\":2,\"1596\":5,\"1597\":7,\"1598\":2,\"1600\":2,\"1625\":2,\"1668\":2,\"2458\":1}}],[\"spectrogram2waveform\",{\"0\":{\"2482\":1},\"1\":{\"2482\":1}}],[\"spectrogram\",{\"0\":{\"411\":1,\"1776\":1,\"1791\":1,\"1832\":2,\"1835\":1,\"1836\":1,\"1899\":1,\"1900\":1,\"1923\":2,\"1924\":1,\"1925\":1,\"1978\":1,\"1982\":1,\"2414\":1,\"2418\":1},\"1\":{\"286\":8,\"686\":1,\"778\":1,\"792\":2,\"865\":1,\"1119\":1,\"1413\":1,\"1414\":1,\"1419\":3,\"1529\":1,\"1533\":3,\"1553\":1,\"1598\":1,\"1607\":4,\"1617\":2,\"1625\":1,\"1750\":3,\"1758\":3,\"1776\":1,\"1791\":3,\"1810\":4,\"1811\":3,\"1812\":1,\"1832\":4,\"1835\":1,\"1836\":2,\"1899\":1,\"1900\":1,\"1923\":3,\"1924\":1,\"1925\":1,\"1973\":1,\"1978\":2,\"1979\":1,\"1981\":1,\"1982\":1,\"1983\":1,\"1993\":1,\"2130\":1,\"2133\":1,\"2223\":2,\"2227\":2,\"2231\":2,\"2245\":1,\"2414\":2,\"2418\":1,\"2425\":1,\"2429\":1,\"2431\":2,\"2482\":1,\"2490\":2,\"2495\":2}}],[\"spectrograms\",{\"1\":{\"273\":1,\"1529\":1}}],[\"spectrum\",{\"1\":{\"128\":1,\"203\":1,\"1029\":1,\"1062\":1,\"1112\":1,\"1118\":2,\"1124\":1,\"1155\":1,\"1157\":1,\"1235\":1,\"1250\":2,\"1251\":1,\"1280\":2,\"1281\":1,\"1282\":1,\"1283\":1,\"1607\":1,\"2240\":1}}],[\"spec=none\",{\"1\":{\"1174\":1}}],[\"spec\",{\"0\":{\"1765\":1,\"1831\":1,\"1840\":1,\"1841\":1,\"1884\":1,\"1922\":2,\"1929\":1,\"1930\":1},\"1\":{\"720\":1,\"1119\":3,\"1174\":2,\"1212\":1,\"1250\":5,\"1251\":5,\"1533\":1,\"1607\":2,\"1664\":3,\"1665\":4,\"1691\":4,\"1978\":1,\"1980\":1,\"1982\":1,\"2149\":2,\"2166\":2,\"2414\":1,\"2416\":1,\"2418\":1}}],[\"specaugment\",{\"0\":{\"1831\":1},\"1\":{\"625\":1,\"699\":2,\"833\":1}}],[\"specaug\",{\"0\":{\"686\":2,\"833\":3},\"1\":{\"243\":3,\"625\":2,\"686\":3,\"699\":2,\"736\":1,\"737\":1,\"768\":1,\"777\":1,\"791\":2,\"833\":4,\"954\":1,\"958\":1,\"974\":1,\"1640\":1,\"1641\":1,\"1702\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2184\":1,\"2216\":1,\"2221\":1}}],[\"specially\",{\"1\":{\"269\":1,\"278\":1}}],[\"special\",{\"1\":{\"96\":1,\"197\":1,\"200\":3,\"240\":1,\"242\":7,\"243\":1,\"265\":1,\"269\":3,\"274\":1,\"278\":3,\"287\":1,\"824\":1,\"943\":1,\"2143\":5,\"2355\":1}}],[\"specifier=\",{\"1\":{\"2134\":2}}],[\"specifier\",{\"1\":{\"200\":2,\"2134\":4,\"2149\":3}}],[\"specifiers\",{\"1\":{\"200\":7,\"240\":1}}],[\"specifies\",{\"1\":{\"47\":1,\"74\":1,\"162\":1,\"200\":1,\"786\":1,\"800\":1,\"921\":1,\"935\":1,\"960\":1}}],[\"specified\",{\"0\":{\"118\":2,\"124\":1,\"174\":1},\"1\":{\"3\":1,\"22\":1,\"24\":2,\"26\":1,\"43\":2,\"50\":4,\"51\":1,\"52\":1,\"59\":1,\"68\":1,\"79\":1,\"90\":1,\"118\":1,\"135\":1,\"168\":1,\"174\":3,\"200\":2,\"201\":1,\"223\":3,\"266\":3,\"275\":3,\"276\":2,\"285\":3,\"545\":1,\"614\":1,\"641\":1,\"651\":1,\"663\":1,\"671\":1,\"820\":1,\"828\":1,\"847\":1,\"911\":2,\"1061\":1,\"1062\":1,\"1157\":1,\"1209\":1,\"1279\":2,\"1306\":1,\"1354\":1,\"1371\":1,\"1400\":1,\"1441\":3,\"1454\":1,\"1456\":1,\"1469\":2,\"1582\":1,\"1624\":1,\"1665\":1,\"1677\":1,\"1691\":1,\"1749\":1,\"1815\":1,\"1843\":1,\"1892\":1,\"2134\":1,\"2145\":1,\"2304\":1,\"2354\":1,\"2355\":2}}],[\"specifically\",{\"1\":{\"240\":1,\"785\":1,\"1270\":1,\"1271\":1,\"2345\":1,\"2357\":1}}],[\"specification\",{\"1\":{\"41\":1,\"43\":1}}],[\"specific\",{\"1\":{\"44\":1,\"45\":1,\"47\":1,\"70\":1,\"81\":2,\"127\":1,\"128\":1,\"145\":1,\"162\":4,\"163\":1,\"168\":2,\"243\":1,\"247\":1,\"276\":1,\"286\":1,\"747\":1,\"846\":1,\"911\":1,\"1053\":1,\"1164\":2,\"1655\":1,\"1683\":1,\"2130\":4,\"2134\":1,\"2142\":1,\"2184\":1,\"2240\":1,\"2249\":1,\"2262\":2,\"2344\":1,\"2345\":1,\"2354\":1,\"2355\":1}}],[\"specifying\",{\"1\":{\"43\":1,\"110\":2,\"175\":1,\"196\":1,\"223\":2,\"268\":2,\"277\":2,\"286\":1,\"2130\":1,\"2286\":1,\"2355\":2}}],[\"specify\",{\"1\":{\"41\":1,\"48\":1,\"50\":1,\"67\":1,\"69\":1,\"79\":1,\"93\":1,\"99\":1,\"100\":1,\"118\":1,\"127\":1,\"128\":2,\"132\":1,\"162\":3,\"195\":1,\"196\":1,\"200\":3,\"212\":1,\"217\":1,\"223\":2,\"266\":3,\"268\":1,\"269\":1,\"275\":3,\"277\":1,\"278\":1,\"285\":4,\"286\":3,\"521\":1,\"524\":1,\"525\":1,\"536\":2,\"1883\":1,\"2355\":1}}],[\"speedperturbation\",{\"0\":{\"1833\":1},\"1\":{\"1833\":2}}],[\"speedup\",{\"1\":{\"852\":1}}],[\"speeds\",{\"1\":{\"535\":1,\"1270\":1}}],[\"speed\",{\"0\":{\"535\":1,\"1697\":1},\"1\":{\"41\":1,\"45\":1,\"53\":1,\"145\":1,\"173\":1,\"197\":1,\"199\":1,\"200\":3,\"204\":1,\"205\":3,\"223\":2,\"234\":1,\"235\":3,\"240\":1,\"242\":3,\"253\":1,\"254\":2,\"262\":1,\"484\":2,\"490\":2,\"535\":4,\"821\":2,\"1246\":1,\"1526\":1,\"1552\":1,\"1553\":1,\"1625\":1,\"1626\":1,\"1655\":2,\"1697\":4,\"1698\":4,\"1788\":1,\"1833\":6,\"2240\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2447\":1}}],[\"speechlmpreprocessor\",{\"0\":{\"2143\":1},\"1\":{\"2143\":1}}],[\"speechlmjobtemplate\",{\"0\":{\"2142\":1},\"1\":{\"2142\":1}}],[\"speechlm\",{\"0\":{\"2130\":2,\"2131\":1,\"2132\":1,\"2133\":2,\"2134\":1,\"2135\":1,\"2136\":2,\"2137\":2,\"2138\":2,\"2139\":1,\"2140\":2,\"2141\":1,\"2142\":3,\"2143\":3,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2148\":2,\"2149\":1,\"2150\":1,\"2151\":1,\"2152\":1,\"2153\":1,\"2154\":1,\"2155\":1,\"2156\":1,\"2157\":1,\"2158\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2162\":1,\"2163\":1,\"2164\":1,\"2165\":1,\"2166\":1,\"2543\":1},\"1\":{\"2130\":2,\"2131\":2,\"2132\":1,\"2133\":2,\"2134\":2,\"2136\":2,\"2137\":2,\"2138\":2,\"2139\":1,\"2140\":2,\"2141\":1,\"2142\":9,\"2143\":4,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2148\":2,\"2149\":1,\"2150\":1,\"2151\":1,\"2152\":1,\"2153\":1,\"2154\":1,\"2155\":1,\"2156\":1,\"2157\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2162\":1,\"2163\":1,\"2164\":1,\"2166\":1,\"2344\":1,\"2345\":1}}],[\"speechlm1\",{\"1\":{\"251\":1,\"2344\":1}}],[\"speeches\",{\"1\":{\"1329\":1}}],[\"speech=none\",{\"1\":{\"760\":1,\"850\":1}}],[\"speechbleu\",{\"1\":{\"286\":2}}],[\"speechbertscore\",{\"1\":{\"286\":2}}],[\"speechbrain\",{\"1\":{\"285\":1}}],[\"speech2text\",{\"1\":{\"263\":1}}],[\"speechflow\",{\"1\":{\"232\":1,\"258\":1}}],[\"speechprompt\",{\"1\":{\"201\":1}}],[\"speechprocessing\",{\"0\":{\"190\":1}}],[\"speechcommands\",{\"1\":{\"201\":1}}],[\"speechrecognition\",{\"0\":{\"191\":1,\"192\":1}}],[\"speech\",{\"0\":{\"10\":1,\"11\":1,\"15\":1,\"178\":1,\"179\":1,\"181\":1,\"184\":1,\"185\":1,\"187\":1,\"198\":1,\"203\":1,\"215\":1,\"222\":1,\"226\":2,\"229\":1,\"230\":2,\"239\":2,\"240\":1,\"251\":1,\"260\":1,\"283\":1,\"291\":1,\"292\":1,\"340\":1},\"1\":{\"6\":2,\"9\":1,\"10\":1,\"11\":3,\"15\":1,\"79\":4,\"81\":1,\"82\":5,\"126\":1,\"134\":3,\"135\":1,\"136\":1,\"156\":1,\"162\":3,\"179\":1,\"190\":5,\"193\":5,\"200\":1,\"201\":2,\"202\":1,\"205\":1,\"207\":1,\"218\":1,\"222\":3,\"223\":18,\"224\":2,\"226\":2,\"228\":2,\"240\":1,\"242\":1,\"243\":5,\"246\":3,\"247\":8,\"267\":1,\"276\":1,\"284\":2,\"285\":1,\"286\":8,\"287\":2,\"290\":5,\"536\":4,\"543\":1,\"625\":19,\"639\":5,\"691\":1,\"736\":4,\"760\":1,\"768\":2,\"777\":4,\"790\":1,\"791\":1,\"815\":1,\"833\":1,\"850\":9,\"864\":1,\"954\":9,\"958\":15,\"974\":11,\"1061\":3,\"1062\":4,\"1066\":1,\"1112\":1,\"1113\":2,\"1125\":1,\"1126\":4,\"1130\":1,\"1131\":1,\"1155\":16,\"1156\":19,\"1157\":19,\"1158\":14,\"1172\":1,\"1185\":1,\"1210\":1,\"1217\":2,\"1223\":1,\"1250\":1,\"1251\":3,\"1252\":1,\"1264\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1279\":2,\"1280\":3,\"1281\":3,\"1282\":3,\"1283\":2,\"1309\":1,\"1310\":1,\"1311\":3,\"1318\":4,\"1321\":1,\"1322\":3,\"1323\":1,\"1327\":10,\"1328\":4,\"1330\":12,\"1334\":1,\"1354\":8,\"1546\":1,\"1552\":1,\"1553\":1,\"1585\":13,\"1598\":9,\"1600\":8,\"1611\":1,\"1612\":1,\"1616\":1,\"1622\":1,\"1625\":10,\"1626\":2,\"1640\":10,\"1641\":10,\"1667\":6,\"1674\":2,\"1702\":9,\"1708\":1,\"1709\":1,\"1710\":1,\"1717\":1,\"1719\":5,\"1720\":2,\"1721\":1,\"1725\":6,\"1729\":1,\"1730\":2,\"1752\":1,\"1770\":1,\"1788\":2,\"1795\":1,\"1806\":3,\"1862\":1,\"1880\":1,\"1965\":4,\"1971\":2,\"1975\":14,\"1976\":4,\"1992\":4,\"1993\":4,\"1994\":4,\"1995\":4,\"1996\":9,\"1997\":14,\"2000\":1,\"2001\":1,\"2034\":1,\"2040\":1,\"2043\":1,\"2044\":3,\"2045\":2,\"2054\":2,\"2065\":15,\"2127\":10,\"2132\":1,\"2141\":1,\"2142\":1,\"2168\":1,\"2184\":11,\"2216\":6,\"2217\":2,\"2221\":10,\"2256\":1,\"2262\":5,\"2336\":3,\"2337\":3,\"2341\":4,\"2345\":1,\"2346\":7,\"2350\":2,\"2356\":3,\"2360\":5,\"2361\":6,\"2362\":3,\"2368\":6,\"2403\":2,\"2404\":1,\"2408\":16,\"2411\":2,\"2412\":2,\"2423\":1,\"2425\":3,\"2429\":3,\"2430\":1,\"2431\":1,\"2432\":2,\"2433\":1,\"2445\":2,\"2446\":28,\"2447\":2,\"2462\":8}}],[\"semodule\",{\"0\":{\"2200\":1},\"1\":{\"2200\":1}}],[\"semantic\",{\"1\":{\"2130\":2,\"2136\":3}}],[\"semi\",{\"1\":{\"795\":1}}],[\"semitone\",{\"1\":{\"267\":5,\"276\":5}}],[\"seki\",{\"1\":{\"1730\":1}}],[\"seanetresnetblock2d\",{\"0\":{\"1460\":1},\"1\":{\"1460\":1}}],[\"seanetresnetblock\",{\"0\":{\"1458\":1},\"1\":{\"1458\":1}}],[\"seanetencoder2d\",{\"0\":{\"1456\":1},\"1\":{\"1456\":1}}],[\"seanetencoder\",{\"0\":{\"1454\":1},\"1\":{\"1454\":1}}],[\"seanetdecoder2d\",{\"0\":{\"1452\":1},\"1\":{\"1452\":1}}],[\"seanetdecoder\",{\"0\":{\"1450\":1},\"1\":{\"1450\":1}}],[\"seanet\",{\"0\":{\"1387\":1,\"1424\":1,\"1428\":1,\"1430\":1,\"1437\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1476\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1493\":1,\"1494\":1,\"1503\":1,\"1505\":1,\"1506\":1},\"1\":{\"1387\":1,\"1424\":1,\"1428\":1,\"1430\":1,\"1437\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1458\":2,\"1460\":2,\"1462\":1,\"1476\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1493\":1,\"1494\":1,\"1503\":1,\"1505\":1,\"1506\":1}}],[\"searchable\",{\"1\":{\"692\":1,\"850\":2}}],[\"searches\",{\"1\":{\"262\":1}}],[\"search\",{\"0\":{\"306\":1,\"319\":1,\"325\":1,\"394\":1,\"400\":1,\"412\":1,\"426\":1,\"434\":1,\"441\":1,\"447\":1,\"467\":1,\"473\":1,\"503\":1,\"509\":1,\"616\":1,\"628\":1,\"631\":1,\"696\":1,\"697\":1,\"743\":1,\"763\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1732\":1,\"1806\":1,\"1807\":1,\"1862\":2},\"1\":{\"45\":21,\"125\":1,\"129\":1,\"139\":1,\"145\":12,\"148\":1,\"175\":2,\"262\":1,\"290\":1,\"321\":2,\"616\":13,\"628\":2,\"631\":2,\"696\":22,\"697\":20,\"743\":2,\"763\":2,\"1719\":13,\"1720\":5,\"1721\":8,\"1722\":1,\"1723\":1,\"1725\":14,\"1726\":4,\"1727\":4,\"1730\":1,\"1731\":2,\"1732\":1,\"1762\":1,\"1775\":1,\"1787\":2,\"1804\":1,\"1805\":1,\"1806\":13,\"1807\":1,\"1821\":1,\"1822\":3,\"1862\":6,\"1966\":2,\"2311\":1}}],[\"session\",{\"1\":{\"1011\":1}}],[\"seglstm\",{\"0\":{\"1255\":1},\"1\":{\"1202\":1,\"1255\":1,\"1259\":1,\"1261\":1}}],[\"seg2\",{\"1\":{\"1164\":1}}],[\"seg1\",{\"1\":{\"1164\":1}}],[\"seg\",{\"1\":{\"1164\":1,\"1255\":1,\"1259\":2,\"1261\":2,\"2363\":1}}],[\"segement\",{\"1\":{\"704\":1}}],[\"segmenter\",{\"0\":{\"2455\":2,\"2463\":2,\"2464\":2},\"1\":{\"2455\":2,\"2462\":2,\"2463\":2,\"2464\":2}}],[\"segmented\",{\"1\":{\"1632\":1,\"1633\":1}}],[\"segment=false\",{\"1\":{\"1735\":2}}],[\"segmenting\",{\"1\":{\"1164\":2}}],[\"segment\",{\"1\":{\"73\":2,\"269\":1,\"278\":1,\"335\":4,\"342\":4,\"361\":4,\"1059\":1,\"1136\":3,\"1141\":3,\"1162\":3,\"1164\":1,\"1252\":3,\"1259\":2,\"1261\":2,\"1279\":5,\"1280\":5,\"1281\":5,\"1283\":5,\"1356\":1,\"1358\":1,\"1509\":1,\"1511\":1,\"1526\":3,\"1543\":1,\"1545\":1,\"1552\":4,\"1553\":1,\"1598\":1,\"1599\":4,\"1600\":3,\"1625\":1,\"1626\":4,\"1632\":4,\"1633\":4,\"2065\":4,\"2156\":1,\"2346\":2,\"2368\":3,\"2435\":1,\"2455\":2,\"2463\":2,\"2464\":2}}],[\"segments=none\",{\"1\":{\"1780\":1}}],[\"segments\",{\"0\":{\"1632\":2,\"1633\":2,\"2156\":1},\"1\":{\"73\":5,\"196\":5,\"235\":2,\"268\":5,\"269\":4,\"277\":5,\"278\":4,\"548\":2,\"551\":2,\"606\":1,\"1011\":2,\"1259\":1,\"1261\":1,\"1279\":2,\"1280\":1,\"1281\":2,\"1283\":1,\"1552\":1,\"1599\":1,\"1626\":1,\"1632\":3,\"1633\":3,\"1881\":1,\"2065\":2,\"2156\":5,\"2157\":4,\"2238\":1}}],[\"segmentations\",{\"0\":{\"73\":1}}],[\"segmentation\",{\"0\":{\"299\":1,\"419\":1},\"1\":{\"71\":1,\"265\":1,\"269\":3,\"274\":1,\"278\":3,\"1131\":1,\"1172\":1,\"1259\":2,\"1261\":2}}],[\"sep\",{\"1\":{\"1396\":1,\"1422\":2,\"2280\":1}}],[\"seperated\",{\"1\":{\"780\":1}}],[\"seperate\",{\"1\":{\"777\":1,\"1156\":1,\"1940\":1,\"1942\":1}}],[\"separable\",{\"1\":{\"768\":1,\"1265\":1,\"1301\":1,\"1306\":1,\"1371\":1,\"1372\":1,\"1583\":1}}],[\"separators\",{\"1\":{\"223\":1,\"225\":1}}],[\"separator\",{\"0\":{\"980\":2,\"1044\":2,\"1046\":2,\"1048\":2,\"1053\":1,\"1062\":2,\"1107\":2,\"1117\":2,\"1118\":2,\"1125\":2,\"1130\":2,\"1131\":2,\"1136\":2,\"1141\":2,\"1142\":2,\"1159\":2,\"1162\":2,\"1182\":2,\"1183\":2,\"1184\":2,\"1190\":2,\"1192\":2,\"1194\":2,\"1217\":1,\"1232\":2,\"1252\":2,\"1261\":2,\"1267\":2,\"1269\":2,\"1270\":2,\"1271\":2,\"1278\":2,\"1280\":2,\"1283\":2,\"1334\":2,\"1350\":2},\"1\":{\"223\":17,\"224\":2,\"225\":15,\"287\":3,\"980\":6,\"1044\":2,\"1046\":2,\"1048\":2,\"1053\":1,\"1062\":3,\"1107\":3,\"1117\":3,\"1118\":3,\"1125\":4,\"1130\":3,\"1131\":3,\"1136\":3,\"1141\":3,\"1142\":2,\"1155\":2,\"1157\":3,\"1159\":2,\"1162\":3,\"1182\":2,\"1183\":2,\"1184\":2,\"1190\":2,\"1192\":2,\"1194\":2,\"1217\":1,\"1232\":3,\"1252\":2,\"1261\":3,\"1267\":3,\"1269\":2,\"1270\":2,\"1271\":2,\"1278\":3,\"1280\":2,\"1283\":2,\"1334\":2,\"1350\":2,\"2286\":3}}],[\"separation\",{\"0\":{\"179\":1,\"520\":1},\"1\":{\"11\":1,\"84\":1,\"179\":1,\"190\":1,\"223\":5,\"520\":3,\"691\":1,\"1066\":1,\"1112\":1,\"1113\":1,\"1117\":1,\"1130\":1,\"1131\":1,\"1155\":2,\"1157\":3,\"1172\":1,\"1185\":1,\"1246\":1,\"1250\":1,\"1251\":1,\"1252\":2,\"1269\":2,\"1270\":2,\"1271\":2,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1}}],[\"separately\",{\"1\":{\"1334\":2,\"2355\":1}}],[\"separate\",{\"1\":{\"96\":1,\"246\":1,\"247\":1,\"271\":1,\"280\":1,\"287\":2,\"290\":1,\"756\":1,\"760\":2,\"773\":1,\"821\":1,\"1155\":1,\"1157\":1,\"1422\":1}}],[\"separatespeech\",{\"0\":{\"348\":1,\"355\":1,\"367\":1}}],[\"separates\",{\"1\":{\"3\":1}}],[\"separated\",{\"1\":{\"3\":1,\"43\":1,\"50\":1,\"51\":1,\"79\":1,\"211\":1,\"223\":1,\"1162\":1,\"2134\":2}}],[\"seprated\",{\"1\":{\"276\":1}}],[\"serializable\",{\"1\":{\"2477\":1}}],[\"serializer\",{\"1\":{\"1778\":1}}],[\"serialize\",{\"1\":{\"1778\":1}}],[\"serialiterator\",{\"0\":{\"1823\":1},\"1\":{\"1823\":2}}],[\"serizel\",{\"1\":{\"1327\":1,\"1330\":1}}],[\"series\",{\"1\":{\"202\":1,\"1655\":2}}],[\"server\",{\"1\":{\"287\":2}}],[\"serves\",{\"1\":{\"232\":1,\"258\":1,\"2262\":1,\"2344\":1}}],[\"sends\",{\"1\":{\"2309\":1}}],[\"send\",{\"1\":{\"1931\":1}}],[\"sense\",{\"1\":{\"1444\":1,\"1448\":1}}],[\"sensitive\",{\"1\":{\"175\":1,\"211\":1}}],[\"sent\",{\"1\":{\"663\":1,\"1031\":1,\"1112\":1,\"1250\":1}}],[\"sentencepiece\",{\"0\":{\"2288\":1},\"1\":{\"243\":1,\"513\":1,\"514\":1,\"2288\":1}}],[\"sentencepiecestokenizer\",{\"0\":{\"2288\":1},\"1\":{\"2288\":1}}],[\"sentencepieces\",{\"1\":{\"106\":1}}],[\"sentence\",{\"1\":{\"136\":1,\"240\":1,\"242\":3,\"243\":1,\"269\":1,\"278\":1,\"514\":2,\"627\":4,\"691\":1,\"740\":4,\"1760\":1,\"1943\":1}}],[\"seqlen\",{\"1\":{\"1760\":4,\"1782\":2}}],[\"seqlength\",{\"1\":{\"786\":1,\"800\":1,\"867\":1,\"921\":1,\"935\":1}}],[\"sequeunce\",{\"1\":{\"1758\":1,\"2231\":1}}],[\"sequentialrnnlm\",{\"0\":{\"1945\":1},\"1\":{\"1822\":1,\"1945\":1}}],[\"sequential\",{\"1\":{\"82\":1,\"768\":1,\"1623\":1,\"1719\":5,\"1720\":1,\"1725\":6,\"1751\":2,\"1796\":2,\"1863\":1,\"1866\":1,\"1945\":1,\"2134\":1}}],[\"sequenceiterfactory\",{\"0\":{\"1650\":1},\"1\":{\"1650\":1}}],[\"sequenceidentity\",{\"0\":{\"827\":1},\"1\":{\"827\":2,\"829\":1}}],[\"sequenceresidualblock\",{\"0\":{\"830\":1},\"1\":{\"830\":2}}],[\"sequencemodel\",{\"0\":{\"828\":1},\"1\":{\"828\":2,\"912\":1}}],[\"sequencemodule\",{\"0\":{\"829\":1},\"1\":{\"724\":2,\"725\":2,\"726\":1,\"727\":1,\"728\":2,\"729\":2,\"744\":1,\"784\":2,\"827\":2,\"828\":1,\"829\":4,\"830\":2,\"858\":1,\"859\":2}}],[\"sequence=\",{\"1\":{\"82\":1}}],[\"sequences\",{\"1\":{\"48\":2,\"614\":4,\"615\":2,\"616\":3,\"617\":9,\"618\":9,\"619\":2,\"620\":14,\"621\":5,\"622\":2,\"623\":2,\"624\":9,\"625\":15,\"626\":2,\"627\":10,\"630\":2,\"632\":4,\"633\":2,\"634\":6,\"636\":6,\"637\":2,\"638\":2,\"640\":2,\"641\":7,\"642\":2,\"643\":6,\"645\":3,\"648\":2,\"649\":2,\"651\":4,\"667\":2,\"696\":1,\"697\":1,\"706\":5,\"740\":10,\"847\":10,\"1019\":1,\"1024\":1,\"1629\":1,\"1730\":2,\"1736\":8,\"1749\":11,\"1750\":5,\"1753\":2,\"1758\":2,\"1779\":4,\"1788\":1,\"1810\":1,\"1814\":3,\"1815\":10,\"1816\":3,\"1839\":1,\"1843\":4,\"1847\":6,\"1851\":7,\"1858\":1,\"1888\":2,\"1907\":1,\"1910\":2,\"1944\":1,\"1947\":1,\"1991\":1,\"1993\":1,\"2007\":1,\"2136\":1,\"2143\":3,\"2155\":7,\"2215\":2,\"2219\":2,\"2223\":2,\"2224\":1,\"2227\":1,\"2231\":2,\"2239\":1,\"2240\":1,\"2245\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2427\":2,\"2431\":1,\"2432\":1,\"2433\":2,\"2447\":1}}],[\"sequence\",{\"0\":{\"1019\":1,\"1574\":1,\"1650\":1,\"1910\":1},\"1\":{\"45\":1,\"80\":1,\"82\":2,\"97\":1,\"101\":1,\"145\":1,\"202\":1,\"275\":1,\"276\":6,\"287\":2,\"515\":1,\"536\":1,\"614\":4,\"616\":6,\"617\":1,\"618\":1,\"620\":1,\"624\":1,\"628\":1,\"631\":2,\"633\":2,\"634\":4,\"637\":4,\"639\":6,\"641\":4,\"643\":5,\"644\":2,\"646\":1,\"647\":2,\"651\":4,\"670\":2,\"696\":8,\"697\":7,\"703\":7,\"706\":4,\"717\":2,\"724\":3,\"725\":3,\"726\":2,\"727\":2,\"728\":3,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"744\":3,\"748\":1,\"755\":5,\"766\":1,\"767\":1,\"770\":1,\"784\":1,\"785\":5,\"786\":2,\"798\":1,\"800\":2,\"817\":1,\"818\":1,\"820\":1,\"822\":1,\"828\":6,\"829\":9,\"830\":5,\"833\":3,\"847\":5,\"859\":2,\"867\":2,\"878\":4,\"879\":4,\"881\":5,\"882\":4,\"883\":4,\"884\":5,\"919\":4,\"921\":1,\"922\":3,\"935\":1,\"936\":3,\"937\":3,\"942\":1,\"943\":1,\"959\":1,\"984\":1,\"1019\":1,\"1020\":1,\"1025\":1,\"1029\":2,\"1126\":1,\"1127\":1,\"1176\":1,\"1235\":2,\"1294\":1,\"1296\":1,\"1359\":1,\"1397\":3,\"1422\":3,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1521\":2,\"1552\":1,\"1556\":6,\"1574\":1,\"1599\":2,\"1626\":1,\"1643\":3,\"1645\":2,\"1646\":3,\"1650\":3,\"1664\":1,\"1665\":1,\"1691\":1,\"1708\":2,\"1709\":2,\"1719\":4,\"1721\":2,\"1725\":6,\"1729\":1,\"1730\":1,\"1731\":1,\"1736\":3,\"1749\":2,\"1750\":6,\"1751\":1,\"1758\":4,\"1782\":1,\"1815\":5,\"1822\":2,\"1843\":3,\"1847\":3,\"1862\":2,\"1868\":1,\"1870\":1,\"1897\":2,\"1907\":1,\"1910\":2,\"1915\":1,\"1919\":1,\"1927\":1,\"1949\":2,\"1992\":2,\"1993\":9,\"1995\":2,\"2002\":1,\"2007\":1,\"2130\":6,\"2143\":1,\"2155\":3,\"2183\":1,\"2184\":1,\"2190\":1,\"2208\":1,\"2223\":2,\"2224\":4,\"2227\":3,\"2228\":2,\"2229\":2,\"2231\":4,\"2235\":3,\"2236\":3,\"2239\":5,\"2240\":5,\"2245\":7,\"2249\":3,\"2262\":1,\"2298\":3,\"2335\":1,\"2339\":4,\"2347\":2,\"2348\":4,\"2350\":1,\"2369\":6,\"2370\":8,\"2371\":2,\"2372\":4,\"2374\":1,\"2376\":1,\"2411\":7,\"2412\":9,\"2423\":9,\"2425\":2,\"2429\":2,\"2431\":9,\"2432\":11,\"2443\":2,\"2447\":6,\"2449\":1}}],[\"seq=true\",{\"1\":{\"1452\":1,\"1456\":1}}],[\"seq2seq\",{\"1\":{\"82\":1,\"96\":1,\"1855\":1,\"2006\":1}}],[\"seqs\",{\"1\":{\"48\":3,\"1760\":2}}],[\"seq\",{\"0\":{\"1945\":1},\"1\":{\"48\":5,\"98\":1,\"301\":2,\"421\":2,\"616\":2,\"674\":2,\"750\":2,\"795\":1,\"1133\":1,\"1134\":1,\"1137\":1,\"1164\":1,\"1208\":1,\"1255\":1,\"1257\":1,\"1294\":1,\"1359\":1,\"1665\":1,\"1806\":1,\"1822\":1,\"1945\":1,\"2124\":1,\"2128\":1,\"2130\":5,\"2133\":2,\"2136\":2,\"2137\":3,\"2155\":3,\"2183\":1,\"2190\":1,\"2208\":1}}],[\"seen=none\",{\"1\":{\"2489\":1}}],[\"seen\",{\"1\":{\"626\":1}}],[\"seeing\",{\"0\":{\"112\":1}}],[\"seem\",{\"1\":{\"96\":1,\"242\":1}}],[\"seed=none\",{\"1\":{\"1717\":1,\"1802\":1,\"1833\":1,\"1852\":1}}],[\"seed=0\",{\"1\":{\"1651\":1}}],[\"seeds\",{\"1\":{\"242\":1,\"2134\":1}}],[\"seed\",{\"0\":{\"2321\":2},\"1\":{\"91\":1,\"104\":2,\"243\":1,\"290\":1,\"301\":2,\"309\":2,\"315\":2,\"321\":2,\"327\":2,\"331\":2,\"335\":2,\"342\":2,\"349\":2,\"361\":2,\"368\":4,\"377\":2,\"385\":2,\"389\":2,\"396\":2,\"404\":2,\"406\":4,\"421\":2,\"429\":2,\"436\":2,\"442\":2,\"449\":2,\"457\":2,\"463\":2,\"469\":2,\"475\":2,\"484\":4,\"490\":4,\"498\":2,\"505\":2,\"1643\":1,\"1645\":4,\"1646\":1,\"1648\":1,\"1649\":1,\"1650\":2,\"1651\":1,\"1998\":1,\"2000\":1,\"2001\":1,\"2134\":3,\"2249\":2,\"2253\":2,\"2321\":3,\"2339\":2,\"2348\":1,\"2370\":2,\"2372\":1}}],[\"see\",{\"1\":{\"48\":1,\"60\":1,\"67\":1,\"69\":1,\"81\":1,\"92\":1,\"93\":1,\"97\":1,\"104\":1,\"110\":1,\"119\":1,\"120\":1,\"121\":1,\"125\":1,\"127\":1,\"133\":1,\"135\":1,\"139\":1,\"143\":1,\"147\":2,\"148\":1,\"160\":1,\"162\":1,\"175\":1,\"195\":1,\"196\":1,\"197\":2,\"200\":5,\"205\":5,\"206\":1,\"211\":2,\"212\":1,\"213\":1,\"217\":4,\"218\":1,\"220\":1,\"223\":1,\"224\":1,\"242\":5,\"254\":2,\"255\":1,\"266\":5,\"267\":4,\"268\":2,\"270\":1,\"271\":1,\"275\":6,\"276\":2,\"277\":2,\"279\":1,\"280\":1,\"285\":4,\"286\":2,\"287\":1,\"288\":1,\"290\":10,\"617\":2,\"618\":2,\"620\":2,\"624\":2,\"626\":2,\"636\":2,\"645\":2,\"661\":2,\"669\":1,\"705\":1,\"756\":2,\"773\":2,\"787\":1,\"804\":2,\"819\":1,\"829\":1,\"837\":1,\"927\":1,\"974\":1,\"1002\":1,\"1155\":2,\"1156\":1,\"1157\":2,\"1158\":1,\"1168\":1,\"1268\":1,\"1274\":1,\"1533\":1,\"1608\":1,\"1654\":1,\"1668\":3,\"1678\":3,\"1683\":1,\"1730\":1,\"1784\":2,\"1786\":1,\"1812\":1,\"1818\":1,\"1820\":1,\"1883\":1,\"1901\":1,\"1903\":1,\"1945\":1,\"1963\":1,\"2286\":1,\"2344\":1,\"2355\":2,\"2367\":1,\"2435\":1}}],[\"secs\",{\"1\":{\"286\":3}}],[\"secondary\",{\"1\":{\"637\":1,\"846\":2,\"2220\":2}}],[\"seconds\",{\"1\":{\"113\":1,\"2150\":2,\"2353\":1,\"2364\":1}}],[\"second\",{\"1\":{\"73\":1,\"80\":1,\"97\":2,\"98\":1,\"138\":2,\"141\":1,\"168\":1,\"267\":5,\"269\":1,\"276\":3,\"278\":1,\"665\":1,\"852\":1,\"856\":1,\"1279\":1,\"1280\":2,\"1281\":1,\"1283\":2,\"1444\":1,\"1448\":1,\"1597\":1,\"1604\":1,\"1606\":1,\"1655\":1,\"1680\":3,\"1692\":3,\"1698\":3,\"1737\":1,\"2355\":1}}],[\"sec\",{\"1\":{\"39\":1,\"136\":2,\"449\":2,\"537\":2,\"1820\":1}}],[\"sections\",{\"1\":{\"138\":1,\"140\":1,\"243\":1,\"269\":1,\"278\":1}}],[\"section\",{\"1\":{\"3\":1,\"84\":1,\"133\":1,\"139\":1,\"143\":1,\"147\":2,\"150\":1,\"195\":1,\"196\":1,\"197\":1,\"262\":1,\"268\":1,\"277\":1,\"1124\":1,\"1180\":1,\"1181\":1,\"2355\":2}}],[\"several\",{\"1\":{\"38\":1,\"39\":1,\"43\":1,\"46\":1,\"102\":2,\"119\":1,\"128\":1,\"163\":1,\"223\":1,\"243\":1,\"262\":2,\"286\":1,\"290\":1,\"1356\":1}}],[\"selector\",{\"0\":{\"1734\":1},\"1\":{\"1734\":1}}],[\"selective\",{\"1\":{\"2203\":1}}],[\"selecting\",{\"1\":{\"246\":1,\"1356\":1}}],[\"selection\",{\"1\":{\"45\":1,\"145\":1,\"147\":2,\"616\":1,\"661\":1,\"674\":6,\"696\":1,\"697\":1,\"747\":2,\"846\":4,\"2044\":10,\"2220\":4}}],[\"selects\",{\"1\":{\"84\":1}}],[\"selected\",{\"1\":{\"43\":1,\"44\":1,\"46\":1,\"67\":1,\"127\":1,\"175\":1,\"616\":1,\"939\":1,\"978\":1,\"1000\":1,\"1400\":1,\"1441\":1,\"1469\":1,\"1717\":1,\"1730\":2,\"1920\":1,\"2044\":14}}],[\"select\",{\"0\":{\"166\":1,\"1920\":1,\"1921\":1},\"1\":{\"19\":1,\"37\":1,\"71\":1,\"132\":1,\"161\":1,\"247\":3,\"267\":1,\"285\":1,\"449\":2,\"614\":1,\"616\":1,\"634\":2,\"641\":1,\"643\":2,\"651\":1,\"847\":1,\"1029\":1,\"1107\":1,\"1117\":2,\"1130\":2,\"1131\":2,\"1134\":1,\"1136\":2,\"1137\":1,\"1139\":1,\"1141\":2,\"1162\":1,\"1185\":1,\"1210\":1,\"1232\":2,\"1235\":1,\"1257\":1,\"1261\":1,\"1267\":1,\"1268\":2,\"1278\":1,\"1279\":1,\"1280\":3,\"1281\":1,\"1282\":1,\"1283\":2,\"1691\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1730\":2,\"1731\":4,\"1734\":1,\"1749\":1,\"1799\":2,\"1815\":1,\"1822\":4,\"1843\":1,\"1920\":1,\"1921\":1,\"1966\":4}}],[\"selfattention\",{\"0\":{\"649\":1},\"1\":{\"642\":1,\"643\":1,\"649\":5,\"709\":1,\"710\":1,\"774\":1,\"780\":1,\"2126\":1,\"2191\":1}}],[\"selfattn\",{\"1\":{\"243\":1,\"700\":1,\"709\":1,\"710\":1,\"733\":1,\"734\":1,\"774\":1,\"780\":1,\"1107\":1,\"1519\":1,\"1526\":1,\"1535\":1,\"1536\":2,\"1546\":1,\"1552\":1,\"1553\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1622\":1,\"1625\":1,\"1626\":1,\"1994\":1,\"2126\":1,\"2191\":1,\"2239\":2,\"2240\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2447\":1}}],[\"self\",{\"0\":{\"7\":1,\"128\":1,\"231\":1,\"257\":1},\"1\":{\"7\":1,\"8\":1,\"78\":1,\"106\":1,\"128\":1,\"190\":1,\"205\":1,\"207\":2,\"232\":1,\"243\":1,\"258\":1,\"262\":1,\"273\":1,\"276\":1,\"281\":1,\"617\":4,\"618\":4,\"624\":4,\"692\":3,\"696\":3,\"701\":1,\"724\":2,\"725\":2,\"726\":2,\"727\":2,\"728\":2,\"731\":1,\"732\":1,\"735\":1,\"744\":2,\"749\":1,\"766\":1,\"767\":1,\"774\":3,\"775\":1,\"787\":1,\"790\":1,\"819\":1,\"824\":2,\"828\":3,\"829\":4,\"830\":3,\"846\":3,\"848\":1,\"850\":2,\"851\":2,\"859\":2,\"960\":1,\"1055\":1,\"1057\":1,\"1063\":2,\"1070\":2,\"1071\":2,\"1107\":2,\"1126\":1,\"1245\":1,\"1250\":1,\"1269\":1,\"1270\":2,\"1271\":1,\"1274\":2,\"1280\":1,\"1290\":1,\"1519\":3,\"1526\":1,\"1535\":3,\"1536\":3,\"1546\":3,\"1552\":4,\"1553\":1,\"1598\":1,\"1599\":6,\"1600\":1,\"1622\":3,\"1625\":1,\"1626\":4,\"1718\":1,\"1719\":15,\"1720\":4,\"1725\":17,\"1735\":3,\"1751\":4,\"1756\":1,\"1757\":1,\"1759\":3,\"1782\":1,\"1789\":1,\"1790\":1,\"1794\":1,\"1806\":6,\"1847\":3,\"1863\":2,\"1864\":3,\"1891\":6,\"1992\":3,\"1994\":1,\"1995\":2,\"2131\":1,\"2133\":1,\"2136\":1,\"2215\":2,\"2239\":4,\"2240\":3,\"2246\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2305\":1,\"2325\":1,\"2327\":1,\"2334\":2,\"2355\":20,\"2411\":6,\"2412\":6,\"2420\":1,\"2423\":6,\"2432\":2,\"2447\":6}}],[\"setdev\",{\"1\":{\"286\":1}}],[\"set=aishell\",{\"1\":{\"243\":2}}],[\"sets\",{\"1\":{\"126\":1,\"195\":1,\"197\":1,\"200\":1,\"201\":2,\"205\":3,\"211\":3,\"217\":1,\"235\":2,\"242\":1,\"243\":3,\"254\":1,\"259\":1,\"266\":3,\"275\":3,\"285\":5,\"286\":4,\"821\":1,\"1441\":1,\"2039\":1,\"2045\":1,\"2357\":1}}],[\"sets=\",{\"1\":{\"126\":1,\"243\":1}}],[\"settings\",{\"1\":{\"135\":1,\"276\":1,\"2131\":1}}],[\"setting\",{\"1\":{\"45\":1,\"48\":1,\"67\":1,\"127\":1,\"139\":1,\"145\":1,\"162\":1,\"168\":1,\"175\":2,\"200\":4,\"205\":3,\"211\":2,\"217\":3,\"242\":3,\"254\":1,\"266\":2,\"267\":5,\"275\":2,\"276\":5,\"285\":2,\"286\":1,\"536\":1,\"756\":1,\"773\":1,\"1402\":1,\"1467\":1,\"1594\":1,\"1595\":1,\"1748\":1,\"2355\":1}}],[\"set\",{\"0\":{\"48\":1,\"2321\":2},\"1\":{\"18\":1,\"22\":2,\"24\":1,\"26\":1,\"41\":3,\"42\":1,\"43\":1,\"48\":1,\"66\":2,\"67\":1,\"88\":2,\"94\":1,\"101\":1,\"107\":1,\"110\":1,\"126\":4,\"128\":1,\"139\":2,\"141\":2,\"147\":1,\"148\":1,\"150\":1,\"162\":1,\"166\":2,\"175\":2,\"195\":2,\"196\":3,\"197\":3,\"200\":5,\"205\":1,\"211\":2,\"213\":4,\"218\":10,\"223\":3,\"224\":1,\"233\":2,\"235\":1,\"236\":1,\"242\":1,\"243\":13,\"246\":1,\"252\":2,\"254\":6,\"259\":2,\"263\":1,\"266\":1,\"267\":12,\"268\":4,\"275\":1,\"276\":18,\"277\":4,\"285\":11,\"286\":18,\"290\":2,\"295\":2,\"415\":2,\"515\":1,\"517\":1,\"614\":2,\"619\":1,\"620\":1,\"622\":1,\"623\":1,\"634\":2,\"641\":2,\"643\":2,\"650\":2,\"651\":2,\"703\":1,\"755\":1,\"760\":1,\"768\":1,\"785\":1,\"786\":1,\"795\":1,\"800\":1,\"804\":1,\"817\":1,\"821\":2,\"824\":1,\"847\":2,\"881\":1,\"884\":1,\"911\":4,\"922\":3,\"932\":1,\"934\":1,\"936\":3,\"937\":3,\"992\":1,\"994\":1,\"997\":1,\"1000\":1,\"1002\":1,\"1004\":1,\"1008\":1,\"1012\":1,\"1016\":1,\"1022\":1,\"1029\":1,\"1064\":1,\"1078\":1,\"1153\":1,\"1155\":1,\"1157\":1,\"1202\":1,\"1235\":1,\"1246\":1,\"1247\":1,\"1262\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1290\":1,\"1334\":1,\"1400\":1,\"1513\":1,\"1525\":1,\"1548\":1,\"1551\":1,\"1552\":3,\"1592\":1,\"1596\":2,\"1597\":2,\"1599\":4,\"1605\":1,\"1609\":1,\"1610\":1,\"1619\":1,\"1626\":3,\"1628\":1,\"1651\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1721\":8,\"1725\":2,\"1749\":2,\"1750\":2,\"1794\":1,\"1806\":1,\"1815\":2,\"1854\":1,\"1901\":1,\"1903\":1,\"1918\":1,\"1927\":2,\"1963\":2,\"1992\":3,\"1993\":3,\"1995\":3,\"2000\":2,\"2001\":3,\"2039\":3,\"2049\":2,\"2159\":2,\"2184\":1,\"2216\":2,\"2224\":2,\"2232\":1,\"2235\":3,\"2236\":3,\"2238\":1,\"2239\":4,\"2240\":3,\"2245\":3,\"2249\":2,\"2314\":1,\"2321\":2,\"2329\":1,\"2330\":1,\"2331\":1,\"2342\":1,\"2354\":1,\"2355\":8,\"2359\":1,\"2365\":2,\"2384\":1,\"2385\":1,\"2411\":3,\"2412\":3,\"2423\":3,\"2431\":3,\"2432\":3,\"2447\":3}}],[\"setups\",{\"1\":{\"232\":1,\"258\":1,\"2044\":1}}],[\"setupwhere\",{\"1\":{\"200\":1}}],[\"setup\",{\"0\":{\"1630\":1},\"1\":{\"1\":2,\"3\":1,\"37\":1,\"41\":1,\"92\":1,\"109\":1,\"121\":1,\"162\":10,\"163\":1,\"173\":2,\"195\":1,\"197\":1,\"224\":1,\"243\":1,\"249\":1,\"717\":1,\"756\":2,\"773\":2,\"819\":1,\"821\":1,\"824\":1,\"958\":1,\"961\":1,\"1450\":1,\"1452\":1,\"1644\":1,\"2127\":1,\"2338\":1,\"2355\":3}}],[\"se++\",{\"1\":{\"11\":1}}],[\"se\",{\"0\":{\"179\":1},\"1\":{\"11\":1,\"179\":3,\"190\":1,\"225\":1,\"1210\":1,\"1264\":1,\"1334\":1,\"2184\":1}}],[\"sot\",{\"1\":{\"511\":2,\"2283\":1,\"2284\":1,\"2293\":1}}],[\"sota\",{\"1\":{\"43\":1}}],[\"softmax\",{\"0\":{\"947\":1,\"949\":1,\"2207\":2},\"1\":{\"536\":3,\"633\":2,\"692\":1,\"706\":6,\"755\":2,\"760\":1,\"775\":1,\"787\":3,\"790\":1,\"820\":1,\"846\":1,\"850\":1,\"947\":1,\"949\":1,\"1619\":2,\"1621\":2,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1715\":1,\"1768\":1,\"2167\":1,\"2207\":4,\"2462\":1}}],[\"soft\",{\"1\":{\"262\":1,\"616\":1,\"696\":1,\"697\":1,\"1130\":2}}],[\"softplus\",{\"1\":{\"141\":3,\"635\":8,\"664\":5}}],[\"solution\",{\"1\":{\"2438\":1,\"2440\":1}}],[\"solutions\",{\"1\":{\"290\":1}}],[\"solving\",{\"1\":{\"1246\":1}}],[\"solves\",{\"1\":{\"1308\":1}}],[\"solved\",{\"1\":{\"269\":2,\"278\":2}}],[\"solve\",{\"0\":{\"1357\":1},\"1\":{\"262\":1,\"290\":1,\"1246\":1,\"1357\":2}}],[\"solver\",{\"0\":{\"1132\":1,\"1204\":1,\"1209\":1,\"1228\":1},\"1\":{\"223\":3,\"794\":1,\"1126\":1,\"1127\":1,\"1132\":3,\"1167\":1,\"1204\":4,\"1209\":4,\"1217\":1,\"1228\":3}}],[\"solid\",{\"1\":{\"262\":1}}],[\"sons\",{\"1\":{\"1319\":1}}],[\"song\",{\"1\":{\"1211\":1}}],[\"songs\",{\"1\":{\"269\":1,\"278\":1}}],[\"son\",{\"1\":{\"256\":1}}],[\"soo\",{\"1\":{\"256\":1}}],[\"soon\",{\"1\":{\"65\":1,\"276\":1}}],[\"sos\",{\"0\":{\"1858\":2},\"1\":{\"242\":6,\"625\":1,\"736\":1,\"795\":1,\"796\":1,\"1719\":2,\"1721\":2,\"1725\":2,\"1726\":3,\"1727\":3,\"1858\":4,\"1862\":2,\"1942\":1,\"1996\":1,\"1997\":1,\"2221\":1,\"2462\":1}}],[\"sophisticated\",{\"1\":{\"262\":1}}],[\"sop\",{\"1\":{\"242\":6,\"1996\":1,\"1997\":1}}],[\"soplin\",{\"1\":{\"10\":1,\"156\":1}}],[\"sox\",{\"1\":{\"159\":3,\"286\":2,\"592\":1,\"1678\":3,\"1833\":5}}],[\"sort\",{\"1\":{\"224\":1,\"449\":2,\"616\":2,\"696\":2,\"697\":2,\"1157\":2,\"2002\":2,\"2003\":2,\"2004\":2,\"2005\":4,\"2007\":4,\"2359\":4,\"2480\":1}}],[\"sorting\",{\"1\":{\"96\":1,\"97\":1,\"224\":2,\"2006\":1}}],[\"sortedbatchsampler\",{\"0\":{\"2005\":1},\"1\":{\"2005\":1}}],[\"sorted\",{\"0\":{\"2005\":1},\"1\":{\"95\":1,\"97\":2,\"242\":1,\"449\":2,\"616\":1,\"696\":1,\"697\":1,\"1719\":1,\"1725\":1,\"1806\":1,\"1936\":1,\"2005\":2,\"2007\":2,\"2146\":1,\"2147\":1}}],[\"socket\",{\"1\":{\"66\":4,\"67\":4}}],[\"souden\",{\"1\":{\"1126\":1,\"1217\":1,\"1321\":1,\"1322\":1}}],[\"soundwriter\",{\"0\":{\"1829\":1},\"1\":{\"1829\":2,\"1830\":1}}],[\"soundreader\",{\"0\":{\"1828\":1},\"1\":{\"1828\":1}}],[\"soundhdf5writer\",{\"0\":{\"1826\":1},\"1\":{\"1826\":2,\"1827\":1}}],[\"soundhdf5reader\",{\"0\":{\"1825\":1},\"1\":{\"1825\":1}}],[\"soundhdf5file\",{\"0\":{\"1824\":1},\"1\":{\"1824\":2}}],[\"soundscpwriter\",{\"0\":{\"1009\":1},\"1\":{\"1009\":1,\"1010\":2}}],[\"soundscpreader\",{\"0\":{\"1007\":1},\"1\":{\"994\":1,\"1004\":1,\"1007\":1,\"1008\":3}}],[\"sounds\",{\"1\":{\"286\":1,\"993\":1}}],[\"soundstreamgenerator\",{\"0\":{\"1468\":1},\"1\":{\"1468\":1}}],[\"soundstreamdiscriminator\",{\"0\":{\"1467\":1},\"1\":{\"1467\":1}}],[\"soundstream16k\",{\"1\":{\"220\":1}}],[\"soundstream\",{\"0\":{\"1466\":3,\"1467\":2,\"1468\":2},\"1\":{\"219\":1,\"1385\":2,\"1396\":4,\"1466\":5,\"1467\":4,\"1468\":7}}],[\"sound\",{\"0\":{\"993\":1,\"1007\":1,\"1009\":1,\"1028\":1,\"2393\":1,\"2397\":1,\"2399\":1},\"1\":{\"79\":3,\"290\":1,\"561\":4,\"567\":2,\"993\":1,\"1007\":1,\"1009\":1,\"1028\":1,\"1551\":1,\"1679\":1,\"1824\":2,\"1825\":2,\"1827\":1,\"1828\":2,\"1830\":1,\"2101\":2,\"2343\":1,\"2352\":1,\"2393\":1,\"2397\":1,\"2399\":1}}],[\"soundfile\",{\"0\":{\"74\":1,\"1028\":1},\"1\":{\"71\":4,\"1028\":1}}],[\"soundifile\",{\"1\":{\"71\":1}}],[\"soumi\",{\"1\":{\"7\":1,\"10\":1}}],[\"sources\",{\"1\":{\"6\":1,\"520\":1,\"1264\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1334\":1,\"1354\":2,\"1719\":2,\"1725\":3,\"1751\":1}}],[\"source\",{\"0\":{\"520\":1,\"670\":1},\"1\":{\"6\":1,\"9\":1,\"14\":1,\"26\":3,\"52\":1,\"70\":1,\"79\":1,\"96\":1,\"110\":1,\"242\":1,\"243\":1,\"244\":1,\"245\":1,\"269\":2,\"278\":2,\"293\":1,\"295\":1,\"301\":1,\"309\":1,\"315\":1,\"321\":1,\"327\":1,\"331\":1,\"335\":1,\"342\":1,\"349\":1,\"356\":1,\"361\":1,\"368\":1,\"372\":1,\"374\":1,\"377\":1,\"385\":1,\"389\":1,\"396\":1,\"402\":1,\"404\":1,\"406\":1,\"415\":1,\"421\":1,\"429\":1,\"436\":1,\"442\":1,\"449\":1,\"457\":1,\"461\":1,\"463\":1,\"469\":1,\"475\":1,\"481\":1,\"484\":1,\"490\":1,\"496\":1,\"498\":1,\"505\":1,\"511\":1,\"513\":1,\"514\":1,\"515\":1,\"516\":1,\"517\":1,\"518\":1,\"519\":1,\"520\":4,\"521\":1,\"522\":1,\"523\":1,\"524\":1,\"525\":1,\"526\":1,\"527\":1,\"528\":1,\"529\":1,\"530\":1,\"531\":1,\"532\":1,\"533\":1,\"534\":1,\"535\":2,\"536\":1,\"537\":1,\"538\":1,\"541\":1,\"543\":1,\"545\":1,\"548\":1,\"551\":1,\"554\":1,\"556\":1,\"558\":1,\"561\":1,\"564\":1,\"567\":1,\"570\":1,\"572\":1,\"575\":1,\"578\":1,\"581\":1,\"583\":1,\"586\":1,\"589\":1,\"592\":1,\"594\":1,\"596\":1,\"598\":1,\"600\":1,\"603\":1,\"606\":1,\"609\":1,\"611\":1,\"614\":2,\"615\":1,\"616\":1,\"617\":4,\"618\":4,\"619\":2,\"620\":4,\"621\":1,\"622\":2,\"623\":2,\"624\":4,\"625\":1,\"626\":1,\"627\":1,\"628\":1,\"629\":1,\"630\":1,\"631\":1,\"632\":1,\"633\":1,\"634\":3,\"635\":1,\"636\":3,\"637\":1,\"638\":1,\"639\":1,\"640\":1,\"641\":3,\"642\":1,\"643\":3,\"644\":3,\"645\":1,\"646\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"651\":2,\"652\":1,\"653\":1,\"654\":1,\"655\":1,\"656\":1,\"657\":1,\"658\":1,\"659\":1,\"660\":1,\"661\":1,\"662\":1,\"663\":1,\"664\":1,\"665\":1,\"666\":1,\"667\":1,\"668\":1,\"669\":2,\"670\":3,\"671\":1,\"672\":1,\"673\":1,\"674\":1,\"675\":10,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"688\":1,\"689\":1,\"691\":1,\"692\":1,\"693\":1,\"695\":1,\"696\":1,\"697\":1,\"698\":1,\"699\":3,\"700\":1,\"701\":1,\"702\":1,\"703\":1,\"704\":1,\"706\":1,\"707\":1,\"708\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"715\":1,\"716\":1,\"717\":1,\"718\":1,\"720\":1,\"722\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"738\":1,\"740\":1,\"741\":1,\"743\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"750\":1,\"751\":1,\"752\":1,\"754\":1,\"755\":1,\"756\":1,\"757\":1,\"759\":1,\"760\":1,\"761\":1,\"762\":1,\"763\":1,\"764\":1,\"765\":1,\"766\":1,\"767\":1,\"768\":1,\"769\":1,\"770\":1,\"771\":1,\"772\":1,\"773\":1,\"774\":1,\"775\":1,\"776\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"784\":1,\"785\":1,\"786\":1,\"787\":1,\"788\":1,\"790\":1,\"791\":1,\"793\":1,\"794\":1,\"795\":1,\"796\":1,\"798\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"817\":1,\"820\":1,\"821\":1,\"823\":1,\"824\":1,\"825\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"831\":1,\"832\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":1,\"847\":3,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"862\":1,\"864\":1,\"866\":1,\"867\":1,\"868\":1,\"869\":1,\"870\":1,\"871\":1,\"872\":1,\"873\":1,\"874\":1,\"875\":1,\"876\":1,\"877\":1,\"878\":1,\"879\":1,\"880\":2,\"881\":1,\"882\":1,\"883\":1,\"884\":1,\"885\":1,\"886\":2,\"887\":1,\"888\":1,\"889\":1,\"890\":1,\"891\":1,\"892\":1,\"893\":1,\"894\":1,\"895\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":1,\"900\":1,\"901\":1,\"902\":1,\"903\":1,\"904\":1,\"905\":1,\"906\":1,\"907\":1,\"908\":1,\"909\":1,\"910\":1,\"911\":1,\"912\":1,\"913\":1,\"914\":1,\"915\":1,\"916\":1,\"917\":1,\"918\":1,\"919\":1,\"920\":1,\"921\":1,\"922\":1,\"923\":1,\"924\":1,\"925\":1,\"926\":1,\"927\":1,\"928\":1,\"929\":1,\"930\":1,\"931\":1,\"933\":1,\"935\":1,\"936\":1,\"937\":1,\"938\":1,\"939\":1,\"940\":1,\"941\":1,\"942\":1,\"943\":1,\"944\":1,\"945\":1,\"946\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":1,\"959\":1,\"960\":1,\"962\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"978\":1,\"979\":1,\"980\":1,\"981\":1,\"982\":1,\"983\":1,\"984\":1,\"985\":1,\"987\":1,\"989\":1,\"991\":1,\"993\":1,\"995\":1,\"996\":1,\"998\":1,\"1000\":1,\"1001\":1,\"1003\":1,\"1005\":1,\"1007\":1,\"1009\":1,\"1011\":1,\"1013\":1,\"1015\":1,\"1017\":1,\"1019\":1,\"1021\":1,\"1022\":1,\"1024\":1,\"1026\":1,\"1028\":1,\"1029\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1050\":1,\"1051\":1,\"1053\":2,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1061\":1,\"1062\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1070\":1,\"1071\":1,\"1072\":1,\"1073\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1080\":1,\"1082\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1107\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1116\":1,\"1117\":1,\"1118\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1128\":1,\"1129\":1,\"1130\":1,\"1131\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1137\":1,\"1139\":1,\"1141\":1,\"1142\":2,\"1144\":1,\"1145\":1,\"1147\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1155\":1,\"1156\":1,\"1157\":1,\"1158\":1,\"1159\":1,\"1161\":1,\"1162\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":2,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1176\":1,\"1177\":1,\"1179\":1,\"1180\":1,\"1181\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1189\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":1,\"1204\":1,\"1205\":1,\"1207\":1,\"1208\":1,\"1209\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":2,\"1218\":1,\"1219\":1,\"1221\":1,\"1222\":1,\"1223\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1228\":1,\"1229\":1,\"1230\":1,\"1232\":1,\"1233\":1,\"1235\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1244\":1,\"1245\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1267\":1,\"1268\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1273\":1,\"1274\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1278\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1291\":1,\"1292\":1,\"1293\":1,\"1294\":1,\"1295\":1,\"1296\":1,\"1297\":1,\"1298\":1,\"1299\":1,\"1300\":1,\"1301\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1305\":1,\"1306\":1,\"1307\":1,\"1308\":1,\"1309\":1,\"1310\":1,\"1311\":1,\"1312\":1,\"1313\":1,\"1314\":1,\"1315\":1,\"1316\":1,\"1317\":1,\"1318\":1,\"1319\":1,\"1320\":1,\"1321\":1,\"1322\":1,\"1323\":1,\"1324\":1,\"1325\":1,\"1326\":1,\"1327\":1,\"1328\":1,\"1329\":2,\"1330\":1,\"1331\":1,\"1332\":1,\"1333\":1,\"1334\":3,\"1335\":1,\"1336\":1,\"1337\":1,\"1338\":1,\"1339\":1,\"1340\":1,\"1341\":1,\"1342\":1,\"1343\":1,\"1344\":1,\"1345\":1,\"1346\":1,\"1347\":1,\"1348\":1,\"1349\":1,\"1350\":1,\"1351\":1,\"1352\":1,\"1353\":1,\"1354\":1,\"1355\":1,\"1356\":1,\"1357\":1,\"1358\":1,\"1359\":1,\"1360\":1,\"1361\":1,\"1362\":1,\"1363\":1,\"1364\":1,\"1365\":1,\"1366\":1,\"1367\":1,\"1368\":1,\"1369\":1,\"1370\":1,\"1371\":1,\"1372\":1,\"1373\":1,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1378\":1,\"1379\":1,\"1380\":1,\"1381\":1,\"1382\":1,\"1383\":1,\"1385\":1,\"1386\":1,\"1387\":1,\"1389\":1,\"1390\":1,\"1391\":1,\"1392\":1,\"1394\":1,\"1395\":1,\"1396\":1,\"1397\":1,\"1398\":1,\"1400\":1,\"1401\":1,\"1402\":1,\"1403\":1,\"1404\":1,\"1406\":1,\"1408\":1,\"1409\":1,\"1410\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1432\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1466\":1,\"1467\":1,\"1468\":1,\"1469\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1486\":1,\"1487\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1505\":1,\"1506\":1,\"1507\":1,\"1508\":1,\"1509\":1,\"1511\":1,\"1513\":1,\"1514\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1519\":1,\"1520\":1,\"1521\":1,\"1522\":1,\"1524\":1,\"1525\":1,\"1526\":1,\"1527\":1,\"1529\":1,\"1530\":1,\"1532\":1,\"1533\":1,\"1534\":1,\"1535\":1,\"1536\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1546\":1,\"1547\":1,\"1548\":1,\"1549\":1,\"1551\":1,\"1552\":1,\"1553\":1,\"1554\":1,\"1556\":1,\"1557\":1,\"1558\":1,\"1559\":1,\"1560\":1,\"1561\":1,\"1562\":1,\"1563\":1,\"1564\":1,\"1565\":1,\"1566\":1,\"1567\":1,\"1568\":1,\"1569\":1,\"1570\":1,\"1571\":1,\"1572\":1,\"1573\":1,\"1574\":1,\"1575\":1,\"1576\":1,\"1577\":1,\"1578\":1,\"1579\":1,\"1580\":1,\"1581\":1,\"1582\":1,\"1583\":1,\"1584\":1,\"1585\":1,\"1586\":1,\"1587\":1,\"1588\":1,\"1589\":1,\"1590\":1,\"1591\":1,\"1592\":1,\"1593\":1,\"1594\":1,\"1595\":1,\"1596\":1,\"1597\":1,\"1598\":1,\"1599\":1,\"1600\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1604\":1,\"1605\":1,\"1606\":1,\"1607\":1,\"1608\":1,\"1609\":1,\"1610\":1,\"1611\":1,\"1612\":1,\"1613\":1,\"1614\":1,\"1615\":1,\"1616\":1,\"1617\":1,\"1618\":1,\"1619\":1,\"1620\":1,\"1621\":1,\"1622\":1,\"1623\":1,\"1624\":1,\"1625\":1,\"1626\":1,\"1627\":1,\"1628\":1,\"1629\":1,\"1630\":1,\"1631\":1,\"1632\":1,\"1633\":1,\"1634\":1,\"1635\":1,\"1636\":1,\"1637\":1,\"1638\":1,\"1640\":1,\"1641\":1,\"1642\":1,\"1643\":1,\"1645\":1,\"1646\":1,\"1648\":1,\"1649\":1,\"1650\":1,\"1651\":1,\"1652\":1,\"1654\":1,\"1655\":1,\"1656\":1,\"1657\":1,\"1659\":1,\"1660\":1,\"1661\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1666\":1,\"1667\":1,\"1668\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1672\":1,\"1673\":1,\"1674\":1,\"1675\":1,\"1676\":1,\"1677\":1,\"1679\":1,\"1680\":1,\"1681\":1,\"1682\":1,\"1683\":1,\"1684\":1,\"1685\":1,\"1686\":1,\"1687\":1,\"1688\":1,\"1689\":1,\"1690\":1,\"1691\":1,\"1692\":1,\"1693\":1,\"1694\":1,\"1695\":1,\"1696\":1,\"1697\":1,\"1698\":1,\"1699\":1,\"1700\":1,\"1701\":1,\"1702\":1,\"1703\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1717\":1,\"1718\":1,\"1719\":1,\"1720\":1,\"1721\":1,\"1722\":1,\"1723\":1,\"1724\":1,\"1725\":1,\"1726\":1,\"1727\":1,\"1728\":1,\"1729\":1,\"1730\":1,\"1731\":1,\"1732\":1,\"1733\":1,\"1734\":1,\"1735\":1,\"1736\":1,\"1737\":1,\"1738\":1,\"1739\":1,\"1740\":1,\"1741\":1,\"1742\":1,\"1743\":1,\"1744\":1,\"1745\":1,\"1746\":1,\"1747\":1,\"1748\":1,\"1749\":1,\"1750\":1,\"1751\":1,\"1752\":1,\"1754\":1,\"1756\":1,\"1757\":1,\"1758\":1,\"1759\":1,\"1760\":1,\"1761\":1,\"1762\":1,\"1763\":1,\"1764\":1,\"1765\":1,\"1766\":1,\"1767\":1,\"1768\":1,\"1769\":1,\"1770\":1,\"1771\":1,\"1772\":1,\"1773\":1,\"1775\":1,\"1776\":1,\"1777\":1,\"1778\":1,\"1779\":1,\"1780\":1,\"1781\":1,\"1782\":1,\"1783\":1,\"1784\":1,\"1785\":1,\"1786\":1,\"1787\":1,\"1788\":1,\"1789\":1,\"1790\":1,\"1791\":1,\"1792\":1,\"1793\":1,\"1794\":2,\"1795\":1,\"1796\":1,\"1797\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1801\":1,\"1802\":1,\"1803\":1,\"1804\":1,\"1806\":1,\"1807\":1,\"1808\":1,\"1809\":1,\"1810\":1,\"1811\":1,\"1813\":1,\"1814\":1,\"1815\":3,\"1816\":1,\"1817\":1,\"1818\":1,\"1819\":1,\"1820\":1,\"1821\":1,\"1823\":1,\"1824\":1,\"1825\":1,\"1826\":1,\"1828\":1,\"1829\":1,\"1831\":1,\"1832\":1,\"1833\":1,\"1834\":1,\"1835\":1,\"1836\":1,\"1837\":1,\"1838\":1,\"1839\":1,\"1840\":1,\"1841\":1,\"1842\":1,\"1843\":1,\"1844\":1,\"1845\":1,\"1847\":1,\"1848\":1,\"1849\":1,\"1850\":1,\"1851\":1,\"1852\":1,\"1853\":1,\"1854\":1,\"1855\":1,\"1857\":1,\"1858\":1,\"1859\":1,\"1860\":1,\"1861\":1,\"1862\":1,\"1863\":1,\"1864\":1,\"1865\":1,\"1866\":1,\"1867\":1,\"1868\":1,\"1869\":1,\"1870\":1,\"1871\":1,\"1872\":1,\"1873\":1,\"1874\":1,\"1875\":1,\"1876\":1,\"1877\":1,\"1878\":1,\"1879\":1,\"1880\":1,\"1881\":1,\"1883\":1,\"1884\":1,\"1885\":1,\"1886\":1,\"1887\":1,\"1888\":1,\"1889\":1,\"1891\":1,\"1892\":1,\"1893\":1,\"1894\":1,\"1895\":1,\"1896\":1,\"1897\":1,\"1898\":1,\"1899\":1,\"1900\":1,\"1901\":1,\"1903\":1,\"1905\":1,\"1907\":1,\"1908\":1,\"1910\":1,\"1911\":1,\"1913\":1,\"1914\":1,\"1915\":1,\"1916\":1,\"1917\":1,\"1918\":1,\"1919\":1,\"1920\":1,\"1921\":1,\"1922\":1,\"1923\":1,\"1924\":1,\"1925\":1,\"1926\":1,\"1927\":1,\"1928\":1,\"1929\":1,\"1930\":1,\"1931\":1,\"1932\":1,\"1934\":1,\"1935\":1,\"1936\":1,\"1937\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1948\":1,\"1949\":1,\"1950\":1,\"1951\":1,\"1952\":1,\"1953\":1,\"1954\":1,\"1955\":1,\"1957\":1,\"1959\":1,\"1960\":1,\"1961\":1,\"1962\":1,\"1963\":1,\"1964\":1,\"1965\":1,\"1966\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":2,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1992\":1,\"1993\":1,\"1994\":1,\"1995\":1,\"1996\":1,\"1997\":1,\"1998\":2,\"1999\":1,\"2000\":1,\"2001\":1,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":1,\"2006\":1,\"2007\":1,\"2008\":1,\"2009\":1,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":1,\"2015\":1,\"2016\":1,\"2017\":1,\"2018\":1,\"2019\":1,\"2020\":1,\"2021\":1,\"2022\":1,\"2023\":1,\"2024\":1,\"2025\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2036\":1,\"2037\":1,\"2038\":1,\"2039\":1,\"2040\":1,\"2041\":1,\"2042\":1,\"2043\":1,\"2044\":1,\"2045\":1,\"2046\":1,\"2047\":1,\"2048\":1,\"2049\":1,\"2050\":1,\"2051\":1,\"2052\":1,\"2053\":1,\"2054\":1,\"2055\":1,\"2056\":1,\"2057\":1,\"2058\":1,\"2059\":1,\"2060\":1,\"2061\":1,\"2062\":1,\"2063\":1,\"2064\":1,\"2065\":1,\"2066\":1,\"2067\":1,\"2068\":1,\"2069\":1,\"2070\":1,\"2071\":1,\"2072\":1,\"2073\":1,\"2074\":1,\"2075\":1,\"2076\":1,\"2077\":1,\"2078\":1,\"2079\":1,\"2080\":1,\"2081\":1,\"2082\":1,\"2083\":1,\"2084\":1,\"2085\":1,\"2086\":1,\"2087\":1,\"2088\":1,\"2089\":1,\"2090\":1,\"2091\":1,\"2092\":1,\"2093\":1,\"2094\":1,\"2095\":1,\"2096\":1,\"2097\":1,\"2098\":1,\"2099\":1,\"2100\":1,\"2101\":1,\"2102\":1,\"2103\":1,\"2104\":1,\"2105\":1,\"2106\":1,\"2107\":1,\"2108\":1,\"2109\":1,\"2110\":1,\"2111\":1,\"2112\":1,\"2113\":1,\"2114\":1,\"2115\":1,\"2116\":1,\"2117\":1,\"2118\":1,\"2119\":1,\"2120\":1,\"2121\":1,\"2122\":1,\"2123\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2128\":1,\"2129\":1,\"2130\":1,\"2131\":1,\"2132\":1,\"2133\":1,\"2134\":1,\"2135\":1,\"2136\":1,\"2137\":1,\"2138\":1,\"2139\":1,\"2140\":1,\"2141\":1,\"2142\":1,\"2143\":1,\"2144\":1,\"2145\":1,\"2146\":1,\"2147\":1,\"2148\":1,\"2149\":1,\"2150\":1,\"2151\":1,\"2152\":1,\"2153\":1,\"2154\":1,\"2155\":1,\"2156\":1,\"2157\":1,\"2158\":1,\"2159\":1,\"2160\":1,\"2161\":1,\"2162\":1,\"2163\":1,\"2164\":1,\"2165\":1,\"2166\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2218\":1,\"2219\":1,\"2220\":1,\"2221\":1,\"2222\":1,\"2223\":1,\"2226\":1,\"2227\":1,\"2228\":1,\"2229\":1,\"2230\":1,\"2231\":1,\"2232\":1,\"2233\":1,\"2234\":1,\"2235\":1,\"2236\":1,\"2237\":1,\"2238\":1,\"2239\":1,\"2240\":1,\"2241\":1,\"2242\":1,\"2243\":1,\"2244\":1,\"2245\":1,\"2246\":1,\"2247\":1,\"2248\":1,\"2249\":1,\"2250\":1,\"2251\":1,\"2252\":1,\"2253\":1,\"2254\":1,\"2255\":1,\"2256\":1,\"2257\":1,\"2258\":1,\"2259\":1,\"2260\":1,\"2261\":1,\"2262\":1,\"2263\":1,\"2264\":1,\"2265\":1,\"2266\":1,\"2267\":1,\"2268\":1,\"2269\":1,\"2270\":1,\"2271\":1,\"2272\":1,\"2273\":1,\"2274\":1,\"2275\":1,\"2276\":1,\"2277\":1,\"2278\":1,\"2279\":1,\"2280\":1,\"2281\":1,\"2282\":1,\"2283\":1,\"2284\":1,\"2285\":1,\"2286\":1,\"2287\":1,\"2288\":1,\"2289\":1,\"2291\":1,\"2292\":1,\"2293\":1,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":1,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2307\":1,\"2308\":1,\"2309\":1,\"2310\":1,\"2311\":1,\"2312\":1,\"2313\":1,\"2314\":1,\"2316\":1,\"2317\":1,\"2318\":1,\"2319\":1,\"2320\":1,\"2321\":1,\"2322\":1,\"2323\":1,\"2324\":1,\"2325\":1,\"2327\":1,\"2328\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2332\":1,\"2333\":1,\"2334\":1,\"2335\":1,\"2336\":1,\"2337\":1,\"2338\":1,\"2339\":1,\"2340\":1,\"2341\":3,\"2342\":1,\"2344\":1,\"2345\":1,\"2346\":1,\"2347\":1,\"2348\":1,\"2349\":1,\"2350\":1,\"2351\":1,\"2353\":1,\"2354\":1,\"2355\":1,\"2356\":1,\"2357\":1,\"2358\":1,\"2359\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2363\":1,\"2364\":1,\"2365\":1,\"2366\":1,\"2367\":1,\"2368\":1,\"2369\":1,\"2370\":1,\"2371\":1,\"2372\":1,\"2373\":1,\"2374\":1,\"2375\":1,\"2376\":1,\"2378\":1,\"2379\":1,\"2380\":1,\"2381\":1,\"2382\":1,\"2383\":1,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2388\":1,\"2389\":1,\"2390\":1,\"2391\":1,\"2392\":1,\"2393\":1,\"2394\":1,\"2395\":1,\"2396\":1,\"2397\":1,\"2398\":1,\"2399\":1,\"2400\":1,\"2401\":1,\"2403\":1,\"2404\":1,\"2407\":1,\"2408\":1,\"2409\":1,\"2411\":1,\"2412\":1,\"2413\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2421\":1,\"2422\":1,\"2423\":1,\"2424\":1,\"2425\":1,\"2426\":1,\"2427\":1,\"2428\":1,\"2429\":1,\"2430\":1,\"2431\":1,\"2432\":3,\"2433\":1,\"2434\":1,\"2436\":1,\"2437\":1,\"2438\":1,\"2439\":1,\"2440\":1,\"2441\":1,\"2442\":1,\"2443\":1,\"2445\":1,\"2446\":1,\"2447\":1,\"2448\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":2,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1,\"2474\":1,\"2475\":1,\"2476\":1,\"2477\":1,\"2478\":1,\"2480\":1,\"2481\":1,\"2482\":1,\"2483\":1,\"2484\":1,\"2486\":1,\"2487\":1,\"2489\":1,\"2490\":1,\"2491\":1,\"2492\":1,\"2494\":1,\"2495\":1,\"2496\":1,\"2497\":1,\"2498\":1,\"2499\":1,\"2501\":1,\"2503\":1,\"2504\":1,\"2506\":1,\"2507\":1}}],[\"some2\",{\"1\":{\"1956\":1}}],[\"some1\",{\"1\":{\"1956\":1}}],[\"somewhat\",{\"1\":{\"246\":1}}],[\"somewhere\",{\"1\":{\"69\":1,\"2315\":5}}],[\"somehow\",{\"1\":{\"197\":1,\"1656\":1}}],[\"something\",{\"1\":{\"259\":2,\"269\":1,\"278\":1,\"2355\":1}}],[\"sometype\",{\"1\":{\"81\":5}}],[\"sometimes\",{\"1\":{\"47\":1,\"91\":1,\"128\":1,\"252\":1,\"290\":1,\"827\":1,\"1246\":1}}],[\"someki2022espnet\",{\"1\":{\"16\":1}}],[\"someki\",{\"1\":{\"5\":1,\"16\":1}}],[\"some\",{\"1\":{\"3\":2,\"24\":1,\"26\":1,\"43\":2,\"54\":1,\"56\":1,\"60\":1,\"69\":4,\"71\":2,\"79\":4,\"80\":7,\"86\":1,\"102\":1,\"104\":2,\"106\":1,\"110\":1,\"118\":2,\"128\":2,\"138\":3,\"141\":3,\"145\":1,\"152\":1,\"153\":2,\"159\":2,\"162\":1,\"163\":2,\"165\":1,\"166\":1,\"174\":2,\"175\":1,\"197\":3,\"201\":1,\"213\":1,\"223\":2,\"224\":1,\"225\":1,\"268\":2,\"269\":4,\"277\":2,\"278\":4,\"285\":1,\"286\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"744\":1,\"828\":1,\"829\":1,\"830\":1,\"859\":1,\"986\":3,\"992\":4,\"994\":4,\"997\":4,\"999\":4,\"1004\":4,\"1006\":4,\"1008\":8,\"1016\":4,\"1018\":4,\"1023\":4,\"1027\":12,\"1127\":1,\"1164\":2,\"1246\":1,\"1354\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1484\":1,\"1655\":1,\"1845\":1,\"1883\":1,\"1963\":1,\"2240\":1,\"2298\":1,\"2355\":2,\"2380\":1,\"2474\":1}}],[\"so\",{\"1\":{\"3\":1,\"37\":1,\"67\":1,\"79\":2,\"81\":1,\"82\":2,\"84\":1,\"107\":1,\"108\":1,\"135\":1,\"152\":1,\"159\":1,\"161\":1,\"163\":1,\"168\":1,\"173\":1,\"174\":1,\"195\":1,\"196\":1,\"197\":3,\"213\":1,\"223\":1,\"224\":1,\"225\":2,\"240\":1,\"242\":1,\"243\":1,\"268\":1,\"277\":1,\"290\":1,\"536\":1,\"768\":2,\"820\":1,\"828\":1,\"830\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1306\":2,\"1371\":2,\"1514\":1,\"1558\":1,\"1609\":1,\"1883\":1,\"1943\":1,\"1966\":1,\"2006\":1,\"2355\":1}}],[\"sttask\",{\"0\":{\"2267\":1},\"1\":{\"2263\":1,\"2267\":1}}],[\"stiffness=1\",{\"1\":{\"1225\":1}}],[\"stiffness\",{\"1\":{\"1224\":1,\"1225\":3}}],[\"still\",{\"1\":{\"106\":2,\"135\":1,\"137\":1,\"153\":1,\"242\":2,\"267\":1,\"276\":1,\"286\":1,\"1008\":1,\"1397\":1,\"1679\":1}}],[\"stm\",{\"1\":{\"578\":5,\"611\":4}}],[\"stft2logmelspectrogram\",{\"0\":{\"1836\":1,\"1925\":1},\"1\":{\"1836\":1,\"1925\":1}}],[\"stftdiscriminator\",{\"1\":{\"1422\":1}}],[\"stftdecoder\",{\"0\":{\"1250\":1},\"1\":{\"1250\":1}}],[\"stftencoder\",{\"0\":{\"1251\":1},\"1\":{\"1251\":1}}],[\"stft\",{\"0\":{\"525\":1,\"551\":1,\"1250\":1,\"1251\":1,\"1383\":1,\"1385\":1,\"1386\":1,\"1411\":1,\"1669\":2,\"1835\":1,\"1924\":1},\"1\":{\"223\":2,\"225\":2,\"266\":1,\"275\":1,\"525\":6,\"551\":2,\"720\":2,\"1062\":1,\"1145\":1,\"1155\":3,\"1157\":4,\"1168\":1,\"1210\":3,\"1250\":2,\"1251\":3,\"1264\":3,\"1269\":3,\"1270\":3,\"1271\":3,\"1280\":1,\"1283\":1,\"1314\":1,\"1315\":1,\"1334\":10,\"1351\":1,\"1352\":1,\"1376\":1,\"1377\":1,\"1383\":1,\"1385\":7,\"1386\":2,\"1392\":3,\"1397\":2,\"1401\":1,\"1402\":2,\"1409\":1,\"1411\":1,\"1413\":2,\"1422\":3,\"1466\":1,\"1467\":2,\"1533\":6,\"1552\":2,\"1662\":1,\"1669\":4,\"1680\":2,\"1692\":2,\"1698\":2,\"1835\":2,\"1836\":1,\"1899\":1,\"1924\":2,\"1925\":2,\"1978\":1,\"1980\":1,\"1982\":1,\"2414\":1,\"2416\":1,\"2418\":1}}],[\"st1\",{\"1\":{\"195\":1,\"197\":1,\"223\":1,\"227\":1,\"230\":1}}],[\"std=0\",{\"1\":{\"1487\":1,\"1545\":1}}],[\"std2\",{\"1\":{\"637\":2}}],[\"std1\",{\"1\":{\"637\":2}}],[\"std\",{\"0\":{\"1567\":1},\"1\":{\"633\":2,\"638\":2,\"646\":2,\"647\":2,\"699\":1,\"1269\":1,\"1270\":5,\"1271\":5,\"1334\":2,\"1545\":3,\"1567\":1,\"1728\":1,\"1850\":1,\"2183\":1}}],[\"stderr\",{\"1\":{\"167\":1}}],[\"stdout\",{\"1\":{\"167\":1}}],[\"stonmask\",{\"1\":{\"2404\":1}}],[\"stonemask\",{\"1\":{\"2404\":1}}],[\"storage\",{\"1\":{\"704\":1,\"705\":1,\"2354\":1}}],[\"stores\",{\"1\":{\"755\":1,\"824\":1,\"1000\":1,\"2065\":2}}],[\"store\",{\"1\":{\"246\":1,\"266\":1,\"269\":1,\"275\":1,\"278\":1,\"285\":1,\"527\":1,\"756\":1,\"773\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1283\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1710\":1,\"1711\":1,\"1712\":1,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1768\":1}}],[\"stored\",{\"1\":{\"3\":1,\"243\":1,\"756\":1,\"773\":1,\"803\":1}}],[\"stochasticdurationpredictor\",{\"0\":{\"1616\":1},\"1\":{\"1616\":2}}],[\"stochasticdepth\",{\"0\":{\"837\":1},\"1\":{\"837\":1}}],[\"stochastic\",{\"0\":{\"939\":1},\"1\":{\"700\":1,\"701\":3,\"709\":1,\"780\":1,\"820\":1,\"828\":1,\"830\":1,\"837\":2,\"939\":5,\"1064\":1,\"1153\":1,\"1262\":1,\"1616\":2,\"1625\":4,\"1626\":12,\"1759\":2,\"2191\":1}}],[\"stoi\",{\"1\":{\"285\":3}}],[\"stopping\",{\"1\":{\"118\":1,\"2348\":1,\"2355\":1,\"2359\":1,\"2370\":2,\"2372\":1}}],[\"stopped\",{\"1\":{\"106\":1}}],[\"stop\",{\"0\":{\"118\":1,\"174\":1},\"1\":{\"118\":1,\"126\":1,\"174\":3,\"175\":1,\"201\":4,\"212\":4,\"218\":4,\"243\":4,\"267\":19,\"276\":13,\"285\":1,\"286\":10,\"290\":1,\"1419\":1,\"1598\":2,\"1599\":6,\"1600\":2,\"1654\":1,\"1666\":1,\"1750\":3,\"1839\":3,\"1991\":2,\"1993\":2,\"2223\":1,\"2224\":2,\"2245\":2,\"2412\":6,\"2423\":6,\"2431\":2,\"2432\":1,\"2447\":6}}],[\"steady\",{\"1\":{\"1224\":1,\"1225\":1}}],[\"stereo\",{\"1\":{\"70\":1,\"1422\":1}}],[\"step=0\",{\"1\":{\"2134\":1}}],[\"steplr\",{\"1\":{\"2021\":1}}],[\"stepping\",{\"1\":{\"829\":1,\"2355\":1}}],[\"step2\",{\"1\":{\"159\":1}}],[\"steps=1\",{\"1\":{\"1253\":1}}],[\"steps\",{\"1\":{\"3\":1,\"37\":2,\"45\":2,\"91\":1,\"109\":1,\"145\":1,\"150\":1,\"197\":1,\"200\":1,\"224\":1,\"228\":3,\"232\":1,\"243\":1,\"259\":2,\"286\":1,\"535\":1,\"616\":1,\"625\":3,\"669\":1,\"696\":1,\"697\":1,\"699\":1,\"738\":1,\"747\":1,\"846\":1,\"1050\":1,\"1116\":1,\"1189\":1,\"1224\":1,\"1225\":1,\"1245\":1,\"1253\":3,\"1484\":1,\"1514\":1,\"1545\":1,\"1692\":6,\"1926\":1,\"2014\":5,\"2015\":11,\"2016\":1,\"2017\":1,\"2018\":12,\"2019\":1,\"2020\":1,\"2021\":2,\"2355\":2,\"2423\":1,\"2428\":2,\"2432\":2,\"2442\":1}}],[\"step\",{\"0\":{\"3\":2,\"161\":1,\"162\":1,\"163\":1,\"2021\":1,\"2389\":1},\"1\":{\"0\":2,\"45\":5,\"107\":2,\"113\":2,\"128\":1,\"139\":1,\"145\":2,\"162\":1,\"200\":2,\"205\":1,\"212\":2,\"218\":2,\"222\":8,\"224\":5,\"225\":4,\"242\":1,\"259\":1,\"267\":2,\"276\":2,\"286\":4,\"614\":2,\"616\":2,\"625\":1,\"627\":1,\"634\":2,\"637\":2,\"641\":2,\"643\":2,\"651\":2,\"692\":3,\"696\":3,\"697\":3,\"706\":2,\"724\":4,\"725\":4,\"726\":3,\"728\":4,\"729\":4,\"744\":4,\"784\":5,\"789\":1,\"790\":2,\"819\":4,\"821\":3,\"822\":1,\"823\":1,\"824\":4,\"828\":4,\"829\":5,\"830\":4,\"847\":2,\"850\":2,\"859\":3,\"861\":1,\"878\":1,\"879\":1,\"881\":1,\"961\":1,\"1050\":1,\"1116\":1,\"1149\":1,\"1161\":1,\"1189\":1,\"1203\":1,\"1218\":1,\"1221\":1,\"1229\":1,\"1244\":1,\"1245\":1,\"1350\":5,\"1356\":3,\"1484\":1,\"1545\":1,\"1708\":1,\"1709\":2,\"1726\":2,\"1727\":2,\"1749\":2,\"1797\":1,\"1815\":2,\"1819\":1,\"1823\":1,\"1834\":1,\"1843\":2,\"1966\":1,\"1992\":2,\"1997\":2,\"2000\":2,\"2010\":1,\"2011\":1,\"2012\":1,\"2013\":1,\"2014\":4,\"2015\":6,\"2018\":1,\"2019\":7,\"2020\":11,\"2021\":13,\"2134\":2,\"2355\":27,\"2367\":2,\"2369\":2,\"2389\":1,\"2426\":3,\"2428\":4,\"2434\":1,\"2435\":2,\"2462\":1,\"2469\":1,\"2470\":3,\"2471\":2,\"2472\":1,\"2473\":2}}],[\"student\",{\"1\":{\"190\":1}}],[\"studio\",{\"1\":{\"18\":2}}],[\"stuctures\",{\"1\":{\"69\":1}}],[\"str2triple\",{\"0\":{\"2501\":1},\"1\":{\"2501\":2,\"2502\":1}}],[\"str2pair\",{\"0\":{\"2499\":1},\"1\":{\"2499\":2,\"2500\":1}}],[\"str2bool\",{\"0\":{\"2498\":1},\"1\":{\"1987\":1,\"1991\":2,\"2458\":6,\"2460\":3,\"2462\":6,\"2463\":3,\"2464\":3,\"2469\":2,\"2470\":3,\"2471\":1,\"2472\":1,\"2498\":1}}],[\"structural\",{\"1\":{\"2427\":1}}],[\"structured\",{\"1\":{\"107\":1,\"2443\":1}}],[\"structure\",{\"0\":{\"37\":1,\"109\":1},\"1\":{\"196\":3,\"197\":1,\"212\":1,\"213\":3,\"218\":1,\"226\":1,\"267\":1,\"268\":3,\"276\":1,\"277\":3,\"286\":1,\"720\":1,\"760\":1,\"768\":1,\"815\":1,\"1548\":1,\"1655\":1,\"1810\":1,\"1980\":1,\"1982\":1,\"2416\":1,\"2418\":1}}],[\"strm\",{\"1\":{\"796\":1}}],[\"strornone\",{\"1\":{\"1678\":1,\"1680\":1,\"1692\":1,\"1698\":1}}],[\"strorlist\",{\"1\":{\"1280\":1}}],[\"strordict\",{\"1\":{\"760\":1}}],[\"strong\",{\"1\":{\"2001\":1}}],[\"strongly\",{\"1\":{\"536\":1,\"2000\":1,\"2001\":1}}],[\"strongest\",{\"1\":{\"262\":1}}],[\"str=\",{\"1\":{\"674\":9,\"2340\":2}}],[\"stretch\",{\"0\":{\"1698\":1},\"1\":{\"1698\":1}}],[\"stretch2d\",{\"0\":{\"1617\":1},\"1\":{\"1617\":3}}],[\"strengths\",{\"1\":{\"262\":1}}],[\"stream=none\",{\"1\":{\"2507\":1}}],[\"streams\",{\"1\":{\"2130\":10,\"2136\":12,\"2143\":1}}],[\"streamable\",{\"1\":{\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1}}],[\"streampositionalencoding\",{\"0\":{\"1837\":1},\"1\":{\"710\":1,\"711\":1,\"1837\":1}}],[\"streaming\",{\"0\":{\"129\":1,\"146\":1,\"315\":1,\"349\":1,\"469\":1,\"697\":1,\"743\":1,\"763\":1,\"1727\":1,\"1732\":1},\"1\":{\"129\":1,\"130\":1,\"131\":2,\"132\":2,\"133\":1,\"136\":1,\"137\":1,\"146\":1,\"147\":2,\"148\":1,\"150\":2,\"178\":2,\"301\":2,\"315\":1,\"321\":2,\"349\":1,\"429\":2,\"442\":2,\"469\":1,\"505\":2,\"527\":1,\"616\":3,\"617\":2,\"618\":2,\"619\":1,\"620\":3,\"622\":1,\"623\":1,\"624\":2,\"626\":1,\"636\":2,\"697\":1,\"703\":1,\"743\":1,\"755\":1,\"763\":1,\"785\":1,\"786\":1,\"800\":1,\"867\":1,\"881\":1,\"884\":1,\"922\":1,\"936\":1,\"937\":1,\"1031\":4,\"1035\":6,\"1045\":1,\"1112\":4,\"1113\":6,\"1232\":1,\"1250\":5,\"1251\":7,\"1261\":1,\"1267\":1,\"1720\":3,\"1721\":8,\"1727\":1,\"1731\":2,\"1732\":1,\"1837\":1,\"2249\":1}}],[\"stream\",{\"1\":{\"74\":2,\"755\":3,\"785\":3,\"803\":1,\"804\":2,\"931\":1,\"932\":2,\"933\":1,\"934\":2,\"1031\":1,\"1035\":1,\"1112\":1,\"1113\":1,\"1260\":1,\"1316\":1,\"1389\":3,\"1395\":2,\"1401\":3,\"1408\":3,\"1466\":3,\"2044\":6,\"2054\":1,\"2065\":3,\"2130\":20,\"2136\":17,\"2137\":9,\"2143\":4,\"2148\":1,\"2480\":1}}],[\"str\",{\"0\":{\"2096\":1,\"2499\":1,\"2501\":1,\"2503\":1,\"2504\":1},\"1\":{\"139\":1,\"141\":5,\"142\":3,\"143\":1,\"614\":8,\"616\":1,\"625\":6,\"626\":3,\"627\":2,\"628\":1,\"631\":2,\"632\":1,\"633\":3,\"634\":13,\"637\":1,\"641\":1,\"643\":2,\"653\":1,\"655\":2,\"656\":2,\"657\":2,\"658\":1,\"659\":2,\"660\":2,\"661\":5,\"662\":1,\"664\":1,\"666\":1,\"671\":2,\"672\":1,\"673\":2,\"674\":11,\"675\":1,\"692\":1,\"696\":2,\"697\":2,\"699\":2,\"700\":6,\"701\":2,\"702\":3,\"706\":3,\"709\":12,\"710\":4,\"711\":2,\"712\":1,\"713\":1,\"715\":1,\"720\":2,\"731\":1,\"732\":1,\"733\":7,\"734\":7,\"736\":7,\"737\":7,\"738\":1,\"740\":2,\"743\":1,\"745\":2,\"746\":5,\"747\":4,\"748\":1,\"752\":2,\"759\":2,\"760\":7,\"761\":1,\"762\":1,\"763\":2,\"765\":1,\"766\":1,\"767\":1,\"768\":5,\"770\":1,\"771\":1,\"774\":14,\"775\":1,\"777\":6,\"778\":3,\"780\":18,\"781\":3,\"783\":3,\"787\":2,\"790\":2,\"791\":2,\"795\":2,\"796\":1,\"798\":1,\"815\":2,\"820\":1,\"833\":1,\"837\":1,\"846\":3,\"847\":3,\"848\":1,\"849\":2,\"850\":1,\"851\":1,\"862\":1,\"864\":3,\"869\":1,\"902\":1,\"930\":2,\"939\":2,\"951\":1,\"954\":3,\"958\":4,\"959\":1,\"962\":2,\"974\":2,\"980\":2,\"985\":1,\"987\":3,\"989\":3,\"991\":1,\"996\":1,\"998\":2,\"1000\":1,\"1001\":1,\"1003\":1,\"1005\":2,\"1008\":2,\"1009\":5,\"1013\":1,\"1015\":1,\"1017\":2,\"1019\":3,\"1021\":3,\"1022\":4,\"1024\":3,\"1026\":5,\"1028\":2,\"1029\":4,\"1037\":1,\"1040\":1,\"1053\":3,\"1061\":1,\"1062\":2,\"1064\":1,\"1070\":1,\"1071\":1,\"1107\":11,\"1117\":2,\"1118\":2,\"1119\":7,\"1125\":2,\"1126\":3,\"1127\":2,\"1130\":2,\"1131\":2,\"1136\":2,\"1139\":3,\"1141\":4,\"1155\":3,\"1156\":4,\"1157\":5,\"1158\":3,\"1162\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":2,\"1175\":1,\"1185\":3,\"1202\":1,\"1210\":2,\"1217\":6,\"1232\":2,\"1235\":2,\"1250\":1,\"1251\":1,\"1253\":3,\"1261\":2,\"1262\":1,\"1267\":4,\"1268\":8,\"1274\":2,\"1276\":1,\"1278\":3,\"1279\":4,\"1280\":7,\"1281\":4,\"1282\":2,\"1283\":6,\"1290\":1,\"1318\":1,\"1326\":1,\"1328\":1,\"1354\":1,\"1381\":3,\"1389\":12,\"1390\":1,\"1391\":6,\"1392\":4,\"1395\":6,\"1396\":6,\"1397\":5,\"1401\":12,\"1402\":9,\"1403\":6,\"1408\":12,\"1409\":10,\"1410\":1,\"1419\":2,\"1420\":2,\"1424\":2,\"1426\":2,\"1428\":2,\"1430\":2,\"1442\":3,\"1444\":3,\"1446\":2,\"1448\":2,\"1450\":9,\"1452\":9,\"1454\":7,\"1456\":7,\"1458\":7,\"1460\":7,\"1466\":12,\"1467\":8,\"1468\":6,\"1476\":1,\"1477\":1,\"1483\":1,\"1485\":1,\"1493\":1,\"1494\":1,\"1508\":2,\"1509\":3,\"1511\":4,\"1513\":4,\"1519\":8,\"1521\":7,\"1526\":35,\"1534\":1,\"1535\":8,\"1536\":8,\"1539\":3,\"1546\":8,\"1548\":2,\"1549\":10,\"1551\":4,\"1552\":14,\"1553\":31,\"1558\":2,\"1559\":1,\"1576\":2,\"1582\":6,\"1584\":2,\"1585\":5,\"1591\":2,\"1592\":4,\"1593\":2,\"1594\":6,\"1595\":5,\"1596\":4,\"1597\":4,\"1598\":23,\"1599\":21,\"1600\":27,\"1604\":8,\"1605\":8,\"1606\":12,\"1607\":2,\"1609\":4,\"1610\":4,\"1614\":4,\"1615\":8,\"1617\":2,\"1618\":2,\"1619\":8,\"1620\":2,\"1621\":4,\"1622\":8,\"1624\":6,\"1625\":23,\"1626\":8,\"1640\":6,\"1641\":4,\"1643\":2,\"1644\":2,\"1645\":1,\"1646\":2,\"1647\":2,\"1655\":3,\"1656\":1,\"1664\":1,\"1665\":1,\"1668\":3,\"1669\":1,\"1670\":1,\"1674\":1,\"1675\":2,\"1677\":2,\"1678\":1,\"1680\":1,\"1681\":2,\"1683\":4,\"1688\":1,\"1692\":1,\"1695\":1,\"1698\":1,\"1699\":1,\"1702\":2,\"1719\":17,\"1720\":4,\"1721\":10,\"1722\":4,\"1725\":25,\"1726\":2,\"1727\":2,\"1748\":2,\"1749\":7,\"1754\":1,\"1758\":1,\"1762\":1,\"1766\":2,\"1775\":2,\"1806\":4,\"1807\":2,\"1814\":1,\"1815\":3,\"1816\":1,\"1824\":4,\"1842\":1,\"1843\":2,\"1854\":1,\"1862\":8,\"1863\":8,\"1864\":5,\"1865\":2,\"1866\":1,\"1867\":4,\"1871\":2,\"1872\":1,\"1876\":1,\"1881\":4,\"1883\":4,\"1889\":1,\"1891\":3,\"1895\":1,\"1911\":3,\"1913\":2,\"1914\":3,\"1916\":3,\"1921\":2,\"1926\":1,\"1937\":2,\"1940\":2,\"1942\":6,\"1944\":1,\"1945\":1,\"1947\":1,\"1949\":2,\"1950\":2,\"1951\":4,\"1952\":2,\"1953\":3,\"1954\":6,\"1955\":4,\"1957\":1,\"1959\":8,\"1965\":7,\"1968\":1,\"1970\":1,\"1971\":2,\"1973\":1,\"1975\":12,\"1976\":1,\"1978\":1,\"1979\":1,\"1980\":2,\"1981\":1,\"1982\":1,\"1983\":1,\"1991\":1,\"1992\":3,\"1993\":8,\"1994\":8,\"1995\":3,\"1996\":10,\"1997\":10,\"1999\":1,\"2000\":5,\"2001\":3,\"2002\":5,\"2003\":4,\"2004\":4,\"2005\":3,\"2006\":2,\"2007\":6,\"2008\":5,\"2030\":1,\"2032\":1,\"2039\":16,\"2040\":2,\"2043\":8,\"2044\":32,\"2045\":6,\"2049\":10,\"2054\":6,\"2055\":8,\"2056\":8,\"2066\":8,\"2096\":1,\"2126\":6,\"2127\":8,\"2128\":1,\"2129\":2,\"2130\":4,\"2131\":6,\"2132\":6,\"2133\":5,\"2134\":7,\"2136\":6,\"2137\":5,\"2138\":2,\"2139\":1,\"2141\":3,\"2142\":1,\"2143\":4,\"2144\":1,\"2145\":1,\"2149\":3,\"2150\":1,\"2151\":1,\"2154\":1,\"2156\":3,\"2157\":4,\"2160\":2,\"2161\":1,\"2166\":2,\"2184\":2,\"2187\":1,\"2191\":12,\"2192\":1,\"2198\":1,\"2203\":2,\"2216\":6,\"2219\":2,\"2220\":4,\"2221\":12,\"2222\":2,\"2228\":6,\"2229\":6,\"2231\":1,\"2232\":3,\"2235\":21,\"2236\":19,\"2238\":3,\"2239\":33,\"2240\":29,\"2241\":2,\"2245\":21,\"2246\":9,\"2247\":9,\"2248\":9,\"2249\":26,\"2250\":9,\"2251\":9,\"2252\":9,\"2253\":10,\"2254\":9,\"2255\":9,\"2256\":9,\"2257\":9,\"2258\":2,\"2259\":9,\"2260\":9,\"2261\":9,\"2262\":2,\"2263\":12,\"2264\":9,\"2265\":5,\"2266\":9,\"2267\":9,\"2268\":12,\"2269\":9,\"2270\":12,\"2271\":12,\"2272\":9,\"2273\":9,\"2274\":4,\"2275\":8,\"2278\":3,\"2279\":5,\"2280\":2,\"2283\":7,\"2284\":9,\"2285\":10,\"2286\":3,\"2287\":6,\"2288\":5,\"2289\":1,\"2291\":5,\"2292\":7,\"2293\":11,\"2294\":1,\"2295\":1,\"2296\":1,\"2297\":1,\"2298\":4,\"2300\":1,\"2301\":1,\"2302\":1,\"2303\":1,\"2304\":1,\"2308\":2,\"2310\":1,\"2312\":1,\"2314\":2,\"2316\":1,\"2317\":1,\"2324\":1,\"2325\":2,\"2327\":5,\"2329\":2,\"2331\":1,\"2334\":5,\"2335\":1,\"2336\":21,\"2337\":21,\"2338\":4,\"2339\":8,\"2340\":6,\"2341\":5,\"2342\":10,\"2343\":1,\"2344\":5,\"2345\":2,\"2346\":7,\"2347\":3,\"2348\":8,\"2349\":1,\"2350\":4,\"2351\":10,\"2352\":1,\"2353\":7,\"2354\":5,\"2356\":20,\"2359\":29,\"2360\":21,\"2361\":23,\"2362\":19,\"2363\":19,\"2364\":7,\"2365\":3,\"2366\":8,\"2367\":6,\"2368\":8,\"2369\":6,\"2370\":16,\"2371\":3,\"2372\":8,\"2376\":5,\"2378\":1,\"2381\":1,\"2382\":2,\"2384\":1,\"2385\":1,\"2386\":1,\"2387\":1,\"2400\":1,\"2402\":1,\"2403\":2,\"2404\":1,\"2406\":1,\"2408\":6,\"2409\":2,\"2410\":1,\"2411\":20,\"2412\":20,\"2414\":1,\"2415\":1,\"2416\":2,\"2417\":1,\"2418\":1,\"2419\":1,\"2422\":2,\"2423\":21,\"2427\":2,\"2428\":2,\"2431\":11,\"2432\":14,\"2435\":1,\"2442\":2,\"2445\":2,\"2446\":6,\"2447\":19,\"2457\":1,\"2462\":7,\"2469\":1,\"2470\":1,\"2472\":1,\"2473\":1,\"2482\":1,\"2484\":1,\"2490\":1,\"2492\":1,\"2496\":1,\"2497\":1,\"2498\":1,\"2499\":5,\"2500\":2,\"2501\":6,\"2502\":1,\"2503\":3,\"2504\":4}}],[\"strict\",{\"1\":{\"2355\":1,\"2369\":1}}],[\"strip=false\",{\"1\":{\"2286\":1}}],[\"strings\",{\"1\":{\"1064\":1,\"1078\":1,\"1153\":1,\"1202\":1,\"1262\":1,\"1290\":1,\"1656\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"2232\":1,\"2238\":1,\"2478\":1}}],[\"string\",{\"0\":{\"2311\":1},\"1\":{\"134\":2,\"142\":1,\"167\":1,\"786\":1,\"800\":1,\"921\":1,\"935\":1,\"1117\":1,\"1130\":1,\"1131\":1,\"1134\":1,\"1136\":1,\"1137\":1,\"1141\":1,\"1232\":1,\"1257\":1,\"1280\":1,\"1283\":1,\"1719\":4,\"1720\":2,\"1725\":4,\"1806\":2,\"2044\":2,\"2137\":5,\"2143\":1,\"2149\":1,\"2151\":1,\"2310\":1,\"2311\":2}}],[\"strides=\",{\"1\":{\"1514\":1,\"1534\":1}}],[\"strides\",{\"1\":{\"1119\":2,\"1385\":3,\"1386\":3,\"1392\":1,\"1401\":1,\"1402\":1,\"1466\":1,\"1467\":1,\"1509\":1,\"1511\":2,\"1514\":1,\"1530\":1,\"1534\":2,\"1543\":1,\"1549\":1,\"1553\":1,\"1746\":2}}],[\"stride=4160\",{\"1\":{\"2102\":1}}],[\"stride=64\",{\"1\":{\"1269\":1,\"1270\":1}}],[\"stride=\",{\"1\":{\"1080\":1,\"1082\":1,\"1108\":1,\"1124\":2,\"1147\":1}}],[\"stride=128\",{\"1\":{\"1334\":1}}],[\"stride=1\",{\"1\":{\"693\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"860\":1,\"885\":1,\"892\":1,\"945\":1,\"1265\":1,\"1300\":1,\"1302\":1,\"1303\":1,\"1304\":1,\"1346\":1,\"1347\":1,\"1383\":1,\"2177\":1,\"2181\":1}}],[\"stride\",{\"1\":{\"43\":9,\"141\":3,\"620\":3,\"665\":1,\"691\":2,\"712\":1,\"768\":3,\"846\":1,\"859\":1,\"893\":1,\"894\":1,\"973\":1,\"981\":1,\"1112\":1,\"1113\":1,\"1120\":1,\"1122\":1,\"1124\":4,\"1125\":6,\"1147\":2,\"1148\":1,\"1164\":1,\"1180\":3,\"1181\":3,\"1265\":2,\"1269\":2,\"1270\":2,\"1271\":2,\"1272\":1,\"1334\":2,\"1368\":2,\"1392\":2,\"1401\":1,\"1402\":1,\"1408\":1,\"1409\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1466\":1,\"1467\":1,\"1484\":2,\"1514\":1,\"1526\":1,\"1549\":1,\"1553\":1,\"1594\":1,\"1595\":1,\"1598\":2,\"1599\":3,\"1600\":2,\"1606\":1,\"1625\":1,\"1668\":2,\"1736\":3,\"1746\":1,\"2196\":1,\"2245\":3,\"2411\":3,\"2412\":3,\"2423\":3,\"2425\":3,\"2429\":3,\"2431\":3,\"2432\":3,\"2460\":1}}],[\"strating\",{\"1\":{\"1883\":1}}],[\"strategies\",{\"1\":{\"696\":1,\"697\":1,\"2130\":1,\"2134\":1}}],[\"strategy\",{\"1\":{\"96\":1,\"286\":1,\"706\":2,\"2000\":1,\"2001\":1,\"2049\":1,\"2146\":1,\"2147\":1,\"2348\":1,\"2370\":2,\"2372\":1}}],[\"strange\",{\"1\":{\"96\":1}}],[\"st\",{\"0\":{\"185\":1,\"262\":1,\"463\":1,\"469\":1,\"2221\":1,\"2267\":1,\"2546\":1},\"1\":{\"10\":2,\"162\":1,\"185\":1,\"190\":1,\"193\":1,\"227\":1,\"228\":2,\"242\":6,\"260\":4,\"261\":4,\"262\":4,\"263\":7,\"402\":1,\"403\":6,\"406\":13,\"463\":6,\"469\":6,\"995\":1,\"1439\":1,\"1441\":1,\"1760\":1,\"1892\":1,\"1975\":3,\"2221\":5,\"2262\":1,\"2267\":1}}],[\"style=false\",{\"1\":{\"2480\":1}}],[\"style=none\",{\"1\":{\"2480\":1}}],[\"style=style\",{\"1\":{\"269\":1,\"278\":1}}],[\"styletokenlayer\",{\"0\":{\"2430\":1},\"1\":{\"2430\":1}}],[\"styleencoder\",{\"0\":{\"2429\":1},\"1\":{\"2429\":1}}],[\"stylegan2\",{\"1\":{\"1110\":1}}],[\"stylemelgangenerator\",{\"0\":{\"1619\":1},\"1\":{\"1619\":2}}],[\"stylemelgandiscriminator\",{\"0\":{\"1618\":1},\"1\":{\"1618\":2}}],[\"stylemelgan\",{\"1\":{\"286\":1}}],[\"styles\",{\"1\":{\"225\":1}}],[\"style\",{\"0\":{\"32\":1,\"196\":1,\"1618\":2,\"1619\":2,\"1620\":1,\"1621\":1,\"1859\":1,\"1898\":1,\"2421\":1,\"2425\":1,\"2429\":1,\"2430\":1},\"1\":{\"6\":2,\"32\":1,\"38\":2,\"43\":2,\"106\":1,\"194\":1,\"197\":1,\"200\":2,\"205\":2,\"211\":1,\"213\":1,\"217\":3,\"218\":1,\"223\":1,\"224\":2,\"225\":1,\"231\":1,\"235\":1,\"240\":1,\"242\":1,\"244\":1,\"252\":1,\"254\":2,\"266\":1,\"267\":1,\"268\":1,\"275\":1,\"276\":1,\"277\":1,\"285\":2,\"286\":4,\"290\":4,\"295\":2,\"415\":2,\"570\":1,\"709\":3,\"710\":1,\"735\":1,\"750\":1,\"774\":3,\"780\":3,\"828\":1,\"1107\":3,\"1519\":4,\"1526\":1,\"1535\":3,\"1536\":3,\"1546\":3,\"1552\":3,\"1553\":1,\"1598\":1,\"1599\":4,\"1600\":1,\"1618\":3,\"1619\":3,\"1620\":1,\"1621\":1,\"1622\":3,\"1625\":1,\"1626\":3,\"1859\":2,\"1881\":1,\"1883\":1,\"1898\":1,\"1993\":1,\"1994\":1,\"2126\":1,\"2191\":3,\"2239\":3,\"2240\":2,\"2245\":2,\"2411\":5,\"2412\":5,\"2421\":1,\"2423\":5,\"2425\":3,\"2429\":7,\"2430\":7,\"2431\":2,\"2432\":2,\"2447\":3}}],[\"stamps\",{\"1\":{\"295\":1,\"415\":1}}],[\"staccato\",{\"1\":{\"269\":2,\"278\":2}}],[\"stacking\",{\"1\":{\"1611\":1}}],[\"stacks\",{\"1\":{\"980\":1,\"1267\":1,\"1268\":1,\"1552\":3,\"1553\":1,\"1605\":3,\"1610\":3,\"1611\":2,\"1612\":2,\"1613\":3,\"1625\":1,\"1626\":3,\"1628\":3}}],[\"stack\",{\"0\":{\"1359\":1,\"1615\":1},\"1\":{\"142\":1,\"634\":2,\"980\":3,\"993\":1,\"994\":1,\"1267\":3,\"1268\":3,\"1359\":1,\"1605\":4,\"1615\":2,\"2155\":2}}],[\"stacked\",{\"1\":{\"82\":1,\"994\":2,\"1117\":1,\"1124\":1,\"1125\":1,\"1126\":1,\"1130\":1,\"1131\":1,\"1133\":1,\"1134\":1,\"1136\":1,\"1137\":1,\"1139\":1,\"1141\":1,\"1232\":1,\"1252\":1,\"1311\":1,\"2155\":1}}],[\"stability\",{\"1\":{\"615\":1,\"640\":1,\"648\":1,\"1210\":1,\"1400\":1,\"1469\":1}}],[\"stabilize\",{\"1\":{\"262\":1,\"1246\":1}}],[\"stable\",{\"1\":{\"60\":1,\"104\":1,\"248\":1,\"846\":1,\"1247\":1}}],[\"standardupdater\",{\"0\":{\"1834\":1},\"1\":{\"1834\":3}}],[\"standardized\",{\"1\":{\"265\":1,\"269\":1,\"274\":1,\"278\":1}}],[\"standardizes\",{\"1\":{\"211\":1}}],[\"standard\",{\"1\":{\"143\":1,\"162\":1,\"235\":1,\"633\":1,\"637\":2,\"638\":1,\"646\":1,\"647\":1,\"652\":2,\"696\":1,\"701\":1,\"735\":1,\"785\":1,\"786\":1,\"809\":1,\"818\":1,\"829\":1,\"882\":1,\"883\":1,\"921\":1,\"922\":3,\"1002\":2,\"1301\":1,\"1334\":1,\"1372\":1,\"1770\":1,\"1771\":2,\"2280\":1,\"2307\":2}}],[\"standalone\",{\"1\":{\"138\":1,\"145\":1}}],[\"stand\",{\"1\":{\"133\":1}}],[\"stat\",{\"0\":{\"2183\":1,\"2208\":1},\"1\":{\"2183\":1,\"2208\":1}}],[\"status\",{\"0\":{\"112\":1,\"114\":1},\"1\":{\"801\":2,\"1132\":1,\"1167\":1,\"1204\":1,\"1209\":1,\"1228\":1}}],[\"statistics\",{\"0\":{\"2159\":1},\"1\":{\"199\":2,\"200\":5,\"204\":2,\"205\":5,\"210\":1,\"211\":3,\"216\":1,\"217\":3,\"218\":1,\"227\":1,\"234\":1,\"235\":2,\"240\":2,\"242\":5,\"253\":1,\"254\":2,\"265\":1,\"266\":4,\"267\":4,\"274\":1,\"275\":4,\"276\":4,\"284\":1,\"285\":3,\"286\":4,\"293\":1,\"625\":1,\"1126\":1,\"1354\":4,\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1521\":1,\"1526\":1,\"1548\":1,\"1553\":1,\"1585\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1853\":1,\"1951\":1,\"1965\":2,\"2130\":4,\"2131\":2,\"2134\":1,\"2137\":1,\"2149\":2,\"2152\":1,\"2153\":1,\"2159\":2,\"2161\":1,\"2166\":1,\"2208\":1,\"2228\":1,\"2229\":1,\"2235\":1,\"2236\":1,\"2239\":1,\"2240\":1,\"2245\":1,\"2327\":1,\"2408\":1,\"2411\":1,\"2412\":1,\"2423\":1,\"2431\":1,\"2432\":1,\"2446\":1,\"2447\":1}}],[\"statistical\",{\"1\":{\"102\":1,\"2183\":1}}],[\"staticmethod\",{\"1\":{\"756\":4,\"773\":4}}],[\"static\",{\"1\":{\"2\":1,\"647\":1,\"654\":2,\"674\":4,\"747\":2,\"756\":2,\"773\":2,\"787\":1,\"846\":4,\"866\":2,\"867\":2,\"974\":1,\"1063\":2,\"1157\":1,\"1269\":1,\"1270\":1,\"1334\":3,\"1654\":2,\"1666\":2,\"1668\":3,\"1725\":2,\"2220\":4,\"2338\":1,\"2369\":1}}],[\"statspooling\",{\"0\":{\"2208\":1},\"1\":{\"2208\":1}}],[\"stats=stats\",{\"1\":{\"2327\":2}}],[\"stats=true\",{\"1\":{\"1078\":1}}],[\"stats=false\",{\"1\":{\"243\":1}}],[\"stats\",{\"0\":{\"293\":1,\"1354\":1,\"1951\":2,\"2149\":2,\"2152\":1,\"2153\":1,\"2161\":2,\"2166\":1},\"1\":{\"78\":3,\"128\":1,\"218\":1,\"222\":1,\"223\":3,\"228\":2,\"243\":6,\"267\":2,\"276\":2,\"286\":5,\"293\":2,\"625\":4,\"640\":1,\"666\":1,\"736\":1,\"737\":1,\"777\":1,\"958\":1,\"1065\":1,\"1079\":1,\"1132\":1,\"1155\":2,\"1157\":3,\"1158\":1,\"1167\":1,\"1204\":1,\"1209\":1,\"1228\":1,\"1354\":2,\"1389\":1,\"1395\":1,\"1401\":1,\"1408\":1,\"1466\":1,\"1521\":1,\"1526\":1,\"1548\":4,\"1553\":1,\"1585\":1,\"1598\":1,\"1600\":1,\"1625\":1,\"1656\":2,\"1702\":2,\"1728\":1,\"1846\":1,\"1951\":3,\"1959\":1,\"1965\":1,\"1975\":1,\"1996\":1,\"1997\":1,\"2127\":1,\"2134\":5,\"2149\":2,\"2152\":1,\"2153\":1,\"2161\":3,\"2166\":1,\"2216\":1,\"2221\":1,\"2325\":3,\"2327\":2,\"2359\":3,\"2367\":1}}],[\"stated\",{\"1\":{\"927\":1}}],[\"statedecoder\",{\"1\":{\"828\":1,\"829\":1,\"830\":1}}],[\"state=none\",{\"1\":{\"819\":1,\"820\":1,\"821\":1,\"823\":1,\"824\":1,\"827\":1,\"828\":1,\"829\":2,\"830\":1,\"1257\":1,\"1814\":1,\"1816\":1}}],[\"state=64\",{\"1\":{\"817\":1}}],[\"statelessdecoder\",{\"0\":{\"651\":1},\"1\":{\"651\":2}}],[\"stateless\",{\"0\":{\"651\":1},\"1\":{\"91\":1,\"142\":2,\"631\":1,\"651\":2}}],[\"states=none\",{\"1\":{\"1232\":1,\"1261\":1}}],[\"states\",{\"0\":{\"1868\":1,\"1871\":1},\"1\":{\"87\":3,\"128\":1,\"614\":18,\"633\":2,\"634\":29,\"641\":22,\"642\":2,\"643\":27,\"649\":3,\"651\":22,\"678\":1,\"692\":4,\"699\":2,\"700\":2,\"709\":3,\"710\":6,\"711\":6,\"733\":2,\"734\":2,\"745\":2,\"746\":2,\"747\":2,\"748\":2,\"760\":3,\"771\":2,\"774\":3,\"780\":2,\"790\":3,\"791\":1,\"798\":1,\"820\":3,\"846\":2,\"847\":25,\"849\":2,\"850\":3,\"862\":1,\"1202\":1,\"1259\":1,\"1260\":1,\"1261\":1,\"1590\":1,\"1704\":1,\"1705\":1,\"1706\":1,\"1707\":1,\"1708\":1,\"1709\":1,\"1710\":1,\"1711\":1,\"1712\":5,\"1713\":1,\"1714\":1,\"1715\":1,\"1716\":1,\"1719\":9,\"1720\":1,\"1721\":1,\"1722\":3,\"1723\":4,\"1724\":3,\"1725\":9,\"1730\":1,\"1731\":1,\"1732\":1,\"1749\":23,\"1750\":4,\"1758\":3,\"1768\":1,\"1787\":3,\"1801\":1,\"1807\":3,\"1814\":1,\"1815\":25,\"1816\":1,\"1843\":23,\"1856\":4,\"1868\":5,\"1870\":3,\"1871\":7,\"1894\":2,\"1918\":2,\"1921\":4,\"1944\":3,\"1945\":3,\"1947\":3,\"1966\":2,\"1971\":3,\"1992\":4,\"1993\":1,\"1995\":1,\"2129\":2,\"2223\":2,\"2224\":1,\"2231\":3,\"2427\":1,\"2428\":1}}],[\"state\",{\"0\":{\"688\":1,\"689\":1,\"707\":1,\"708\":1,\"718\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"730\":1,\"744\":1,\"751\":1,\"757\":1,\"769\":1,\"784\":1,\"788\":1,\"793\":1,\"809\":1,\"811\":1,\"813\":1,\"817\":1,\"821\":1,\"823\":1,\"824\":1,\"827\":1,\"828\":1,\"829\":1,\"830\":1,\"835\":1,\"837\":1,\"842\":1,\"844\":1,\"852\":1,\"856\":1,\"858\":1,\"859\":1,\"860\":1,\"870\":1,\"871\":1,\"872\":1,\"877\":1,\"892\":1,\"895\":1,\"898\":1,\"903\":1,\"906\":1,\"907\":1,\"912\":1,\"913\":1,\"914\":1,\"924\":1,\"925\":1,\"926\":1,\"928\":1,\"929\":1,\"938\":1,\"939\":1,\"942\":1,\"943\":1,\"944\":1,\"945\":1,\"1870\":1,\"1894\":1,\"1916\":1,\"1918\":1,\"1921\":1,\"2106\":1,\"2308\":1},\"1\":{\"46\":2,\"87\":4,\"90\":1,\"102\":1,\"110\":1,\"190\":3,\"614\":8,\"628\":2,\"629\":1,\"630\":5,\"631\":8,\"632\":4,\"633\":3,\"634\":7,\"635\":1,\"637\":10,\"641\":7,\"642\":2,\"643\":3,\"649\":6,\"650\":1,\"651\":6,\"652\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"688\":1,\"689\":2,\"692\":4,\"693\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"706\":5,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"715\":1,\"718\":2,\"720\":1,\"724\":8,\"725\":8,\"726\":9,\"727\":6,\"728\":8,\"729\":4,\"730\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"741\":1,\"743\":2,\"744\":8,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"751\":2,\"752\":1,\"754\":1,\"757\":2,\"759\":1,\"760\":5,\"763\":4,\"766\":1,\"767\":1,\"769\":1,\"770\":3,\"771\":1,\"774\":1,\"775\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"784\":4,\"786\":1,\"787\":2,\"788\":2,\"790\":3,\"791\":1,\"793\":2,\"794\":1,\"796\":1,\"797\":8,\"798\":1,\"800\":1,\"805\":1,\"807\":1,\"809\":2,\"811\":2,\"813\":2,\"815\":1,\"817\":3,\"819\":5,\"820\":8,\"821\":6,\"822\":6,\"823\":10,\"824\":12,\"825\":1,\"827\":1,\"828\":15,\"829\":19,\"830\":15,\"833\":1,\"835\":2,\"837\":2,\"839\":1,\"841\":1,\"842\":2,\"844\":2,\"846\":1,\"847\":10,\"848\":1,\"849\":1,\"850\":5,\"851\":1,\"852\":2,\"854\":1,\"856\":2,\"858\":2,\"859\":9,\"860\":2,\"861\":1,\"862\":1,\"864\":1,\"877\":1,\"892\":1,\"895\":1,\"898\":1,\"903\":1,\"906\":1,\"907\":2,\"912\":1,\"913\":1,\"914\":1,\"924\":1,\"925\":1,\"926\":1,\"928\":1,\"929\":1,\"938\":2,\"939\":1,\"942\":1,\"943\":1,\"944\":1,\"945\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":1,\"959\":3,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"979\":1,\"981\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1050\":3,\"1051\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1072\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1116\":3,\"1117\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1125\":1,\"1126\":1,\"1127\":1,\"1130\":1,\"1131\":1,\"1132\":1,\"1133\":2,\"1134\":2,\"1136\":1,\"1137\":2,\"1139\":2,\"1141\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1156\":1,\"1158\":1,\"1159\":1,\"1161\":3,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1177\":1,\"1179\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":2,\"1187\":1,\"1189\":3,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":3,\"1203\":1,\"1205\":1,\"1207\":1,\"1208\":2,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":1,\"1218\":3,\"1219\":1,\"1221\":3,\"1222\":1,\"1223\":1,\"1224\":1,\"1225\":1,\"1226\":1,\"1229\":3,\"1230\":1,\"1232\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1244\":3,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":2,\"1253\":1,\"1255\":2,\"1257\":2,\"1259\":3,\"1261\":3,\"1262\":1,\"1264\":1,\"1265\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1333\":1,\"1334\":1,\"1381\":1,\"1383\":1,\"1387\":1,\"1392\":1,\"1398\":1,\"1400\":1,\"1404\":1,\"1406\":1,\"1411\":1,\"1417\":1,\"1419\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1433\":1,\"1435\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":2,\"1464\":1,\"1469\":1,\"1508\":1,\"1509\":1,\"1511\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1529\":6,\"1530\":1,\"1533\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1547\":1,\"1554\":1,\"1576\":1,\"1588\":1,\"1590\":3,\"1601\":1,\"1602\":1,\"1603\":1,\"1638\":1,\"1640\":1,\"1641\":1,\"1652\":1,\"1656\":1,\"1657\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1667\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1702\":1,\"1704\":4,\"1705\":4,\"1706\":4,\"1707\":3,\"1708\":4,\"1709\":4,\"1710\":4,\"1711\":4,\"1712\":4,\"1713\":4,\"1714\":4,\"1715\":4,\"1716\":4,\"1719\":4,\"1720\":2,\"1724\":4,\"1725\":4,\"1729\":3,\"1730\":11,\"1731\":28,\"1732\":2,\"1749\":9,\"1762\":2,\"1768\":4,\"1775\":4,\"1779\":4,\"1787\":4,\"1798\":4,\"1799\":7,\"1800\":6,\"1801\":3,\"1805\":4,\"1806\":2,\"1814\":2,\"1815\":9,\"1816\":2,\"1822\":19,\"1838\":1,\"1843\":9,\"1848\":6,\"1868\":1,\"1870\":3,\"1894\":2,\"1916\":3,\"1918\":1,\"1921\":5,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":6,\"1945\":2,\"1946\":6,\"1947\":6,\"1959\":1,\"1965\":1,\"1966\":15,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1992\":2,\"1994\":1,\"1996\":1,\"1997\":1,\"2010\":3,\"2011\":3,\"2012\":3,\"2013\":3,\"2020\":3,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2129\":1,\"2134\":16,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2221\":1,\"2222\":1,\"2232\":1,\"2238\":1,\"2305\":1,\"2308\":7,\"2314\":1,\"2325\":1,\"2327\":1,\"2355\":1,\"2359\":3,\"2367\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2407\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2434\":1,\"2443\":1,\"2445\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1}}],[\"stage1\",{\"1\":{\"195\":1,\"197\":1,\"269\":1,\"278\":1}}],[\"stage>\",{\"1\":{\"127\":1}}],[\"stages\",{\"1\":{\"111\":1,\"118\":6,\"120\":1,\"127\":1,\"174\":2,\"200\":1,\"205\":2,\"206\":1,\"211\":1,\"212\":1,\"217\":1,\"218\":1,\"223\":1,\"228\":4,\"232\":1,\"235\":1,\"236\":1,\"242\":1,\"243\":1,\"254\":1,\"255\":1,\"258\":1,\"259\":1,\"266\":1,\"267\":1,\"275\":1,\"276\":1,\"285\":1,\"286\":1,\"1068\":1,\"1087\":1,\"1091\":1,\"1230\":1,\"2131\":1}}],[\"stage3\",{\"1\":{\"68\":1}}],[\"stage\",{\"0\":{\"118\":2,\"174\":2},\"1\":{\"41\":1,\"118\":4,\"126\":2,\"127\":2,\"131\":1,\"133\":1,\"136\":2,\"137\":1,\"164\":1,\"174\":7,\"191\":2,\"192\":2,\"193\":1,\"195\":1,\"200\":17,\"201\":7,\"205\":12,\"211\":18,\"212\":10,\"217\":9,\"218\":10,\"222\":13,\"223\":23,\"224\":1,\"228\":29,\"235\":1,\"242\":10,\"243\":15,\"254\":7,\"263\":1,\"266\":20,\"267\":57,\"269\":2,\"275\":21,\"276\":42,\"278\":2,\"285\":16,\"286\":51,\"290\":1,\"748\":1,\"820\":2,\"828\":2,\"830\":1,\"1064\":1,\"1155\":2,\"1157\":2,\"1209\":1,\"2017\":1,\"2018\":3}}],[\"start=none\",{\"1\":{\"2480\":1}}],[\"start=false\",{\"1\":{\"1093\":1,\"1233\":1}}],[\"starts\",{\"1\":{\"2015\":1}}],[\"startb2\",{\"1\":{\"268\":1,\"277\":1}}],[\"startb1\",{\"1\":{\"268\":1,\"277\":1}}],[\"starta2\",{\"1\":{\"268\":1,\"277\":1}}],[\"starta1\",{\"1\":{\"268\":1,\"277\":1}}],[\"starttime2\",{\"1\":{\"242\":2}}],[\"starttime1\",{\"1\":{\"242\":2}}],[\"start\",{\"0\":{\"118\":1,\"174\":1},\"1\":{\"50\":1,\"78\":1,\"87\":1,\"91\":2,\"134\":3,\"135\":1,\"136\":1,\"174\":3,\"196\":3,\"200\":1,\"201\":1,\"219\":1,\"242\":1,\"243\":1,\"247\":2,\"268\":4,\"277\":4,\"286\":1,\"290\":1,\"543\":1,\"846\":1,\"961\":3,\"1000\":3,\"1025\":3,\"1028\":1,\"1063\":1,\"1316\":2,\"1320\":1,\"1389\":1,\"1396\":1,\"1401\":1,\"1408\":1,\"1419\":2,\"1466\":1,\"1478\":1,\"1552\":1,\"1556\":5,\"1599\":1,\"1626\":1,\"1632\":1,\"1633\":3,\"1654\":1,\"1666\":1,\"1719\":1,\"1721\":1,\"1725\":1,\"1727\":1,\"1797\":1,\"1823\":1,\"1837\":1,\"1862\":1,\"2018\":1,\"2065\":1,\"2130\":1,\"2136\":2,\"2150\":3,\"2156\":1,\"2220\":1,\"2359\":1,\"2367\":3,\"2434\":2}}],[\"started\",{\"1\":{\"26\":1,\"113\":1,\"262\":1}}],[\"starting\",{\"1\":{\"3\":1,\"88\":2,\"118\":1,\"290\":2,\"699\":1,\"831\":1,\"1556\":1,\"2039\":1,\"2134\":1}}],[\"shfl\",{\"1\":{\"704\":1}}],[\"shef\",{\"1\":{\"1717\":1}}],[\"shen\",{\"1\":{\"269\":1,\"278\":1}}],[\"shelf\",{\"1\":{\"8\":1}}],[\"shellcheck\",{\"1\":{\"224\":1}}],[\"shell\",{\"0\":{\"117\":1},\"1\":{\"3\":1,\"40\":1,\"68\":1,\"69\":1,\"117\":1,\"127\":1,\"166\":2,\"167\":1,\"224\":1}}],[\"shoud\",{\"1\":{\"1259\":1,\"1720\":1,\"1721\":1}}],[\"shouldn\",{\"1\":{\"821\":1}}],[\"should\",{\"1\":{\"24\":3,\"32\":1,\"42\":1,\"43\":1,\"46\":1,\"78\":1,\"79\":1,\"94\":1,\"106\":1,\"107\":1,\"108\":1,\"128\":1,\"131\":1,\"138\":1,\"139\":2,\"147\":1,\"148\":1,\"150\":2,\"154\":1,\"166\":1,\"173\":1,\"213\":2,\"224\":3,\"242\":6,\"243\":2,\"267\":1,\"269\":1,\"276\":7,\"278\":1,\"286\":1,\"663\":1,\"676\":1,\"677\":1,\"678\":1,\"679\":1,\"680\":1,\"681\":1,\"682\":1,\"683\":1,\"684\":1,\"685\":1,\"686\":1,\"687\":1,\"689\":1,\"690\":1,\"693\":1,\"694\":1,\"713\":1,\"714\":1,\"718\":1,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"723\":1,\"724\":3,\"725\":3,\"726\":1,\"727\":1,\"728\":3,\"729\":2,\"738\":1,\"739\":1,\"741\":1,\"742\":1,\"744\":3,\"752\":1,\"753\":1,\"756\":4,\"757\":1,\"758\":1,\"770\":1,\"773\":4,\"778\":1,\"779\":1,\"781\":1,\"782\":1,\"784\":2,\"785\":1,\"788\":1,\"789\":1,\"791\":1,\"792\":1,\"796\":1,\"797\":1,\"798\":1,\"799\":1,\"805\":1,\"806\":1,\"807\":1,\"808\":1,\"809\":1,\"810\":1,\"811\":1,\"812\":1,\"813\":1,\"814\":1,\"815\":1,\"816\":1,\"819\":1,\"821\":2,\"822\":1,\"825\":1,\"826\":1,\"828\":4,\"829\":5,\"830\":4,\"833\":1,\"834\":1,\"835\":1,\"836\":1,\"837\":1,\"838\":1,\"839\":1,\"840\":1,\"842\":1,\"843\":1,\"844\":1,\"845\":1,\"852\":1,\"853\":1,\"854\":1,\"855\":1,\"856\":1,\"857\":1,\"859\":1,\"860\":1,\"861\":1,\"862\":1,\"863\":1,\"864\":1,\"865\":1,\"866\":2,\"867\":2,\"922\":1,\"950\":1,\"951\":1,\"952\":1,\"953\":1,\"956\":1,\"957\":1,\"959\":1,\"960\":1,\"962\":1,\"963\":1,\"964\":1,\"965\":1,\"966\":1,\"967\":1,\"968\":1,\"969\":1,\"970\":1,\"1029\":1,\"1030\":1,\"1031\":2,\"1032\":1,\"1033\":1,\"1034\":1,\"1035\":1,\"1036\":1,\"1037\":1,\"1038\":1,\"1039\":1,\"1040\":1,\"1041\":1,\"1042\":1,\"1043\":1,\"1044\":1,\"1045\":1,\"1046\":1,\"1047\":1,\"1048\":1,\"1049\":1,\"1051\":1,\"1052\":1,\"1055\":1,\"1056\":1,\"1057\":1,\"1058\":1,\"1059\":1,\"1060\":1,\"1064\":1,\"1066\":1,\"1067\":1,\"1068\":1,\"1069\":1,\"1076\":1,\"1077\":1,\"1078\":2,\"1079\":1,\"1080\":1,\"1081\":1,\"1082\":1,\"1083\":1,\"1084\":1,\"1085\":1,\"1087\":1,\"1088\":1,\"1089\":1,\"1090\":1,\"1091\":1,\"1092\":1,\"1093\":1,\"1094\":1,\"1095\":1,\"1096\":1,\"1097\":1,\"1098\":1,\"1099\":1,\"1100\":1,\"1101\":1,\"1102\":1,\"1103\":1,\"1104\":1,\"1105\":1,\"1106\":1,\"1108\":1,\"1109\":1,\"1110\":1,\"1111\":1,\"1112\":1,\"1114\":1,\"1115\":1,\"1120\":1,\"1121\":1,\"1122\":1,\"1123\":1,\"1124\":1,\"1125\":1,\"1133\":1,\"1134\":2,\"1135\":1,\"1137\":2,\"1138\":1,\"1139\":1,\"1140\":1,\"1142\":1,\"1143\":1,\"1145\":1,\"1146\":1,\"1149\":1,\"1150\":1,\"1151\":1,\"1152\":1,\"1153\":2,\"1154\":1,\"1155\":1,\"1157\":1,\"1159\":1,\"1160\":1,\"1165\":1,\"1166\":1,\"1168\":1,\"1169\":1,\"1177\":1,\"1178\":1,\"1185\":1,\"1186\":1,\"1187\":1,\"1188\":1,\"1190\":1,\"1191\":1,\"1192\":1,\"1193\":1,\"1194\":1,\"1195\":1,\"1196\":1,\"1197\":1,\"1200\":1,\"1201\":1,\"1202\":2,\"1203\":1,\"1205\":1,\"1206\":1,\"1208\":1,\"1209\":1,\"1211\":1,\"1212\":1,\"1213\":1,\"1214\":1,\"1215\":1,\"1216\":1,\"1219\":1,\"1220\":1,\"1222\":1,\"1226\":1,\"1227\":1,\"1228\":1,\"1230\":1,\"1231\":1,\"1233\":1,\"1234\":1,\"1235\":1,\"1236\":1,\"1237\":1,\"1238\":1,\"1239\":1,\"1240\":1,\"1241\":1,\"1242\":1,\"1243\":1,\"1248\":1,\"1249\":1,\"1250\":1,\"1253\":1,\"1254\":1,\"1255\":2,\"1256\":1,\"1257\":2,\"1258\":1,\"1259\":1,\"1260\":1,\"1262\":2,\"1263\":1,\"1265\":1,\"1266\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1284\":1,\"1285\":1,\"1286\":1,\"1287\":1,\"1288\":1,\"1289\":1,\"1290\":1,\"1383\":1,\"1384\":1,\"1387\":1,\"1388\":1,\"1392\":1,\"1393\":1,\"1398\":1,\"1399\":1,\"1404\":1,\"1405\":1,\"1406\":1,\"1407\":1,\"1411\":1,\"1412\":1,\"1413\":1,\"1414\":1,\"1415\":1,\"1416\":1,\"1417\":1,\"1418\":1,\"1420\":1,\"1421\":1,\"1422\":1,\"1423\":1,\"1424\":1,\"1425\":1,\"1426\":1,\"1427\":1,\"1428\":1,\"1429\":1,\"1430\":1,\"1431\":1,\"1433\":1,\"1434\":1,\"1435\":1,\"1436\":1,\"1437\":1,\"1438\":1,\"1439\":1,\"1440\":1,\"1442\":1,\"1443\":1,\"1444\":1,\"1445\":1,\"1446\":1,\"1447\":1,\"1448\":1,\"1449\":1,\"1450\":1,\"1451\":1,\"1452\":1,\"1453\":1,\"1454\":1,\"1455\":1,\"1456\":1,\"1457\":1,\"1458\":1,\"1459\":1,\"1460\":1,\"1461\":1,\"1462\":1,\"1463\":1,\"1464\":1,\"1465\":1,\"1469\":1,\"1470\":1,\"1509\":1,\"1510\":1,\"1511\":1,\"1512\":1,\"1517\":1,\"1518\":1,\"1522\":1,\"1523\":1,\"1527\":1,\"1528\":1,\"1530\":1,\"1531\":1,\"1537\":1,\"1538\":1,\"1539\":1,\"1540\":1,\"1541\":1,\"1542\":1,\"1543\":1,\"1544\":1,\"1545\":1,\"1549\":1,\"1550\":1,\"1554\":1,\"1555\":1,\"1638\":1,\"1639\":1,\"1652\":1,\"1653\":1,\"1656\":1,\"1657\":1,\"1658\":1,\"1660\":1,\"1662\":2,\"1663\":1,\"1664\":1,\"1665\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1697\":1,\"1698\":1,\"1751\":1,\"1758\":1,\"1794\":1,\"1932\":1,\"1938\":2,\"1939\":1,\"1940\":1,\"1941\":1,\"1942\":1,\"1943\":1,\"1945\":1,\"1946\":1,\"1957\":1,\"1958\":1,\"1967\":1,\"1968\":1,\"1969\":1,\"1970\":1,\"1972\":1,\"1973\":1,\"1975\":1,\"1976\":1,\"1978\":1,\"1979\":1,\"1980\":1,\"1981\":1,\"1982\":1,\"1983\":1,\"1985\":1,\"1986\":1,\"1988\":1,\"1989\":1,\"2026\":1,\"2027\":1,\"2028\":1,\"2029\":1,\"2030\":1,\"2031\":1,\"2032\":1,\"2033\":1,\"2034\":1,\"2035\":1,\"2049\":1,\"2124\":1,\"2125\":1,\"2130\":1,\"2131\":2,\"2134\":1,\"2136\":1,\"2168\":1,\"2169\":1,\"2170\":1,\"2171\":1,\"2172\":1,\"2173\":1,\"2174\":1,\"2175\":1,\"2177\":1,\"2178\":1,\"2179\":1,\"2180\":1,\"2181\":1,\"2182\":1,\"2185\":1,\"2186\":1,\"2188\":1,\"2189\":1,\"2192\":1,\"2193\":1,\"2194\":1,\"2195\":1,\"2196\":1,\"2197\":1,\"2198\":1,\"2199\":1,\"2200\":1,\"2201\":1,\"2203\":1,\"2204\":1,\"2205\":1,\"2206\":1,\"2209\":1,\"2210\":1,\"2211\":1,\"2212\":1,\"2216\":1,\"2217\":1,\"2231\":1,\"2232\":1,\"2238\":1,\"2246\":2,\"2248\":2,\"2249\":2,\"2250\":2,\"2251\":2,\"2252\":2,\"2253\":2,\"2254\":2,\"2255\":2,\"2256\":2,\"2257\":2,\"2259\":2,\"2260\":2,\"2261\":2,\"2263\":2,\"2264\":2,\"2265\":2,\"2266\":2,\"2267\":2,\"2268\":2,\"2269\":2,\"2270\":2,\"2271\":2,\"2272\":2,\"2273\":2,\"2305\":1,\"2306\":1,\"2325\":1,\"2326\":1,\"2355\":2,\"2401\":1,\"2402\":1,\"2405\":1,\"2406\":1,\"2409\":1,\"2410\":1,\"2414\":1,\"2415\":1,\"2416\":1,\"2417\":1,\"2418\":1,\"2419\":1,\"2434\":1,\"2435\":1,\"2443\":1,\"2444\":1,\"2449\":1,\"2450\":1,\"2451\":1,\"2452\":1,\"2453\":1,\"2454\":1,\"2456\":1,\"2457\":1,\"2458\":1,\"2459\":1,\"2460\":1,\"2461\":1,\"2465\":1,\"2466\":1,\"2467\":1,\"2468\":1}}],[\"shot\",{\"1\":{\"792\":1}}],[\"shortcut\",{\"1\":{\"1876\":1}}],[\"shortcut=false\",{\"1\":{\"1240\":1,\"1242\":1}}],[\"shorter\",{\"1\":{\"831\":1}}],[\"short\",{\"0\":{\"663\":1,\"1869\":1},\"1\":{\"147\":4,\"197\":1,\"199\":1,\"200\":1,\"204\":1,\"205\":2,\"211\":1,\"216\":1,\"217\":2,\"222\":1,\"223\":1,\"227\":1,\"228\":1,\"240\":1,\"242\":2,\"266\":1,\"275\":1,\"284\":1,\"285\":3,\"653\":1,\"661\":4,\"663\":2,\"1643\":1,\"1646\":1,\"1735\":2,\"1842\":2,\"1869\":2,\"2336\":1,\"2337\":1,\"2346\":1,\"2353\":3,\"2356\":1,\"2360\":1,\"2361\":1,\"2362\":1,\"2364\":3,\"2368\":1}}],[\"shorthand\",{\"1\":{\"107\":1}}],[\"shown\",{\"1\":{\"90\":1,\"92\":1,\"223\":1,\"243\":1,\"1246\":1,\"2249\":1,\"2355\":1}}],[\"shows\",{\"1\":{\"84\":1,\"247\":1,\"276\":1,\"2143\":1}}],[\"show\",{\"0\":{\"84\":1,\"113\":1,\"114\":1,\"534\":1},\"1\":{\"80\":1,\"84\":5,\"91\":1,\"117\":1,\"134\":1,\"201\":1,\"206\":1,\"212\":2,\"218\":1,\"223\":1,\"255\":1,\"267\":2,\"276\":2,\"286\":3,\"335\":2,\"342\":2,\"361\":2}}],[\"shuffling\",{\"1\":{\"2134\":1,\"2249\":1,\"2253\":1}}],[\"shuffle=true\",{\"1\":{\"2134\":1}}],[\"shuffled\",{\"1\":{\"1644\":1}}],[\"shuffle\",{\"1\":{\"1642\":1,\"1643\":1,\"1644\":1,\"1645\":4,\"1646\":1,\"1647\":1,\"1648\":2,\"1650\":3,\"1797\":1,\"1823\":1,\"2134\":3}}],[\"shuld\",{\"1\":{\"852\":1}}],[\"shuai\",{\"1\":{\"13\":1}}],[\"shun\",{\"1\":{\"10\":1,\"14\":1}}],[\"shall\",{\"1\":{\"768\":1}}],[\"shallow\",{\"1\":{\"262\":1,\"2130\":1}}],[\"shape=none\",{\"1\":{\"1824\":1}}],[\"shape=false\",{\"1\":{\"1772\":1,\"1780\":1,\"1825\":1,\"1828\":1}}],[\"shape=\",{\"1\":{\"1072\":1,\"1074\":1,\"1179\":1,\"1296\":1}}],[\"shape`\",{\"1\":{\"692\":2,\"790\":1,\"850\":1,\"1992\":1}}],[\"shapes\",{\"1\":{\"243\":1,\"702\":1,\"705\":1,\"724\":1,\"725\":1,\"728\":1,\"729\":1,\"804\":1,\"829\":2,\"830\":1,\"859\":1,\"932\":1,\"934\":1,\"1180\":1,\"1181\":1,\"1719\":1,\"1725\":1,\"1806\":1,\"2155\":1}}],[\"shape2\",{\"1\":{\"100\":2}}],[\"shape\",{\"0\":{\"521\":1,\"567\":1,\"1300\":1,\"1302\":1,\"1495\":1},\"1\":{\"95\":1,\"96\":6,\"97\":6,\"98\":8,\"99\":4,\"100\":9,\"101\":4,\"200\":2,\"205\":2,\"211\":1,\"217\":1,\"218\":2,\"235\":1,\"242\":2,\"254\":1,\"266\":1,\"267\":2,\"275\":1,\"276\":2,\"285\":1,\"286\":2,\"377\":2,\"449\":2,\"521\":4,\"567\":2,\"615\":2,\"640\":2,\"648\":2,\"692\":1,\"702\":4,\"703\":6,\"712\":4,\"717\":2,\"722\":2,\"724\":1,\"725\":1,\"726\":2,\"727\":1,\"728\":1,\"744\":1,\"755\":6,\"760\":2,\"785\":3,\"787\":2,\"790\":1,\"797\":1,\"804\":3,\"819\":2,\"820\":2,\"823\":1,\"824\":3,\"828\":2,\"829\":3,\"830\":1,\"832\":3,\"850\":1,\"852\":1,\"854\":1,\"856\":1,\"859\":2,\"878\":5,\"879\":5,\"881\":7,\"882\":5,\"883\":5,\"884\":7,\"919\":2,\"922\":3,\"932\":3,\"934\":3,\"936\":3,\"937\":3,\"987\":2,\"988\":4,\"989\":2,\"990\":4,\"994\":1,\"1010\":1,\"1061\":2,\"1063\":2,\"1064\":1,\"1089\":1,\"1093\":1,\"1118\":1,\"1119\":2,\"1133\":2,\"1134\":1,\"1137\":1,\"1163\":2,\"1164\":4,\"1181\":1,\"1196\":1,\"1198\":3,\"1208\":3,\"1224\":3,\"1225\":3,\"1233\":1,\"1245\":3,\"1255\":1,\"1257\":1,\"1259\":1,\"1264\":2,\"1269\":1,\"1270\":1,\"1271\":1,\"1290\":2,\"1300\":1,\"1301\":5,\"1302\":1,\"1306\":5,\"1314\":4,\"1315\":2,\"1325\":2,\"1331\":1,\"1332\":1,\"1333\":2,\"1334\":5,\"1350\":3,\"1351\":1,\"1352\":1,\"1371\":5,\"1372\":5,\"1374\":1,\"1375\":1,\"1376\":1,\"1377\":1,\"1387\":1,\"1391\":4,\"1400\":1,\"1403\":4,\"1406\":3,\"1410\":4,\"1468\":4,\"1495\":2,\"1504\":1,\"1514\":1,\"1515\":4,\"1516\":3,\"1535\":3,\"1556\":15,\"1719\":4,\"1720\":1,\"1723\":1,\"1724\":1,\"1725\":3,\"1731\":2,\"1733\":2,\"1748\":2,\"1751\":1,\"1787\":1,\"1798\":1,\"1799\":1,\"1800\":1,\"1803\":2,\"1805\":1,\"1806\":2,\"1822\":1,\"1849\":2,\"1854\":5,\"1881\":3,\"1901\":1,\"1903\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1951\":1,\"1992\":1,\"2000\":4,\"2001\":4,\"2002\":1,\"2003\":1,\"2004\":1,\"2005\":2,\"2007\":2,\"2008\":1,\"2130\":4,\"2133\":2,\"2136\":3,\"2139\":4,\"2155\":3,\"2167\":2,\"2176\":2,\"2183\":3,\"2190\":3,\"2207\":2,\"2208\":3,\"2258\":2,\"2378\":2,\"2435\":2,\"2439\":1}}],[\"shard\",{\"1\":{\"2166\":1}}],[\"sharding\",{\"1\":{\"2134\":1,\"2162\":1}}],[\"sharded\",{\"0\":{\"57\":1},\"1\":{\"57\":2,\"377\":2,\"449\":2,\"2348\":1,\"2370\":2,\"2372\":1}}],[\"sharing\",{\"0\":{\"127\":1},\"1\":{\"127\":1,\"141\":1,\"200\":1,\"205\":1,\"217\":1,\"223\":2,\"235\":1,\"242\":1,\"285\":1}}],[\"shared\",{\"0\":{\"1382\":1,\"1383\":1,\"1385\":1,\"1386\":1,\"1387\":1,\"1392\":1,\"1394\":1,\"1400\":1,\"1411\":1,\"1413\":1,\"1415\":1,\"1417\":1,\"1419\":1,\"1420\":1,\"1422\":1,\"1424\":1,\"1426\":1,\"1428\":1,\"1430\":1,\"1432\":1,\"1437\":1,\"1439\":1,\"1441\":1,\"1442\":1,\"1444\":1,\"1446\":1,\"1448\":1,\"1450\":1,\"1452\":1,\"1454\":1,\"1456\":1,\"1458\":1,\"1460\":1,\"1462\":1,\"1464\":1,\"1469\":1,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1505\":1,\"1506\":1,\"1507\":1},\"1\":{\"60\":1,\"62\":1,\"141\":1,\"142\":2,\"614\":1,\"629\":1,\"633\":1,\"634\":1,\"635\":1,\"650\":1,\"652\":1,\"675\":1,\"676\":1,\"678\":1,\"680\":1,\"682\":1,\"684\":1,\"686\":1,\"689\":1,\"692\":1,\"693\":1,\"699\":1,\"700\":1,\"701\":1,\"702\":1,\"705\":1,\"706\":1,\"709\":1,\"710\":1,\"711\":1,\"712\":1,\"713\":1,\"715\":1,\"718\":1,\"720\":1,\"724\":1,\"725\":1,\"726\":1,\"727\":1,\"728\":1,\"729\":1,\"731\":1,\"732\":1,\"733\":1,\"734\":1,\"735\":1,\"736\":1,\"737\":1,\"741\":1,\"744\":1,\"745\":1,\"746\":1,\"747\":1,\"748\":1,\"749\":1,\"751\":1,\"752\":1,\"754\":1,\"757\":1,\"759\":1,\"760\":1,\"766\":1,\"767\":1,\"771\":1,\"774\":1,\"775\":1,\"777\":1,\"778\":1,\"780\":1,\"781\":1,\"783\":1,\"786\":1,\"787\":1,\"788\":1,\"790\":1,\"791\":1,\"793\":1,\"794\":1,\"796\":1,\"798\":1,\"800\":1,\"805\":1,\"807\":1,\"809\":1,\"811\":1,\"813\":1,\"815\":1,\"820\":1,\"823\":1,\"825\":1,\"828\":1,\"829\":1,\"830\":1,\"833\":1,\"835\":1,\"837\":1,\"839\":1,\"841\":1,\"842\":1,\"844\":1,\"846\":1,\"847\":1,\"848\":1,\"849\":1,\"850\":1,\"851\":1,\"852\":1,\"854\":1,\"856\":1,\"859\":1,\"860\":1,\"862\":1,\"864\":1,\"947\":1,\"948\":1,\"949\":1,\"950\":1,\"952\":1,\"954\":1,\"955\":1,\"956\":1,\"958\":1,\"963\":1,\"965\":1,\"967\":1,\"969\":1,\"971\":1,\"972\":1,\"973\":1,\"974\":1,\"975\":1,\"976\":1,\"977\":1,\"979\":1,\"981\":1,\"1030\":1,\"1032\":1,\"1034\":1,\"1036\":1,\"1038\":1,\"1040\":1,\"1042\":1,\"1044\":1,\"1046\":1,\"1048\":1,\"1051\":1,\"1054\":1,\"1055\":1,\"1057\":1,\"1059\":1,\"1063\":1,\"1064\":1,\"1065\":1,\"1066\":1,\"1068\":1,\"1072\":1,\"1074\":1,\"1075\":1,\"1076\":1,\"1078\":1,\"1084\":1,\"1086\":1,\"1087\":1,\"1089\":1,\"1091\":1,\"1093\":1,\"1095\":1,\"1097\":1,\"1099\":1,\"1101\":1,\"1103\":1,\"1105\":1,\"1108\":1,\"1110\":1,\"1112\":1,\"1113\":1,\"1114\":1,\"1119\":1,\"1120\":1,\"1122\":1,\"1126\":1,\"1127\":1,\"1132\":1,\"1133\":1,\"1134\":1,\"1137\":1,\"1139\":1,\"1142\":1,\"1144\":1,\"1145\":1,\"1148\":1,\"1149\":1,\"1151\":1,\"1153\":1,\"1156\":1,\"1158\":1,\"1159\":1,\"1163\":1,\"1164\":1,\"1165\":1,\"1167\":1,\"1168\":1,\"1170\":1,\"1171\":1,\"1172\":1,\"1173\":1,\"1174\":1,\"1175\":1,\"1177\":1,\"1179\":1,\"1182\":1,\"1183\":1,\"1184\":1,\"1185\":1,\"1187\":1,\"1190\":1,\"1192\":1,\"1194\":1,\"1196\":1,\"1198\":1,\"1199\":1,\"1200\":1,\"1202\":1,\"1205\":1,\"1207\":1,\"1208\":1,\"1210\":1,\"1211\":1,\"1213\":1,\"1215\":1,\"1217\":2,\"1219\":1,\"1222\":1,\"1223\":1,\"1226\":1,\"1230\":1,\"1233\":1,\"1236\":1,\"1238\":1,\"1240\":1,\"1242\":1,\"1246\":1,\"1247\":1,\"1248\":1,\"1250\":1,\"1251\":1,\"1252\":1,\"1253\":1,\"1255\":1,\"1257\":1,\"1259\":1,\"1261\":1,\"1262\":1,\"1264\":1,\"1265\":1,\"1269\":1,\"1270\":1,\"1271\":1,\"1272\":1,\"1275\":1,\"1276\":1,\"1277\":1,\"1279\":1,\"1281\":1,\"1282\":1,\"1284\":1,\"1286\":1,\"1288\":1,\"1290\":1,\"1333\":1,\"1334\":1,\"1381\":1,\"1382\":1,\"1383\":2,\"1385\":1,\"1386\":1,\"1387\":2,\"1392\":2,\"1394\":1,\"1398\":1,\"1400\":2,\"1404\":1,\"1406\":1,\"1411\":2,\"1413\":1,\"1415\":1,\"1417\":2,\"1419\":2,\"1420\":1,\"1422\":1,\"1424\":2,\"1426\":2,\"1428\":2,\"1430\":2,\"1432\":1,\"1433\":1,\"1435\":1,\"1437\":2,\"1439\":2,\"1441\":2,\"1442\":2,\"1444\":2,\"1446\":2,\"1448\":2,\"1450\":2,\"1452\":2,\"1454\":2,\"1456\":2,\"1458\":2,\"1460\":2,\"1462\":2,\"1464\":2,\"1469\":2,\"1471\":1,\"1472\":1,\"1473\":1,\"1474\":1,\"1475\":1,\"1476\":1,\"1477\":1,\"1478\":1,\"1479\":1,\"1480\":1,\"1481\":1,\"1482\":1,\"1483\":1,\"1484\":1,\"1485\":1,\"1488\":1,\"1489\":1,\"1490\":1,\"1491\":1,\"1492\":1,\"1493\":1,\"1494\":1,\"1495\":1,\"1496\":1,\"1497\":1,\"1498\":1,\"1499\":1,\"1500\":1,\"1501\":1,\"1502\":1,\"1503\":1,\"1504\":1,\"1505\":1,\"1506\":1,\"1507\":1,\"1508\":1,\"1509\":1,\"1511\":1,\"1515\":1,\"1516\":1,\"1517\":1,\"1522\":1,\"1527\":1,\"1530\":1,\"1533\":1,\"1537\":1,\"1539\":1,\"1541\":1,\"1543\":1,\"1545\":1,\"1547\":1,\"1554\":1,\"1576\":1,\"1588\":1,\"1590\":1,\"1601\":1,\"1602\":1,\"1603\":1,\"1638\":1,\"1640\":1,\"1641\":1,\"1652\":1,\"1656\":1,\"1657\":1,\"1660\":1,\"1662\":1,\"1664\":1,\"1665\":1,\"1667\":1,\"1669\":1,\"1670\":1,\"1671\":1,\"1702\":1,\"1838\":1,\"1938\":1,\"1940\":1,\"1942\":1,\"1944\":1,\"1945\":1,\"1947\":1,\"1959\":1,\"1965\":1,\"1967\":1,\"1969\":1,\"1971\":1,\"1972\":1,\"1974\":1,\"1975\":1,\"1977\":1,\"1978\":1,\"1980\":1,\"1982\":1,\"1984\":1,\"1985\":1,\"1987\":1,\"1988\":1,\"1990\":1,\"1991\":1,\"1994\":1,\"1996\":1,\"1997\":1,\"2026\":1,\"2028\":1,\"2030\":1,\"2032\":1,\"2034\":1,\"2124\":1,\"2126\":1,\"2127\":1,\"2129\":1,\"2167\":1,\"2168\":1,\"2170\":1,\"2172\":1,\"2174\":1,\"2176\":1,\"2177\":1,\"2179\":1,\"2181\":1,\"2183\":1,\"2184\":1,\"2185\":1,\"2187\":1,\"2188\":1,\"2190\":1,\"2191\":1,\"2192\":1,\"2194\":1,\"2196\":1,\"2198\":1,\"2200\":1,\"2202\":1,\"2203\":1,\"2205\":1,\"2207\":1,\"2208\":1,\"2209\":1,\"2211\":1,\"2213\":1,\"2214\":1,\"2215\":1,\"2216\":1,\"2221\":1,\"2222\":1,\"2232\":1,\"2238\":1,\"2305\":1,\"2325\":1,\"2327\":1,\"2401\":1,\"2403\":1,\"2405\":1,\"2407\":1,\"2409\":1,\"2414\":1,\"2416\":1,\"2418\":1,\"2420\":1,\"2434\":1,\"2443\":1,\"2445\":1,\"2449\":1,\"2451\":1,\"2453\":1,\"2455\":1,\"2456\":1,\"2458\":1,\"2460\":1,\"2462\":1,\"2463\":1,\"2464\":1,\"2465\":1,\"2467\":1,\"2469\":1,\"2470\":1,\"2471\":1,\"2472\":1,\"2473\":1,\"2481\":1}}],[\"share\",{\"1\":{\"45\":1,\"50\":1,\"127\":1,\"145\":1,\"150\":1,\"518\":1,\"674\":2,\"737\":2,\"1051\":1,\"1158\":1,\"1938\":1,\"1959\":2}}],[\"sharma2023espnet\",{\"1\":{\"15\":1}}],[\"sharma\",{\"1\":{\"6\":1,\"15\":2,\"202\":1,\"244\":1}}],[\"shanmugam\",{\"1\":{\"11\":1}}],[\"shakeel\",{\"1\":{\"6\":1}}],[\"shih\",{\"1\":{\"790\":1}}],[\"shikhar\",{\"1\":{\"211\":1,\"212\":1,\"214\":1}}],[\"shift=152000\",{\"1\":{\"2102\":1}}],[\"shifting\",{\"1\":{\"141\":1,\"629\":1,\"664\":1,\"1692\":2}}],[\"shifts\",{\"1\":{\"135\":1}}],[\"shift\",{\"0\":{\"1692\":1},\"1\":{\"36\":1,\"128\":1,\"134\":5,\"135\":5,\"136\":1,\"141\":1,\"267\":20,\"276\":7,\"286\":5,\"516\":3,\"523\":3,\"536\":2,\"537\":3,\"543\":2,\"548\":2,\"551\":2,\"558\":2,\"575\":2,\"606\":2,\"629\":2,\"644\":1,\"664\":2,\"1262\":3,\"1556\":7,\"1643\":1,\"1646\":1,\"1692\":3,\"1776\":1,\"1785\":1,\"1791\":1,\"1817\":1,\"1832\":1,\"1835\":1,\"1899\":1,\"1900\":1,\"1919\":2,\"1923\":1,\"1924\":1,\"2228\":1,\"2229\":1,\"2350\":1,\"2378\":1,\"2379\":1,\"2434\":1,\"2482\":3,\"2490\":3}}],[\"shifted\",{\"1\":{\"36\":1,\"1235\":1,\"1290\":2,\"1556\":1}}],[\"shi2022muskits\",{\"1\":{\"13\":1}}],[\"shigeki\",{\"1\":{\"10\":1,\"156\":1}}],[\"shinnosuke\",{\"1\":{\"9\":1}}],[\"shinji\",{\"1\":{\"5\":1,\"6\":1,\"7\":1,\"8\":1,\"9\":2,\"10\":2,\"11\":2,\"12\":1,\"13\":1,\"14\":1,\"15\":1,\"16\":1,\"156\":1,\"202\":1,\"207\":1}}],[\"shi\",{\"1\":{\"5\":1,\"6\":2,\"8\":1,\"9\":1,\"10\":1,\"11\":1,\"13\":1,\"14\":1,\"244\":1}}],[\"sh\",{\"0\":{\"40\":1,\"116\":1,\"223\":1,\"228\":1,\"515\":1,\"516\":1,\"517\":1,\"518\":1,\"519\":1,\"520\":1,\"521\":1,\"522\":1,\"523\":1,\"524\":1,\"525\":1,\"526\":1,\"527\":1,\"528\":1,\"529\":1,\"530\":1,\"531\":1,\"532\":1,\"533\":1,\"534\":1,\"535\":1,\"536\":1,\"537\":1},\"1\":{\"1\":6,\"2\":1,\"3\":3,\"22\":3,\"23\":2,\"24\":5,\"25\":1,\"26\":5,\"31\":2,\"32\":2,\"33\":1,\"37\":3,\"38\":2,\"39\":1,\"40\":5,\"41\":7,\"47\":6,\"68\":5,\"69\":1,\"71\":2,\"73\":1,\"76\":2,\"107\":1,\"108\":5,\"109\":6,\"110\":8,\"111\":2,\"117\":5,\"118\":6,\"119\":5,\"120\":2,\"121\":2,\"124\":3,\"125\":1,\"126\":1,\"127\":3,\"128\":4,\"131\":2,\"136\":1,\"137\":1,\"139\":3,\"152\":1,\"153\":2,\"154\":1,\"161\":2,\"162\":13,\"163\":7,\"164\":2,\"166\":2,\"173\":1,\"174\":2,\"175\":1,\"195\":3,\"196\":5,\"197\":22,\"200\":1,\"201\":4,\"205\":1,\"206\":6,\"211\":1,\"212\":10,\"213\":4,\"217\":1,\"218\":9,\"219\":1,\"220\":1,\"222\":2,\"223\":10,\"224\":16,\"225\":1,\"227\":12,\"228\":3,\"235\":1,\"236\":3,\"242\":2,\"243\":11,\"247\":1,\"254\":1,\"255\":6,\"259\":2,\"266\":1,\"267\":38,\"268\":5,\"270\":1,\"271\":1,\"272\":1,\"275\":1,\"276\":30,\"277\":5,\"279\":1,\"280\":1,\"282\":1,\"284\":1,\"285\":8,\"286\":39,\"287\":1,\"288\":1,\"289\":2,\"290\":5,\"515\":2,\"516\":2,\"517\":2,\"518\":2,\"519\":1,\"520\":3,\"521\":2,\"522\":2,\"523\":2,\"524\":2,\"525\":2,\"526\":1,\"527\":5,\"528\":1,\"529\":1,\"531\":1,\"533\":1,\"535\":3,\"536\":5,\"537\":2,\"1155\":1,\"1157\":1}}],[\"s\",{\"0\":{\"0\":1},\"1\":{\"18\":1,\"40\":1,\"67\":1,\"70\":1,\"71\":1,\"82\":1,\"96\":1,\"102\":1,\"106\":1,\"107\":1,\"110\":2,\"113\":1,\"126\":2,\"128\":1,\"140\":1,\"142\":1,\"161\":1,\"162\":1,\"166\":1,\"167\":1,\"168\":2,\"175\":1,\"185\":1,\"190\":1,\"196\":2,\"200\":1,\"205\":1,\"212\":1,\"213\":2,\"218\":1,\"223\":1,\"240\":1,\"243\":2,\"247\":3,\"249\":1,\"254\":3,\"262\":2,\"267\":1,\"268\":2,\"269\":2,\"276\":1,\"277\":2,\"278\":2,\"286\":2,\"287\":7,\"289\":1,\"290\":1,\"594\":1,\"625\":3,\"632\":3,\"639\":2,\"691\":1,\"692\":1,\"696\":1,\"697\":1,\"709\":1,\"710\":1,\"711\":1,\"774\":1,\"780\":1,\"790\":1,\"791\":1,\"817\":1,\"824\":4,\"833\":1,\"849\":1,\"864\":1,\"877\":1,\"947\":1,\"992\":1,\"994\":1,\"997\":1,\"1000\":1,\"1002\":1,\"1004\":1,\"1008\":1,\"1012\":1,\"1016\":1,\"1051\":1,\"1080\":1,\"1107\":1,\"1202\":1,\"1210\":2,\"1252\":1,\"1264\":2,\"1269\":6,\"1270\":6,\"1271\":6,\"1278\":1,\"1279\":1,\"1280\":1,\"1281\":1,\"1282\":1,\"1283\":1,\"1321\":2,\"1323\":2,\"1334\":2,\"1350\":1,\"1509\":1,\"1511\":1,\"1516\":1,\"1546\":2,\"1553\":1,\"1587\":1,\"1596\":1,\"1599\":2,\"1622\":2,\"1672\":1,\"1673\":1,\"1678\":1,\"1683\":1,\"1687\":1,\"1735\":1,\"1751\":1,\"1759\":1,\"1794\":1,\"1833\":1,\"1880\":1,\"1907\":1,\"1992\":1,\"1995\":1,\"2014\":1,\"2045\":1,\"2129\":1,\"2130\":1,\"2133\":1,\"2136\":1,\"2137\":2,\"2203\":1,\"2235\":1,\"2236\":1,\"2239\":2,\"2240\":2,\"2298\":1,\"2329\":1,\"2330\":1,\"2331\":1,\"2355\":3,\"2359\":1,\"2411\":2,\"2412\":2,\"2423\":2,\"2432\":2,\"2442\":2,\"2447\":2}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n,id:o}})=>{const u=bt[s];e==="suggest"?self.postMessage([e,o,tt(t,u,n)]):e==="search"?self.postMessage([e,o,Z(t,u,n)]):self.postMessage({suggestions:[e,o,tt(t,u,n)],results:[e,o,Z(t,u,n)]})};
//# sourceMappingURL=index.js.map
