# This configuration uses larger adim, more heads, and conv1d for position-wise layer compared to v1.
# If you cannot use 3 gpus, it is better to make batch-bins small, increase accum_grad and epochs.
# This model should be able to genearate speeech from around 40k iterations.

# network architecture related
model-module: espnet.nets.pytorch_backend.e2e_tts_transformer:Transformer
embed-dim: 0
eprenet-conv-layers: 0  # one more linear layer w/o non-linear will be added for 0-centor
eprenet-conv-filts: 0
eprenet-conv-chans: 0
dprenet-layers: 2  # one more linear layer w/o non-linear will be added for 0-centor
dprenet-units: 256
adim: 512
aheads: 8
elayers: 6
eunits: 1024
dlayers: 6
dunits: 1024
positionwise-layer-type: conv1d
positionwise-conv-kernel-size: 1
postnet-layers: 5
postnet-filts: 5
postnet-chans: 256
use-masking: True
bce-pos-weight: 5.0
use-batch-norm: True
use-scaled-pos-enc: True
encoder-normalize-before: False
decoder-normalize-before: False
encoder-concat-after: False
decoder-concat-after: False
reduction-factor: 1

# minibatch related
batch-sort-key: output # shuffle or input or output
batch-bins: 541353  # (char) 1197 batches containing from 6 to 51 samples (avg 10 samples).
                    # (phn) 1246 batches containing from 6 to 48 samples (avg 10 samples).



# training related
transformer-init: pytorch
transformer-warmup-steps: 4000
transformer-lr: 1.0
initial-encoder-alpha: 1.0
initial-decoder-alpha: 1.0
eprenet-dropout-rate: 0.0
dprenet-dropout-rate: 0.5
postnet-dropout-rate: 0.5
transformer-enc-dropout-rate: 0.1
transformer-enc-positional-dropout-rate: 0.1
transformer-enc-attn-dropout-rate: 0.1
transformer-dec-dropout-rate: 0.1
transformer-dec-positional-dropout-rate: 0.1
transformer-dec-attn-dropout-rate: 0.1
transformer-enc-dec-attn-dropout-rate: 0.1
use-guided-attn-loss: true
num-heads-applied-guided-attn: 2
num-layers-applied-guided-attn: 2
modules-applied-guided-attn: ["encoder-decoder"]

# optimization related
opt: noam
accum-grad: 6
grad-clip: 1.0
weight-decay: 0.0
patience: 0
epochs: 800  # 1000 epochs * ~1200 batches / 6 accum-grad = 160,000 iters

# other
save-interval-epoch: 50
