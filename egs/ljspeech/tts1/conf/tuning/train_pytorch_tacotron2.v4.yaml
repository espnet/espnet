# This configuration uses reduction factor = 1 and location-sensitive attention.
# Furthermore, to accelerate the learning of diaogonal attention, we additionally
# use guided attention loss. This leads super fast and robust attention learning.
# Also, this configuration uses length-weighted normalized loss instead of the
# avearge over all of the bins.

# encoder related
embed-dim: 512
elayers: 1
eunits: 512
econv-layers: 3 # if set 0, no conv layer is used
econv-chans: 512
econv-filts: 5

# decoder related
dlayers: 2
dunits: 1024
prenet-layers: 2  # if set 0, no prenet is used
prenet-units: 256
postnet-layers: 5 # if set 0, no postnet is used
postnet-chans: 512
postnet-filts: 5

# attention related
atype: location
adim: 128
aconv-chans: 32
aconv-filts: 15            # resulting in filter-size = aconv-filts * 2 + 1
cumulate-att-w: true       # whether to cumulate attetion weight
use-batch-norm: true       # whether to use batch normalization in conv layer
use-concate: true          # whether to concatenate encoder embedding with decoder lstm outputs
use-residual: false        # whether to use residual connection in encoder convolution
use-masking: false         # whether to apply mask for the padded part in loss calculation
use-weighted-masking: true # whether to apply weighted mask for the padded part in loss calculation
bce-pos-weight: 1.0        # weight for positive samples of stop token in cross-entropy calculation
use-guided-attn-loss: true
guided-attn-loss-sigma: 0.4
reduction-factor: 1

# minibatch related
batch-size: 32
batch-sort-key: shuffle # shuffle or input or output
maxlen-in: 150     # if input length  > maxlen-in, batchsize is reduced (if use "shuffle", not effect)
maxlen-out: 400    # if output length > maxlen-out, batchsize is reduced (if use "shuffle", not effect)

# optimization related
lr: 1e-3
eps: 1e-6
weight-decay: 0.0
dropout-rate: 0.5
zoneout-rate: 0.1
epochs: 200
patience: 0
