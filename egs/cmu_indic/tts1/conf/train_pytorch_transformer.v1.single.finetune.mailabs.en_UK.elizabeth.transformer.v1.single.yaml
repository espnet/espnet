model-module: espnet.nets.pytorch_backend.e2e_tts_transformer:Transformer
embed-dim: 0
eprenet-conv-layers: 0
eprenet-conv-filts: 0
eprenet-conv-chans: 0
dprenet-layers: 2
dprenet-units: 256
adim: 384
aheads: 4
elayers: 6
eunits: 1536
dlayers: 6
dunits: 1536
postnet-layers: 5
postnet-filts: 5
postnet-chans: 256
use-masking: true
bce-pos-weight: 5.0
use-batch-norm: true
use-scaled-pos-enc: true
encoder-normalize-before: false
decoder-normalize-before: false
encoder-concat-after: false
decoder-concat-after: false
reduction-factor: 1
batch-sort-key: input
batch-bins: 759000
transformer-init: pytorch
transformer-warmup-steps: 4000
transformer-lr: 0.1
initial-encoder-alpha: 1.0
initial-decoder-alpha: 1.0
eprenet-dropout-rate: 0.0
dprenet-dropout-rate: 0.5
postnet-dropout-rate: 0.5
transformer-enc-dropout-rate: 0.1
transformer-enc-positional-dropout-rate: 0.1
transformer-enc-attn-dropout-rate: 0.1
transformer-dec-dropout-rate: 0.1
transformer-dec-positional-dropout-rate: 0.1
transformer-dec-attn-dropout-rate: 0.1
transformer-enc-dec-attn-dropout-rate: 0.1
use-guided-attn-loss: true
num-heads-applied-guided-attn: 2
num-layers-applied-guided-attn: 2
modules-applied-guided-attn:
- encoder-decoder
opt: noam
accum-grad: 6
grad-clip: 1.0
weight-decay: 0.0
patience: 0
epochs: 1000
save-interval-epoch: 20
pretrained-model: downloads/mailabs.en_UK.elizabeth.transformer.v1.single/exp/en_UK_elizabeth_train_trim_pytorch_train_pytorch_transformer.v1.single/results/model.last1.avg.best
