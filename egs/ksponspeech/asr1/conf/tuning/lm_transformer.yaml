# This Transformer LM setting w/ 4 GPUs took around 60 days for 50 epochs.
# However, you can get better results in 6 days for 5 epochs (WER: 2.2/5.4/2.6/5.7)
# than LSTM LM (WER: 2.6/5.6/2.6/5.7) in 60 days for 20 epochs
# And if you does not have 4 GPUs, try accum-grad=4.

# network architecture
model-module: transformer
att-unit: 512
embed-unit: 128
head: 8
layer: 16
pos-enc: none
unit: 2048

# minibatch related
batchsize: 32
maxlen: 40

# optimization related
opt: adam
schedulers: lr=cosine
dropout-rate: 0.0
epoch: 50
gradclip: 1.0
lr: 1e-4
lr-cosine-total: 100000
lr-cosine-warmup: 1000
patience: 0
sortagrad: 0
