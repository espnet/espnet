layer: 4
unit: 2048
opt: sgd       # or adam
sortagrad: 0   # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs
batchsize: 512 # batch size in LM training
epoch: 20      # if the data size is large, we can reduce this
patience: 3
maxlen: 40     # if sentence length > lm_maxlen, lm_batchsize is automatically reduced
dropout-rate: 0.0
