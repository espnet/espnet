#!/bin/bash

# general configuration
backend=pytorch

ngpu=1          # number of gpus ("0" uses cpu, otherwise use gpu)
seed=1
debugmode=1
N=0            # number of minibatches to be used (mainly for debugging). "0" uses all minibatches.
verbose=0      # verbose option
resume=

# network archtecture
# encoder related
etype=blstmp # encoder architecture type
elayers=6
eunits=320
eprojs=320
subsample=1_2_2_1_1 # elayers 6 #skip every n frame from input to nth layers
#subsample=1 #_2_1_1 # elayers 2 #skip every n frame from input to nth layers
#subsample=1_2_2      # elayers 4 # skip every n frame from input to nth layers
# decoder related
dlayers=1
dunits=300

# attention related
atype=location
adim=320
awin=5
aheads=4
aconv_chans=10
aconv_filts=100

# hybrid CTC/attention
mtlalpha=0.5

# label smoothing
lsm_type=unigram
lsm_weight=0.05

# minibatch related
batchsize=30
maxlen_in=800  # if input length  > maxlen_in, batchsize is automatically reduced
maxlen_out=150 # if output length > maxlen_out, batchsize is automatically reduced

# optimization related
opt=adadelta
epochs=15

addtmpopt=''
[ $backend != "pytorch" ] && addtmpopt=_${backend}

tag_train=${etype}_e${elayers}_subsample${subsample}_unit${eunits}_proj${eprojs}_d${dlayers}_unit${dunits}_${atype}_aconvc${aconv_chans}_aconvf${aconv_filts}_mtlalpha${mtlalpha}_${opt}_bs${batchsize}_mli${maxlen_in}_mlo${maxlen_out}${addtmpopt}

opt_train="--ngpu ${ngpu} \
        --backend ${backend} \
        --debugmode ${debugmode} \
        --dict $lang/train_units.txt \
        --minibatches ${N} \
        --verbose ${verbose} \
        --resume ${resume} \
        --seed ${seed} \
        --etype ${etype} \
        --elayers ${elayers} \
        --eunits ${eunits} \
        --eprojs ${eprojs} \
        --subsample ${subsample} \
        --dlayers ${dlayers} \
        --dunits ${dunits} \
        --atype ${atype} \
        --adim ${adim} \
        --awin ${awin} \
        --aheads ${aheads} \
        --aconv-chans ${aconv_chans} \
        --aconv-filts ${aconv_filts} \
        --mtlalpha ${mtlalpha} \
        --lsm-type ${lsm_type} \
        --lsm-weight ${lsm_weight} \
        --batch-size ${batchsize} \
        --maxlen-in ${maxlen_in} \
        --maxlen-out ${maxlen_out} \
        --opt ${opt} \
        --epochs ${epochs}"

# decoding parameters
beam_size=20
penalty=0.0
maxlenratio=0.0
minlenratio=0.0
ctc_weight=0.3
recog_model=acc.best # set a model to be used for decoding: 'acc.best' or 'loss.best'

tag_eval=beam${beam_size}_e${recog_model}_p${penalty}_len${minlenratio}-${maxlenratio}

opt_eval="--ngpu 0\
	--backend ${backend} \
	--beam-size ${beam_size} \
	--penalty ${penalty} \
	--ctc-weight ${ctc_weight} \
	--maxlenratio ${maxlenratio} \
	--minlenratio ${minlenratio}"
