# Tacotron2-VC configuration w/o pretraining, using 1 gpu.

# v1.1: encoder-reduction-factor=1, reduction-factor=2, batch-size=1113600

model-module: espnet.nets.pytorch_backend.e2e_vc_tacotron2:Tacotron2

# encoder related
elayers: 1
eunits: 512
econv-layers: 3 # if set 0, no conv layer is used
econv-chans: 512
econv-filts: 5

# decoder related
dlayers: 2
dunits: 1024
prenet-layers: 2  # if set 0, no prenet is used
prenet-units: 256
postnet-layers: 5 # if set 0, no postnet is used
postnet-chans: 512
postnet-filts: 5

# attention related
atype: location
adim: 128
aconv-chans: 32
aconv-filts: 15      # resulting in filter-size = aconv-filts * 2 + 1
cumulate-att-w: true # whether to cumulate attetion weight
use-batch-norm: true # whether to use batch normalization in conv layer
use-concate: true    # whether to concatenate encoder embedding with decoder lstm outputs
use-residual: false  # whether to use residual connection in encoder convolution
use-masking: true    # whether to mask the padded part in loss calculation
bce-pos-weight: 1.0  # weight for positive samples of stop token in cross-entropy calculation
use-guided-attn-loss: true
guided-attn-loss-lambda: 10000.0
guided-attn-loss-sigma: 0.4
reduction-factor: 2
encoder-reduction-factor: 1
src-reconstruction-loss-lambda: 10.0
trg-reconstruction-loss-lambda: 10.0
enc-init-mods: enc
dec-init-mods: dec

# minibatch related
batch-sort-key: input # shuffle or input or output
batch-bins: 1113600    # 16 * (870 * 80 + 870 * 80) = 16 * 139200
                      # batch-size * (max_out * dim_out + max_in * dim_in)

# optimization related
lr: 1e-4
eps: 1e-6
weight-decay: 0.0
dropout-rate: 0.5
zoneout-rate: 0.1
epochs: 5000
patience: 0

# other
save-interval-epoch: 500
eval-interval-epoch: 100
